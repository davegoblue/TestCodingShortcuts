---
title: "COVID Tracking Project - Updated for 2021"
author: "davegoblue"
date: "1/20/2021"
output: html_document
---

## Background
This file is designed to update the analysis routine for the COVID Tracking Project data to allow for use of 2021 data.  This code is largely a subset of, and update to, code contained in Coronavirus_Statistics_CTP_v003.Rmd.  This file includes the latest code for analyzing data from [The COVID Tracking Project](https://covidtracking.com/).  The COVID Tracking Project contains data on positive tests, hospitalizations, deaths, and the like, for coronavirus in the US.  Downloaded data are unique by state and date.

Companion code is in Coronavirus_Statistics_Shared_v004.R and Coronavirus_Statistics_Functions_CTP_v004.R.  The code leverages tidyverse and a variable mapping file throughout:  
```{r}

# All functions assume that tidyverse and its components are loaded and available
# Other functions are declared in the sourcing files or use library::function()
library(tidyverse)

# If the same function is in both files, use the version from the more specific source
# For now, soruce from _v003, eventually update to _v004
# source("./Coronavirus_Statistics_Functions_Shared_v003.R")
# source("./Coronavirus_Statistics_Functions_CTP_v003.R")

# Create a variable mapping file
varMapper <- c("cases"="Cases", 
               "newCases"="Increase in cases, most recent 30 days",
               "casesroll7"="Rolling 7-day mean cases", 
               "deaths"="Deaths", 
               "newDeaths"="Increase in deaths, most recent 30 days",
               "deathsroll7"="Rolling 7-day mean deaths", 
               "cpm"="Cases per million",
               "cpm7"="Cases per day (7-day rolling mean) per million", 
               "newcpm"="Increase in cases, most recent 30 days, per million",
               "dpm"="Deaths per million", 
               "dpm7"="Deaths per day (7-day rolling mean) per million", 
               "newdpm"="Increase in deaths, most recent 30 days, per million", 
               "hpm7"="Currently Hospitalized per million (7-day rolling mean)", 
               "tpm"="Tests per million", 
               "tpm7"="Tests per million per day (7-day rolling mean)"
               )

# Helper functions used early in the process
# Function for saving an R object to RDS, including a check for whether the object already exists
saveToRDS <- function(obj, 
                      file=paste0(deparse(substitute(obj)), ".RDS"), 
                      dir="./RInputFiles/Coronavirus/", 
                      ovrWrite=FALSE, 
                      ovrWriteError=TRUE,
                      makeReadOnly=TRUE
                      ) {
    
    # FUNCTION ARGUMENTS:
    # obj: the R object to save
    # file: the file name to save as
    # dir: the directory to save in (file path will be paste0(dir, file))
    # ovrWrite: boolean, should the file be overwritten if it already exists?
    # ovrWriteError: boolean, should an error be thrown if an attempt is made to overwrite the file?
    # makeReadOnly: boolean, should the output file be made read-only?
    
    # Create the file name
    locFile <- paste0(dir, file)
    
    # Check if the file already exists and proceed as per options
    if (file.exists(locFile)) {
        cat("\nFile already exists:", locFile, "\n")
        if (!ovrWrite & ovrWriteError) stop("\nAborting due to ovrWrite=FALSE and ovrWriteError=TRUE")
        if (!ovrWrite) {
            cat("\nNot replacing the existing file since ovrWrite=FALSE\n")
            return(NULL)
        }
    }
    
    # Save the file and update the permissions to read-only (if flag is set)
    saveRDS(obj, file=locFile)
    if (makeReadOnly) Sys.chmod(locFile, mode="0555", use_umask = FALSE)
    
}



# Function for reading an R object from RDS
readFromRDS <- function(file, 
                        dir="./RInputFiles/Coronavirus/", 
                        addSuffix=".RDS", 
                        deparseSub=FALSE
                        ) {
    
    # FUNCTION ARGUMENTS:
    # file: the file name to read in
    # dir: the directory the file is in
    # addSuffix: the suffix that should be added to file (file path will be paste0(dir, file, addSuffix))
    # deparseSub: whether to deparse and substitute file (use it as the text name)
    
    # Convert file if needed
    if (deparseSub) file <- deparse(substitute(file))
    
    # Ensure that file is of type character
    if (!isTRUE(all.equal(class(file), "character"))) {
        stop("\nUnable to read since file is not a character\n")
    }
    
    # Create the file name
    locFile <- paste0(dir, file, addSuffix)
    
    # Read the file (will be the return)
    readRDS(locFile)
    
}


```
  
## Running Code  
The main function is readRunCOVIDTrackingProject(), which performs multiple tasks:  
  
STEP 1: Extracts a file of population by state (by default uses 2015 population from usmap::statepop)  
STEP 2a^: Downloads the latest data from COVID Tracking Project if requested  
STEP 2b^: Reads in data from a specified local file (may have just been downloaded in step 2a), and checks control total trends against a previous version of the file  
STEP 3^: Processed the loaded data file for keeping proper variables, dropping non-valid states, etc.  
STEP 4^: Adds per-capita metrics for cases, deaths, tests, and hospitalizations  
STEP 5: Adds existing clusters by state if passed as an argument to useClusters=, otherwise creates new segments based on user-defined parameters  
STEP 6^^: Creates assessment plots for the state-level clusters  
STEP 7^^: Creates consolidated plots of cases, hospitalizations, deaths, and tests  
STEP 8^^: Optionally, creates plots of cumulative burden by segments and by state  
STEP 9: Returns a list of key data frames, modeling objects, named cluster vectors, etc.  
  
^ The user can instead specify a previously processed file and skip steps 2a, 2b, 3, and 4.  The previously processed file needs to be formatted and filtered such that it can be used "as is"  
^^ The user can skip the segment-level assessments by setting skipAssessmentPlots=TRUE  
  
The main function and the helper functions are updated to allow for using 2021 data.

## Main Function  
The main function, readRunCOVIDTrackingProject() is copied:  
```{r}

# Function to download/load, process, segment, and analyze data from COVID Tracking Project
readRunCOVIDTrackingProject <- function(thruLabel, 
                                        downloadTo=NULL, 
                                        readFrom=downloadTo, 
                                        compareFile=NULL,
                                        dfPerCapita=NULL,
                                        useClusters=NULL,
                                        hierarchical=TRUE,
                                        returnList=!hierarchical, 
                                        kCut=6,
                                        reAssignState=vector("list", 0),
                                        makeCumulativePlots=TRUE,
                                        skipAssessmentPlots=FALSE,
                                        ...
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # thruLabel: the label for when the data are through (e.g., "Aug 30, 2020")
    # donwloadTo: download the most recent COVID Tracking Project data to this location
    #             NULL means do not download any data
    # readFrom: location for reading in the COVID Tracking Project data (defaults to donwloadTo)
    # compareFile: name of the file to use for comparisons when reading in raw data (NULL means no comparison)
    # dfPerCapita: file can be passed directly, which bypasses the loading and processing steps
    # useClusters: file containing clusters by state (NULL means make the clusters from the data)
    # hierarchical: boolean, should hierarchical clusters be produced (if FALSE, will be k-means)?
    # returnList: boolean, should a list be returned or just the cluster object?
    #             refers to what is returned by clusterStates(); the main function always returns a list
    # kCut: number of segments when cutting the hierarchical tree
    # reAssignState: mapping file for assigning a state to another state's cluster
    #                format list("stateToChange"="stateClusterToAssign")
    # makeCumulativePlots: whether to make plots of cumulative metrics
    # skipAssessmentPlots: boolean to skip the plots for assessClusters()
    #                      especially useful if just exploring dendrograms or silhouette widths
    # ...: arguments to be passed to clusterStates(), will be used only if useClusters is NULL
    
    
    # STEP 1: Get state data
    stateData <- getStateData()
    
    
    # STEPS 2-4 are run only if dfPerCapita does not exist
    if (is.null(dfPerCapita)) {
        
        # STEP 2a: Download latest COVID Tracking Project data (if requested)
        if (!is.null(downloadTo)) downloadCOVIDbyState(fileName=downloadTo)
        
        # STEP 2b: Read-in COVID Tracking Project data
        dfRaw <- readCOViDbyState(readFrom, checkFile=compareFile)
        glimpse(dfRaw)
        
        # STEP 3: Process the data so that it includes all requested key variables
        varsFilter <- c("date", "state", "positiveIncrease", "deathIncrease", 
                        "hospitalizedCurrently", "totalTestResultsIncrease"
        )
        dfFiltered <- processCVData(dfRaw, 
                                    varsKeep=varsFilter, 
                                    varsRename=c(positiveIncrease="cases", 
                                                 deathIncrease="deaths", 
                                                 hospitalizedCurrently="hosp", 
                                                 totalTestResultsIncrease="tests"
                                    )
        )
        glimpse(dfFiltered)
        
        # STEP 4: Convert to per capita
        dfPerCapita <- helperMakePerCapita(dfFiltered, 
                                           mapVars=c("cases"="cpm", "deaths"="dpm", 
                                                     "hosp"="hpm", "tests"="tpm"
                                           ), 
                                           popData=stateData
        )
        glimpse(dfPerCapita)
        
    } else {
        dfRaw <- NULL
        dfFiltered <- NULL
    }
    
    
    # STEP 5: Create the clusters (if they have not been passed)
    if (is.null(useClusters)) {
        # Run the clustering process
        clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
        # If hierarchical clusters, cut the tree, otherwise use the output object directly
        if (hierarchical) {
            useClusters <- cutree(clData, k=kCut)
        } else {
            useClusters <- clData$objCluster$cluster
        }
        # If requested, manually assign clusters to the cluster for another state
        for (xNum in seq_len(length(reAssignState))) {
            useClusters[names(reAssignState)[xNum]] <- useClusters[reAssignState[[xNum]]]
        }
        
    }
    
    
    # STEP 5a: Stop the process and return what is available if skipAssessmentPlots is TRUE
    if (skipAssessmentPlots) {
        return(list(stateData=stateData, 
                    dfRaw=dfRaw, 
                    dfFiltered=dfFiltered, 
                    dfPerCapita=dfPerCapita, 
                    useClusters=useClusters, 
                    plotData=NULL, 
                    consolidatedPlotData=NULL, 
                    clCum=NULL
                    )
               )
    }
    
    
    # STEP 6: Create the cluster assessments
    plotData <- assessClusters(useClusters, 
                               dfState=stateData, 
                               dfBurden=dfPerCapita,
                               thruLabel=thruLabel,
                               plotsTogether=TRUE
    )
    
    
    # STEP 7: Plot the consolidated metrics
    subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
    consolidatedPlotData <- plotConsolidatedMetrics(plotData, 
                                                    varMain=c("state", "cluster", "date", "pop",
                                                              "cases", "deaths", "hosp", "tests"
                                                    ), 
                                                    subT=subT, 
                                                    nrowPlot2=2
    )
    
    # STEP 8: Create cumulative metrics if requested
    if (makeCumulativePlots) {
        consPos <- consolidatedPlotData %>%
            ungroup() %>%
            select(state, cluster, date, name, vpm7) %>%
            arrange(state, cluster, date, name) %>%
            pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
            mutate(pctpos=cases/tests) %>%
            pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
            filter(!is.na(vpm7))
        clCum <- makeCumulative(consPos)
        plotCumulativeData(clCum, 
                           keyMetricp2="", 
                           flagsp2="", 
                           makep1=TRUE, 
                           makep2=FALSE
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="deaths", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "deaths")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="cases", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "cases")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="tests", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "tests")
        )
    } else {
        clCum <- NULL
    }
    
    
    # STEP 9: Return a list of the key data
    list(stateData=stateData, 
         dfRaw=dfRaw, 
         dfFiltered=dfFiltered, 
         dfPerCapita=dfPerCapita, 
         useClusters=useClusters, 
         plotData=plotData, 
         consolidatedPlotData=consolidatedPlotData, 
         clCum=clCum
    )
    
    
}

```

The following functions are examined for update:  
  
* getStateData - updated to default to using 2019 census data  
* downloadCOVIDbyState - function is OK, copied as-is  
* readCOVIDbyState - updated for cleaner log file  
* processCVData - function is OK, copied as-is  
* helperMakePerCapita - function is OK, copied as-is  
* clusterStates - updated with a custom shape function for year-month  
* assessClusters - function is OK  
* plotConsolidatedMetrics - function is OK  
* makeCumulative - function is OK  
* plotCumulativeData - function is OK  

So, the main effort will be to update the clusterStates() function.  There is also an opportunity to clean up the other functions so that output is directed to a separate log file and warnings caused by not setting .groups can also be addressed.

State population data have been downloaded from [US Census](https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/state/detail/).  The file is processed and saved so that it can be used in an updated getStateData() function:  
```{r cache=TRUE}

# Read in the state population file
statePop2019 <- readr::read_csv("./RInputFiles/Coronavirus/SCPRC-EST2019-18+POP-RES.csv")

# Create mapping file for state name to state abbreviation
stateMap <- tibble::tibble(stateName=c(state.name, "District of Columbia"), stateAbb=c(state.abb, "DC"))

# Create fields for state (abbreviation) and pop_2019
statePop2019 <- statePop2019 %>%
    full_join(stateMap, by=c("NAME"="stateName")) %>%
    mutate(pop_2019=POPESTIMATE2019)

# Check if anything did not merge properly
# Expected that United States and Puerto Rico will not merge, others should be a good match
statePop2019 %>%
    filter(is.na(stateAbb) | is.na(pop_2019))

# Delete the Puerto Rico and US totals data
statePop2019 <- statePop2019 %>%
    filter(!is.na(stateAbb))

# Glimpse file and then save to RDS
glimpse(statePop2019)
saveToRDS(statePop2019, ovrWrite=FALSE, ovrWriteError=FALSE)

```

The getStateData() function is updated to use defaults for this new file:  
```{r}

# Function to extract and format key state data
getStateData <- function(df=readFromRDS("statePop2019"), 
                         renameVars=c("stateAbb"="state", "NAME"="name", "pop_2019"="pop"), 
                         keepVars=c("state", "name", "pop")
                         ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing state data
    # renameVars: variables to be renamed, using named list with format "originalName"="newName"
    # keepVars: variables to be kept in the final file
    
    # Rename variables where appropriate
    names(df) <- ifelse(is.na(renameVars[names(df)]), names(df), renameVars[names(df)])
    
    # Return file with only key variables kept
    df %>%
        select_at(vars(all_of(keepVars)))
    
}

```
  
The function is then tested, with totals by state compared against previous:  
```{r}

# Using the new defaults
pop_2019 <- getStateData()
pop_2019

# Comparison to previous
fullPop <- pop_2019 %>%
    full_join(usmap::statepop, by=c("state"="abbr")) %>%
    mutate(pctChg=pop/pop_2015-1)

# Flag for any differences in name
fullPop %>%
    filter(is.na(name) | is.na(full) | name != full)

# Plot population and percent change
fullPop %>%
    ggplot(aes(x=fct_reorder(state, pctChg), y=pctChg)) + 
    geom_col(fill="lightblue") + 
    coord_flip() + 
    labs(title="Change in population from usmap::statepop to US Census 2019 estimate", 
         x="", 
         y="Percent change from 2015 (usmap) to 2019 (US Census)"
         )

```
  
The data appear to be reasonably aligned, with fast growing states and shrinking states roughly as expected.  

The function downloadCOVIDbyState() appears to be OK as-is, and is copied below:  
```{r}

# NO CHANGES MADE TO FUNCTION
# Function to download data for COVID Tracking Project
downloadCOVIDbyState <- function(fileName, 
                                 api="https://api.covidtracking.com/v1/states/daily.csv", 
                                 ovrWrite=FALSE
                                 ) {
    
    # COVID Tracking Project API allows data downloads for personal, non-commercial use
    # https://covidtracking.com/data/api
    
    # FUNCTION ARGUMENTS:
    # fileName: the filename that the data will be saved to
    # api: The API link for data downloads
    # ovrWrite: whether to allow overwriting of the existing fileName
    
    # Check whether fileName already exists
    if (file.exists(fileName)) {
        cat("\nFile already exists at:", fileName, "\n")
        if (ovrWrite) cat("Will over-write with current data from", api, "\n")
        else stop("Exiting due to ovrWrite=FALSE and a duplicate fileName\n")
    }
    
    # Download the file 
    download.file(api, destfile=fileName)
    
    # Show statistics on downloaded file
    file.info(fileName)
    
}

```
  
The download function is then run using 2021 data:  
```{r cache=TRUE}

# Example for downloading the 22-JAN-21 data file
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_210122.csv"
if (!file.exists(locDownload)) downloadCOVIDbyState(fileName=locDownload)

```

The function readCOVIDbyState() is copied below, with capability to direct control totals for changed items in an easier to read manner:  
```{r}

# Function to read, convert, and sanity check a downloaded file
readCOViDbyState <- function(fileName, 
                             checkFile=NULL, 
                             controlFields=c("positiveIncrease", "deathIncrease", "hospitalizedCurrently"), 
                             controlBy=c("state"), 
                             dateChangePlot=FALSE, 
                             dateMetricPrint=TRUE, 
                             controlByMetricPrint=TRUE, 
                             writeLog=NULL, 
                             ovrwriteLog=TRUE
                             ) {
    
    # FUNCTION ARGUMENTS:
    # fileName: the file name for reading the data
    # checkFile: a file that can be used for comparison purposes (NULL means do not compare to anything)
    # controlFields: fields that will be explicitly checked against checkFile
    # controlBy: level of aggregation at which fields will be explicitly checked against checkFile
    # dateChangePlot: boolean, should the change in dates included be plotte rather than listed?
    # dateMetricPrint: boolean, should the list of date-metric changes be printed?
    # controlByMetricPrint: boolean, should the list of controlBy-metric changes be printed?
    # writeLog: write detailed comparison to log file (NULL means do not write)
    # ovrwriteLog: boolean, should the log be started from scratch with the date comparisons?
    
    # Helper function to check for similarity of key elements
    helperSimilarity <- function(newData, refData, label, countOnly=FALSE, logFile=NULL, logAppend=TRUE) {
        d1 <- setdiff(refData, newData)
        d2 <- setdiff(newData, refData)
        cat("\n\nChecking for similarity of:", label)
        cat("\nIn reference but not in current:", if(countOnly) length(d1) else d1)
        cat("\nIn current but not in reference:", if(countOnly) length(d2) else d2)
        if (countOnly & !is.null(logFile)) {
            cat("\nDetailed differences available in:", logFile)
            capture.output(cat("\nIn reference but not in current:\n", paste(d1, collapse="\n")), 
                           cat("\nIn current but not in reference:\n", paste(d2, collapse="\n")), 
                           file=logFile, 
                           append=logAppend
                           )
        }
        if (countOnly) return(list(d1=d1, d2=d2))
    }
    
    # Read in the file and convert the numeric date field to date using ymd format
    df <- readr::read_csv(fileName) %>% 
        mutate(date=lubridate::ymd(date))
    
    # Check that the file is unique by date-state
    if ((df %>% select(date, state) %>% anyDuplicated()) != 0) {
        stop("\nDuplicates by date and state, investigate and fix\n")
    } else {
        cat("\nFile is unique by state and date\n")
    }
    
    # Check for overall control totals in new file
    cat("\n\nOverall control totals in file:\n")
    df %>% 
        summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>% 
        print()
    
    # Get control totals by date for new file
    dfByDate <- df %>% 
        group_by(date) %>%
        summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
        ungroup() %>%
        pivot_longer(-date, values_to="newValue")
    
    # If there is no checkFile, then just produce a plot of the key metrics
    if (is.null(checkFile)) {
        p1 <- dfByDate %>% 
            ggplot(aes(x=date, y=newValue)) + 
            geom_line() + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            labs(title="Control totals by date for new file (no reference file)", x="", y="Summed Value")
        print(p1)
    } else {
        # Check for similarity of fields, dates, and states
        cat("\n*** COMPARISONS TO REFERENCE FILE:", deparse(substitute(checkFile)))
        helperSimilarity(newData=names(df), refData=names(checkFile), label="column names")
        helperSimilarity(newData=df %>% pull(state) %>% unique(), 
                         refData=checkFile %>% pull(state) %>% unique(), 
                         label="states"
        )
        dateChangeList <- helperSimilarity(newData=df %>% 
                                               pull(date) %>% 
                                               unique() %>% 
                                               format("%Y-%m-%d") %>%
                                               sort(), 
                                           refData=checkFile %>% 
                                               pull(date) %>% 
                                               unique() %>% 
                                               format("%Y-%m-%d") %>%
                                               sort(),
                                           label="dates", 
                                           countOnly=dateChangePlot, 
                                           logFile=writeLog, 
                                           logAppend=!ovrwriteLog
                                           )
        
        # Plot date changes if requested
        if (dateChangePlot) {
            pDate <- tibble::tibble(date=as.Date(c(dateChangeList$d1, dateChangeList$d2)), 
                                    type=c(rep("Control File Only", length(dateChangeList$d1)), 
                                           rep("New File Only", length(dateChangeList$d2))
                                           )
                                    ) %>%
                ggplot(aes(x=date, fill=type)) + 
                geom_bar() + 
                coord_flip() + 
                labs(x="", y="", title="Dates in one file and not in the other")
            print(pDate)
        }
        
        # Check for similarity of control totals by date in files
        checkByDate <- checkFile %>% 
            group_by(date) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-date, values_to="oldValue")
        deltaDate <- dfByDate %>% 
            inner_join(checkByDate, by=c("date", "name")) %>%
            filter(abs(newValue-oldValue)>=5, 
                   pmax(newValue, oldValue)>=1.01*pmin(newValue, oldValue)
            ) %>%
            as.data.frame()
        cat("\n\nDifference of 5+ that is at least 1% (summed to date and metric):", nrow(deltaDate))
        if (dateMetricPrint) {
            cat("\n")
            print(deltaDate)
        }
        else if (!is.null(writeLog)) {
            cat("\nDetailed output available in log:", writeLog)
            capture.output(cat("\n\nChange by date:\n"), print(deltaDate), file=writeLog, append=TRUE)
        }
        p1 <- dfByDate %>% 
            full_join(checkByDate, by=c("date", "name")) %>%
            pivot_longer(-c(date, name), names_to="newOld") %>%
            ggplot(aes(x=date, y=value, group=newOld, color=newOld)) + 
            geom_line() + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            labs(title="Control totals by date for new and reference file", x="", y="Summed Value")
        print(p1)
        
        # Check for similarity of control totals by controlBy in files
        dfByControl <- df %>% 
            semi_join(select(checkFile, date), by="date") %>%
            group_by_at(vars(all_of(controlBy))) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-all_of(controlBy), values_to="newValue")
        checkByControl <- checkFile %>% 
            group_by_at(vars(all_of(controlBy))) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-all_of(controlBy), values_to="oldValue")
        deltaBy <- dfByControl %>% 
            inner_join(checkByControl, by=c(controlBy, "name")) %>%
            filter(abs(newValue-oldValue)>=5, 
                   pmax(newValue, oldValue)>=1.01*pmin(newValue, oldValue)
            ) %>%
            as.data.frame()
        cat("\n\nDifference of 5+ that is at least 1% (summed to", 
            controlBy, 
            "and metric):", 
            nrow(deltaBy), 
            "\n"
            )
        if (controlByMetricPrint) print(deltaBy)
    }
    
    # Return the processed data file
    df
    
}

```

The function is then applied to the downloaded 21-JAN-21 data, including a comparison to a previous 2020 file:  
```{r}

# Reading the file as a standalone
df1 <- readCOViDbyState(locDownload)

# Reading the file with a comparison to a previous file, previous approach
df2 <- readCOViDbyState(locDownload, 
                        checkFile=readFromRDS("test_hier5_201001")$dfRaw
                        )

# Reading the file with a comparison to a previous file, key output directed to a log
df3 <- readCOViDbyState(locDownload, 
                        checkFile=readFromRDS("test_hier5_201001")$dfRaw, 
                        dateChangePlot=TRUE, 
                        dateMetricPrint=FALSE, 
                        writeLog="./RInputFiles/Coronavirus/testLogCTP_v001.log", 
                        ovrwriteLog=TRUE
                        )

# Confirm that files are identical
identical(df1, df2)
identical(df1, df3)

```

The added functionality allows for a cleaner log, with detailed output optionally sent to a separate file.  Next steps are to continue updating elements of the main function.

The processCVData() and helperMakePerCapita() functions are copied, along with the associated helper function:  
```{r}

# Function to select relevant variables and observations, and report on control totals
processCVData <- function(dfFull, 
                          varsKeep=c("date", "state", "positiveIncrease", "deathIncrease"), 
                          varsRename=c("positiveIncrease"="cases", "deathIncrease"="deaths"), 
                          stateList=c(state.abb, "DC")
                          ) {
    
    # FUNCTION ARGUMENTS
    # dfFull: the full data file originally loaded
    # varsKeep: variables to keep from the full file
    # varsRename: variables to be renamed, using a named vector of form originalName=newName
    # stateList: variables for filtering state (NULL means do not run any filters)
    
    # Select only the key variables
    df <- dfFull %>%
        select_at(vars(all_of(varsKeep)))
    
    # Apply the renaming of variables
    names(df) <- ifelse(is.na(varsRename[names(df)]), names(df), varsRename[names(df)])
    
    # Designate each record as being either a valid state or not
    if (!is.null(stateList)) {
        df <- df %>%
            mutate(validState=state %in% stateList)
    } else {
        df <- df %>%
            mutate(validState=TRUE)
    }
    
    # Summarize the control totals for the data, based on whether the state is valid
    cat("\n\nControl totals - note that validState other than TRUE will be discarded\n\n")
    df %>%
        mutate(n=1) %>%
        group_by(validState) %>%
        summarize_if(is.numeric, sum) %>%
        print()
    
    # Return the file, filtered to where validState is TRUE, and deleting variable validState
    df %>%
        filter(validState) %>%
        select(-validState)
    
}



# Function to add per capita and rolling to the base data frame
# Updated function to take an arbitrary number of variables and convert them
helperMakePerCapita <- function(df, 
                                mapVars=c("cases"="cpm", "deaths"="dpm"),
                                popData=stateData,
                                k=7
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: the initial data frame for conversion
    # mapVars: named vector of variables to be converted 'original name'='converted name'
    # k: the rolling time period to use
    
    # Create the variables for per capita
    for (origVar in names(mapVars)) {
        df <- df %>% 
            helperPerCapita(origVar=origVar, newName=mapVars[origVar], popData=popData)
    }
    # Arrange the data by date in preparation for rolling aggregations
    df <- df %>% 
        group_by(state) %>% 
        arrange(date)
    # Create the rolling variables
    for (newVar in mapVars) {
        df <- df %>% 
            helperRollingAgg(origVar=newVar, newName=paste0(newVar, k), k=k)
    }
    
    # Return the updated data frame, ungrouped
    df %>%
        ungroup()
    
}



# Helper function to create per capita metrics
helperPerCapita <- function(df, 
                            origVar, 
                            newName,
                            byVar="state",
                            popVar="pop",
                            popData=stateData,
                            mult=1000000
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame currently being processed
    # origVar: the variables to be converted to per capita
    # newName: the new per capita variable name
    # byVar: the variable that will be merged by
    # popVar: the name of the population variable in the popData file
    # popData: the file containing the population data
    # mult: the multiplier, so that the metric is "per mult people"
    
    # Create the per capita variable
    df %>%
        inner_join(select_at(popData, vars(all_of(c(byVar, popVar)))), by=byVar) %>%
        mutate(!!newName:=mult*get(origVar)/get(popVar)) %>%
        select(-all_of(popVar))
    
}



# Helper function to create rolling aggregates
helperRollingAgg <- function(df, 
                             origVar, 
                             newName,
                             func=zoo::rollmean,
                             k=7, 
                             fill=NA, 
                             ...
                             ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing the data
    # origVar: the original data column name
    # newName: the new variable column name
    # func: the function to be applied (zoo::rollmean will be by far the most common)
    # k: the periodicity (k=7 is rolling weekly data)
    # fill: how to fill leading.trailing data to maintain the same vector lengths
    # ...: any other arguments to be passed to func
    
    # Create the appropriate variable
    df %>%
        mutate(!!newName:=func(get(origVar), k=k, fill=fill, ...))
    
}

```

The processCVData() function is applied to df3:  
```{r}

# STEP 3: Process the data so that it includes all requested key variables
varsFilter <- c("date", "state", "positiveIncrease", "deathIncrease", 
                "hospitalizedCurrently", "totalTestResultsIncrease"
                )
dfFilter3 <- processCVData(df3, 
                           varsKeep=varsFilter, 
                           varsRename=c(positiveIncrease="cases", 
                                        deathIncrease="deaths", 
                                        hospitalizedCurrently="hosp", 
                                        totalTestResultsIncrease="tests"
                                        )
                           )
glimpse(dfFilter3)

```

The helperMakePerCapita() function is applied to dfFilter3:  
```{r}

# STEP 4: Convert to per capita
dfPer3 <- helperMakePerCapita(dfFilter3, 
                              mapVars=c("cases"="cpm", "deaths"="dpm", "hosp"="hpm", "tests"="tpm"), 
                              popData=pop_2019
                              )
glimpse(dfPer3)

```

The clusterStates() function is updated to allow for both month and year in calculating shape (since data will now be from 2020 and 2021).  Addition of a custom shape function suffices:  
```{r}

# Updates to the clustering function
clusterStates <- function(df, 
                          caseVar="cpm", 
                          deathVar="dpm",
                          shapeFunc=lubridate::month, 
                          minShape=NULL, 
                          maxShape=NULL,
                          minDeath=0,
                          maxDeath=Inf,
                          minCase=0,
                          maxCase=Inf,
                          ratioTotalvsShape=1, 
                          ratioDeathvsCase=1, 
                          hierarchical=TRUE, 
                          hierMethod="complete", 
                          nCenters=3, 
                          iter.max=10,
                          nstart=1,
                          testCenters=NULL,
                          returnList=FALSE, 
                          hmlSegs=3, 
                          eslSegs=2,
                          seed=NULL
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing cases and deaths data
    # caseVar: the variable containing the cases per capita data
    # deathVar: the variable containing the deaths per capita data
    # shapeFunc: the function to be used for creating the shape of the curve
    # minShape: the minimum value to be used for shape (to avoid very small amounts of data in Jan/Feb/Mar)
    #           shape is the month, so 4 means start with April data (NULL means keep everything)
    # maxShape: the maximum value to be used for shape (to avoid very small amounts of data in a partial month)
    #           shape is the month, so 9 means end with September data (NULL means keep everything)
    # minDeath: use this value as a floor for the death metric when calculating shape
    # maxDeath: use this value as a maximum when calculating distance using deaths 
    # minCase: use this value as a floor for the case metric when calculating shape
    # maxCase: use this value as a maximum when calculating distance using cases 
    # ratioTotalvsShape: amount of standard deviation to be kept in total variable vs shape variables
    # ratioDeathvsCase: amount of standard deviation to be kept in deaths vs cases 
    #                   (total death data will be scaled to have sd this many times higher than cases)
    #                   (death percentages by time period will be scaled directly by this amount)
    # hierarchical: whether to create hierarchical clusters
    #               TRUE means run hierarchical clustering
    #               FALSE means run kmeans clustering
    #               NA means run rules-based clustering
    # hierMethod: the method for hierarchical clustering (e.g., 'complete' or 'single')
    # nCenters: the number of centers to use for kmeans clustering
    # testCenters: integer vector of centers to test (will create an elbow plot); NULL means do not test
    # iter.max: maximumum number of kmeans iterations (default in kmeans algorithm is 10)
    # nstart: number of random sets chosen for kmeans (default in kmeans algorithm is 1)
    # returnList: boolean, if FALSE just the cluster object is returned
    #                      if TRUE, a list is returned with dfCluster and the cluster object
    # hmlSegs: number of segments to create for volume of burden integrated over time
    # eslSegs: number of segments to create for shape of burden over time
    # seed: set the seed to this value (NULL means no seed)
    
    # Extract key information (aggregates and by shapeFunc for each state)
    df <- df %>%
        select_at(vars(all_of(c("date", "state", caseVar, deathVar)))) %>%
        purrr::set_names(c("date", "state", "cases", "deaths")) %>%
        mutate(timeBucket=shapeFunc(date)) %>%
        group_by(state, timeBucket) %>%
        summarize(cases=sum(cases), deaths=sum(deaths), .groups="drop") %>%
        ungroup()
    
    # Limit to only relevant time buckets if requested
    if (!is.null(minShape)) {
        df <- df %>%
            filter(timeBucket >= minShape)
    }
    if (!is.null(maxShape)) {
        df <- df %>%
            filter(timeBucket <= maxShape)
    }
    
    # Extract an aggregate by state, scaled so that they have the proper ratio
    dfAgg <- df %>%
        group_by(state) %>%
        summarize(totalCases=sum(cases), totalDeaths=sum(deaths), .groups="drop") %>%
        mutate(totalCases=pmin(totalCases, maxCase), totalDeaths=pmin(totalDeaths, maxDeath)) %>%
        ungroup() %>%
        mutate(totalDeaths=ratioDeathvsCase*totalDeaths*sd(totalCases)/sd(totalDeaths))
    
    # Extract the percentages (shapes) by month, scaled so that they have the proper ratio
    dfShape <- df %>%
        pivot_longer(-c(state, timeBucket)) %>%
        group_by(state, name) %>%
        mutate(tot=pmax(sum(value), ifelse(name=="deaths", minDeath, minCase)), 
               value=ifelse(name=="deaths", ratioDeathvsCase, 1) * value / tot) %>%
        select(-tot) %>%
        pivot_wider(state, names_from=c(name, timeBucket), values_from=value) %>%
        ungroup()
    
    # Function to calculate SD of a subset of columns
    calcSumSD <- function(df) {
        df %>% 
            ungroup() %>% 
            select(-state) %>% 
            summarize_all(.funs=sd) %>% 
            as.vector() %>% 
            sum()
    }
    
    # Down-weight the aggregate data so that there is the proper sum of sd in aggregates and shapes
    aggSD <- calcSumSD(dfAgg)
    shapeSD <- calcSumSD(dfShape)
    dfAgg <- dfAgg %>%
        mutate_if(is.numeric, ~. * ratioTotalvsShape * shapeSD / aggSD)
    
    # Combine so there is one row per state
    dfCluster <- dfAgg %>%
        inner_join(dfShape, by="state")
    
    # convert 'state' to rowname
    keyData <- dfCluster %>% 
        column_to_rownames("state")
    
    # Create rules-based segments (NA) or hierarchical segments (TRUE) or kmeans segments (FALSE)
    if (is.na(hierarchical)) {
        # Create pseudo-rules-based segments
        if (!is.null(seed)) set.seed(seed)
        # STEP 1: Classify high-medium-low based on deaths and cases
        hml <- kmeans(select(keyData, starts_with("total")), 
                      centers=hmlSegs, iter.max=iter.max, nstart=nstart
        )
        # STEP 2: Classify early-late based on shape
        esl <- kmeans(select(keyData, -starts_with("total")), 
                      centers=eslSegs, iter.max=iter.max, nstart=nstart
        )
        # STEP 3: Create a final segment
        objCluster <- eslSegs*(hml$cluster-1) + esl$cluster
    } else if (isTRUE(hierarchical)) {
        # Create hierarchical segments
        objCluster <-  hclust(dist(keyData), method=hierMethod)
        plot(objCluster)
    } else {
        # Create k-means segments
        # Create an elbow plot if testCenters is not NULL
        if (!is.null(testCenters)) {
            helperElbow(keyData, testCenters=testCenters, iter.max=iter.max, nstart=nstart, silhouette=TRUE)
        }
        # Create the kmeans cluster object, setting a seed if requested
        if (!is.null(seed)) set.seed(seed)
        objCluster <- kmeans(keyData, centers=nCenters, iter.max=iter.max, nstart=nstart)
        cat("\nCluster means and counts\n")
        n=objCluster$size %>% cbind(objCluster$centers) %>% round(2) %>% t() %>% print()
    }
    
    # Return the data and object is a list if returnList is TRUE, otherwise return only the clustering object
    if (!isTRUE(returnList)) {
        objCluster
    } else {
        list(objCluster=objCluster, dfCluster=dfCluster)
    }
    
}


# Function to create an elbow plot for various numbers of clusters in the data
helperElbow <- function(mtx, 
                        testCenters, 
                        iter.max, 
                        nstart, 
                        silhouette=FALSE
                        ) {
    
    # FUNCTION ARGUMENTS:
    # mtx: a numeric matrix, or an object that can be coerced to a numeric matrix (no character fields)
    # testCenters: integer vector for the centers to be tested
    # iter.max: parameter passed to kmeans
    # nstart: parameter passed to kmeans
    # silhouette: whether to calculate the silhouette score
    
    # Create an object for storing tot.withinss and silhouetteScore
    totWithin <- vector("numeric", length(testCenters))
    silhouetteScore <- vector("numeric", length(testCenters))
    
    # Create the distancing data (required for silhouette score)
    if (silhouette) distData <- dist(mtx)
    
    # Run k-means for every value in testCenters, and store $tot.withinss (and silhouetteScore, if requested)
    n <- 1
    for (k in testCenters) {
        km <- kmeans(mtx, centers=k, iter.max=iter.max, nstart=nstart)
        totWithin[n] <- km$tot.withinss
        if (silhouette & (k > 1)) silhouetteScore[n] <- mean(cluster::silhouette(km$cluster, distData)[, 3])
        n <- n + 1
    }
    
    # Create the elbow plot
    p1 <- tibble::tibble(n=testCenters, wss=totWithin) %>%
        ggplot(aes(x=n, y=wss)) + 
        geom_point() + 
        geom_line() + 
        geom_text(aes(y=wss + 0.05*max(totWithin), x=n+0.2, label=round(wss, 1))) + 
        labs(x="Number of segments", y="Total Within Sum-Squares", title="Elbow plot") + 
        ylim(c(0, NA))
    
    # Create the silhouette plot if requested
    if (silhouette) {
        p2 <- tibble::tibble(n=testCenters, ss=silhouetteScore) %>%
            ggplot(aes(x=n, y=ss)) + 
            geom_point() + 
            geom_line() + 
            geom_text(aes(y=ss + 0.05*max(silhouetteScore), x=n+0.2, label=round(ss, 1))) + 
            labs(x="Number of segments", y="Mean silhouette width", title="Silhouette plot") + 
            ylim(c(-1, NA))
        gridExtra::grid.arrange(p1, p2, nrow=1)
    } else {
        print(p1)
    }
    
}


```

The shapefunc capability appears capable of working if a custom function is used:  
```{r}

customTimeBucket <- function(x) {
    paste0(lubridate::year(x), "-", stringr::str_pad(lubridate::month(x), width=2, side="left", pad="0"))
}

# Example for rules-based clustering
clRules3 <- clusterStates(df=dfPer3, 
                          hierarchical=NA, 
                          returnList=TRUE, 
                          shapeFunc=customTimeBucket, 
                          minShape="2020-04", 
                          maxShape="2021-01", 
                          hmlSegs=3, 
                          eslSegs=3, 
                          seed=2101231503
                          )

# Example for kmeans clustering (elbow plot)
clKMeans3 <- clusterStates(df=dfPer3, 
                           hierarchical=FALSE, 
                           returnList=TRUE, 
                           shapeFunc=customTimeBucket, 
                           minShape="2020-04", 
                           maxShape="2021-01", 
                           iter.max=50,
                           nstart=25,
                           testCenters=1:20,
                           seed=2101231503
                           )

# Example for kmeans clustering (clusters)
clKMeans3 <- clusterStates(df=dfPer3, 
                           hierarchical=FALSE, 
                           returnList=TRUE, 
                           shapeFunc=customTimeBucket, 
                           minShape="2020-04", 
                           maxShape="2021-01", 
                           nCenters=6, 
                           iter.max=50,
                           nstart=25,
                           seed=2101231503
                           )

# Example for hierarchical clustering (clusters)
clHier3 <- clusterStates(df=dfPer3, 
                         hierarchical=TRUE, 
                         returnList=FALSE, 
                         shapeFunc=customTimeBucket, 
                         minShape="2020-04", 
                         maxShape="2021-01"
                         )

```

Next steps are to update the assessClusters() function.