---
title: "COVID Tracking Project - Updated for 2021"
author: "davegoblue"
date: "1/20/2021"
output: html_document
---

## Background
This file is designed to update the analysis routine for the COVID Tracking Project data to allow for use of 2021 data.  This code is largely a subset of, and update to, code contained in Coronavirus_Statistics_CTP_v003.Rmd.  This file includes the latest code for analyzing data from [The COVID Tracking Project](https://covidtracking.com/).  The COVID Tracking Project contains data on positive tests, hospitalizations, deaths, and the like, for coronavirus in the US.  Downloaded data are unique by state and date.

Companion code is in Coronavirus_Statistics_Shared_v004.R and Coronavirus_Statistics_Functions_CTP_v004.R.  The code leverages tidyverse and a variable mapping file throughout:  
```{r}

# All functions assume that tidyverse and its components are loaded and available
# Other functions are declared in the sourcing files or use library::function()
library(tidyverse)

# If the same function is in both files, use the version from the more specific source
# For now, soruce from _v003, eventually update to _v004
# source("./Coronavirus_Statistics_Functions_Shared_v003.R")
# source("./Coronavirus_Statistics_Functions_CTP_v003.R")

# Create a variable mapping file
varMapper <- c("cases"="Cases", 
               "newCases"="Increase in cases, most recent 30 days",
               "casesroll7"="Rolling 7-day mean cases", 
               "deaths"="Deaths", 
               "newDeaths"="Increase in deaths, most recent 30 days",
               "deathsroll7"="Rolling 7-day mean deaths", 
               "cpm"="Cases per million",
               "cpm7"="Cases per day (7-day rolling mean) per million", 
               "newcpm"="Increase in cases, most recent 30 days, per million",
               "dpm"="Deaths per million", 
               "dpm7"="Deaths per day (7-day rolling mean) per million", 
               "newdpm"="Increase in deaths, most recent 30 days, per million", 
               "hpm7"="Currently Hospitalized per million (7-day rolling mean)", 
               "tpm"="Tests per million", 
               "tpm7"="Tests per million per day (7-day rolling mean)"
               )

# Helper functions used early in the process
# Function for saving an R object to RDS, including a check for whether the object already exists
saveToRDS <- function(obj, 
                      file=paste0(deparse(substitute(obj)), ".RDS"), 
                      dir="./RInputFiles/Coronavirus/", 
                      ovrWrite=FALSE, 
                      ovrWriteError=TRUE,
                      makeReadOnly=TRUE
                      ) {
    
    # FUNCTION ARGUMENTS:
    # obj: the R object to save
    # file: the file name to save as
    # dir: the directory to save in (file path will be paste0(dir, file))
    # ovrWrite: boolean, should the file be overwritten if it already exists?
    # ovrWriteError: boolean, should an error be thrown if an attempt is made to overwrite the file?
    # makeReadOnly: boolean, should the output file be made read-only?
    
    # Create the file name
    locFile <- paste0(dir, file)
    
    # Check if the file already exists and proceed as per options
    if (file.exists(locFile)) {
        cat("\nFile already exists:", locFile, "\n")
        if (!ovrWrite & ovrWriteError) stop("\nAborting due to ovrWrite=FALSE and ovrWriteError=TRUE")
        if (!ovrWrite) {
            cat("\nNot replacing the existing file since ovrWrite=FALSE\n")
            return(NULL)
        }
    }
    
    # Save the file and update the permissions to read-only (if flag is set)
    saveRDS(obj, file=locFile)
    if (makeReadOnly) Sys.chmod(locFile, mode="0555", use_umask = FALSE)
    
}



# Function for reading an R object from RDS
readFromRDS <- function(file, 
                        dir="./RInputFiles/Coronavirus/", 
                        addSuffix=".RDS", 
                        deparseSub=FALSE
                        ) {
    
    # FUNCTION ARGUMENTS:
    # file: the file name to read in
    # dir: the directory the file is in
    # addSuffix: the suffix that should be added to file (file path will be paste0(dir, file, addSuffix))
    # deparseSub: whether to deparse and substitute file (use it as the text name)
    
    # Convert file if needed
    if (deparseSub) file <- deparse(substitute(file))
    
    # Ensure that file is of type character
    if (!isTRUE(all.equal(class(file), "character"))) {
        stop("\nUnable to read since file is not a character\n")
    }
    
    # Create the file name
    locFile <- paste0(dir, file, addSuffix)
    
    # Read the file (will be the return)
    readRDS(locFile)
    
}


```
  
## Running Code  
The main function is readRunCOVIDTrackingProject(), which performs multiple tasks:  
  
STEP 1: Extracts a file of population by state (by default uses 2015 population from usmap::statepop)  
STEP 2a^: Downloads the latest data from COVID Tracking Project if requested  
STEP 2b^: Reads in data from a specified local file (may have just been downloaded in step 2a), and checks control total trends against a previous version of the file  
STEP 3^: Processed the loaded data file for keeping proper variables, dropping non-valid states, etc.  
STEP 4^: Adds per-capita metrics for cases, deaths, tests, and hospitalizations  
STEP 5: Adds existing clusters by state if passed as an argument to useClusters=, otherwise creates new segments based on user-defined parameters  
STEP 6^^: Creates assessment plots for the state-level clusters  
STEP 7^^: Creates consolidated plots of cases, hospitalizations, deaths, and tests  
STEP 8^^: Optionally, creates plots of cumulative burden by segments and by state  
STEP 9: Returns a list of key data frames, modeling objects, named cluster vectors, etc.  
  
^ The user can instead specify a previously processed file and skip steps 2a, 2b, 3, and 4.  The previously processed file needs to be formatted and filtered such that it can be used "as is"  
^^ The user can skip the segment-level assessments by setting skipAssessmentPlots=TRUE  
  
The main function and the helper functions are updated to allow for using 2021 data.

## Main Function  
The main function, readRunCOVIDTrackingProject() is copied:  
```{r}

# Function to download/load, process, segment, and analyze data from COVID Tracking Project
readRunCOVIDTrackingProject <- function(thruLabel, 
                                        downloadTo=NULL, 
                                        readFrom=downloadTo, 
                                        compareFile=NULL,
                                        dfPerCapita=NULL,
                                        useClusters=NULL,
                                        hierarchical=TRUE,
                                        returnList=!hierarchical, 
                                        kCut=6,
                                        reAssignState=vector("list", 0),
                                        makeCumulativePlots=TRUE,
                                        skipAssessmentPlots=FALSE,
                                        ...
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # thruLabel: the label for when the data are through (e.g., "Aug 30, 2020")
    # donwloadTo: download the most recent COVID Tracking Project data to this location
    #             NULL means do not download any data
    # readFrom: location for reading in the COVID Tracking Project data (defaults to donwloadTo)
    # compareFile: name of the file to use for comparisons when reading in raw data (NULL means no comparison)
    # dfPerCapita: file can be passed directly, which bypasses the loading and processing steps
    # useClusters: file containing clusters by state (NULL means make the clusters from the data)
    # hierarchical: boolean, should hierarchical clusters be produced (if FALSE, will be k-means)?
    # returnList: boolean, should a list be returned or just the cluster object?
    #             refers to what is returned by clusterStates(); the main function always returns a list
    # kCut: number of segments when cutting the hierarchical tree
    # reAssignState: mapping file for assigning a state to another state's cluster
    #                format list("stateToChange"="stateClusterToAssign")
    # makeCumulativePlots: whether to make plots of cumulative metrics
    # skipAssessmentPlots: boolean to skip the plots for assessClusters()
    #                      especially useful if just exploring dendrograms or silhouette widths
    # ...: arguments to be passed to clusterStates(), will be used only if useClusters is NULL
    
    
    # STEP 1: Get state data
    stateData <- getStateData()
    
    
    # STEPS 2-4 are run only if dfPerCapita does not exist
    if (is.null(dfPerCapita)) {
        
        # STEP 2a: Download latest COVID Tracking Project data (if requested)
        if (!is.null(downloadTo)) downloadCOVIDbyState(fileName=downloadTo)
        
        # STEP 2b: Read-in COVID Tracking Project data
        dfRaw <- readCOViDbyState(readFrom, checkFile=compareFile)
        glimpse(dfRaw)
        
        # STEP 3: Process the data so that it includes all requested key variables
        varsFilter <- c("date", "state", "positiveIncrease", "deathIncrease", 
                        "hospitalizedCurrently", "totalTestResultsIncrease"
        )
        dfFiltered <- processCVData(dfRaw, 
                                    varsKeep=varsFilter, 
                                    varsRename=c(positiveIncrease="cases", 
                                                 deathIncrease="deaths", 
                                                 hospitalizedCurrently="hosp", 
                                                 totalTestResultsIncrease="tests"
                                    )
        )
        glimpse(dfFiltered)
        
        # STEP 4: Convert to per capita
        dfPerCapita <- helperMakePerCapita(dfFiltered, 
                                           mapVars=c("cases"="cpm", "deaths"="dpm", 
                                                     "hosp"="hpm", "tests"="tpm"
                                           ), 
                                           popData=stateData
        )
        glimpse(dfPerCapita)
        
    } else {
        dfRaw <- NULL
        dfFiltered <- NULL
    }
    
    
    # STEP 5: Create the clusters (if they have not been passed)
    if (is.null(useClusters)) {
        # Run the clustering process
        clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
        # If hierarchical clusters, cut the tree, otherwise use the output object directly
        if (hierarchical) {
            useClusters <- cutree(clData, k=kCut)
        } else {
            useClusters <- clData$objCluster$cluster
        }
        # If requested, manually assign clusters to the cluster for another state
        for (xNum in seq_len(length(reAssignState))) {
            useClusters[names(reAssignState)[xNum]] <- useClusters[reAssignState[[xNum]]]
        }
        
    }
    
    
    # STEP 5a: Stop the process and return what is available if skipAssessmentPlots is TRUE
    if (skipAssessmentPlots) {
        return(list(stateData=stateData, 
                    dfRaw=dfRaw, 
                    dfFiltered=dfFiltered, 
                    dfPerCapita=dfPerCapita, 
                    useClusters=useClusters, 
                    plotData=NULL, 
                    consolidatedPlotData=NULL, 
                    clCum=NULL
                    )
               )
    }
    
    
    # STEP 6: Create the cluster assessments
    plotData <- assessClusters(useClusters, 
                               dfState=stateData, 
                               dfBurden=dfPerCapita,
                               thruLabel=thruLabel,
                               plotsTogether=TRUE
    )
    
    
    # STEP 7: Plot the consolidated metrics
    subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
    consolidatedPlotData <- plotConsolidatedMetrics(plotData, 
                                                    varMain=c("state", "cluster", "date", "pop",
                                                              "cases", "deaths", "hosp", "tests"
                                                    ), 
                                                    subT=subT, 
                                                    nrowPlot2=2
    )
    
    # STEP 8: Create cumulative metrics if requested
    if (makeCumulativePlots) {
        consPos <- consolidatedPlotData %>%
            ungroup() %>%
            select(state, cluster, date, name, vpm7) %>%
            arrange(state, cluster, date, name) %>%
            pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
            mutate(pctpos=cases/tests) %>%
            pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
            filter(!is.na(vpm7))
        clCum <- makeCumulative(consPos)
        plotCumulativeData(clCum, 
                           keyMetricp2="", 
                           flagsp2="", 
                           makep1=TRUE, 
                           makep2=FALSE
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="deaths", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "deaths")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="cases", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "cases")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="tests", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "tests")
        )
    } else {
        clCum <- NULL
    }
    
    
    # STEP 9: Return a list of the key data
    list(stateData=stateData, 
         dfRaw=dfRaw, 
         dfFiltered=dfFiltered, 
         dfPerCapita=dfPerCapita, 
         useClusters=useClusters, 
         plotData=plotData, 
         consolidatedPlotData=consolidatedPlotData, 
         clCum=clCum
    )
    
    
}

```

The following functions are examined for update:  
  
* getStateData - updated to default to using 2019 census data  
* downloadCOVIDbyState - function is OK, copied as-is  
* readCOVIDbyState - updated for cleaner log file  
* processCVData - function is OK, copied as-is  
* helperMakePerCapita - function is OK, copied as-is  
* clusterStates - updated with a custom shape function for year-month  
* assessClusters - function is OK  
* plotConsolidatedMetrics - function is OK  
* makeCumulative - function is OK  
* plotCumulativeData - function is OK  

So, the main effort will be to update the clusterStates() function.  There is also an opportunity to clean up the other functions so that output is directed to a separate log file and warnings caused by not setting .groups can also be addressed.

State population data have been downloaded from [US Census](https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/state/detail/).  The file is processed and saved so that it can be used in an updated getStateData() function:  
```{r cache=TRUE}

# Read in the state population file
statePop2019 <- readr::read_csv("./RInputFiles/Coronavirus/SCPRC-EST2019-18+POP-RES.csv")

# Create mapping file for state name to state abbreviation
stateMap <- tibble::tibble(stateName=c(state.name, "District of Columbia"), stateAbb=c(state.abb, "DC"))

# Create fields for state (abbreviation) and pop_2019
statePop2019 <- statePop2019 %>%
    full_join(stateMap, by=c("NAME"="stateName")) %>%
    mutate(pop_2019=POPESTIMATE2019)

# Check if anything did not merge properly
# Expected that United States and Puerto Rico will not merge, others should be a good match
statePop2019 %>%
    filter(is.na(stateAbb) | is.na(pop_2019))

# Delete the Puerto Rico and US totals data
statePop2019 <- statePop2019 %>%
    filter(!is.na(stateAbb))

# Glimpse file and then save to RDS
glimpse(statePop2019)
saveToRDS(statePop2019, ovrWrite=FALSE, ovrWriteError=FALSE)

```

The getStateData() function is updated to use defaults for this new file:  
```{r}

# Function to extract and format key state data
getStateData <- function(df=readFromRDS("statePop2019"), 
                         renameVars=c("stateAbb"="state", "NAME"="name", "pop_2019"="pop"), 
                         keepVars=c("state", "name", "pop")
                         ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing state data
    # renameVars: variables to be renamed, using named list with format "originalName"="newName"
    # keepVars: variables to be kept in the final file
    
    # Rename variables where appropriate
    names(df) <- ifelse(is.na(renameVars[names(df)]), names(df), renameVars[names(df)])
    
    # Return file with only key variables kept
    df %>%
        select_at(vars(all_of(keepVars)))
    
}

```
  
The function is then tested, with totals by state compared against previous:  
```{r}

# Using the new defaults
pop_2019 <- getStateData()
pop_2019

# Comparison to previous
fullPop <- pop_2019 %>%
    full_join(usmap::statepop, by=c("state"="abbr")) %>%
    mutate(pctChg=pop/pop_2015-1)

# Flag for any differences in name
fullPop %>%
    filter(is.na(name) | is.na(full) | name != full)

# Plot population and percent change
fullPop %>%
    ggplot(aes(x=fct_reorder(state, pctChg), y=pctChg)) + 
    geom_col(fill="lightblue") + 
    coord_flip() + 
    labs(title="Change in population from usmap::statepop to US Census 2019 estimate", 
         x="", 
         y="Percent change from 2015 (usmap) to 2019 (US Census)"
         )

```
  
The data appear to be reasonably aligned, with fast growing states and shrinking states roughly as expected.  

The function downloadCOVIDbyState() appears to be OK as-is, and is copied below:  
```{r}

# NO CHANGES MADE TO FUNCTION
# Function to download data for COVID Tracking Project
downloadCOVIDbyState <- function(fileName, 
                                 api="https://api.covidtracking.com/v1/states/daily.csv", 
                                 ovrWrite=FALSE
                                 ) {
    
    # COVID Tracking Project API allows data downloads for personal, non-commercial use
    # https://covidtracking.com/data/api
    
    # FUNCTION ARGUMENTS:
    # fileName: the filename that the data will be saved to
    # api: The API link for data downloads
    # ovrWrite: whether to allow overwriting of the existing fileName
    
    # Check whether fileName already exists
    if (file.exists(fileName)) {
        cat("\nFile already exists at:", fileName, "\n")
        if (ovrWrite) cat("Will over-write with current data from", api, "\n")
        else stop("Exiting due to ovrWrite=FALSE and a duplicate fileName\n")
    }
    
    # Download the file 
    download.file(api, destfile=fileName)
    
    # Show statistics on downloaded file
    file.info(fileName)
    
}

```
  
The download function is then run using 2021 data:  
```{r cache=TRUE}

# Example for downloading the 22-JAN-21 data file
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_210122.csv"
if (!file.exists(locDownload)) downloadCOVIDbyState(fileName=locDownload)

```

The function readCOVIDbyState() is copied below, with capability to direct control totals for changed items in an easier to read manner:  
```{r}

# Function to read, convert, and sanity check a downloaded file
readCOViDbyState <- function(fileName, 
                             checkFile=NULL, 
                             controlFields=c("positiveIncrease", "deathIncrease", "hospitalizedCurrently"), 
                             controlBy=c("state"), 
                             dateChangePlot=FALSE, 
                             dateMetricPrint=TRUE, 
                             controlByMetricPrint=TRUE, 
                             writeLog=NULL, 
                             ovrwriteLog=TRUE
                             ) {
    
    # FUNCTION ARGUMENTS:
    # fileName: the file name for reading the data
    # checkFile: a file that can be used for comparison purposes (NULL means do not compare to anything)
    # controlFields: fields that will be explicitly checked against checkFile
    # controlBy: level of aggregation at which fields will be explicitly checked against checkFile
    # dateChangePlot: boolean, should the change in dates included be plotte rather than listed?
    # dateMetricPrint: boolean, should the list of date-metric changes be printed?
    # controlByMetricPrint: boolean, should the list of controlBy-metric changes be printed?
    # writeLog: write detailed comparison to log file (NULL means do not write)
    # ovrwriteLog: boolean, should the log be started from scratch with the date comparisons?
    
    # Helper function to check for similarity of key elements
    helperSimilarity <- function(newData, refData, label, countOnly=FALSE, logFile=NULL, logAppend=TRUE) {
        d1 <- setdiff(refData, newData)
        d2 <- setdiff(newData, refData)
        cat("\n\nChecking for similarity of:", label)
        cat("\nIn reference but not in current:", if(countOnly) length(d1) else d1)
        cat("\nIn current but not in reference:", if(countOnly) length(d2) else d2)
        if (countOnly & !is.null(logFile)) {
            cat("\nDetailed differences available in:", logFile)
            capture.output(cat("\nIn reference but not in current:\n", paste(d1, collapse="\n"), sep=""), 
                           cat("\nIn current but not in reference:\n", paste(d2, collapse="\n"), sep=""), 
                           file=logFile, 
                           append=logAppend
                           )
        }
        if (countOnly) return(list(d1=d1, d2=d2))
    }
    
    # Read in the file and convert the numeric date field to date using ymd format
    df <- readr::read_csv(fileName) %>% 
        mutate(date=lubridate::ymd(date))
    
    # Check that the file is unique by date-state
    if ((df %>% select(date, state) %>% anyDuplicated()) != 0) {
        stop("\nDuplicates by date and state, investigate and fix\n")
    } else {
        cat("\nFile is unique by state and date\n")
    }
    
    # Check for overall control totals in new file
    cat("\n\nOverall control totals in file:\n")
    df %>% 
        summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>% 
        print()
    
    # Get control totals by date for new file
    dfByDate <- df %>% 
        group_by(date) %>%
        summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
        ungroup() %>%
        pivot_longer(-date, values_to="newValue")
    
    # If there is no checkFile, then just produce a plot of the key metrics
    if (is.null(checkFile)) {
        p1 <- dfByDate %>% 
            ggplot(aes(x=date, y=newValue)) + 
            geom_line() + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            labs(title="Control totals by date for new file (no reference file)", x="", y="Summed Value")
        print(p1)
    } else {
        # Check for similarity of fields, dates, and states
        cat("\n*** COMPARISONS TO REFERENCE FILE:", deparse(substitute(checkFile)))
        helperSimilarity(newData=names(df), refData=names(checkFile), label="column names")
        helperSimilarity(newData=df %>% pull(state) %>% unique(), 
                         refData=checkFile %>% pull(state) %>% unique(), 
                         label="states"
        )
        dateChangeList <- helperSimilarity(newData=df %>% 
                                               pull(date) %>% 
                                               unique() %>% 
                                               format("%Y-%m-%d") %>%
                                               sort(), 
                                           refData=checkFile %>% 
                                               pull(date) %>% 
                                               unique() %>% 
                                               format("%Y-%m-%d") %>%
                                               sort(),
                                           label="dates", 
                                           countOnly=dateChangePlot, 
                                           logFile=writeLog, 
                                           logAppend=!ovrwriteLog
                                           )
        
        # Plot date changes if requested
        if (dateChangePlot) {
            pDate <- tibble::tibble(date=as.Date(c(dateChangeList$d1, dateChangeList$d2)), 
                                    type=c(rep("Control File Only", length(dateChangeList$d1)), 
                                           rep("New File Only", length(dateChangeList$d2))
                                           )
                                    ) %>%
                ggplot(aes(x=date, fill=type)) + 
                geom_bar() + 
                coord_flip() + 
                labs(x="", y="", title="Dates in one file and not in the other")
            print(pDate)
        }
        
        # Check for similarity of control totals by date in files
        checkByDate <- checkFile %>% 
            group_by(date) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-date, values_to="oldValue")
        deltaDate <- dfByDate %>% 
            inner_join(checkByDate, by=c("date", "name")) %>%
            filter(abs(newValue-oldValue)>=5, 
                   pmax(newValue, oldValue)>=1.01*pmin(newValue, oldValue)
            ) %>%
            as.data.frame()
        cat("\n\nDifference of 5+ that is at least 1% (summed to date and metric):", nrow(deltaDate))
        if (dateMetricPrint) {
            cat("\n")
            print(deltaDate)
        }
        else if (!is.null(writeLog)) {
            cat("\nDetailed output available in log:", writeLog)
            capture.output(cat("\n\nChange by date:\n"), print(deltaDate), file=writeLog, append=TRUE)
        }
        p1 <- dfByDate %>% 
            full_join(checkByDate, by=c("date", "name")) %>%
            pivot_longer(-c(date, name), names_to="newOld") %>%
            ggplot(aes(x=date, y=value, group=newOld, color=newOld)) + 
            geom_line() + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            labs(title="Control totals by date for new and reference file", x="", y="Summed Value")
        print(p1)
        
        # Check for similarity of control totals by controlBy in files
        dfByControl <- df %>% 
            semi_join(select(checkFile, date), by="date") %>%
            group_by_at(vars(all_of(controlBy))) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-all_of(controlBy), values_to="newValue")
        checkByControl <- checkFile %>% 
            group_by_at(vars(all_of(controlBy))) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-all_of(controlBy), values_to="oldValue")
        deltaBy <- dfByControl %>% 
            inner_join(checkByControl, by=c(controlBy, "name")) %>%
            filter(abs(newValue-oldValue)>=5, 
                   pmax(newValue, oldValue)>=1.01*pmin(newValue, oldValue)
            ) %>%
            as.data.frame()
        cat("\n\nDifference of 5+ that is at least 1% (summed to", 
            controlBy, 
            "and metric):", 
            nrow(deltaBy), 
            "\n"
            )
        if (controlByMetricPrint) print(deltaBy)
    }
    
    # Return the processed data file
    df
    
}

```

The function is then applied to the downloaded 21-JAN-21 data, including a comparison to a previous 2020 file:  
```{r}

# Reading the file as a standalone
df1 <- readCOViDbyState(locDownload)

# Reading the file with a comparison to a previous file, previous approach
df2 <- readCOViDbyState(locDownload, 
                        checkFile=readFromRDS("test_hier5_201001")$dfRaw
                        )

# Reading the file with a comparison to a previous file, key output directed to a log
df3 <- readCOViDbyState(locDownload, 
                        checkFile=readFromRDS("test_hier5_201001")$dfRaw, 
                        dateChangePlot=TRUE, 
                        dateMetricPrint=FALSE, 
                        writeLog="./RInputFiles/Coronavirus/testLogCTP_v001.log", 
                        ovrwriteLog=TRUE
                        )

# Confirm that files are identical
identical(df1, df2)
identical(df1, df3)

```

The added functionality allows for a cleaner log, with detailed output optionally sent to a separate file.  Next steps are to continue updating elements of the main function.

The processCVData() and helperMakePerCapita() functions are copied, along with the associated helper function:  
```{r}

# Function to select relevant variables and observations, and report on control totals
processCVData <- function(dfFull, 
                          varsKeep=c("date", "state", "positiveIncrease", "deathIncrease"), 
                          varsRename=c("positiveIncrease"="cases", "deathIncrease"="deaths"), 
                          stateList=c(state.abb, "DC")
                          ) {
    
    # FUNCTION ARGUMENTS
    # dfFull: the full data file originally loaded
    # varsKeep: variables to keep from the full file
    # varsRename: variables to be renamed, using a named vector of form originalName=newName
    # stateList: variables for filtering state (NULL means do not run any filters)
    
    # Select only the key variables
    df <- dfFull %>%
        select_at(vars(all_of(varsKeep)))
    
    # Apply the renaming of variables
    names(df) <- ifelse(is.na(varsRename[names(df)]), names(df), varsRename[names(df)])
    
    # Designate each record as being either a valid state or not
    if (!is.null(stateList)) {
        df <- df %>%
            mutate(validState=state %in% stateList)
    } else {
        df <- df %>%
            mutate(validState=TRUE)
    }
    
    # Summarize the control totals for the data, based on whether the state is valid
    cat("\n\nControl totals - note that validState other than TRUE will be discarded\n(na.rm=TRUE)\n\n")
    df %>%
        mutate(n=1) %>%
        group_by(validState) %>%
        summarize_if(is.numeric, sum, na.rm=TRUE) %>%
        print()
    
    # Return the file, filtered to where validState is TRUE, and deleting variable validState
    df %>%
        filter(validState) %>%
        select(-validState)
    
}



# Function to add per capita and rolling to the base data frame
# Updated function to take an arbitrary number of variables and convert them
helperMakePerCapita <- function(df, 
                                mapVars=c("cases"="cpm", "deaths"="dpm"),
                                popData=stateData,
                                k=7
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: the initial data frame for conversion
    # mapVars: named vector of variables to be converted 'original name'='converted name'
    # k: the rolling time period to use
    
    # Create the variables for per capita
    for (origVar in names(mapVars)) {
        df <- df %>% 
            helperPerCapita(origVar=origVar, newName=mapVars[origVar], popData=popData)
    }
    # Arrange the data by date in preparation for rolling aggregations
    df <- df %>% 
        group_by(state) %>% 
        arrange(date)
    # Create the rolling variables
    for (newVar in mapVars) {
        df <- df %>% 
            helperRollingAgg(origVar=newVar, newName=paste0(newVar, k), k=k)
    }
    
    # Return the updated data frame, ungrouped
    df %>%
        ungroup()
    
}



# Helper function to create per capita metrics
helperPerCapita <- function(df, 
                            origVar, 
                            newName,
                            byVar="state",
                            popVar="pop",
                            popData=stateData,
                            mult=1000000
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame currently being processed
    # origVar: the variables to be converted to per capita
    # newName: the new per capita variable name
    # byVar: the variable that will be merged by
    # popVar: the name of the population variable in the popData file
    # popData: the file containing the population data
    # mult: the multiplier, so that the metric is "per mult people"
    
    # Create the per capita variable
    df %>%
        inner_join(select_at(popData, vars(all_of(c(byVar, popVar)))), by=byVar) %>%
        mutate(!!newName:=mult*get(origVar)/get(popVar)) %>%
        select(-all_of(popVar))
    
}



# Helper function to create rolling aggregates
helperRollingAgg <- function(df, 
                             origVar, 
                             newName,
                             func=zoo::rollmean,
                             k=7, 
                             fill=NA, 
                             ...
                             ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing the data
    # origVar: the original data column name
    # newName: the new variable column name
    # func: the function to be applied (zoo::rollmean will be by far the most common)
    # k: the periodicity (k=7 is rolling weekly data)
    # fill: how to fill leading.trailing data to maintain the same vector lengths
    # ...: any other arguments to be passed to func
    
    # Create the appropriate variable
    df %>%
        mutate(!!newName:=func(get(origVar), k=k, fill=fill, ...))
    
}

```

The processCVData() function is applied to df3:  
```{r}

# STEP 3: Process the data so that it includes all requested key variables
varsFilter <- c("date", "state", "positiveIncrease", "deathIncrease", 
                "hospitalizedCurrently", "totalTestResultsIncrease"
                )
dfFilter3 <- processCVData(df3, 
                           varsKeep=varsFilter, 
                           varsRename=c(positiveIncrease="cases", 
                                        deathIncrease="deaths", 
                                        hospitalizedCurrently="hosp", 
                                        totalTestResultsIncrease="tests"
                                        )
                           )
glimpse(dfFilter3)

```

The helperMakePerCapita() function is applied to dfFilter3:  
```{r}

# STEP 4: Convert to per capita
dfPer3 <- helperMakePerCapita(dfFilter3, 
                              mapVars=c("cases"="cpm", "deaths"="dpm", "hosp"="hpm", "tests"="tpm"), 
                              popData=pop_2019
                              )
glimpse(dfPer3)

```

The clusterStates() function is updated to allow for both month and year in calculating shape (since data will now be from 2020 and 2021).  Addition of a custom shape function suffices:  
```{r}

# Updates to the clustering function
clusterStates <- function(df, 
                          caseVar="cpm", 
                          deathVar="dpm",
                          shapeFunc=lubridate::month, 
                          minShape=NULL, 
                          maxShape=NULL,
                          minDeath=0,
                          maxDeath=Inf,
                          minCase=0,
                          maxCase=Inf,
                          ratioTotalvsShape=1, 
                          ratioDeathvsCase=1, 
                          hierarchical=TRUE, 
                          hierMethod="complete", 
                          nCenters=3, 
                          iter.max=10,
                          nstart=1,
                          testCenters=NULL,
                          returnList=FALSE, 
                          hmlSegs=3, 
                          eslSegs=2,
                          seed=NULL
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing cases and deaths data
    # caseVar: the variable containing the cases per capita data
    # deathVar: the variable containing the deaths per capita data
    # shapeFunc: the function to be used for creating the shape of the curve
    # minShape: the minimum value to be used for shape (to avoid very small amounts of data in Jan/Feb/Mar)
    #           shape is the month, so 4 means start with April data (NULL means keep everything)
    # maxShape: the maximum value to be used for shape (to avoid very small amounts of data in a partial month)
    #           shape is the month, so 9 means end with September data (NULL means keep everything)
    # minDeath: use this value as a floor for the death metric when calculating shape
    # maxDeath: use this value as a maximum when calculating distance using deaths 
    # minCase: use this value as a floor for the case metric when calculating shape
    # maxCase: use this value as a maximum when calculating distance using cases 
    # ratioTotalvsShape: amount of standard deviation to be kept in total variable vs shape variables
    # ratioDeathvsCase: amount of standard deviation to be kept in deaths vs cases 
    #                   (total death data will be scaled to have sd this many times higher than cases)
    #                   (death percentages by time period will be scaled directly by this amount)
    # hierarchical: whether to create hierarchical clusters
    #               TRUE means run hierarchical clustering
    #               FALSE means run kmeans clustering
    #               NA means run rules-based clustering
    # hierMethod: the method for hierarchical clustering (e.g., 'complete' or 'single')
    # nCenters: the number of centers to use for kmeans clustering
    # testCenters: integer vector of centers to test (will create an elbow plot); NULL means do not test
    # iter.max: maximumum number of kmeans iterations (default in kmeans algorithm is 10)
    # nstart: number of random sets chosen for kmeans (default in kmeans algorithm is 1)
    # returnList: boolean, if FALSE just the cluster object is returned
    #                      if TRUE, a list is returned with dfCluster and the cluster object
    # hmlSegs: number of segments to create for volume of burden integrated over time
    # eslSegs: number of segments to create for shape of burden over time
    # seed: set the seed to this value (NULL means no seed)
    
    # Extract key information (aggregates and by shapeFunc for each state)
    df <- df %>%
        select_at(vars(all_of(c("date", "state", caseVar, deathVar)))) %>%
        purrr::set_names(c("date", "state", "cases", "deaths")) %>%
        mutate(timeBucket=shapeFunc(date)) %>%
        group_by(state, timeBucket) %>%
        summarize(cases=sum(cases), deaths=sum(deaths), .groups="drop") %>%
        ungroup()
    
    # Limit to only relevant time buckets if requested
    if (!is.null(minShape)) {
        df <- df %>%
            filter(timeBucket >= minShape)
    }
    if (!is.null(maxShape)) {
        df <- df %>%
            filter(timeBucket <= maxShape)
    }
    
    # Extract an aggregate by state, scaled so that they have the proper ratio
    dfAgg <- df %>%
        group_by(state) %>%
        summarize(totalCases=sum(cases), totalDeaths=sum(deaths), .groups="drop") %>%
        mutate(totalCases=pmin(totalCases, maxCase), totalDeaths=pmin(totalDeaths, maxDeath)) %>%
        ungroup() %>%
        mutate(totalDeaths=ratioDeathvsCase*totalDeaths*sd(totalCases)/sd(totalDeaths))
    
    # Extract the percentages (shapes) by month, scaled so that they have the proper ratio
    dfShape <- df %>%
        pivot_longer(-c(state, timeBucket)) %>%
        group_by(state, name) %>%
        mutate(tot=pmax(sum(value), ifelse(name=="deaths", minDeath, minCase)), 
               value=ifelse(name=="deaths", ratioDeathvsCase, 1) * value / tot) %>%
        select(-tot) %>%
        pivot_wider(state, names_from=c(name, timeBucket), values_from=value) %>%
        ungroup()
    
    # Function to calculate SD of a subset of columns
    calcSumSD <- function(df) {
        df %>% 
            ungroup() %>% 
            select(-state) %>% 
            summarize_all(.funs=sd) %>% 
            as.vector() %>% 
            sum()
    }
    
    # Down-weight the aggregate data so that there is the proper sum of sd in aggregates and shapes
    aggSD <- calcSumSD(dfAgg)
    shapeSD <- calcSumSD(dfShape)
    dfAgg <- dfAgg %>%
        mutate_if(is.numeric, ~. * ratioTotalvsShape * shapeSD / aggSD)
    
    # Combine so there is one row per state
    dfCluster <- dfAgg %>%
        inner_join(dfShape, by="state")
    
    # convert 'state' to rowname
    keyData <- dfCluster %>% 
        column_to_rownames("state")
    
    # Create rules-based segments (NA) or hierarchical segments (TRUE) or kmeans segments (FALSE)
    if (is.na(hierarchical)) {
        # Create pseudo-rules-based segments
        if (!is.null(seed)) set.seed(seed)
        # STEP 1: Classify high-medium-low based on deaths and cases
        hml <- kmeans(select(keyData, starts_with("total")), 
                      centers=hmlSegs, iter.max=iter.max, nstart=nstart
        )
        # STEP 2: Classify early-late based on shape
        esl <- kmeans(select(keyData, -starts_with("total")), 
                      centers=eslSegs, iter.max=iter.max, nstart=nstart
        )
        # STEP 3: Create a final segment
        objCluster <- eslSegs*(hml$cluster-1) + esl$cluster
    } else if (isTRUE(hierarchical)) {
        # Create hierarchical segments
        objCluster <-  hclust(dist(keyData), method=hierMethod)
        plot(objCluster)
    } else {
        # Create k-means segments
        # Create an elbow plot if testCenters is not NULL
        if (!is.null(testCenters)) {
            helperElbow(keyData, testCenters=testCenters, iter.max=iter.max, nstart=nstart, silhouette=TRUE)
        }
        # Create the kmeans cluster object, setting a seed if requested
        if (!is.null(seed)) set.seed(seed)
        objCluster <- kmeans(keyData, centers=nCenters, iter.max=iter.max, nstart=nstart)
        cat("\nCluster means and counts\n")
        n=objCluster$size %>% cbind(objCluster$centers) %>% round(2) %>% t() %>% print()
    }
    
    # Return the data and object is a list if returnList is TRUE, otherwise return only the clustering object
    if (!isTRUE(returnList)) {
        objCluster
    } else {
        list(objCluster=objCluster, dfCluster=dfCluster)
    }
    
}


# Function to create an elbow plot for various numbers of clusters in the data
helperElbow <- function(mtx, 
                        testCenters, 
                        iter.max, 
                        nstart, 
                        silhouette=FALSE
                        ) {
    
    # FUNCTION ARGUMENTS:
    # mtx: a numeric matrix, or an object that can be coerced to a numeric matrix (no character fields)
    # testCenters: integer vector for the centers to be tested
    # iter.max: parameter passed to kmeans
    # nstart: parameter passed to kmeans
    # silhouette: whether to calculate the silhouette score
    
    # Create an object for storing tot.withinss and silhouetteScore
    totWithin <- vector("numeric", length(testCenters))
    silhouetteScore <- vector("numeric", length(testCenters))
    
    # Create the distancing data (required for silhouette score)
    if (silhouette) distData <- dist(mtx)
    
    # Run k-means for every value in testCenters, and store $tot.withinss (and silhouetteScore, if requested)
    n <- 1
    for (k in testCenters) {
        km <- kmeans(mtx, centers=k, iter.max=iter.max, nstart=nstart)
        totWithin[n] <- km$tot.withinss
        if (silhouette & (k > 1)) silhouetteScore[n] <- mean(cluster::silhouette(km$cluster, distData)[, 3])
        n <- n + 1
    }
    
    # Create the elbow plot
    p1 <- tibble::tibble(n=testCenters, wss=totWithin) %>%
        ggplot(aes(x=n, y=wss)) + 
        geom_point() + 
        geom_line() + 
        geom_text(aes(y=wss + 0.05*max(totWithin), x=n+0.2, label=round(wss, 1))) + 
        labs(x="Number of segments", y="Total Within Sum-Squares", title="Elbow plot") + 
        ylim(c(0, NA))
    
    # Create the silhouette plot if requested
    if (silhouette) {
        p2 <- tibble::tibble(n=testCenters, ss=silhouetteScore) %>%
            ggplot(aes(x=n, y=ss)) + 
            geom_point() + 
            geom_line() + 
            geom_text(aes(y=ss + 0.05*max(silhouetteScore), x=n+0.2, label=round(ss, 1))) + 
            labs(x="Number of segments", y="Mean silhouette width", title="Silhouette plot") + 
            ylim(c(-1, NA))
        gridExtra::grid.arrange(p1, p2, nrow=1)
    } else {
        print(p1)
    }
    
}


```

The shapefunc capability appears capable of working if a custom function is used:  
```{r}

customTimeBucket <- function(x) {
    paste0(lubridate::year(x), "-", stringr::str_pad(lubridate::month(x), width=2, side="left", pad="0"))
}

# Example for rules-based clustering
clRules3 <- clusterStates(df=dfPer3, 
                          hierarchical=NA, 
                          returnList=TRUE, 
                          shapeFunc=customTimeBucket, 
                          minShape="2020-04", 
                          maxShape="2021-01", 
                          hmlSegs=3, 
                          eslSegs=3, 
                          seed=2101231503
                          )

# Example for kmeans clustering (elbow plot)
clKMeans3 <- clusterStates(df=dfPer3, 
                           hierarchical=FALSE, 
                           returnList=TRUE, 
                           shapeFunc=customTimeBucket, 
                           minShape="2020-04", 
                           maxShape="2021-01", 
                           iter.max=50,
                           nstart=25,
                           testCenters=1:20,
                           seed=2101231503
                           )

# Example for kmeans clustering (clusters)
clKMeans3 <- clusterStates(df=dfPer3, 
                           hierarchical=FALSE, 
                           returnList=TRUE, 
                           shapeFunc=customTimeBucket, 
                           minShape="2020-04", 
                           maxShape="2021-01", 
                           nCenters=6, 
                           iter.max=50,
                           nstart=25,
                           seed=2101231503
                           )

# Example for hierarchical clustering (clusters)
clHier3 <- clusterStates(df=dfPer3, 
                         hierarchical=TRUE, 
                         returnList=FALSE, 
                         shapeFunc=customTimeBucket, 
                         minShape="2020-04", 
                         maxShape="2021-01"
                         )

```

The assessClusters() function is then copied with a small number of minor edits made:  
```{r}

assessClusters <- function(clusters, 
                           dfState=stateData, 
                           dfBurden=cvFilteredPerCapita,
                           thruLabel="Aug 20, 2020",
                           isCounty=FALSE,
                           plotsTogether=FALSE, 
                           clusterPlotsTogether=plotsTogether,
                           recentTotalTogether=plotsTogether, 
                           clusterAggTogether=plotsTogether, 
                           makeSummaryPlots=TRUE, 
                           makeTotalvsPerCapitaPlots=!isCounty, 
                           makeRecentvsTotalPlots=TRUE, 
                           makeTotalvsElementPlots=TRUE, 
                           showMap=!isCounty, 
                           orderCluster=FALSE
                           ) {
    
    # FUNCTION ARGUMENTS:
    # clusters: the named vector containing the clusters by state
    # dfState: the file containing the states and populations
    # dfBurden: the data containing the relevant per capita burden statistics by state-date
    # thruLabel: label for plots for 'data through'
    # isCounty: boolean, is this a plot of county-level data that have been named 'state'?
    #           FALSE means that it is normal state-level data
    # plotsTogether: boolean, should plots be consolidated on fewer pages?
    # clusterPlotsTogether: boolean, should plots p1-p4 be consolidated?
    # recentTotalTogether: boolean, should recent total plots p7-p8 be consolidated?
    # clusterAggTogether: boolean, should aggregate plots p9/p11 and p10/p12 be consolidated?
    # makeSummaryPlots: boolean, should the summary plots be made?
    # makeTotalvsPerCapitaPlots: boolean, should the total and per capita plots be produced?
    # makeRecentvsTotalPlots: boolean, should the recent vs. total plots be created?
    # makeTotalvsElementPlots: boolean, should the total vs. element plots be created?
    # showMap: boolean, whether to create a map colored by cluster (will show segment counts otherwise)
    # orderCluster: if FALSE, ignore; if TRUE, order by "dpm"; if anything else, order by orderCluster
    # ...: any additional arguments passed from a calling function
    #      most common would be orderCluster=TRUE, a request for the clusters to be ordered by disease burden
    
    # Attach the clusters to the state population data
    dfState <- as.data.frame(clusters) %>%
        set_names("cluster") %>%
        rownames_to_column("state") %>%
        inner_join(dfState, by="state") %>%
        mutate(cluster=factor(cluster))
    
    # Plot the rolling 7-day mean dialy disease burden by cluster
    dfPlot <- dfState %>%
        inner_join(dfBurden, by="state") %>%
        tibble::as_tibble()
    
    # Reorder the clusters if requested
    if (!isFALSE(orderCluster)) {
        if (isTRUE(orderCluster)) burdenParam <- "dpm" else burdenParam <- orderCluster
        dfPlot <- changeOrderLabel(dfPlot, grpVars="state", burdenVar=burdenParam)
    }
    
    # Call the helper function to make the overall summary statistics
    if (makeSummaryPlots) {
        helperClusterSummaryPlots(dfState=dfState, 
                                  dfPlot=dfPlot, 
                                  showMap=showMap, 
                                  clusterPlotsTogether=clusterPlotsTogether, 
                                  mapLevel=if(isCounty) "counties" else "states"
        )
    }
    
    # These are primarily useful for state-level data and default to FALSE when isCounty is TRUE
    if (makeTotalvsPerCapitaPlots) {
        helperCallTotalPerCapita(dfPlot=dfPlot, thruLabel=thruLabel)
    }
    
    # Call the helper function to make recent vs. total plots
    if (makeRecentvsTotalPlots) {
        helperCallRecentvsTotal(dfPlot=dfPlot, 
                                thruLabel=thruLabel, 
                                labelPoints=!isCounty, 
                                recentTotalTogether = recentTotalTogether
        )
    }
    
    # Call the total vs. elements helper function
    if (makeTotalvsElementPlots) {
        helperCallTotalvsElements(dfPlot=dfPlot, 
                                  aggAndTotal=!isCounty, 
                                  clusterAggTogether=clusterPlotsTogether
        )
    }
    
    # Return the plotting data frame
    dfPlot
    
}



# Function to reorder and relabel factors
changeOrderLabel <- function(df, 
                             fctVar="cluster",
                             grpVars=c(),
                             burdenVar="dpm", 
                             wgtVar="pop",
                             exclfct="999"
                             ) {
    
    # FUNCTION ARGUMENTS
    # df: the data frame
    # fctVar: the factor variable
    # grpVars: the variable that the data are aurrently at (e.g., "fipsCounty" for county-level in df)
    # burdenVar: the disease burden variable for sorting
    # wgtVar: the weight variable for sorting
    # exclfct: the factor level to be excluded from analysis
    
    # General approach
    # 1. Data are aggregated to c(fctVar, grpVars) as x=sum(burdenVar*wgtVar) and y=mean(wgtVar)
    # 2. Data are aggregated to fctVar as z=sum(x)/sum(y)
    # 3. Factors are reordered from high to low on z, with the excluded factor added back last (if it exists)
    
    # Check if exclfct exists in the factor variable
    fctDummy <- exclfct %in% levels(df[, fctVar, drop=TRUE])
    
    # Create the summary of impact by segment
    newLevels <- df %>%
        filter(get(fctVar) != exclfct) %>%
        group_by_at(vars(all_of(c(fctVar, grpVars)))) %>%
        summarize(x=sum(get(burdenVar)*get(wgtVar)), y=mean(get(wgtVar)), .groups="drop") %>%
        group_by_at(vars(all_of(fctVar))) %>%
        summarize(z=sum(x)/sum(y), .groups="drop") %>%
        arrange(-z) %>%
        pull(fctVar) %>%
        as.character()
    
    # Add back the dummy factor at the end (if it exists)
    if (fctDummy) newLevels <- c(newLevels, exclfct)
    
    # Reassign the levels in df
    df %>% 
        mutate(!!fctVar:=factor(get(fctVar), levels=newLevels, labels=newLevels))
    
}



# Helper function to make the overall cluster summary statistics
helperClusterSummaryPlots <- function(dfState, 
                                      dfPlot,
                                      showMap, 
                                      clusterPlotsTogether,
                                      weightedMean=TRUE,
                                      mapLevel="states"
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # dfState: contains the state/county-level data
    # dfPlot: contains the joined data for plotting
    # showMap: boolean for whether to create a map (if FALSE, segment membership counts are shown instead)
    # clusterPlotsTogether: boolean, whether to put all four plots on the same page
    # weightedMean: boolean, whether to create weighted mean by segment (if FALSE, median by segment is taken)
    # mapLevel: the level to be used on the map
    
    # Reorder the cluster levels in dfState to match dfPlot
    # Convert factor order to match dfPlot (can be reordered by argument to the calling function)
    dfState <- dfState %>%
        mutate(cluster=factor(cluster, levels=levels(dfPlot$cluster)))
    
    # Plot the segments on a map or show segment membership
    if (showMap) {
        if (mapLevel=="counties") {
            dfMap <- dfState %>%
                mutate(fips=stringr::str_pad(state, width=5, side="left", pad="0")) %>%
                select(fips, cluster)
        } else {
            dfMap <- dfState
        }
        # Create the map
        p1 <- usmap::plot_usmap(regions=mapLevel, data=dfMap, values="cluster") + 
            scale_fill_discrete("cluster") + 
            theme(legend.position="right")
    } else {
        p1 <- dfState %>%
            count(cluster) %>%
            ggplot(aes(x=fct_rev(cluster), y=n)) + 
            geom_col(aes(fill=cluster)) +
            geom_text(aes(y=n/2, label=n)) +
            coord_flip() + 
            labs(x="", y="# Counties", title="Membership by segment")
    }
    
    # Plot the population totals by segment
    # Show population totals by cluster
    p2 <- dfState %>%
        group_by(cluster) %>%
        summarize(pop=sum(pop)/1000000, .groups="drop") %>%
        ggplot(aes(x=fct_rev(cluster), y=pop)) + 
        geom_col(aes(fill=cluster)) + 
        geom_text(aes(y=pop/2, label=round(pop))) + 
        labs(y="Population (millions)", x="Cluster", title="Population by cluster (millions)") +
        coord_flip()
    
    # Plot the rolling 7-day mean daily disease burden by cluster
    # Create the p3Data to be either median of all elements in cluster or weighted mean
    p3 <- dfPlot %>%        
        select(date, cluster, cases=cpm7, deaths=dpm7, pop) %>%
        pivot_longer((-c(date, cluster, pop))) %>%
        filter(!is.na(value)) %>%
        group_by(date, cluster, name) %>%
        summarize(mdnValue=median(value), wtdValue=sum(pop*value)/sum(pop), .groups="drop") %>%
        ggplot(aes(x=date, y=if(weightedMean) wtdValue else mdnValue)) +
        geom_line(aes(group=cluster, color=cluster)) +
        facet_wrap(~name, scales="free_y") +
        labs(x="",
             y="Rolling 7-day mean, per million",
             title="Rolling 7-day mean daily disease burden, per million",
             subtitle=paste0(if(weightedMean) "Weighted mean" else "Median", 
                             " per day for states assigned to cluster"
             )
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b")
    
    # Plot the total cases and total deaths by cluster
    p4 <- dfPlot %>%
        group_by(cluster) %>%
        summarize(cases=sum(cases), deaths=sum(deaths), .groups="drop") %>%
        pivot_longer(-cluster) %>%
        ggplot(aes(x=fct_rev(cluster), y=value/1000)) + 
        geom_col(aes(fill=cluster)) + 
        geom_text(aes(y=value/2000, label=round(value/1000))) +
        coord_flip() + 
        facet_wrap(~varMapper[name], scales="free_x") + 
        labs(x="Cluster", y="Burden (000s)", title="Total cases and deaths by segment")
    
    # Place the plots together if plotsTogether is TRUE, otherwise just print
    if (isTRUE(clusterPlotsTogether)) {
        gridExtra::grid.arrange(p1, p2, p3, p4, nrow=2, ncol=2)
    } else {
        print(p1); print(p2); print(p3); print(p4)
    }
    
}



# Helper function to make total and per capita by state (calls its own helper function)
helperCallTotalPerCapita <- function(dfPlot, 
                                     thruLabel
                                     ) {
    
    # FUNCTION ARGUMENTS:
    # dfPlot: the plotting data frame
    # thruLabel: the date that data are through
    
    # Plot total cases and total deaths by state, colored by cluster
    helperBarDeathsCases(dfPlot, 
                         numVars=c("cases", "deaths"), 
                         title=paste0("Coronavirus impact by state through ", thruLabel), 
                         xVar=c("state"), 
                         fillVar=c("cluster")
    )
    
    # Plot cases per million and deaths per million by state, colored by cluster
    helperBarDeathsCases(dfPlot, 
                         numVars=c("cpm", "dpm"), 
                         title=paste0("Coronavirus impact by state through ", thruLabel), 
                         xVar=c("state"), 
                         fillVar=c("cluster")
    )
    
}



# Helper function to make recent vs. total plots
helperCallRecentvsTotal <- function(dfPlot, 
                                    thruLabel, 
                                    labelPoints, 
                                    recentTotalTogether
                                    ) {
    
    # FUNCTION ARGUMENTS:
    # dfPlot: the plotting data frame
    # thruLabel: the date that data are through
    # labelPoints: boolean, whether to label the individual points
    # recentTotalTogether: boolean, whether to put these plots together on one page
    
    # Plot last-30 vs total for cases per million by state, colored by cluster
    p7 <- helperRecentvsTotal(dfPlot, 
                              xVar="cpm", 
                              yVar="newcpm", 
                              title=paste0("Coronavirus burden through ", thruLabel), 
                              labelPlot=labelPoints,
                              printPlot=FALSE
    )
    
    # Plot last-30 vs total for deaths per million by state, colored by cluster
    p8 <- helperRecentvsTotal(dfPlot, 
                              xVar="dpm", 
                              yVar="newdpm", 
                              title=paste0("Coronavirus burden through ", thruLabel), 
                              labelPlot=labelPoints,
                              printPlot=FALSE
    )
    
    # Print the plots either as a single page or separately
    if (isTRUE(recentTotalTogether)) {
        gridExtra::grid.arrange(p7, p8, nrow=1)
    } else {
        print(p7); print(p8)
    }    
    
}



# Helper function to create total vs. elements plots
helperCallTotalvsElements <- function(dfPlot, 
                                      aggAndTotal, 
                                      clusterAggTogether,
                                      ...
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # dfPlot: the plotting data frame
    # aggAndTotal: boolean, should each individual line be plotted (if FALSE an 80% CI is plotted instead)
    # clusterAggTogether: boolean, whether to print the plots all on a single page
    # ...: any other arguments to pass to helperTotalvsElements (especially pctRibbon or aggFunc)
    
    # Plot the cases per million on a free y-scale and a fixed y-scale
    p9 <- helperTotalvsElements(dfPlot, 
                                keyVar="cpm7", 
                                aggAndTotal=aggAndTotal,
                                title="Cases per million, 7-day rolling mean", 
                                printPlot=FALSE, 
                                ...
    )
    p10 <- helperTotalvsElements(dfPlot, 
                                 keyVar="cpm7", 
                                 aggAndTotal=aggAndTotal,
                                 title="Cases per million, 7-day rolling mean", 
                                 facetScales="fixed", 
                                 printPlot=FALSE, 
                                 ...
    )
    
    # Plot the deaths per million on a free y-scale and a fixed y-scale
    p11 <- helperTotalvsElements(dfPlot, 
                                 keyVar="dpm7", 
                                 aggAndTotal=aggAndTotal,
                                 title="Deaths per million, 7-day rolling mean", 
                                 printPlot=FALSE, 
                                 ...
    )
    p12 <- helperTotalvsElements(dfPlot, 
                                 keyVar="dpm7", 
                                 aggAndTotal=aggAndTotal,
                                 title="Deaths per million, 7-day rolling mean", 
                                 facetScales="fixed", 
                                 printPlot=FALSE, 
                                 ...
    )
    
    if (isTRUE(clusterAggTogether)) {
        gridExtra::grid.arrange(p9, p11, nrow=1)
        gridExtra::grid.arrange(p10, p12, nrow=1)
    } else {
        print(p9); print(p10); print(p11); print(p12)
    }
    
}



# Function to create side-by-side plots for a deaths and cases metric
# Data in df will be aggregated to be unique by byVar using aggFunc
helperBarDeathsCases <- function(df, 
                                 numVars,
                                 title="",
                                 xVar="state",
                                 fillVar=NULL,
                                 aggFunc=sum, 
                                 mapper=varMapper
                                 ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing the data
    # numVars: the relevant numeric variables for plotting
    # title: plot title, default is nothing
    # xVar: the x-axis variable for plotting
    # fillVar: the variable that will color the bars in the final plot (NULL means use "lightblue" for all)
    # aggFunc: the aggregate function (will be applied to create data unique by byVar)
    # mapper: mapping file to convert x/y variables to descriptive axes (named vector "variable"="label")
    
    # OVERALL FUNCTION PROCESS:
    # 1.  Variables in numVar are aggregated by aggFunc to be unique by c(xVar, fillVar)
    # 2.  Data are pivoted longer
    # 3.  Bar charts are created, with coloring by fillVar if provided
    
    # Create the byVar for summing
    byVar <- xVar
    if (!is.null(fillVar)) { byVar <- c(byVar, fillVar) }
    
    # Process the data and create the plot
    p1 <- df %>%
        select_at(vars(all_of(c(byVar, numVars)))) %>%
        group_by_at(vars(all_of(byVar))) %>%
        summarize_all(aggFunc) %>%
        pivot_longer(-all_of(byVar)) %>%
        ggplot(aes(x=fct_reorder(get(xVar), value, .fun=min), y=value)) + 
        coord_flip() + 
        facet_wrap(~mapper[name], scales="free_x") + 
        labs(x="", y="", title=title) + 
        if (is.null(fillVar)) geom_col(fill="lightblue") else geom_col(aes_string(fill=fillVar))
    
    # Print the plot
    print(p1)
    
}



# Helper function to assess 30-day change vs. total
helperRecentvsTotal <- function(df, 
                                xVar, 
                                yVar,
                                title,
                                recencyDays=30, 
                                ablineSlope=0.5, 
                                mapper=varMapper, 
                                labelPlot=TRUE,
                                printPlot=TRUE
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: the tibble containing data by state by day
    # xVar: the x-variable
    # yVar: the y-variable
    # title: the plot title
    # recencyDays: number of days to consider as recent
    # ablineSlope: dashed line will be drawn with this slope and intercept 0
    # mapper: mapping file to convert x/y variables to descriptive axes (named vector "variable"="label")
    # labelPlot: boolean, whether to show the labels for each point
    # printPlot: boolean, whether to display the plot (if FALSE, plot object is returned)
    
    # Get the most date cutoff
    dateCutoff <- df %>% pull(date) %>% max() - recencyDays + 1
    cat("\nRecency is defined as", format(dateCutoff, "%Y-%m-%d"), "through current\n")
    
    # Create the plot
    p1 <- df %>%
        mutate(newCases=ifelse(date >= dateCutoff, cases, 0), 
               newDeaths=ifelse(date >= dateCutoff, deaths, 0), 
               newcpm=ifelse(date >= dateCutoff, cpm, 0), 
               newdpm=ifelse(date >= dateCutoff, dpm, 0)
        ) %>%
        group_by(state, cluster) %>%
        summarize_if(is.numeric, .funs=sum) %>%
        ungroup() %>%
        ggplot(aes_string(x=xVar, y=yVar)) + 
        labs(x=mapper[xVar], 
             y=mapper[yVar], 
             title=title, 
             subtitle=paste0("Dashed line represents ", 
                             round(100*ablineSlope), 
                             "% of total is new in last ", 
                             recencyDays,
                             " days"
             )
        ) + 
        geom_abline(lty=2, slope=ablineSlope) + 
        lims(x=c(0, NA), y=c(0, NA)) + 
        theme(legend.position = "bottom")
    
    # Add the appropriate geom (scatterplot if labelPlot is FALSE)
    if (labelPlot) p1 <- p1 + geom_text(aes(color=cluster, label=state))
    else p1 <- p1 + geom_point(aes(color=cluster), alpha=0.5)
    
    if (isTRUE(printPlot)) {
        print(p1)
    } else {
        p1
    }
    
}



# Function to plot cluster vs. individual elements on a key metric
helperTotalvsElements <- function(df, 
                                  keyVar, 
                                  title,
                                  aggAndTotal=TRUE,
                                  pctRibbon=0.8,
                                  aggFunc=if(aggAndTotal) median else mean, 
                                  mapper=varMapper, 
                                  facetScales="free_y", 
                                  printPlot=TRUE
                                  ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing n-day rolling averages
    # keyVar: the variable to be plotted
    # title: the plot title
    # aggAndTotal: boolean, whether to plot every individual observation along with the cluster aggregate
    # pctRibbon: if aggAndTotal is FALSE, a ribbon covering this percentage of the data will be plotted
    # aggFunc: how to aggregate the elements to the segment
    #          CAUTION that this is an aggregate of averages, rather than a population-weighted aggregate
    # mapper: the variable mapping file to get the appropriate label for keyVar
    # facetScales: the scaling for the facets - "free_y" to let them all float, "fixed" to have them the same
    # printPlot: boolean, if TRUE print the plot (otherwise return the plot object)
    
    # Create an appropriate subtitle
    subtitle <- if(facetScales=="free_y") {
        "Caution that each facet has its own y axis with different scales"
    } else if (facetScales=="fixed") { 
        "All facets are on the same scale"
    } else {
        "Update subtitle code in function helperTotalvsElements"
    }
    
    # Create an appropriate caption
    xCap <- "1. Cluster aggregate is mean over all observations (NOT population-weighted)"
    xCap <- paste0(xCap, "\n2. Ribbons reflect range covering ", round(pctRibbon*100), "% of observations")
    caption <- if(aggAndTotal) {
        "Cluster aggregate is median, weighting each observation equally\n(NOT population-weighted)"
    } else {
        xCap
    }
    
    # Create the plots for segment-level data
    p1 <- df %>%
        rbind(mutate(., state="cluster")) %>%
        group_by(state, cluster, date) %>%
        summarize_at(vars(all_of(keyVar)), .funs=aggFunc) %>%
        ungroup() %>%
        filter(!is.na(get(keyVar))) %>%
        ggplot(aes_string(x="date")) + 
        geom_line(data=~filter(., state == "cluster"), 
                  aes(y=get(keyVar), group=state, color=cluster), 
                  lwd=1.5
        ) + 
        facet_wrap(~cluster, scales=facetScales) + 
        labs(x="", 
             y=mapper[keyVar], 
             title=title, 
             subtitle=subtitle,
             caption=caption
        ) + 
        ylim(c(0, NA)) + 
        theme(legend.position="bottom")
    
    # Add an appropriate individual metric (either every observation or quantiles)
    if (aggAndTotal) {
        p1 <- p1 + 
            geom_line(data=~filter(., state != "cluster"), 
                      aes(y=get(keyVar), group=state), 
                      alpha=0.25
            )
    } else {
        dfRibbon <- df %>%
            filter(!is.na(get(keyVar))) %>%
            group_by(cluster, date) %>%
            summarize(ylow=quantile(get(keyVar), 0.5-0.5*pctRibbon), 
                      yhigh=quantile(get(keyVar), 0.5+0.5*pctRibbon), 
                      .groups="drop"
            )
        p1 <- p1 + 
            geom_ribbon(data=dfRibbon, 
                        aes(ymin=ylow, ymax=yhigh), 
                        alpha=0.25
            )
    }
    
    # Print plot if requested, otherwise return it
    if (isTRUE(printPlot)) {
        print(p1)
    } else {
        p1
    }
    
}

```
  
The function is then run using the latest hierarchical clusters:  
```{r}

# Example for running assessClusters
plotDataHier3 <- assessClusters(cutree(clHier3, k=6), 
                                dfState=pop_2019, 
                                dfBurden=dfPer3,
                                thruLabel="2021-01-20",
                                plotsTogether=TRUE
                                )

```

The plotConsolidatedMetrics() function is copied:  
```{r}

# Function to create plots of consolidated metrics
plotConsolidatedMetrics <- function(dfMain, 
                                    dfHosp=NULL, 
                                    varMain=c("state", "cluster", "date", "pop", "cases", "deaths", "hosp"),
                                    varDropHosp=c("n", "pop"), 
                                    joinBy=c("state", "cluster", "date"), 
                                    subT=NULL, 
                                    nrowPlot2=1
                                    ) {
    
    # FUNCTION ARGUMENTS:
    # dfMain: the main file produced by assessClusters(), containing data by state-cluster-date
    # dfHosp: the separate file with hospital data (NULL means data already available in dfMain)
    # varMain: variables to keep from dfMain
    # varDropHosp: variables to drop from dfHosp
    # joinBy: variables for joining dfMain and dfHosp
    # subT: plot subtitle (NULL will use the defaults), 
    # nrowPlot2: number of rows for the facetting to use on plot 2
    
    if (is.null(subT)) {
        subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new)"
    }
    
    # Filter dfMain to include only variables in varMain
    dfMain <- dfMain %>%
        select_at(vars(all_of(varMain)))
    
    # Left join dfMain to dfHosp unless dfHosp is NULL
    if (!is.null(dfHosp)) {
        dfHosp <- dfHosp %>%
            select_at(vars(all_of(names(dfHosp)[!(names(dfHosp) %in% varDropHosp)])))
        dfMain <- dfMain %>%
            left_join(dfHosp, by=all_of(joinBy))
    }
    
    # Check that variables state, cluster, date, pop are all available
    if (!(c("state", "cluster", "date", "pop") %in% names(dfMain) %>% all())) {
        stop("\nFunction requires variables state, cluster, date, and pop after processing\n")
    }
    
    # Create the relevant plotting data
    dfPlot <- dfMain %>%
        pivot_longer(-c(state, cluster, date, pop)) %>%
        filter(!is.na(value)) %>%
        rbind(mutate(., state="cluster")) %>%
        group_by_at(vars(all_of(c(joinBy, "name")))) %>%
        summarize(value=sum(value), pop=sum(pop), .groups="drop") %>%
        mutate(vpm=1000000*value/pop) %>%
        arrange(state, cluster, name, date) %>%
        group_by(state, cluster, name) %>%
        helperRollingAgg(origVar="vpm", newName="vpm7")    
    
    # Create facetted plots for totals by metric by segment
    p1 <- dfPlot %>%
        filter(!is.na(vpm7)) %>%
        ggplot(aes(x=date, y=vpm7)) + 
        geom_line(data=~filter(., state=="cluster"), aes(group=cluster, color=cluster), lwd=1.5) +
        geom_line(data=~filter(., state!="cluster"), aes(group=state), alpha=0.25) + 
        facet_grid(name ~ cluster, scales="free_y") + 
        labs(x="", 
             y="Rolling 7-day mean per million", 
             title="Key metrics by cluster (7-day rolling mean per million)", 
             subtitle=subT
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b") + 
        theme(axis.text.x=element_text(angle=90))
    print(p1)
    
    # Create all-segment plot by metric
    p2 <- dfPlot %>%
        filter(!is.na(vpm7)) %>%
        ggplot(aes(x=date, y=vpm7)) + 
        geom_line(data=~filter(., state=="cluster"), aes(group=cluster, color=cluster), lwd=1.5) +
        facet_wrap(~ name, scales="free_y", nrow=nrowPlot2) + 
        labs(x="", 
             y="Rolling 7-day mean per million", 
             title="Key metrics by cluster (7-day rolling mean per million)", 
             subtitle=subT
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b") + 
        theme(axis.text.x=element_text(angle=90))
    print(p2)
    
    # Create all-metric plot by segment (define 100% as peak for segment-metric)
    p3 <- dfPlot %>%
        filter(!is.na(vpm7)) %>%
        group_by(state, cluster, name) %>%
        mutate(spm7=vpm7/max(vpm7)) %>%
        ggplot(aes(x=date, y=spm7)) + 
        geom_line(data=~filter(., state=="cluster"), aes(group=name, color=name), lwd=1) +
        facet_wrap(~ cluster, scales="free_y") + 
        labs(x="", 
             y="% of Maximum", 
             title="Key metrics by cluster (% of cluster maximum for variable)", 
             subtitle=subT
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b") + 
        scale_color_discrete("variable") +
        theme(axis.text.x=element_text(angle=90))
    print(p3)
    
    # Return the plotting data
    dfPlot
    
}

```

```{r}

# Example for consolidatedPlotData()
subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
consolidatedPlotDataHier3 <- plotConsolidatedMetrics(plotDataHier3, 
                                                     varMain=c("state", "cluster", "date", "pop",
                                                               "cases", "deaths", "hosp", "tests"
                                                               ), 
                                                     subT=subT, 
                                                     nrowPlot2=2
                                                     )

```

The makeCumulative() and plotCumulative() functions are copied:  
```{r}

# Function to convert a file to cumulative totals
makeCumulative <- function(df, 
                           typeVar="name", 
                           typeKeep=c("cases", "deaths", "tests"), 
                           valVar="vpm7", 
                           groupVars=c("state", "cluster", "name"), 
                           arrangeVars=c("date"), 
                           newName="cum7"
                           ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame containing the metrics
    # typeVar: the variable holding the metric type (default is 'name')
    # typeKeep: the values of typeVar to be kept
    # valVar: the variable holding the metric value (default is 'vpm7')
    # groupVars: groups for calculating cumulative sum
    # arrangeVars: variables to be sorted by before calculating cumulative sum
    # newName: the name for the new variable
    
    # Create the cumulative data
    dfCum <- df %>%
        filter(get(typeVar) %in% typeKeep, !is.na(get(valVar))) %>%
        arrange_at(vars(all_of(c(groupVars, arrangeVars)))) %>%
        group_by_at(groupVars) %>%
        mutate(!!newName:=cumsum(get(valVar))) %>%
        ungroup()
    
    # Return the processed data
    dfCum
    
}



# Function to plot cumulative data
plotCumulativeData <- function(df, 
                               keyMetricp2,
                               flagsp2,
                               p2Desc=keyMetricp2,
                               keyVar="cum7", 
                               makep1=FALSE, 
                               makep2=TRUE
                               ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame of cumulative data
    # keyMetricp2: the key metric to be plotted in the second plot (e.g., "deaths", "cases", "tests")
    # flagsp2: states to be treated as flagged in the second plot
    # p2Desc: the description to be used in plot 2
    # keyVar: the key variable to plot
    # makep1: boolean, whether to make the first plot
    # makep2: boolean, whether to make the second plot
    
    # Plot the cumulative data by cluster (if flag is set for producing this)
    if (isTRUE(makep1)) {
        p1 <- df %>%
            filter(state=="cluster") %>%
            ggplot(aes(x=date, y=get(keyVar))) + 
            geom_line(aes(group=cluster, color=cluster)) + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            scale_x_date(date_breaks="1 months", date_labels="%m") + 
            labs(x="Month", 
                 y="Cumulative Burden (per million)", 
                 title="Cumulative burden by segment (per million)"
            )
        print(p1)
    }
    
    
    # Plot the cumulative totals over time for one metric, and flag the highest
    if (isTRUE(makep2)) {
        p2 <- df %>%
            filter(state!="cluster", name==keyMetricp2) %>%
            mutate(bold=ifelse(state %in% flagsp2, 1, 0)) %>%
            ggplot(aes(x=date, y=get(keyVar))) + 
            geom_line(aes(group=state, color=cluster, alpha=0.4+0.6*bold, size=0.5+0.5*bold)) + 
            geom_text(data=~filter(., bold==1, date==max(date)), 
                      aes(x=date+lubridate::days(10), 
                          label=paste0(state, ": ", round(get(keyVar), 0)), 
                          color=cluster
                      ), 
                      size=3, 
                      fontface="bold"
            ) +
            scale_x_date(date_breaks="1 months", date_labels="%m") + 
            scale_alpha_identity() +
            scale_size_identity() +
            labs(x="Month", 
                 y=paste0("Cumulative ", p2Desc, " (per million)"), 
                 title=paste0("Cumulative coronavirus ", p2Desc, " by state (per million)"), 
                 subtitle="Top 5 states for total and growth rate are bolded and labelled"
            )
        print(p2)
    }
    
}


# Function to find and flag states that are high on a key value or change in key value
findFlagStates <- function(df, 
                           keyMetricVal, 
                           keyMetricVar="name", 
                           cumVar="cum7", 
                           prevDays=30, 
                           nFlag=5
                           ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing the cumulative data
    # keyMetricVal: the metric of interest (e.g., "deaths", "cases", "tests")
    # keyMetricVar: the variable name for the column containing the metric of interest
    # cumVar: variable containing the cumulative totals
    # prevDays: the number of days previous to use for defining growth
    # nFlag: the number of states to flag for either total or growth (top-n of each)
    
    # Find top-5 in either total or recent increase
    dfFlag <- df %>%
        filter(get(keyMetricVar)==keyMetricVal, state!="cluster") %>%
        select_at(vars(all_of(c("state", "date", cumVar)))) %>%
        group_by(state) %>%
        summarize(maxVal=max(get(cumVar)), 
                  tminus=sum(ifelse(date==max(date)-lubridate::days(prevDays), get(cumVar), 0)), 
                  .groups="drop"
        ) %>%
        ungroup() %>%
        mutate(growth=maxVal-tminus, 
               rkTotal=min_rank(-maxVal), 
               rkGrowth=min_rank(-growth), 
               flag=ifelse(pmin(rkTotal, rkGrowth)<=nFlag, 1, 0)
        ) %>%
        arrange(-flag, rkTotal)
    
    # Return the top values as a vector of states
    dfFlag %>%
        filter(flag==1) %>%
        pull(state)
    
}

```

```{r}

consPosHier3 <- consolidatedPlotDataHier3 %>%
    ungroup() %>%
    select(state, cluster, date, name, vpm7) %>%
    arrange(state, cluster, date, name) %>%
    pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
    mutate(pctpos=cases/tests) %>%
    pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
    filter(!is.na(vpm7))

clCumHier3 <- makeCumulative(consPosHier3)

plotCumulativeData(clCumHier3, 
                   keyMetricp2="", 
                   flagsp2="", 
                   makep1=TRUE, 
                   makep2=FALSE
                   )
plotCumulativeData(clCumHier3, 
                   keyMetricp2="deaths", 
                   flagsp2=findFlagStates(clCumHier3, keyMetricVal = "deaths")
                   )
plotCumulativeData(clCumHier3, 
                   keyMetricp2="cases", 
                   flagsp2=findFlagStates(clCumHier3, keyMetricVal = "cases")
                   )
plotCumulativeData(clCumHier3, 
                   keyMetricp2="tests", 
                   flagsp2=findFlagStates(clCumHier3, keyMetricVal = "tests")
                   )

```
  
The functions appear to all be working for 2021 data from COVID Tracking Project. Next steps are to try the integrated function on the latest data.

The integrated function is run using data downloaded on 26-JAN-2021, with a similar approach as previous:  
```{r cache=TRUE}

# Create new segments with updated data
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_210126.csv"
ctp_hier7_210126 <- readRunCOVIDTrackingProject(thruLabel="Jan 25, 2021", 
                                                downloadTo=if(file.exists(locDownload)) NULL else locDownload,
                                                readFrom=locDownload, 
                                                compareFile=readFromRDS("test_hier5_201025")$dfRaw,
                                                hierarchical=TRUE, 
                                                reAssignState=list("HI"="ME"), 
                                                kCut=8, 
                                                minShape=3, 
                                                ratioDeathvsCase = 5, 
                                                ratioTotalvsShape = 0.25, 
                                                minDeath=100, 
                                                minCase=10000
                                                )
saveToRDS(ctp_hier7_210126, ovrWriteError=FALSE)

```

The main function is modified slightly to allow for taking advantage of the updated functions:  
```{r}

# Function to download/load, process, segment, and analyze data from COVID Tracking Project
readRunCOVIDTrackingProject <- function(thruLabel, 
                                        downloadTo=NULL, 
                                        readFrom=downloadTo, 
                                        compareFile=NULL,
                                        dateChangePlot=FALSE,
                                        dateMetricPrint=TRUE,
                                        writeLog=NULL,
                                        ovrwriteLog=TRUE,
                                        dfPerCapita=NULL,
                                        useClusters=NULL,
                                        hierarchical=TRUE,
                                        returnList=!hierarchical, 
                                        kCut=6,
                                        reAssignState=vector("list", 0),
                                        makeCumulativePlots=TRUE,
                                        skipAssessmentPlots=FALSE,
                                        ...
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # thruLabel: the label for when the data are through (e.g., "Aug 30, 2020")
    # donwloadTo: download the most recent COVID Tracking Project data to this location
    #             NULL means do not download any data
    # readFrom: location for reading in the COVID Tracking Project data (defaults to donwloadTo)
    # compareFile: name of the file to use for comparisons when reading in raw data (NULL means no comparison)
    # dateChangePlot: boolean, should changes in dates be captured as a plot rather than as a list?
    # dateMetricPrint: boolean, should the changes by date and metric be printed to the main log?
    # writeLog: name of a separate log file for capturing detailed data on changes between files
    #           NULL means no detailed data captured
    # ovrwriteLog: boolean, should the log file be overwritten and started again from scratch?
    # dfPerCapita: file can be passed directly, which bypasses the loading and processing steps
    # useClusters: file containing clusters by state (NULL means make the clusters from the data)
    # hierarchical: boolean, should hierarchical clusters be produced (if FALSE, will be k-means)?
    # returnList: boolean, should a list be returned or just the cluster object?
    #             refers to what is returned by clusterStates(); the main function always returns a list
    # kCut: number of segments when cutting the hierarchical tree
    # reAssignState: mapping file for assigning a state to another state's cluster
    #                format list("stateToChange"="stateClusterToAssign")
    # makeCumulativePlots: whether to make plots of cumulative metrics
    # skipAssessmentPlots: boolean to skip the plots for assessClusters()
    #                      especially useful if just exploring dendrograms or silhouette widths
    # ...: arguments to be passed to clusterStates(), will be used only if useClusters is NULL
    
    
    # STEP 1: Get state data
    stateData <- getStateData()
    
    # Helper function for glimpsing
    glimpseFile <- function(x, txt) {
        cat(txt)
        glimpse(x)
    }
            
    # STEPS 2-4 are run only if dfPerCapita does not exist
    if (is.null(dfPerCapita)) {
        
        # STEP 2a: Download latest COVID Tracking Project data (if requested)
        if (!is.null(downloadTo)) downloadCOVIDbyState(fileName=downloadTo)
        
        # STEP 2b: Read-in COVID Tracking Project data
        dfRaw <- readCOViDbyState(readFrom, 
                                  checkFile=compareFile, 
                                  dateChangePlot=dateChangePlot, 
                                  dateMetricPrint=dateMetricPrint, 
                                  writeLog=writeLog, 
                                  ovrwriteLog=ovrwriteLog
                                  )
        if (is.null(writeLog)) glimpseFile(dfRaw, txt="\nRaw data file:\n")
        else capture.output(glimpseFile(dfRaw, txt="\nRaw data file:\n"), file=writeLog, append=TRUE)
        
        
        
        # STEP 3: Process the data so that it includes all requested key variables
        varsFilter <- c("date", "state", "positiveIncrease", "deathIncrease", 
                        "hospitalizedCurrently", "totalTestResultsIncrease"
        )
        dfFiltered <- processCVData(dfRaw, 
                                    varsKeep=varsFilter, 
                                    varsRename=c(positiveIncrease="cases", 
                                                 deathIncrease="deaths", 
                                                 hospitalizedCurrently="hosp", 
                                                 totalTestResultsIncrease="tests"
                                    )
        )
        if (is.null(writeLog)) glimpseFile(dfFiltered, txt="\nFiltered data file:\n")
        else capture.output(glimpseFile(dfFiltered, txt="\nFiltered data file:\n"), file=writeLog, append=TRUE)
        
        # STEP 4: Convert to per capita
        dfPerCapita <- helperMakePerCapita(dfFiltered, 
                                           mapVars=c("cases"="cpm", "deaths"="dpm", 
                                                     "hosp"="hpm", "tests"="tpm"
                                           ), 
                                           popData=stateData
        )
        if (is.null(writeLog)) glimpseFile(dfPerCapita, txt="\nPer capita data file:\n")
        else capture.output(glimpseFile(dfPerCapita, txt="\nPer capita data file:\n"), 
                            file=writeLog, 
                            append=TRUE
                            )
        
    } else {
        dfRaw <- NULL
        dfFiltered <- NULL
    }
    
    
    # STEP 5: Create the clusters (if they have not been passed)
    if (is.null(useClusters)) {
        # Run the clustering process
        clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
        # If hierarchical clusters, cut the tree, otherwise use the output object directly
        if (hierarchical) {
            useClusters <- cutree(clData, k=kCut)
        } else {
            useClusters <- clData$objCluster$cluster
        }
        # If requested, manually assign clusters to the cluster for another state
        for (xNum in seq_len(length(reAssignState))) {
            useClusters[names(reAssignState)[xNum]] <- useClusters[reAssignState[[xNum]]]
        }
        
    }
    
    
    # STEP 5a: Stop the process and return what is available if skipAssessmentPlots is TRUE
    if (skipAssessmentPlots) {
        return(list(stateData=stateData, 
                    dfRaw=dfRaw, 
                    dfFiltered=dfFiltered, 
                    dfPerCapita=dfPerCapita, 
                    useClusters=useClusters, 
                    plotData=NULL, 
                    consolidatedPlotData=NULL, 
                    clCum=NULL
                    )
               )
    }
    
    
    # STEP 6: Create the cluster assessments
    plotData <- assessClusters(useClusters, 
                               dfState=stateData, 
                               dfBurden=dfPerCapita,
                               thruLabel=thruLabel,
                               plotsTogether=TRUE
    )
    
    
    # STEP 7: Plot the consolidated metrics
    subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
    consolidatedPlotData <- plotConsolidatedMetrics(plotData, 
                                                    varMain=c("state", "cluster", "date", "pop",
                                                              "cases", "deaths", "hosp", "tests"
                                                    ), 
                                                    subT=subT, 
                                                    nrowPlot2=2
    )
    
    # STEP 8: Create cumulative metrics if requested
    if (makeCumulativePlots) {
        consPos <- consolidatedPlotData %>%
            ungroup() %>%
            select(state, cluster, date, name, vpm7) %>%
            arrange(state, cluster, date, name) %>%
            pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
            mutate(pctpos=cases/tests) %>%
            pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
            filter(!is.na(vpm7))
        clCum <- makeCumulative(consPos)
        plotCumulativeData(clCum, 
                           keyMetricp2="", 
                           flagsp2="", 
                           makep1=TRUE, 
                           makep2=FALSE
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="deaths", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "deaths")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="cases", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "cases")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="tests", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "tests")
        )
    } else {
        clCum <- NULL
    }
    
    
    # STEP 9: Return a list of the key data
    list(stateData=stateData, 
         dfRaw=dfRaw, 
         dfFiltered=dfFiltered, 
         dfPerCapita=dfPerCapita, 
         useClusters=useClusters, 
         plotData=plotData, 
         consolidatedPlotData=consolidatedPlotData, 
         clCum=clCum
    )
    
    
}

```

The updated function is then run:  
```{r cache=TRUE}

# Custom shape function for managing 2020 and 2021
customTimeBucket <- function(x) {
    paste0(lubridate::year(x), "-", stringr::str_pad(lubridate::month(x), width=2, side="left", pad="0"))
}

# Create new segments with updated data
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_210127.csv"
locLog <- "./RInputFiles/Coronavirus/downloaded_20210127.log"
ctp_hier6_210127 <- readRunCOVIDTrackingProject(thruLabel="Jan 26, 2021", 
                                                downloadTo=if(file.exists(locDownload)) NULL else locDownload,
                                                readFrom=locDownload, 
                                                compareFile=readFromRDS("test_hier5_201025")$dfRaw,
                                                dateChangePlot=TRUE,
                                                dateMetricPrint=FALSE,
                                                writeLog=locLog,
                                                hierarchical=TRUE, 
                                                reAssignState=list("HI"="ME", "VT"="ME"), 
                                                kCut=8, 
                                                shapeFunc=customTimeBucket,
                                                minShape="2020-04", 
                                                maxShape="2021-01",
                                                ratioDeathvsCase = 5, 
                                                ratioTotalvsShape = 0.25, 
                                                minDeath=100, 
                                                minCase=10000
                                                )
saveToRDS(ctp_hier6_210127, ovrWriteError=FALSE)

```

The updated functionality appears to be working for 2021.

The assessClusters() function can be used to create different clusters, such as for US census regions:  
```{r}

stateCensus <- c(as.character(state.region), "South")
names(stateCensus) <- c(state.abb, "DC")

ctp_census_hier6_210127 <- assessClusters(stateCensus, 
                                          dfState=ctp_hier6_210127$stateData, 
                                          dfBurden=ctp_hier6_210127$dfPerCapita, 
                                          thruLabel="Jan 26, 2021", 
                                          plotsTogether=TRUE, 
                                          makeTotalvsPerCapitaPlots=FALSE, 
                                          makeRecentvsTotalPlots=FALSE
                                          )

ctp_census_hier6_210127

```

Consolidated metrics can also be plotted:  
```{r}

# Plot the consolidated metrics
subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
consolidatedPlot_hier6_210127 <- plotConsolidatedMetrics(ctp_census_hier6_210127, 
                                                         varMain=c("state", "cluster", "date", "pop",
                                                                   "cases", "deaths", "hosp", "tests"
                                                                   ), 
                                                         subT=subT, 
                                                         nrowPlot2=2
                                                         )

```

Cumulative metrics can also be plotted:  
```{r}

# Create cumulative metrics
consPos_hier6_210127 <- consolidatedPlot_hier6_210127 %>%
    ungroup() %>%
    select(state, cluster, date, name, vpm7) %>%
    arrange(state, cluster, date, name) %>%
    pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
    mutate(pctpos=cases/tests) %>%
    pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
    filter(!is.na(vpm7))

clCum_hier6_210127 <- makeCumulative(consPos_hier6_210127)

# Cumulative burden per million by segment
plotCumulativeData(clCum_hier6_210127, 
                   keyMetricp2="", 
                   flagsp2="", 
                   makep1=TRUE, 
                   makep2=FALSE
                   )
  
# Evolution of deaths      
plotCumulativeData(clCum_hier6_210127, 
                   keyMetricp2="deaths", 
                   flagsp2=findFlagStates(clCum_hier6_210127, keyMetricVal = "deaths")
                   )

# Evolution of cases
plotCumulativeData(clCum_hier6_210127, 
                   keyMetricp2="cases", 
                   flagsp2=findFlagStates(clCum_hier6_210127, keyMetricVal = "cases")
                   )

# Evolution of tests
plotCumulativeData(clCum_hier6_210127, 
                   keyMetricp2="tests", 
                   flagsp2=findFlagStates(clCum_hier6_210127, keyMetricVal = "tests")
                   )

```

A function is written to allow for creating plots using an existing frame and a new list of segments:  
```{r}

# Function to create plots for a new segment mapping and existing state-level data
createSegmentPlots <- function(stateSegments, 
                               popData=NULL, 
                               burdenData=NULL,
                               lstData=NULL, 
                               thruLabel="Latest data", 
                               plotsTogether=TRUE, 
                               makeTotalvsPerCapitaPlots=FALSE,
                               makeRecentvsTotalPlots=FALSE
                               ) {

    # FUNCTION ARGUMENTS:
    # stateSegments: a named vector containing the states and their segments
    # popData: tibble or data frame containing population data by state (NULL means get from lstData)
    # burdenData: tibble or data frame containing per capita burden data by state (NULL means get from lstData)
    # lstData: list file containing elements for popData and burdenData 
    #          (NULL means get from popData and burdenData)
    # thruLabel: plot label for when the data are through
    # plotsTogether: pass-through argument for assessClusters()
    # makeTotalvsPerCapitaPlots: pass-through argument for assessClusters()
    # makeRecentvsTotalPlots: pass-through argument for assessClusters()

    # Get popData and/or burdenData from lstData if needed
    if (is.null(popData)) {
        if (is.null(lstData)) stop("\nMust provide either popData or lstData\n")
        popData <- lstData[["stateData"]]
    }
    if (is.null(burdenData)) {
        if (is.null(lstData)) stop("\nMust provide burdenData or lstData\n")
        burdenData <- lstData[["dfPerCapita"]]
    }

    # Run the cluster assessment algorithm
    ctp_new <- assessClusters(stateSegments, 
                              dfState=popData, 
                              dfBurden=burdenData, 
                              thruLabel=thruLabel, 
                              plotsTogether=plotsTogether, 
                              makeTotalvsPerCapitaPlots=makeTotalvsPerCapitaPlots, 
                              makeRecentvsTotalPlots=makeRecentvsTotalPlots
                              )
    
    # Plot the consolidated metrics
    subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
    consolidatedPlot_new <- plotConsolidatedMetrics(ctp_new, 
                                                    varMain=c("state", "cluster", "date", "pop",
                                                              "cases", "deaths", "hosp", "tests"
                                                              ), 
                                                    subT=subT, 
                                                    nrowPlot2=2
                                                    )


    # Create cumulative metrics
    consPos_new <- consolidatedPlot_new %>%
        ungroup() %>%
        select(state, cluster, date, name, vpm7) %>%
        arrange(state, cluster, date, name) %>%
        pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
        mutate(pctpos=cases/tests) %>%
        pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
        filter(!is.na(vpm7))

    clCum_new <- makeCumulative(consPos_new)

    # Cumulative burden per million by segment
    plotCumulativeData(clCum_new, 
                       keyMetricp2="", 
                       flagsp2="", 
                       makep1=TRUE, 
                       makep2=FALSE
                       )

    # Return the data frames as a list
    list(ctp_new=ctp_new, consolidatedPlot_new=consolidatedPlot_new, clCum_new=clCum_new)
    
}

# Test the function
stateCensus <- c(as.character(state.region), "South")
names(stateCensus) <- c(state.abb, "DC")

# Using lstData
createSegmentPlots(stateCensus, lstData=ctp_hier6_210127, thruLabel="Jan 26, 2021")

# Using popData and burdenData
createSegmentPlots(stateCensus, 
                   popData=ctp_hier6_210127$stateData, 
                   burdenData=ctp_hier6_210127$dfPerCapita,
                   thruLabel="Jan 26, 2021"
                   )
    
```

Data through January 2021 are downloaded, with existing segments used:  
```{r cache=TRUE}

# Create new segments with updated data
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_210201.csv"
locLog <- "./RInputFiles/Coronavirus/downloaded_20210201.log"
ctp_hier6_210201 <- readRunCOVIDTrackingProject(thruLabel="Jan 31, 2021", 
                                                downloadTo=if(file.exists(locDownload)) NULL else locDownload,
                                                readFrom=locDownload, 
                                                compareFile=readFromRDS("test_hier5_201025")$dfRaw,
                                                dateChangePlot=TRUE,
                                                dateMetricPrint=FALSE,
                                                writeLog=locLog,
                                                useClusters=ctp_hier6_210127$useClusters
                                                )
saveToRDS(ctp_hier6_210201, ovrWriteError=FALSE)

```

The updated data are summarized as census regions:  
```{r}

# Using the list created by the most recent run
createSegmentPlots(stateCensus, lstData=ctp_hier6_210201, thruLabel="Jan 31, 2021")

```

The updated data are summarized by census division:  
```{r}

# Test the function
stateCensusDivision <- c(as.character(state.division), "South Atlantic")
names(stateCensusDivision) <- c(state.abb, "DC")

# Using the list created by the most recent run
createSegmentPlots(stateCensusDivision, lstData=ctp_hier6_210201, thruLabel="Jan 31, 2021")

```

The updated data are summarized by population density:  
```{r}

# Create a state density file
stateDensity <- tibble::tibble(state=c(state.abb, "DC"), 
                               sqm=c(state.area, 68), 
                               region=c(as.character(state.region), "South")
                               ) %>%
    inner_join(select(statePop2019, stateAbb, pop_2019), by=c("state"="stateAbb")) %>%
    mutate(density=pop_2019/sqm) %>%
    arrange(state)

# Plot density components
stateDensity %>%
    ggplot(aes(x=sqm, y=pop_2019)) + 
    geom_text(aes(label=state)) + 
    scale_x_log10() + 
    scale_y_log10() + 
    labs(x="Square miles", y="2019 population", title="Population density components by state")

# Plot densities
stateDensity %>%
    ggplot(aes(y=density, x=pop_2019)) + 
    geom_text(aes(label=state)) + 
    scale_x_log10() + 
    scale_y_log10() + 
    labs(y="Population density", x="2019 population", title="Population density by state")

# Get summary statistics
summary(stateDensity)

# Create density buckets
stateDensity <- stateDensity %>%
    mutate(densityBucket=case_when(density < 30 ~ "1 - Low", 
                                   density < 100 ~ "2 - Medium Low", 
                                   density < 300 ~ "3 - Medium High", 
                                   TRUE ~ "4 - High"
                                   )
           )

# Create vector
stateDensityVector <- stateDensity$densityBucket
names(stateDensityVector) <- stateDensity$state

# Create summaries based on state population density
createSegmentPlots(stateDensityVector, lstData=ctp_hier6_210201, thruLabel="Jan 31, 2021")

```

Functionality is added to pick a subset of states and to plot cumulative and daily metrics solely for those:  
```{r}

# Plot a subset of states
subsetStatesCTP <- function(df, 
                            stateSubset=c(state.abb, "DC"), 
                            stateHighlight=c(), 
                            metricsPlot=c("deaths", "cases", "hosp"),
                            plotCum=c("deaths"=TRUE, "cases"=TRUE, "tests"=TRUE, "hosp"=FALSE), 
                            plotLab=c("deaths"="Corovavirus deaths per milliton", 
                                      "cases"="Coronavirus cases per million", 
                                      "tests"="Coronavirus tests per million", 
                                      "hosp"="Total currently hospitalized with cornavirus per million"
                                      )
                            ) {

    # FUNCTION ARGUMENTS:
    # df: the processed list file OR a data frame that is of the same format as consolidatedPlotData
    # stateSubset: the subset of states to plot
    # stateHighlight: the subset of states to plot using bold and colors
    #                 states in stateSubset but not in stateHighlight will be in light grey
    # metricsPlot: the metric(s) to be plotted
    # plotCum: named vector, names are the available metrics, value is whether to make a cumulative plot
    # plotLab: the label to be used for the plot title
    
    # Check that valid metric(s) have been passed
    if (!all(metricsPlot %in% names(plotCum))) {
        cat("\nmetricsPlot includes metrics that are not available:\n")
        cat("Metrics requested:", metricsPlot)
        cat("\nMetrics available:", names(plotCum))
        stop("\n")
    }
    
    # Extract the relevant data from the list
    if ("list" %in% class(df)) {
        df <- df[["consolidatedPlotData"]]
    }
    
    # Filter to the states of interest and create cumulative vpm by state-date-name
    df <- df %>%
        filter(state %in% c(stateSubset, stateHighlight)) %>%
        ungroup() %>%
        arrange(state, name, date) %>%
        group_by(state, name) %>%
        mutate(cumvpm=cumsum(vpm)) %>%
        ungroup()
    
    # Create a plot for the metric(s) of interest
    for (thisMetric in metricsPlot) {
        # Create the daily ploy using vpm7
        p1 <- df %>%
            filter(name==thisMetric, !is.na(vpm7)) %>%
            ggplot(aes(x=date, y=vpm7)) + 
            geom_line(aes(group=state), color="lightgrey") + 
            geom_line(data=~filter(., state %in% stateHighlight), aes(group=state, color=state), lwd=1) +
            labs(x="", y="Rolling 7-day average (per million)", title=plotLab[thisMetric]) + 
            scale_color_discrete("")
        # Create the cumulative plot using cumvpm iff plotCum[thisMetric] is TRUE
        if (isTRUE(plotCum[thisMetric])) {
            p2 <- df %>%
                filter(name==thisMetric) %>%
                ggplot(aes(x=date, y=cumvpm)) + 
                geom_line(aes(group=state), color="lightgrey") + 
                geom_line(data=~filter(., state %in% stateHighlight), aes(group=state, color=state), lwd=1) +
                labs(x="", y="Cumulative (per million)", title=plotLab[thisMetric]) + 
                # scale_y_continuous(position="right") + 
                guides(color=FALSE)
            gridExtra::grid.arrange(p1, p2, nrow=1)
        } else {
            print(p1)
        }
    }
    
}

```

The function is run to highlight select states in the Great Lakes region:  
```{r}

subsetStatesCTP(ctp_hier6_210201, stateHighlight=c("WI", "IL", "IN", "OH", "MI"))

```

The function is run to highlight select states with large populations or disease burdens:  
```{r}

subsetStatesCTP(ctp_hier6_210201, stateHighlight=c("CA", "TX", "FL", "NY", "NJ", "MA"))

```

A function is written to attempt to align two curves (align B to A) as best possible, using only:  
  
1.  Cut-offs for the time period of interest for reference curve A  
2.  Potential lag/lead to be applied to curve B  
3.  A single scalar for curve B  
  
The attempt is to see how similar some of the outbreak curves are:  
```{r}

# Function that attempts to best align outbreak curves with a lag/lead and a scalar
alignOutbreaks <- function(dfA, 
                           dfB, 
                           minA=NULL, 
                           maxA=NULL, 
                           lagLeadTryB=-30:30, 
                           nameA="valueA", 
                           nameB="valueB", 
                           nameScaled=paste0("Scaled ", nameB), 
                           yAxisName="Metric", 
                           noPlotA=FALSE
                           ) {
    
    # FUNCTION ARGUMENTS:
    # dfA: the reference curve that will be kept constant (should have date-value)
    # dfB: the second curve where an attempt will be made to find the best alignment (should have date-value)
    # minA: the minimum date to be used for curve A (NULL means no forced minimum)
    # maxA: the maximum date to be used for curve A (NULL means no forced maximum)
    # lagLeadTryB: the vector of lags and leads to be attempted for curve B
    #              if a lag/lead requested would "run out" of curve B data, report and do not use
    # nameA: name to be used for series A
    # nameB: name to be used for series B
    # yAxisName: name to be used for y-axis
    # noPlotA: boolean, flags to not plot unscaled series A (useful if A and B are on very different scales)
    
    # Create the analysis frame for A
    useA <- dfA
    if (!is.null(minA)) useA <- useA %>% filter(date >= minA)
    if (!is.null(maxA)) useA <- useA %>% filter(date <= maxA)
    
    # Create a list for storing the results of each lagLeadTryB
    resultList <- vector("list", length(lagLeadTryB))
    
    # Run the process for each value of lagLeadTryB
    ctr <- 1
    for (lagLead in lagLeadTryB) {
        # Create the analysis data frame for B using only date-value and with the lag-lead applied
        useB <- dfB %>% select(date, value)
        if (lagLead < 0) useB <- useB %>% mutate(value=lag(value, -lagLead))
        if (lagLead > 0) useB <- useB %>% mutate(value=lead(value, lagLead))
        # Limit useB to the same time period as useA
        useAll <- select(useA, date, valueA=value) %>%
            left_join(select(useB, date, valueB=value), by="date")
        # Check for NA values and skip the regression if any
        nNA <- useAll %>% is.na() %>% sum()
        if (nNA>0) {
            cat("\nNA values exist, regression not run.  Value for laglead is:", lagLead)
            lmAB <- NA
            rsqAB <- NA
            rmseAB <- NA
            scalarAB <- NA
        } else {
            lmAB <- lm(valueB ~ valueA + 0, data=useAll)
            rsqAB <- summary(lmAB)$r.squared
            rmseAB <- summary(lmAB)$sigma
            scalarAB <- coef(lmAB)["valueA"]
        }
        # Create a vector for name mapping
        nameMap <- c("valueA"=nameA, "valueB"=nameB, "scaledB"=nameScaled)
        # Create the plot data
        p1Data <- useAll %>%
            mutate(scaledB=valueA*scalarAB) %>%
            pivot_longer(-date) %>%
            mutate(name=factor(nameMap[name], levels=c(nameA, nameB, nameScaled)))
        # Optionally, remove nameA
        if (isTRUE(noPlotA)) p1Data <- p1Data %>% filter(name!=nameA)
        # Plot the curves
        p1 <- p1Data %>% 
            ggplot(aes(x=date, y=value)) + 
            geom_line(aes(group=name, color=name)) + 
            labs(x="", 
                 y=yAxisName, 
                 title=paste0("Applying a lead/lag value of: ", lagLead), 
                 subtitle=paste0("Best scalar: ", 
                                 round(scalarAB, 3), 
                                 " drives RMSE: ", 
                                 round(rmseAB, 3), 
                                 " and R**2: ", 
                                 round(rsqAB, 3)
                                 )
                 ) + 
            scale_color_discrete("")
        # Store the data in the list
        resultList[[ctr]] <- list(useAll=useAll, 
                                  lagLead=lagLead, 
                                  p1=p1, 
                                  lmAB=lmAB, 
                                  rsqAB=rsqAB, 
                                  rmseAB=rmseAB, 
                                  scalarAB=scalarAB
                                  )
        # Increment the counter
        ctr <- ctr + 1
    }
    
    # Return the list object
    resultList
    
}

```

An example for the function is run using WI and MI, with WI as the reference state:  
```{r}

# Create a test data frame
testData <- ctp_hier6_210201$consolidatedPlotData %>%
    ungroup() %>%
    filter(state %in% c("WI", "MI"), name=="deaths", !is.na(vpm7)) %>%
    select(state, date, metric=name, value=vpm7)

testList <- alignOutbreaks(dfA=filter(testData, state=="WI"), 
                           dfB=filter(testData, state=="MI"), 
                           minA=as.Date("2020-07-01"), 
                           maxA=as.Date("2020-12-29"), 
                           lagLeadTryB=c(-30, -15, 0, 15, 30)
                           )

# Show the plots for lagLead of -15, 0, +15, +30
gridExtra::grid.arrange(testList[[2]]$p1, testList[[3]]$p1, testList[[4]]$p1, testList[[5]]$p1, nrow=2)

```

The curves clearly align better and worse at different levels of lag/lead.

The function can be run more granularly, using lag/lead from 0-30:  
```{r}

vecLagLead <- 0:30
testList <- alignOutbreaks(dfA=filter(testData, state=="WI"), 
                           dfB=filter(testData, state=="MI"), 
                           minA=as.Date("2020-07-01"), 
                           maxA=as.Date("2020-12-29"), 
                           lagLeadTryB=vecLagLead
                           )

rmseLagLead <- tibble::tibble(lagLead=vecLagLead, 
                              rmse=sapply(testList, "[[", "rmseAB"), 
                              rsq=sapply(testList, "[[", "rsqAB")
                              )

rmseLagLead %>%
    pivot_longer(-lagLead) %>%
    ggplot(aes(x=lagLead, y=value)) +
    geom_line(aes(group=name, color=name)) + 
    facet_wrap(~name, scales="free_y") + 
    scale_color_discrete("Metric") + 
    labs(x="Lag or lead", y="Model Performance", title="Model performance by metric and lag/lead")

bestLagLead <- rmseLagLead %>%
    filter(rmse==min(rmse)) %>%
    pull(lagLead)

gridExtra::grid.arrange(testList[[1]]$p1, 
                        testList[[which(vecLagLead==bestLagLead)]]$p1, 
                        testList[[length(testList)]]$p1, 
                        nrow=2
                        )

```

Next steps are to clean up the charts and labels and to apply to different combinations of data:  
```{r}

vecLagLead <- 0:30
testList <- alignOutbreaks(dfA=filter(testData, state=="WI"), 
                           dfB=filter(testData, state=="MI"), 
                           minA=as.Date("2020-07-01"), 
                           maxA=as.Date("2020-12-29"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="MI", 
                           nameB="Lag/Lead WI", 
                           nameScaled="Lag/Lead/Scale WI",
                           yAxisName="Deaths per million (7 day avg.)"
                           )

rmseLagLead <- tibble::tibble(lagLead=vecLagLead, 
                              rmse=sapply(testList, "[[", "rmseAB"), 
                              rsq=sapply(testList, "[[", "rsqAB")
                              )

rmseLagLead %>%
    pivot_longer(-lagLead) %>%
    ggplot(aes(x=lagLead, y=value)) +
    geom_line(aes(group=name, color=name)) + 
    facet_wrap(~name, scales="free_y") + 
    scale_color_discrete("Metric") + 
    labs(x="Lag or lead", y="Model Performance", title="Model performance by metric and lag/lead")

bestLagLead <- rmseLagLead %>%
    filter(rmse==min(rmse)) %>%
    pull(lagLead)

gridExtra::grid.arrange(testList[[1]]$p1, 
                        testList[[which(vecLagLead==bestLagLead)]]$p1, 
                        testList[[length(testList)]]$p1, 
                        nrow=2
                        )

```

The function is run for FL against itself (cases vs. deaths):  
```{r}

# Create a test data frame
testData <- ctp_hier6_210201$consolidatedPlotData %>%
    ungroup() %>%
    filter(!is.na(vpm7)) %>%
    select(state, date, metric=name, value=vpm7)

vecLagLead <- -30:0
testList <- alignOutbreaks(dfA=filter(testData, state=="FL", metric=="deaths"), 
                           dfB=filter(testData, state=="FL", metric=="cases"), 
                           minA=as.Date("2020-06-01"), 
                           maxA=as.Date("2020-12-01"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="FL deaths", 
                           nameB="FL cases", 
                           nameScaled="Lag/Lead/Scale",
                           yAxisName="Per million (7 day avg.)"
                           )

# Function for assessing list performance
assessLagLead <- function(lst, 
                          subTp1=NULL
                          ) {
    
    # FUNCTION ARGUMENTS:
    # lst: a processed list file
    # subTp1: subtitle to be used for plot 1 (NULL means no subtitle)
    
    # Pull out key elements from the processed list file
    rmseLagLead <- tibble::tibble(lagLead=sapply(lst, "[[", "lagLead"), 
                                  rmse=sapply(lst, "[[", "rmseAB"), 
                                  rsq=sapply(lst, "[[", "rsqAB"), 
                                  bestScalar=sapply(lst, "[[", "scalarAB")
                                  )

    # Plot for lag/lead data
    p1 <- rmseLagLead %>%
        pivot_longer(-lagLead) %>%
        ggplot(aes(x=lagLead, y=value)) +
        geom_line(aes(group=name, color=name)) + 
        facet_wrap(~name, scales="free_y") + 
        scale_color_discrete("Metric") + 
        labs(x="Lag or lead", y="Model Performance", title="Model performance by metric and lag/lead")
    if (!is.null(subTp1)) p1 <- p1 + labs(subtitle=subTp1)
    print(p1)

    # Find the best lag/lead value
    bestLagLead <- rmseLagLead %>%
        filter(rmse==min(rmse)) %>%
        pull(lagLead)
    
    # Show a plot of the best lag lead, as well as the smallest and the largest
    gridExtra::grid.arrange(lst[[1]]$p1, 
                            lst[[which(rmseLagLead$lagLead==bestLagLead)]]$p1, 
                            lst[[length(lst)]]$p1, 
                            nrow=2
                            )
    
}


# Run for testList containing FL data
assessLagLead(testList, subTp1="FL deaths as function of FL cases\n(negative lag/lead means cases first)")

```
  
Deaths in FL appear to follow reported cases at a ratio of roughly 1:50 with a lag of ~20 days.

The process is run for the curves in MI and ND, which are on different seasonality:  
```{r}

vecLagLead <- seq(-90, 0, by=5)
testList <- alignOutbreaks(dfA=filter(testData, state=="MI", metric=="deaths"), 
                           dfB=filter(testData, state=="ND", metric=="deaths"), 
                           minA=as.Date("2020-06-30"), 
                           maxA=as.Date("2020-12-28"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="MI deaths", 
                           nameB="ND deaths", 
                           nameScaled="Lag/Lead/Scale",
                           yAxisName="Deaths per million (7 day avg.)"
                           )

# Run for testList containing FL/ND data
assessLagLead(testList, subTp1="ND deaths as function of MI deaths\n(negative lag/lead means ND first)")

```

The MI and WI data are plotted, to allow for the most interesting part of the curve to be pulled:  
```{r}

testData %>%
  filter(state %in% c("WI", "MI"), metric=="deaths") %>%
  ggplot(aes(x=date, y=value)) + 
  geom_line(aes(group=state, color=state)) +
  facet_wrap(~state)
  
```

The time frame of mid-September to late December is especially interesting in WI:  
```{r}

vecLagLead <- seq(-30, 30, by=5)
testList <- alignOutbreaks(dfA=filter(testData, state=="WI", metric=="deaths"), 
                           dfB=filter(testData, state=="MI", metric=="deaths"), 
                           minA=as.Date("2020-08-15"), 
                           maxA=as.Date("2020-12-29"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="WI deaths", 
                           nameB="MI deaths", 
                           nameScaled="Lag/Lead/Scale",
                           yAxisName="Deaths per million (7 day avg.)"
                           )

# Run for testList containing MI/WI data
assessLagLead(testList, subTp1="MI deaths as function of WI deaths\n(negative lag/lead means MI first)")

```

The function is updated to allow for optional skipping of the plot of 'nameA', in the case of it being on a very different scale (e.g., deaths vs. cases).  The data are then run for WI deaths and WI cases:  
```{r}

vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="WI", metric=="cases"), 
                           dfB=filter(testData, state=="WI", metric=="deaths"), 
                           minA=as.Date("2020-08-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="WI cases", 
                           nameB="WI deaths", 
                           nameScaled="Lag/Lead/Scale WI cases",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE
                           )

# Run for testList containing MI/WI data
assessLagLead(testList, subTp1="WI deaths as function of WI cases\n(positive lag/lead means cases first)")

```

Deaths in Wisconsin have been running at about 1% of confirmed cases with about a 16-day lag.

The same process is run for the FL data:  
```{r}

vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="FL", metric=="cases"), 
                           dfB=filter(testData, state=="FL", metric=="deaths"), 
                           minA=as.Date("2020-04-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="FL cases", 
                           nameB="FL deaths with lag/lead", 
                           nameScaled="Scaled FL cases",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE
                           )

# Run for testList containing FL data
assessLagLead(testList, subTp1="FL deaths as function of FL cases\n(positive lag/lead means cases first)")

```

Deaths in Florida have been running at about 1.6% of confirmed cases with about a 24-day lag.

The same process is run for the OH data, excluding the first wave of cases through Q2:  
```{r}

vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="OH", metric=="cases"), 
                           dfB=filter(testData, state=="OH", metric=="deaths"), 
                           minA=as.Date("2020-06-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="OH cases", 
                           nameB="OH deaths with lag/lead", 
                           nameScaled="Scaled OH cases",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE
                           )

# Run for testList containing OH data
assessLagLead(testList, subTp1="OH deaths as function of OH cases\n(positive lag/lead means cases first)")

```

Deaths in Ohio have been running at about 0.8% of confirmed cases with about a 10-day lag.

The same process is run IA:  
```{r}

vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="IA", metric=="cases"), 
                           dfB=filter(testData, state=="IA", metric=="deaths"), 
                           minA=as.Date("2020-06-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="IA cases", 
                           nameB="IA deaths with lag/lead", 
                           nameScaled="Scaled IA cases",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE
                           )

# Run for testList containing IA data
assessLagLead(testList, subTp1="IA deaths as function of IA cases\n(positive lag/lead means cases first)")

```

Iowa has a very clear pandemic spike, with deaths at 1.8% of reported cases on about a 28-day lag.

The same process is run for IL:  
```{r}

vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="IL", metric=="cases"), 
                           dfB=filter(testData, state=="IL", metric=="deaths"), 
                           minA=as.Date("2020-06-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="IL cases", 
                           nameB="IL deaths with lag/lead", 
                           nameScaled="Scaled IL cases",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE
                           )

# Run for testList containing IL data
assessLagLead(testList, subTp1="IL deaths as function of IL cases\n(positive lag/lead means cases first)")

```

Illinois also has a clear pandemic spike, with deaths at 1.5% of reported cases on about a 22-day lag.

The process is run for IL using total hospitalized rather than cases as the leading variable:  
```{r}

vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="IL", metric=="hosp"), 
                           dfB=filter(testData, state=="IL", metric=="deaths"), 
                           minA=as.Date("2020-06-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="IL total hospitalized", 
                           nameB="IL deaths with lag/lead", 
                           nameScaled="Scaled IL hospitalized",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE
                           )

# Run for testList containing IL data
assessLagLead(testList, subTp1="IL deaths as function of IL hospitalized\n(positive lag/lead means hospitalized first)")

```

In general, the death rate in IL is best predicted as 2.6% of the total hospitalized, with a 12-day lag.  This relationship appears to have a (currently not modeled) y-axis offset, as even in the period with very low deaths per million, there are still a bolus of hospitalized patients.  So, death appears more accuratelt to be modeled as death = a * (lag(hospitalized, 12) - b). This could be captured by allowing lm(y~x, ...) rather than the current lm(y~x+0, ...).

The capability is added for regression to include an intercept:  
```{r}

# Function that attempts to best align outbreak curves with a lag/lead and a scalar
alignOutbreaks <- function(dfA, 
                           dfB, 
                           minA=NULL, 
                           maxA=NULL, 
                           lagLeadTryB=-30:30, 
                           nameA="valueA", 
                           nameB="valueB", 
                           nameScaled=paste0("Scaled ", nameB), 
                           yAxisName="Metric", 
                           noPlotA=FALSE, 
                           useIntercept=FALSE
                           ) {
    
    # FUNCTION ARGUMENTS:
    # dfA: the reference curve that will be kept constant (should have date-value)
    # dfB: the second curve where an attempt will be made to find the best alignment (should have date-value)
    # minA: the minimum date to be used for curve A (NULL means no forced minimum)
    # maxA: the maximum date to be used for curve A (NULL means no forced maximum)
    # lagLeadTryB: the vector of lags and leads to be attempted for curve B
    #              if a lag/lead requested would "run out" of curve B data, report and do not use
    # nameA: name to be used for series A
    # nameB: name to be used for series B
    # yAxisName: name to be used for y-axis
    # noPlotA: boolean, flags to not plot unscaled series A (useful if A and B are on very different scales)
    # useIntercept: boolean, should the regression include an intercept?
    
    # Create the analysis frame for A
    useA <- dfA
    if (!is.null(minA)) useA <- useA %>% filter(date >= minA)
    if (!is.null(maxA)) useA <- useA %>% filter(date <= maxA)
    
    # Create a list for storing the results of each lagLeadTryB
    resultList <- vector("list", length(lagLeadTryB))
    
    # Run the process for each value of lagLeadTryB
    ctr <- 1
    for (lagLead in lagLeadTryB) {
        # Create the analysis data frame for B using only date-value and with the lag-lead applied
        useB <- dfB %>% select(date, value)
        if (lagLead < 0) useB <- useB %>% mutate(value=lag(value, -lagLead))
        if (lagLead > 0) useB <- useB %>% mutate(value=lead(value, lagLead))
        # Limit useB to the same time period as useA
        useAll <- select(useA, date, valueA=value) %>%
            left_join(select(useB, date, valueB=value), by="date")
        # Check for NA values and skip the regression if any
        nNA <- useAll %>% is.na() %>% sum()
        if (nNA>0) {
            cat("\nNA values exist, regression not run.  Value for laglead is:", lagLead)
            lmAB <- NA
            rsqAB <- NA
            rmseAB <- NA
            scalarAB <- NA
        } else {
            if (isTRUE(useIntercept)) {
                lmAB <- lm(valueB ~ valueA, data=useAll)
                interceptAB <- coef(lmAB)["(Intercept)"]
            }
            else {
                lmAB <- lm(valueB ~ valueA + 0, data=useAll)
                interceptAB <- 0
            }
            rsqAB <- summary(lmAB)$r.squared
            rmseAB <- summary(lmAB)$sigma
            scalarAB <- coef(lmAB)["valueA"]
        }
        # Create a vector for name mapping
        nameMap <- c("valueA"=nameA, "valueB"=nameB, "scaledB"=nameScaled)
        # Create the plot data
        p1Data <- useAll %>%
            mutate(scaledB=valueA*scalarAB + interceptAB) %>%
            pivot_longer(-date) %>%
            mutate(name=factor(nameMap[name], levels=c(nameA, nameB, nameScaled)))
        # Optionally, remove nameA
        if (isTRUE(noPlotA)) p1Data <- p1Data %>% filter(name!=nameA)
        # Plot the curves
        p1 <- p1Data %>% 
            ggplot(aes(x=date, y=value)) + 
            geom_line(aes(group=name, color=name)) + 
            labs(x="", 
                 y=yAxisName, 
                 title=paste0("Applying a lead/lag value of: ", lagLead), 
                 subtitle=paste0("Best scalar: ", 
                                 round(scalarAB, 3), 
                                 " with intercept: ", 
                                 round(interceptAB, 3),
                                 "\ndrives RMSE: ", 
                                 round(rmseAB, 3), 
                                 " and R**2: ", 
                                 round(rsqAB, 3)
                                 )
                 ) + 
            scale_color_discrete("")
        # Store the data in the list
        resultList[[ctr]] <- list(useAll=useAll, 
                                  lagLead=lagLead, 
                                  p1=p1, 
                                  lmAB=lmAB, 
                                  rsqAB=rsqAB, 
                                  rmseAB=rmseAB, 
                                  scalarAB=scalarAB, 
                                  interceptAB=interceptAB
                                  )
        # Increment the counter
        ctr <- ctr + 1
    }
    
    # Return the list object
    resultList
    
}


# Function for assessing list performance (updated to allow for intercept)
assessLagLead <- function(lst, 
                          subTp1=NULL, 
                          checkIntercept=FALSE
                          ) {
    
    # FUNCTION ARGUMENTS:
    # lst: a processed list file
    # subTp1: subtitle to be used for plot 1 (NULL means no subtitle)
    # checkIntercept: boolean, should lst be checked for an intercept value?
    
    # Pull out key elements from the processed list file
    rmseLagLead <- tibble::tibble(lagLead=sapply(lst, "[[", "lagLead"), 
                                  rmse=sapply(lst, "[[", "rmseAB"), 
                                  rsq=sapply(lst, "[[", "rsqAB"), 
                                  bestScalar=sapply(lst, "[[", "scalarAB"), 
                                  bestIntercept=if(checkIntercept) sapply(lst, "[[", "interceptAB") else 0
                                  )

    # Is the intercept of interest for the plot?
    if (sum(rmseLagLead$bestIntercept != 0) == 0) noInt <- TRUE else noInt <- FALSE
    
    # Plot for lag/lead data
    plotData <- rmseLagLead %>%
        pivot_longer(-lagLead)
    if (noInt) plotData <- plotData %>% filter(name != "bestIntercept")
    p1 <- plotData %>%
        ggplot(aes(x=lagLead, y=value)) +
        geom_line(aes(group=name, color=name)) + 
        facet_wrap(~name, scales="free_y", nrow=2-as.integer(noInt)) + 
        scale_color_discrete("Metric") + 
        labs(x="Lag or lead", y="Model Performance", title="Model performance by metric and lag/lead")
    if (!is.null(subTp1)) p1 <- p1 + labs(subtitle=subTp1)
    print(p1)

    # Find the best lag/lead value
    bestLagLead <- rmseLagLead %>%
        filter(rmse==min(rmse)) %>%
        pull(lagLead)
    
    # Show a plot of the best lag lead, as well as the smallest and the largest
    gridExtra::grid.arrange(lst[[1]]$p1, 
                            lst[[which(rmseLagLead$lagLead==bestLagLead)]]$p1, 
                            lst[[length(lst)]]$p1, 
                            nrow=2
                            )
    
}

```
  
The new function is tested, first using data "as is" and second using data modeled with an intercept:  
```{r}

# RUNNING AS-IS
vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="IL", metric=="hosp"), 
                           dfB=filter(testData, state=="IL", metric=="deaths"), 
                           minA=as.Date("2020-06-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="IL total hospitalized", 
                           nameB="IL deaths with lag/lead", 
                           nameScaled="Scaled IL hospitalized",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE
                           )

# Run for testList containing IL data
assessLagLead(testList, subTp1="IL deaths as function of IL hospitalized\n(positive lag/lead means hospitalized first)")


# RUNNING WITH INTERCEPT
vecLagLead <- seq(-10, 36, by=2)
testList <- alignOutbreaks(dfA=filter(testData, state=="IL", metric=="hosp"), 
                           dfB=filter(testData, state=="IL", metric=="deaths"), 
                           minA=as.Date("2020-06-15"), 
                           maxA=as.Date("2020-12-20"), 
                           lagLeadTryB=vecLagLead, 
                           nameA="IL total hospitalized", 
                           nameB="IL deaths with lag/lead", 
                           nameScaled="Scaled IL hosp\nwith intercept",
                           yAxisName="Per million (7 day avg.)", 
                           noPlotA=TRUE, 
                           useIntercept=TRUE
                           )

# Run for testList containing IL data
assessLagLead(testList, 
              subTp1="IL deaths vs. IL hospitalized\n(positive lag/lead means hospitalized first)", 
              checkIntercept=TRUE
              )

```

Adding an intercept creates an almost perfect alignment between the Illinois total hospitalized curve and the Illinois death curve, with a best lag of 12 days.

A capability is added to create population-weighted summary statistics for a grouping of states:  
```{r}

weightByPop <- function(df, 
                        popData=rename(statePop2019, state=stateAbb, pop=pop_2019), 
                        inclStates=NULL, 
                        stateName="Aggregate"
                        ) {
    
    # FUNCTION ARGUMENTS:
    # df: a file with state-date-metric-value
    # popData: a file containing at least state-pop
    # inclStates: states to be included (NULL means all states that are in both df and pop)
    # stateName: value to be used for the new state name
    
    # Keep all states that are in both df and popData if inclStates is NULL
    if (is.null(inclStates)) {
        inclStates <- popData %>%
            semi_join(df, by="state") %>%
            pull(state)
    }
    
    # Merge in the population data, keep only states in inclStates, aggregate, and return
    df %>%
        filter(state %in% all_of(inclStates)) %>%
        inner_join(select(popData, state, pop), by="state") %>%
        group_by(date, metric) %>%
        summarize(value=sum(pop*value)/sum(pop), .groups="drop") %>%
        mutate(state=stateName)
    
}

```
  
An example is run for the defaults:  
```{r}

dfAgg <- weightByPop(testData)
dfAgg

dfAgg %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="US burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
         ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# National cases vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAgg, state=="Aggregate", metric=="cases"), 
                              dfB=filter(dfAgg, state=="Aggregate", metric=="deaths"), 
                              minA=as.Date("2020-06-15"), 
                              maxA=as.Date("2020-12-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="US cases", 
                              nameB="US deaths with lag/lead", 
                              nameScaled="Scaled US cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
                              )

# Run for testList containing IL data
assessLagLead(testListAgg, 
              subTp1="US deaths as function of US cases\n(positive lag/lead means cases first)"
              )

```

Across the US, deaths tend to run at 1.5% of confirmed cases with around a 26-day lag.

The similar process is run for hospitalizations vs. deaths:  
```{r}

# National cases vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAgg, state=="Aggregate", metric=="hosp"), 
                              dfB=filter(dfAgg, state=="Aggregate", metric=="deaths"), 
                              minA=as.Date("2020-06-15"), 
                              maxA=as.Date("2020-12-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="US total hospitalization", 
                              nameB="US deaths with lag/lead", 
                              nameScaled="Scaled US total hospitalization\nwith intercept",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE, 
                              useIntercept=TRUE
                              )

# Run for testList containing all-US data
assessLagLead(testListAgg, 
              subTp1="US deaths as function of total US hospitalizations\n(positive lag/lead means hospitalization first)", 
              checkIntercept=TRUE
              )

```

Across the US, deaths tend to be ~2.3% of total hospitalizations with around an 8-day lag and a small negative intercept.

The data are run for the main cluster of midwestern states, using cases and deaths:  
```{r}

mwStates <- ctp_hier6_210201$useClusters[ctp_hier6_210201$useClusters==ctp_hier6_210201$useClusters["IL"]]

dfAggMW <- weightByPop(testData, inclStates=names(mwStates), stateName=paste0("Cluster ", mwStates[1]))
dfAggMW

dfAggMW %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
         ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# Cluster cases vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggMW, metric=="cases"), 
                              dfB=filter(dfAggMW, metric=="deaths"), 
                              minA=as.Date("2020-06-15"), 
                              maxA=as.Date("2020-12-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
                              )

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
              )

```
  
Deaths in the predominantly midwestern cluster tend to run at ~1.4% of diagnosed cases with a lag of around 22 days.  This is similar to the findings for the full US.

The data are run for the main cluster of southern states, using cases and deaths:  
```{r}

soStates <- ctp_hier6_210201$useClusters[ctp_hier6_210201$useClusters==ctp_hier6_210201$useClusters["TX"]]

dfAggSo <- weightByPop(testData, inclStates=names(soStates), stateName=paste0("Cluster ", soStates[1]))
dfAggSo

dfAggSo %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
         ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# Cluster cases vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggSo, metric=="cases"), 
                              dfB=filter(dfAggSo, metric=="deaths"), 
                              minA=as.Date("2020-06-15"), 
                              maxA=as.Date("2020-12-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
                              )

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
              )

```
  
Deaths in the predominantly southern cluster tend to run at ~1.8% of diagnosed cases with a lag of around 28 days.  This is similar to the findings for the full US.

The midwest and southern clusters are run against each other, using deaths as the core metric.  To get only the second outbreak for both, timing is moved to mid-September and an intercept is added:  
```{r}

# Cluster cases vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggMW, metric=="deaths"), 
                              dfB=filter(dfAggSo, metric=="deaths"), 
                              minA=as.Date("2020-09-15"), 
                              maxA=as.Date("2020-12-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Midwest cluster deaths", 
                              nameB="Southern cluster deaths with lag/lead", 
                              nameScaled="Midwest scaled cluster deaths",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE, 
                              useIntercept=TRUE
                              )

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Southern cluster deaths as function of Midwest cluster deaths\n(positive lag/lead means Midwest first)", 
              checkIntercept=TRUE
              )

```

There are some meaningful differences in the "second outbreak" curves, though generally the southern second outbreak slightly lags the midwest second outbreak.

Deaths vs. cases are explored for the high impact northeastern cluster, focused on the second wave:  
```{r}

hiStates <- ctp_hier6_210201$useClusters[ctp_hier6_210201$useClusters==ctp_hier6_210201$useClusters["NY"]]

dfAggHi <- weightByPop(testData, inclStates=names(hiStates), stateName=paste0("Cluster ", hiStates[1]))
dfAggHi

dfAggHi %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
         ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# Cluster cases vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggHi, metric=="cases"), 
                              dfB=filter(dfAggHi, metric=="deaths"), 
                              minA=as.Date("2020-06-15"), 
                              maxA=as.Date("2020-12-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
                              )

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
              )

```

This cluster has deaths at about 1.7% of confirmed cases with roughly a 26 day lag, broadly consistent with the national averages.

Deaths vs. cases are explored for the high impact northeastern cluster, focused on the first wave:  
```{r}

# Cluster cases vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggHi, metric=="cases"), 
                              dfB=filter(dfAggHi, metric=="deaths"), 
                              minA=as.Date("2020-03-01"), 
                              maxA=as.Date("2020-07-15"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
                              )

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
              )

```

In the first wave of the outbreak, this cluster has deaths at about 7.4% of confirmed cases with roughly a 6 day lag.  This is a much higher CFR on a much shorter lag than is observed in later waves of the pandemic.

Deaths vs. hospitalizations are explored for the high impact northeastern cluster, focused on the first wave:  
```{r}

# Cluster hospitalizations vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggHi, metric=="hosp"), 
                              dfB=filter(dfAggHi, metric=="deaths"), 
                              minA=as.Date("2020-03-01"), 
                              maxA=as.Date("2020-07-15"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster total hospitalizaed", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster\ntotal hospitalized",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
                              )

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster total hospitalized\n(positive lag/lead means hospitalized first)"
              )

```

In the first wave of the outbreak, this cluster has deaths at about 3.6% of total hospitalized with roughly no lag.  This is perhaps reflective of cases being diagnosed very late in the disease progression, as hospitalizations would typically be expected to peak prior to deaths for a disease where many hospitalized patients recover.

Deaths vs. hospitalizations are then explored for the second wave of the high impact northeastern cluster:  
```{r}

# Cluster hospitalizations vs. deaths
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggHi, metric=="hosp"), 
                              dfB=filter(dfAggHi, metric=="deaths"), 
                              minA=as.Date("2020-06-15"), 
                              maxA=as.Date("2020-12-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster total hospitalizaed", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster\ntotal hospitalized",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
                              )

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster total hospitalized\n(positive lag/lead means hospitalized first)"
              )

```

In the second wave of the outbreak, this cluster has deaths at about 2.2% of total hospitalized with roughly a 6-day lag.  This is perhaps reflective of cases being diagnosed earlier in the later wave.

Updated data through February 28, 2021 are downloaded, with existing segments applied:  
```{r cache=TRUE}

# Create new segments with updated data
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_210301.csv"
locLog <- "./RInputFiles/Coronavirus/downloaded_20210301.log"
ctp_hier6_210301 <- readRunCOVIDTrackingProject(thruLabel="Feb 28, 2021", 
                                                downloadTo=if(file.exists(locDownload)) NULL else locDownload,
                                                readFrom=locDownload, 
                                                compareFile=readFromRDS("ctp_hier6_210201")$dfRaw,
                                                dateChangePlot=TRUE,
                                                dateMetricPrint=FALSE,
                                                writeLog=locLog,
                                                useClusters=ctp_hier6_210201$useClusters
                                                )
saveToRDS(ctp_hier6_210301, ovrWriteError=FALSE)

```

Key files are updated so that lag-lead can be run using the new data:  
```{r}

# Create a data frame from the core burden data
testData_210301 <- ctp_hier6_210301$consolidatedPlotData %>%
    ungroup() %>%
    filter(!is.na(vpm7)) %>%
    select(state, date, metric=name, value=vpm7)

# Create the list of high-impact northeastern states (duplicated from previous)
hiStates <- ctp_hier6_210301$useClusters[ctp_hier6_210301$useClusters==ctp_hier6_210301$useClusters["NY"]]

# Create an aggregate from the most recent data
dfAggHi_210301 <- weightByPop(testData_210301, 
                              inclStates=names(hiStates), 
                              stateName=paste0("Cluster ", hiStates[1])
                              )
dfAggHi_210301

# Plot the burden data
dfAggHi_210301 %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
    ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggHi_210301, metric=="cases"), 
                              dfB=filter(dfAggHi_210301, metric=="deaths"), 
                              minA=as.Date("2020-07-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
)

```

With the most recent data, the second wave for this cluster has CFR at 1.2% on roughly an 8-day lag.  This is a significantly shorter lag than found previously, which merits further exploration.  It may be in part driven by cases declining then rising around the time of the year-end holiday, while deaths follow a smoother trend.  This would tend to change the best lag and CFR for aligning the curves, particularly the peaks.

The same process is run for the full national data:  
```{r}

# Create an aggregate from the most recent data
dfAgg_210301 <- weightByPop(testData_210301, stateName="US")
dfAgg_210301

# Plot the burden data
dfAgg_210301 %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
    ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAgg_210301, metric=="cases"), 
                              dfB=filter(dfAgg_210301, metric=="deaths"), 
                              minA=as.Date("2020-09-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
)

```

Possibly owing in part to major holiday timing, there are dips and spikes in cases that are inconsistent with a smooth pandemic curve.  Nationally, the best fit to the data is obtained with a 1.5% CFR on a 24-day lag, though the spikiness and trends raise greater uncertainty as to these estimates.

The hospitalizations vs deaths analysis is run for the full national data:  
```{r}

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAgg_210301, metric=="hosp"), 
                              dfB=filter(dfAgg_210301, metric=="deaths"), 
                              minA=as.Date("2020-08-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster total hospitalized", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster total hospitalized",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster total hospitalized\n(positive lag/lead means hospital first)"
)

```

The best fit is that deaths run at 2.4% of total hospitalized with a 10-day lag.

The cases vs. hospitalizations analysis is run for the full national data:  
```{r}

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAgg_210301, metric=="cases"), 
                              dfB=filter(dfAgg_210301, metric=="hosp"), 
                              minA=as.Date("2020-08-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster total hospitalized with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster total hospitalized as function of cluster cases\n(positive lag/lead means cases first)"
)

```

The best fit is that total hospitalized runs at 57% of cases with a 6-day lag.

The analysis is then run for the cluster focused on midwestern states, using cases and deaths:  
```{r}

# Create the list of high-impact northeastern states (duplicated from previous)
mwStates <- ctp_hier6_210301$useClusters[ctp_hier6_210301$useClusters==ctp_hier6_210301$useClusters["IL"]]

# Create an aggregate from the most recent data
dfAggMW_210301 <- weightByPop(testData_210301, 
                              inclStates=names(mwStates), 
                              stateName=paste0("Cluster ", mwStates[1])
                              )
dfAggMW_210301

# Plot the burden data
dfAggMW_210301 %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
    ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggMW_210301, metric=="cases"), 
                              dfB=filter(dfAggMW_210301, metric=="deaths"), 
                              minA=as.Date("2020-07-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
)

```

There appears to be some significant spikiness to the deaths data, but best alignment has deaths at 1.4% of confirmed cases, with a 20-day lag.

Some of the spikiness is driven by reporting issues in states like Ohio and Indiana.  There are large one-day spikes in deaths in each state driven by previous undercounting due to issues in the death certificate matching process.  As such, deaths are reported at times meaningfully different than when they occurred.  Examples for 7-day rolling mean deaths per million by state in the primary midwestern cluster:  
```{r}

testData_210301 %>% 
    filter(metric=="deaths", state %in% names(mwStates)) %>% 
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=state, color=state)) + 
    facet_wrap(~state)

```

The process can then be re-run excluding the Ohio and Indiana data which appear to be spurious:  
```{r}

# Create an aggregate from the most recent data
dfAggMW_210301_2 <- weightByPop(testData_210301, 
                                inclStates=names(mwStates)[!(names(mwStates) %in% c("OH", "IN"))], 
                                stateName=paste0("Cluster ", mwStates[1])
                                )
dfAggMW_210301_2

# Plot the burden data
dfAggMW_210301_2 %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
         ) + 
    facet_wrap(~metric, scales="free_y") + 
    guides(color=FALSE)

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggMW_210301_2, metric=="cases"), 
                              dfB=filter(dfAggMW_210301_2, metric=="deaths"), 
                              minA=as.Date("2020-07-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster cases\n(positive lag/lead means cases first)"
              )

```

The best alignment is now a 1.5% CFR with a 24-day lag, very close to the 1.4% CFR on a 20-day lag observed previously with Ohio and Indiana included.  RMSE lowers from ~1.2 to ~0.9 and the curves are visually more closely aligned.  It is encouraging that best-fit CFR and lag are reasonably similar even when some spurious data are included.

A check is made for any other states with significant spikes in deaths data:  
```{r}

spikyStates <- testData_210301 %>% 
    filter(metric=="deaths", state != "cluster") %>% 
    group_by(state) %>%
    mutate(spiky=abs(value-lag(value, 7))>15) %>%
    filter(spiky) %>%
    pull(state) %>%
    unique()

testData_210301 %>% 
    filter(metric=="deaths", state %in% spikyStates) %>% 
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=state, color=state)) + 
    facet_wrap(~state)

```

* The spikes in CT, NJ, NY, and SD appear to be true, continuous pandemic spikes  
* The spikes in AL, IA appear to be a combination of pandemic spikes and generally noisy data  
* The spikes in IN and OH are documented publicly as corrections to account for previous under-counting  
* The spike in VA is more recent, and its cause is not evident from the data  
  
Hospitalizations vs. deaths is run for the primary midwestern cluster, again excluding OH and IN:  
```{r}

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggMW_210301_2, metric=="hosp"), 
                              dfB=filter(dfAggMW_210301_2, metric=="deaths"), 
                              minA=as.Date("2020-07-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster total hospitalized", 
                              nameB="Cluster deaths with lag/lead", 
                              nameScaled="Scaled cluster total hospitalized",
                              yAxisName="Per million (7 day avg.)", 
                              useIntercept=TRUE,
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster deaths as function of cluster total hospitalized\n(positive lag/lead means hospital first)"
              )

```

Deaths tend to change proportional to 3.3% of change in total hospitalized, on a 12-day lag.

Cases vs. hospitalizations is run for the primary midwestern cluster, again excluding OH and IN:  
```{r}

# Cluster cases vs. deaths for second wave
vecLagLead <- seq(-10, 36, by=2)
testListAgg <- alignOutbreaks(dfA=filter(dfAggMW_210301_2, metric=="cases"), 
                              dfB=filter(dfAggMW_210301_2, metric=="hosp"), 
                              minA=as.Date("2020-07-15"), 
                              maxA=as.Date("2021-01-20"), 
                              lagLeadTryB=vecLagLead, 
                              nameA="Cluster cases", 
                              nameB="Cluster total hospitalized with lag/lead", 
                              nameScaled="Scaled cluster cases",
                              yAxisName="Per million (7 day avg.)", 
                              useIntercept=TRUE,
                              noPlotA=TRUE
)

# Run for testList containing Cluster data
assessLagLead(testListAgg, 
              subTp1="Cluster total hospitalized as function of cluster cases\n(positive lag/lead means cases first)"
              )

```

Total hospitalized tends to change proportional to 44.4% of change in cases, on a 10-day lag.

There is good consilience among these estimates:  
  
* Deaths move proportional to 3.3% of movements in total hospitalized on a 12-day lag  
* Total hospitalized move proportional to 44.4% of change in cases on a 10-day lag  
* In combination, deaths would be estimated to move proportional to 1.46% (3.3% x 44.4%) of change in cases on a 22-day (12-day plus 10-day) lag  
* Modeled, deaths are estimated to move proportional to 1.5% of change in cases on a 24-day lag  
  
Suppose that the case data is taken for July 15 to most recent, what is the projected number hospitalized and dead?  
```{r}

# Cases data from July 15, 2020
dfCases <- dfAggMW_210301_2 %>%
    filter(date >= as.Date("2020-07-15"), metric=="cases")

# Deaths data from July 15, 2020
dfDeaths <- dfAggMW_210301_2 %>%
    filter(date >= as.Date("2020-07-15"), metric=="deaths")

# Create an estimate for hospitalized (44.4% of cases plus 34.6 on a 10-day lag)
dfHosp <- dfCases %>%
    mutate(metric="hosp", value=0.444*value + 34.6, date=date+10)

# Create an estimate for deaths (3.3% of hospitalized minus 0.997 on a 12-day lag)
dfDeaths1 <- dfHosp %>%
    mutate(metric="deaths1", value=0.033*value - 0.997, date=date+12)

# Create the alternate estimate for deaths (1.5% of cases on a 24-day lag)
dfDeaths2 <- dfCases %>%
    mutate(metric="deaths2", value=0.015*value, date=date+24)

# Create the consolidated database
dfAll <- bind_rows(dfCases, dfDeaths, dfHosp, dfDeaths1, dfDeaths2)

# Plot the estimates and actuals for deaths
dfAll %>%
    filter(metric %in% c("deaths", "deaths1", "deaths2")) %>%
    mutate(metric=factor(metric, 
                         levels=c("deaths", "deaths1", "deaths2"), 
                         labels=c("Actual deaths", "Indirect estimate", "Direct estimate")
                         )
           ) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=metric, color=metric)) + 
    labs(x="", y="Deaths per million per day", title="Actual and estimated deaths") + 
    scale_color_discrete("")

```

Both estimates are reasonably close to the actual data, and provide roughly a 3-week look-ahead estimate for progression of the disease.

Consolidated data are created for each of the segments:  
```{r}

# Create a list for storing processed data by cluster
vecClusters <- unique(ctp_hier6_210301$useClusters)

# Populate a list with data for each of the clusters
dfAggList <- lapply(vecClusters, 
                    FUN=function(x) {
                        keyStates <- names(ctp_hier6_210301$useClusters)[ctp_hier6_210301$useClusters==x]
                        weightByPop(testData_210301, 
                                    inclStates=keyStates[!(keyStates %in% c("OH", "IN"))], 
                                    stateName=paste0("cluster_", x)
                                    )
                    }
                    )
names(dfAggList) <- paste0("cluster_", vecClusters)

# Plot the burden data
dfAggList %>%
    bind_rows() %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=state, group=state)) + 
    labs(x="", 
         y="Per million per day (7-day mean)", 
         title="Cluster-level burden", 
         subtitle="Each day includes only states that reported that day (numerator and denominator)"
         ) + 
    facet_wrap(~metric, scales="free_y")

```

The alignOutbreaks() function is then run for each cluster, using deaths vs. cases:  
```{r}

# Cluster cases vs. deaths for full list
vecLagLead <- seq(-10, 36, by=2)
allListAgg <- lapply(dfAggList, 
                     FUN=function(x) {
                         alignOutbreaks(dfA=filter(x, metric=="cases"), 
                                        dfB=filter(x, metric=="deaths"), 
                                        minA=as.Date("2020-07-15"), 
                                        maxA=as.Date("2021-01-20"), 
                                        lagLeadTryB=vecLagLead, 
                                        nameA="Cluster cases", 
                                        nameB="Cluster deaths with lag/lead", 
                                        nameScaled="Scaled cluster cases",
                                        yAxisName="Per million (7 day avg.)", 
                                        useIntercept=FALSE,
                                        noPlotA=TRUE
                                        )
                         }
                     )

```

The RMSE and R-squared can then be found for each cluster at each value of lag:  
```{r}

allRMSER2 <- lapply(allListAgg, 
                    FUN=function(x) {
                        sapply(x, FUN=function(y) c(lagLead=y$lagLead, 
                                                    rsq=y$rsqAB, 
                                                    rmse=y$rmseAB, 
                                                    scalar=unname(y$scalarAB), 
                                                    intercept=y$interceptAB
                                                    )
                               )  %>%
                            t() %>%
                            tibble::as_tibble()
                        }
                    ) %>%
    bind_rows(.id="cluster")

allRMSER2 %>%
    pivot_longer(-c("cluster", "lagLead")) %>%
    ggplot(aes(x=lagLead, y=value)) + 
    geom_line(aes(group=cluster, color=cluster)) + 
    labs(x="Lag/lead (positive means cases first)", y="", title="Metrics for deaths vs. cases by cluster") + 
    facet_wrap(~name, scales="free_y")

```

For each cluster, the lag/lead with the lowest RMSE and/or highest R-squared is pulled:  
```{r}

allRMSER2 %>%
    group_by(cluster) %>%
    filter(rsq==max(rsq) | rmse==min(rmse)) %>%
    ungroup()

```

As expected, the highest R-squared and the lowest RMSE tend to occur at the same lag/lead.  There is a small difference for cluster 4, and highest R-squared is chosen as the best curve-fitting metric:  
```{r}

# Capture the best lag/lead by cluster
bestLagLead <- allRMSER2 %>%
    group_by(cluster) %>%
    filter(rsq==max(rsq)) %>%
    ungroup()
bestLagLead

bestLagLead %>%
    pivot_longer(-c("cluster")) %>%
    filter(name %in% c("lagLead", "scalar")) %>%
    mutate(name=factor(name, 
                       levels=c("lagLead", "scalar"), 
                       labels=c("Best lag/lead (positive means cases first)", "Best scalar (deaths ~ cases)")
                       ), 
           cluster=str_replace_all(str_to_title(cluster), "_", " ")
           ) %>%
    ggplot(aes(x=fct_rev(cluster), y=value)) + 
    geom_text(aes(label=ifelse(abs(value) < 1 & value != 0, paste0(100*round(value, 4), "%"), value)), 
              size=4, 
              color="red"
              ) + 
    labs(x="", 
         y="", 
         title="Modeling deaths vs. lag/lead cases (using data from July 2020 to January 2021)", 
         subtitle="Best-fit parameters for linear model: deaths ~ best scalar * lag(cases, best lag/lead) + 0"
         ) +
    facet_wrap(~name, scales="free_x", ncol=1) + 
    coord_flip()

```

The graphs are then plotted for each of the clusters:  
```{r}

# Get the number to pull from the lists
listTemp <- allRMSER2 %>%
    group_by(cluster) %>%
    mutate(n=row_number()) %>%
    ungroup() %>%
    inner_join(select(bestLagLead, cluster, lagLead))
listTemp

# Iterate over listTemp
listPlots <- vector("list", nrow(listTemp))
for (ctr in 1:length(listPlots)) {
    listPlots[ctr] <- allListAgg[[listTemp$cluster[ctr]]][[listTemp$n[ctr]]]["p1"]
}

# Show plots
gridExtra::grid.arrange(listPlots[[1]], listPlots[[2]], listPlots[[3]],
                        listPlots[[4]], listPlots[[5]], listPlots[[6]], 
                        ncol=2
                        )

```

Curves fit reasonably well for each cluster.  The approach above seems inefficient and produces a consolidated plot with poor aesthetic.

There are two more efficient ways to use grid.arrange on a list of ggplot:  
```{r}

# Option 1: do.call
do.call(gridExtra::grid.arrange, listPlots)

# Option 2: grobs=
gridExtra::grid.arrange(grobs=listPlots, ncol=2)

```

For improved plotting, the legends are moved to the bottom o each plot:  
```{r}

gridExtra::grid.arrange(grobs=lapply(listPlots, 
                                     FUN=function(x) { 
                                         x + 
                                             theme(legend.position="bottom") +
                                             guides(color=guide_legend(nrow=2)) 
                                         }
                                     ), 
                        ncol=3
                        )

```

The plotting area can be increased using fig.width (default 7) and fig.height (default 5):  
```{r, fig.height=9, fig.width=9}

gridExtra::grid.arrange(grobs=lapply(listPlots, 
                                     FUN=function(x) { 
                                         x + 
                                             theme(legend.position="bottom") +
                                             guides(color=guide_legend(nrow=2)) 
                                         }
                                     ), 
                        ncol=3
                        )

```

The plot titles can also be updated to provide more details:  
```{r, fig.height=9, fig.width=9}

names(listPlots) <- listTemp$cluster
listPlots_002 <- lapply(seq_along(listPlots), 
                        FUN=function(ctr) {
                            listPlots[[ctr]] + 
                                labs(title=paste0(names(listPlots)[ctr], 
                                                  ":\n", 
                                                  listPlots[[ctr]]$labels$title
                                                  )
                                     ) 
                            }
                        )
gridExtra::grid.arrange(grobs=lapply(listPlots_002, 
                                     FUN=function(x) { 
                                         x + 
                                             theme(legend.position="bottom") +
                                             guides(color=guide_legend(nrow=2))
                                         }
                                     ), 
                        ncol=3
                        )

```

The datasets can be combined, and a similar plot can be made using facets:  
```{r}

plotData <- lapply(listPlots_002, FUN=function(x) x$data) %>%
    bind_rows(.id="listNumber") %>%
    mutate(listNumber=listTemp$cluster[as.integer(listNumber)])

p1 <- plotData %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, color=name)) + 
    facet_wrap(~listNumber) + 
    labs(x="", 
         y="Per million (7 day avg.)", 
         title="Curve alignment for lag(deaths) ~ cases",
         subtitle="Positive lag/lead means cases first"
         ) + 
    scale_color_discrete("") +
    theme(legend.position="bottom")

p1
p1 + facet_wrap(~listNumber, scales="free_y")
```

The next steps are to add the summary statistics:  
```{r, fig.height=9, fig.width=9}

p1 + 
    geom_text(data=mutate(rename(bestLagLead, listNumber=cluster), 
                          date=as.Date("2020-08-01"), 
                          value=Inf
                          ), 
              aes(label=paste0("Best lag/lead: ", lagLead, 
                               "\nBest scalar: ", round(scalar, 3),
                               "\nR**2: ", round(rsq, 3),
                               "\nRMSE: ", round(rmse, 2)
                               )
                  ), 
              vjust=1, 
              hjust=0, 
              size=3
              ) +
    facet_wrap(~listNumber, scales="free_y")

```

The facet is advantageous for the specific information in these plots.

A similar process is then run for cases and hospitalizations:  
```{r}

# Cluster cases vs. deaths for full list
vecLagLead <- seq(-10, 36, by=2)
allListCaseHosp <- lapply(dfAggList, 
                          FUN=function(x) {
                              alignOutbreaks(dfA=filter(x, metric=="cases"), 
                                             dfB=filter(x, metric=="hosp"), 
                                             minA=as.Date("2020-07-15"), 
                                             maxA=as.Date("2021-01-20"), 
                                             lagLeadTryB=vecLagLead, 
                                             nameA="Cluster cases", 
                                             nameB="Cluster total hosp with lag/lead", 
                                             nameScaled="Scaled cluster cases",
                                             yAxisName="Per million (7 day avg.)", 
                                             useIntercept=TRUE,
                                             noPlotA=TRUE
                                             )
                              }
                          )

allRMSER2CaseHosp <- lapply(allListCaseHosp, 
                            FUN=function(x) {
                                sapply(x, FUN=function(y) c(lagLead=y$lagLead, 
                                                            rsq=y$rsqAB, 
                                                            rmse=y$rmseAB, 
                                                            scalar=unname(y$scalarAB), 
                                                            intercept=unname(y$interceptAB)
                                                            )
                                       )  %>%
                                    t() %>%
                                    tibble::as_tibble()
                                }
                            ) %>%
    bind_rows(.id="cluster")

allRMSER2CaseHosp %>%
    pivot_longer(-c("cluster", "lagLead")) %>%
    ggplot(aes(x=lagLead, y=value)) + 
    geom_line(aes(group=cluster, color=cluster)) + 
    labs(x="Lag/lead (positive means cases first)", 
         y="", 
         title="Metrics for total hospitalized vs. cases by cluster"
         ) + 
    facet_wrap(~name, scales="free_y")

```

As expected, there appears to be a shorter lag between cases and total hospitalized than between cases and deaths.

For each cluster, the lag/lead with the lowest RMSE and/or highest R-squared is pulled:  
```{r}

allRMSER2CaseHosp %>%
    group_by(cluster) %>%
    filter(rsq==max(rsq) | rmse==min(rmse)) %>%
    ungroup()

```

The highest R-squared and the lowest RMSE tend to occur at the same lag/lead for each cluster.  Highest r-squared is used for the clusters where lowest RMSE occurs at a different lag than highest R-squared:  
```{r}

# Capture the best lag/lead by cluster
bestLagLeadCaseHosp <- allRMSER2CaseHosp %>%
    group_by(cluster) %>%
    filter(rsq==max(rsq)) %>%
    ungroup()
bestLagLeadCaseHosp

bestLagLeadCaseHosp %>%
    pivot_longer(-c("cluster")) %>%
    filter(name %in% c("lagLead", "scalar")) %>%
    mutate(name=factor(name, 
                       levels=c("lagLead", "scalar"), 
                       labels=c("Best lag/lead (positive means cases first)", "Best scalar (hosp ~ cases)")
                       ), 
           cluster=str_replace_all(str_to_title(cluster), "_", " ")
           ) %>%
    ggplot(aes(x=fct_rev(cluster), y=value)) + 
    geom_text(aes(label=ifelse(abs(value) < 1 & value != 0, paste0(100*round(value, 4), "%"), value)), 
              size=4, 
              color="red"
              ) + 
    labs(x="", 
         y="", 
         title="Modeling total hospitalized vs. lag/lead cases (using data from July 2020 to January 2021)", 
         subtitle="Best-fit parameters for linear model: hosp ~ best scalar * lag(cases, best lag/lead) + 0"
         ) +
    facet_wrap(~name, scales="free_x", ncol=1) + 
    coord_flip()

```

There tends to be an 8-day lag between change in cases and change in total hospitalized, with change in total hospitalized running at roughly 50% of change in 7-day average cases.

The best-fit plots are combined using gridExtra:  
```{r, fig.height=9, fig.width=9}

# Get the number to pull from the lists
listTempCaseHosp <- allRMSER2CaseHosp %>%
    group_by(cluster) %>%
    mutate(n=row_number()) %>%
    ungroup() %>%
    inner_join(select(bestLagLeadCaseHosp, cluster, lagLead))
listTempCaseHosp

# Iterate over listTemp
listPlotsCaseHosp <- vector("list", nrow(listTempCaseHosp))
for (ctr in 1:length(listPlotsCaseHosp)) {
    listPlotsCaseHosp[ctr] <- allListCaseHosp[[listTempCaseHosp$cluster[ctr]]][[listTempCaseHosp$n[ctr]]]["p1"]
}

names(listPlotsCaseHosp) <- listTempCaseHosp$cluster
listPlotsCaseHosp_002 <- lapply(seq_along(listPlotsCaseHosp), 
                                FUN=function(ctr) {
                                    listPlotsCaseHosp[[ctr]] + 
                                        labs(title=paste0(names(listPlotsCaseHosp)[ctr], 
                                                          ":\n", 
                                                          listPlotsCaseHosp[[ctr]]$labels$title
                                                          )
                                        )
                                    }
                                )

gridExtra::grid.arrange(grobs=lapply(listPlotsCaseHosp_002, 
                                     FUN=function(x) { 
                                         x + 
                                             theme(legend.position="bottom") +
                                             guides(color=guide_legend(nrow=2))
                                         }
                                     ), 
                        ncol=3
                        )

```

Best-fit curves appear to be well aligned.

The datasets can be combined, and a similar plot can be made using facets:  
```{r, fig.height=9, fig.width=9}

plotDataCaseHosp <- lapply(listPlotsCaseHosp_002, FUN=function(x) x$data) %>%
    bind_rows(.id="listNumber") %>%
    mutate(listNumber=listTempCaseHosp$cluster[as.integer(listNumber)])

p1 <- plotDataCaseHosp %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, color=name)) + 
    facet_wrap(~listNumber) + 
    labs(x="", 
         y="Per million (7 day avg.)", 
         title="Curve alignment for lag(hosp) ~ cases",
         subtitle="Positive lag/lead means cases first"
         ) + 
    scale_color_discrete("") +
    theme(legend.position="bottom")

p1 + 
    geom_text(data=mutate(rename(bestLagLeadCaseHosp, listNumber=cluster), 
                          date=as.Date("2020-08-01"), 
                          value=Inf
                          ), 
              aes(label=paste0("Best lag/lead: ", lagLead, 
                               "\nBest scalar: ", round(scalar, 3),
                               "\nBest intercept: ", round(intercept, 1), 
                               "\nR**2: ", round(rsq, 3),
                               "\nRMSE: ", round(rmse, 1)
                               )
                  ), 
              vjust=1, 
              hjust=0, 
              size=3
              ) +
    facet_wrap(~listNumber, scales="free_y")

```

The facet is advantageous for the specific information in these plots.

A similar process is then run for hospitalizations and deaths:  
```{r}

# Cluster hosp vs. deaths for full list
vecLagLead <- seq(-10, 36, by=2)
allListHospDead <- lapply(dfAggList, 
                          FUN=function(x) {
                              alignOutbreaks(dfA=filter(x, metric=="hosp"), 
                                             dfB=filter(x, metric=="deaths"), 
                                             minA=as.Date("2020-07-15"), 
                                             maxA=as.Date("2021-01-20"), 
                                             lagLeadTryB=vecLagLead, 
                                             nameA="Cluster total hospitalized", 
                                             nameB="Cluster deaths", 
                                             nameScaled="Scaled cluster total hosp",
                                             yAxisName="Per million (7 day avg.)", 
                                             useIntercept=FALSE,
                                             noPlotA=TRUE
                                             )
                              }
                          )

allRMSER2HospDead <- lapply(allListHospDead, 
                            FUN=function(x) {
                                sapply(x, FUN=function(y) c(lagLead=y$lagLead, 
                                                            rsq=y$rsqAB, 
                                                            rmse=y$rmseAB, 
                                                            scalar=unname(y$scalarAB), 
                                                            intercept=unname(y$interceptAB)
                                                            )
                                       )  %>%
                                    t() %>%
                                    tibble::as_tibble()
                                }
                            ) %>%
    bind_rows(.id="cluster")

allRMSER2HospDead %>%
    pivot_longer(-c("cluster", "lagLead")) %>%
    ggplot(aes(x=lagLead, y=value)) + 
    geom_line(aes(group=cluster, color=cluster)) + 
    labs(x="Lag/lead (positive means hosp first)", 
         y="", 
         title="Metrics for deaths vs. total hospitalized by cluster"
         ) + 
    facet_wrap(~name, scales="free_y")

```

There appears to be a modest but meaningful lag between total hospitalized and deaths, with around 2% of the change in total hospitalized being reasonably predictive of future change in deaths.

For each cluster, the lag/lead with the lowest RMSE and/or highest R-squared is pulled:  
```{r}

allRMSER2HospDead %>%
    group_by(cluster) %>%
    filter(rsq==max(rsq) | rmse==min(rmse)) %>%
    ungroup()

```

The highest R-squared and the lowest RMSE occur at the same lag/lead for each cluster:  
```{r}

# Capture the best lag/lead by cluster
bestLagLeadHospDead <- allRMSER2HospDead %>%
    group_by(cluster) %>%
    filter(rsq==max(rsq)) %>%
    ungroup()
bestLagLeadHospDead

bestLagLeadHospDead %>%
    pivot_longer(-c("cluster")) %>%
    filter(name %in% c("lagLead", "scalar")) %>%
    mutate(name=factor(name, 
                       levels=c("lagLead", "scalar"), 
                       labels=c("Best lag/lead (positive means hosp first)", "Best scalar (deaths ~ hosp)")
                       ), 
           cluster=str_replace_all(str_to_title(cluster), "_", " ")
           ) %>%
    ggplot(aes(x=fct_rev(cluster), y=value)) + 
    geom_text(aes(label=ifelse(abs(value) < 1 & value != 0, paste0(100*round(value, 4), "%"), value)), 
              size=4, 
              color="red"
              ) + 
    labs(x="", 
         y="", 
         title="Modeling deaths vs. lag/lead total hospitalized (using data from July 2020 to January 2021)", 
         subtitle="Best-fit parameters for linear model: deaths ~ best scalar * lag(hosp, best lag/lead) + 0"
         ) +
    facet_wrap(~name, scales="free_x", ncol=1) + 
    coord_flip()

```

There tends to be an 1-2 week lag between change in total hospitalized and change in deaths.  Change in deaths runs at roughly 2%-3% of change in total hospitalized.

The best-fit plots are combined using gridExtra:  
```{r, fig.height=9, fig.width=9}

# Get the number to pull from the lists
listTempHospDead <- allRMSER2HospDead %>%
    group_by(cluster) %>%
    mutate(n=row_number()) %>%
    ungroup() %>%
    inner_join(select(bestLagLeadHospDead, cluster, lagLead))
listTempHospDead

# Iterate over listTemp
listPlotsHospDead <- vector("list", nrow(listTempHospDead))
for (ctr in 1:length(listPlotsHospDead)) {
    listPlotsHospDead[ctr] <- allListHospDead[[listTempHospDead$cluster[ctr]]][[listTempHospDead$n[ctr]]]["p1"]
}

names(listPlotsHospDead) <- listTempHospDead$cluster
listPlotsHospDead_002 <- lapply(seq_along(listPlotsHospDead), 
                                FUN=function(ctr) {
                                    listPlotsHospDead[[ctr]] + 
                                        labs(title=paste0(names(listPlotsHospDead)[ctr], 
                                                          ":\n", 
                                                          listPlotsHospDead[[ctr]]$labels$title
                                                          )
                                        )
                                    }
                                )

gridExtra::grid.arrange(grobs=lapply(listPlotsHospDead_002, 
                                     FUN=function(x) { 
                                         x + 
                                             theme(legend.position="bottom") +
                                             guides(color=guide_legend(nrow=2))
                                         }
                                     ), 
                        ncol=3
                        )

```

Best-fit curves appear to be well aligned.

The datasets can be combined, and a similar plot can be made using facets:  
```{r, fig.height=9, fig.width=9}

plotDataHospDead <- lapply(listPlotsHospDead_002, FUN=function(x) x$data) %>%
    bind_rows(.id="listNumber") %>%
    mutate(listNumber=listTempHospDead$cluster[as.integer(listNumber)])

p1 <- plotDataHospDead %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, color=name)) + 
    facet_wrap(~listNumber) + 
    labs(x="", 
         y="Per million (7 day avg.)", 
         title="Curve alignment for lag(deaths) ~ total hospitalized",
         subtitle="Positive lag/lead means hosp first"
         ) + 
    scale_color_discrete("") +
    theme(legend.position="bottom")

p1 + 
    geom_text(data=mutate(rename(bestLagLeadHospDead, listNumber=cluster), 
                          date=as.Date("2020-08-01"), 
                          value=Inf
                          ), 
              aes(label=paste0("Best lag/lead: ", lagLead, 
                               "\nBest scalar: ", round(scalar, 3),
                               "\nBest intercept: ", round(intercept, 1), 
                               "\nR**2: ", round(rsq, 3),
                               "\nRMSE: ", round(rmse, 2)
                               )
                  ), 
              vjust=1, 
              hjust=0, 
              size=3
              ) +
    facet_wrap(~listNumber, scales="free_y")

```

The facet is advantageous for the specific information in these plots.

The process ic converted to functional form so that a single set of parameters can be passed:  
```{r}

# Mapping file for metric name to axis name(s)
alignNameMap <- list("hosp"=c("total hospitalized", "total hosp"), 
                     "cases"=c("cases", "cases"), 
                     "deaths"=c("deaths", "deaths")
                     )

# Function for aligning metrics
makeAlignment <- function(lst, 
                          vecLagLead=seq(-10, 36, by=2), 
                          algMetrics=c("cases", "deaths"), 
                          algDates=c("2020-07-15", "2021-01-20"), 
                          mapper=alignNameMap, 
                          useIntercept=FALSE, 
                          plotA=FALSE
                          ) {
    
    # FUNCTION ARGUMENTS
    # lst: A list containing the key data by cluster
    # vecLagLead: the vector of lags and leads to try
    # algMetrics: a length 2 character vector where algMetrics[1] is investigated as a driver of algMetrics[2]
    # algDates: a length 2 character vector with yyyy-mm-dd for the minimum date and then maximum date
    # mapper: list for mapping algMetrics to axis names
    # useIntercept: should an intercept be modeled when aligning algMetrics?
    # plotA: should A and B be plotted together without scaling (only makes sense if they are of similar scale)
    
    # Cluster algMetrics for full list
    allList <- lapply(lst, 
                      FUN=function(x) {
                          alignOutbreaks(dfA=filter(x, metric==algMetrics[1]), 
                                         dfB=filter(x, metric==algMetrics[2]), 
                                         minA=as.Date(algDates[1]), 
                                         maxA=as.Date(algDates[2]), 
                                         lagLeadTryB=vecLagLead, 
                                         nameA=paste0("Cluster ", mapper[[algMetrics[1]]][1]), 
                                         nameB=paste0("Cluster ", mapper[[algMetrics[2]]][1]), 
                                         nameScaled=paste0("Scaled cluster ", mapper[[algMetrics[1]]][2]),
                                         yAxisName="Per million (7 day avg.)", 
                                         useIntercept=useIntercept,
                                         noPlotA=isFALSE(plotA)
                                         )
                          }
                      )

    # Pull all relevant RMSE data and convert to tibble
    allRMSER2 <- lapply(allList, 
                        FUN=function(x) {
                            sapply(x, FUN=function(y) c(lagLead=y$lagLead, 
                                                        rsq=y$rsqAB, 
                                                        rmse=y$rmseAB, 
                                                        scalar=unname(y$scalarAB), 
                                                        intercept=unname(y$interceptAB)
                                                        )
                                   )  %>%
                                t() %>%
                                tibble::as_tibble()
                            }
                        ) %>%
        bind_rows(.id="cluster")

    # Plot the alignment metrics data
    p1 <- allRMSER2 %>%
        pivot_longer(-c("cluster", "lagLead")) %>%
        ggplot(aes(x=lagLead, y=value)) + 
        geom_line(aes(group=cluster, color=cluster)) + 
        labs(x=paste0("Lag/lead (positive means ", mapper[[algMetrics[1]]][2], " first)"), 
             y="", 
             title=paste0("Metrics for ", 
                          mapper[[algMetrics[2]]][1], 
                          " vs. ", 
                          mapper[[algMetrics[1]]][1], 
                          " by cluster"
                          )
             ) + 
        facet_wrap(~name, scales="free_y")
    print(p1)

    # Show the lowest RMSE and highest R-squared (will sometimes, but not always, be the same)    
    allRMSER2 %>%
        group_by(cluster) %>%
        filter(rsq==max(rsq) | rmse==min(rmse)) %>%
        ungroup() %>%
        print()

    # Capture the best lag/lead by cluster (use highest R-squared rather than lowest RMESE if different lags)
    bestLagLead <- allRMSER2 %>%
        group_by(cluster) %>%
        filter(rsq==max(rsq)) %>%
        ungroup()
    print(bestLagLead)

    # Plot the best-fit alignment metrics
    p2 <- bestLagLead %>%
        pivot_longer(-c("cluster")) %>%
        filter(name %in% c("lagLead", "scalar")) %>%
        mutate(name=factor(name, 
                           levels=c("lagLead", "scalar"), 
                           labels=c(paste0("Best lag/lead (positive means ", 
                                           mapper[[algMetrics[1]]][1], 
                                           " first)"
                                           ), 
                                    paste0("Best scalar (", 
                                           mapper[[algMetrics[1]]][2], 
                                           " ~ ", 
                                           mapper[[algMetrics[1]]][1], 
                                           ")"
                                           )
                                    )
                           ), 
               cluster=str_replace_all(str_to_title(cluster), "_", " ")
               ) %>%
        ggplot(aes(x=fct_rev(cluster), y=value)) + 
        geom_text(aes(label=ifelse(abs(value) < 1 & value != 0, paste0(100*round(value, 4), "%"), value)), 
                  size=4, 
                  color="red"
                  ) + 
        labs(x="", 
             y="", 
             title=paste0("Modeling ", 
                          mapper[[algMetrics[2]]][1], 
                          " vs. lag/lead ", 
                          mapper[[algMetrics[1]]][1], 
                          " (using data from ", 
                          format(as.Date(algDates[1]), "%b %Y"), 
                          " to ", 
                          format(as.Date(algDates[2]), "%b %Y"), 
                          ")"
                          ), 
             subtitle=paste0("Best-fit parameters for linear model: ", 
                             mapper[[algMetrics[2]]][1],
                             " ~ best scalar * lag(", 
                             mapper[[algMetrics[1]]][1], 
                             ", best lag/lead)", 
                             ifelse(useIntercept, "", " + 0")
                             )
             ) +
        facet_wrap(~name, scales="free_x", ncol=1) + 
        coord_flip()
    print(p2)

    # Return a list of the core list file, RMSER2 data, and best lag/lead
    list(allList=allList, allRMSER2=allRMSER2, bestLagLead=bestLagLead, algMetrics=algMetrics)
    
}

caseDeath <- makeAlignment(dfAggList)

```

Next steps are to create a function for plotting data from the output list:  
```{r}

# Function for plotting aligned curves
plotAlignment <- function(lst, 
                          mapper=alignNameMap
                          ) {
    
    # FUNCTION ARGUMENTS:
    # lst: list containing outputs from makeAlignment
    # mapper: list for mapping algMetrics to axis names
    
    # Get the number to pull from the lists
    dfTemp <- lst[["allRMSER2"]] %>%
        group_by(cluster) %>%
        mutate(n=row_number()) %>%
        ungroup() %>%
        inner_join(select(lst[["bestLagLead"]], cluster, lagLead))

    # Iterate over listTemp
    listPlots <- vector("list", nrow(dfTemp))
    for (ctr in 1:length(listPlots)) {
        listPlots[ctr] <- lst[["allList"]][[dfTemp$cluster[ctr]]][[dfTemp$n[ctr]]]["p1"]
    }
    
    # Name plots and update final list for plotting
    names(listPlots) <- dfTemp$cluster
    listPlots <- lapply(seq_along(listPlots), 
                        FUN=function(ctr) {
                            listPlots[[ctr]] + 
                                labs(title=paste0(names(listPlots)[ctr], 
                                                  ":\n", 
                                                  listPlots[[ctr]]$labels$title
                                                  )
                                     )
                            }
                        )

    # Use grid.arrange to show the key plots
    gridExtra::grid.arrange(grobs=lapply(listPlots, 
                                         FUN=function(x) { 
                                             x + 
                                                 theme(legend.position="bottom") +
                                                 guides(color=guide_legend(nrow=2))
                                             }
                                         ), 
                            ncol=3
                            )
    
    # Create combined data for facetting
    plotData <- lapply(listPlots, FUN=function(x) x$data) %>%
        bind_rows(.id="listNumber") %>%
        mutate(listNumber=dfTemp$cluster[as.integer(listNumber)])
    
    # Create a baseline plot from the facetted data
    p1 <- plotData %>%
        ggplot(aes(x=date, y=value)) + 
        geom_line(aes(group=name, color=name)) + 
        facet_wrap(~listNumber) + 
        labs(x="", 
             y="Per million (7 day avg.)", 
             title=paste0("Curve alignment for lag(", 
                          mapper[[lst[["algMetrics"]][2]]][1], 
                          ") ~ ", 
                          mapper[[lst[["algMetrics"]][1]]][1]
                          ),
             subtitle=paste0("Positive lag/lead means ", lst[["algMetrics"]][1], " first")
             ) + 
        scale_color_discrete("") +
        theme(legend.position="bottom")

    # Find a good date for showing key statistics (left justify at this date)
    keyDate <- plotData %>%
        group_by(listNumber) %>%
        summarize(date=min(date), .groups="drop") %>%
        pull(date) %>%
        min() %>%
        lubridate::ceiling_date("months")
    
    # Add the key statistics
    p1 <- p1 + 
        geom_text(data=mutate(rename(lst[["bestLagLead"]], listNumber=cluster), 
                              date=as.Date(keyDate), 
                              value=Inf
                              ), 
                  aes(label=paste0("Best lag/lead: ", lagLead, 
                                   "\nBest scalar: ", round(scalar, 3),
                                   "\nBest intercept: ", round(intercept, 1), 
                                   "\nR**2: ", round(rsq, 3),
                                   "\nRMSE: ", round(rmse, 2)
                                   )
                      ), 
                  vjust=1, 
                  hjust=0, 
                  size=3
                  ) +
        facet_wrap(~listNumber, scales="free_y")

    # Print the facetted plot
    print(p1)
    
    # Return key elements
    list(inputList=lst, listPlots=listPlots, plotData=plotData, facetPlot=p1)
}

```

The data can then be plotted using a wider and taller space:  
```{r, fig.height=9, fig.width=9}

caseDeathPlots <- plotAlignment(caseDeath)

```

Updated data through March 31, 2021 are downloaded, with existing segments applied:  
```{r cache=TRUE}

# Create new segments with updated data
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_210401.csv"
locLog <- "./RInputFiles/Coronavirus/downloaded_20210401.log"
ctp_hier6_210401 <- readRunCOVIDTrackingProject(thruLabel="Mar 31, 2021", 
                                                downloadTo=if(file.exists(locDownload)) NULL else locDownload,
                                                readFrom=locDownload, 
                                                compareFile=readFromRDS("ctp_hier6_210301")$dfRaw,
                                                dateChangePlot=TRUE,
                                                dateMetricPrint=FALSE,
                                                writeLog=locLog,
                                                useClusters=ctp_hier6_210201$useClusters
                                                )
saveToRDS(ctp_hier6_210401, ovrWriteError=FALSE)

```
  
As of March 7, Cornonavirus Tracking Project no longer update data, and they recommend using [US government sources](https://covidtracking.com/analysis-updates/federal-covid-data-101-how-to-find-data).
