---
title: "Data Camp Insights"
author: "davegoblue"
date: "April 26, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
## Background and Overview  
DataCamp offer interactive courses related to R Programming.  While some is review, it is helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  This document summarizes a few key insights from:  

This document is currently split between _v003 and v_003_a due to the need to keep the number of DLL that it opens below the hard-coded maximum.  This introductory section needs to be re-written, and the contents consolidated, at a future date.
  
* R Programming (Introductory R, Intermediate R, Writing Functions in R, Object Oriented Programming in R, Introduction to Tidyverse)  
* Importing and Cleaning Data (Cleaning Data in R, Importing Data in to R)  
* Data Manipulation (dplyr, data.table, xts/zoo, dplyr joins, cases for EDA/Time Series/Pitch Data)  
* Data Visualization (base, ggplot2 parts I/II/III, ggvis, geospatial)  
* Statistics (8 refresher modules)  
* Machine Learning (3 modules + 1 text mining case)  
* R Studio (2 parts) and R Markdown (1 module)  
  
The original DataCamp_Insights_v001 and DataCamp_Insights_v002 documents have been split for this document:  
  
* This DataCamp_Insights_v003 document contains evolving sections on R Programming, Machine Learning, and RStudio / R Markdown  
* Importing and Cleaning Data components have been moved to DataCamp_ImportClean_v002  
* Data Manipulation components have been moved to DataCamp_DataManipulation_v002  
* Visualization components have been moved to DataCamp_Visualization_v002  
* Statistics components have been moved to DataCamp_Statistics_v002  
  

###_Working with Dates and Times in R_  
  
Chapter 1 - Dates and Times in R  
  
Introduction to dates - including the built-in methods for R:  
  
* Differences in M-D-Y and D-M-Y  
* ISO8601 is a standard for dates - components should be decreasing such as YYYY-MM-DD  
	* The numbers should all be padded with leading zeroes  
    * A separator is not required, but it must be a dash (-) if used  
* R will generally require input using as.Date(<isoFormattedString>)  
* Some functions that read in data will automatically recognize and parse dates in a variety of formats  
    * In particular the import functions, like read_csv(), in the readr package will recognize dates in a few common formats  
    * There is also the anytime() function in the anytime package whose sole goal is to automatically parse strings as dates regardless of the format  
  
Why use dates?  
  
* Behind the scenes, dates are stored as the number of days since 1970-01-01  
	* Can compare dates, take differences of dates, use dates for plotting, and the like  
* R releases have a major, minor, and patch  
	* Patch starts at zero with a new minor and increments by 1  
    * Minor starts at zero with a new major and incerements by 1  
  
What about times?  
  
* R also has the built-in capability to handle datetimes  
* ISO8601 has standards for datetimes also - YYYYMMDD HH:MM:SS  
* Two capabilities for storing times in R  
	* POSIXlt - list with named components  
    * POSIXct - seconds since 1970-01-01 00:00:00 (typically better for data frames, and focus of this module)  
* Can convert to POSIXct using as.POSIXct(<ISOString>)  
* Can pass a timezone, and the default assumption is local time  
    * If the string is passed as YYYYMMDDTHH:MM:SSZ then the assumption is made of Zulu (UTC) time  
* One drawback is that as.POSIXct() does not naturally recognize the timezones, so some additional work is required to properly enter a datetime  
  
Why lubridate?  
  
* The lubridate package is designed to make it easier to work with dates and times  
	* Part of the tidyverse - designed for humans, and integrates nicely to data analysis pipelines  
    * Consistent behavior regardless of the underlying objects  
* Easier to use, and more forgiving of formats  
* Has capability for time spans (time between two times, such as time for reign of monarchs)  
  
Example code includes:  
```{r}

library(dplyr)
library(ggplot2)


# The date R 3.0.0 was released
x <- "2013-04-03"

# Examine structure of x
str(x)

# Use as.Date() to interpret x as a date
x_date <- as.Date(x)

# Examine structure of x_date
str(x_date)

# Store April 10 2014 as a Date
april_10_2014 <- as.Date("2014-04-10")


# Load the readr package
library(readr)

# Use read_csv() to import rversions.csv
releases <- read_csv("./RInputFiles/rversions.csv")

# Examine the structure of the date column
str(releases$date)

# Load the anytime package
library(anytime)

# Various ways of writing Sep 10 2009
sep_10_2009 <- c("September 10 2009", "2009-09-10", "10 Sep 2009", "09-10-2009")

# Use anytime() to parse sep_10_2009
anytime(sep_10_2009)


# Set the x axis to the date column
ggplot(releases, aes(x = date, y = type)) +
  geom_line(aes(group = 1, color = factor(major)))

# Limit the axis to between 2010-01-01 and 2014-01-01
ggplot(releases, aes(x = date, y = type)) +
  geom_line(aes(group = 1, color = factor(major))) +
  xlim(as.Date("2010-01-01"), as.Date("2014-01-01"))

# Specify breaks every ten years and labels with "%Y"
ggplot(releases, aes(x = date, y = type)) +
  geom_line(aes(group = 1, color = factor(major))) +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y")


# Find the largest date
last_release_date <- max(releases$date)

# Filter row for last release
last_release <- filter(releases, date == last_release_date)

# Print last_release
last_release

# How long since last release?
Sys.Date() - last_release_date


# Use as.POSIXct to enter the datetime 
as.POSIXct("2010-10-01 12:12:00")

# Use as.POSIXct again but set the timezone to `"America/Los_Angeles"`
as.POSIXct("2010-10-01 12:12:00", tz = "America/Los_Angeles")

# Use read_csv to import rversions.csv
releases <- read_csv("./RInputFiles/rversions.csv")

# Examine structure of datetime column
str(releases$datetime)


# Import "cran-logs_2015-04-17.csv" with read_csv()
logs <- read_csv("./RInputFiles/cran-logs_2015-04-17.csv")

# Print logs
logs

# Store the release time as a POSIXct object
release_time <- as.POSIXct("2015-04-16 07:13:33", tz = "UTC")

# When is the first download of 3.2.0?
logs %>% 
  filter(r_version == "3.2.0")

# Examine histograms of downloads by version
ggplot(logs, aes(x = datetime)) +
  geom_histogram() +
  geom_vline(aes(xintercept = as.numeric(release_time)))+
  facet_wrap(~ r_version, ncol = 1)

```
  
  
  
***
  
Chapter 2 - Parsing and Manipulating Dates with lubridate  
  
Parsing dates with lubridate:  
  
* lubridate::ymd() will manage dates in format ymd, even if they are not properly ISO formatted (have separators, English abbreviations, and the like)  
	* Analogous behaviors from ydm(), mdy(), myd(), dmy(), dym(), day_hm()  
    * Assumes UTC unless otherwise specified  
    * All the functions with y, m and d in any order exist  
    * If your dates have times as well, you can use the functions that start with ymd, dmy, mdy or ydm and are followed by any of _h, _hm or _hms  
    * To see all the functions available look at ymd() for dates and ymd_hms() for datetimes  
* lubridate::parse_date_time(x=, orders=)  
	* The orders = argument is a sequence of characters, reflecting the order in the input  
    * y-year with century, Y-year without century, m-month, d-day, H-hours (24-hour), M-minutes, S-seconds, and many others  
    * a-abbreviated weekday, A-full weekday, b-abbreviate month, B-full month, I-hours (12-hour), p-AM/PM, z-timezone (offset in minutes/seconds from UTC)  
    * Can pass a vector of sequences to orders=, such as orders=c("ymd", "dmy"), if some of the dates are formatted differently than others  
  
Weather in Auckland (data from Weather Underground, METAR from Auckland airport):  
  
* Data are available in akl_weather_daily.csv and akl_weather_hourly_2016.csv  
* The lubridate::make_date(year, month, date) will produce a date from its components (these components can be vectors, such as columns in a frame  
	* There is also a lubridate::make_datetime(year, month, day, hour, min, sec)  
  
Extracting parts of a datetime:  
  
* The lubridate::year() will pull out the year from a datetime object  
	* month(), day(), hour(), minute(), second() will do the same  
    * wday() is the weekday (1-7), while yday() is the Julian date (1-366) and tz() is the timezone  
* The extractors can also be used to set a component of the datetime object  
* Several functions return booleans, more or less answers to "is this a" questions  
	* leap_year(), am(), pm(), dst(), quarter() will return 1-4, semester() will return 1-2  
    * Months of course are different lengths so we should really correct for that, take a look at days_in_month() for helping with that  
  
Rounding datetimes:  
  
* The lubridate::floor_date(unit=) will round-down to the requested unit, such as "hour"  
	* round_date() for nearest  
    * ceiling_date() for round-up  
* Units can be specified as "second", "minute", "hour", "day", "week", "month", "bimonth", "quarter", "halfyear", "year"  
  
Example code includes:  
```{r}

library(lubridate)
library(readr)
library(dplyr)
library(ggplot2)
library(ggridges)
library(stringr)


# Parse x 
x <- "2010 September 20th" # 2010-09-20
ymd(x)

# Parse y 
y <- "02.01.2010"  # 2010-01-02
dmy(y)

# Parse z 
z <- "Sep, 12th 2010 14:00"  # 2010-09-12T14:00
mdy_hm(z)


# Specify an order string to parse x
x <- "Monday June 1st 2010 at 4pm"
parse_date_time(x, orders = "AmdyIp")

# Specify order to include both "mdy" and "dmy"
two_orders <- c("October 7, 2001", "October 13, 2002", "April 13, 2003", 
  "17 April 2005", "23 April 2017")
parse_date_time(two_orders, orders = c("mdy", "dmy"))

# Specify order to include "dOmY", "OmY" and "Y"
short_dates <- c("11 December 1282", "May 1372", "1253")
parse_date_time(short_dates, orders = c("dOmY", "OmY", "Y"))


# Import CSV with read_csv()
akl_daily_raw <- read_csv("./RInputFiles/akl_weather_daily.csv")

# Print akl_daily_raw
akl_daily_raw

# Parse date 
akl_daily <- akl_daily_raw %>%
  mutate(date = ymd(date))

# Print akl_daily
akl_daily

# Plot to check work
ggplot(akl_daily, aes(x = date, y = max_temp)) +
  geom_line() 


# Import "akl_weather_hourly_2016.csv"
akl_hourly_raw <- read_csv("./RInputFiles/akl_weather_hourly_2016.csv")

# Print akl_hourly_raw
akl_hourly_raw

# Use make_date() to combine year, month and mday 
akl_hourly  <- akl_hourly_raw  %>% 
  mutate(date = make_date(year = year, month = month, day = mday))

# Parse datetime_string 
akl_hourly <- akl_hourly  %>% 
  mutate(
    datetime_string = paste(date, time, sep = "T"),
    datetime = ymd_hms(datetime_string)
  )

# Print date, time and datetime columns of akl_hourly
akl_hourly %>% select(date, time, datetime)

# Plot to check work
ggplot(akl_hourly, aes(x = datetime, y = temperature)) +
  geom_line()


# Examine the head() of release_time
releases <- read_csv("./RInputFiles/rversions.csv")
release_time <- releases %>% pull(datetime)
head(release_time)

# Examine the head() of the months of release_time
head(month(release_time))

# Extract the month of releases 
month(release_time) %>% table()

# Extract the year of releases
year(release_time) %>% table()

# How often is the hour before 12 (noon)?
mean(hour(release_time) < 12)

# How often is the release in am?
mean(am(release_time))


# Use wday() to tabulate release by day of the week
wday(releases$datetime) %>% table()

# Add label = TRUE to make table more readable
wday(releases$datetime, label=TRUE) %>% table()

# Create column wday to hold labelled week days
releases$wday <- wday(releases$datetime, label=TRUE)

# Plot barchart of weekday by type of release
ggplot(releases, aes(x=wday)) +
  geom_bar() +
  facet_wrap(~ type, ncol = 1, scale = "free_y")


# Add columns for year, yday and month
akl_daily <- akl_daily %>%
  mutate(
    year = year(date),
    yday = yday(date),
    month = month(date, label=TRUE))

# Plot max_temp by yday for all years
ggplot(akl_daily, aes(x = yday, y = max_temp)) +
  geom_line(aes(group = year), alpha = 0.5)

# Examine distribtion of max_temp by month
ggplot(akl_daily, aes(x = max_temp, y = month, height = ..density..)) +
  geom_density_ridges(stat = "density")


# Create new columns hour, month and rainy
akl_hourly <- akl_hourly %>%
  mutate(
    hour = hour(datetime),
    month = month(datetime, label=TRUE),
    rainy = (weather == "Precipitation")
  )

# Filter for hours between 8am and 10pm (inclusive)
akl_day <- akl_hourly %>% 
  filter(hour >= 8, hour <= 22)

# Summarise for each date if there is any rain
rainy_days <- akl_day %>% 
  group_by(month, date) %>%
  summarise(
    any_rain = any(rainy)
  )

# Summarise for each month, the number of days with rain
rainy_days %>% 
  summarise(
    days_rainy = sum(any_rain)
  )


r_3_4_1 <- ymd_hms("2016-05-03 07:13:28 UTC")

# Round down to day
floor_date(r_3_4_1, unit = "day")

# Round to nearest 5 minutes
round_date(r_3_4_1, unit = "5 minutes")

# Round up to week 
ceiling_date(r_3_4_1, unit = "week")

# Subtract r_3_4_1 rounded down to day
r_3_4_1 - floor_date(r_3_4_1, unit = "day")


# Create day_hour, datetime rounded down to hour
akl_hourly <- akl_hourly %>%
  mutate(
    day_hour = floor_date(datetime, unit = "hour")
  )

# Count observations per hour  
akl_hourly %>% 
  count(day_hour) 

# Find day_hours with n != 2  
akl_hourly %>% 
  count(day_hour) %>%
  filter(n != 2) %>% 
  arrange(desc(n))


```
  
  
  
***
  
Chapter 3 - Arithmetic with Dates and Times  
  
Taking differences of datetimes:  
  
* Pure subtraction will give the days between two datetimes, reported on the command line as "Time difference of x days"  
	* The difftime(day1, day2, units=) function is the same as day1 - day2, but with additional control of being able to request units (secs, mins, hours, days, weeks)  
* The today() function gives you today's date as a Date object  
* The now() function gives you the current date-time as a POSIXct object  
  
Time spans - difficult because they do not have a constant meaning (e.g., impact of daylight savings time):  
  
* The lubridate package manages time spans as EITHER period or duration  
	* The period is the way a human thinks about it - 1 day means same exact hour-minute-second tomorrow  
    * The duration is the way a stopwatch thinks about it - 1 day means 24 hours from now  
* The period time span in lubridate is called by adding an "s" to the end of the relevant function  
	* For example, days(x=1) will be exactly +1 in the days category only (all other units untouched)  
* The duration in lubridate is called by adding a "d" to the front of the relevant period function  
	* For example, ddays(x=1) will add 24 hours to the datetime  
* There was an eclipse over North America on 2017-08-21 at 18:26:40  
	* It's possible to predict the next eclipse with similar geometry by calculating the time and date one Saros in the future  
    * A Saros is a length of time that corresponds to 223 Synodic months, a Synodic month being the period of the Moon's phases, a duration of 29 days, 12 hours, 44 minutes and 3 seconds  
* What should ymd("2018-01-31") + months(1) return? Should it be 30, 31 or 28 days in the future? Try it  
	* In general lubridate returns the same day of the month in the next month, but since the 31st of February doesn't exist lubridate returns a missing value, NA  
    * There are alternative addition and subtraction operators: %m+% and %m-% that have different behavior  
    * Rather than returning an NA for a non-existent date, they roll back to the last existing date  
    * But use these operators with caution, unlike + and -, you might not get x back from x %m+% months(1) %m-% months(1)  
    * If you'd prefer that the date was rolled forward check out add_with_rollback() which has roll_to_first argument  
  
Intervals - third option in lubridate for storing times:  
  
* Can find length, whether an object is in the interval, whether various intervals overlap, and the like  
	* Intervals can be created either by using interval(datetime1, datetime2) or datetime1 %--% datetime2  
* There are many lubridate functions for working with intervals  
	* int_start() and int_end() will give back the start and end date for the interval  
    * int_length() will give back the interval length in seconds  
    * as.period() will return the interval length as a period, while as.duration() will return the interval length as a duration  
    * aDateTime %within% anInterval will return a boolean that answers the question  
    * The int_overlaps(int1, int2) will return a boolean for whether there is any overlap  
* Intervals tend to be best when you have a specific start and end date  
	* Otherwise, use periods for human purposes and durations for technical purposes  
* The operator %within% tests if the datetime (or interval) on the left hand side is within the interval of the right hand side  
	* int_overlaps() performs a similar test, but will return true if two intervals overlap at all  
  
Example code includes:  
```{r}

# The date of landing and moment of step
date_landing <- mdy("July 20, 1969")
moment_step <- mdy_hms("July 20, 1969, 02:56:15", tz = "UTC")

# How many days since the first man on the moon?
difftime(today(), date_landing, units = "days")

# How many seconds since the first man on the moon?
difftime(now(), moment_step, units = "secs")


# Three dates
mar_11 <- ymd_hms("2017-03-11 12:00:00", 
  tz = "America/Los_Angeles")
mar_12 <- ymd_hms("2017-03-12 12:00:00", 
  tz = "America/Los_Angeles")
mar_13 <- ymd_hms("2017-03-13 12:00:00", 
  tz = "America/Los_Angeles")

# Difference between mar_13 and mar_12 in seconds
difftime(mar_13, mar_12, units = "secs")

# Difference between mar_12 and mar_11 in seconds
difftime(mar_12, mar_11, units = "secs")


# Add a period of one week to mon_2pm
mon_2pm <- dmy_hm("27 Aug 2018 14:00")
mon_2pm + weeks(1)

# Add a duration of 81 hours to tue_9am
tue_9am <- dmy_hm("28 Aug 2018 9:00")
tue_9am + dhours(81)

# Subtract a period of five years from today()
today() - years(5)

# Subtract a duration of five years from today()
today() - dyears(5)


# Time of North American Eclipse 2017
eclipse_2017 <- ymd_hms("2017-08-21 18:26:40")

# Duration of 29 days, 12 hours, 44 mins and 3 secs
synodic <- ddays(29) + dhours(12) + dminutes(44) + dseconds(3)

# 223 synodic months
saros <- 223 * synodic

# Add saros to eclipse_2017
eclipse_2017 + saros


# Add a period of 8 hours to today
today_8am <- today() + hours(8)

# Sequence of two weeks from 1 to 26
every_two_weeks <- 1:26 * weeks(2)

# Create datetime for every two weeks for a year
today_8am + every_two_weeks


jan_31 <- ymd("2018-01-31")
# A sequence of 1 to 12 periods of 1 month
month_seq <- 1:12 * months(1)

# Add 1 to 12 months to jan_31
jan_31 + month_seq

# Replace + with %m+%
jan_31 %m+% month_seq

# Replace + with %m-%
jan_31 %m-% month_seq


# Create monarchs
mNames <- c('Elizabeth II' ,'Victoria' ,'George V' ,'George III' ,'George VI' ,'George IV' ,'Edward VII' ,'William IV' ,'Edward VIII' ,'George III(also United Kingdom)' ,'George II' ,'George I' ,'Anne' ,'Henry III' ,'Edward III' ,'Elizabeth I' ,'Henry VI' ,'Henry VI' ,'Æthelred II' ,'Æthelred II' ,'Henry VIII' ,'Charles II' ,'Henry I' ,'Henry II(co-ruler with Henry the Young King)' ,'Edward I' ,'Alfred the Great' ,'Edward the Elder' ,'Charles I' ,'Henry VII' ,'Edward the Confessor' ,'Richard II' ,'James I' ,'Edward IV' ,'Edward IV' ,'William I' ,'Edward II' ,'Cnut' ,'Stephen' ,'Stephen' ,'John' ,'Edgar I' ,'Æthelstan' ,'Henry IV' ,'William III(co-ruler with Mary II)' ,'Henry the Young King(co-ruler with Henry II)' ,'William II' ,'Richard I' ,'Eadred' ,'Henry V' ,'Edmund I' ,'Edward VI' ,'Mary II(co-ruler with William III)' ,'Mary I' ,'Anne(also Kingdom of Great Britain)' ,'Eadwig' ,'James II' ,'Edward the Martyr' ,'Harold I' ,'Harthacnut' ,'Richard III' ,'Louis (disputed)' ,'Harold II' ,'Edmund II' ,'Matilda (disputed)' ,'Edward V' ,'Edgar II' ,'Sweyn Forkbeard' ,'Jane (disputed)' ,'James VI' ,'William I' ,'Constantine II' ,'David II' ,'Alexander III' ,'Malcolm III' ,'Alexander II' ,'James I' ,'Malcolm II' ,'James V' ,'David I' ,'James III' ,'Charles II' ,'Charles II' ,'James IV' ,'Mary I' ,'Charles I' ,'Kenneth II' ,'James II' ,'Robert I' ,'Robert II' ,'Alexander I' ,'Macbeth' ,'Robert III' ,'Constantine I' ,'Kenneth MacAlpin' ,'William II' ,'Malcolm IV' ,'Giric(co-ruler with Eochaid?)' ,'Donald II' ,'Malcolm I' ,'Edgar' ,'Kenneth III' ,'Indulf' ,'Duncan I' ,'Mary II' ,'Amlaíb' ,'Anne(also Kingdom of Great Britain)' ,'Dub' ,'Cuilén' ,'Domnall mac Ailpín' ,'James VII' ,'Margaret' ,'John Balliol' ,'Donald III' ,'Constantine III' ,'Áed mac Cináeda' ,'Lulach' ,'Duncan II' ,'Ruaidrí Ua Conchobair' ,'Edward Bruce (disputed)' ,'Brian Ua Néill (disputed)' ,'Gruffudd ap Cynan' ,'Llywelyn the Great' ,'Owain Gwynedd' ,'Dafydd ab Owain Gwynedd' ,'Hywel ab Owain Gwynedd' ,'Llywelyn ap Gruffudd' ,'Owain Glyndwr (disputed)' ,'Owain Goch ap Gruffydd' ,'Owain Lawgoch (disputed)' ,'Dafydd ap Llywelyn' ,'Dafydd ap Gruffydd')
mDominion <- c('United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'United Kingdom' ,'Great Britain' ,'Great Britain' ,'Great Britain' ,'Great Britain' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'England' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Scotland' ,'Ireland' ,'Ireland' ,'Ireland' ,'Gwynedd' ,'Gwynedd' ,'Gwynedd' ,'Gwynedd' ,'Gwynedd' ,'Wales' ,'Wales' ,'Wales' ,'Wales' ,'Wales' ,'Wales')
mFrom <- c('1952-02-06' ,'1837-06-20' ,'1910-05-06' ,'1801-01-01' ,'1936-12-11' ,'1820-01-29' ,'1901-01-22' ,'1830-06-26' ,'1936-01-20' ,'1760-10-25' ,'1727-06-22' ,'1714-08-01' ,'1707-05-01' ,'NA' ,'1327-01-25' ,'1558-11-17' ,'1422-08-31' ,'1470-10-31' ,'978-03-18' ,'1014-02-03' ,'1509-04-22' ,'1649-01-30' ,'1100-08-03' ,'1154-10-25' ,'1272-11-20' ,'871-04-24' ,'899-10-27' ,'1625-03-27' ,'1485-08-22' ,'1042-06-08' ,'1377-06-22' ,'1603-03-24' ,'1461-03-04' ,'1471-04-11' ,'1066-12-12' ,'1307-07-07' ,'1016-11-30' ,'1135-12-22' ,'1141-11-01' ,'1199-04-06' ,'959-10-01' ,'924-08-02' ,'1399-09-29' ,'1689-02-13' ,'1170-06-14' ,'1087-09-09' ,'1189-07-06' ,'946-05-26' ,'1413-03-21' ,'939-10-27' ,'1547-01-28' ,'1689-02-13' ,'1553-07-19' ,'1702-03-08' ,'955-11-23' ,'1685-02-06' ,'975-07-09' ,'1037-11-12' ,'1040-03-17' ,'1483-06-26' ,'1216-06-14' ,'1066-01-05' ,'1016-04-23' ,'1141-04-07' ,'1483-04-09' ,'1066-10-15' ,'1013-12-25' ,'1553-07-10' ,'1567-07-24' ,'1165-12-09' ,'900-01-01' ,'1329-06-07' ,'1249-07-06' ,'1058-03-17' ,'1214-12-04' ,'1406-04-04' ,'1005-03-25' ,'1513-09-09' ,'1124-04-23' ,'1460-08-03' ,'1649-01-30' ,'1660-05-29' ,'1488-06-11' ,'1542-12-14' ,'1625-03-27' ,'971-01-01' ,'1437-02-21' ,'1306-03-25' ,'1371-02-22' ,'1107-01-08' ,'1040-08-14' ,'1390-04-19' ,'862-01-01' ,'843-01-01' ,'1689-05-11' ,'1153-05-24' ,'878-01-01' ,'889-01-01' ,'943-01-01' ,'1097-01-01' ,'997-01-01' ,'954-01-01' ,'1034-11-25' ,'1689-04-11' ,'971-01-01' ,'1702-03-08' ,'962-01-01' ,'NA' ,'858-01-01' ,'1685-02-06' ,'1286-11-25' ,'1292-11-17' ,'1093-11-13' ,'1095-01-01' ,'877-01-01' ,'1057-08-15' ,'1094-05-01' ,'1166-01-01' ,'1315-06-01' ,'1258-01-01' ,'1081-01-01' ,'1195-01-01' ,'1137-01-01' ,'1170-01-01' ,'1170-01-01' ,'1253-01-01' ,'1400-09-16' ,'1246-02-25' ,'1372-05-01' ,'1240-04-12' ,'1282-12-11')
mTo <- c('2018-02-08' ,'1901-01-22' ,'1936-01-20' ,'1820-01-29' ,'1952-02-06' ,'1830-06-26' ,'1910-05-06' ,'1837-06-20' ,'1936-12-11' ,'1801-01-01' ,'1760-10-25' ,'1727-06-11' ,'1714-08-01' ,'1272-11-16' ,'1377-06-21' ,'1603-03-24' ,'1461-03-04' ,'1471-04-11' ,'1013-12-25' ,'1016-04-23' ,'1547-01-28' ,'1685-02-06' ,'1135-12-01' ,'1189-07-06' ,'1307-07-07' ,'899-10-26' ,'924-07-17' ,'1649-01-30' ,'1509-04-21' ,'1066-01-05' ,'1399-09-29' ,'1625-03-27' ,'1470-10-03' ,'1483-04-09' ,'1087-09-09' ,'1327-01-20' ,'1035-11-12' ,'1141-04-07' ,'1154-10-25' ,'1216-10-19' ,'975-07-08' ,'939-10-27' ,'1413-03-20' ,'1702-03-08' ,'1183-06-11' ,'1100-08-02' ,'1199-04-06' ,'955-11-23' ,'1422-08-31' ,'946-05-26' ,'1553-07-06' ,'1694-12-28' ,'1558-11-17' ,'1707-04-30' ,'959-10-01' ,'1688-12-11' ,'978-03-18' ,'1040-03-17' ,'1042-06-08' ,'1485-08-22' ,'1217-09-22' ,'1066-10-14' ,'1016-11-30' ,'1141-11-01' ,'1483-06-26' ,'1066-12-17' ,'1014-02-03' ,'1553-07-19' ,'1625-03-27' ,'1214-12-04' ,'943-01-01' ,'1371-02-22' ,'1286-03-19' ,'1093-11-13' ,'1249-07-06' ,'1437-02-21' ,'1034-11-25' ,'1542-12-14' ,'1153-05-24' ,'1488-06-11' ,'1651-09-03' ,'1685-02-06' ,'1513-09-09' ,'1567-07-24' ,'1649-01-30' ,'995-01-01' ,'1460-08-03' ,'1329-06-07' ,'1390-04-19' ,'1124-04-23' ,'1057-08-15' ,'1406-04-04' ,'877-01-01' ,'858-02-13' ,'1702-03-08' ,'1165-12-09' ,'889-01-01' ,'900-01-01' ,'954-01-01' ,'1107-01-08' ,'1005-03-25' ,'962-01-01' ,'1040-08-14' ,'1694-12-28' ,'977-01-01' ,'1707-04-30' ,'NA' ,'971-01-01' ,'862-04-13' ,'1688-12-11' ,'1290-09-26' ,'1296-07-10' ,'1097-01-01' ,'1097-01-01' ,'878-01-01' ,'1058-03-17' ,'1094-11-12' ,'1193-01-01' ,'1318-10-14' ,'1260-01-01' ,'1137-01-01' ,'1240-04-11' ,'1170-01-01' ,'1195-01-01' ,'1170-01-01' ,'1282-12-11' ,'1416-01-01' ,'1255-01-01' ,'1378-07-01' ,'1246-02-25' ,'1283-10-03')

padMDate <- function(x) { 
    if (is.na(x[1]) | x[1] == "NA") { 
        NA 
    } else { 
        paste0(c(str_pad(x[1], 4, pad="0"), x[2], x[3]), collapse="-") 
    } 
}



monarchs <- tibble::tibble(name=mNames, dominion=mDominion, 
                           from=ymd(sapply(str_split(mFrom, "-"), FUN=padMDate)), 
                           to=ymd(sapply(str_split(mTo, "-"), FUN=padMDate))
                           )

# Print monarchs
monarchs

# Create an interval for reign
monarchs <- monarchs %>%
  mutate(reign = from %--% to) 

# Find the length of reign, and arrange
monarchs %>%
  mutate(length = int_length(reign)) %>% 
  arrange(desc(length)) %>%
  select(name, length, dominion)


# Print halleys
pDate <- c('66-01-26', '141-03-25', '218-04-06', '295-04-07', '374-02-13', '451-07-03', '530-11-15', '607-03-26', '684-11-26', '760-06-10', '837-02-25', '912-07-27', '989-09-02', '1066-03-25', '1145-04-19', '1222-09-10', '1301-10-22', '1378-11-09', '1456-01-08', '1531-08-26', '1607-10-27', '1682-09-15', '1758-03-13', '1835-11-16', '1910-04-20', '1986-02-09', '2061-07-28')
sDate <- c('66-01-25', '141-03-22', '218-04-06', '295-04-07', '374-02-13', '451-06-28', '530-09-27', '607-03-15', '684-10-02', '760-05-20', '837-02-25', '912-07-18', '989-09-02', '1066-01-01', '1145-04-15', '1222-09-10', '1301-10-22', '1378-11-09', '1456-01-08', '1531-08-26', '1607-10-27', '1682-09-15', '1758-03-13', '1835-08-01', '1910-04-20', '1986-02-09', '2061-07-28')
eDate <- c('66-01-26', '141-03-25', '218-05-17', '295-04-20', '374-02-16', '451-07-03', '530-11-15', '607-03-26', '684-11-26', '760-06-10', '837-02-28', '912-07-27', '989-09-05', '1066-03-25', '1145-04-19', '1222-09-28', '1301-10-31', '1378-11-14', '1456-06-09', '1531-08-26', '1607-10-27', '1682-09-15', '1758-12-25', '1835-11-16', '1910-05-20', '1986-02-09', '2061-07-28')

halleys <- tibble::tibble(perihelion_date=ymd(sapply(str_split(pDate, "-"), FUN=padMDate)), 
                          start_date=ymd(sapply(str_split(sDate, "-"), FUN=padMDate)), 
                          end_date=ymd(sapply(str_split(eDate, "-"), FUN=padMDate))
                          )


# New column for interval from start to end date
halleys <- halleys %>%
  mutate(visible = start_date %--% end_date)

# The visitation of 1066
halleys_1066 <- halleys[14, ]

# Monarchs in power on perihelion date
monarchs %>%
  filter(halleys_1066$perihelion_date %within% reign) %>%
  select(name, from, to, dominion)

# Monarchs whose reign overlaps visible time
monarchs %>%
  filter(int_overlaps(halleys_1066$visible, reign)) %>%
  select(name, from, to, dominion)


# New columns for duration and period
monarchs <- monarchs %>%
  mutate(
    duration = as.duration(reign),
    period = as.period(reign))

# Examine results    
monarchs %>% 
    select(name, duration, period) %>%
    head(10) %>%
    print.data.frame()

```
  
  
  
***
  
Chapter 4 - Problems in Practice  
  
Time zones - ways to keep track of times in different locations (can pose analysis challenges):  
  
* Typically captured as an offset from GMT, but specified in R using tz= since the offset to GMT can change during the year (DST for example)  
	* Sys.timezone() gives the timezone on your computer  
    * OlsonNames() gives all the timezones that R is aware of  
    * The OlsonNames() function matches with an international standard as to which cities are included  
    * The lubridate::tz() will extract the timezone from a specific datetime  
* Can change the timezone without changing the underlying clock time components by using lubridate::force_tz()  
	* force_tz(ymd_hm("2017-12-12 12:00", tz="America/Los_Angeles"), tzone="America/Boston") will produce 2017-12-12 12:00 EST (note that the 12:00 is held, with ONLY time-zone changed)  
* Can view the time in a different zone by using lubridate::with_tz()  
	* with_tz(ymd_hm("2017-12-12 12:00", tz="America/Los_Angeles"), tzone="America/Boston") will produce 2017-12-12 15:00 EST (note that 15:00 EST and 12:00 PST are the same)  
* For this entire course, if you've ever had a time, it's always had an accompanying date, i.e. a datetime. But sometimes you just have a time without a date  
	* If you find yourself in this situation, the hms package provides an hms class of object for holding times without dates, and the best place to start would be with as.hms()  
    * readr knows the hms class, so if it comes across something that looks like a time it will use it  
  
Importing and exporting datetimes:  
  
* The parse_date_time() function is designed to be forgiving and flexible, but at the expense of being slow (since it considers many possible formats)
	* The fasttime::fastPOSIXct() is designed to very quickly read a proper ISO formatting of "YYYY-MM-DD"  
    * The lubridate::fast_strptime(x=, format=) is also fast, but it requires a valid strptime format like "%Y-%m-%d" rather than the more flexible/forgiving parse_date_time(x=, order="ymd")  
    * See help for strptime() for the valid strings  
* The readr::write_csv() will write datetime objects in a proper ISO format, making for easy read-in  
* Can also use the lubridate::stamp() capability to build a function that will format things based on an example you provide  
	* my_stamp <- stamp("Tuesday October 10 2017")  
    * my_stamp has been created by lubridate::stamp() as function(x) format(x, format="%A %B %d %Y") to match the example given  
  
Wrap-up:  
  
* Chapter 1: base R objects Date, POSIXct  
	* lubridate, zoo, xts, and the like all work together with each other and these  
* Chapter 2: importing and manipulating datetime obects  
* Chapter 3: challenges of arithmetic with datetimes  
	* periods, durations, intervals  
* Chapter 4: time zones, and import/outputs  
  
Example code includes:  
```{r}

# Game2: CAN vs NZL in Edmonton
game2 <- mdy_hm("June 11 2015 19:00")

# Game3: CHN vs NZL in Winnipeg
game3 <- mdy_hm("June 15 2015 18:30")

# Set the timezone to "America/Edmonton"
game2_local <- force_tz(game2, tzone = "America/Edmonton")
game2_local

# Set the timezone to "America/Winnipeg"
game3_local <- force_tz(game3, tzone = "America/Winnipeg")
game3_local

# How long does the team have to rest?
as.period(game2_local %--% game3_local)


# What time is game2_local in NZ?
with_tz(game2_local, tzone = "Pacific/Auckland")

# What time is game2_local in Corvallis, Oregon?
with_tz(game2_local, tzone = "America/Los_Angeles")

# What time is game3_local in NZ?
with_tz(game3_local, tzone = "Pacific/Auckland")


# Examine datetime and date_utc columns
head(akl_hourly$datetime)
head(akl_hourly$date_utc)
  
# Force datetime to Pacific/Auckland
akl_hourly <- akl_hourly %>%
  mutate(
    datetime = force_tz(datetime, tzone = "Pacific/Auckland"))

# Reexamine datetime
head(akl_hourly$datetime)
  
# Are datetime and date_utc the same moments
table(akl_hourly$datetime - akl_hourly$date_utc)


# Import auckland hourly data 
akl_hourly <- read_csv("./RInputFiles/akl_weather_hourly_2016.csv")

# Examine structure of time column
str(akl_hourly$time)

# Examine head of time column
head(akl_hourly$time)

# A plot using just time
ggplot(akl_hourly, aes(x = time, y = temperature)) +
  geom_line(aes(group = make_date(year, month, mday)), alpha = 0.2)


library(microbenchmark)
library(fasttime)

# Examine structure of dates
dates <- paste0(gsub(" ", "T", as.character(akl_hourly$date_utc)), "Z")

str(dates)

# Use fastPOSIXct() to parse dates
fastPOSIXct(dates) %>% str()

# Compare speed of fastPOSIXct() to ymd_hms()
microbenchmark(
  ymd_hms = ymd_hms(dates),
  fasttime = fastPOSIXct(dates),
  times = 20)


# Head of dates
head(dates)

# Parse dates with fast_strptime
fast_strptime(dates, 
    format = "%Y-%m-%dT%H:%M:%SZ") %>% str()

# Comparse speed to ymd_hms() and fasttime
microbenchmark(
  ymd_hms = ymd_hms(dates),
  fasttime = fastPOSIXct(dates),
  fast_strptime = fast_strptime(dates, 
    format = "%Y-%m-%dT%H:%M:%SZ"),
  times = 20)


finished <- "I finished 'Dates and Times in R' on Thursday, September 20, 2017!"
# Create a stamp based on "Sep 20 2017"
date_stamp <- stamp("September 20, 2017", orders="mdy")

# Print date_stamp
date_stamp

# Call date_stamp on today()
date_stamp(today())

# Create and call a stamp based on "09/20/2017"
stamp("09/20/2017", orders="mdy")(today())

# Use string finished for stamp()
stamp(finished, orders="amdy")(today())

```
  
  
  
***
  
###_Scalable Data Processing in R_  
  
Chapter 1 - Working with Increasingly Large Data Sets  
  
What is scalable data processing?:  
  
* Working with data that is too large for one computer  
* Scalable code lets you work in parallel, and use resources as they become available  
* Data sets are frequently much bigger than available RAM, which is a challenge since R by default runs using R  
    * "R is not well suited to working with data larger than 10%-20% of a computer's RAM" - The R Installation and Administration Manual  
    * When a computer runs out of RAM, it "swaps" to the hard drive, vastly slowing down the calculations  
* A more scalable solution is as follows  
	* Move a subset of data in to RAM  
    * Process the subset  
    * Keep the results and discard the subset  
* Code may be slow due to complexity of calculations  
	* Consider the disk operations needed  
* Benchmarking using microbenchmark() can be critical  
  
Working with "out of core" objects using the Bigmemory Project:  
  
* Package "bigmemory" was written by Kane (instructor for this course) to store, manipulate, and process matrices exceeding RAM  
	* Core object is a big.matrix and it is designed to manage situations where disk space is much greater than RAM  
    * The process of moving data to RAM only when needed is called "out of core" processing  
* By default, a big.matrix keeps data on the disk, only moving the data to RAM as needed  
	* The movements to/from RAM are implicit, which is to say that they are managed by the package  
    * Only a single import is needed  
* The big.matrix is created using big.matrix(nrow=, ncol=, init=, type=, backingfile=, descriptorfile=)  
	* The nrow, ncol are the same as matrix(), while init is the initial value to assign everywhere and type is a quoted type such as "double" or "integer"  
    * The backingfile is a quoted file name that will hold the binary representation of the big.matrix on the disk, with extension .bin  
    * The descriptorfile is a quoted file name that will hold some metadata such as the number of rows/columns, name, and the like  
* Supposing that x is a big,matrix, then the default print(x) obtained by x on the command line is to show a few slots/pointers  
	* To have contents of x printed, use x[ , ]  
    * Assignments can be made using x[myRow, myColumn] <- myValue  
* The read.big.matrix() function is meant to look similar to read.table() but, in addition, needs to know:  
	* what type of numeric values you want to read ("char", "short", "integer", "double")  
    * name of the file that will hold the matrix's data (the backing file)  
    * name of the file to hold information about the matrix (a descriptor file)   
    * Result will be a file on the disk holding the value read in along with a descriptor file which holds extra information (like the number of columns and rows) about the resulting big.matrix object  
* A final advantage to using big.matrix is that if you know how to use R's matrices, then you know how to use a big.matrix  
	* You can subset columns and rows just as you would a regular matrix, using a numeric or character vector and the object returned is an R matrix  
    * Likewise, assignments are the same as with R matrices and after those assignments are made they are stored on disk and can be used in the current and future R sessions  
    * One thing to remember is that $ is not valid for getting a column of either a matrix or a big.matrix  
  
References vs. Copies:  
  
* Can subset and make assignments to a big.matrix much like a matrix  
* There are a few key differences between a big.matrix and a matrix  
	* big.matrix is stored on the disk (persists across R sessions, can be shared across R sessions)  
    * R typically makes copies during assignment, which is why changing a variable inside a function (playing with the copy) has no impact on the variable outside the function  
    * However, some objects such as environments are not copied, so modifying them inside a function modified them globally (outside the function) also  
    * The big.matrix is not copied, and is instead a reference object; thus, you have to explicitly request a copy, which means 1) you have more control, but 2) you need to be more careful  
* The reference vs. copy for big.matrix objects seems in some ways similar to Python  
	* a <- b will set a to reference the same data as b; changing a or changing b means changing both  
    * a <- deepcopy() will produce a copy of a and assign it to b; much like a = b[:] in Python  
  
Example code includes:  
```{r}

# Load the microbenchmark package
library(microbenchmark)

# Compare the timings for sorting different sizes of vector
mb <- microbenchmark(
  # Sort a random normal vector length 1e5
  "1e5" = sort(rnorm(1e5)),
  # Sort a random normal vector length 2.5e5
  "2.5e5" = sort(rnorm(2.5e5)),
  # Sort a random normal vector length 5e5
  "5e5" = sort(rnorm(5e5)),
  "7.5e5" = sort(rnorm(7.5e5)),
  "1e6" = sort(rnorm(1e6)),
  times = 10
)

# Plot the resulting benchmark object
plot(mb)


# Load the bigmemory package
library(bigmemory)

# Create the big.matrix object: x
x <- read.big.matrix("./RInputFiles/mortgage-sample.csv", header = TRUE, 
                     type = "integer", 
                     backingfile = "mortgage-sample.bin", 
                     descriptorfile = "mortgage-sample.desc")
    
# Find the dimensions of x
dim(x)


# Attach mortgage-sample.desc
mort <- attach.big.matrix("mortgage-sample.desc")

# Find the dimensions of mort
dim(mort)

# Look at the first 6 rows of mort
head(mort)


# Create mort
mort <- attach.big.matrix("mortgage-sample.desc")

# Look at the first 3 rows
mort[1:3, ]

# Create a table of the number of mortgages for each year in the data set
table(mort[, "year"])

a <- getLoadedDLLs()
length(a)

R.utils::gcDLLs()

a <- getLoadedDLLs()
length(a)

# Load the biganalytics package (error in loading to Knit file, works OK otherwise)
library(biganalytics)

# Get the column means of mort
colmean(mort)

# Use biganalytics' summary function to get a summary of the data
summary(mort)


# Use deepcopy() to create first_three
first_three <- deepcopy(mort, cols = 1:3, 
                        backingfile = "first_three.bin", 
                        descriptorfile = "first_three.desc")

# Set first_three_2 equal to first_three
first_three_2 <- first_three

# Set the value in the first row and first column of first_three to NA
first_three[1, 1] <- NA

# Verify the change shows up in first_three_2
first_three_2[1, 1]

# but not in mort
mort[1, 1]

```
  
  
  
***
  
Chapter 2 - Processing and Analyzing Data with bigmemory  
  
The Bigmemory Suite of Packages:  
  
* Many packages have been designed to work together with a big.matrix object  
	* biganalytics - summarizing  
    * bigtabulate - split and tabulate (includes the bigtable(x, quotedColumnVector))  
    * bigalgebra - linear algenra  
    * bigpca - PCA  
    * bigFastLM - linear regressions  
    * biglasso - lasso regressions  
    * bigrf - random forests  
  
* FHFA Dataset has data about millions of mortgages - difference in ownership rates, defaults, etc.  
	* Course will use a 70,000 record subset  
    * Raw data (full 2.5 GB dataset) available at FHFA (fhfa.gov)  
    * Code works the same on subsets and full data sets  
  
Split-Apply-Combine (aka Split-Compute-Combine), run in this course using split() Map() Reduce():  
  
* The split() function partitions the data, whether randomly or based on a factor variable  
	* split(myData, myFactor) will produce a list, with each element of the list containing the requested data (one per myFactor)  
* The Map() function processes each of the partitions  
	* Map(myFunction, mySplitList) will apply the myFunction to each of the items in the mySplitList, with the output a list named like mySplitList  
* The Reduce() function combines the (typically processed) data from a list  
	* Reduce(myFunction, myMapList) will apply the myFunction while combining the items in myMapList  
    * A common function might be rbind or '+' (add them up)  
  
Visualize results using tidyverse:  
  
* The pipe (%>%) operator works well with many of the big.matrix functions, since the first argument is a dataset  
* Can combine some of big.matrix processing outputs with standard packages like dplyr and tidyr and ggplot  
  
Limitations of bigmemory - process is useful for dense, numeric matrices that can be stored on hard disk:  
  
* Underlying structures are compatible with low-level linear algebra libraries for fast fitting  
* If you have different column types, you can try the ff package (similar to bigmemory but includes structures like a data.frame)  
* The bigmemory object is said to be "random access", which means it is equally easy to get access to any specific component  
* There are some big drawbacks to the "random access" capabilities, however  
	* Cannot add rows or columns - need to create an entirely new object and port over the relevant data  
    * Need enough disk space to hold the entire matrix in a block  
    * Can instead use other tools to process data using a "continuous chunks" approach - discussed in the next chapter  
  
Example code includes:  
```{r}

library(bigtabulate)
library(tidyr)
library(ggplot2)
library(biganalytics)
library(dplyr)


race_cat <- c('Native Am', 'Asian', 'Black', 'Pacific Is', 'White', 'Two or More', 'Hispanic', 'Not Avail')

# Call bigtable to create a variable called race_table
race_table <- bigtable(mort, "borrower_race")

# Rename the elements of race_table
names(race_table) <- race_cat
race_table


# Create a table of the borrower race by year
race_year_table <- bigtable(mort, c("borrower_race", "year"))

# Convert rydf to a data frame
rydf <- as.data.frame(race_year_table)

# Create the new column Race
rydf$Race <- race_cat

# Let's see what it looks like
rydf


female_residence_prop <- function(x, rows) {
    x_subset <- x[rows, ]
    # Find the proporation of female borrowers in urban areas
    prop_female_urban <- sum(x_subset[, "borrower_gender"] == 2 & 
                                 x_subset[, "msa"] == 1) / 
        sum(x_subset[, "msa"] == 1)
    # Find the proporation of female borrowers in rural areas
    prop_female_rural <- sum(x_subset[, "borrower_gender"] == 2 & 
                                 x_subset[, "msa"] == 0) / 
        sum(x_subset[, "msa"] == 0)
    
    c(prop_female_urban, prop_female_rural)
}

# Find the proportion of female borrowers in 2015
female_residence_prop(mort, mort[, "year"] == 2015)


# Split the row numbers of the mortage data by year
spl <- split(1:nrow(mort), mort[, "year"])

# Call str on spl
str(spl)


# For each of the row splits, find the female residence proportion
all_years <- Map(function(rows) female_residence_prop(mort, rows), spl)

# Call str on all_years
str(all_years)


# Collect the results as rows in a matrix
prop_female <- Reduce(rbind, all_years)

# Rename the row and column names
dimnames(prop_female) <- list(names(all_years), c("prop_female_urban", "prop_femal_rural"))

# View the matrix
prop_female


# Convert prop_female to a data frame
prop_female_df <- as.data.frame(prop_female)

# Add a new column Year
prop_female_df$Year <- row.names(prop_female_df)

# Call gather on prop_female_df
prop_female_long <- gather(prop_female_df, Region, Prop, -Year)

# Create a line plot
ggplot(prop_female_long, aes(x = Year, y = Prop, group = Region, color = Region)) + 
    geom_line()


# Call summary on mort
summary(mort)

bir_df_wide <- bigtable(mort, c("borrower_income_ratio", "year")) %>% 
    as.data.frame() %>% 
    tibble::rownames_to_column() %>% 
    filter(rowname %in% c(1, 2, 3)) %>% 
    select(-rowname) %>%
    # Create a new column called BIR with the corresponding table categories
    mutate(BIR = c(">=0,<=50%", ">50, <=80%", ">80%"))

bir_df_wide

bir_df_wide %>% 
    # Transform the wide-formatted data.frame into the long format
    gather(Year, Count, -BIR) %>%
    # Use ggplot to create a line plot
    ggplot(aes(x = Year, y = Count, group = BIR, color = BIR)) + 
    geom_line()

```
  
  
  
***
  
Chapter 3 - Working with iotools  
  
Introduction to chunk-wise processing - solution to challenges from bigmemory:  
  
* The iotools allows for processing the data in "chunks", allowing for data frames, data across many machines, and the like  
* Can process chunks either sequentially (keep as needed after each chunk runs) or independently  
	* Independent processing is typically harder to code (final result must be combined), but allows for parallel processing  
* Sometimes Split-Apply-Combine cannot be made to work, such as trying to find a median (even keeping some extra data per chunk -- such as sum and count when end goal is mean -- will not work)  
	* Fortunately, most regressions can be successfully run using the Split-Apply-Combine methodology  
* An operation that gives the same answer whether you apply it to an entire data set or to chunks of a data set and then on the results on the chunks is sometimes called foldable  
	* The max() and min() operations are an example of this  
  
First look at iotools: Importing data:  
  
* Basic components of chunk-wise processing include 1) load pieces of data, 2) convert to native objects, 3) perform computation and store results, and 4) repeated as needed until finished  
* Loading data often takes more time than processing the data (retrieval from disk and conversion to readable formats)  
* The iotools package is designed to separate the physical loading of data and the parsing of data in to R objects for better flexibility and performance  
	* readAsRaw() reads the entire data in to a raw vector  
    * read.chunk() reads the data in chunks in to a raw vector  
* The iotools can then parse the data in to either a matrix or a data frame  
	* mstrsplit() converts raw data in to a matrix  
    * dstrsplit() converts raw data in to a data frame  
    * read.delim.raw() = readAsRaw() + dstrsplit()  
* Processing contiguous chunks means there is no need to have read all the data in advance (such as to create the spl vector by 1:nrows by myVar)  
* When processing a sequence of contiguous chunks of data on a hard drive, iotools can turn a raw object into a data.frame or matrix while - at the same time - retrieving the next chunk of data  
    * These optimizations allow iotools to quickly process very large files  
  
Using chunk.apply - effectively moves away from what is functionally a "for loop" to allow better parallel processing:  
  
* iotools is the basis of hmr which allows for running R on TB of data using Hadoop  
* The general usage is chunk.apply(myFile=, myFunction=, CH.MAX.SIZE=)  # this will apply myFunction across chunks of size CH.MAX.SIZE in myFile  
	* Output will be a matrix where each row is one of the chunks and each column is one of (or the only) output from myFunction for that chunk  
    * There is an optional parallel= option; the argument supplied is the number of parallel clusters to be used  
* By default, chunk.apply() aggregates the processed data using the rbind() function  
	* This means that you can create a table from each of the chunks and then add up the rows of the resulting matrix to get the total counts for the table  
* When the parallel parameter is set to a value greater than one on Linux and Unix machine (including the Mac) multiple processes read and process data at the same time thereby reducing the execution time  
	* On Windows the parallel parameter is ignored  
  
Example code includes:  
```{r}

foldable_range <- function(x) {
  if (is.list(x)) {
    # If x is a list then reduce it by the min and max of each element in the list
    c(Reduce(min, x), Reduce(max, x))
  } else {
    # Otherwise, assume it's a vector and find it's range
    range(x)
  }
}

# Verify that foldable_range() works on the record_number column
foldable_range(mort[, "record_number"])


# Split the mortgage data by year
spl <- split(1:nrow(mort), mort[, "year"])

# Use foldable_range() to get the range of the record numbers
foldable_range(Map(function(s) foldable_range(mort[s, "record_number"]), spl))


# Load the iotools and microbenchmark packages
library(iotools)
library(microbenchmark)

# Time the reading of files
microbenchmark(
    # Time the reading of a file using read.delim five times
    read.delim("./RInputFiles/mortgage-sample.csv", header = FALSE, sep = ","),
    # Time the reading of a file using read.delim.raw five times
    read.delim.raw("./RInputFiles/mortgage-sample.csv", header = FALSE, sep = ","),
    times = 5
)


# Read mortgage-sample.csv as a raw vector
raw_file_content <- readAsRaw("./RInputFiles/mortgage-sample.csv")

# Convert the raw vector contents to a matrix
mort_mat <- mstrsplit(raw_file_content, sep = ",", type = "integer", skip = 1)

# Look at the first 6 rows
head(mort_mat)

# Convert the raw file contents to a data.frame
mort_df <- dstrsplit(raw_file_content, sep = ",", col_types = rep("integer", 16), skip = 1)

# Look at the first 6 rows
head(mort_df)


# We have created a file connection fc to the "mortgage-sample.csv" file and read in the first line to get rid of the header.
# Define the function to apply to each chunk
make_table <- function(chunk) {
    # Read each chunk as a matrix
    x <- mstrsplit(chunk, type = "integer", sep = ",")
    # Create a table of the number of borrowers (column 3) for each chunk
    table(x[, 3])
}

# Create a file connection to mortgage-sample.csv
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")

# Read the first line to get rid of the header
(col_names <- readLines(fc, n = 1))
(col_names <- lapply(str_split(col_names, '\\",\\"'), FUN=function(x) { str_replace(x, '\\"', '') })[[1]])

# Read the data in chunks
counts <- chunk.apply(fc, make_table, CH.MAX.SIZE = 1e5)

# Close the file connection
close(fc)

# Print counts
counts

# Sum up the chunks
colSums(counts)


msa_map <- c("rural", "urban")
# Define the function to apply to each chunk
make_msa_table <- function(chunk) {
    # Read each chunk as a data frame
    x <- dstrsplit(chunk, col_types = rep("integer", length(col_names)), sep = ",")
    # Set the column names of the data frame that's been read
    colnames(x) <- col_names
    # Create new column, msa_pretty, with a string description of where the borrower lives
    x$msa_pretty <- msa_map[x$msa + 1]
    # Create a table from the msa_pretty column
    table(x$msa_pretty)
}

# Create a file connection to mortgage-sample.csv
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")

# Read the first line to get rid of the header
readLines(fc, n = 1)

# Read the data in chunks
counts <- chunk.apply(fc, make_msa_table, CH.MAX.SIZE = 1e5)

# Close the file connection
close(fc)

# Aggregate the counts as before
colSums(counts)


iotools_read_fun <- function(parallel) {
    fc <- file("./RInputFiles/mortgage-sample.csv", "rb")
    readLines(fc, n = 1)
    chunk.apply(fc, make_msa_table,
                CH.MAX.SIZE = 1e5, parallel = parallel)
    close(fc)
}

# Benchmark the new function
microbenchmark(
    # Use one process
    iotools_read_fun(1), 
    # Use three processes
    iotools_read_fun(3), 
    times = 20
)


```
  
  
  
***
  
Chapter 4 - Case Study: Preliminary Analysis of Housing Data  
  
Overview of types of analysis for this chapter:  
  
* Compare proportions of people receiving mortgages  
* Amount of "missingness" in the data  
* Changes in 1) mortgage demographic proportions over time, and 2) city vs. rural mortgages, and 3) proportions of federally insured loans  
  
Are the data missing at random?  
  
* Missing data is pervasive, including in this housing dataset  
* Three components of missing data  
	* Missing Completely at Random (MCAR) - no way to predict where/what, meaning rows with missing data can just be dropped  
    * Missing at Random (MAR) - missingness is dependent on variables in the dataset, meaning that multiple imputation can be successful  
    * Missing Not At Random (MNAR) - typically due to deterministic relationships between missing data and other variables, beyond the scope of this course  
* Assumption for this exercise will be that data are checked for MAR and assumed to be MCAR if they are not MAR  
	* For each column, recode the column as a 1/0 for missing, then run a logit on all the other variables  
    * If the other variables have a statistically significant prediction effect on the 1/0 column, then that column is MAR rather than MCAR  
    * Need to have a smart p-value for significance depending on number of regressions that have been run  
  
Analyzing the Housing Data:  
  
* Adjusted counts - adjusting group sizes allows you to compare different groups as though they were the same size  
* Proportional change can show growth (or decline) of groups over time  
  
Borrower Lending Trends: City vs. Rural:  
  
* Looking at city (MSA == 1) vs rural  
* Looking at federally guaranteed loans  
	* Can use Borrower Income Ratio (borrower income divided by median income in the area)  
  
Wrap up:  
  
* Split-Compute-Combine (aka Split-Apply-Combine) as enabled by bigmemory and iotools  
* Operations can be run on a single machine in series, a single machine in parallel, or across multiple machines  
* Summary of the bigmemory approach  
    * Good for dense, large matrices that might otherwise overhwlem RAM  
    * Looks like a regular R matrix  
* Summary of the iotools approach:  
    * Good for much larger data that can be processed in sequential chunks  
    * More flexible than bigmemory in that it can handle data frames and files saved on multiple disks  
  
Example code includes:  
```{r}

# Create a table of borrower_race column
race_table <- bigtable(mort, "borrower_race")

# Rename the elements
names(race_table) <- race_cat[as.numeric(names(race_table))]

# Find the proportion
race_table[1:7] / sum(race_table[1:7])

mort_names <- col_names

# Create table of the borrower_race 
race_table_chunks <- chunk.apply(
    "./RInputFiles/mortgage-sample.csv", function(chunk) { 
        x <- mstrsplit(chunk, sep = ",", type = "integer") 
        colnames(x) <- mort_names 
        table(x[, "borrower_race"])
}, CH.MAX.SIZE = 1e5)

# Add up the columns
race_table <- colSums(race_table_chunks)

# Find the proportion
borrower_proportion <- race_table[1:7] / sum(race_table[1:7])

pop_proportion <- c(0.009, 0.048, 0.126, 0.002, 0.724, 0.029, 0.163)
names(pop_proportion) <- race_cat[1:7]
# Create the matrix
matrix(c(pop_proportion, borrower_proportion), byrow = TRUE, nrow = 2,
  dimnames = list(c("Population Proportion", "Borrower Proportion"), race_cat[1:7]))


# Create a variable indicating if borrower_race is missing in the mortgage data
borrower_race_ind <- mort[, "borrower_race"] == 9

# Create a factor variable indicating the affordability
affordability_factor <- factor(mort[, "affordability"])

# Perform a logistic regression
summary(glm(borrower_race_ind ~ affordability_factor, family = binomial))


# Open a connection to the file and skip the header
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")
readLines(fc, n = 1)

# Create a function to read chunks
make_table <- function(chunk) {
    # Create a matrix
    m <- mstrsplit(chunk, sep = ",", type = "integer")
    colnames(m) <- mort_names
    # Create the output table
    bigtable(m, c("borrower_race", "year"))
}

# Import data using chunk.apply
race_year_table <- chunk.apply(fc, make_table)

# Close connection
close(fc)

# Cast it to a data frame
rydf <- as.data.frame(race_year_table)

# Create a new column Race with race/ethnicity
rydf$Race <- race_cat


# Note: We removed the row corresponding to "Not Avail".
# View rydf
rydf <- 
    rydf %>% 
    filter(Race !="Not Avail")
rydf 

# View pop_proportion
pop_proportion

# Gather on all variables except Race
rydfl <- gather(rydf, Year, Count, -Race)

# Create a new adjusted count variable
rydfl$Adjusted_Count <- rydfl$Count / pop_proportion[rydfl$Race]

# Plot
ggplot(rydfl, aes(x = Year, y = Adjusted_Count, group = Race, color = Race)) + 
    geom_line()


# View rydf
rydf

# Normalize the columns
for (i in seq_len(nrow(rydf))) {
  rydf[i, 1:8] <- rydf[i, 1:8] / rydf[i, 1]
}

# Convert the data to long format
rydf_long <- gather(rydf, Year, Proportion, -Race)

# Plot
ggplot(rydf_long, aes(x = Year, y = Proportion, group = Race, color = Race)) + 
    geom_line()


# Open a connection to the file and skip the header
fc <- file("./RInputFiles/mortgage-sample.csv", "rb")
readLines(fc, n = 1)

# Create a function to read chunks
make_table <- function(chunk) {
    # Create a matrix
    m <- mstrsplit(chunk, sep = ",", type = "integer")
    colnames(m) <- mort_names
    # Create the output table
    bigtable(m, c("msa", "year"))
}

# Import data using chunk.apply
msa_year_table <- chunk.apply(fc, make_table)

# Close connection
close(fc)

# Convert to a data frame
df_msa <- as.data.frame(msa_year_table)

# Rename columns
df_msa$MSA <- c("rural", "city")

# Gather on all columns except Year
df_msa_long <- gather(df_msa, Year, Count, -MSA)

# Plot 
ggplot(df_msa_long, aes(x = Year, y = Count, group = MSA, color = MSA)) + 
    geom_line()


# Tabulate borrower_income_ratio and federal_guarantee
ir_by_fg <- bigtable(mort, c("borrower_income_ratio", "federal_guarantee"))

# Label the columns and rows of the table
income_cat <- c('0 <= 50', '50 < 80', '> 80', 'Not Applicable')
guarantee_cat <- c('FHA/VA', 'RHS', 'HECM', 'No Guarantee')
dimnames(ir_by_fg) <- list(income_cat, guarantee_cat)

# For each row in ir_by_fg, divide by the sum of the row
for (i in seq_len(nrow(ir_by_fg))) {
  ir_by_fg[i, ] = ir_by_fg[i, ] / sum(ir_by_fg[i, ])
}

# Print
ir_by_fg


# Quirky fix so that the files can be used again later
rm(mort)
rm(x)
rm(first_three)
rm(first_three_2)
gc()

```
  
  
  
***
  
###_Working with Web Data in R_  
  
Chapter 1 - Downloading Files and Using API Clients  
  
Introduction: Working with Web Data in R:  
  
* Methods for getting data from the internet in to R - frequently automatic, such as giving an internet address to read.csv()  
* Using the httr package (tidyverse) to query API using GET() and POST()  
* Using JSON and XML formats (nested data structures)  
* CSS (cascading style sheets) for extracts  
* Can use download.file() so that there is no need for repeatedly querying the same remote files  
* You could use write.table(), but then you have to worry about accidentally writing out data in a format R can't read back in  
	* An easy way to avoid this risk is to use saveRDS() and readRDS(), which save R objects in an R-specific file format, with the data structure intact  
    * That means you can use it for any type of R object (even ones that don't turn into tables easily), and not worry you'll lose data reading it back in  
    * saveRDS() takes two arguments, object, pointing to the R object to save and file pointing to where to save it to  
    * readRDS() expects file, referring to the path to the RDS file to read in  
  
Understanding Application Programming Interfaces (API) - automatically handling data changes:  
  
* Data are frequently made available by way of API  
	* "websites, but for machines", allowing you to query/download data automatically  
* R has several API interaction capabilities  
	* Native interfaces to API  
    * Hides API complexity  
    * Allows for reading data as R object  
* Can find R packages for API by googling CRAN - packages frequently exist already  
	* Example is library(pageviews) to get pageview counts  
  
Access tokens and API:  
  
* API cients (by way of R packages) abstract away the complications of getting the data  
* The API owner frequently does care how your API client interacts with it, though  
	* Overwhelming API causes problems for owner and many users  
    * Access tokens are sometimes used to monitor and throttle usage  
* Getting access tokens is frequently straightforward  
	* Usually requires registering an e-mail address  
    * Sometimes requires an explanation  
    * Example is www.wordnik.com, which can be accessed by way of library(bidnik)  
  
Example code includes:  
```{r cache=TRUE}

# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"

# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)

# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)

# Examine the objects with head()
head(csv_data)
head(tsv_data)


# Download the file with download.file()
download.file(url = csv_url, destfile = "./RInputFiles/feed_data.csv")

# Read it in with read.csv()
csv_data <- read.csv("./RInputFiles/feed_data.csv")


# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight ** 2

# Save it to disk with saveRDS()
saveRDS(csv_data, "./RInputFiles/modified_feed_data.RDS")

# Read it back in with readRDS()
modified_feed_data <- readRDS("./RInputFiles/modified_feed_data.RDS")

# Examine modified_feed_data
str(modified_feed_data)


# Load pageviews
# library(pageviews)

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- pageviews::article_pageviews(project = "en.wikipedia", "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)


# Load birdnik
# library(birdnik)

# Get the word frequency for "vector", using api_key to access it
# vector_frequency <- word_frequency(api_key, "vector")


```

	

  



	


	
	
	
	
	
	


