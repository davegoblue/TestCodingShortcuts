---
title: "Associative Arrays (Hash)"
author: "davegoblue"
date: "May 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary  
Lookups can be very time-consuming for a large number of elements (time can be ~ N^2).  Associative arrays manage this with a hash table, which is a very convenient (and fast) way for computers to manage lookups.  
  
This program tests the difference in lookup times for simplified examples.  Because there is added complexity to creating and managing hash tables, this technique makes more sense for routines where lookup time is a significant component of a medium-long overall run time.

## Analysis  
###_Background_  
I became interested in this idea when I learned about dictionaries in Python.  The dictionary is more or less the (hashable) associative array in Python, with each key associated to a single value.  The claim from a reputable source is that lookup times in a hash table are more or less independent of the size of the associative array (time ~ 1 per lookup meaning time ~N for N lookups).  
  
I found an article about associative arrays in R at <https://gavinband.wordpress.com/2011/04/06/associative-arrays-in-r/> and decided to explore the topic further.

###_Baseline_  
For baseline processing times, I copied the approach from the gavinband article referenced above.  These runs are done in "standard R", which is to say that the process is not run in parallel and can access only 25% of the CPU.  The comparison runs will follow the same procedure:  
```{r}
baseTime <- proc.time()
maxN <- 20000
myList <- list()

for (intCtr in 1:maxN) {
    myList[[sprintf("key%d", intCtr)]] <- intCtr
}

print("Elapsed time: ")
print(proc.time() - baseTime)

print("QC only:")
testVals <- sample(1:maxN, 10, replace=FALSE)
print(testVals)
print(myList[testVals])

```
  
Setting up the 20,000 item list takes 5-10 seconds, so it is not all that fast.  Now, we pull 20,000 draws at random and measure the time required:
```{r}
set.seed(1606200859)

myVals <- sample(1:maxN, maxN, replace=TRUE)  # Make maxN draws, allowing for replacement

## Initialize timing after deciding which items to draw
baseTime <- proc.time()

for (eachVal in myVals) {
    myDummy <- myList[[sprintf("key%d", eachVal)]]
}

print("Elapsed time for lookups: ")
print(proc.time() - baseTime)

```

This lookup takes 2-4 seconds for 20,000 items, which is again not stupendously fast for a very modest amount of data.  As the lookup starts to become more complex, run times could be an issue.

###_Hash Function_  
A hash function is written to roughly match the gavinband article, and run with similar parameters:  
```{r}
makeHash <- function(myKeys) {
    result <- new.env(hash=TRUE, parent=emptyenv(), size=length(myKeys))
    for (eachKey in myKeys) {
        result[[eachKey]] <- eachKey
    }
    return(result)
}

baseTime <- proc.time()
hashList <- makeHash(myKeys=sprintf("key%d", 1:maxN))

for (intCtr in 1:maxN) {
    hashList[[sprintf("key%d", intCtr)]] <- intCtr
}

print("Elapsed time: ")
print(proc.time() - baseTime)

print("QC only:")
print(testVals)
print(as.list(hashList)[testVals])

```
  
Note that the setup time has been reduced to ~0.1 seconds.  Building the hash table is ~30x faster.  
  
The proof in the pudding though is the lookup time for the hash table - how well does it perform in this example:  
```{r}

str(myVals)  # Use same items as previous

## Initialize timing after deciding which items to draw
baseTime <- proc.time()

for (eachVal in myVals) {
    myDummy <- hashList[[sprintf("key%d", eachVal)]]
}

print("Elapsed time for lookups: ")
print(proc.time() - baseTime)
```

The lookup time has been reduced to ~0.1 seconds, again showing the hash table to be ~30x faster.  This is very intriguing and merits further exploration and self-learning!  

###_Use R like Python Dictionary?_  
Python has a nice dictionary feature that runs very quickly.  Below is a test to see whether R can be used in a similar fashion.  First, the "mbox.txt" file is loaded from "../../../PythonDirectory/BasicPython_v001/":  
```{r}

pyDir <- "../../../PythonDirectory/BasicPython_v001/"
pyFile <- "mbox.txt"

emailLines <- readLines(paste0(pyDir, pyFile))
str(emailLines)

addrLines <- emailLines[grep("[@]",emailLines)]
str(addrLines)

addrWords <- character(0)
for (eachLine in addrLines) { 
    keyWords <- strsplit(eachLine," ")
    for (eachWord in keyWords[[1]]) { 
        if(grepl("[@]", eachWord)) { 
            addrWords <- c(addrWords, eachWord) 
        } 
    } 
}
str(addrWords)

```
  
So, addrWords is now a character string containing 22,019 e-mail addresses, formatted to varying degrees of cleanliness.  Suppose we want to create the count of each clean element in the vector using base R without associative arrays:  
```{r, cache=TRUE}

# First, make a small subset of the emailFile consisting of just e-mail addresses
emailSmall <- gsub("[<>();]","",addrWords)

# Create a blank data frame of the appropriate maximum length and type
ctWords <- data.frame(eachWord=character(length(emailSmall)), 
                      ctWord=integer(length(emailSmall)), stringsAsFactors = FALSE
                      )

nextBlank <- 1

startTime <- proc.time()

# Loop through each of the words in the emailVector
for (allWords in emailSmall) {
    curMatch <- which(ctWords$eachWord == allWords)
    if (length(curMatch) > 0) {
        ctWords[curMatch,"ctWord"] = ctWords[curMatch,"ctWord"] + 1
    } else {
        ctWords[nextBlank, "eachWord"] <- allWords
        ctWords[nextBlank, "ctWord"] <- 1
        nextBlank <- nextBlank + 1
    }
}

proc.time() - startTime

```
  
Just taking the 22,000 words and trying to match them to a length 22,000 data frame is a rather time-consuming exercise taking ~20 seconds.  The process tends to scale as O(N^2), so a full 400k word lookup could take ~2 hours!  Hopefully, hashing can improve on this:  
```{r}

myWordHash <- new.env(hash=TRUE, parent=emptyenv(), size=length(emailSmall))

startTime <- proc.time()

# Loop through each of the words in the emailVector
for (allWords in emailSmall) {
    if (length(myWordHash[[allWords]]) > 0) {
        myWordHash[[allWords]] <- myWordHash[[allWords]] + 1
    } else {
        myWordHash[[allWords]] <- 1
    }

}

proc.time() - startTime

misMatch <- 0

# Next a check is run to see if we have the same answers as ctWords
for (intCtr in 1:nrow(ctWords)) {
    if (ctWords[intCtr, "ctWord"] == 0) { break }
    
    if (myWordHash[[ ctWords[intCtr, "eachWord"] ]] != ctWords[intCtr, "ctWord"]) {
        misMatch <- misMatch + 1
    }
}

print(paste0("Total mismatches: ", misMatch, " of ", intCtr))

proc.time() - startTime

```
  
So, the hash table creates and then validates the same information in ~0.2 seconds.  The next step is to figure out how to query the new environment so that I can use the hashed data!
  
The names() command applied to a hashed environment works in a manner that pulls down the keys, with the hash table containing the values.  So, for example:  
```{r}

startTime <- proc.time()

eNames <- names(myWordHash)
eCts <- integer(0)

for (eachName in eNames) { 
    eCts <- c(eCts, myWordHash[[eachName]]) 
}

dfOrder <- order(-eCts, eNames)

proc.time() - startTime

sum( ctWords[order(-ctWords$ctWord, ctWords$eachWord), ][ctWords$eachWord != "", ] != 
     data.frame(eachWord = eNames, ctWord=eCts)[dfOrder, ] )

```
  
This works at roughly the same speed as Python, or ~0.1 seconds.  The data are identical, suggesting there is good promise in clever use of hashed environments to serve as an R key-value dictionary.  
  
###_Lookups for Character Vectors_  
Character vectors can be particularly difficult to look-up.  An interesting and related question is what would be the lookup time (data frame vs. hashed environment) for 20^4 drawn from LETTERS[1:20]?  First, the experiment is run with a data frame:  
```{r}

set.seed(1606211532)

nRuns <- 20^4
let20 <- LETTERS[1:20]

baseGrid <- expand.grid(let20, let20, let20, let20, stringsAsFactors=FALSE)
charFrame <- data.frame(frameKey=apply(baseGrid, 1, FUN=paste0, collapse=""), 
                        frameValue=1:nRuns, 
                        stringsAsFactors = FALSE
                        )

nTrials <- 2000
sampNums <- sample(length(charFrame$frameKey), nTrials, replace=TRUE)
keysLook <- charFrame$frameKey[sampNums]
valsFind <- integer(nTrials)
```
  
The below chunk is merely a continuation of the above chunk, split apart so that the random seed and setup need not be cached even while the longer lookup is:  
```{r, cache=TRUE}
# Run the process using the data frame lookup
startTime <- proc.time()

ctCur <- 1
for (eachKey in keysLook) {
    valsFind[ctCur] <- charFrame$frameValue[charFrame$frameKey==eachKey]
    ctCur <- ctCur + 1
}

proc.time() - startTime

# Run the process using the integer index lookup
startTime <- proc.time()

ctCur <- 1
valsFindIdx <- integer(nTrials)
for (eachIndex in sampNums) {
    valsFindIdx[ctCur] <- charFrame$frameValue[eachIndex]
    ctCur <- ctCur + 1
}

proc.time() - startTime

identical(valsFind, valsFindIdx)

```
  
This serves as another example of the power of the relational (index or hash) philosophy.  Running the text lookup 2,000 times against a 160,000 row table takes ~20 seconds, while doing the same work while knowing the index number to find takes ~0.1 seconds.  
  
A similar process is then run using a hashed environment in place of the 160,000 x 2 charFrame data frame:  
```{r}

myCharHash <- new.env(hash=TRUE, parent=emptyenv(), size=nrow(charFrame))

# Set up the hash table
startTime <- proc.time()

for (intCtr in 1:nrow(charFrame)) {
    myCharHash[[ charFrame$frameKey[intCtr] ]] <- charFrame$frameValue[intCtr]
}

proc.time() - startTime


# Use the hash table, same as above
startTime <- proc.time()

ctCur <- 1
valsFindHash <- integer(nTrials)  # Pulls nTrials from previous chunk

# Will use the same keysLook as previous chunk
for (eachKey in keysLook) {
    valsFindHash[ctCur] <- myCharHash[[ eachKey ]]
    ctCur <- ctCur + 1
}

proc.time() - startTime

identical(valsFind, valsFindHash)

```
  
This example nicely illustrates the trade-offs of the hashing process.  Creating the hash table incurs a one-time cost of 5-10 seconds.  In exchange, the 2,000 lookups take only 0.1 seconds (roughly the same as the relational index method) rather than ~20 seconds.  So, if there will be many lookups, the hash table is a great way to implement the relational concept while still being able to use characters for lookup.  
  
###_Catloguing words in a document_  
One common use of dictionaries is to catalogue the words in a document.  This requires several steps:  
  
* Read the text (typically line by line)  
* Strip/separate by white space (get rid of \t and \n and "   " and etc.)  
* Strip out the punctuation (alternately, regex to keep just the alphanumeric)  
* Convert everything to the same case (usually lower), and place it in the dictionary with counts  
  
Python is excellent at this, and the goal of the below is to get R to perform "like Python" on the romeo_full.txt file from the Severance Coursera course on Python.  First, the Python code is shown commented, then the R code is written:  
```{r}

# import string
# import re
# 
# 
# wordDict = dict()
# fileName = "romeo-full.txt"
# fileHandle = open(fileName, 'r')
# for eachLine in fileHandle:
#     lineWords = eachLine.strip().translate(None, string.punctuation).lower().split()
#     for eachWord in lineWords:
#         wordDict[eachWord] = wordDict.get(eachWord, 0) + 1
# 
# tupleList = list()
# for key, val in wordDict.items():
#     tupleList.append((val, key))
# tupleList.sort(reverse=True)
# print tupleList[0:19]
# 
# The above will print the 19 most popular words (lower-case, puncuation removed) in romeo-full.txt
# 

romeoLines <- readLines("romeo-full.txt")
str(romeoLines)

startTime <- proc.time() 

myRomeoHash <- new.env(hash=TRUE, parent=emptyenv(), size=20*length(romeoLines))

for (eachLine in romeoLines) {
    # Trim the line, split by 1+ whitespaces, make it lowercase, and get rid of the punctuation
    modLine <- gsub("[[:punct:]]", "", tolower(strsplit(trimws(eachLine), "\\s+")[[1]] ) )
    # Establish or increment the hashed dictionary
    for (eachWord in modLine) {
        myRomeoHash[[eachWord]] <- 1 + ifelse(length(myRomeoHash[[eachWord]]) > 0, 
                                              myRomeoHash[[eachWord]], 0
                                              )
    }
}

romeoKeys <- names(myRomeoHash)
romeoVals <- integer(0)

for (eachKey in romeoKeys) { 
    romeoVals <- c(romeoVals, myRomeoHash[[eachKey]]) 
}

dfOrder <- order(-romeoVals, romeoKeys)

print(data.frame(key=romeoKeys[dfOrder], val=romeoVals[dfOrder])[1:19, 2:1])

print(proc.time() - startTime)

```
  
The program runs at a reasonable clip, taking ~0.3 seconds to create the dictionary and print the list.  Python is essentially instantaneous, so there is some lag even with this small file.  It will be nice to test this further on a larger dataset.  
  
###_Catloguing words in a document (part 2)_  
A larger sample file "Tom Sawyer" was downloaded from the Gutenberg project for continued exploration of hash tables.  While a larger file (~0.5 MB), it still runs instantaneously in Python.  The source for the document is <http://www.gutenberg.org/ebooks/74>.  
  
The hashing routine is converted to a function, and re-run with the new data:  
```{r}

# Declare the function
hashWordCount <- function(useFile) {
    
    myWordHash <- new.env(hash=TRUE, parent=emptyenv(), size=10*length(useFile))
    
    for (eachLine in useFile) {
        # Trim the line, split by 1+ whitespaces, make it lowercase, and get rid of the punctuation
        modLine <- gsub("[[:punct:]]", "", tolower(strsplit(trimws(eachLine), "\\s+")[[1]] ) )
        # Establish or increment the hashed dictionary
        for (eachWord in modLine) {
            if (nchar(eachWord) == 0) { next }
            # print(paste(eachWord, nchar(eachWord)))
            myWordHash[[eachWord]] <- 1 + ifelse(length(myWordHash[[eachWord]]) > 0, 
                                                 myWordHash[[eachWord]], 0
                                                 )
        }
    }
    
    return(myWordHash)
    
}


# Read in the book
sawyerLines <- readLines("tomsawyer_74.txt")
str(sawyerLines)


# Run the hashing function
startTime <- proc.time() 
sawyerHash <- hashWordCount(sawyerLines)
print(proc.time() - startTime)


# Find the names and get the sort order
sawyerKeys <- names(sawyerHash)
sawyerVals <- integer(0)

for (eachKey in sawyerKeys) { 
    sawyerVals <- c(sawyerVals, sawyerHash[[eachKey]]) 
}

dfOrder <- order(-sawyerVals, sawyerKeys)

print(data.frame(key=sawyerKeys[dfOrder], val=sawyerVals[dfOrder])[1:19, 2:1])

hashTime <- proc.time() - startTime
print(hashTime)

```
  
While Python is essentially instantaeous at this task, R takes ~4-5 seconds.  The hash table is a significant improvement in the run-time, but there is still a cost relative to the effortless nature in Python.  This could become more significant as file sizes increase.  

For comparison, the routine is run using more standard character matching for a comparison of timing:  
```{r, cache=TRUE}

sKeys <- character(0)
sVals <- integer(0)


# Look at the stand-alone time for the string operations
startTime <- proc.time()
for (eachLine in sawyerLines) {
    # Trim the line, split by 1+ whitespaces, make it lowercase, and get rid of the punctuation
    modLine <- gsub("[[:punct:]]", "", tolower(strsplit(trimws(eachLine), "\\s+")[[1]] ) )
}
stringTime <- proc.time() - startTime
print(stringTime)


# Look at the combined time for the string operations AND the dictionary/hashing
startTime <- proc.time()
for (eachLine in sawyerLines) {
    # Trim the line, split by 1+ whitespaces, make it lowercase, and get rid of the punctuation
    modLine <- gsub("[[:punct:]]", "", tolower(strsplit(trimws(eachLine), "\\s+")[[1]] ) )
    # Establish or increment the keys and values dictionary
    for (eachWord in modLine) {
        if (nchar(eachWord) == 0) { next }
        idxMatch <- which(sKeys == eachWord)
        if (length(idxMatch) == 0) {
            sKeys <- c(sKeys, eachWord)
            sVals <- c(sVals, 1)
        } else {
            sVals[idxMatch] <- sVals[idxMatch] + 1
        }
    }
}
combTime <- proc.time() - startTime
print(combTime)


```
  
The string operations take ~3 seconds, while the integrated string operations and character matching take ~25 seconds and the integrated string operations and hashing take ~5 seconds.  Interpolating, the hashing appears to drive an order-of-magnitude improvement in run times, though still taking much longer than Python.  
  
###_English Language Dictionary Sample_  
Next, a version of the dictionary is read from <https://raw.githubusercontent.com/sujithps/Dictionary/master/Oxford%20English%20Dictionary.txt>.  This file has a few features of interest:  
  
* There is a blank line between every word  
* Some of the words are suffix/prefix and contain a -  
* Some of the words are actually two words  
* Etc.  
  
As a first step, the file is read, blank lines and lines with less than 1 space (2 words) deleted, and a table created of the second entry:  
```{r, cache=TRUE}
oedLines <- readLines("OED_from_Github.txt")
str(oedLines)
oedTrim <- trimws(oedLines)

foo <- function(x) { length(x) > 1 }
oedContent <- oedTrim[sapply(strsplit(oedTrim,"\\s+"), FUN=foo)]

fooTwo <- function(x) { strsplit(x, "\\s+")[[1]][2] }
keyTable <- table( sapply(oedContent, FUN=fooTwo) )
print(keyTable[order(-keyTable)][1:30])
print(sum(keyTable[order(-keyTable)][1:30])/sum(keyTable))
```
  
As a first pass, only entries flagged as "n.", "adj.", "v.", "adv.", "prep.", "conj.", and "pron." will be used.  These will be single words that are flagged as noun, adjective, verb, adverb, preposition, conjugation, and pronoun:  
```{r}
# fooThree <- function(x) { (strsplit(x, "\\s+")[[1]][2] %in% c("n.", "adj.", "v.", "adv.") ) }
fooThree <- function(x) { 
    length(grepRaw("[vn]\\.|ad[jv]\\.|prep\\.|pron\\.|conj\\.", strsplit(x, "\\s+")[[1]][2])) > 0 
}
oedSubset <- oedContent[ sapply(oedContent, FUN=fooThree) ]
str(oedSubset)

```
  
And so, there will be ~29k words to explore as a dictionary.  As before, the hashed environment is created, with each word added to it:  
```{r}
oedDictHash <- new.env(hash=TRUE, parent=emptyenv(), size=round(1.5 * length(oedSubset), 0) )

for (eachLine in oedSubset) {
    
    oedWord <- strsplit(eachLine,"\\s+")[[1]][1]
    oedDef <- paste(strsplit(eachLine,"\\s+")[[1]][-1], collapse=" ")    
    
    # oedWord <- strsplit(trimws(eachLine),"\\s+")[[1]][1]
    # oedDef <- paste(strsplit(trimws(eachLine),"\\s+")[[1]][-1], collapse=" ")
    
    oedDictHash[[ tolower(oedWord) ]] <- oedDef
}

length(names(oedDictHash))

```
  
The dictionary contains ~28k words, roughly 3% less than was originally read in.  This suggests there are some duplicates in the oed file, and only the LAST definition entered is stored.  
  
Next, the words in the sawyer dictionary are assessed, to see how many of them have been included:  
```{r}
sawyerMatch <- logical(0)
for (eachKey in sawyerKeys) {
    sawyerMatch <- c(sawyerMatch, length(oedDictHash[[eachKey]]) > 0)
}
sum(sawyerMatch) / length(sawyerMatch)
sum(sawyerMatch * sawyerVals) / sum(sawyerVals)

print(data.frame(key=sawyerKeys[dfOrder], val=sawyerVals[dfOrder], 
                 inDict=sawyerMatch[dfOrder]
                 )[1:19, c(2:1, 3)])
```
  
This initial pass matches ~40% of the unique words in Tom Sawyer (~60% of the words weighted by word usage), but misses some common words.  Some very common words are missing, and it is surprising that 60% of Twain's unique words cannot be found.  An area to explore further.  