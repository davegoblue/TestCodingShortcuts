---
title: "Associative Arrays (Hash)"
author: "davegoblue"
date: "May 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary  
Lookups can be very time-consuming for a large number of elements (time can be ~ N^2).  Associative arrays manage this with a hash table, which is a very convenient (and fast) way for computers to manage lookups.  
  
This program tests the difference in lookup times for simplified examples.  Because there is added complexity to creating and managing hash tables, this technique makes more sense for routines where lookup time is a significant component of a medium-long overall run time.

## Analysis  
###_Background_  
I became interested in this idea when I learned about dictionaries in Python.  The dictionary is more or less the (hashable) associative array in Python, with each key associated to a single value.  The claim from a reputable source is that lookup times in a hash table are more or less independent of the size of the associative array (time ~ 1 per lookup meaning time ~N for N lookups).  
  
I found an article about associative arrays in R at <https://gavinband.wordpress.com/2011/04/06/associative-arrays-in-r/> and decided to explore the topic further.

###_Baseline_  
For baseline processing times, I copied the approach from the gavinband article referenced above.  These runs are done in "standard R", which is to say that the process is not run in parallel and can access only 25% of the CPU.  The comparison runs will follow the same procedure:  
```{r}
baseTime <- proc.time()
maxN <- 20000
myList <- list()

for (intCtr in 1:maxN) {
    myList[[sprintf("key%d", intCtr)]] <- intCtr
}

print("Elapsed time: ")
print(proc.time() - baseTime)

print("QC only:")
testVals <- sample(1:maxN, 10, replace=FALSE)
print(testVals)
print(myList[testVals])

```
  
Setting up the 20,000 item list takes ~6 seconds, so it is not all that fast.  Now, we pull 20,000 draws at random and measure the time required:
```{r}
myVals <- sample(1:maxN, maxN, replace=TRUE)  # Make maxN draws, allowing for replacement

## Initialize timing after deciding which items to draw
baseTime <- proc.time()

for (eachVal in myVals) {
    myDummy <- myList[[sprintf("key%d", eachVal)]]
}

print("Elapsed time for lookups: ")
print(proc.time() - baseTime)

```

This lookup takes ~2.5 seconds for 20,000 items, which is again not stupendously fast for a very modest amount of data.  As the lookup starts to become more complex, run times could be an issue.

###_Hash Function_  
A hash function is written to roughly match the gavinband article, and run with similar parameters:  
```{r}
makeHash <- function(myKeys) {
    result <- new.env(hash=TRUE, parent=emptyenv(), size=length(myKeys))
    for (eachKey in myKeys) {
        result[[eachKey]] <- eachKey
    }
    return(result)
}

baseTime <- proc.time()
hashList <- makeHash(myKeys=sprintf("key%d", 1:maxN))

for (intCtr in 1:maxN) {
    hashList[[sprintf("key%d", intCtr)]] <- intCtr
}

print("Elapsed time: ")
print(proc.time() - baseTime)

print("QC only:")
print(testVals)
print(as.list(hashList)[testVals])

```
  
Note that the setup time has been reduced to ~0.2 seconds.  Building the hash table is ~30x faster.  
  
The proof in the pudding though is the lookup time for the hash table - how well does it perform in this example:  
```{r}

str(myVals)  # Use same items as previous

## Initialize timing after deciding which items to draw
baseTime <- proc.time()

for (eachVal in myVals) {
    myDummy <- hashList[[sprintf("key%d", eachVal)]]
}

print("Elapsed time for lookups: ")
print(proc.time() - baseTime)
```

The lookup time has been reduced to ~0.1 seconds, again showing the hash table to be ~30x faster.  This is very intriguing and merits further exploration and self-learning!  
