---
title: "Associative Arrays (Hash)"
author: "davegoblue"
date: "May 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary  
Lookups can be very time-consuming for a large number of elements (time can be ~ N^2).  Associative arrays manage this with a hash table, which is a very convenient (and fast) way for computers to manage lookups.  
  
This program tests the difference in lookup times for simplified examples.  Because there is added complexity to creating and managing hash tables, this technique makes more sense for routines where lookup time is a significant component of a medium-long overall run time.

## Analysis  
###_Background_  
I became interested in this idea when I learned about dictionaries in Python.  The dictionary is more or less the (hashable) associative array in Python, with each key associated to a single value.  The claim from a reputable source is that lookup times in a hash table are more or less independent of the size of the associative array (time ~ 1 per lookup meaning time ~N for N lookups).  
  
I found an article about associative arrays in R at <https://gavinband.wordpress.com/2011/04/06/associative-arrays-in-r/> and decided to explore the topic further.

###_Baseline_  
For baseline processing times, I copied the approach from the gavinband article referenced above.  These runs are done in "standard R", which is to say that the process is not run in parallel and can access only 25% of the CPU.  The comparison runs will follow the same procedure:  
```{r}
baseTime <- proc.time()
maxN <- 20000
myList <- list()

for (intCtr in 1:maxN) {
    myList[[sprintf("key%d", intCtr)]] <- intCtr
}

print("Elapsed time: ")
print(proc.time() - baseTime)

print("QC only:")
testVals <- sample(1:maxN, 10, replace=FALSE)
print(testVals)
print(myList[testVals])

```
  
Setting up the 20,000 item list takes 5-10 seconds, so it is not all that fast.  Now, we pull 20,000 draws at random and measure the time required:
```{r}
set.seed(1606200859)

myVals <- sample(1:maxN, maxN, replace=TRUE)  # Make maxN draws, allowing for replacement

## Initialize timing after deciding which items to draw
baseTime <- proc.time()

for (eachVal in myVals) {
    myDummy <- myList[[sprintf("key%d", eachVal)]]
}

print("Elapsed time for lookups: ")
print(proc.time() - baseTime)

```

This lookup takes 2-4 seconds for 20,000 items, which is again not stupendously fast for a very modest amount of data.  As the lookup starts to become more complex, run times could be an issue.

###_Hash Function_  
A hash function is written to roughly match the gavinband article, and run with similar parameters:  
```{r}
makeHash <- function(myKeys) {
    result <- new.env(hash=TRUE, parent=emptyenv(), size=length(myKeys))
    for (eachKey in myKeys) {
        result[[eachKey]] <- eachKey
    }
    return(result)
}

baseTime <- proc.time()
hashList <- makeHash(myKeys=sprintf("key%d", 1:maxN))

for (intCtr in 1:maxN) {
    hashList[[sprintf("key%d", intCtr)]] <- intCtr
}

print("Elapsed time: ")
print(proc.time() - baseTime)

print("QC only:")
print(testVals)
print(as.list(hashList)[testVals])

```
  
Note that the setup time has been reduced to ~0.1 seconds.  Building the hash table is ~30x faster.  
  
The proof in the pudding though is the lookup time for the hash table - how well does it perform in this example:  
```{r}

str(myVals)  # Use same items as previous

## Initialize timing after deciding which items to draw
baseTime <- proc.time()

for (eachVal in myVals) {
    myDummy <- hashList[[sprintf("key%d", eachVal)]]
}

print("Elapsed time for lookups: ")
print(proc.time() - baseTime)
```

The lookup time has been reduced to ~0.1 seconds, again showing the hash table to be ~30x faster.  This is very intriguing and merits further exploration and self-learning!  

###_Use R like Python Dictionary?_  
Python has a nice dictionary feature that runs very quickly.  Below is a test to see whether R can be used in a similar fashion.  First, the "mbox.txt" file is loaded from "../../../PythonDirectory/BasicPython_v001/":  
```{r}

pyDir <- "../../../PythonDirectory/BasicPython_v001/"
pyFile <- "mbox.txt"

emailLines <- readLines(paste0(pyDir, pyFile))
str(emailLines)

addrLines <- emailLines[grep("[@]",emailLines)]
str(addrLines)

addrWords <- character(0)
for (eachLine in addrLines) { 
    keyWords <- strsplit(eachLine," ")
    for (eachWord in keyWords[[1]]) { 
        if(grepl("[@]", eachWord)) { 
            addrWords <- c(addrWords, eachWord) 
        } 
    } 
}
str(addrWords)

```
  
So, addrWords is now a character string containing 22,019 e-mail addresses, formatted to varying degrees of cleanliness.  Suppose we want to create the count of each clean element in the vector using base R without associative arrays:  
```{r, cache=TRUE}

# First, make a small subset of the emailFile consisting of just e-mail addresses
emailSmall <- gsub("[<>();]","",addrWords)

# Create a blank data frame of the appropriate maximum length and type
ctWords <- data.frame(eachWord=character(length(emailSmall)), 
                      ctWord=integer(length(emailSmall)), stringsAsFactors = FALSE
                      )

nextBlank <- 1

startTime <- proc.time()

# Loop through each of the words in the emailVector
for (allWords in emailSmall) {
    curMatch <- which(ctWords$eachWord == allWords)
    if (length(curMatch) > 0) {
        ctWords[curMatch,"ctWord"] = ctWords[curMatch,"ctWord"] + 1
    } else {
        ctWords[nextBlank, "eachWord"] <- allWords
        ctWords[nextBlank, "ctWord"] <- 1
        nextBlank <- nextBlank + 1
    }
}

proc.time() - startTime

```
  
Just taking the 22,000 words and trying to match them to a length 22,000 data frame is a rather time-consuming exercise taking ~20 seconds.  The process tends to scale as O(N^2), so a full 400k word lookup could take ~2 hours!  Hopefully, hashing can improve on this:  
```{r}

myWordHash <- new.env(hash=TRUE, parent=emptyenv(), size=length(emailSmall))

startTime <- proc.time()

# Loop through each of the words in the emailVector
for (allWords in emailSmall) {
    if (length(myWordHash[[allWords]]) > 0) {
        myWordHash[[allWords]] <- myWordHash[[allWords]] + 1
    } else {
        myWordHash[[allWords]] <- 1
    }

}

proc.time() - startTime

misMatch <- 0

# Next a check is run to see if we have the same answers as ctWords
for (intCtr in 1:nrow(ctWords)) {
    if (ctWords[intCtr, "ctWord"] == 0) { break }
    
    if (myWordHash[[ ctWords[intCtr, "eachWord"] ]] != ctWords[intCtr, "ctWord"]) {
        misMatch <- misMatch + 1
    }
}

print(paste0("Total mismatches: ", misMatch, " of ", intCtr))

proc.time() - startTime

```
  
So, the hash table creates and then validates the same information in ~0.2 seconds.  The next step is to figure out how to query the new environment so that I can use the hashed data!
  
The names() command applied to a hashed environment works in a manner that pulls down the keys, with the hash table containing the values.  So, for example:  
```{r}

startTime <- proc.time()

eNames <- names(myWordHash)
eCts <- integer(0)

for (eachName in eNames) { 
    eCts <- c(eCts, myWordHash[[eachName]]) 
}

dfOrder <- order(-eCts, eNames)

proc.time() - startTime

sum( ctWords[order(-ctWords$ctWord, ctWords$eachWord), ][ctWords$eachWord != "", ] != 
     data.frame(eachWord = eNames, ctWord=eCts)[dfOrder, ] )

```
  
This works at roughly the same speed as Python, or ~0.1 seconds.  The data are identical, suggesting there is good promise in clever use of hashed environments to serve as an R key-value dictionary.  
  
###_Lookups for Character Vectors_  
Character vectors can be particularly difficult to look-up.  An interesting and related question is what would be the lookup time (data frame vs. hashed environment) for 20^4 drawn from LETTERS[1:20]?  First, the experiment is run with a data frame:  
```{r}

set.seed(1606211532)

nRuns <- 20^4
let20 <- LETTERS[1:20]

baseGrid <- expand.grid(let20, let20, let20, let20, stringsAsFactors=FALSE)
charFrame <- data.frame(frameKey=apply(baseGrid, 1, FUN=paste0, collapse=""), 
                        frameValue=1:nRuns, 
                        stringsAsFactors = FALSE
                        )

nTrials <- 2000
sampNums <- sample(length(charFrame$frameKey), nTrials, replace=TRUE)
keysLook <- charFrame$frameKey[sampNums]
valsFind <- integer(nTrials)
```
  
The below chunk is merely a continuation of the above chunk, split apart so that the random seed and setup need not be cached even while the longer lookup is:  
```{r, cache=TRUE}
# Run the process using the data frame lookup
startTime <- proc.time()

ctCur <- 1
for (eachKey in keysLook) {
    valsFind[ctCur] <- charFrame$frameValue[charFrame$frameKey==eachKey]
    ctCur <- ctCur + 1
}

proc.time() - startTime

# Run the process using the integer index lookup
startTime <- proc.time()

ctCur <- 1
valsFindIdx <- integer(nTrials)
for (eachIndex in sampNums) {
    valsFindIdx[ctCur] <- charFrame$frameValue[eachIndex]
    ctCur <- ctCur + 1
}

proc.time() - startTime

identical(valsFind, valsFindIdx)

```
  
This serves as another example of the power of the relational (index or hash) philosophy.  Running the text lookup 2,000 times against a 160,000 row table takes ~20 seconds, while doing the same work while knowing the index number to find takes ~0.1 seconds.  
  
A similar process is then run using a hashed environment in place of the 160,000 x 2 charFrame data frame:  
```{r}

myCharHash <- new.env(hash=TRUE, parent=emptyenv(), size=nrow(charFrame))

# Set up the hash table
startTime <- proc.time()

for (intCtr in 1:nrow(charFrame)) {
    myCharHash[[ charFrame$frameKey[intCtr] ]] <- charFrame$frameValue[intCtr]
}

proc.time() - startTime


# Use the hash table, same as above
startTime <- proc.time()

ctCur <- 1
valsFindHash <- integer(nTrials)  # Pulls nTrials from previous chunk

# Will use the same keysLook as previous chunk
for (eachKey in keysLook) {
    valsFindHash[ctCur] <- myCharHash[[ eachKey ]]
    ctCur <- ctCur + 1
}

proc.time() - startTime

identical(valsFind, valsFindHash)

```
  
This example nicely illustrates the trade-offs of the hashing process.  Creating the hash table incurs a one-time cost of 5-10 seconds.  In exchange, the 2,000 lookups take only 0.1 seconds (roughly the same as the relational index method) rather than ~20 seconds.  So, if there will be many lookups, the hash table is a great way to implement the relational concept while still being able to use characters for lookup.  
  
