---
title: "Weather Exploratory Data Analysis"
author: "davegoblue"
date: "6/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
The file 'WeatherDownloads_202005_v002.Rmd' contains code for dowloading and processing historical weather data as contained in METAR archives hosted by Iowa State University.

Data have been dowloaded and processed for several stations (airports) and years, with .rds files saved in "./RInputFiles/ProcessedMETAR".

This module will perform exploratory data analysis on the processed weather files, containing the final functions created in WeatherEDA_202005_v001.
  
#### _Data Availability_  
Each processed data file contains one year of hourly weather data for one station.  Files are saved as './RInputFiles/ProcessedMETAR/metar_kxxx_yyyy.rds' where xxx is the three-digit airport code and yyyy is the four-digit year.

Each file contains the following variables:  
  
* METAR (chr) - the extracted portion of the METAR based on a regex string  
* WindDir (chr) - the previaling wind direction in degrees, stored as a character since 'VRB' means variable  
* WindSpeed (int) - the prevailing wind speed in knots  
* WindGust (dbl) - the wind gust speed in knots (NA if there is no recorded wind gust at that hour)  
* Dummy (chr) - artifact, always a blank space  
* Visibility (dbl) - surface visibility in statute miles  
* TempC (int) - temperature in degrees Celsius  
* DewC (int) - dew point in degrees Celsius  
* Altimeter (int) - altimeter in inches of mercury  
* SLP (int) - the raw sea-level-pressure reading from the METAR  
* FahrC (chr) - the raw temperature string pulled from the METAR (Tttttdddd) where tttt is the Fahrenheit temperature recorded in Celsius and dddd is the Fahrenheit dew point recorded in Celsius  
* dtime (dttm) - the date-time associated with the observation  
* origMETAR (chr) - the full METAR associated with the observation  
* TempF (dbl) - the Fahrenheit temperature associated with converting FahrC to Fahrenheit  
* DewF (dbl) - the Fahrenheit dew point associated with converting FahrC to Fahrenheit  
* modSLP (dbl) - Sea-Level Pressure (SLP), adjusted to reflect that SLP is recorded as 0-1000 but reflects data that are 950-1050  
* nSKC (int) - number of times 'SKC' (human-confirmed cloud-free) is recorded in the observation (should be 0 or 1)  
* nCLR (int) - number of times 'CLR' (austomated-sensor cloud-free) is recorded in the observation (should be 0 or 1, and should never have both nSKC>0 and nCLR>0)  
* cloudn (chr) - the nth cloud layer recorded in the METAR (layers begin with FEW, SCT, BKN, OVC or VV)  
* cTypen (chr) - the cloud type of the nth cloud layer (FEW, BKN, SCT, OVC, or VV)  
* cLeveln (dbl) - the cloud height in feet of the nth cloud layer  
* wType (fct) - highest level of obscuration recorded in the METAR (VV > OVC > BKN > SCT > FEW > CLR/SKC)  
* year (dbl) - year of the observation  
* monthint (dbl) - month of the observation as a number (e.g., 6=June)  
* month (fct) - month of the observation as three-character abbreviation, saved as a factor (e.g., Jun=June)  
* day (int) - day of the month of the observation  
  
#### _Mappings and Helper Functions_  
A handful of mapping and helper functions are created to make plotting and formatting easier for the remainder of the code:  
```{r}

# The process frequently uses tidyverse and lubridate
library(tidyverse)
library(lubridate)


# The main path for the files
filePath <- "./RInputFiles/ProcessedMETAR/"


# Descriptive names for key variables
varMapper <- c(WindDir="Wind Direction (degrees)", 
               predomDir="General Prevailing Wind Direction",
               WindSpeed="Wind Speed (kts)",
               WindSpeed5="Wind Speed (kts), rounded to nearest 5 knots", 
               Visibility="Visibility (SM)", 
               TempC="Temperature (C)", 
               DewC="Dew Point (C)", 
               Altimeter="Altimeter (inches Hg)",
               Altimeter10="Altimeter (inches Hg), rounded to nearest 0.1 inHg", 
               modSLP="Sea-Level Pressure (hPa)", 
               TempF="Temperature (F)",
               DewF="Dew Point (F)", 
               TempF5="Temperature (F), rounded to nearest 5 degrees",
               DewF5="Dew Point (F), rounded to nearest 5 degrees", 
               cType1="First Cloud Layer Type", 
               cLevel1="First Cloud Layer Height (ft)",
               month="Month", 
               year="Year",
               wType="Greatest Sky Obscuration", 
               day="Day of Month"
               )


# File name to city name mapper
cityNameMapper <- c(kdtw_2016="Detroit, MI (2016)", 
                    kewr_2016="Newark, NJ (2016)",
                    kgrb_2016="Green Bay, WI (2016)",
                    kgrr_2016="Grand Rapids, MI (2016)",
                    kiah_2016="Houston, TX (2016)",
                    kind_2016="Indianapolis, IN (2016)",
                    klas_2015="Las Vegas, NV (2015)",
                    klas_2016="Las Vegas, NV (2016)", 
                    klas_2017="Las Vegas, NV (2017)", 
                    klnk_2016="Lincoln, NE (2016)",
                    kmke_2016="Milwaukee, WI (2016)",
                    kmsn_2016="Madison, WI (2016)",
                    kmsp_2016="Minneapolis, MN (2016)",
                    kmsy_2015="New Orleans, LA (2015)",
                    kmsy_2016="New Orleans, LA (2016)", 
                    kmsy_2017="New Orleans, LA (2017)", 
                    kord_2015="Chicago, IL (2015)",
                    kord_2016="Chicago, IL (2016)", 
                    kord_2017="Chicago, IL (2017)", 
                    ksan_2015="San Diego, CA (2015)",
                    ksan_2016="San Diego, CA (2016)",
                    ksan_2017="San Diego, CA (2017)",
                    ktvc_2016="Traverse City, MI (2016)"
                    )


# This is a helper function to create a locale description
getLocaleDescription <- function(x, mapper=cityNameMapper) {
    
    # Initialize the description as NULL
    desc <- NULL
    
    for (potMatch in names(mapper)) {
        if (str_detect(string=x, pattern=potMatch)) {
            desc <- mapper[potMatch]
            break
        }
    }
    
    # If the mapping failed, use UNMAPPED_x as the description
    if (is.null(desc)) {
        desc <- paste0("UNMAPPED_", x)
        cat("\nUnable to find a description, will use ", desc, "\n\n", sep="")
    } else {
        cat("\nWill use ", desc, " as the description for ", x, "\n\n", sep="")
    }
    
    # Return the descriptive name
    desc
    
}


# Helper function to zero-pad (especially for minutes/hours/months that are 0-9)
zeroPad <- function(x, width=2, side="left", pad="0") {
    
    str_pad(x, width=width, side=side, pad=pad)
    
}


# Helper function to take a date-time and a minutes and create a new date-time
helperBEDateTime <- function(dt, mins) {
    
    # Create the date as character (lubridate and dplyr do not always work well together)
    dateChar <- ifelse(str_length(mins)==4 & str_sub(mins, 1, 2)=="23", 
                       as.character(lubridate::as_date(lubridate::date(dt)-1)), 
                       as.character(lubridate::date(dt))
                       )
    
    # Create the hours and minutes as character
    hrMinChar <- ifelse(str_length(mins)==4, 
                        mins, 
                        paste0(zeroPad(lubridate::hour(dt)), mins)
                        )
    
    # Return the appropriate date-time
    lubridate::ymd_hm(paste0(dateChar, " ", hrMinChar))

}


# Helper function to convert 4-digit function to temperature
convertTemp <- function(x, convF=TRUE) {
    
    # Convert to numeric
    temp <- ifelse(str_sub(x, 1, 1)==1, -1, 1) * as.numeric(str_sub(x, 2, 4)) / 10
    
    # Convert to Fahrenheit if requested
    if (convF) {
        temp <- round(32 + temp * 9/5, 0)
    }
    
    temp
    
}


# Helper function to convert 5-character precipitation to inches
convertPrecip <- function(x, begPos=-4, endPos=-1) {
    
    # Convert to numeric and divide by 100 to get inches
    as.numeric(str_sub(x, begPos, endPos)) / 100

}


# Function to split facets across multiple plots
multiPageFacet <- function(gg, 
                           facetVar, 
                           maxPerPage=12, 
                           balancePages=FALSE, 
                           sameYLimits=NULL, 
                           useYLow=NULL
                           ) {
    
    # FUNCTION ARGUMENTS:
    # gg: a ggplot object that is ready except for facetting
    # facetVar: the facetting variable, passed as character
    # maxPerPage: the maximum number of objects desired for facetting
    # balancePages: boolean, whether to make roughly equal number of facets per page
    # sameYLimits: variable to be used to set constant y-max across plots (NULL means allow to float)
    # useYLow: variable to be used to set constant y-min across plots (NULL means allow to float); will be ignored if sameYLimits is NULL
    
    # Find the levels for facetVar
    facetLevels <- gg$data %>%
        pull(facetVar) %>%
        unique()
    
    # The number of facet variables will drive the number of pages to create
    nPages <- ceiling(length(facetLevels) / maxPerPage)
    
    # Recreate maxPerPage if balancePages is requested
    if (balancePages) { maxPerPage <- ceiling(length(facetLevels) / nPages) }
    
    # Create the groupings to be applied
    groups <- vector("list", nPages)
    for (ctr in 1:nPages) {
        groups[[ctr]] <- facetLevels[(1 + (ctr-1)*maxPerPage):(min(length(facetLevels), ctr*maxPerPage))]
    }
    
    # Get the ylimits if applicable
    if (!is.null(sameYLimits)) {
        ymin <- 0
        ymax <- gg$data %>%
            pull(sameYLimits) %>%
            max()
        if (!is.null(useYLow)) {
            ymin <- gg$data %>%
                pull(useYLow) %>%
                min()
        }
    }

    # Create the facetplots
    for (ctr in 1:nPages) {
        ggTemp <- gg
        ggTemp$data <- ggTemp$data %>% 
            filter(get(facetVar) %in% groups[[ctr]])
        p1 <- ggTemp +
            facet_wrap(as.formula(paste("~", facetVar)))
        if (!is.null(sameYLimits)) { p1 <- p1 + ylim(ymin, ymax) }
        print(p1)
    }
    
}

```


#### _Basic EDA Functions_  
The core EDA function is combinedEDA(), which is called by wrapCombinedEDA() for creating log and PDF files, and which is in turn called by logAndPDFCombinedEDA() so that it can be easily run for multiple locales.  

Functions include:  
```{r}

# The core function for running combined EDA
combinedEDA <- function(filename=NULL, 
                        tbl=NULL,
                        desc=NULL,
                        mets=c("WindDir", "WindSpeed", "TempC", "DewC", "Altimeter", 
                               "modSLP", "cType1", "cLevel1", "month", "day"
                               ),
                        corPairs=list(c("TempC", "TempF"), 
                                      c("TempC", "DewC"), 
                                      c("Altimeter", "modSLP"), 
                                      c("Altimeter", "WindSpeed")
                                      ),
                        fctPairs=list(c("month", "TempF"), 
                                      c("month", "DewF"), 
                                      c("month", "WindSpeed"), 
                                      c("month", "Altimeter"), 
                                      c("wType", "Visibility"), 
                                      c("wType", "WindSpeed"), 
                                      c("WindDir", "WindSpeed"), 
                                      c("WindDir", "TempF")
                                      ),
                        heatVars=c("TempC", "TempF", 
                                   "DewC", "DewF", 
                                   "Altimeter", "modSLP", 
                                   "WindSpeed", "Visibility", 
                                   "monthint", "day"
                                   ),
                        lmVars=list(c("modSLP", "Altimeter"), 
                                    c("modSLP", "Altimeter + TempF"), 
                                    c("TempF", "DewF"), 
                                    c("WindSpeed", "Altimeter + TempF + DewF + month")
                                    ),
                        mapVariables=varMapper,
                        mapFileNames=cityNameMapper,
                        path="./RInputFiles/ProcessedMETAR/"
                        ) {
    
    # Require that either filename OR tbl be passed
    if (is.null(filename) & is.null(tbl)) {
        cat("\nMust provide either a filename or an already-loaded tibble\n")
        stop("\nfilename=NULL and tbl=NULL may not both be passed to combinedEDA()\n")
    }
    
    # Require that either 1) filename and mapFileNames, OR 2) desc be passed
    if ((is.null(filename) | is.null(mapFileNames)) & is.null(desc)) {
        cat("\nMust provide either a filename with mapFileNames or a file description\n")
        stop("\nWhen desc=NULL must have non-null entries for both filename= and mapFileNames=\n")
    }
    
    # Find the description if it is NULL (default)
    if (is.null(desc)) {
        desc <- getLocaleDescription(filename, mapper=mapFileNames)
    }
    
    # Warn if both filename and tbl are passed, since tbl will be used
    if (!is.null(filename) & !is.null(tbl)) {
        cat("\nA tibble has been passed and will be used as the dataset for this function\n")
        warning("\nArgument filename=", filename, " is NOT loaded since a tibble was passed\n")
    }
    
    # Read in the file unless tbl has already been passed to the routine
    if (is.null(tbl)) {
        tbl <- readRDS(paste0(path, filename))
    }
    
    # Plot counts by metric
    plotcountsByMetric(tbl, mets=mets, title=desc, diagnose=TRUE)
    
    # Plot relationships between two variables
    for (ys in corPairs) {
        plotNumCor(tbl, var1=ys[1], var2=ys[2], subT=desc, diagnose=TRUE)
    }
    
    # plot numeric vs. factor
    for (ys in fctPairs) {
        plotFactorNumeric(tbl, fctVar=ys[1], numVar=ys[2], subT=desc, showXLabel=FALSE, diagnose=TRUE)
    }
    
    # Heatmap for variable correlations
    corMETAR(tbl, numVars=heatVars, subT=paste0(desc, " METAR"))

    # Run linear rergression
    for (ys in lmVars) {
        lmMETAR(tbl, y=ys[1], x=ys[2], yName=varMapper[ys[1]], subT=desc)
    }
    
    # Run the basic wind plots
    basicWindPlots(tbl, desc=desc, gran="")
    
    # Return the tibble
    tbl
    
}



# The core function for placing combined EDA outputs in log and PDF files
wrapCombinedEDA <- function(readFile, 
                            readPath="./RInputFiles/ProcessedMETAR/", 
                            mapFileNames=cityNameMapper,
                            desc=NULL,
                            writeLogFile=NULL,
                            writeLogPDF=NULL,
                            writeLogPath=NULL,
                            appendWriteFile=FALSE,
                            ...
                            ) {
    
    # Read in the requested file
    tbl <- readRDS(paste0(readPath, readFile))

    # Find the description if it has not been passed
    if (is.null(desc)) {
        desc <- getLocaleDescription(readFile, mapper=mapFileNames)
    }
    
    # Helper function that only runs the combinedEDA() routine
    coreFunc <- function() { combinedEDA(tbl=tbl, desc=desc, mapFileNames=mapFileNames, ...) }
    
    # If writeLogPDF is not NULL, direct the graphs to a suitable PDF
    if (!is.null(writeLogPDF)) {
        
        # Prepend the provided log path if it has not been made available
        if (!is.null(writeLogPath)) {
            writeLogPDF <- paste0(writeLogPath, writeLogPDF)
        }
        
        # Provide the location of the EDA pdf file
        cat("\nEDA PDF file is available at:", writeLogPDF, "\n")

        # Redirect the writing to writeLogPDF
        pdf(writeLogPDF)
    }
    
    # Run EDA on the tbl using capture.output to redirect to a log file if specified
    if (!is.null(writeLogFile)) {
        
        # Prepend the provided log path if it has not been made available
        if (!is.null(writeLogPath)) {
            writeLogFile <- paste0(writeLogPath, writeLogFile)
        }
        
        # Provide the location of the EDA log file
        cat("\nEDA log file is available at:", writeLogFile, "\n")
        
        # Run EDA such that the output goes to the log file
        capture.output(coreFunc(), 
                       file=writeLogFile, 
                       append=appendWriteFile
                       )
        
    } else {
        # Run EDA such that output stays in stdout
        coreFunc()
    }
    
    # If writeLogPDF is not NULL, redirect to stdout
    if (!is.null(writeLogPDF)) {
        dev.off()
    }
    
    # Return the tbl
    tbl
    
}



# The core function that calls wrapCombinedEDA
logAndPDFCombinedEDA <- function(tblName, filePath="./RInputFiles/ProcessedMETAR/") {
    
    # Create the RDS file name
    rdsName <- paste0("metar_", tblName, ".rds")
    cat("\nRDS Name:", rdsName)
    
    # Create the log file name
    logName <- paste0("metar_", tblName, "_EDA.log")
    cat("\nLog Name:", logName)
    
    # Create the PDF file name
    pdfName <- paste0("metar_", tblName, "_EDA.pdf")
    cat("\nPDF Name:", pdfName)
    
    # Call wrapCombinedEDA()
    tbl <- wrapCombinedEDA(rdsName, 
                           readPath=filePath, 
                           writeLogFile=logName, 
                           writeLogPDF=pdfName,
                           writeLogPath=filePath
                           )
    
    # Return the tbl
    tbl
    
}

```
  
There are a number of associated functions called by combinedEDA, including:  
  
* plotCountsByMetric() - bar plots for counts by variable  
* plotNumCor() - plot two numeric variables against each other  
* plotFactorNumeric() - boxplot a numeric variable against a factor variable  
* corMETAR() - correlations between METAR variables  
* lmMETAR() - linear regression modeling for METAR variables  
* basicWindPlots() - plot wind speed and direction  
  
```{r}

# Helper function for generating plots by key variables
plotcountsByMetric <- function(df, 
                               mets, 
                               title="", 
                               rotateOn=20, 
                               dropNA=TRUE, 
                               diagnose=FALSE,
                               mapper=varMapper,
                               facetOn=NULL, 
                               showCentral=FALSE, 
                               multiPageWrap=FALSE, 
                               maxPerPage=9,
                               balancePages=FALSE
                               ) {
    
    # Function arguments
    # df: dataframe or tibble containing raw data
    # mets: character vector of variables for plotting counts
    # title: character vector for plot title
    # rotateOn: integer, x-axis labels will be rotated by 90 degrees if # categories >= rotateOn
    # dropNA: boolean for whether to drop all NA prior to plotting (recommended for avoiding warnings)
    # diagnose: boolean for whether to note in the log the number of NA observations dropped
    # mapper: named list containing mapping from variable name to well-formatted name for titles and axes
    # facetOn: a facetting variable for the supplied df (NULL for no faceting)
    # showCentral: boolean for whether to show the central tendency over-plotted on the main data
    # multiPageWrap: boolean, will call multiPageFacet() rather than facetWrap() if TRUE
    # maxPerPage: integer, maximum facets per plot, passed to multiPageFacet()
    # balancePages: boolean, whether to balance the number of facets per page across all pages or allow the final page to have as few as 1 facets
    
    # Function usage
    # 1.  By default, the function plots overall counts by metric for a given input
    # 2.  If facetOn is passed as a non-NULL, then the data in #1 will be facetted by facetOn
    # 3.  If showCentral=TRUE, then the overall mean will be plotted as a point on the main plot (only makes sense if facetOn has been selected)
    # 4.  If multiPageWrap=TRUE, then the faceting data will be paginated so that there is easier readability for the plots on any given page
    
    
    # Plot of counts by key metric
    for (x in mets) {
        # If a facetting variable is provided, need to include this in the group_by
        useVars <- x
        if (!is.null(facetOn)) { useVars <- c(facetOn, useVars) }
        dat <- df %>%
            group_by_at(vars(all_of(useVars))) %>%
            summarize(n=n())
        
        if (dropNA) {
            nOrig <- nrow(dat)
            sumOrig <- sum(dat$n)
            dat <- dat %>%
                filter_all(all_vars(!is.na(.)))
            if (diagnose & (nOrig > nrow(dat))) { 
                cat("\nDropping", 
                    nOrig-nrow(dat), 
                    "rows with", 
                    sumOrig-sum(dat$n), 
                    "observations due to NA\n"
                    )
            }
        }
        
        # Create the main plot
        p <- dat %>%
            ggplot(aes_string(x=x, y="n")) + 
            geom_col() + 
            labs(title=title,
                 subtitle=paste0("Counts By: ", mapper[x]), 
                 x=paste0(x, " - ", mapper[x]),
                 y="Count"
                 )
        # If the rotateOn criteria is exceeded, rotate the x-axis by 90 degrees
        if (nrow(dat) >= rotateOn) {
            p <- p + theme(axis.text.x=element_text(angle=90))
        }
        # If showCentral=TRUE, add a dot plot for the overall average
        if (showCentral) {
            # Get the median number of observations by facet, or the total if facetOn=NULL
            if (is.null(facetOn)) {
                useN <- sum(dat$n)
            } else {
                useN <- dat %>%
                    group_by_at(vars(all_of(facetOn))) %>%
                    summarize(n=sum(n)) %>%
                    pull(n) %>%
                    median()
            }
            # Get the overall percentages by x
            centralData <- helperCountsByMetric(tbl=dat, ctVar=x, sumOn="n") %>%
                mutate(centralValue=nPct*useN)
            # Apply the median
            p <- p + geom_point(data=centralData, aes(y=centralValue), color="red", size=2)
        }
        # If facetting has been requested, facet by the desired variable
        # multiPageFacet will print by default, so use print(p) only if multiPageWrap=FALSE
        if (!is.null(facetOn)) {
            if (multiPageWrap) {
                multiPageFacet(p, 
                               facetVar=facetOn, 
                               maxPerPage=maxPerPage, 
                               balancePages=balancePages, 
                               sameYLimits="n"
                               )
            } else {
                p <- p + facet_wrap(as.formula(paste("~", facetOn)))
                print(p)
            }
        } else {
            # Print the plot
            print(p)
        }
    }
}


# Create a function for plotting two variables against each other
plotNumCor <- function(met, 
                       var1, 
                       var2, 
                       alpha=0.5,
                       maxSize=6,
                       title=NULL, 
                       subT="", 
                       dropNA=TRUE, 
                       diagnose=FALSE,
                       mapper=varMapper, 
                       facetOn=NULL, 
                       showCentral=FALSE
                       ) {
    
    # Function arguments
    # met: dataframe or tibble containing raw data
    # var1: character vector of variable to be used for the x-axis
    # var2: character vector of variable to be used for the y-axis
    # alpha: the alpha for the plotted points
    # maxSize: the maximum size for the plotted points (ggplot default is 6)
    # title: character vector for plot title
    # subT: character vector for plot subtitle
    # dropNA: boolean for whether to drop all NA prior to plotting (recommended for avoiding warnings)
    # diagnose: boolean for whether to note in the log the number of NA observations dropped
    # mapper: named list containing mapping from variable name to well-formatted name for titles and axes
    # facetOn: a facetting variable for the supplied met (NULL for no faceting)
    # showCentral: boolean for whether to show the central tendency over-plotted on the main data
    
    # Function usage
    # 1.  By default, the function plots overall counts by the provided x/y metrics, with each point sized based on the number of observations, and with an lm smooth overlaid
    # 2.  If facetOn is passed as a non-NULL, then the data in #1 will be facetted by facetOn
    # 3.  If showCentral=TRUE, then the lm smooth that best first to the overall data will be plotted (only makes sense if facetOn has been selected)
    
    # Create the title if not passed
    if (is.null(title)) { 
        title <- paste0("Hourly Observations of ", mapper[var1], " and ", mapper[var2]) 
    }

    # If a facetting variable is provided, need to include this in the group_by
    useVars <- c(var1, var2)
    if (!is.null(facetOn)) { useVars <- c(facetOn, useVars) }
        
    # Pull the counts by useVars
    dat <- met %>%
        group_by_at(vars(all_of(useVars))) %>%
        summarize(n=n()) 
    
    # If NA requested to be excluded, remove anything with NA
    if (dropNA) {
        nOrig <- nrow(dat)
        sumOrig <- sum(dat$n)
        dat <- dat %>%
            filter_all(all_vars(!is.na(.)))
        if (diagnose) { 
            cat("\nDropping", 
                nOrig-nrow(dat), 
                "rows with", 
                sumOrig-sum(dat$n), 
                "observations due to NA\n"
                )
        }
    }
    
    p <- dat %>%
        ggplot(aes_string(x=var1, y=var2)) + 
        geom_point(alpha=alpha, aes_string(size="n")) + 
        geom_smooth(method="lm", aes_string(weight="n")) + 
        labs(x=paste0(mapper[var1], " - ", var1), 
             y=paste0(mapper[var2], " - ", var2), 
             title=title, 
             subtitle=subT
             ) + 
        scale_size_area(max_size=maxSize)
    
    # If showCentral=TRUE, add a dashed line for the overall data
    if (showCentral) {
        p <- p + helperNumCor(dat, xVar=var1, yVar=var2, sumOn="n")
    }

    # If facetting has been requested, facet by the desired variable
    if (!is.null(facetOn)) {
        p <- p + facet_wrap(as.formula(paste("~", facetOn)))
    }
    
    print(p)
}


# Updated function for plotting numeric by factor
plotFactorNumeric <- function(met, 
                              fctVar, 
                              numVar, 
                              title=NULL, 
                              subT="", 
                              diagnose=TRUE,
                              showXLabel=TRUE,
                              mapper=varMapper,
                              facetOn=NULL, 
                              showCentral=FALSE, 
                              multiPageWrap = FALSE, 
                              maxPerPage = 9, 
                              balancePages = FALSE, 
                              sameYLimits=numVar, 
                              useYLow=numVar
                              ) {
    
    # Function arguments
    # met: dataframe or tibble containing raw data
    # fctVar: character vector of variable to be used for the x-axis (factor in the boxplot)
    # numVar: character vector of variable to be used for the y-axis (numeric in the boxplot)
    # title: character vector for plot title
    # subT: character vector for plot subtitle
    # diagnose: boolean for whether to note in the log the number of NA observations dropped
    # showXLabel: boolean for whether to include the x-label (e.g., set to FALSE if using 'month')
    # mapper: named list containing mapping from variable name to well-formatted name for titles and axes
    # facetOn: a facetting variable for the supplied met (NULL for no faceting)
    # showCentral: boolean for whether to show the central tendency over-plotted on the main data
    # multiPageWrap: boolean, will call multiPageFacet() rather than facetWrap() if TRUE
    # maxPerPage: integer, maximum facets per plot, passed to multiPageFacet()
    # balancePages: boolean, whether to balance the number of facets per page across all pages or allow the final page to have as few as 1 facets
    # sameYLimits: variable for making all y-axes maxima the same across multiPageFacet; numVar is the numeric variable passed; will be set to NULL if ylimits has been passed (is not null)
    # useYLow: variable for making all y-axes minima the same across multiPageFacet; numVar is the numeric variable passed; will be set to NULL if ylimits has been passed (is not null)
    
    # Function usage
    # 1.  By default, the function creates the boxplot of numVar by fctVar
    # 2.  If facetOn is passed as a non-NULL, then the data in #1 will be facetted by facetOn
    # 3.  If showCentral=TRUE, then the overall median of numVar by fctVar will be plotted as a red dot
    # 4.  If multiPageWrap=TRUE, then the faceting data will be paginated so that there is easier readability for the plots on any given page

    
    # Create the title if not passed
    if (is.null(title)) { 
        title <- paste0("Hourly Observations of ", mapper[numVar], " by ", mapper[fctVar])
    }
    
    # Remove the NA variables
    nOrig <- nrow(met)
    dat <- met %>%
        filter(!is.na(get(fctVar)), !is.na(get(numVar)))
    if (diagnose) { cat("\nRemoving", nOrig-nrow(dat), "records due to NA\n") }
    
    # Create the base plot
    p <- dat %>%
        ggplot(aes_string(x=fctVar, y=numVar)) + 
        geom_boxplot(fill="lightblue") + 
        labs(title=title, 
             subtitle=subT, 
             x=ifelse(showXLabel, paste0(mapper[fctVar], " - ", fctVar), ""), 
             y=paste0(mapper[numVar], " - ", numVar)
             )
    
    # If showCentral=TRUE, add a dot plot for the overall average
    if (showCentral) {
        centData <- helperFactorNumeric(dat, .f=median, byVar=fctVar, numVar=numVar)
        p <- p + geom_point(data=centData, aes(y=helpFN), size=2, color="red")
    }

    # If facetting has been requested, facet by the desired variable
    # multiPageFacet will print by default, so use print(p) only if multiPageWrap=FALSE
    if (!is.null(facetOn)) {
        if (multiPageWrap) {
            multiPageFacet(p, 
                           facetVar=facetOn, 
                           maxPerPage=maxPerPage, 
                           balancePages=balancePages, 
                           sameYLimits=sameYLimits, 
                           useYLow=useYLow
                           )
        } else {
            p <- p + facet_wrap(as.formula(paste("~", facetOn)))
            print(p)
        }
    } else {
        # Print the plot
        print(p)
    }
    
}


# Function to calculate, display, and plot variable correlations
corMETAR <- function(met, numVars, subT="") {

    # Keep only complete cases and report on data kept
    dfUse <- met %>%
        select_at(vars(all_of(numVars))) %>%
        filter(complete.cases(.))
    
    nU <- nrow(dfUse)
    nM <- nrow(met)
    myPct <- round(100*nU/nM, 1)
    cat("\n *** Correlations use ", nU, " complete cases (", myPct, "% of ", nM, " total) ***\n", sep="")
    
    # Create the correlation matrix
    mtxCorr <- dfUse %>%
        cor()

    # Print the correlations
    mtxCorr %>%
        round(2) %>%
        print()

    # Display a heat map
    corrplot::corrplot(mtxCorr, 
                       method="color", 
                       title=paste0("Hourly Weather Correlations\n", subT), 
                       mar=c(0, 0, 2, 0)
                       )
}


# Function for linear regressions on METAR data
lmMETAR <- function(met, 
                    y, 
                    x, 
                    yName, 
                    subT=""
                    ) {
    
    # Convert to formula
    myChar <- paste0(y, " ~ ", x)
    cat("\n *** Regression call is:", myChar, "***\n")
    
    # Run regression
    regr <- lm(formula(myChar), data=met)
    
    # Summarize regression
    print(summary(regr))
    
    # Predict the new values
    pred <- predict(regr, newdata=met)
    
    # Plot the predictions
    p <- met %>%
        select_at(vars(all_of(y))) %>%
        mutate(pred=pred) %>%
        filter_all(all_vars(!is.na(.))) %>%
        group_by_at(vars(all_of(c(y, "pred")))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=y, y="pred")) + 
        geom_point(aes(size=n), alpha=0.25) + 
        geom_smooth(aes(weight=n), method="lm") + 
        labs(title=paste0("Predicted vs. Actual ", yName, " - ", x, " as Predictor"), 
             subtitle=subT, 
             x=paste0("Actual ", yName), 
             y=paste0("Predicted ", yName)
             )
    print(p)
    
}


# Generate basic wind plots
basicWindPlots <- function(met, 
                           dirVar="WindDir", 
                           spdVar="WindSpeed",
                           desc="", 
                           gran="", 
                           mapper=varMapper
                           ) {

    # Plot for the wind direction
    wDir <- met %>%
        ggplot(aes_string(x=dirVar)) + 
        geom_bar() + 
        labs(title=paste0(desc, " Wind Direction"), subtitle=gran, 
             y="# Hourly Observations", x=mapper[dirVar]
             ) + 
        theme(axis.text.x=element_text(angle=90))
    print(wDir)

    # Plot for the minimum, average, and maximum wind speed by wind direction
    # Wind direction 000 is reserved for 0 KT wind, while VRB is reserved for 3-6 KT variable winds
    wSpeedByDir <- met %>%
        filter(!is.na(get(dirVar))) %>%
        group_by_at(vars(all_of(dirVar))) %>%
        summarize(minWind=min(get(spdVar)), meanWind=mean(get(spdVar)), maxWind=max(get(spdVar))) %>%
        ggplot(aes_string(x=dirVar)) +
        geom_point(aes(y=meanWind), color="red", size=2) +
        geom_errorbar(aes(ymin=minWind, ymax=maxWind)) +
        labs(title=paste0(desc, " Wind Speed (Max, Mean, Min) By Wind Direction"), 
             subtitle=gran,
             y=mapper[spdVar], 
             x=mapper[dirVar]
             ) + 
        theme(axis.text.x=element_text(angle=90))
    print(wSpeedByDir)

    # Plot for the wind speed
    pctZero <- sum(pull(met, spdVar)==0, na.rm=TRUE) / nrow(met)
    wSpeed <- met %>%
        filter_at(vars(all_of(spdVar)), all_vars(!is.na(.))) %>%
        ggplot(aes_string(x=spdVar)) +
        geom_bar(aes(y=..count../sum(..count..))) +
        labs(title=paste0(round(100*pctZero), "% of wind speeds in ", desc, " measure 0 Knots"),
             subtitle=gran,
             y="% Hourly Observations", 
             x=mapper[spdVar]
             )
    print(wSpeed)
    
    # Polar plot for wind speed and wind direction
    wData <- met %>%
        filter_at(vars(all_of(dirVar)), all_vars(!is.na(.) & !(. %in% c("000", "VRB")))) %>%
        filter_at(vars(all_of(spdVar)), all_vars(!is.na(.))) %>%
        mutate_at(vars(all_of(dirVar)), as.numeric) %>%
        group_by_at(vars(all_of(c(dirVar, spdVar)))) %>%
        summarize(n=n())
        
    wPolarDirSpeed <- wData %>%
        ggplot(aes_string(x=spdVar, y=dirVar)) +
        geom_point(alpha=0.1, aes(size=n)) +
        coord_polar(theta="y") +
        labs(title=paste0(desc, " Direction vs. Wind Speed"), 
             subtitle=gran, 
             x=mapper[spdVar], 
             y=mapper[dirVar]
             ) +
        scale_y_continuous(limits=c(0, 360), breaks=c(0, 90, 180, 270, 360)) +
        scale_x_continuous(limits=c(0, 40), breaks=c(0, 5, 10, 15, 20, 25, 30, 35, 40)) +
        geom_point(aes(x=0, y=0), color="red", size=2)
    print(wPolarDirSpeed)

}


```
  
The basic EDA can then be run for all of the downloaded files, with results cached and output directed to appropriate log and PDF files:  
```{r cache=TRUE}

fileNames <- dir("./RInputFiles/ProcessedMETAR/", pattern="^metar_k[a-z]{3}_201\\d\\.rds") %>%
    str_replace(pattern=".rds", replacement="") %>%
    str_replace(pattern="metar_", replacement="")

cat("\nEDA process will be run for all of:\n\n", paste0(fileNames, collapse="\n"), "\n", sep="")

for (fName in fileNames) {
    assign(fName, logAndPDFCombinedEDA(fName))
}

```

#### _Precipitation Intervals_  
METAR data include descriptions of the precipitation occuring at any given time.  Two of the most common precipitation forms are rain (RA) and snow (SN).  These can occur together, denoted as RASN or SNRA in the METAR.

Further, the precipitation type can be classified using a prefix as light (-), moderate (no prefix), or heavy (+).  So, RA would be moderate rain, -SNRA would be a light snow-rain mix, +RA would be heavy rain.

Additionally, the timing of the precipitation event is captured in the remarks using B (begin) and E (end).  So, an hourly METAR of RAB20E35B50 would mean rain started at 20 past the hour, ended at 35 past the hour, and began again at 50 past the hour.  Since METAR are often taken just before the top of the hour, a four-digit time is used if it is in the 'previous' hour; for example, RAB1959E36 in the 2053Z METAR.

Broadly speaking, precipitation at a specific point in time can be extracted from the main METAR (preceding RMK) whil prcipitation intervals can be extracted by parsing the remarks (following RMK).  There can be misalignment between these; for example, an interval that suggests rain is occuring at a time the METAR does not show rain.

Precipitation events of three main types (rain, snow, thunder) can be extracted from the METAR, and the associated intervals built from the remarks can be edited such that they align with the METAR extractions.

There are several main functions for the precipitation intervals process:  
  
* wrapPrecipTimes() - wrapper function to run for multiple locales for a given precipitation type and direct output to a pdf/log file  
* getPrecipTimes() - function to run for multiple locales for a given precipitation type (called by wrapPrecipTimes)  
* combinePrecipFunctions() - calls multiple functions to extract and align precipitations states and precipitation intervals for a specific locale and precipitation type (called by getPrecipTimes)  
* makePrecipTimeGraph() - creates plots across locales for a specific precipitation type (called by wrapPrecipTimes)  
  
```{r}

# Function to combine precipitation functions
combinePrecipFunctions <- function(df, 
                                   pType="(?<!FZ)RA", 
                                   errorLengthInconsistency=TRUE
                                   ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble containing processed METAR data, including dtime and origMETAR
    # pType: regex code for the precipitation type of interest
    # errorLengthInconsistency: boolean, whether to error out if beginTimes, endTimes, or precipInts have different lengths or imply intervals of non-positive durection
    
    # Extract the current precipitation state, lagged precipitation state, and string of begin/end times
    precip1 <- fnPrecip1(df, pType=pType)
    
    # Find and delete continuity (next before previous) and consistency (belongs to wrong METAR) errors
    precip3 <- fnPrecip2(precip1) %>%
        fnPrecip3()
    
    # Find issues with precipitation begin and end times compared to each other and precipitation states
    precip4 <- fnPrecip4(precip3, precip1)
    
    # Correct for the issues with begin and end times and create a vector of begin times and end times
    precip5 <- fnPrecip5(precip3, issueTimes=precip4, lagCurFrame=precip1)
    
    # Check intervals for consistency with precipitation states
    precip6 <- fnPrecip6(precip5, precip1)
    
    # Extract beginTimes, endTimes, and precipInts
    beginTimes <- precip6$beginTimes
    endTimes <- precip6$endTimes
    precipInts <- precip6$precipInts
    
    # Check that the intervals are all positive and with same number of begin times and end times
    lenBT <- length(beginTimes)
    lenET <- length(endTimes)
    lenPI <- length(precipInts)
    piNotPositive <- (lubridate::time_length(precipInts) <= 0)
    
    # Check for same lengths    
    if (max(lenBT, lenET, lenPI) != min(lenBT, lenET, lenPI)) {
        cat("Inconsistent lengths - beginTimes:", lenBT, ", endTimes:", lenET, "precipInts:", lenPI, "\n")
        if (errorLengthInconsistency) { 
            stop("Exiting due to inconsistent lengths") 
        } else {
            cat("\nContinuing to process, output frame will append NA where needed\n")
            maxLen <- max(lenBT, lenET, lenPI)
            beginTimes <- c(beginTimes, rep(NA, maxLen-lenBT))
            endTimes <- c(endTimes, rep(NA, maxLen-lenET))
            precipInts <- c(precipInts, rep(NA, maxLen-lenPI))
        }
    }
    
    # Check for positive intervals
    if (sum(piNotPositive) > 0) {
        cat("\nPrecipitation intervals are non-positive:", 
            sum(piNotPositive), 
            "at positions",
            which(piNotPositive), 
            "\n"
            )
        if (errorLengthInconsistency) { stop("Exiting due to inconsistency in interval lengths") } 
    }
    
    
    # Return the relevant information
    list(pStateFrame=precip1, 
         pIssueTimes=precip4,
         mismatches=precip6$mismatches,
         mismatchTimes=precip6$mismatchTimes,
         beginEndInterval=tibble::tibble(beginTimes=beginTimes, endTime=endTimes, precipInts=precipInts)
         )
    
}


# Function to get the precipitation times for a specific type and list of file names
getPrecipTimes <- function(files, pType="(?<!FZ)RA", pExt="_RA") {

    precipList <- vector("list", length(files))
    names(precipList) <- paste0(files, pExt)

    for (city in files) {
    
        cat("\n\n*** Processing for:", cityNameMapper[city], "\n")
        precipList[[paste0(city, pExt)]] <- combinePrecipFunctions(get(city), 
                                                                   pType=pType, 
                                                                   errorLengthInconsistency=TRUE
                                                                   )
    
    }

    # Return precipList
    precipList

}


# Function to create plots for a specific precipitation type across locales
makePrecipTimeGraph <- function(precipList, pTypeName="rain") {
    
    precipLength <- precipByLocale(precipList)
    plotPrecipitation(precipLength, pTypeName=pTypeName)
    
    # Return precipLength
    precipLength
}



# Function to run precipitation extraction, send output to logs if requested, and return list
wrapPrecipTimes <- function(files, 
                            pType, 
                            pExt, 
                            pTypeName,
                            writeLogFile=NULL,
                            writeLogPDF=NULL,
                            writeLogPath=NULL,
                            appendWriteFile=FALSE,
                            ...
                            ) {
    
    # Helper function that only runs the getPrecipTimes() routine
    corePrecipTimes <- function() { getPrecipTimes(files, pType=pType, pExt=pExt) }

    
    # Run getPrecipTimes, using capture.output to redirect to a log file if specified
    if (!is.null(writeLogFile)) {
        
        # Prepend the provided log path if it has not been made available
        if (!is.null(writeLogPath)) {
            writeLogFile <- paste0(writeLogPath, writeLogFile)
        }
        
        # Provide the location of the EDA log file
        cat("\nPrecipitation times log file is available at:", writeLogFile, "\n")
        
        # Run EDA such that the output goes to the log file
        capture.output(precipList <- corePrecipTimes(), 
                       file=writeLogFile, 
                       append=appendWriteFile
                       )
        
    } else {
        # Run getPrecipTimes such that output stays in stdout
        precipList <- corePrecipTimes()
    }

    # Write a summary of the number of mismatches
    cat("\nSummary of mismatch lengths after processing - see log file if needed\n")
    sapply(precipList, FUN=function(x) { length(x[["mismatchTimes"]]) }) %>% 
        t() %>% 
        t() %>% 
        print()
    
    # Run makePrecipTimeGraph(), sending PDF to file if requested
    # If writeLogPDF is not NULL, direct the graphs to a suitable PDF
    if (!is.null(writeLogPDF)) {
        
        # Prepend the provided log path if it has not been made available
        if (!is.null(writeLogPath)) {
            writeLogPDF <- paste0(writeLogPath, writeLogPDF)
        }
        
        # Provide the location of the EDA pdf file
        cat("\nPrecipitation times PDF file is available at:", writeLogPDF, "\n")

        # Redirect the writing to writeLogPDF
        pdf(writeLogPDF)
    }
        
    # Run the plotting routine
    precipLength <- makePrecipTimeGraph(precipList, pTypeName=pTypeName)
    
    # If writeLogPDF is not NULL, redirect to stdout
    if (!is.null(writeLogPDF)) {
        dev.off()
    }
    
    # Return precipList and precipLength
    list(precipList=precipList, precipLength=precipLength)
    
}

```
  
There are many functions called by the main precipitation extraction and interval functions, including:  
  
* fnPrecip1() - extracts precipitation state from METAR and precipitation ranges from remarks  
* fnPrecip2() - converts B/E times in remarks to date-times  
* fnPrecip3() - extract issues with continuity (time in remarks belongs to a different METAR or comes at or after the next time listed)  
* fnPrecip4() - finds consistency issues such as precipitation begin time when there is already precipitation, end time when there is no precipitation, and lack of begin/end time when state changes  
* fnPrecip5() - works through any inconsistencies and automates a solution to regain consistency (assumes that the states listed in the METAR are the source of truth)  
* fnPrecip6() - confirms consistency of the final intervals and output a list containing the begin times, end times, intervals, and mismatches  
* precipByLocale() - extracts precipitation hours and events from processed intervals data  
* plotPrecipitation() - plots precipitation hours and events, facetted by month or locale  
  
```{r}

# Function to extract precipitation state and range data from METAR
fnPrecip1 <- function(df, pType, showRegex=TRUE) {

    # The remarks pattern is created based on the precipitation type
    keyPattern <- paste0("(", pType, "[B|E]\\d+[0-9BE]*)")
    if (showRegex) { cat("\nRegex search code is:", keyPattern, "\n") }
    
    # Extract the current precipitation state, lagged precipitation state, and range data from METAR
    df <- df %>% 
        select(origMETAR, dtime) %>% 
        mutate(strPrecip=str_extract(origMETAR, pattern=keyPattern), 
               curPrecip=str_detect(origMETAR, pattern=paste0("\\d{6}Z.*", pType, ".*RMK")),
               lagPrecip=lag(curPrecip, 1)
               )
    
    # Confirm that df is unique by dtime (algorithm depends on this)
    dups <- df %>%
        select(dtime) %>%
        duplicated() %>%
        any()
    if (dups) {
        stop("\nThere are duplicate values of dtime - investigate and fix\n")
    }
    
    # Returnt the file
    df
    
}


# Function to create times for each piece of B/E data
fnPrecip2 <- function(df) {

    # Create the begin and end times by splitting precipString, then format as tibble and reattach dtime
    bEData <- df %>%
        pull(strPrecip) %>%
        str_extract_all(pattern="[BE]\\d+", simplify=TRUE) %>%
        as.data.frame(stringsAsFactors=FALSE) %>%
        tibble::as_tibble() %>%
        bind_cols(select(df, dtime))
    
    # Pivot longer, create a dummy record where is.na(V1), and split in to time and state change
    bEData <- bEData %>%
        pivot_longer(-dtime) %>%
        mutate(value=ifelse(name=="V1" & is.na(value), paste0("N", lubridate::minute(dtime)), value)) %>%
        filter(value != "") %>%
        mutate(chgType=str_sub(value, 1, 1), 
               chgNum=str_sub(value, 2, -1), 
               chgTime=helperBEDateTime(dt=dtime, mins=chgNum)
               )
    
    # Return the data
    bEData
    
}


# Function to extract issues - continuity, distance from METAR
fnPrecip3 <- function(df) {

    # Issues that can exist with the data
    # 1. Greater than or equal to 3600 seconds before the current dtime, or after the current time
    # 2. At or prior to the previous time
    # 3. First record starts with an end time (there would never be a begin time associated)
    issueCheck <- df %>%
        mutate(deltaMETAR=dtime-chgTime, 
               deltaPrev=ifelse(row_number()==1, ifelse(chgType=="E", -1, 3600), chgTime-lag(chgTime, 1)), 
               issueCons=(deltaMETAR < 0 | deltaMETAR > 3599 | deltaPrev <= 0)
               )
    
    cat("\nContinuity or consistency error - record(s) will be deleted\n")
    issueCheck %>%
        filter(issueCons) %>%
        print()
    
    issueCheck %>%
        select(-deltaMETAR, -deltaPrev) %>%
        filter(!issueCons)
    
}


# Function to find wrong sequence of times
fnPrecip4 <- function(dfTimes, dfOrig) {
    
    # Pivot dfTimes back to a single record, discarding any of the consistency issues
    # OK if not all dtimes included; will get from dfOrig
    dfCheck <- dfTimes %>%
        filter(!issueCons) %>%
        mutate(newValue=case_when(chgType=="N" ~ 0, chgType=="B" ~ 1, chgType=="E" ~ -1)) %>%
        pivot_wider(dtime, names_from="name", values_from="newValue") %>%
        right_join(select(dfOrig, dtime, curPrecip, lagPrecip), by="dtime") %>%
        mutate_if(is.numeric, ~ifelse(is.na(.), 0, .))
    
    # Create the cumsum of state change
    cs <- dfCheck %>%
        select(-dtime, -curPrecip) %>%
        mutate(lagPrecip=as.integer(lagPrecip)) %>%
        select(lagPrecip, everything()) %>%
        apply(1, FUN=cumsum) %>%
        t()
    
    # Create the list of issues
    # If cumsum is every anything other than 1 or 0 there is a problem
    # If the last value of cumsum does not equal curPrecip there is a problem
    issue01 <- which(apply(cs, 1, FUN=max) > 1 | apply(cs, 1, FUN=min) < 0)
    issuelc <- which(dfCheck$curPrecip != cs[, ncol(cs)])
    issueAll <- sort(unique(c(issue01, issuelc)))
    
    cat("\n\nIssues with begin when precipitation or end when no precipitation:", length(issue01))
    cat("\nIssues where state change does not add to the total:", length(issuelc))
    cat("\nTotal issues (may be less than sum due to same datetime in both):", length(issueAll), "\n\n")
    # print(dfCheck[issueAll, ] %>% select(dtime, lagPrecip, everything()))
    
    dfCheck[sort(unique(c(issue01, issuelc))), "dtime"]
    
}


# Function to work through each bad date-time and suggest new ranges
fnPrecip5 <- function(df, issueTimes, lagCurFrame) {
    
    # Create a dataframe for issueTimes
    issues <- issueTimes %>%
        mutate(issue=TRUE)
    
    # Split df in to issues and non-issue
    df <- df %>%
        left_join(issues, by="dtime") %>%
        mutate(issue=ifelse(is.na(issue), FALSE, issue), 
               lastRecord=(row_number()==n())
               ) %>%
        left_join(select(lagCurFrame, dtime, lagPrecip, curPrecip), by="dtime")
    
    dfIssues <- df %>%
        filter(issue)
    dfNoIssues <- df %>%
        filter(!issue)
    
    # Note the records to be worked through
    cat("\nRecords to be addressed include:\n")
    dfIssues %>%
        select(-issueCons, -issue) %>%
        print()

    # Work through a record by starting with lagPrecip
    # If first record inconsistent with lagPrecip, flag for deletion and keep state; update state otherwise
    # if next record inconsistent with previous state, flag for deletion and keep state; update otherwise
    # If final record inconsistent with curPrecip, add a record using dtime as the time
    
    # Get the unique times with issues
    dtimeVec <- dfIssues %>% pull(dtime) %>% unique()
    
    # Create empty vectors for deletes and adds
    delVec <- as.POSIXct(character(0))
    addBegin <- as.POSIXct(character(0))
    addEnd <- as.POSIXct(character(0))
    
    # Populate the delete and add vectors
    for (dt in dtimeVec) {
        
        # Pull the records for this time
        dtRecords <- dfIssues %>% filter(dtime==lubridate::as_datetime(dt))
        
        # Initialize the previous state to lagPrecip and the error vector to blank
        preState <-dtRecords$lagPrecip[1]
        
        # Loop through and flag state change errors
        for (ctr in 1:nrow(dtRecords)) {
            
            # Grow the deletions vector
            if ((!preState & dtRecords$chgType[ctr]=="E") | (preState & dtRecords$chgType[ctr]=="B")) {
                delVec <- c(delVec, dtRecords$chgTime[ctr])
                # do not modify the state, it has not changed due to the deletion
            } else {
                # do not grow the deletion vector, the state change is OK here if chgType is not "N"
                if (dtRecords$chgType[ctr] != "N") { preState <- !preState }
            }
            
            # Create a single addition if needed
            if (ctr==nrow(dtRecords) & preState != dtRecords$curPrecip[ctr]) {
                if (dtRecords$curPrecip[ctr]) { addBegin <- c(addBegin, dt) }
                if (!dtRecords$curPrecip[ctr]) { addEnd <- c(addEnd, dt) }
            }
        }
        
    }
    
    # If there is precipitation at the very end, addEnd for dTime+1
    fixEnd <- dfNoIssues %>%
        filter(lastRecord & curPrecip) %>%
        mutate(timeUse=dtime+1) %>%
        pull(timeUse)
    if (length(fixEnd) > 0) {
        addEnd <- c(addEnd, fixEnd)
        cat("\nAdding final end time to cap interval at end of file:", as.character(fixEnd), "\n")
    }
    
    # Print the key vectors
    cat("\nStart/end times deleted:\n")
    print(lubridate::as_datetime(delVec))
    cat("\nBegin times added\n")
    print(lubridate::as_datetime(addBegin))
    cat("\nEnd times added:\n")
    print(lubridate::as_datetime(addEnd))
    
    # Create the full list of issue start times
    beginIssues <- dfIssues %>%
        filter(chgType=="B") %>%
        pull(chgTime)
    endIssues <- dfIssues %>%
        filter(chgType=="E") %>%
        pull(chgTime)
    
    beginIssues <- c(beginIssues[!beginIssues %in% delVec], addBegin)
    endIssues <- c(endIssues[!endIssues %in% delVec], addEnd)

    # cat("\nNew begin times list from issues:\n")
    # print(beginIssues)
    # cat("\nNew end times list from issues:\n")
    # print(endIssues)
    
    # Create the full list of start and end times
    beginOK <- dfNoIssues %>%
        filter(chgType=="B") %>%
        pull(chgTime)
    endOK <- dfNoIssues %>%
        filter(chgType=="E") %>%
        pull(chgTime)
    
    # Return a list of beginTimes and endTimes
    list(beginTimes=sort(c(beginOK, beginIssues)), 
         endTimes=sort(c(endOK, endIssues))
         )
    
}


fnPrecip6 <- function(lst, df) {

    # Extract the beginning and interval times
    begins <- lst[["beginTimes"]]
    ends <- lst[["endTimes"]]
    durs <- ends - begins

    # Create intervals from the raw list file
    precipInts <- interval(begins, ends - 1) # make the interval stop the minute before the end time
    
    # Extract the METAR and date-time information and check for overlaps
    dtime <- df %>% pull(dtime)
    metar <- df %>% pull(origMETAR)
    precipMETAR <- df %>% pull(curPrecip)
    intMETAR <- sapply(dtime, FUN=function(x) {x %within% precipInts %>% any()})

    # Check for the consistency of the observations and print the mismatches
    print(table(precipMETAR, intMETAR))

    mism <- which(precipMETAR != intMETAR)
    if (length(mism) == 0) {
        cat("\nFull matches between METAR observations and intervals\n")
    } else {
        for (x in mism) {
            cat("\nMismatch at time", strftime(dtime[x], format="%Y-%m-%d %H:%M", tz="UTC"), "UTC\n")
            print(metar[max(1, x-2):min(length(metar), x+2)])
        }
    }
    
    list(beginTimes=lst$beginTimes, endTimes=lst$endTimes, 
         precipInts=precipInts, mismatches=mism, mismatchTimes=dtime[mism]
         )
    
}


# Extract precipitation information (total hours, number of events) by locale and month
# Need to fix issue of hard-coding minTime and maxTime to 2016 (fixed, but uses a single minTime and a single maxTime which means each batch of years must be processed separately)
precipByLocale <- function(lst, 
                           listItem="beginEndInterval", 
                           subName=-4, 
                           intervalVar="precipInts", 
                           minTime=NULL,
                           maxTime=NULL, 
                           mapper=cityNameMapper
                           ) {
    
    # FUNCTION ARGUMENTS:
    # lst - the list containing the output for the specified precipitation type
    # listItem - the name of the item to extract from each element of the list
    # subName - the second argument for str_sub() for converting list item name to original file name
    # intervalVar - name of the interval variable in listItem
    # minTime - ignore any interval with a start time before this date (guess from data if NULL)
    # maxTime - ignore any interval with a start time after this date (guess from data if NULL)
    # mapper - named vector for mapping source to descriptive locale
    
    # Get the names of the objects in the list
    beNames <- names(lst) %>% str_sub(1, subName)
    
    # Get the interval data by locale and apply the original file names
    beData <- sapply(lst, FUN="[", listItem)
    names(beData) <- beNames
    
    # If minTime is NULL or maxTime is NULL, infer a best starting month from the data
    if (is.null(minTime) | is.null(maxTime)) {
        
        # Grab total occurrences of dtime in "pStateFrame"
        # Set minTime and maxTime to keep only year-month combos that average 27+ full days of data
        metData <- sapply(lst, FUN="[", "pStateFrame") %>%
            bind_rows() %>%
            count(ym=paste0(lubridate::year(dtime), "-", zeroPad(lubridate::month(dtime)))) %>%
            mutate(pct=n/max(n), okUse=(pct > 27/31)) %>%
            group_by(okUse) %>%
            mutate(ymMin=first(ym), ymMax=last(ym)) %>%
            ungroup()
        
        # Grab the overall minimum and maximum times
        ovrMin <- metData %>%
            filter(okUse) %>%
            pull(ymMin) %>%
            first()
        ovrMax <- metData %>%
            filter(okUse) %>%
            pull(ymMax) %>%
            first()
        
        # Flag any issues
        contIssues <- metData %>%
            mutate(metProb1=okUse & (ym < ovrMin | ym > ovrMax), 
                   metProb2=!okUse & ym >= ovrMin & ym <= ovrMax
                   )
        if (contIssues %>% summarize(sum(metProb1 | metProb2)) %>% pull() > 0) {
            cat("\nMinimum and maximum time alignment issue\n")
            print(contIssues)
        }
        
        # Set the minimum time based on day 1 of ovrMin and maximum time based on last day of ovrMax
        if (is.null(minTime)) { minTime <- lubridate::ymd(paste0(ovrMin, "-01")) }
        if (is.null(maxTime)) { maxTime <- lubridate::ymd(paste0(ovrMax, "-01")) + months(1) - days(1) }
        cat("\nWill use minTime:", as.character(minTime), "and maxTime:", as.character(maxTime), "\n")
        
    }

    # Get the total precipitation length and total precipitation events by month
    # Exclude any event that begins before beginDate or after afterDate
    # Count the precipitation as being in the month when it begins
    
    # Helper function for extraction of total precipitation by month
    getMonthly <- function(x, y=intervalVar, minT=minTime, maxT=maxTime) {
        x %>%
            mutate(intStart=lubridate::int_start(get(intervalVar)), 
                   intHours=lubridate::time_length(get(intervalVar))/3600, 
                   month=factor(month.abb[lubridate::month(intStart)], levels=month.abb)
                   ) %>%
            filter(lubridate::as_date(intStart) >= minT, lubridate::as_date(intStart) <= maxT) %>%
            group_by(month) %>%
            summarize(hours=sum(intHours), events=n()) %>%
            right_join(tibble::tibble(month=factor(month.abb, levels=month.abb)), by="month") %>%
            mutate(hours=ifelse(is.na(hours), 0, hours), 
                   events=ifelse(is.na(events), 0 , events)
                   ) %>%
            select(month, hours, events) %>%
            pivot_longer(-month, names_to="variable", values_to="value")
    }
    
    # Extract total hours and events by month
    monPrecip <- purrr::map_dfr(beData, .f=getMonthly, .id="source")
    
    # Pivot wider so that file is unique by source-month with hours and events are columns
    # Add locale name from mapper
    monPrecip %>%
        pivot_wider(c(source, month), names_from="variable", values_from="value") %>%
        mutate(locale=mapper[source]) %>%
        select(source, locale, everything())
    
}


# Plot precipitation length and events, facetted by locale or month
plotPrecipitation <- function(tbl, 
                              pTypeName=""
                              ) {
    
    # FUNCTION ARGUMENTS:
    # tbl: the tibble containing the processed data
    # pTypeName: the type of precipitation as a character
    
    # Plot 1: Total precipitation length by locale
    p1 <- tbl %>%
        group_by(locale) %>%
        summarize(hours=sum(hours)) %>%
        ggplot(aes(x=fct_reorder(locale, hours), y=hours)) +
        geom_col(fill="lightblue") +
        geom_text(aes(label=round(hours), y=hours/2)) + 
        coord_flip() + 
        labs(x="", 
             y="Hours", 
             title=paste0("Total hours of ", stringr::str_to_lower(pTypeName), " by locale")
             )
    print(p1)
    
    
    # Plot 2: Total precipitation events by locale
    p2 <- tbl %>%
        group_by(locale) %>%
        summarize(events=sum(events)) %>%
        ggplot(aes(x=fct_reorder(locale, events), y=events)) +
        geom_col(fill="lightblue") +
        geom_text(aes(label=round(events), y=events/2)) + 
        coord_flip() + 
        labs(x="", 
             y="# Unique Events", 
             title=paste0("Total unique events of ", stringr::str_to_lower(pTypeName), " by locale"), 
             caption="An event is defined by a begin and end and may be as short as a minute\nor with as little as a minute gap to the next event"
             )
    print(p2)
    
    
    # Plot 3: Precipitation length by month, facetted by locale
    mm <- tbl %>%
        group_by(month) %>%
        summarize(hours=mean(hours))
    p3 <- tbl %>%
        ggplot(aes(x=month, y=hours)) +
        geom_col(fill="lightblue") +
        labs(y="Hours", 
             x="", 
             title=paste0("Total hours of ", stringr::str_to_lower(pTypeName), " by month"), 
             subtitle="Red dots are the monthly average across locales in this plot"
             ) + 
        geom_point(data=mm, color="red") +
        facet_wrap(~ locale) + 
        theme(axis.text.x=element_text(angle=90))
    print(p3)
    
    # Plot 4: Precipitation events by month, facetted by locale
    mm <- tbl %>%
        group_by(month) %>%
        summarize(events=mean(events))
    p4 <- tbl %>%
        ggplot(aes(x=month, y=events)) +
        geom_col(fill="lightblue") +
        labs(y="# Unique Events", 
             x="", 
             title=paste0("Total unique events of ", stringr::str_to_lower(pTypeName), " by month"), 
             subtitle="Red dots are the monthly average across locales in this plot", 
             caption="An event is defined by a begin and end and may be as short as a minute\nor with as little as a minute gap to the next event"
             ) + 
        geom_point(data=mm, color="red") +
        facet_wrap(~ locale) + 
        theme(axis.text.x=element_text(angle=90))
    print(p4)
    
}


```
  
The functions can then be run for rain, snow, and thunder, using only the 2016 data:  
```{r cache=TRUE}

memFiles <- ls(pattern="^k[a-z]{3}_2016$")
cat("\nProcesses will be run for files:\n\n", paste0(memFiles, collapse="\n"), "\n", sep="")

# Run for rain, with logs and pdf sent to files
rain_2016_List <- wrapPrecipTimes(memFiles, 
                                  pType="(?<!FZ)RA", 
                                  pExt="_RA", 
                                  pTypeName="rain", 
                                  writeLogFile="rain_2016_IntervalTimes.log",
                                  writeLogPDF="rain_2016_IntervalTimes.pdf",
                                  writeLogPath=filePath,
                                  appendWriteFile=FALSE
                                  )

# Run for snow, with logs and pdf sent to files
snow_2016_List <- wrapPrecipTimes(memFiles, 
                                  pType="(?<!BL)SN", 
                                  pExt="_SN", 
                                  pTypeName="snow", 
                                  writeLogFile="snow_2016_IntervalTimes.log",
                                  writeLogPDF="snow_2016_IntervalTimes.pdf",
                                  writeLogPath=filePath,
                                  appendWriteFile=FALSE
                                  )

# Run for thunder, with logs and pdf sent to files
thunder_2016_List <- wrapPrecipTimes(memFiles, 
                                     pType="(?<!VC)TS", 
                                     pExt="_TS", 
                                     pTypeName="thunder", 
                                     writeLogFile="thunder_2016_IntervalTimes.log",
                                     writeLogPDF="thunder_2016_IntervalTimes.pdf",
                                     writeLogPath=filePath,
                                     appendWriteFile=FALSE
                                     )


```
  
The functions can then be run for rain, snow, and thunder, using only the 2015 data:  
```{r cache=TRUE}

memFiles <- ls(pattern="^k[a-z]{3}_2015$")
cat("\nProcesses will be run for files:\n\n", paste0(memFiles, collapse="\n"), "\n", sep="")

# Run for rain, with logs and pdf sent to files
rain_2015_List <- wrapPrecipTimes(memFiles, 
                                  pType="(?<!FZ)RA", 
                                  pExt="_RA", 
                                  pTypeName="rain", 
                                  writeLogFile="rain_2015_IntervalTimes.log",
                                  writeLogPDF="rain_2015_IntervalTimes.pdf",
                                  writeLogPath=filePath,
                                  appendWriteFile=FALSE
                                  )

# Run for snow, with logs and pdf sent to files
snow_2015_List <- wrapPrecipTimes(memFiles, 
                                  pType="(?<!BL)SN", 
                                  pExt="_SN", 
                                  pTypeName="snow", 
                                  writeLogFile="snow_2015_IntervalTimes.log",
                                  writeLogPDF="snow_2015_IntervalTimes.pdf",
                                  writeLogPath=filePath,
                                  appendWriteFile=FALSE
                                  )

# Run for thunder, with logs and pdf sent to files
thunder_2015_List <- wrapPrecipTimes(memFiles, 
                                     pType="(?<!VC)TS", 
                                     pExt="_TS", 
                                     pTypeName="thunder", 
                                     writeLogFile="thunder_2015_IntervalTimes.log",
                                     writeLogPDF="thunder_2015_IntervalTimes.pdf",
                                     writeLogPath=filePath,
                                     appendWriteFile=FALSE
                                     )

```
  
The functions can then be run for rain, snow, and thunder, using only the 2017 data:  
```{r cache=TRUE}

memFiles <- ls(pattern="^k[a-z]{3}_2017$")
cat("\nProcesses will be run for files:\n\n", paste0(memFiles, collapse="\n"), "\n", sep="")

# Run for rain, with logs and pdf sent to files
rain_2017_List <- wrapPrecipTimes(memFiles, 
                                  pType="(?<!FZ)RA", 
                                  pExt="_RA", 
                                  pTypeName="rain", 
                                  writeLogFile="rain_2017_IntervalTimes.log",
                                  writeLogPDF="rain_2017_IntervalTimes.pdf",
                                  writeLogPath=filePath,
                                  appendWriteFile=FALSE
                                  )

# Run for snow, with logs and pdf sent to files
snow_2017_List <- wrapPrecipTimes(memFiles, 
                                  pType="(?<!BL)SN", 
                                  pExt="_SN", 
                                  pTypeName="snow", 
                                  writeLogFile="snow_2017_IntervalTimes.log",
                                  writeLogPDF="snow_2017_IntervalTimes.pdf",
                                  writeLogPath=filePath,
                                  appendWriteFile=FALSE
                                  )

# Run for thunder, with logs and pdf sent to files
thunder_2017_List <- wrapPrecipTimes(memFiles, 
                                     pType="(?<!VC)TS", 
                                     pExt="_TS", 
                                     pTypeName="thunder", 
                                     writeLogFile="thunder_2017_IntervalTimes.log",
                                     writeLogPDF="thunder_2017_IntervalTimes.pdf",
                                     writeLogPath=filePath,
                                     appendWriteFile=FALSE
                                     )

```
  
#### _Extracting Precipitation and Temperature Summaries_  
METAR periodically contain aggregated summaries of the day's weather.  Two areas summarized include temperature and precipitation:  
  
* RMK.* 4dddddddd - the first dddd are the daily high temperature (leading d is 1 for negative, trailing d is decimal, report is in Celsius) and the second dddd are the daily low temperature  
* RMK.* Pdddd - the amount of liquid precipipitation in the past hour, in hundredths of inches  
* RMK.* 6dddd - the amount of liquid precipitation in the past 3/6 hours, in hundredths of inches  
* RMK.* 7dddd - the amount of liquid precipitation in the past 24 hours, in hundredths of inches  
  
A function is written to extract daily high and low temperatures:  
```{r}

# Extract the daily high and low temperature (convert to rounded Fahrenheit)
getDailyHighLow <- function(df) {

    # If a character has been passed, convert to the associated object
    if (is.character(df)) {
        df <- get(df)
    }
    
    df %>%
        select(dtime, origMETAR) %>%
        mutate(tempData=str_extract(origMETAR, pattern="RMK.* 4\\d{8}"), 
               tempFHi=convertTemp(str_sub(tempData, -8, -5)), 
               tempFLo=convertTemp(str_sub(tempData, -4, -1))
               ) %>%
        select(-origMETAR)
    
}

```
  
The function can then be run on all the relevant files:  
```{r}

# Extract all of the relevant files
fileNames <- names(cityNameMapper)
fileNames

# Extracted high-low temperatures
all_hilo <- map_dfr(.x=fileNames, .f=getDailyHighLow, .id="source") %>%
    mutate(source=fileNames[as.integer(source)], locale=cityNameMapper[source]) %>%
    select(source, locale, everything())
all_hilo

```
  
A function can then be written to create relevant plots for the high-low temperature timing, routed to a file if requested:  
```{r}

plotDailyHiLo <- function(df, 
                          pdfName=NULL,
                          pdfPath=filePath
                          ) {
    
    # FUNCTION ARGUMENTS
    # df: the data frame or tibble containing the extracted high-low temperature data
    # pdfName: the file name for saving the PDF plots (NULL means print to stdout)
    # pdfPath: location to save PDF plots (only used if pdfName is not null; NULL means no pre-pended path)
    
    # Send to file if pdfName is provided
    if (!is.null(pdfName)) {
        
        # Pre-pend pdfPath if provided
        if (!is.null(pdfPath)) { pdfName <- paste0(pdfPath, pdfName) }
        
        # Provide the location of the EDA pdf file
        cat("\nDaily High-Low Temperature PDF file is available at:", pdfName, "\n")

        # Redirect the writing to writeLogPDF
        pdf(pdfName)
    }
    

    # Data with the hour and notNA status
    dat <- df %>%
        mutate(hour=lubridate::hour(lubridate::round_date(dtime, unit="hours")), 
               month=lubridate::month(dtime),
               notNA=!is.na(tempFHi)
               )
    
    # Plot for counts of !is.na by hour
    p1 <- dat %>%
        ggplot(aes(x=hour, fill=notNA)) + 
        geom_bar(position="stack") + 
        scale_fill_discrete("Hi-Lo Data") + 
        labs(y="Number of Daily Observations", x="METAR Hour", 
             title="Daily High-Low temperatures are captured based on time zone"
             )
    multiPageFacet(p1, facetVar="locale", balancePages=TRUE, maxPerPage=6)
    
    # Summary of notNA times by locale
    cat("\nZulu times for high-low temperatures by locale\n\n")
    notNATimes <- dat %>%
        mutate(hourZ=ifelse(notNA, paste0("Time_", zeroPad(hour), "Z"), "Time_NA")) %>%
        group_by(locale, hourZ) %>%
        summarize(n=n()) %>%
        ungroup() 
    notNANames <- notNATimes %>%
        filter(hourZ != "Time_NA") %>%
        pull(hourZ) %>%
        unique() %>%
        sort()
    notNATimes %>%
        pivot_wider(locale, names_from="hourZ", values_from="n") %>%
        mutate_at(vars(all_of(notNANames)), ~ifelse(is.na(.), 0, .)) %>%
        select_at(vars(all_of(c("locale", notNANames, "Time_NA")))) %>%
        as.data.frame() %>%
        print()

    # Box plots of high and low temperatures by locale by month
    p2 <- dat %>%
        filter(!is.na(tempData)) %>%
        ggplot(aes(x=factor(month.abb[month], levels=month.abb))) + 
        geom_boxplot(aes(y=tempFHi), fill="pink") + 
        labs(y="Temperature (F)", x="", 
             title="Daily High Temperatures by Month by Locale"
             ) + 
        theme(axis.text.x=element_text(angle=90))
    multiPageFacet(p2, 
                   facetVar="locale", 
                   maxPerPage=12, 
                   balancePages=TRUE, 
                   sameYLimits="tempFHi",
                   useYLow="tempFHi"
                   )

    p3 <- dat %>%
        filter(!is.na(tempData)) %>%
        ggplot(aes(x=factor(month.abb[month], levels=month.abb))) + 
        geom_boxplot(aes(y=tempFLo), fill="lightblue") + 
        labs(y="Temperature (F)", x="", 
             title="Daily Low Temperatures by Month by Locale"
             ) + 
        theme(axis.text.x=element_text(angle=90))
    multiPageFacet(p3, 
                   facetVar="locale", 
                   maxPerPage=12, 
                   balancePages=TRUE, 
                   sameYLimits="tempFLo",
                   useYLow="tempFLo"
                   )

    # If pdfName is not NULL, redirect to stdout
    if (!is.null(pdfName)) {
        dev.off()
    }

}

```
  
The function can then be run for all the high-low temperature data, with output directed to PDF:  
```{r cache=TRUE}

plotDailyHiLo(all_hilo, pdfName="DailyHighLow.pdf")

```

The high-low temperatures appear to be recorded at 6Z in Central time zone locales; at 5Z in East time zone locales; at 8Z in 2 West time zone locales.
  
A function is also written to extract the 1-hour, 3/6-hour, and 24-hour liquid precipitation totals from METAR data:  
```{r}

# Extract precipitation amounts in inches
getPrecipAmount <- function(df) {
    
    # If a character has been passed, convert to the associated object
    if (is.character(df)) {
        df <- get(df)
    }

    # Helper function convertPrecip() contained in the helper functions section of this file    
    df %>%
        select(dtime, origMETAR) %>%
        mutate(p1Inches=convertPrecip(str_extract(origMETAR, pattern="RMK.* P\\d{4}")), 
               p36Inches=convertPrecip(str_extract(origMETAR, pattern="RMK.* 6\\d{4}")), 
               p24Inches=convertPrecip(str_extract(origMETAR, pattern="RMK.* 7\\d{4}"))
               ) %>%
        select(-origMETAR)
    
}

```
  
The function can be run for all the data:  
```{r}

# Extracted precipitation amounts
all_pin <- map_dfr(.x=fileNames, .f=getPrecipAmount, .id="source") %>%
    mutate(source=fileNames[as.integer(source)], locale=cityNameMapper[source]) %>%
    select(source, locale, everything())
all_pin

```
  
A function can be created to extract the time periods in which 1-hour, 3/6-hour, and 24-hour precipitation summaries are captured:  
```{r}

# Function to plot the time periods in which precipitation summaries are captured
plotPrecipNAHours <- function(df, 
                              mapper, 
                              pdfName=NULL, 
                              pdfPath=filePath
                              ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble containing the precipitation data
    # mapper: file containing variables mapped to descriptive plotting names
    # pdfName: name for PDF files (NULL means print to stdout)
    # pdfPath: path for PDF files (only used if pdfName is not null; if pdfPath is null, no pre-pend)

    # Send to file if pdfName is provided
    if (!is.null(pdfName)) {
        
        # Pre-pend pdfPath if provided
        if (!is.null(pdfPath)) { pdfName <- paste0(pdfPath, pdfName) }
        
        # Provide the location of the EDA pdf file
        cat("\nDaily High-Low Temperature PDF file is available at:", pdfName, "\n")

        # Redirect the writing to writeLogPDF
        pdf(pdfName)
    }
    
    # Plot the time periods provided in mapper
    for (plotVar in names(mapper)) {
        # Counts of !is.na by hour
        p1 <- df %>%
            mutate(hour=factor(lubridate::hour(lubridate::round_date(dtime, unit="hours"))), 
                   notNA=!is.na(get(plotVar))
                   ) %>%
            ggplot(aes(x=hour, fill=notNA)) + 
            geom_bar(position="stack") + 
            scale_fill_discrete(paste0(mapper[plotVar], " Data")) + 
            labs(y="Number of Days with Observations at Hour", 
                 x="METAR Hour", 
                 title=paste0("Capture for ", mapper[plotVar])
                 ) + 
            facet_wrap(~locale) + 
            scale_x_discrete(breaks=seq(0, 23, by=3))
        print(p1)
    }

    # If pdfName is not NULL, redirect to stdout
    if (!is.null(pdfName)) {
        dev.off()
    }
    
}

```
  
Plots can then be created for the 1-hour, 3/6-hour, and 24-hour summaries:  
```{r cache=TRUE}

# Map of variable to chart name
mapChartVar <- c(p1Inches="1-hr Precip", p36Inches="3-hr or 6-hr Precip", p24Inches="24-hr Precip")

# Run the precipitation plots
plotPrecipNAHours(all_pin, mapper=mapChartVar, pdfName="Precipitation_NonNA_Hours.pdf")

```
  
Further, summaries can be created for when the various times are used:  
```{r}

# Plot the time periods provided in mapper
for (plotVar in names(mapChartVar)) {
    # Counts of !is.na by hour
    p1 <- all_pin %>%
        mutate(hour=factor(lubridate::hour(lubridate::round_date(dtime, unit="hours"))), 
               notNA=!is.na(get(plotVar))
               ) %>%
        group_by(source, hour) %>%
        summarize(pct=mean(notNA)) %>%
        ggplot(aes(x=hour, y=pct, group=source)) + 
        geom_line(alpha=0.25) + 
        labs(y="Percent of Days with Observations at Hour", 
             x="METAR Hour (Z)", 
             title=paste0("Capture for ", mapChartVar[plotVar]), 
             caption="Each line is a single location/year"
             ) +
        scale_x_discrete(breaks=seq(0, 23, by=3))
    print(p1)
}

```
  
While there is significant variation in the frequency of precipitation by locale and year, several trends are evident in the data:  
  
* Hourly precipitation data is recorded as needed in any hour (if precipitation occurred in the past hour)  
* 3/6 hourly precipitation is recorded as needed in any time period 0Z-3Z-6Z-9Z-12Z-15Z-18Z-21Z as needed (if precipitation occurred in the past 3/6 hours)  
* 24-hourly precipitation is recorded as needed at 12Z (if precipitation occurred in the past 24 hours)  
  
The precipitation sums can also be checked for consistency:  
```{r}

# Sum precipitation by year for p1, p36 (using only 0Z-6Z-12Z-18Z), p24
mod_pin <- all_pin %>%
    mutate(p1Inches=ifelse(is.na(p1Inches), 0, p1Inches),
           p36ok=lubridate::hour(lubridate::round_date(dtime, unit="hours")) %in% c(0, 6, 12, 18),
           p36Inches=ifelse(is.na(p36Inches) | !p36ok, 0, p36Inches),
           p24Inches=ifelse(is.na(p24Inches), 0, p24Inches)
           )

mod_pin %>%
    group_by(locale) %>%
    summarize_if(is.numeric, sum) %>%
    rename(`1-Hour`=p1Inches, `6-Hour`=p36Inches, `24-Hour`=p24Inches) %>%
    pivot_longer(-locale, names_to="Unit", values_to="Inches") %>%
    ggplot(aes(x=Unit, y=Inches)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(label=round(Inches, 1), y=Inches+10)) +
    labs(title="Annual precipitation when summing by various units", x="") +
    facet_wrap(~locale)

```
  
There are some minor differences when summing annual precipitation by different units.  The biggest outlier is the Houston 2016 1-hour total being ~10 inches greater than the 6-hour totals or 24-hour totals:  
```{r}

# Explore the Houston data by month
all_pin %>% 
    filter(source=="kiah_2016") %>% 
    group_by(month=lubridate::month(dtime)) %>% 
    summarize(p1=sum(p1Inches, na.rm=TRUE), p24=sum(p24Inches, na.rm=TRUE))

# Explore the April data by day
all_pin %>% 
    filter(source=="kiah_2016", lubridate::month(dtime)==4) %>% 
    group_by(day=lubridate::day(dtime)) %>% 
    summarize(p1=sum(p1Inches, na.rm=TRUE), p24=sum(p24Inches, na.rm=TRUE)) %>%
    filter(p1>0 | p24 > 0)

# Explore the April 18 data
kiah_2016 %>%
    filter(dtime == lubridate::ymd_hm("2016-04-18 17:53")) %>%
    pull(origMETAR)

```

Houston experienced near-record flooding on April 18, 2016 due to ~10 inches of rain.  The 1-hour totals appear to capture this, while the 24-hour total is absent on April 18 and 7//// (data bad) on April 19.  The April 18, 2016 1753Z METAR captures this extreme rainfall event in an explanatory note.
  
An automated routine is designed to check for other material mimatches in liquid precipitation by month.  A material mismatch is defined as rainfall totals where the minimum is under 90% of the maximum, and there is a gap of at least 0.25 inches.  The 1-hour and 24-hour summaries are used:  
```{r}

pIssues <- mod_pin %>%
    mutate(month=lubridate::month(dtime)) %>%
    group_by(locale, month) %>%
    summarize(p1Inches=sum(p1Inches), p24Inches=sum(p24Inches)) %>%
    ungroup() %>%
    mutate(absDelta=abs(p1Inches-p24Inches), 
           pctDelta=absDelta/pmax(p1Inches, p24Inches), 
           mism=(absDelta >= 0.25) & (pctDelta > 0.1)
           )

pIssues %>%
    count(mism)

pIssues %>% 
    filter(mism) %>% 
    arrange(-absDelta)

```
  
There are meaningful mismatches in the data, with 50 month-locale combinations having a material mismatch.  And, since material mismatch was defined in a forgiving manner (need to be 0.25 inches and 10% different), there are likely a number of underlying factors.  Further, since the annual sums are roughly the same across metrics in most cases, there is no obvious pattern such as "1-hour sums tend to record high".

This data may be of lower integrity, which should be considered during the modeling process.
  
#### _Cross-Locale EDA Summaries_  
The basic EDA summaries can also be run across locales.  First, a function is written to attach locale names and create a single file for 1+ locations:  
```{r}

# Combine files from a character list
combineProcessedFiles <- function(charList, mapper=cityNameMapper) {
    
    # Combine the objects represented by charList, and name the list items using charList
    listFiles <- lapply(charList, FUN=function(x) { get(x) })
    names(listFiles) <- charList
    
    # Bind rows, and add the descriptive locale name as sourceName
    tblFiles <- bind_rows(listFiles, .id="source") %>%
        mutate(sourceName=mapper[source])
    
    tblFiles
    
}

```
  
The function is then applied to every file in cityNameMapper:  
```{r}

# Combine the 2016 data
allData <- combineProcessedFiles(names(cityNameMapper))

# Show counts by sourceName
allData %>%
    count(source, sourceName)

```
  
The following global summaries will be useful:  
  
* plotCountsByMetric() - overall percentage by metric, applied to total counts for locale  
* plotNumCor() - overall geom_smooth()  
* plotFactorNumeric() - overall mean/median of numeric by factor  
* corMETAR() and lmMETAR() - not applicable, though could be run on full dataset  
* basicWindPlots() - tbd, perhaps adapt along with consolidatePlotWind  
* consolidatePlotWind() - tbd, perhaps adapt along with basicWindPlots  
  
Helper functions can be created for:  
  
* helperCountsByMetric() - get the overall percentage by metric for variable x  
* helperNumCor() - get an overall geom_smooth for variable y vs. variable x  
* helperFactorNumeric() - get the overall mean or median for numeric variable y by factor variable x  
  
The helper functions are created below:  
```{r}

# Helper function to get overall percentage by metric for variable x
helperCountsByMetric <- function(tbl, 
                                 ctVar, 
                                 sumOn="dummyVar"
                                 ) {

    # FUNCTION ARGUMENTS:
    # tbl: the tibble or data frame
    # ctVar: the variable for which overall percentage is desired
    # sumOn: the variable to be summed (default is counts, and the program makes "dummyVar" for this)
    
    tbl %>%
        mutate(dummyVar=1) %>%
        select_at(vars(all_of(c(ctVar, sumOn)))) %>%
        filter_all(all_vars(!is.na(.))) %>%
        group_by_at(ctVar) %>%
        summarize(n=sum(get(sumOn))) %>%
        mutate(nPct=n/sum(n))
        
}


# Helper function to get a geom_smooth for variable y vs variable x
helperNumCor <- function(tbl, 
                         xVar, 
                         yVar, 
                         sumOn="dummyVar",
                         se=TRUE, 
                         color="red", 
                         method="lm", 
                         lty=2
                         ) {
    
    # FUNCTION ARGUMENTS:
    # tbl: the tibble or data frame
    # xVar: the x-axis variable
    # yVar: the y-axis variable
    # sumOn: the variable to be summed (default is counts, and the program makes "dummyVar" for this)
    # se: whether to include the standard error in the overall geom_smooth()
    # color: color for the main line in the overall geom_smooth()
    # method: will be passed as geom_smooth(method=method)
    # lty: the line-type for the main line in the geom_smooth (default, 2, is dashed)
    
    # Generate the overall totals for sumOn by xVar and yVar
    plotData <- tbl %>%
        mutate(dummyVar=1) %>%
        select_at(vars(all_of(c(xVar, yVar, sumOn)))) %>%
        filter_all(all_vars(!is.na(.))) %>%
        group_by_at(vars(all_of(c(xVar, yVar)))) %>%
        summarize(nTotal=sum(get(sumOn)))
    
    geom_smooth(data=plotData, 
                aes_string(x=xVar, y=yVar, weight="nTotal"), 
                se=se, 
                color=color, 
                method=method, 
                lty=lty
                )
    
}


# Helper function to calculate .f(numVar) by byVar
helperFactorNumeric <- function(tbl, 
                                .f, 
                                byVar, 
                                numVar, 
                                ...
                                ) {
    
    # FUNCTION ARGUMENTS:
    # tbl: the tibble or data frame
    # .f: the function to be applied
    # byVar: the grouping variable for applying the function
    # numVar: the numeric variable the function is applied to
    # ...: other arguments passed to .f, such as na.rm=TRUE
    
    tbl %>%
        select_at(vars(all_of(c(byVar, numVar)))) %>%
        filter_all(all_vars(!is.na(.))) %>%
        group_by_at(byVar) %>%
        summarize(helpFN=.f(get(numVar), ...))
    
}

```
  
The main plotting functions above were already written to call and use the helper functions.

It is now possible to re-run the EDA plotting routines across all locales:  
```{r cache=TRUE}

# Create rounded TempF and DewF in allData
allData <- allData %>%
    mutate(TempF5=5*round(round(TempF)/5), 
           DewF5=5*round(round(DewF)/5), 
           WindSpeed5=5*round(WindSpeed/5),
           Altimeter10=round(Altimeter, 1)
           )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleCountsByMetric.pdf"))

# Counts by Metric for allData
plotcountsByMetric(allData, 
                   mets=c("month", "year",
                          "WindDir", "WindSpeed5", 
                          "Visibility", "Altimeter10",
                          "TempF5", "DewF5", 
                          "wType"
                          ), 
                   title="Comparisons Across Locales (red dots are the median)", 
                   facetOn="sourceName",
                   showCentral=TRUE, 
                   multiPageWrap=TRUE, 
                   maxPerPage=12, 
                   balancePages=TRUE
                   )

# Redirect to standard plotting
dev.off()

```
  
The cross-locale comparisons bring out a few salient features:  
  
DATA VOLUMES:  
* All locale-year combinations have roughly the same amount of data by month, all focused on the year of interest (with 1-2 days on either side added as buffers)  
  
WIND DIRECTION and WIND SPEED:  
* Las Vegas has an excess of no/variable wind and of southerly winds, both appropriate for a desert  
* Houston has an excess of no wind and southerly winds, both appropriate for the Gulf Coast  
* San Diego has an excess of no wind and of northwesterly wind, both appropriate for the Pacific coast  
* Chicago, Grand Rapids, Indianapolis, Minneapolis, and Newark all have lower occurences of no wind, appropriate for relatively cold mid-latitude cities  
* Detroit, Green Bay, Lincoln, Madison, Milwaukee, and New Orleans all look "about average"; this is not surprising in the first four cases given the predominance of cold, mid-latitude locales, but is surprising for New Orleans  
* Traverse City is surprising in showing a predominance of no/variable winds; this is unexpected for a cold-weather city in the mid-latitudes and merits further examination  
  
VISIBILITY:  
* The overwhelming majority of visibilities are 10SM (the highest that is recorded; more or less means unlimited in the METAR)  
* There is a data issue with a Visibility > 10 that should be addressed  
* Las Vegas is slightly more likely than most to have unlimited Visibility, while Detroit and Grand Rapids are slightly less likely than most to have unlimited visibility  
  
ALTIMETER:  
* Las Vegas skews low as appropriate for a high-altitude desert locale  
* New Orleans and Houston show less variance, perhaps driven by being roughly at sea level and in close proximity to the Gulf of Mexico  
* San Diego shows very low variance, perhaps driven by being roughly at sea level and in close proximity to the Pacific Ocean  
  
TEMPERATURE:  
* Houston, Las Vegas, and New Orleans skew warm as expected  
* San Diego has very low variance as expected  
* At a gross level, the other cities look similar to the median but with slightly more cold, likely driven by the predominance of cold, mid-latitude locales in the data file  
  
DEW POINT:  
* Houston and New Orleans skew very high as expected  
* Las Vegas skews very low as expected  
* San Diego has very low variance as expected  
  
SKY OBSCURATION:  
* Lincoln and Green Bay are the most likely to be CLR (clear, no clouds on the automated sensor).  This may be driven by a difference in maximum sensor heights, and is unexpected in Green Bay which should be frequently cloudy due to its latitude and proximity to a large body of water  
* Detroit, Traverse City, Grand Rapids, and Minneapolis are especially likely to be overcast  
* Las Vegas is especially likely to not have an overcast
  
Comparisons are run for a few of the numerical correlations:  
```{r cache=TRUE}

# Example for allData - using mixes of WindSpeed, Altimeter, TempF, DewF, TempC, DewC
numCorList <- list(c("TempC", "TempF"), 
                   c("DewC", "DewF"), 
                   c("TempF", "DewF"), 
                   c("Altimeter", "WindSpeed"), 
                   c("Altimeter", "TempF")
                   )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleCorrelations.pdf"))

# Run the list through plotNumCor()
for (x in numCorList) {
    plotNumCor(allData, 
               var1=x[1], 
               var2=x[2], 
               alpha=0.2,
               maxSize=3,
               subT="Red dashed line is the overall slope", 
               diagnose=TRUE, 
               facetOn="sourceName", 
               showCentral=TRUE
               )
}

# Redirect to standard plotting
dev.off()

```
  
The cross-locale comparisons bring out a few salient features:  
  
FAHRENHEIT AND CELSIUS:  
* As expected, TempF/C are perfectly correlated and DewF/C are perfectly correlated.  Since the observations were taken in the US, the TempF/DewF data will be used (TempC/DewC are conversions from the measured TempF/DewF to match the international standard for METAR reporting)  
  
TEMPERATURE AND DEW POINT:  
* While many cities have different clusters of temperature/dew point, all but Las Vegas follow a pattern where the temperature and the dew point tend to run together at a similar rate  
* In Las Vegas, the dew point is largely independent of the temperature  
  
ALTIMETER AND WIND SPEED:  
* As expected, when the altimeter rises, on average, the wind speed falls  
* This tendency is less pronounced in Houston and New Orleans; and more pronounced in Las Vegas, San Diego, and Indianapolis  
  
ALTIMETER AND TEMPERATURE:  
* Overall, low temperatures and high altimeters tend to be observed together  
* This is especially so in Houston, Las Vegas, and New Orleans  
* This is modest or perhaps even close to non-existent in Grand Rapids, Green Bay, and Traverse City  
  
Comparisons are then run for numeric variables against factor variables:  
```{r cache=TRUE}

# Modify windDir so that it is just N, NE, E, SE, S, SW, W, NW, 000, Variable
modData <- allData %>%
    mutate(tempDir=ifelse(is.na(WindDir) | WindDir %in% c("000", "VRB"), -1, as.numeric(WindDir)),
           predomDir=factor(case_when(is.na(WindDir) ~ "Error", 
                                      WindDir=="000" ~ "000", 
                                      WindDir=="VRB" ~ "VRB", 
                                      tempDir >= 337.5 ~ "N", 
                                      tempDir <= 22.5 ~ "N",
                                      tempDir <= 67.5 ~ "NE", 
                                      tempDir <= 112.5 ~ "E", 
                                      tempDir <= 157.5 ~ "SE", 
                                      tempDir <= 202.5 ~ "S", 
                                      tempDir <= 247.5 ~ "SW", 
                                      tempDir <= 292.5 ~ "W", 
                                      tempDir <= 337.5 ~ "NW",
                                      TRUE ~ "Error"
                                      ), 
                            levels=c("Error", "000", "VRB", "NE", "E", "SE", "S", "SW", "W", "NW", "N")
                            )
           )

# Key factor variables include month, wType, predomDir
# Key numeric variables include WindSpeed, Altimeter, TempF, DewF, Visibility
fctNumList <- list(c("month", "WindSpeed"), 
                   c("month", "Altimeter"), 
                   c("month", "TempF"), 
                   c("month", "DewF"), 
                   c("month", "Visibility"),
                   c("wType", "WindSpeed"),
                   c("wType", "Altimeter"),
                   c("wType", "Visibility"),
                   c("predomDir", "WindSpeed"),
                   c("predomDir", "Altimeter"),
                   c("predomDir", "TempF"),
                   c("predomDir", "DewF")
                   )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleFactorVsNumeric.pdf"))

for (x in fctNumList) {
    plotFactorNumeric(modData, 
                      fctVar=x[1], 
                      numVar=x[2], 
                      subT="Red dots are the overall average", 
                      showXLabel=FALSE,
                      diagnose=TRUE, 
                      facetOn="sourceName",
                      showCentral=TRUE, 
                      multiPageWrap=TRUE,
                      maxPerPage=12,
                      balancePages=TRUE
                      )
}

# Redirect to standard plotting
dev.off()

```
  
The cross-locale comparisons bring out a few salient points:  
  
WIND SPEED BY MONTH:  
* The plot is too busy; need to rethink, potentially by removing the outliers and plotting only the box  
  
ALTIMETER BY MONTH:  
* Same as above; this plot is too busy  
  
TEMPERATURE BY MONTH:  
* Seasonal patterns are observed in most of the data, with a warm season centered around July and a cold season centered around December  
* Cities run warm or cold relative to average as expected based on their climates    
  
DEW POINT BY MONTH:  
* Seasonal patterns are observed in most of the data, with the humid seasons tracking with the warm seasons  
* Houston and New Orleans run consistently above the average dew point by month  
* San Diego runs above the average dew point during the cold season  
* Las Vegas runs below the average dew point during the warm season  
  
VISIBILITY BY MONTH:  
* Visibilities are overwhelmingly likely to be 10 SM  
* The Newark, NJ outlier described earlier (19 SM) should be deleted  
* Detroit is especially likely to have Visibility less than 10 SM  
* Grand Rapids and Traverse City have meaningul occurences of Visibility less than 10 SM during the cold season  
  
WIND SPEED BY SKY OBSCURATION:  
* This chart is too busy as per above  
  
ALTIMETER BY SKY OBSCURATION:  
* Not much at a glance  
  
VISIBILITY BY SKY OBSCURATION:  
* The VV and OVC sky obscurations are most associated with low visibilities; VV in particular is almost always associated with very low visibility  
  
WIND SPEED BY WIND DIRECTION:  
* Wind direction "000" is always associated with wind speed 0, as expected  
* Wind direction "VRB" is always associated with a low but non-zero wind speed, as expected  
* While some cities are windier than others, there is no pronounced tendency for wind speed to be highly associated with a given wind direction in any locale  
  
ALTIMETER BY WIND DIRECTION:  
* No gross trends observed  
* The plot is rather busy, especially given the low variance in median/IQR for altimeter relative to the outlier points  
  
TEMPERATURE/DEW POINT BY WIND DIRECTION:  
* Plot is not great for reading and interpreting  
  
Next steps are to modify a few of the plots for better interpretability (less busy, more variation of the core data metric relative to the full y-axis, etc.), investigate and rectify the data issues observed, and save a version of the file for further analysis.
  
A modified boxplot function is created to plot only the median and the IQR, with the goal of having more of the variance in the median visible on the plot:  
```{r}

plotMedianIQR <- function(met, 
                          fctVar, 
                          numVar, 
                          title=NULL, 
                          subT="", 
                          mid=0.5,
                          rng=c(0.25, 0.75),
                          diagnose=TRUE,
                          showXLabel=TRUE,
                          mapper=varMapper,
                          facetOn=NULL, 
                          showCentral=FALSE, 
                          ylimits=NULL, 
                          multiPageWrap = FALSE, 
                          maxPerPage = 9, 
                          balancePages = FALSE, 
                          sameYLimits="hiPoint", 
                          useYLow="loPoint"
                          ) {
    
    # Function arguments
    # met: dataframe or tibble containing raw data
    # fctVar: character vector of variable to be used for the x-axis (factor in the boxplot)
    # numVar: character vector of variable to be used for the y-axis (numeric in the boxplot)
    # title: character vector for plot title
    # subT: character vector for plot subtitle
    # mid: float between 0 and 1 for the quantile to be used as the midpoint
    # rng: length-two float vector for (lo, hi) to be used as the dimensions of the box
    # diagnose: boolean for whether to note in the log the number of NA observations dropped
    # showXLabel: boolean for whether to include the x-label (e.g., set to FALSE if using 'month')
    # mapper: named list containing mapping from variable name to well-formatted name for titles and axes
    # facetOn: a facetting variable for the supplied met (NULL for no faceting)
    # showCentral: boolean for whether to show the central tendency over-plotted on the main data
    # ylimits: length-two numeric for the y-axis minimum and maximum (default NULL uses plot defaults)
    # multiPageWrap: boolean, will call multiPageFacet() rather than facetWrap() if TRUE
    # maxPerPage: integer, maximum facets per plot, passed to multiPageFacet()
    # balancePages: boolean, whether to balance the number of facets per page across all pages or allow the final page to have as few as 1 facets
    # sameYLimits: variable for making all y-axes maxima the same across multiPageFacet; "hiPoint" is created by this function as the ymax variable; will be set to NULL if ylimits has been passed (is not null)
    # useYLow: variable for making all y-axes minima the same across multiPageFacet; "loPoint" is created by this function as the ymin variable; will be set to NULL if ylimits has been passed (is not null)
    
    # Function usage
    # 1.  By default, the function creates a modified boxplot of numVar by fctVar - line at mid, box going from rng[1] to rng[2]
    # 2.  If facetOn is passed as a non-NULL, then the data in #1 will be facetted by facetOn
    # 3.  If showCentral=TRUE, then the overall median of numVar by fctVar will be plotted as a red dot
    # 4.  If multiPageWrap=TRUE, then the faceting data will be paginated so that there is easier readability for the plots on any given page

    
    # Check that the quantile variables are sensible
    if (length(mid) != 1 | length(rng) != 2) {
        stop("Must pass a single value as mid and a length-two vector as rng\n")    
    }
    if (min(c(mid, rng)) < 0 | max(c(mid, rng)) > 1) {
        stop("All values of mid and rng must be between 0 and 1, inclusive\n")
    }
    if ((mid < rng[1]) | (mid > rng[2])) {
        stop("mid must be at least as big as rng[1] and no greater than rng[2]\n")
    }
    quants <- paste0(round(100*c(rng[1], mid, rng[2]), 0), "%", collapse=" ")
    
    # Create the title if not passed
    if (is.null(title)) { 
        title <- paste0("Hourly Observations of ", mapper[numVar], " by ", mapper[fctVar])
    }
    
    # Remove the NA variables
    nOrig <- nrow(met)
    dat <- met %>%
        filter(!is.na(get(fctVar)), !is.na(get(numVar)))
    if (diagnose) { cat("\nRemoving", nOrig-nrow(dat), "records due to NA\n") }
    
    # Create the quantile data by fctVar and (if passed) facetOn
    groupVars <- fctVar 
    if (!is.null(facetOn)) { groupVars <- c(groupVars, facetOn) }
    plotData <- dat %>%
        group_by_at(groupVars) %>%
        summarize(midPoint=quantile(get(numVar), probs=mid), 
                  loPoint=quantile(get(numVar), probs=rng[1]),
                  hiPoint=quantile(get(numVar), probs=rng[2])
                  )
    
    # Create the base plot
    p <- plotData %>%
        ggplot(aes_string(x=fctVar, y="midPoint")) +
        geom_crossbar(aes(ymin=loPoint, ymax=hiPoint), fill="lightblue") +
        labs(title=title,
             subtitle=subT,
             x=ifelse(showXLabel, paste0(mapper[fctVar], " - ", fctVar), ""),
             y=paste0(mapper[numVar], " - ", numVar), 
             caption=paste0("Quantiles plotted: ", quants)
             )
    
    # If showCentral=TRUE, add a dot plot for the overall value of 'mid'
    if (showCentral) {
        centData <- helperFactorNumeric(dat, .f=quantile, byVar=fctVar, numVar=numVar, probs=mid)
        p <- p + geom_point(data=centData, aes(y=helpFN), size=2, color="red")
    }
    
    # If ylim has been passed, use it
    if (!is.null(ylimits)) {
        p <- p + ylim(ylimits)
        # Set sameYLimits and useYLow to NULL since adding ylim() guarantees this in a user-specified manner
        sameYLimits <- NULL
        useYLow <- NULL
    }

    # If facetting has been requested, facet by the desired variable
    # multiPageFacet will print by default, so use print(p) only if multiPageWrap=FALSE
    if (!is.null(facetOn)) {
        if (multiPageWrap) {
            multiPageFacet(p, 
                           facetVar=facetOn, 
                           maxPerPage=maxPerPage, 
                           balancePages=balancePages, 
                           sameYLimits=sameYLimits, 
                           useYLow=useYLow
                           )
        } else {
            p <- p + facet_wrap(as.formula(paste("~", facetOn)))
            print(p)
        }
    } else {
        # Print the plot
        print(p)
    }
    
}

```
  
The function can then be applied in an attempt to get a better look at a few of the comparisons:  
```{r cache=TRUE}

# Key factor variables include month, wType, predomDir
# Key numeric variables include WindSpeed, Altimeter, TempF, DewF, Visibility
fctNumListIQR <- list(c("month", "WindSpeed"), 
                      c("month", "Altimeter"), 
                      c("wType", "WindSpeed"),
                      c("predomDir", "Altimeter"),
                      c("predomDir", "TempF"),
                      c("predomDir", "DewF")
                      )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleIQRFactorVsNumeric.pdf"))

for (x in fctNumListIQR) {
    plotMedianIQR(modData, 
                  fctVar=x[1], 
                  numVar=x[2], 
                  subT="Red dots are the overall mid-quantile", 
                  showXLabel=FALSE,
                  diagnose=TRUE, 
                  facetOn="sourceName",
                  showCentral=TRUE, 
                  multiPageWrap=TRUE,
                  maxPerPage=12,
                  balancePages=TRUE
                  )
}

# Redirect to standard plotting
dev.off()

```
  
Limiting the observations to Q1-Median-Q2 brings out a bit more information in the plots:  
  
WIND SPEED BY MONTH:  
* San Diego is meaningfully less windy than the median, especially during the cold season  
* Traverse City is frequently less windy than the mdeian, which is surprising  
* Chicago frequently runs above the median, as do Lincoln, Milwaukee, Minneapolis  
  
ALTIMETER BY MONTH:  
* San Diego has very little variance in altimeter relative to the other locales  
* Las Vegas runs especially low on altimeter during the warm season  
* There is a seasonal pattern where altimeters tend to run lower during the warm season while also showing a much smaller Q1-Q3 range  
  
WIND SPEED BY SKY OBSCURATION:  
* Newark shows very high wind speeds when obscuration is VV  
* Should re-run with data y-axis capped at 15 to visualize the other locales  
  
ALTIMETER BY WIND DIRECTION:  
* Altimeters tend to be a bit lower when winds are from the S-SW-W-NW, a pattern that seems consistent across all locales  
* Las Vegas runs especially low on altimeter relative to other locales when winds are from E-SE-S    
  
TEMPERATURE BY WIND DIRECTION:  
* Variable winds are associated with higher temperatures than calm (speed 0) winds  
* Winds from W-NW are generally associated with somewhat lower temperatures  
  
DEW POINT BY WIND DIRECTION:  
* Winds from SW-W-NW-N are associated with generally lower dew points than winds from NE-E-SE-S  
* Las Vegas typically runs low on dew points while Houston and New Orleans typically run high on dew points  
* San Diego has little variance in dew points and does not see any dip when winds are from SW-W-NW-N  
  
The wind speed vs. sky obscuration plot is re-run with y-limits that suppress Newark VV:  
```{r cache=TRUE}

plotMedianIQR(modData, 
              fctVar="wType", 
              numVar="WindSpeed", 
              subT="Red dots are the overall mid-quantile", 
              showXLabel=FALSE,
              diagnose=TRUE, 
              facetOn="sourceName",
              showCentral=TRUE, 
              ylimits=c(0, 15), 
              multiPageWrap=TRUE,
              maxPerPage=12,
              balancePages=TRUE
              )

```
  
Low vertical visibilities (VV) are generally associated with lower winds.  There is also a slight tendency for more obscured skies (OVC) to be associated with slightly stronger winds.  

Further, the previous analysis for counts by wind direction is re-run using the predominant directions:  
```{r cache=TRUE}

# Counts by Metric for predomDir using mod2016Data
plotcountsByMetric(modData, 
                   mets=c("predomDir"), 
                   title="Comparisons Across Locales (red dots are the median)", 
                   facetOn="sourceName",
                   showCentral=TRUE, 
                   multiPageWrap=TRUE, 
                   maxPerPage=12,
                   balancePages=TRUE
                   )

```
  
Findings include:  
  
* Across all locales, winds are more commonly from S-SW-W-NW-N than from NE-E-SE  
* San Diego is especially likely to have either 0 wind or wind from W-NW  
* Traverse City is especially likely to have 0 wind or variable wind (very surprising, and possibly a sign of anomalous data)  
* Minneapolis is especially likely to experience winds from NE  
* New Orleans and Houston rarely experience winds from SW-W-NW relative to other locales  
* Lincoln and Las Vegas are especially pronse to winds from S (and for Las Vegas SW)  
  
There are several issues identified that should be explored and fixed if appropriate:  
  
* Vertical visibility greater than 10 recorded  
* Tendency for maximum obscuration to be CLR in Lincoln and Green Bay  
* Tendency for Traverse City to have no or variable winds  

The cross-locale plotting has been updated as follows:  
  
* Function multiPageFacet() is created and incorporated to allow for facetted graphs to be split in to several pages.  This is particularly useful for the PDF summaries  
* plotNumCor() is updated to allow for a user-specified alpha and maximum point size.  This is useful for correlations of variables where large numbers of points fall in a specific combination, such as TempC/TempF  
  
#### _Cloud Data Exploration_  
Cloud exploration can highlight whether all locales are being compared apples to apples.  This is especially the case if some locales have a different maximum sensor height:  
```{r}

# Select the source, sourceName, dtime and cLevel variables; pivot cLevel down
cLevels <- modData %>%
    select(source, sourceName, dtime, starts_with("cLevel")) %>%
    pivot_longer(-c(source, sourceName, dtime), names_to="level", values_to="height") %>%
    mutate(level=as.integer(str_replace(level, pattern="cLevel", replacement="")))

# Select the source, sourceName, dtime and cType variables; pivot cLevel down
cTypes <- modData %>%
    select(source, sourceName, dtime, starts_with("cType")) %>%
    pivot_longer(-c(source, sourceName, dtime), names_to="level", values_to="type") %>%
    mutate(level=as.integer(str_replace(level, pattern="cType", replacement="")))

cData <- cLevels %>%
    inner_join(cTypes, by=c("source", "sourceName", "dtime", "level"))

# Plot cloud heights, using only non-NA
cData %>%
    filter(!is.na(height)) %>%
    ggplot(aes(x=fct_reorder(sourceName, height, .fun=max, na.rm=TRUE), y=height)) + 
    geom_violin(fill="lightblue") + 
    coord_flip() + 
    labs(x="", y="Cloud Height (feet)", title="Density of cloud heights by locale")

```
  
There are clearly differences in maximum cloud height recorded by locale:  
  
* 6 locales/years record clouds up to 35,000 feet  
* 7 locales/years record clouds up to 30,000 feet; there appear to be few if any clouds recorded above 30,000 feet, so cloud distributions may not be meaningfully impacted by this  
* 2 locales/years record clouds up to 26,000-28,000 feet  
* 6 locales/years record clouds up to 25,000 feet; there are meaningful clouds between 25,000 and 30,000 feet, so these locales may appear to be "more clear than normal" purely due to maximum recorded height  
* 2 locales/years record heights up to 12,000 feet (Lincoln and Green Bay); these are the cities that appeared anomalously clear in the EDA; the reason appears to be the exclusion of mid/high level cloudiness  
  
The distribution of cloud types observed can also be assessed:  
```{r}

# Plot cloud heights, using only non-""
fctLayers <- c("VV", "OVC", "BKN", "SCT", "FEW")

cData %>%
    filter(type!="") %>%
    mutate(type=factor(type, levels=fctLayers)) %>%
    ggplot(aes(x=fct_reorder(sourceName, height, .fun=max, na.rm=TRUE), fill=type)) + 
    geom_bar(position="stack") + 
    coord_flip() + 
    labs(x="", y="Cloud Layer Obscuration", title="Cloud obscuration by locale") + 
    scale_fill_discrete("", rev(fctLayers)) + 
    theme(legend.position="bottom")

```
  
Lincoln and Green Bay stand out for having fewer clouds, likely due to the inability to catch the higher altocumulus and any cirrus cloud (both very common in the mid-latitudes).

Supposing that only clouds of 12,000 feet and under are considered:  
```{r}

# Plot cloud heights, using only non-NA
cData %>%
    filter(!is.na(height)) %>%
    filter(height <= 12000) %>%
    ggplot(aes(x=sourceName, y=height)) + 
    geom_violin(fill="lightblue") + 
    coord_flip() + 
    labs(x="", y="Cloud Height (feet)", title="Density of cloud heights by locale")

cData %>%
    filter(type!="") %>%
    filter(height <= 12000) %>%
    mutate(type=factor(type, levels=fctLayers)) %>%
    ggplot(aes(x=fct_reorder(sourceName, sourceName, .fun=length), fill=type)) + 
    geom_bar(position="stack") + 
    coord_flip() + 
    labs(x="", y="Cloud Layer Obscuration", title="Cloud obscuration by locale (up to 12,000 feet)") + 
    scale_fill_discrete("", rev(fctLayers)) + 
    theme(legend.position="bottom")

```
  
The patterns are much more plausible:  
  
* San Diego, known for marine layer, has clouds that skew very low and more obscured  
* Las Vegas, a desert, has clouds that skew higher and less obscured    
* The remaining cities have maximum cloud densities in the 2500-5000 foot range, very common cloud heights in the US  
* Lincoln and Green Bay remain among the least cloudy locales, though this may be due to hitting OVC (cloud layers above an OVC layer are not observed by the sensor/human and thus not recorded)  
  
A function is written to take only cloud data up through height x, and to add a layer of clouds that are "clear" at a height that is out-of-interval:  
```{r}

# Filter to only clouds up to and including height
cloudsLevel0 <- function(df, 
                         maxHeight, 
                         byVars,
                         baseLevel=0,
                         heightBase=-100,
                         typeBase="CLR"
                         ) {
    
    # Function assumptions
    # Input data are unique by byVars-level and with columns 'height' and 'type'
    # Clouds increase in height with level
    # Clouds are non-decreasing in type (VV > OVC > BKN > SCT > FEW) with level
    
    # FUNCTION ARGUMENTS:
    # df: tibble or dataframe contiaining the clouds data
    # maxHeight: the maximum height to consider (delete all heights above this level)
    # byVars: the variables that make up a unique observation (df should be unique by byVars-level)
    # baseLevel: the level to be created as the base level (by default, level 0)
    # heightBase: the height to be provided to the base level (by default, -100 feet)
    # typeBase: the type of obscuration observed at the base level (by default, CLR)
    
    # Add a cloud level 0 that has height -100 (by default)
    # Include only levels where the cloud height is not NA
    # Include only levels where the cloud height is less than or equal to maxHeight
    modData <- df %>% 
        group_by_at(vars(all_of(byVars))) %>% 
        summarize(level=baseLevel, height=heightBase, type=typeBase) %>% 
        ungroup() %>% 
        bind_rows(df) %>% 
        arrange_at(vars(all_of(c(byVars, "level")))) %>% 
        filter(!is.na(height)) %>% 
        filter(height <= maxHeight)
    
    modData
    
}

```
  
The function is then run using the existing cData:  
```{r}

modCData <- cloudsLevel0(cData, maxHeight=12000, byVars=c("source", "sourceName", "dtime")) %>%
    mutate(type=factor(type, levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR")))
modCData

```
  
An additional function is written to take processed cloud data and to designate 1) the minimum cloud height, 2) the minimum cloud ceiling (a ceiling exists with BKN, OVC, or VV layers), and 3) the maximum obscuration level (using CLR if no clouds exist in that area):  
```{r}

# Helper function to pull out the minimum cloud height, limited to certain obscurations
getMinimumHeight <- function(df, 
                             byVars, 
                             types, 
                             baseLevel=0
                             ) {
    
    # Split the data in to the baseLevel and all other levels
    baseData <- df %>%
        filter(level==baseLevel)
    layerData <- df %>%
        filter(level!=baseLevel)
        
    # Take the layerData, limit to type in types, and find the minimum level
    layerData <- layerData %>%
        filter(type %in% types) %>%
        group_by_at(vars(all_of(byVars))) %>%
        filter(level==min(level)) %>%
        ungroup()
    
    # Put the data back together
    # Keep the maximum level for each set of byVars (will be 0 if no data for byVars in layerData)
    cloudData <- baseData %>%
        bind_rows(layerData) %>%
        arrange_at(vars(all_of(c(byVars, "level")))) %>%
        group_by_at(vars(all_of(byVars))) %>%
        filter(level==max(level))
    
}

# Extract the minimum cloud height, minimum ceiling height, and maximum obscuration
hgtCeilObsc <- function(df, 
                        byVars,
                        baseLevel=0
                        ) {

    # Function assumptions
    # Input data are unique by byVars-level and with columns 'height' and 'type'
    # For each byVars, a row with level=baseLevel, height=heightBase, type=typeBase has been created
    # Clouds increase in height with level
    # Clouds are non-decreasing in type (VV > OVC > BKN > SCT > FEW) with level
    
    # FUNCTION ARGUMENTS:
    # df: tibble or dataframe contiaining the clouds data
    # byVars: the variables that make up a unique observation (df should be unique by byVars-level)
    # baseLevel: the base level in df (by default, level 0)
    # heightBase: the height of the base level in df (by default, -100 feet)
    # typeBase: the type of obscuration at the base level in df (by default, CLR)
    
    # Get the maximum obscuration
    maxObsc <- df %>%
        group_by_at(vars(all_of(byVars))) %>% 
        filter(level==max(level)) %>%
        ungroup()
    
    # Get the minimum height (any type)
    minHeight <- getMinimumHeight(df, 
                                  byVars=byVars, 
                                  types=c("VV", "OVC", "BKN", "SCT", "FEW"), 
                                  baseLevel=baseLevel
                                  )
    
    # Get the minimum ceiling height (VV, OVC, BKN)
    minCeiling <- getMinimumHeight(df, 
                                   byVars=byVars, 
                                   types=c("VV", "OVC", "BKN"), 
                                   baseLevel=baseLevel
                                   )

    # Put the file together
    minCeiling <- minCeiling %>%
        rename(ceilingHeight=height, ceilingType=type, ceilingLevel=level)
    minHeight <- minHeight %>%
        rename(cloudHeight=height, cloudType=type, cloudLevel=level)
    maxObsc <- maxObsc %>%
        rename(obscHeight=height, obscType=type, obscLevel=level)
    
    # Merge
    cloudSummary <- maxObsc %>%
        full_join(minHeight, by=byVars) %>%
        full_join(minCeiling, by=byVars)
    
    cloudSummary
    
}

```
  
The function can then be run on the modified clouds data:  
```{r}

# Get the key clouds data
cloudSummary <- hgtCeilObsc(modCData, byVars=c("source", "sourceName", "dtime"))

# Check for consistency
cloudSummary %>%
    count(ceilingType, obscType)
cloudSummary %>%
    count(cloudType, ceilingType)
cloudSummary %>%
    count(cloudType, obscType)

```
  
Plots for the maximum obscuration (through 12,000 feet) can then be created:  
```{r}

plotMaxObsc <- function(df, 
                        xVar, 
                        fillVar, 
                        title, 
                        subtitle="Up to and including 12,000 feet",
                        orderByVariable=NULL,
                        orderByValue=NULL,
                        posnBar="stack",
                        yLabel="# Hourly Observations",
                        legendLabel="",
                        facetOn=NULL
                        ) {

    # Get the levels to be used
    cLevels <- levels(df %>% pull(fillVar))
    
    # Create the main plot
    p1 <- df %>%
        ggplot(aes_string(fill=fillVar))
    if (!is.null(orderByVariable)) {
        p1 <- p1 + 
            geom_bar(aes(x=fct_reorder(get(xVar), get(orderByVariable)==orderByValue, .fun=sum)),
                     position=posnBar
                     )
    } else {
        p1 <- p1 + 
            geom_bar(aes_string(x=xVar), position=posnBar)
    }
    p1 <- p1 + 
        coord_flip() + 
        labs(x="", 
             y=yLabel, 
             title=title, 
             subtitle=subtitle
             ) + 
        theme(legend.position="bottom") + 
        scale_fill_discrete(legendLabel, rev(cLevels)) + 
        guides(fill=guide_legend(nrow=1))
    if (!is.null(facetOn)) {
        p1 <- p1 + facet_wrap(as.formula(paste("~", facetOn)))
    }
    print(p1)
    
}

# Cloud obscuration by source
plotMaxObsc(cloudSummary, 
            xVar="sourceName", 
            fillVar="obscType", 
            title="Maximum Cloud Obscuration", 
            orderByVariable="obscType",
            orderByValue="CLR"
            )

# Cloud obscuration by month
cloudSummary <- cloudSummary %>%
    mutate(month=lubridate::month(dtime), 
           hour=lubridate::hour(dtime), 
           monthfct=factor(month.abb[month], levels=month.abb[1:12])
           )
plotMaxObsc(cloudSummary, 
            xVar="monthfct", 
            fillVar="obscType", 
            title="Maximum Cloud Obscuration", 
            posnBar="fill"
            )

```
  
A few salient observations stand out about the maximum obscuration level:  
  
* Las Vegas is almost always clear or has just a few clouds up to 12,000 feet  
* Lincoln and Green Bay tend to be either clear up to 12,000 feet or to have an overcast  
* Traverse City is especially prone to being overcast and not having clear skies to 12,000 feet  
* Clear skies to 12,000 feet are roughly as likely in any month, but overcast by 12,000 feet is more common in the cold season while FEW/SCT are more common during the warm season  
  
Groups of cities can be examined, faceted by month:  
```{r}

cityCloudList <- list(c("klas_2016", "ksan_2016", "kiah_2016", "kmsy_2016"), 
                      c("kgrb_2016", "kgrr_2016", "kdtw_2016", "ktvc_2016"), 
                      c("klnk_2016", "kmsp_2016", "kmsn_2016", "kind_2016"), 
                      c("kmke_2016", "kord_2016", "kewr_2016")
                      )

for (x in cityCloudList) {
    cloudUse <- cloudSummary %>%
        filter(source %in% x)
    plotMaxObsc(cloudUse, 
                xVar="monthfct", 
                fillVar="obscType", 
                title="Maximum Cloud Obscuration", 
                facetOn="sourceName", 
                posnBar="fill"
                )
}

```
  
A few findings include:  
  
* While there may be some small seasonal patterns, observations in Las Vegas are almost always CLR/FEW  
* Houston and New Orleans both have few clear observations and few overcasts during June-September  
* San Diego has most of its overcasts during May-Septemver and most of its clear observations from October-April  
* Most of the remaining locales show a seasonal pattern with overcasts more common during the cold season  
* Newark is at a glance less seasonal than the midwestern locales  
  
The ceiling heights can also be assessed:  
```{r}

cloudSummary <- cloudSummary %>%
    mutate(ceilFactor=factor(case_when(ceilingHeight == -100 ~ "None", 
                                       ceilingHeight <= 1000 ~ "0-1000", 
                                       ceilingHeight <= 3000 ~ "1000-3000", 
                                       ceilingHeight <= 6000 ~ "3000-6000",
                                       ceilingHeight <= 12000 ~ "6000-12000"
                                       ), 
                             levels=c("None", "6000-12000", "3000-6000", "1000-3000", "0-1000")
                             )
           )

plotMaxObsc(cloudSummary, 
            xVar="sourceName", 
            fillVar="ceilFactor", 
            title="Ceiling Height", 
            orderByVariable="ceilFactor",
            orderByValue="None"
            )

```

Findings for ceiling height broadly line up with findings for maximum cloud obscuration, as expected:  
  
* Las Vegas is least likely to have a ceiling under 12,000 feet, followed by New Orleans, Lincoln, and Houston  
* San Diego is especially likely to have a ceiling of 1000-3000 feet relative to other locales, likely driven by marine layer  
* Traverse City and Grand Rapids are most likely to have a ceiling, likely driven by being just downwind of a large body of water  
  
The same analysis can be run for the minimum cloud height:  
```{r}

cloudSummary <- cloudSummary %>%
    mutate(minCFactor=factor(case_when(cloudHeight == -100 ~ "None", 
                                       cloudHeight <= 1000 ~ "0-1000", 
                                       cloudHeight <= 3000 ~ "1000-3000", 
                                       cloudHeight <= 6000 ~ "3000-6000",
                                       cloudHeight <= 12000 ~ "6000-12000"
                                       ), 
                             levels=c("None", "6000-12000", "3000-6000", "1000-3000", "0-1000")
                             )
           )

plotMaxObsc(cloudSummary, 
            xVar="sourceName", 
            fillVar="minCFactor", 
            title="Minimum Cloud Height", 
            orderByVariable="minCFactor",
            orderByValue="None"
            )

```
  
At a glance, there seem to be some similarities and differences among the locales:  
  
* San Diego frequently has its lowest clouds in the 0-3000 feet range  
* Las Vegas almost never has clouds below 6000 feet  
* Houston and New Orleans look very similar in minimum cloud heights by month  
* Green Bay and Lincoln appear to have more occurences of clear skies up to 12000 feet than the other cold weather cities  
  
A more analytical approach would look at the distance between the various locales.  Broadly speaking, distance can be calculated based on counts by month by locale.  No scaling is performed since each location has roughly the same number of observations by month, and the intent is to find macro similarities (e.g., if 1 city has 2% data in bucket x and the others all have 0% data in bucket x, this analysis would treat that as a minor difference where with scaled data it would be a primary difference driver).
  
There are three main variable to consider for distance:  
  
* obscType - the obscuration type observed  
* minCFactor - minimum cloud height observed, bucketed as 0-1000, 1000-3000, 3000-6000, 6000-12000, None  
* ceilFactor - ceiling height observed, bucketed as 0-1000, 1000-3000, 3000-6000, 6000-12000, None  
  
Distances can be calculated using any or all of these variables.  A function is created to that process can be repeated:  
```{r}

findCloudDist <- function(df, 
                          byVar, 
                          fctVar,
                          pivotVar="monthfct", 
                          scaleDistData=FALSE, 
                          returnPivotOnly=FALSE
                          ) {
    
    # FUNCTION ARGUMENTS
    # df: data frame or tibble containing one record per locale and time
    # byVar: the variable(s) by which the final distance file should be unique
    # fctVar: the factor variables to be counted up, with the sums being the distance inputs
    # pivotVar: the variable(s) which should be pivoted in to columns to make the file unique by byVar
    # scale: whether to scale the data prior to calculating distances
    # returnDistMatrix: whether to just return the pivoted data frame (default, FALSE, returns the distance matrix calculated from this pivoted data frame rather than the data frame itself)
    
    # Create the counts data by byVar, pivoted by fctVar and pivotVar
    baseData <- df %>%
        group_by_at(vars(all_of(c(byVar, fctVar, pivotVar)))) %>%
        summarize(n=n()) %>%
        ungroup() %>%
        pivot_wider(names_from=all_of(c(fctVar, pivotVar)), values_from="n") %>%
        mutate_if(is.numeric, tidyr::replace_na, replace=0)
    
    # If the data are only yo be pivoted, return and exit the function
    if (returnPivotOnly) {
        return(baseData)
    }
    
    # Split in to descriptors and data
    descData <- baseData %>%
        select_at(vars(all_of(byVar))) %>%
        mutate(rowN=row_number())
    distData <- baseData %>%
        select_at(vars(-any_of(byVar))) %>%
        as.matrix()
    
    # Scale distdata if requested
    if (scaleDistData) {
        distData <- scale(distData)
    }
    
    # Create the distances, convert back to data frame, pivot_longer, and attach the labels
    dist(distData) %>%
        as.matrix() %>%
        as_tibble() %>%
        mutate(row1=row_number()) %>%
        pivot_longer(-row1, names_to="row2", values_to="dist") %>%
        mutate(row2=as.integer(row2)) %>%
        inner_join(descData, by=c("row1"="rowN")) %>%
        rename_at(vars(all_of(byVar)), ~paste0(., "_1")) %>%
        inner_join(descData, by=c("row2"="rowN")) %>%
        rename_at(vars(all_of(byVar)), ~paste0(., "_2"))
    
}

```
  
The function is then run for the obscuration data as an example:  
```{r}

# Find the distances for obscType by locale vs. locale
obscDist <- findCloudDist(cloudSummary, 
                          byVar=c("source", "sourceName"), 
                          fctVar="obscType", 
                          pivotVar="monthfct"
                          )
obscDist

```
  
A function can then be created to plot the distances as a heamap and to return the minimum distance for each locale:  
```{r}

plotCloudDist <- function(df, 
                          var1="sourceName_1", 
                          var2="sourceName_2", 
                          met="dist", 
                          roundDist=0, 
                          title="Distance Between Locales", 
                          subT=""
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: tibble or data frame containing distance data
    # var1: the variable containing the first locale
    # var2: the variable containing the second locale
    # dist: the variable containing the pre-calculated distance between var1 and var2
    # roundDist: the rounding for the distance in the plot

    # Process the data frame and exclude any occurences of distance to self    
    distData <- df %>%
        select_at(vars(all_of(c(var1, var2, met)))) %>%
        filter(get(var1) != get(var2))
    
    # Get the locales by minimum distance to any other locale
    distHiLo <- distData %>%
        group_by_at(vars(all_of(var1))) %>%
        filter(dist==min(dist)) %>%
        arrange(dist) %>%
        pull(var1)
    
    # Create a heatmap of the distances
    distData %>%
        ggplot(aes(x=factor(get(var1), levels=distHiLo), y=factor(get(var2), levels=distHiLo))) + 
        geom_tile(aes(fill=dist)) + 
        geom_text(aes(label=round(dist, roundDist)), color="lightblue") +
        labs(x="", y="", title=title, subtitle=subT) + 
        scale_fill_gradient("Distance", low="black", high="white") + 
        theme(axis.text.x=element_text(angle=90))
    
}

```
  
The process can then be run on the obscuration data as a check:  
```{r}

plotCloudDist(obscDist, subT="Based on % of obscuration type by month")

```
  
FINDINGS FOR OBSCURATION DISTANCE:  
  
* Las Vegas is relatively far from everything, with its only modestly near neighbor being Lincoln  
* Lincoln is relatively far from everything  
* San Diego is far from everything except its moderately near neighbors Houston, New Orleans, and Newark  
* Green Bay is closest to Madison/Minneapolis and is moderately close to most of the midwest cities  
* Newark is reasonably close to many cities, being distant only from Traverse City, Green Bay, Lincoln, and Las Vegas  
* Houston and New Orleans are closest to each other, then to Newark  
* Traverse City and Grand Rapids are relatively close to each other  
* The remaining cold weather cities (Milwaukee, Chicago, Madison, Minneapolis, Indianapolis, Detroit) are all relatively close to each other and also to Grand Rapids  
  
This is suggestive that there is a cold-weather cluster, a hot and humid cluster, and then several locales that are more or less standalone but could be grouped loosely to each other.
  
The functions can then be run on variations of the data:  
```{r}

# Run for minCFactor
findCloudDist(cloudSummary, 
              byVar=c("source", "sourceName"), 
              fctVar="minCFactor", 
              pivotVar="monthfct"
              ) %>%
    plotCloudDist(subT="Based on % in each minimum cloud height bucket by month")

# Run for ceilFactor
findCloudDist(cloudSummary, 
              byVar=c("source", "sourceName"), 
              fctVar="ceilFactor", 
              pivotVar="monthfct"
              ) %>%
    plotCloudDist(subT="Based on % in each ceiling height bucket by month")

```
  
Findings by minimum cloud height are similar to findings by obscuration.  
  
Findings by ceiling height show that Lincoln and Newark are close to each other and that San Diego and Las Vegas are both segments of one.  Traverse City and Grand Rapids are close to each other; as are New Orleans and Houston.  There continues to be a large, close, segment of cold cities.

Of interest is whether a simple kmeans analysis returns the same findings:  
```{r}

set.seed(2006040940)

ceilDistData <- findCloudDist(cloudSummary, 
                              byVar=c("source", "sourceName"), 
                              fctVar="ceilFactor", 
                              pivotVar="monthfct", 
                              returnPivotOnly=TRUE
                              )

tibble::tibble(locale=ceilDistData$sourceName, 
               cluster=kmeans(dist(ceilDistData[3:ncol(ceilDistData)]), centers=5, nstart=1000)$cluster
               ) %>%
    arrange(-cluster) %>%
    as.data.frame()

```
  
The five-segment k-means is consistent - solo segments for San Diego and Las Vegas; a Grand Rapids-Traverse City segment; a cold-weather midwestern segment (Grand Rapids and Traverse City are on the downwind side of Lake Michigan and would be expected to see much different clouds than cities with similar temperatures); and a catch-all segment.
  
The data can also be assessed using hierarchical clustering:  
```{r}

hclust(dist(ceilDistData[3:ncol(ceilDistData)]), method="complete") %>%
    plot(labels=ceilDistData$sourceName, cex=0.5, main="Hierarchical on Ceiling Height: method=complete")

hclust(dist(ceilDistData[3:ncol(ceilDistData)]), method="single") %>%
    plot(labels=ceilDistData$sourceName, cex=0.5, main="Hierarchical on Ceiling Height: method=single")

```
  
Similar conclusions can be drawn from hierarchical clustering based on ceiling height buckets by month:  
  
* San Diego and Las Vegas are each segments of one  
* Houston, New Orleans, Lincoln, and Newark tend to associate together  
* Grand Rapids and Traverse City tend to associate together  
* There are possibly two sub-groupings of the other cold weather locales  
  
Are any differences observed in trends for minimum cloud height?  
```{r}

heightDistData <- findCloudDist(cloudSummary, 
                                byVar=c("source", "sourceName"), 
                                fctVar="minCFactor", 
                                pivotVar="monthfct", 
                                returnPivotOnly=TRUE
                                )

hclust(dist(heightDistData[3:ncol(heightDistData)]), method="complete") %>%
    plot(labels=heightDistData$sourceName, cex=0.5, 
         main="Hierarchical on Minimum Cloud Height: method=complete"
         )

hclust(dist(heightDistData[3:ncol(heightDistData)]), method="single") %>%
    plot(labels=heightDistData$sourceName, cex=0.5, 
         main="Hierarchical on Minimum Cloud Height: method=single"
         )

```
  
There are some differences when using the 'complete' linkage method:  
  
* There is a Green Bay-Lincoln cluster, with Las Vegas merging in relatively soon  
* There are arguably two cold weather clusters (including Grand Rapids and Traverse City), with Newark merging in relatively soon  
  
When using the 'single' linkage method, these patterns become muted:  
  
* There are still arguably two cold-weather clusters  
* Houston and New Orleans form a cluster  
* San Diego and Las Vegas are their own cluster  
* Lincoln, Newark, and Green Bay all fall somewhere in the middle
  
We would expect k-means to pull out 1-2 cold-weather clusters and a Houston-New Orleans cluster:  
```{r}

set.seed(2006041003)

cl5 <- tibble::tibble(locale=heightDistData$sourceName, 
               cluster=kmeans(dist(heightDistData[3:ncol(heightDistData)]), centers=5, nstart=1000)$cluster
               ) %>%
    arrange(-cluster)

cl6 <- tibble::tibble(locale=heightDistData$sourceName, 
               cluster=kmeans(dist(heightDistData[3:ncol(heightDistData)]), centers=6, nstart=1000)$cluster
               ) %>%
    arrange(-cluster)

cl7 <- tibble::tibble(locale=heightDistData$sourceName, 
               cluster=kmeans(dist(heightDistData[3:ncol(heightDistData)]), centers=7, nstart=1000)$cluster
               ) %>%
    arrange(-cluster)


cl5 %>%
    rename(cl5=cluster) %>%
    inner_join(rename(cl6, cl6=cluster), by="locale") %>%
    inner_join(rename(cl7, cl7=cluster), by="locale") %>%
    as.data.frame()

```
  
With five segments, there is:  
  
* Houston and New Orleans  
* San Diego  
* Lincoln and Green Bay  
* Las Vegas  
* Cold weather cities  
  
Adding the sixth cluster splits apart Green Bay and Lincoln.  

Adding the seventh cluster cleaves Grand Rapids-Traverse City-Detroit from the other cold weather cities.  

Next steps are to explore the clouds data, fix any issues, and create/save a consolidated file for further analysis.
  
#### _Storing Output Data_  
The primary EDA data is available, in consolidated form, in the modData file.  Variables to keep include:  
  
* source (chr) - the reporting station and time  
* sourceName (chr) - the descriptive name for source (rename to 'locale')  
* dtime (dttm) - the date-time for the observation  
* origMETAR (chr) - the original METAR associated with the observation  
* year (int) - the year, extracted from dtime  
* monthint (int) - the month, extracted from dtime, as an integer  
* month (fct) - the month, extracted from dtime, as a three-character abbreviation (factor)  
* day (int) - the day of the month, extracted from dtime  
* WindDir (chr) - previaling wind direction in degrees, stored as a character since 'VRB' means variable  
* WindSpeed (int) - the prevailing wind speed in knots  
* WindGust (dbl) - the wind gust speed in knots (NA if there is no recorded wind gust at that hour)  
* Visibility (dbl) - surface visibility in statute miles  
* Altimeter (int) - altimeter in inches of mercury  
* TempF (dbl) - the Fahrenheit temperature  
* DewF (dbl) - the Fahrenheit dew point  
* modSLP (dbl) - Sea-Level Pressure (SLP), adjusted to reflect that SLP is recorded as 0-1000 but reflects data that are 950-1050  
* cTypen (chr) - the cloud type of the nth cloud layer (FEW, BKN, SCT, OVC, or VV)  
* cLeveln (dbl) - the cloud height in feet of the nth cloud layer  
* predomDir (chr) - the predominant wind direction as NE-E-SE-S-SW-W-NW-N-VRB-000-Error  
  
Additionally, other data can be brought in from the EDA:  
  
* raState, snState, tsState from the precipitation analysis  
* tempFHi, tempFLo from the daily temperature analysis  
* p1Inches, p36Inches, p24Inches from the daily precipitation analysis  
* Minimum cloud height and minimum ceiling height from the cloud analysis

And, other key data can be stored for use later:  
  
* Precipitation extraction lists  
* Cloud data  
  
The main data frame is created:  
```{r}

# Select the appropriate variables from modData
finalData <- modData %>%
    select(source, 
           locale=sourceName, 
           dtime, 
           origMETAR, 
           year, 
           monthint, 
           month, 
           day, 
           WindDir,
           WindSpeed,
           WindGust,
           predomDir,
           Visibility,
           Altimeter,
           TempF,
           DewF,
           modSLP,
           starts_with("cType"),
           starts_with("cLevel")
           )

```
  
The precipitation data are integrated, then merged in to the main data frame:  
```{r}

# Function to take a precipitation list and make a tibble
makeTibbleFromPrecip <- function(lst, varName) {
    
    sapply(lst[["precipList"]], FUN="[", "pStateFrame") %>%
        bind_rows(.id="source") %>%
        mutate(source=str_sub(source, 1, 9)) %>%
        select(source, dtime, curPrecip) %>%
        rename_at(vars(all_of(c("curPrecip"))), ~varName)
        
}

thunderData <- map_dfr(.x=list(thunder_2015_List, thunder_2016_List, thunder_2017_List), 
                       .f=makeTibbleFromPrecip, 
                       varName="isThunder"
                       )

rainData <- map_dfr(.x=list(rain_2015_List, rain_2016_List, rain_2017_List), 
                    .f=makeTibbleFromPrecip, 
                    varName="isRain"
                    )

snowData <- map_dfr(.x=list(snow_2015_List, snow_2016_List, snow_2017_List), 
                    .f=makeTibbleFromPrecip, 
                    varName="isSnow"
                    )

# Inner join the precipitation data
precipData <- rainData %>%
    inner_join(snowData, by=c("source", "dtime")) %>%
    inner_join(thunderData, by=c("source", "dtime"))

# Confirm that no rows have been lost
if (nrow(precipData) < max(nrow(rainData), nrow(snowData), nrow(thunderData))) {
    stop("\nPrecipitation data have mislaignment issues")
}


# Merge in to finalData
if (nrow(finalData) != nrow(precipData)) {
    stop("\nPrecipitation data are not aligned with final data")
}

finalData <- finalData %>%
    inner_join(precipData, by=c("source", "dtime"))

# Confirm that no rows have been lost
if (nrow(finalData) < nrow(precipData)) {
    stop("\nPrecipitation data did not merge correctly with final data")
}

```
  
Daily high-low temperatures and precipitation summaries (1 hour, 3/6 hour, 24 hour) are also merged in:  
```{r}

# Merge together the precipitation and temperature data
precipTempData <- all_pin %>%
    select(source, dtime, p1Inches, p36Inches, p24Inches) %>%
    inner_join(select(all_hilo, source, dtime, tempFHi, tempFLo), 
               by=c("source", "dtime")
               )

# Confirm that no rows have been lost
if (nrow(precipTempData) < max(nrow(all_pin), nrow(all_hilo))) {
    stop("\nPrecipitation and temperature summaries have mislaignment issues")
}


# Merge in to finalData
if (nrow(finalData) != nrow(precipTempData)) {
    stop("\nPrecipitation and temperature summaries are not aligned with final data")
}

finalData <- finalData %>%
    inner_join(precipTempData, by=c("source", "dtime"))

# Confirm that no rows have been lost
if (nrow(finalData) < nrow(precipTempData)) {
    stop("\nPrecipitation and temperature summaries did not merge correctly with final data")
}

```
  
Minimum cloud height (through 12,000 feet) and minimum ceiling height (through 12,000 feet) are also merged in:  
```{r}

# Merge in to finalData
if (nrow(finalData) != nrow(cloudSummary)) {
    stop("\nCloud summaries are not aligned with final data")
}

finalData <- finalData %>%
    inner_join(select(cloudSummary, source, dtime, cloudHeight, cloudType, ceilingHeight, ceilingType), 
               by=c("source", "dtime")
               ) %>%
    rename(minHeight=cloudHeight, minType=cloudType)

# Confirm that no rows have been lost
if (nrow(finalData) < nrow(cloudSummary)) {
    stop("\nCloud summaries did not merge correctly with final data")
}

```
  
Address the issues that there is a handful of time overlap (a day or two of 2014 data included in 2015):  
```{r}

stationTime <- finalData %>%
    select(source, dtime) %>%
    mutate(station=str_sub(source, 1, 4)) %>%
    select(station, dtime)

finalDups <- stationTime %>%
    duplicated()

cat("\nDuplicated records occur during:\n")
stationTime %>% 
    filter(finalDups) %>% 
    count(station, date=lubridate::date(dtime))

```
  
As expected, the duplicated occur at the change-over from 2015-2016 and 2016-2017.  To simplify future processing, each source will be filtered to include only the matching year:  
```{r}

filteredData <- finalData %>%
    filter(as.integer(str_sub(source, 6, -1))==year)

cat("\nEnforcing year restriction reduces data rows from:", nrow(finalData), "to:", nrow(filteredData), "\n")

```
  
The list of variables is output, and summaries run:  
```{r}

cat("\nVariables includes in the final file are:\n")
names(filteredData)

cat("\nSummary of the final file:\n")
summary(filteredData)

cat("\nResetting Visibility > 10 to 10\n")
filteredData %>% 
    filter(Visibility > 10)

summary(filteredData$Visibility)
filteredData <- filteredData %>%
    mutate(Visibility=ifelse(is.na(Visibility), NA, pmin(Visibility, 10)))
summary(filteredData$Visibility)

```
  
The final file is then saved for future use:  
```{r}

saveRDS(filteredData, "./RInputFiles/ProcessedMETAR/metar_postEDA_20200617.rds")

```
  
The precipitation extract lists are then consolidated and saved:  
```{r}

allPrecipList <- list(rain2015=thunder_2015_List, 
                      rain2016=rain_2016_List, 
                      rain2017=rain_2017_List,
                      snow2015=snow_2015_List, 
                      snow2016=snow_2016_List, 
                      snow2017=snow_2017_List, 
                      thunder2015=thunder_2015_List, 
                      thunder2016=thunder_2016_List, 
                      thunder2017=thunder_2017_List
                      )
str(allPrecipList, max.level = 2)

saveRDS(allPrecipList, "./RInputFiles/ProcessedMETAR/metar_precipLists_20200617.rds")

```
  
The modified clouds data tibble is also saved:  
```{r}

# Example of the modCData format
modCData

# Check that modCData includes every date-time in filteredData, and no others
modCDataTimes <- modCData %>%
    count(source, dtime)
    
# In filteredData but not in modCDataTimes
filteredData %>%
    select(source, dtime) %>%
    anti_join(modCDataTimes, by=c("source", "dtime"))

# In modCDataTimes but not in filteredData
modCDataTimes %>%
    select(source, dtime) %>%
    anti_join(filteredData, by=c("source", "dtime")) %>%
    count(source, date=lubridate::date(dtime)) %>%
    pivot_wider(source, names_from="date", values_from="n") %>%
    mutate_if(is.numeric, ~ifelse(is.na(.), 0, .)) %>%
    as.data.frame()

# Filter modCData so that year aligns as with fileteredData
modCSaveData <- modCData %>%
    filter(as.integer(str_sub(source, 6, 9))==lubridate::year(dtime))

saveRDS(modCSaveData, "./RInputFiles/ProcessedMETAR/metar_modifiedClouds_20200617.rds")

```
  
With the EDA files saved, next steps are to build models to predict the locale based on the data elements (see WeatherModels_202006_v001).