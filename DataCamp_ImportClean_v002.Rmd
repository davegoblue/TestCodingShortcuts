---
title: "Importing and Cleaning Data"
author: "davegoblue"
date: "February 16, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background and Overview  
DataCamp offer interactive courses related to R Programming.  While some is review, it is helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  A main document DataCamp_Insights_v001 was written to summarize key insights from:  
  
* R Programming (Introduction to R, Intermediate R, Writing Functions in R)  
* Importing and Cleaning Data (Cleaning Data in R, Importing Data in to R)  
* Data Manipulation (dplyr, data.table, xts/zoo)  
* Data Visualization (base, ggplot2 parts I/II/III, ggvis, geospatial)  
  
The original DataCamp_Insights_v001 document has been split:  
  
* The DataCamp_Insights_v002 document contains evolving sections on R Programming and Data Manipulation  
* Importing and Cleaning Data components have been moved to this DataCamp_ImportClean_v002 document  
* Visualization components have been moved to DataCamp_Visualization_v002  
  
## Importing and Cleaning Data (Cleaning Data in R, Importing Data in to R)  
###_Importing Data in to R_  
This course provides an overview of loading data in to R from five main sources:  
  
* Flat files  
* Excel files  
* Statistical software  
* Databases  
* Web data  
  
####_Reading Flat Files_  
At the most basic level, the utlis library easily handles reading most types of flat files:  
  
* read.table(file, header=FALSE, sep="", stringsAsFactors=default.stringsAsFactors(), <many more>)  
* read.csv(file, header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "", ...)  
* read.delim(file, header = TRUE, sep = "\t", quote = "\"",  dec = ".", fill = TRUE, comment.char = "", ...)  
  
There are also European equivalents in case the decimal needs to be set as "," to read in the file:  
  
* read.csv2(file, header = TRUE, sep = ";", quote = "\"", dec = ",", fill = TRUE, comment.char = "", ...)  
* read.delim2(file, header = TRUE, sep = "\t", quote = "\"", dec = ",", fill = TRUE, comment.char = "", ...)  
  
The file.path() command is a nice way to put together file paths.  It is more or less equivalent to paste(, sep="/"), but with the benefit that sep is machine/operating-system dependent, so it may be easier to use across platforms.  
  
Further, there is the option to use colClasses() to specify the type in each column, with NULL meaning do not import.  Abbreviations can be used for these as well:  
```{r}
# colClasses (relevant abbreviations)
R.utils::colClasses("-?cdfilnrzDP")

# file.path example
file.path("..", "myplot.pdf")

# Key documentation for reading flat files
# 
# read.table(file, header = FALSE, sep = "", quote = "\"'",
#            dec = ".", numerals = c("allow.loss", "warn.loss", "no.loss"),
#            row.names, col.names, as.is = !stringsAsFactors,
#            na.strings = "NA", colClasses = NA, nrows = -1,
#            skip = 0, check.names = TRUE, fill = !blank.lines.skip,
#            strip.white = FALSE, blank.lines.skip = TRUE,
#            comment.char = "#",
#            allowEscapes = FALSE, flush = FALSE,
#            stringsAsFactors = default.stringsAsFactors(),
#            fileEncoding = "", encoding = "unknown", text, skipNul = FALSE)
# 
# read.csv(file, header = TRUE, sep = ",", quote = "\"",
#          dec = ".", fill = TRUE, comment.char = "", ...)
# 
# read.csv2(file, header = TRUE, sep = ";", quote = "\"",
#           dec = ",", fill = TRUE, comment.char = "", ...)
# 
# read.delim(file, header = TRUE, sep = "\t", quote = "\"",
#            dec = ".", fill = TRUE, comment.char = "", ...)
# 
# read.delim2(file, header = TRUE, sep = "\t", quote = "\"",
#             dec = ",", fill = TRUE, comment.char = "", ...)
```
  
There are also two libraries that can be especially helpful for reading in flat files - readr and data.table.  
  
* readr::read_delim() handles many data types  
* readr::read_delim(file, delim=",") will read a CSV
    * assumes col_names=TRUE (eq to header=TRUE)
    * assumes col_types=NULL (imputed from first 100 rows, side effect - no need for stringAsFactors = FALSE)  
* col_types can use short type, where c=character, d=double (numeric), i=integer, l=logical (boolean), _=skip  
    * col_names = FALSE means make your own  
    * col_names = c() means here are the column names you should use  
* skip=<number to skip>  
* n_max=<number to read>  
* read_csv() is for CSV  
* read_tsv is for tab-separated values	
  
* data.table() is designed for speed  
    * data.table::fread() is for fast reading  
    * The fread() automatically handles the column names and also infers the column separators  
    * This is a faster, more convenients, and easier to customize version of read.table()  
  
* Wrappers for the readr() function  
    * fac <- col_factor(levels = c("Beef", "Meat", "Poultry"))  
    * int <- col_integer()  
	* hotdogsFactor <- read_tsv("hotdogs.txt",  
	                            col_names = c("type", "calories", "sodium"), 
	                            col_types = list(fac, int, int)
	                            )
  
####_Reading Excel Files_    
Further, the library(readxl) is handy for loading Excel sheets:  
  
* readxl::excel_sheets() will list the sheets  
    * excel_sheets(path)  
* readxl::read_excel() will read in a specific sheet  
    * read_excel(path, sheet = 1, col_names = TRUE, col_types = NULL, na = "", skip=0)	
        * col_names: Either TRUE to use the first row as column names, FALSE to number columns sequentially     from X1 to Xn, or a character vector giving a name for each column  
        * col_types: Either NULL to guess from the spreadsheet or a character vector containing "blank", "numeric", "date" or "text"  
* lapply(excel_sheets(myXLS), FUN=read_excel, path=myXLS) provide all data from all sheets in a list  
  
####_Reading Statistical Software Files_  
R can also load files from common statistical software such as SAS, STATA, SPSS, and MATLAB/Octave.  The packages haven() by Wickham and foreign() by the R core team are two common examples.  The R.matlab() allows for reading to/from MATLAB/Octave:  
  
The library(haven) contains wrappers to the ReadStat package, a C library by Evan Miller, for reading files from SAS, STATA, and SPSS:  
  
* read_sas(filename)	
* read_stata(filename)  
* read_dta(filename)  
    * as_factor(R_column) will help if the type is "labelled"  
    * as.character(as_factor(R_column)) will turn it back to a character vector  
* read_spss(filename) which is a wrapper to read_por() and read_sav()	
  
The library(foreign) can read/write all types of foreign formats, with some caveats:  
  
* Only SAS libraries (.xport) can be read in; seems quite a drawback to not be able to read SAS files!  
* read.dta(file, convert.factors = TRUE, convert.dates=TRUE, missing.type=FALSE)  
* read.spss(file, use.value.labels = TRUE, to.data.frame = FALSE)  
  
Finally, the R.matlab() library is available for reading/writing MATLAB/Octave files.  Per the help file:   
  
* Methods readMat() and writeMat() for reading and writing MAT files. For user with MATLAB v6 or newer installed (either locally or on a remote host), the package also provides methods for controlling MATLAB (trademark) via R and sending and retrieving data between R and MATLAB.  
  
* In brief, this package provides a one-directional interface from R to MATLAB, with communication taking place via a TCP/IP connection and with data transferred either through another connection or via the file system. On the MATLAB side, the TCP/IP connection is handled by a small Java add-on.  
  
* The methods for reading and writing MAT files are stable. The R to MATLAB interface, that is the Matlab class, is less prioritized and should be considered a beta version.  
  
* readMat(con, maxLength=NULL, fixNames=TRUE, drop=c("singletonLists"), sparseMatrixClass=c("Matrix", "SparseM", "matrix"), verbose=FALSE, ...)  
    * Returns a named list structure containing all variables in the MAT file structure.  
  
* writeMat(con, ..., matVersion="5", onWrite=NULL, verbose=FALSE)  
    * con: Binary connection to which the MAT file structure should be written to. A string is interpreted as filename, which then will be opened (and closed afterwards).  
    * ...: Named variables to be written where the names must be unique.  
  
####_Reading Relational DB Files_  
Relational databases in R (DBMS tend to use SQL for queries), including libraries:  
  
* RMySQL  
* RPostgresSQL  
* ROracle  
* RSQLite  

Conventions are specified in DBI; see library(DBI):  
  
* dbConnect(drv, ...)  
* drv  
  
Create the connection as "con" (or whatever) and then use that elsewhere:  
  
* dbListTables(con)  # What tables are in this  
* dbReadTable(con, tablename)  
  
When finished, dbDisconnect(con) as a courtesy so as to not tie up resources.  
  
SQL queries from inside R - per previous, library(DBI) and then create the connection "con":  
  
* dbGetQuery(con, valid_SQL_code)  
    * Appears that \" is an escape character to use quotes inside quotes  
    * dbGetQuery(con, "SELECT name FROM employees WHERE started_at > \"2012-09-01\"")  
    * dbGetQuery(con, "SELECT * FROM products WHERE contract=1")  
* res <- dbSendQuery(con, validSQLcode) # If instead you want to grab only chunks of records  
    * dbFetch(res)  # Can specify chunking, such as dbFetch(res, n=1) for one line at a time  
    * dbHasCompleted(res) # see whether it is done  
        * while(!dbHasCompleted(res)) { chunk <- dbFetch(res, n=1); print(chunk) }  
    * dbClearResult(res)  
  
For example, using "./SQLforDataCampRMD_v01.db", run a few SQL commands:  
```{r}
# uses libraries DBI for the connection and RSQLite to interface with SQLite Browser on my machine
con <- DBI::dbConnect(RSQLite::SQLite(), "SQLforDataCampRMD_v01.db")

# List the tables, and drop dummy if it already exists
DBI::dbListTables(con)
DBI::dbGetQuery(con, "DROP TABLE IF EXISTS dummy")

# Create blank table
DBI::dbListTables(con)
DBI::dbGetQuery(con, "CREATE TABLE IF NOT EXISTS dummy (id PRIMARY KEY, name CHAR)")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (1, 'Amy')")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (2, 'Bill')")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (2, 'Jen')") # Should do nothing
DBI::dbGetQuery(con, "SELECT * FROM dummy")
DBI::dbListTables(con)

# Can continue passing SQL commands back and forth as needed

# Close the connection
DBI::dbDisconnect(con)

```
  
####_Reading Web Data_  
Many of the R read-in libraries already work well with web data.  For example, read.csv("mywebsite.com", stringAsFactors=FALSE) will read a CSV right off the internet.  Further, there are options for:  
  
* download.file(url, dest_path) # Reproducibility advantages over right-click and save  
* library(httr) by Hadley Wickham, including GET() and content()  
  
The jsonlite library is good for working with JSON:  
  
* fromJSON(url) will create a named R list (often creates a data frame also)  
    * JSON objects are name:value pairs  
    * JSON arrays convert to vectors  
    * JSON can also created nested arrays  
* toJSON()  
  
Prettify adds indentation to a JSON string; minify removes all indentation/whitespace:  
  
* prettify()    prettify(txt, indent = 4)  
* minify()	    minify(txt)  
  
```{r}
jsonLoc <- file.path("../../..", "PythonDirectory", "UMModule04", "roster_data.json")
jsonData <- jsonlite::fromJSON(jsonLoc)
str(jsonData)
head(jsonData)
```
  
###_Cleaning Data in R_  
The general analysis pipeline is Collect -> Clean -> Analyze -> Report.  Cleaning is needed so the raw data can work with more traditional tools (e.g., packages in Python or R).  50% - 80% of time is spent in the Collect/Clean realm, even though this is not the most exciting (and thus taught) part of data analysis.  There are generally three stages of data cleaning:  Explore -> Tidy -> Prepare
  
Exploring the Data:  
  
* class()  
* dim()  
* names()  # column names  
* str()  
* dplyr::glimpse()  # a nicer version of str()  
* summary()  
  
Viewing the Data:  
  
* head(file, n=6)  
* tail(file, n=6)  
* hist(variable)  
* plot(x, y)  
* print()  # not recommended for larger datasets  
  
Tidy data - Wickham 2014, Principles of Tidy Data:  
  
* Each row should be an observation	
* Each column should be a variable (attribute)	
* Column headers should not be variables; e.g., eye color should be a single column, not many columns of 0/1  
* Each intersection should be a value (intersection of the observation and attribute)  
* Only one type of observational unit (e.g., each row is a person) per table  
  
The principles of tidy data can be implemented using library(tidyr):  
  
* tidyr::gather(data, key, value, ..., na.rm = FALSE, convert = FALSE, factor_key = FALSE)  # gather the data in to key-value pairs  
    * key, value	Names of key and value columns to create in output.  
    * ...	Specification of columns to gather. Use bare variable names. Select all variables between x and z with x:z, exclude y with -y. For more options, see the select documentation.  
  
* tidyr::spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE)  # spread the key-value pairs in to columns  
    * key	The bare (unquoted) name of the column whose values will be used as column headings.  
    * value	The bare (unquoted) name of the column whose values will populate the cells.  
    * fill	If set, missing values will be replaced with this value. Note that there are two types of missing in the input: explicit missing values (i.e. NA), and implicit missings, rows that simply aren't present. Both types of missing value will be replaced by fill.  
  
* tidyr::separate(data, col, into, sep = "[^[:alnum:]]+", remove = TRUE, convert = FALSE, extra = "warn", fill = "warn", ...)  
    * col	Bare column name.  
    * into	Names of new variables to create as character vector.  
    * sep	Separator between columns.  If character, is interpreted as a regular expression. The default value is a regular expression that matches any sequence of non-alphanumeric values.  If numeric, interpreted as positions to split at. Positive values start at 1 at the far-left of the string; negative value start at -1 at the far-right of the string. The length of sep should be one less than into.  
  
* tidyr::unite(data, col, ..., sep = "_", remove = TRUE)  
    * col	(Bare) name of column to add  
    * ...	Specification of columns to unite. Use bare variable names. Select all variables between x and z with x:z, exclude y with -y. For more options, see the select documentation.  
    * sep	Separator to use between values.  
  
Common symptoms of messy data include:  
  
* Column headers are values rather than variable names -- use tidyr::gather()  
* Variables stored in both rows and columns -- use tidyr::spread()  
* Multiple variables are stored in a single column -- use tidyr::separate()  
* Singe type of observational unit (e.g., people) stored in 2+ tables  
* Multiple types of observational units (e.g., people and pets) stored in a single table  
  
Example code includes:  
```{r}
# tidyr::gather()
stocks <- data.frame(time = as.Date('2009-01-01') + 0:4, 
                     X = rnorm(5, 0, 1), Y = rnorm(5, 0, 2), Z = rnorm(5, 0, 4)
                     )
stocks
# will create new columns stock (each of X, Y, Z) and price (the values that had been in X, Y, and Z), 
# while not gathering the time variable; final table will be time-stock-price
stockGather <- tidyr::gather(stocks, stock, price, -time)  
stockGather

# tidyr::spread()
tidyr::spread(stockGather, stock, price)
# TRUE (this fully reverses what the gather function has done)
identical(tidyr::spread(stockGather, stock, price), stocks)  


# tidyr::separate()
df <- data.frame(x = c(NA, "a.b", "a.d", "b.c"))
df
# by default, the splits occur on anything that is not alphanumeric, 
# so you get column A as whatever is before the dot and column B as whatever is after the dot
dfSep <- tidyr::separate(df, x, c("A", "B"))
dfSep

# tidyr::unite()
tidyr::unite(dfSep, united, c(A, B), sep="")
is.na(dfSep) # caution . . . 
is.na(tidyr::unite(dfSep, united, c(A, B), sep="")) # caution . . . 
```
  
* The library(lubridate) can be helpful for coercing strings to dates  
    * ymd()  # coerces a character string that is in year-month-day originally  
    * mdy() # coerces a character string that is in month-day-year originally  
    * hms() # coerces a character string that is in hours-minutes-seconds originally  
    * ymd_hms() # coerces a character string that is in year-month-day-hours-minutes-seconds  
  
* The library(stringr) can be helpful for working with strings  
	* stringr::str_trim()  # trails the leading and trailing white space  
	* stringr::str_pad(char, width, side, pad)  # adds characters in place of white space at the start/end  
	* stringr::str_detect(inVector, searchTerm) # is searchTerm in each iterm of vector (boolean, same length as inVector)  
	* stringr::str_replace(inVector, searchTerm, replaceTerm) # searchTerm will be replaced by replaceTerm in each iterm of vector (output same length/type as inVector)  
        * social_df$status <- str_replace(social_df$status, "^$", NA) # Nice way to make the blanks in to NA  
  
* The tolower() and toupper() commands can be very useful also  
  
* Missing and special values  
    * May be randomly missing, but very dangerous to assume!  
    * In R, these are NA  
        * Excel may have #N/A  
        * SPSS and SAS may have .  
        * Sometimes just shows up as a missing string  
    * Inf is for infinite  
    * NaN is for not a number  
  
* Finding these special values  
    * is.na()  
	* any(is.na())  
	* sum(is.na())  
	* summary()  # will tell the number of NA  
	* complete.cases()  # TRUE is the entire row is non-NA; FALSE otherwise  
	* na.omit()  # equivalent to x[complete.cases(x), ]  
  
Example code includes:  
```{r}
# lubridate::ymd()
lubridate::ymd("160720")
lubridate::ymd("2016-7-20")
lubridate::ymd("16jul20")
lubridate::ymd("16/07/20")

# lubridate::hms()
lubridate::hms("07h15:00")
lubridate::hms("17 hours, 15 minutes 00 seconds")
lubridate::hms("07-15-00")

# From ?stringr::str_detect
# 
# str_detect(string, pattern)
#   string	Input vector. Either a character vector, or something coercible to one.
#   pattern	Pattern to look for.  The default interpretation is a regular expression, as described in stringi-search-regex. Control options with regex().  Match a fixed string (i.e. by comparing only bytes), using fixed(x). This is fast, but approximate. Generally, for matching human text, you'll want coll(x) which respects character matching rules for the specified locale.  Match character, word, line and sentence boundaries with boundary(). An empty pattern, "", is equivalent to boundary("character").
# 

fruit <- c("apple", "banana", "pear", "pinapple")

stringr::str_detect(fruit, "a")
stringr::str_detect(fruit, "^a")
stringr::str_detect(fruit, "a$")
stringr::str_detect(fruit, "b")
stringr::str_detect(fruit, "[aeiou]")

# Also vectorised over pattern
stringr::str_detect("aecfg", letters)


# From ?stringr::str_replace
#
# str_replace(string, pattern, replacement)
# str_replace_all(string, pattern, replacement)
#   string	Input vector. Either a character vector, or something coercible to one.
#   pattern, replacement	Supply separate pattern and replacement strings to vectorise over the patterns. References of the form \1, \2 will be replaced with the contents of the respective matched group (created by ()) within the pattern.  For str_replace_all only, you can perform multiple patterns and replacements to each string, by passing a named character to pattern.
#

someNA <- c(letters, "", LETTERS, "")
someNA[someNA==""] <- NA
someNA

fruits <- c("one apple", "two pears", "three bananas")
stringr::str_replace(fruits, "[aeiou]", "-")  # Replace FIRST instance
stringr::str_replace_all(fruits, "[aeiou]", "-")  # Replace ALL instances

stringr::str_replace(fruits, "([aeiou])", "\\1\\1\\1")  # Triple up on the first vowel
stringr::str_replace(fruits, "[aeiou]", c("1", "2", "3"))  # First vowel to 1, 2, 3 in word 1, 2, 3
stringr::str_replace(fruits, c("a", "e", "i"), "-")  # First a -> - in word 1, first e -> - in word 2 . . . 

stringr::str_replace_all(fruits, "([aeiou])", "\\1\\1")  # Double up on all vowels
stringr::str_replace_all(fruits, "[aeiou]", c("1", "2", "3"))  # All vowels to 1, 2, 3, in word 1, 2, 3
stringr::str_replace_all(fruits, c("a", "e", "i"), "-")  # All a -> - in word 1, . . . 

```
  
Further, the outline from the weather gathering data cleaning challenge is noted:  
  
* The weather data tidying challenge  
	* Historical weather data from Boston  
	    * 12 months beginning December 2014  
	    * Columns are values (X1 means day 1, X2 means day 2, etc.), while measure (Max, Min, etc.) should be variables  
		* Variables coded incorrectly  
		* Missing and extreme values  
		* Etc.  
	
* STEP 1: UNDERSTAND STRUCTURE  
	* class(), dim(), names(), str(), dplyr::glimpse(), summary()  
* STEP 2: LOOK AT DATA  
	* head(), tail(), print(), hist(), plot()  
* STEP 3: TIDY DATA  
	* gather(), spread(), separate()  
* STEP 4: CONVERT TYPES  
    * lubridate() and variants, as.character() and variants, stringr() and variants, tidyr::unite()  
* STEP 5: MANAGE MISSING and EXTREME (OUTLIER) VALUES  
    * is.na(), sum(is.na()), any(is.na()), which(is.na())  
    * which(a, arr.ind=TRUE) returns a little matrix of rows and columns - nice!	

