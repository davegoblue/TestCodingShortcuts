---
title: "Mapping and Plotting"
author: "davegoblue"
date: "3/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

This document is to provide some plotting examples for reference.

***

#### _Example #1: US State and County Maps_ 
The package usmap contains maps of US states and counties.  There is also some associated data available about state and county demographics.

Example code includes:  
```{r}

library(tidyverse)

# Population by county
data(countypop, package="usmap")
# Population by state
data(statepop, package="usmap")
# Poverty rate by county
data(countypov, package="usmap")
# Poverty rate by state
data(statepov, package="usmap")
# Population of largest city by state
data(citypop, package="usmap")
# Location of earthquakes
data(earthquakes, package="usmap")


# Included datasets
countypop
statepop
countypov
statepov
citypop
earthquakes

# Basic, empty US maps
usmap::plot_usmap(regions="states")
usmap::plot_usmap(regions="counties")

# Basic, empty US maps subsetted to an area
usmap::plot_usmap(regions="states", 
                  include=c("WA", "OR", "CA", "NV", "ID", "MT", "WY", "UT", "CO", "AZ", "NM")
                  )
usmap::plot_usmap(regions="counties", include=c("MN", "WI", "MI", "OH", "PA", "NY", "IN", "IL"))

# Basic, subsetted state map with poverty rates included
usmap::plot_usmap(regions="states", 
                  include=c("WA", "OR", "CA", "NV", "ID", "MT", "WY", "UT", "CO", "AZ", "NM"), 
                  values="pct_pov_2014", data=statepov, labels=TRUE
                  ) + 
    scale_fill_continuous(low="lightblue", high="darkblue", "Poverty Rate (%)") + 
    labs(title="Poverty Rates by Western and Mountain States")

# Basic, subsetted county map with poverty rates included
usmap::plot_usmap(regions="counties", include=c("MN", "WI", "MI", "OH", "PA", "NY", "IN", "IL"),
                  values="pct_pov_2014", data=countypov, labels=FALSE
                  ) + 
    scale_fill_continuous(low="lightblue", high="darkblue", "Poverty Rate (%)") + 
    labs(title="Poverty Rates by County in Great Lakes States")

```

#### _Example #2: Converting and adding lat/lon data_ 
The latitude and longitude data can be converted to a form suitable for usmap by using the usmap_transform function.

Example code includes:  
```{r}

# Transform the earthquakes data
trQuakes <- usmap::usmap_transform(earthquakes)
str(trQuakes)

# Add as a layer to the state map
usmap::plot_usmap(regions="states") + 
    geom_point(data=trQuakes, aes(x=lon.1, y=lat.1, size=mag), alpha=0.4) + 
    labs(title="Earthquakes of Magnitude 2.5+ (H1 2019)")

# Transform the largest city data
trCity <- usmap::usmap_transform(citypop)
str(trCity)

# Add as a layer to the state map
usmap::plot_usmap(regions="states") + 
    geom_point(data=trCity, aes(x=lon.1, y=lat.1, size=city_pop)) + 
    labs(title="Largest City by State")

```
  
#### _Example #3: Filtering and coloring by region_ 
The census region definitions are included, and can be used to filter or color the maps.

Example code includes:  
```{r}

# Filter the map to include only new_england, mid_atlantic, and south_atlantic
usmap::plot_usmap(regions="states", 
                  include=c(usmap::.new_england, usmap::.mid_atlantic, usmap::.south_atlantic)
                  )

# Create regions data for US states
regionData <- usmap::statepop %>%
    mutate(region=as.factor(ifelse(abbr %in% usmap::.midwest_region, 1, 0)))
usmap::plot_usmap(regions="states", data=regionData, values="region") + 
    scale_fill_discrete("Midwest") + 
    labs(title="Midwest Region US States")

# Enhanced Coloring and Labelling
usmap::plot_usmap(regions="states", data=regionData, values="region") + 
    scale_fill_manual(values=c("lightgray", "lightblue"), "", labels=c("Other", "Midwest")) + 
    labs(title="Midwest Region US States")

```

#### _Example #4: Labelling geographies_ 
Since usmap is built on ggplot2, the standard techniques from ggplot2 can be used to enhance the geography labelling.  Further, centroids for the geographies are available in loadable files.

Example code includes:  
```{r}

# Base state map labelled with defaults
usmap::plot_usmap(regions="states", labels=TRUE, label_color="red")

# Base county map labelled with defaults
usmap::plot_usmap(regions="counties", include=c("TX", "OK"), labels=TRUE, label_color="grey")

# Load state centroid data
stCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "states", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 3)), stringsAsFactors = FALSE
                            )

# Load county centroid data
ctCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "counties", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 4)), stringsAsFactors = FALSE
                            )

# Add state labels using geom_text
regionData <- usmap::statepop %>%
    mutate(region=as.factor(ifelse(abbr %in% usmap::.midwest_region, 1, 0))) %>%
    left_join(stCenter %>% select(x, y, full, fips) %>% rename(fname=full)) %>%
    mutate(fname=ifelse(fname=="District of Columbia", "DC", str_replace_all(fname, " ", "\n")))

usmap::plot_usmap(regions="states", data=regionData[, c("fips", "region")], values="region") + 
    scale_fill_manual(values=c("lightgray", "lightblue"), "", labels=c("Other", "Midwest")) + 
    labs(title="Midwest Region US States") + 
    geom_text(data=filter(regionData, region==1), aes(x=x, y=y, label=fname), size=2.5)


# Add county labels using geom_text
regionData <- usmap::countypop %>%
    mutate(region=as.factor(case_when(abbr=="OK" ~ 1, abbr=="TX" ~ 2, TRUE ~ 0))) %>%
    left_join(ctCenter %>% select(x, y, county, fips) %>% rename(cname=county)) %>%
    mutate(cname=str_replace_all(str_replace(cname, " County", ""), " ", "\n"))

usmap::plot_usmap(regions="counties", include=c("TX", "OK"), 
                  data=regionData[, c("fips", "region")], values="region") + 
    scale_fill_manual(values=c("red", "orange"), "", labels=c("Oklahoma", "Texas")) + 
    labs(title="Texas and Oklahoma Counties") + 
    geom_text(data=filter(regionData, abbr %in% c("TX", "OK")), 
              aes(x=x, y=y, label=cname), size=2.5, 
              color=ifelse(pull(filter(regionData, abbr %in% c("TX", "OK")), abbr)=="OK", "white", "black")
              )

```

#### _Example #5: Adding population centers_ 
Separate data exists for key population centers, which can be loaded and then added to maps.

Example code includes:  
```{r}

# Transform the largest city data
str(maps::us.cities)
trCity <- usmap::usmap_transform(select(maps::us.cities, long, lat, everything())) %>% 
    mutate(useName=str_replace_all(str_sub(name, 1, -4), " ", "\n"))
str(trCity)

# Define a key region for plotting
rgnPlot <- c(usmap::.west_south_central, usmap::.east_south_central)
popFilter <- 100000

# Add cities as a layer to the state map
usmap::plot_usmap(regions="states", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("South Central Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               )

# Plot the full nation for cities of 250k +
rgnPlot <- c(usmap::.midwest_region, usmap::.northeast_region, 
             usmap::.south_region, usmap::.west_region
             )
popFilter <- 250000

# Add cities as a layer to the state map
usmap::plot_usmap(regions="states", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("US Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               )

# Plot cities by name for the Four Corners region
rgnPlot <- c("UT", "CO", "NM", "AZ")
popFilter <- 125000

# Add cities as a layer to the state map (points)
usmap::plot_usmap(regions="counties", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("Four Corners Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               )

# Add cities as a layer to the state map (text)
usmap::plot_usmap(regions="counties", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("Four Corners Cities with Population >= ", popFilter/1000, "k")) + 
    geom_text(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop, label=useName)
               )

popFilter <- 50000
popFilter2 <- 250000

# Add cities as a layer to the state map (points and text)
usmap::plot_usmap(regions="counties", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("Four Corners Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               ) + 
    geom_text(data=filter(trCity, pop >= popFilter2, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop, label=useName), color="red", show.legend=FALSE
               )

```
  
#### _Example #6: Custom coloring geographies_  
Using scale_fill_manual(), custom colors can be created by geography.

Example code includes:  
```{r}

# Basic county population map with continuous colors
usmap::countypop %>% 
    filter(abbr %in% c("OH", "IN", "KY")) %>% 
    mutate(pop=pop_2015/1000, name=str_replace(str_replace(county, " County", ""), " ", "\n")) %>%
    usmap::plot_usmap(regions="counties", include=c("OH", "IN", "KY"), data=., values="pop") +
    scale_fill_continuous(low="lightblue", high="darkblue", "Pop. (000)") + 
    labs(title="Indiana, Ohio, and Kentucky - Population by County")

# Custom county population map with colors - red for Indiana, blue for Kentucky, grey for Ohio
popBucket <- c(0, 100, 500)
popLabels <- sapply(1:(length(popBucket)-1), FUN=function(x){paste0(popBucket[x], "-", popBucket[x+1])})
popLabels <- c(popLabels, paste0(popBucket[length(popBucket)], "+"))
guideLabels <- paste(rep(c("OH", "KY", "IN"), each=3), popLabels)

usmap::countypop %>% 
    filter(abbr %in% c("OH", "KY", "IN")) %>% 
    mutate(pop=pop_2015/1000, name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           pBucket=findInterval(pop, popBucket), 
           pColor=rgb(abbr=="IN", 0, abbr=="KY", pBucket/length(popBucket))
           ) %>%
    usmap::plot_usmap(regions="counties", include=c("OH", "IN", "KY"), data=., values="pColor") +
    scale_fill_identity(guide="legend", "Pop. (000)", labels=guideLabels) + 
    labs(title="Indiana, Ohio, and Kentucky - Population by County") + 
    theme(legend.position = "bottom") + 
    guides(fill=guide_legend(nrow=3))

# Custom county poverty rate map with colors - red for Indiana, blue for Kentucky, grey for Ohio
povBucket <- c(0, 15, 30)
povLabels <- sapply(1:(length(povBucket)-1), FUN=function(x){paste0(povBucket[x], "-", povBucket[x+1])})
povLabels <- c(povLabels, paste0(povBucket[length(povBucket)], "+"))
guideLabels <- paste(rep(c("OH", "KY", "IN"), each=3), povLabels)

usmap::countypov %>% 
    filter(abbr %in% c("OH", "KY", "IN")) %>% 
    mutate(name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           pBucket=findInterval(pct_pov_2014, povBucket), 
           pColor=rgb(abbr=="IN", 0, abbr=="KY", pBucket/length(povBucket))
           ) %>%
    usmap::plot_usmap(regions="counties", include=c("OH", "IN", "KY"), data=., values="pColor") +
    scale_fill_identity(guide="legend", "Poverty Rate (%)", labels=guideLabels) + 
    labs(title="Indiana, Ohio, and Kentucky - Poverty Rate by County") + 
    theme(legend.position = "bottom") + 
    guides(fill=guide_legend(nrow=3))

```

#### _Example #7: Custom labeling of key geographies_  
The above techniques can be combined for custom labeling of key geographies.

Example code includes:  
```{r}

# Basic state population data
stateData <- usmap::statepop %>% 
    mutate(pop=round(pop_2015/1000000, 1), 
           name=ifelse(full=="District of Columbia", "DC", str_replace(full, " ", "\n")), 
           lab=paste0(abbr, "\n(", pop, ")\n")
           )

# Load state centroid data
stCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "states", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 3)), stringsAsFactors = FALSE
                            )

# Grab centroids for top 5 states
top5State <- stateData %>%
    top_n(5, pop) %>% 
    left_join(select(stCenter, x, y, fips))

# Plot state population with continuous colors and custom labels
stateData %>% 
    usmap::plot_usmap(regions="states", data=., values="pop") +
    scale_fill_continuous(low="lightblue", high="darkblue", "Pop. (millions)") + 
    labs(title="Population by State", subtitle="Top 5 in millions") + 
    geom_text(data=top5State, aes(x=x, y=y, label=lab), color="white", size=4, fontface="bold")


# Load county centroid data
ctCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "counties", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 4)), stringsAsFactors = FALSE
                            )

# Custom county population map with colors - red for Wisconsin, blue for Michigan
popBucket <- c(0, 100, 500)
popLabels <- sapply(1:(length(popBucket)-1), FUN=function(x){paste0(popBucket[x], "-", popBucket[x+1])})
popLabels <- c(popLabels, paste0(popBucket[length(popBucket)], "+"))
guideLabels <- paste(rep(c("MI", "WI"), each=3), popLabels)

# Grab county data for counties exceeding the top popBucket
ctyData <- usmap::countypop %>%
    filter(abbr %in% c("MI", "WI")) %>%
    mutate(pop=round(pop_2015/1000, 0), 
           name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           lab=paste0(name, "\n(", pop, ")\n")
           )

topCounty <- ctyData %>%
    filter(pop >= max(popBucket)) %>%
    left_join(select(ctCenter, x, y, fips))

# Create county population map
usmap::countypop %>% 
    filter(abbr %in% c("MI", "WI")) %>% 
    mutate(pop=pop_2015/1000, name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           pBucket=findInterval(pop, popBucket), 
           pColor=rgb(abbr=="WI", 0, abbr=="MI", pBucket/length(popBucket))
           ) %>%
    usmap::plot_usmap(regions="counties", include=c("WI", "MI"), data=., values="pColor") +
    scale_fill_identity(guide="legend", "Pop. (000)", labels=guideLabels) + 
    geom_text(data=topCounty, aes(x=x, y=y, label=lab), size=3, fontface="bold", color="white") +
    labs(title="Michigan and Wisconsin - Population by County", subtitle="Labelled Pop. (000) for 500k+") + 
    theme(legend.position = "bottom") + 
    guides(fill=guide_legend(nrow=3)) + 
    theme(panel.background=element_rect(color="black", fill="lightgrey"))

```

#### _Example #8: Plotting Weather Data (Temperatures and Dew Points)_  
The ggridges package has weather data for Lincoln, NE in the data file 'lincoln_weather'.  The data are captured once per day for 366 days of 2016.  Simple plots can be made of the average temperatures and dew points.

Example code includes:  
```{r}

data(lincoln_weather, package="ggridges")
str(lincoln_weather, give.attr=FALSE)

# Extract temperature and dew point data
tdData <- lincoln_weather %>%
    select(CST, maxT=`Max Temperature [F]`, minT=`Min Temperature [F]`, meanT=`Mean Temperature [F]`, 
           maxD=`Max Dew Point [F]`, minD=`Min Dewpoint [F]`, meanD=`Mean Dew Point [F]`
           ) %>%
    mutate(date=as.Date(CST))
str(tdData)

# Plot temperatures by day
tdData %>%
    select(date, maxT, meanT, minT) %>%
    pivot_longer(-date) %>%
    ggplot(aes(x=date, y=value, group=name)) + 
    geom_line(aes(color=name))

# Plot dew points by day
tdData %>%
    select(date, maxD, meanD, minD) %>%
    pivot_longer(-date) %>%
    ggplot(aes(x=date, y=value, group=name)) + 
    geom_line(aes(color=name))


library(xts)

# Create an XTS for temperature data
tdXTS <- xts(select(tdData, minT, meanT, maxT), order.by=tdData$date)

# Create and plot weekly and monthly averages
tdXTS %>%
    apply.weekly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Weekly Temperature Average (Lincoln, NE 2016)")

tdXTS %>%
    apply.monthly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Monthly Temperature Average (Lincoln, NE 2016)")


# Create an XTS for dew-point data
tdXTS <- xts(select(tdData, minD, meanD, maxD), order.by=tdData$date)

# Create and plot weekly and monthly averages
tdXTS %>%
    apply.weekly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Weekly Dew Point Average (Lincoln, NE 2016)")

tdXTS %>%
    apply.monthly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Monthly Dew Point Average (Lincoln, NE 2016)")

```

#### _Example #9: Combining xts and ggplot2_  
The xts package is good for working with time series data, while ggplot2 is strong for customizing plots.  The packages can be combined in using the weather data.

Example code includes:  
```{r}

# Create an XTS for temperature and dewpoint data
tdXTS <- xts(select(tdData, minT, meanT, maxT, minD, meanD, maxD), order.by=tdData$date)

# Use xts for monthly average and ggplot2 for plotting
basePlot <- tdXTS %>%
    apply.monthly(FUN=mean, na.rm=TRUE) %>% 
    data.frame(date=index(.), row.names=NULL) %>% 
    ggplot(aes(x=date-lubridate::days(15))) + 
    geom_ribbon(aes(ymin=minT, ymax=maxT), color="lightblue", fill="lightblue", alpha=0.5) +
    geom_line(aes(y=meanT), color="blue", lwd=1) + 
    labs(x="Month", y="Avg. Temperature (F)", title="Lincoln, NE Weather (2016)", 
         subtitle="Monthly Avg. Temperature (F)"
         )
basePlot

# Add labelling for the three elements
hiMonth <- index(tdXTS %>% apply.monthly(FUN=mean))[3]
loMonth <- index(tdXTS %>% apply.monthly(FUN=mean))[9]
muMonth <- index(tdXTS %>% apply.monthly(FUN=mean))[6]
hiPoint <- c(60, 75)
loPoint <- c(45, 25)
muPoint <- c(72.5, 45)

labFrame <- data.frame(x=c(hiMonth, loMonth, muMonth), 
                       yend=c(hiPoint[1], loPoint[1], muPoint[1]),
                       y=c(hiPoint[2], loPoint[2], muPoint[2]), 
                       text=c("Avg. Monthly High", "Avg. Monthly Low", "Avg. Monthly Mean")) %>%
    mutate(xend=x)

basePlot + 
    geom_segment(data=labFrame, aes(x=x, y=y, xend=xend, yend=yend), arrow=arrow()) + 
    geom_text(data=labFrame, aes(x=x, y=y+c(5, -5, -5), label=text), fontface="bold", size=4)


# Can also create and plot a custom rolling average
baseData <- tdXTS %>%
    data.frame(date=index(.), row.names=NULL)

base7Day <- rollapply(tdXTS, 7, FUN=mean, na.rm=TRUE) %>% 
    data.frame(date=index(.), row.names=NULL)

base30Day <- rollapply(tdXTS, 30, FUN=mean, na.rm=TRUE) %>% 
    data.frame(date=index(.), row.names=NULL)

plotFrame <- bind_rows(baseData, base7Day, base30Day, .id="rolling") %>%
    mutate(rollLabel=case_when(rolling==1 ~ "Daily", 
                               rolling==2 ~ "7 Day Rolling", 
                               rolling==3 ~ "30 Day Rolling",
                               TRUE ~ "ERROR"
                               )
           )
            
plotFrame %>%
    ggplot(aes(x=date)) + 
    geom_line(aes(y=meanT, color=rollLabel, group=rollLabel), lwd=1) + 
    labs(x="Month", y="Avg. Temperature (F)", title="Lincoln, NE Weather (2016)", 
         subtitle="Daily Avg. Temperature (F)"
         )

```

#### _Example #10: Plotting Weather Data (Humidity)_  
Humidity data are also available in the lincoln_weather dataset.  There is a relationship between temperature, dewpoint, and humidity.

Example code includes:  
```{r}

htdData <- lincoln_weather %>%
    select(CST, meanT=`Mean Temperature [F]`, meanD=`Mean Dew Point [F]`, meanH=`Mean Humidity`) %>%
    mutate(date=as.Date(CST), month=lubridate::month(date))
str(htdData)

# Histogram for average humidity
htdData %>%
    ggplot(aes(x=meanH)) + 
    geom_histogram() + 
    labs(title="Mean Humidity Histogram", subtitle="Lincoln, NE (2016)", x="Mean Humidity (%)", y="Count")

htdData %>%
    filter((meanH < 25) | is.na(meanH))

htdData <- htdData %>%
    filter(!((meanH < 25) | is.na(meanH)))

# Updated Histogram for average humidity
htdData %>%
    ggplot(aes(x=meanH)) + 
    geom_histogram() + 
    labs(title="Mean Humidity Histogram", subtitle="Lincoln, NE (2016)", x="Mean Humidity (%)", y="Count")

# Histogram for dewpoint depression (T - D)
htdData %>%
    ggplot(aes(x=meanT-meanD)) + 
    geom_histogram() + 
    labs(title="Mean Dewpoint Depression Histogram", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint Depression (F)", y="Count")

htdData %>% 
    filter(meanD >= meanT)

htdData <- htdData %>%
    filter(meanT >= meanD)

# Updated Histogram for dewpoint depression (T - D)
htdData %>%
    ggplot(aes(x=meanT-meanD, y=..density..)) + 
    geom_histogram(binwidth=1) + 
    geom_density(color="red") + 
    labs(title="Mean Dewpoint Depression Histogram", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint Depression (F)", y="Proportion")

# Average humidity by month
htdData %>%
    group_by(month) %>%
    summarize(meanH=mean(meanH, na.rm=TRUE)) %>%
    ggplot(aes(x=as.factor(month), y=meanH)) + 
    geom_col() + 
    labs(title="Average Humidity by Month", subtitle="Lincoln, NE (2016)", x="Month", y="Mean Humidity (%)")

# Relationship between temperature and dewpoint
htdData %>%
    ggplot(aes(x=meanD, y=meanT)) + 
    geom_point() + 
    geom_abline(slope=1, intercept=0) + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint (F)", y="Mean Temperature (F)"
         )

# Relationship between dewpoint depression and humidity
htdData %>%
    mutate(dpD=meanT-meanD) %>%
    ggplot(aes(x=dpD, y=meanH)) + 
    geom_point() + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint Depression (F)", y="Mean Humidity (%)"
         )

# Relationship between temperature and dewpoint and humidity
humInts <- c(0, 50, 60, 70, 80)
humLabel <- sapply(1:(length(humInts)-1), FUN=function(x) { paste0(humInts[x], "-", humInts[x+1]) })
humLabel <- c(humLabel, paste0(humInts[length(humInts)], "+"))

htdData %>%
    mutate(humBin=factor(findInterval(meanH, humInts), levels=1:length(humInts), labels=humLabel)) %>%
    ggplot(aes(x=meanD, y=meanT, color=humBin)) + 
    geom_point() + 
    geom_smooth(se=FALSE) +
    geom_abline(slope=1, intercept=0) + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint (F)", y="Mean Temperature (F)"
         )

# Expressed using dewpoint depression vs. dewpoint
htdData %>%
    mutate(humBin=factor(findInterval(meanH, humInts), levels=1:length(humInts), labels=humLabel)) %>%
    ggplot(aes(x=meanD, y=meanT-meanD, color=humBin)) + 
    geom_point() + 
    geom_smooth() +
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint (F)", y="Dewpoint Depression (F)"
         )

# Expressed using dewpoint depression vs. temperature
htdData %>%
    mutate(humBin=factor(findInterval(meanH, humInts), levels=1:length(humInts), labels=humLabel)) %>%
    ggplot(aes(x=meanT, y=meanT-meanD, color=humBin)) + 
    geom_point() + 
    geom_smooth(se=FALSE, method="lm") +
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Temperature (F)", y="Dewpoint Depression (F)"
         )

# Linear regression for temperature, dewpoint, and humidity
htdReg <- htdData %>%
    mutate(dpD=meanT-meanD) %>%
    lm(meanH ~ meanT + dpD, data=.)
summary(htdReg)

htdData %>%
    mutate(dpD=meanT-meanD) %>%
    mutate(predH=predict(htdReg, newdata=.)) %>%
    ggplot(aes(x=predH, y=meanH)) + 
    geom_point() + 
    geom_abline(slope=1, intercept=0) + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Predicted Humidity (%)", y="Actual Humidity (%)"
         )

```

#### _Example #11: Plotting Weather Data (Wind)_  
Wind data (speed, gust, direction) are also available in the lincoln_weather dataset..

Example code includes:  
```{r}

# Extract wind data
wdData <- lincoln_weather %>%
    select(CST, maxW=`Max Wind Speed [MPH]`, maxG=`Max Gust Speed [MPH]`, meanW=`Mean Wind Speed[MPH]`, 
           dirW=`WindDir [Degrees]`
           ) %>%
    mutate(date=as.Date(CST))
str(wdData)

# Manage missing data
wdData[!complete.cases(wdData), ]

wdData <- wdData %>%
    filter(dirW != -1, !is.na(dirW)) %>%
    mutate(maxG=ifelse(is.na(maxG), maxW, maxG))
summary(wdData)

# Manage very high wind data
wdData[wdData$maxG >= 60, ]
wdData <- wdData %>%
    filter(maxG <= 80)
summary(wdData)

# Density of wind speeds
wdData %>%
    select(date, meanW, maxW, maxG) %>%
    pivot_longer(-date) %>%
    ggplot(aes(x=value, fill=name)) + 
    geom_density(alpha=0.5) + 
    scale_fill_discrete(name="Wind Speed [MPH]", labels=c("Max Gust", "Max", "Mean")) + 
    labs(title="Lincoln, NE (2016) Wind Speeds", y="Density", x="Wind Speed [MPH]")

# Density of wind direction
wdData %>%
    select(date, dirW) %>%
    ggplot(aes(x=dirW)) + 
    geom_density(alpha=0.5, fill="blue") + 
    labs(title="Winds are mainly from the S and NW", subtitle="Lincoln, NE (2016)", 
         y="Density", x="Wind Direction"
         )

# Wind speed and direction
wdData %>% 
    ggplot(aes(x=meanW, y=dirW)) + 
    geom_point(alpha=0.25) + 
    coord_polar(theta="y") + 
    labs(title="Lincoln, NE (2016)", subtitle="Direction vs. Mean Wind Speed", x="Mean Wind Speed [MPH]") + 
    scale_y_continuous(limits=c(0, 360), breaks=c(0, 90, 180, 270, 360)) + 
    scale_x_continuous(limits=c(0, 30), breaks=c(0, 5, 10, 15, 20, 25, 30)) + 
    geom_point(aes(x=0, y=0), color="red", size=2)

# Wind speed and direction as factors
windDirs <- c("N", "NE", "E", "SE", "S", "SW", "W", "NW")
windSpeeds <- c(0, 5, 10, 15)
windLabels <- sapply(1:(length(windSpeeds)-1), FUN=function(x){ paste0(windSpeeds[x], "-", windSpeeds[x+1]) })
windLabels <- c(windLabels, paste0(windSpeeds[length(windSpeeds)], "+"))
wdData <- wdData %>% 
    mutate(wd=factor(floor(((dirW+22.5)/45) %% 8), levels=0:7, labels=windDirs), 
           ws=factor(findInterval(meanW, windSpeeds), levels=length(windSpeeds):1, labels=rev(windLabels))
           )

# Summary of interaction between wind speed and wind direction 
wdData %>%
    group_by(wd) %>% 
    summarize(n=n(), avgMean=mean(meanW), avgMax=mean(maxW), avgGust=mean(maxG))
table(wdData$wd, wdData$ws)

# Graph of wind speed and wind direction
wdData %>%
    ggplot(aes(x=wd, fill=ws)) + 
    geom_bar() + 
    scale_fill_discrete(name="Wind Speed [MPH]") + 
    labs(title="Lincoln, NE (2016) Wind Speeds and Directions", y="# Days", x="Wind Direction")

# Graph of wind speed and wind direction (polar coordinates)
wdData %>%
    ggplot(aes(x=wd, fill=ws)) + 
    geom_bar() + 
    scale_fill_discrete(name="Wind Speed [MPH]") + 
    labs(title="Lincoln, NE (2016) Wind Speeds and Directions", y="# Days", x="Wind Direction") + 
    coord_polar(start=-0.4)
```

#### _Example #12: Archived granular weather Data (METAR)_  
Iowa State has a great database of archived weather data, including the historical METAR data (meteorological aerodrome report) for a number of reporting stations.

[METAR](https://en.wikipedia.org/wiki/METAR) include information on visibility, wind, temperature, dew point, precipitation, clouds, barometric pressure, and other features that may impact safe aviation.

The data for station KLNK (Lincoln, NE airport) was saved as a CSV from [Iowa State](https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LNK&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2)

Some processing is required before using the METAR data:

Example code includes:  
```{r}

# Load METAR data
klnk <- readr::read_csv("./RInputFiles/metar_klnk_2016.txt", na=c("", "NA", "M"))
str(klnk, give.attr=FALSE)

# Filter to only data that ends with times ending in 54Z
metarKLNK <- klnk %>%
    filter(str_detect(metar, "54Z"))
dim(metarKLNK)


# There should be 24*368=8832 records, so there are a handful (19) of missing METAR observations
minDate <- min(metarKLNK$valid)
expDate <- minDate + lubridate::hours(0:(24*368 - 1))

# Observations expected but not recorded
as.POSIXct(setdiff(expDate, metarKLNK$valid), origin="1970-01-01", tz="UTC")

# Observations recorded but not expected
setdiff(metarKLNK$valid, expDate)

# Confirmation of uniqueness
length(unique(metarKLNK$valid)) == length(metarKLNK$valid)


# Extract wind speeds and direction
# The general wind format is dddssGssKT where ddd is the direction (VRB meaning variable), the main ss is the speed, and the Gss is the gust speed (optional and not always displayed)

mtxWind <- metarKLNK %>%
    pull(metar) %>%
    str_match(pattern="(\\d{3}|VRB)(\\d{2})(G\\d{2})?KT")
head(mtxWind)

table(mtxWind[, 2], useNA="ifany")
table(mtxWind[, 3], useNA="ifany")
table(mtxWind[, 4], useNA="ifany")

# Verify that winds not captured are in fact missing from the METAR
metarKLNK[which(is.na(mtxWind[, 2])), "metar"]

metarKLNK <- metarKLNK %>%
    mutate(dirW=mtxWind[, 2], 
           spdW=as.numeric(mtxWind[, 3]), 
           gustW=as.numeric(str_replace(mtxWind[, 4], "G", ""))
           )

# Plot for the wind direction
metarKLNK %>%
    ggplot(aes(x=dirW)) + 
    geom_bar() + 
    labs(title="Lincoln, NE Wind Direction", subtitle="KLNK METAR (2016)", 
         y="# Hourly Observations", x="Wind Direction"
         )

# Plot for the minimum, average, and maximum wind speed by wind direction
# Wind direction 000 is reserved for 0 KT wind, while VRB is reserved for 3-6 KT variable winds
metarKLNK %>%
    filter(!is.na(dirW)) %>%
    group_by(dirW) %>%
    summarize(minWind=min(spdW), meanWind=mean(spdW), maxWind=max(spdW)) %>%
    ggplot(aes(x=dirW)) + 
    geom_point(aes(y=meanWind), color="red", size=2) + 
    geom_errorbar(aes(ymin=minWind, ymax=maxWind)) + 
    labs(title="Lincoln, NE Wind Direction", subtitle="KLNK METAR (2016)", 
         y="Wind Speed [KT]", x="Wind Direction"
         )

# Plot for the wind speed
# Roughly 10% of the time, there is no wind in Lincoln
metarKLNK %>%
    ggplot(aes(x=spdW)) + 
    geom_bar(aes(y=..count../sum(..count..))) + 
    labs(title="Roughly 10% of wind speeds in Lincoln, NE measure 0 Knots", subtitle="KLNK METAR (2016)", 
         y="% Hourly Observations", x="Wind Speed {KT]"
         )

metarKLNK %>% 
    filter(!is.na(dirW), dirW != "VRB", dirW != "000") %>%
    mutate(dirW=as.numeric(dirW)) %>%
    group_by(dirW, spdW) %>%
    summarize(n=n()) %>%
    ggplot(aes(x=spdW, y=dirW)) + 
    geom_point(alpha=0.1, aes(size=n)) + 
    coord_polar(theta="y") + 
    labs(title="Lincoln, NE (2016)", subtitle="Direction vs. Wind Speed", x="Wind Speed [KT]") + 
    scale_y_continuous(limits=c(0, 360), breaks=c(0, 90, 180, 270, 360)) + 
    scale_x_continuous(limits=c(0, 30), breaks=c(0, 5, 10, 15, 20, 25, 30)) + 
    geom_point(aes(x=0, y=0), color="red", size=2)

```

#### _Example #13: Extracting Key Elements from METAR_  
A properly formatted METAR includes the following information in order, though with variable amounts of other information in between.

dddd54Z ddddd[Gdd]KT dSM [M]dd/[M]dd Adddd RMK SLPddd Tdddddddd

* dddd54Z is the two-digit date and four-digit Zulu time (KLNK METAR are taken at 54 minutes past the hour)  
* ddddd[Gdd]KT is the three-digit wind direction (can be VRB), two digit wind speed in knots, and sometimes the two digit maximum gust in knots  
* dSM is the visibility in statute miles.  This is somewhat tricky in that d can be any of 0-10 but can also be 1/4, 1/2, 3/4, 1 1/4, 1 1/2, 1 3/4, 2 1/2  
* [M]dd/[M]dd is the temperature in celsius and dewpoint in celsius.  M means negative  
* Adddd is the four-digit altimeter reading  
* RMK notes that the remarks are beginning  
* SLPddd notes the three-digit sea-level pressure  
* Tdddddddd notes the four digit temperature in Celsius and the four digit dewpoint in celsius.  If it begins with 1 it is negative.  The fourth digit is the decimal.  It will always be a Celsius reading that best corresponds to integer degrees of Fahrenheit  
  
Example code includes:  
```{r}

metAll <- metarKLNK %>%
    pull(metar)

# Create a search string for METAR
valMet <- "54Z.*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})"

# Find the number of matching elements
str_detect(metAll, pattern=valMet) %>% table()

# The strings that do not match have errors in the raw data (typically, missing wind speed)
metAll[!str_detect(metAll, pattern=valMet)]

# A matrix of string matches can be obtained
mtxParse <- str_match(metAll, pattern=valMet)
head(mtxParse)

# Create a data frame
dfParse <- data.frame(mtxParse, stringsAsFactors=FALSE)
names(dfParse) <- c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
                    "TempC", "DewC", "Altimeter", "SLP", "FahrC"
                    )
dfParse <- tibble::as_tibble(dfParse)
str(dfParse)

# Convert to numeric where appropriate
dfParse <- dfParse %>%
    mutate(WindSpeed = as.integer(WindSpeed), 
           WindGust = as.numeric(WindGust), 
           Visibility = as.numeric(str_replace(Visibility, "SM", "")),
           TempC = as.integer(str_replace(TempC, "M", "-")), 
           DewC = as.integer(str_replace(DewC, "M", "-")), 
           Altimeter = as.integer(str_replace(Altimeter, "A", "")), 
           SLP = as.integer(str_replace(SLP, "SLP", "")), 
           TempF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 2, 5), pattern="^1", "-"))/10, 
           DewF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 6, 9), pattern="^1", "-"))/10
           )

# Investigate the data
set.seed(2003211416)
str(dfParse)
head(dfParse)
tail(dfParse)
dfParse %>% 
    sample_n(20)

# Check for NA values
colSums(is.na(dfParse))

# Plot of counts by key metric
keyMetric <- c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
               "DewC", "Altimeter", "SLP", "TempF", "DewF"
               )

for (x in keyMetric) {
    p <- dfParse %>%
        group_by_at(vars(all_of(x))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=x, y="n")) + 
        geom_col() + 
        labs(title=x, y="Count")
    print(p)
}

# There are three obvious issues
# Visibility is not correctly picked up when there is a / such as 1/2 SM
# Wind gusts are never picked up
# Sea Level Pressure is missing a digit

# Correct for visibility
# Areas that have \\d \\d/\\dSM
sm1 <- which(str_detect(metAll, pattern=" \\d/\\dSM"))
sm2 <- which(str_detect(metAll, pattern=" \\d \\d/\\dSM"))

valSM1 <- str_match(metAll, pattern="\\d/\\dSM")[sm1]
valSM1 <- str_replace(valSM1, "SM", "")
valSM1 <- as.integer(str_sub(valSM1, 1, 1)) / as.integer(str_sub(valSM1, 3, 3))

valSM2 <- str_match(metAll, pattern=" \\d \\d/\\dSM")[sm2]
valSM2 <- as.integer(str_sub(valSM2, 2, 2))

dfParse[sm1, "Visibility"] <- valSM1
dfParse[sm2, "Visibility"] <- dfParse[sm2, "Visibility"] + valSM2

dfParse %>% 
    count(Visibility)


# Correct for wind gusts
gustCheck <- which(str_detect(metAll, pattern="\\d{5}G\\d{2}KT"))
valGust <- str_match(metAll, pattern="\\d{5}G\\d{2}KT")[gustCheck]
valGust <- as.integer(str_sub(valGust, 7, 8))

dfParse[gustCheck, "WindGust"] <- valGust

dfParse %>% 
    count(WindGust) %>% 
    as.data.frame

# Correct for SLP
dfParse <- dfParse %>%
    mutate(modSLP=ifelse(dfParse$SLP < 500, 1000 + dfParse$SLP/10, 900 + dfParse$SLP/10))

dfParse %>%
    group_by(SLP, modSLP) %>%
    summarize(n=n()) %>%
    ggplot(aes(x=SLP, y=modSLP, size=n)) + 
    geom_point(alpha=0.3)

# Check updated plots
keyMetric <- c("WindGust", "Visibility", "modSLP")
for (x in keyMetric) {
    p <- dfParse %>%
        group_by_at(vars(all_of(x))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=x, y="n")) + 
        geom_col() + 
        labs(title=x, y="Count")
    print(p)
}

```

#### _Example #14: Relationships Between METAR Variables_  
Many of the METAR variables are correlated/associated to one another.

Example code includes:  
```{r}

# Define key numeric variables
coreNum <- c("TempC", "TempF", "DewC", "DewF", "Altimeter", "modSLP", "WindSpeed", "Visibility")

# Add the date back to the file (should edit the above instead)
dfParse <- dfParse %>%
    mutate(month=lubridate::month(metarKLNK$valid))
str(dfParse)

# Keep only complete cases and find correlations
mtxCorr <- dfParse %>%
    mutate(month=lubridate::month(metarKLNK$valid)) %>%
    select_at(vars(all_of(coreNum))) %>%
    filter(complete.cases(.)) %>%
    cor()

# Print the correlations and show a heatmap
mtxCorr %>%
    round(2)

corrplot::corrplot(mtxCorr, method="color", title="Lincoln, NE Hourly Weather Correlations (2016)")

# Create a function for plotting two variables against each other
plotNumCor <- function(var1, var2, title=NULL) {
    if (is.null(title)) 
        { title <- paste0("Lincoln, NE (2016) Hourly Correlations of ", var1, " and ", var2) }
    p <- dfParse %>%
        group_by_at(vars(all_of(c(var1, var2)))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=var1, y=var2)) + 
        geom_point(alpha=0.5, aes_string(size="n")) + 
        geom_smooth(method="lm", aes_string(weight="n")) + 
        labs(x=var1, y=var2, title=title)
    print(p)
}

# The three linear or almost linear relationships
plotNumCor("TempC", "TempF")
plotNumCor("DewC", "DewF")
plotNumCor("Altimeter", "modSLP")

# Strongly and positively related
plotNumCor("TempF", "DewF")

# Moderately negatively correlated
plotNumCor("TempF", "Altimeter")
plotNumCor("TempF", "modSLP")
plotNumCor("Altimeter", "WindSpeed")

# Predict modSLP from Altimeter
lmSLP1 <- lm(modSLP ~ Altimeter, data=dfParse)
lmSLP2 <- lm(modSLP ~ Altimeter + TempF, data=dfParse)
summary(lmSLP1)
summary(lmSLP2)

# Plot predictions vs. actual (model 1)
dfParse %>%
    filter(!is.na(modSLP)) %>%
    mutate(pred1=predict(lmSLP1)) %>%
    count(modSLP, pred1) %>%
    ggplot(aes(x=modSLP, y=pred1)) + 
    geom_point(alpha=0.25, aes(size=n)) + 
    geom_smooth(method="lm", aes(weight=n)) + 
    labs(title="Predicted vs. Actual Sea Level Pressure - Altitude Only as Predictor", 
         subtitle="Lincoln, NE (2016) Hourly METAR", x="Sea Level Pressure", y="Predicted"
         )

# Plot predictions vs. actual (model 2)
dfParse %>%
    filter(!is.na(modSLP)) %>%
    mutate(pred2=predict(lmSLP2)) %>%
    count(modSLP, pred2) %>%
    ggplot(aes(x=modSLP, y=pred2)) + 
    geom_point(alpha=0.25, aes(size=n)) + 
    geom_smooth(method="lm", aes(weight=n)) + 
    labs(title="Predicted vs. Actual Sea Level Pressure - Altitude and Temperature as Predictor", 
         subtitle="Lincoln, NE (2016) Hourly METAR", x="Sea Level Pressure", y="Predicted"
         )

```

#### _Example #15: Extracting Cloud Data from METAR_  
[Cloud data](https://en.wikipedia.org/wiki/METAR#Cloud_reporting) is also included in the METAR, with the type of clouds being described as:

* CLR - there are no clouds below 12,000 feet 
* VVddd - there is a vertical visibility of ddd hundred feet (cannot tell where the clouds are above that)  
* FEWddd - there are clouds with bases at ddd feet, and they obscure 25% or less of the sky  
* SCTddd - there are clouds with bases at ddd feet, and they obscure 25%-50% of the sky  
* BKNddd - there are clouds with bases at ddd feet, and they obscure 50%-99% of the sky  
* OVCddd - there is a full overcast with base at ddd feet  
  
The ceiling is considered the lowest height that is measured as any of OVC, BKN, or VV.

Example code includes:  
```{r}

# Extract the CLR records
mtxCLR <- str_extract_all(metarKLNK$metar, pattern=" CLR ", simplify=TRUE)
if (dim(mtxCLR)[[2]] != 1) { stop("Extracted 2+ CLR from some METAR; investigate") }
isCLR <- ifelse(mtxCLR[, 1] == "", 0, 1)

# Extract the VV records
mtxVV <- str_extract_all(metarKLNK$metar, pattern="VV(\\d{3})", simplify=TRUE)
if (dim(mtxVV)[[2]] != 1) { stop("Extracted 2+ VV from some METAR; investigate") }
isVV <- ifelse(mtxVV[, 1] == "", 0, 1)
htVV <- ifelse(mtxVV[, 1] == "", NA, as.integer(str_replace(mtxVV[, 1], "VV", ""))*100)

# Extract the FEW records
mtxFEW <- str_extract_all(metarKLNK$metar, pattern="FEW(\\d{3})", simplify=TRUE)
numFEW <- apply(mtxFEW, 1, FUN=function(x) { sum((x!=""))} )

# Extract the SCT records
mtxSCT <- str_extract_all(metarKLNK$metar, pattern="SCT(\\d{3})", simplify=TRUE)
numSCT <- apply(mtxSCT, 1, FUN=function(x) { sum((x!=""))} )

# Extract the BKN records
mtxBKN <- str_extract_all(metarKLNK$metar, pattern="BKN(\\d{3})", simplify=TRUE)
numBKN <- apply(mtxBKN, 1, FUN=function(x) { sum((x!=""))} )

# Extract the OVC records
mtxOVC <- str_extract_all(metarKLNK$metar, pattern="OVC(\\d{3})", simplify=TRUE)
numOVC <- apply(mtxOVC, 1, FUN=function(x) { sum((x!=""))} )

# Summarize as a data frame
tblClouds <- tibble::tibble(isCLR=isCLR, isVV=isVV, htVV=htVV, numFEW=numFEW, 
                            numSCT=numSCT, numBKN=numBKN, numOVC=numOVC
                            )

# Get the counts
# As expected, if isCLR then nothing else, and if isVV then nothing else
tblClouds %>% 
    count(isCLR, isVV, numFEW, numSCT, numBKN, numOVC) %>%
    as.data.frame()

# Investigate the problem data
metarKLNK$metar[rowSums(tblClouds, na.rm=TRUE)==0]

# Get the counts of most obscuration
tblClouds %>%
    filter(rowSums(., na.rm=TRUE) > 0) %>%
    mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                  numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                  TRUE ~ "Error"
                                  ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                  )
           ) %>%
    ggplot(aes(x=wType, y=..count../sum(..count..))) + 
    geom_bar() + 
    labs(title="Highest Obscuration by Cloud - Lincoln, NE (2016)", x="Cloud Type", 
         y="Proportion of Hourly Measurements"
         )

# Integrate the clouds data
mtxCloud <- cbind(mtxVV, mtxOVC, mtxBKN, mtxSCT, mtxFEW, mtxCLR)

# Cycle through to find levels of a given type
ckClouds <- function(cloudType) {
    isKey <- which(apply(mtxCloud, 2, FUN=function(x) {sum(str_detect(x, cloudType))}) > 0)
    as.integer(str_replace(mtxCloud[, min(isKey)], cloudType, "")) * 100
}
lowOVC <- ckClouds("OVC")
lowVV <- ckClouds("VV")
lowBKN <- ckClouds("BKN")
lowSCT <- ckClouds("SCT")
lowFEW <- ckClouds("FEW")

# Integrate the lowest cloud type by level
lowCloud <- tibble::tibble(lowVV, lowOVC, lowBKN, lowSCT, lowFEW)
lowCloud

# Get the lowest cloud level
minCloud <- lowCloud
minCloud[is.na(minCloud)] <- 999999
minCloudLevel <- apply(minCloud, 1, FUN=min)
minCeilingLevel <- apply(minCloud[, c("lowVV", "lowOVC", "lowBKN")], 1, FUN=min)

noCloudPct <- mean(minCloudLevel == 999999)
noCeilingPct <- mean(minCeilingLevel == 999999)

# Plot the minimum cloud level (where it exists)
data.frame(minCloudLevel, minCeilingLevel) %>%
    filter(minCloudLevel != 999999) %>%
    ggplot(aes(x=minCloudLevel)) + 
    geom_bar(aes(y=..count../sum(..count..))) + 
    geom_text(aes(x=2500, y=0.04, 
                  label=paste0(round(100*noCloudPct), "% of obs. have no clouds")
                  )
              ) + 
    labs(x="Height [ft]", y="Proportion", title="Minimum Cloud Height (when some clouds exist)", 
         subtitle="Lincoln, NE (2016)"
         )

# Plot the minimum ceiling level (where it exists)
data.frame(minCloudLevel, minCeilingLevel) %>%
    filter(minCeilingLevel != 999999) %>%
    ggplot(aes(x=minCeilingLevel)) + 
    geom_bar(aes(y=..count../sum(..count..))) + 
    geom_text(aes(x=2500, y=0.04, 
                  label=paste0(round(100*noCeilingPct), "% of obs. have no ceiling")
                  )
              ) + 
    labs(x="Height [ft]", y="Proportion", title="Minimum Ceiling Height (when a ceiling exists)", 
         subtitle="Lincoln, NE (2016)"
         )

```

#### _Example #16: Plotting by factor variables_  
The month of the year is an interesting data point for plotting against.

Example code includes:  
```{r}

# Integrate the cloud data and convert month to a factor
dfFull <- cbind(dfParse, tblClouds, lowCloud) %>%
    mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                  numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                  TRUE ~ "Error"
                                  ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                  ), 
           month=factor(month, levels=1:12, labels=month.abb)
           )
dfFull <- tibble::as_tibble(dfFull)
str(dfFull)

# Run the boxplot of a factor against a numeric variable
plotFactorNumeric <- function(fctVar, numVar, title=NULL) {
    if (is.null(title)) { title <- paste0("Lincoln, NE (2016) Hourly Weather - ", numVar, " vs. ", fctVar) }
    p <- dfFull %>%
        filter(!is.na(get(fctVar)), !is.na(get(numVar))) %>%
        ggplot(aes_string(x=fctVar, y=numVar)) + 
        geom_boxplot(fill="lightblue") + 
        labs(title=title)
    print(p)
}

# Run for all of the key variables against wind speed and cloud type
keyVar <- c("WindSpeed", "Visibility", "Altimeter", "TempF", "DewF")
for (var in keyVar) { plotFactorNumeric("month", var) }
for (var in keyVar) { plotFactorNumeric("wType", var) }

# Create stacked bars for cloud type by month
dfFull %>%
    filter(!is.na(wType), wType!="Error") %>%
    ggplot(aes(x=month, fill=wType)) + 
    geom_bar(position="fill") + 
    labs(title="Lincoln, NE (2016)", x="", y="Proportion of Month")

```

#### _Example #17: Functional Form - METAR download and initial wind processing_  
Example 12 can be converted to functional form so that the process can be applied to other reporting stations and time periods.

Example code includes:  
```{r}

# Function to make an initial read of the data, filter to METAR records, and check date-times
readMETAR <- function(fileName="./RInputFiles/metar_klnk_2016.txt", timeZ="54Z",
                      expMin=as.POSIXct("2015-12-31 00:54:00", tz="UTC"), expDays=365
                      ) {

    # Load METAR data
    initRead <- readr::read_csv(fileName, na=c("", "NA", "M"))
    str(initRead, give.attr=FALSE)

    # Filter to only data that ends with times ending in 54Z
    filterRead <- initRead %>%
        filter(str_detect(metar, timeZ))
    dim(filterRead)

    # Check that the dates and times included are as expected
    expDate <- expMin + lubridate::hours(0:(24*expDays - 1))
    
    # Observations expected but not recorded
    cat("\n*** OBSERVATIONS EXPECTED BUT NOT RECORDED ***\n")
    print(as.POSIXct(setdiff(expDate, filterRead$valid), origin="1970-01-01", tz="UTC"))

    # Observations recorded but not expected
    cat("\n*** OBSERVATIONS RECORDED BUT NOT EXPECTED ***\n")
    print(as.POSIXct(setdiff(filterRead$valid, expDate), origin="1970-01-01", tz="UTC"))

    # Confirmation of uniqueness
    cat("\n*** Are the extracted records unique? ***\n")
    print(length(unique(filterRead$valid)) == length(filterRead$valid))
    cat("\n")
    
    # Return the dataset as a tibble
    tibble::as_tibble(filterRead)
}
funcMETAR <- readMETAR(expDays=368)
funcMETAR


# Extract wind speeds and direction
# The general wind format is dddssGssKT where ddd is the direction (VRB meaning variable), the main ss is the speed, and the Gss is the gust speed (optional and not always displayed)
extractWind <- function(met) {

    mtxWind <- met %>%
        pull(metar) %>%
        str_match(pattern="(\\d{3}|VRB)(\\d{2})(G\\d{2})?KT")
    cat("\n*** First 6 winds and parsing ***\n")
    print(head(mtxWind))

    cat("\n*** Table of WIND DIRECTION ***\n")
    print(table(mtxWind[, 2], useNA="ifany"))
    cat("\n*** Table of WIND SPEED ***\n")
    print(table(mtxWind[, 3], useNA="ifany"))
    cat("\n*** Table of WIND GUST ***\n")
    print(table(mtxWind[, 4], useNA="ifany"))

    # Verify that winds not captured are in fact missing from the METAR
    cat("\n *** WIND DATA WAS NOT CAPTURED FROM: *** \n")
    print(met[which(is.na(mtxWind[, 2])), "metar"])
    cat("\n")

    met %>%
        mutate(dirW=mtxWind[, 2], 
               spdW=as.numeric(mtxWind[, 3]), 
               gustW=as.numeric(str_replace(mtxWind[, 4], "G", ""))
               )
}
windMETAR <- extractWind(funcMETAR)
windMETAR


# Generate basic wind plots
basicWindPlots <- function(met, desc="Lincoln, NE", gran="KLNK METAR (2016)") {

    # Plot for the wind direction
    wDir <- met %>%
        ggplot(aes(x=dirW)) + 
        geom_bar() + 
        labs(title=paste0(desc, " Wind Direction"), subtitle=gran, 
             y="# Hourly Observations", x="Wind Direction"
             )
    print(wDir)

    # Plot for the minimum, average, and maximum wind speed by wind direction
    # Wind direction 000 is reserved for 0 KT wind, while VRB is reserved for 3-6 KT variable winds
    wSpeedByDir <- met %>%
        filter(!is.na(dirW)) %>%
        group_by(dirW) %>%
        summarize(minWind=min(spdW), meanWind=mean(spdW), maxWind=max(spdW)) %>%
        ggplot(aes(x=dirW)) + 
        geom_point(aes(y=meanWind), color="red", size=2) + 
        geom_errorbar(aes(ymin=minWind, ymax=maxWind)) + 
        labs(title=paste0(desc, " Wind Speed (Max, Mean, Min) By Wind Direction"), subtitle=gran, 
             y="Wind Speed [KT]", x="Wind Direction"
             )
    print(wSpeedByDir)

    # Plot for the wind speed
    pctZero <- sum(met$spdW==0, na.rm=TRUE) / length(met$spdW)
    wSpeed <- met %>%
        ggplot(aes(x=spdW)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        labs(title=paste0(round(100*pctZero), "% of wind speeds in ", desc, " measure 0 Knots"), 
             subtitle=gran, 
             y="% Hourly Observations", x="Wind Speed {KT]"
             )
    print(wSpeed)

    wPolarDirSpeed <- met %>% 
        filter(!is.na(dirW), dirW != "VRB", dirW != "000") %>%
        mutate(dirW=as.numeric(dirW)) %>%
        group_by(dirW, spdW) %>%
        summarize(n=n()) %>%
        ggplot(aes(x=spdW, y=dirW)) + 
        geom_point(alpha=0.1, aes(size=n)) + 
        coord_polar(theta="y") + 
        labs(title=paste0(desc, " Direction vs. Wind Speed"), subtitle=gran, x="Wind Speed [KT]") + 
        scale_y_continuous(limits=c(0, 360), breaks=c(0, 90, 180, 270, 360)) + 
        scale_x_continuous(limits=c(0, 30), breaks=c(0, 5, 10, 15, 20, 25, 30)) + 
        geom_point(aes(x=0, y=0), color="red", size=2)
    print(wPolarDirSpeed)
}
basicWindPlots(windMETAR)

```

#### _Example #18: Functional Form for Extracting Key Elements from METAR_  
METAR parsing can also be converted to a functional form.  This will need to be modified to be more general, since the codes used for a few things like clouds can vary from station to station.
  
Example code includes:  
```{r}

# Code for the initial METAR parsing
initialParseMETAR <- function(met, val, labs) {
    
    # Pull the METAR data
    metAll <- met %>%
        pull(metar)
    
    # Find the number of matching elements
    cat("\n*** Tentative Summary of Element Parsing *** \n")
    str_detect(metAll, pattern=val) %>% 
        table() %>%
        print()

    # The strings that do not match have errors in the raw data (typically, missing wind speed)
    cat("\n*** Data Not Matched *** \n")
    print(metAll[!str_detect(metAll, pattern=val)])

    # A matrix of string matches can be obtained
    mtxParse <- str_match(metAll, pattern=val)
    cat("\n*** Parsing matrix summary *** \n")
    print(dim(mtxParse))
    print(head(mtxParse))

    # Create a data frame
    dfParse <- data.frame(mtxParse, stringsAsFactors=FALSE) %>%
        mutate(dtime=met$valid, origMETAR=met$metar)
    names(dfParse) <- c(labs, "dtime", "origMETAR")
    dfParse <- tibble::as_tibble(dfParse)
    cat("\n*** Summary of the parsed data *** \n")
    glimpse(dfParse)
    
    dfParse
}


# Create a search string for METAR
valMet <- "54Z.*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})"

# Create the names for the search string to parse in to
labsMet <- c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
             "TempC", "DewC", "Altimeter", "SLP", "FahrC"
             )

# Run the METAR parsing on the raw data
initMETAR <- initialParseMETAR(funcMETAR, val=valMet, labs=labsMet)
initMETAR


# Helper function for generating plots by key variables
plotcountsByMetric <- function(df, mets) {
    
    # Plot of counts by key metric
    for (x in mets) {
        p <- df %>%
            group_by_at(vars(all_of(x))) %>%
            summarize(n=n()) %>%
            ggplot(aes_string(x=x, y="n")) + 
            geom_col() + 
            labs(title=x, y="Count")
        print(p)
    }
}


# Code for the conversion of METAR to meaningful numeric
# Should make this much more general later
convertMETAR <- function(met, metrics, seed=2003211416) {
    
    # Convert to numeric where appropriate
    dfParse <- met %>%
        mutate(WindSpeed = as.integer(WindSpeed), 
               WindGust = as.numeric(WindGust), 
               Visibility = as.numeric(str_replace(Visibility, "SM", "")),
               TempC = as.integer(str_replace(TempC, "M", "-")), 
               DewC = as.integer(str_replace(DewC, "M", "-")), 
               Altimeter = as.integer(str_replace(Altimeter, "A", "")), 
               SLP = as.integer(str_replace(SLP, "SLP", "")), 
               TempF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 2, 5), pattern="^1", "-"))/10, 
               DewF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 6, 9), pattern="^1", "-"))/10
               )

    # Investigate the data
    cat("\n *** Parsed data structure, head, tail, and random sample *** \n")
    str(dfParse)
    print(head(dfParse))
    print(tail(dfParse))
    set.seed(seed)
    dfParse %>% 
        sample_n(20) %>%
        print()

    # Check for NA values
    cat("\n *** Number of NA values *** \n")
    print(colSums(is.na(dfParse)))

    # Plot of counts by key metric
    plotcountsByMetric(dfParse, mets=metrics)
    
    # Return the parsed dataset
    dfParse
}

keyMetric <- c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
               "DewC", "Altimeter", "SLP", "TempF", "DewF"
               )
convMETAR <- convertMETAR(initMETAR, metrics=keyMetric)


# There are three obvious issues
# Visibility is not correctly picked up when there is a / such as 1/2 SM
# Wind gusts are never picked up
# Sea Level Pressure is missing a digit

# Address the visibility issues
getVisibility <- function(curMet, origMet) {
    
    # Get the original METAR data
    metAll <- origMet %>%
        pull(metar)
    
    # Correct for visibility
    # Areas that have \\d \\d/\\dSM
    sm1 <- which(str_detect(metAll, pattern=" \\d/\\dSM"))
    sm2 <- which(str_detect(metAll, pattern=" \\d \\d/\\dSM"))

    valSM1 <- str_match(metAll, pattern="\\d/\\dSM")[sm1]
    valSM1 <- str_replace(valSM1, "SM", "")
    valSM1 <- as.integer(str_sub(valSM1, 1, 1)) / as.integer(str_sub(valSM1, 3, 3))

    valSM2 <- str_match(metAll, pattern=" \\d \\d/\\dSM")[sm2]
    valSM2 <- as.integer(str_sub(valSM2, 2, 2))

    curMet[sm1, "Visibility"] <- valSM1
    curMet[sm2, "Visibility"] <- curMet[sm2, "Visibility"] + valSM2

    curMet %>% 
        count(Visibility) %>%
        print()
    
    curMet
}
parseMETAR <- getVisibility(convMETAR, origMet=funcMETAR)


# Correct for wind gusts
getWindGusts <- function(curMet, origMet) {

    metAll <- origMet %>%
        pull(metar)
    
    gustCheck <- which(str_detect(metAll, pattern="\\d{5}G\\d{2}KT"))
    valGust <- str_match(metAll, pattern="\\d{5}G\\d{2}KT")[gustCheck]
    valGust <- as.integer(str_sub(valGust, 7, 8))

    curMet[gustCheck, "WindGust"] <- valGust

    curMet %>% 
        count(WindGust) %>% 
        as.data.frame %>%
        print()
    
    curMet
}
parseMETAR <- getWindGusts(parseMETAR, origMet=funcMETAR)


# Correct for SLP
fixSLP <- function(curMet) {

    dfParse <- curMet %>%
        mutate(modSLP=ifelse(curMet$SLP < 500, 1000 + curMet$SLP/10, 900 + curMet$SLP/10))

    p <- dfParse %>%
        group_by(SLP, modSLP) %>%
        summarize(n=n()) %>%
        ggplot(aes(x=SLP, y=modSLP, size=n)) + 
        geom_point(alpha=0.3)
    print(p)
    
    dfParse
}
parseMETAR <- fixSLP(parseMETAR)


# Check updated plots
plotcountsByMetric(parseMETAR, mets=c("WindGust", "Visibility", "modSLP"))

```

#### _Example #19: Functional Form For Relationships Between METAR Variables_  
The relationships between METAR variables can also be converted to functional form.

Example code includes:  
```{r}

# Function to calculate, display, and plot variable correlations
corMETAR <- function(met, numVars, subT="") {

    # Keep only complete cases and report on data kept
    dfUse <- met %>%
        select_at(vars(all_of(numVars))) %>%
        filter(complete.cases(.))
    
    nU <- nrow(dfUse)
    nM <- nrow(met)
    myPct <- round(100*nU/nM, 1)
    cat("\n *** Correlations use ", nU, " complete cases (", myPct, "% of ", nM, " total) ***\n", sep="")
    
    # Create the correlation matrix
    mtxCorr <- dfUse %>%
        cor()

    # Print the correlations
    mtxCorr %>%
        round(2) %>%
        print()

    # Display a heat map
    corrplot::corrplot(mtxCorr, method="color", title=paste0("Hourly Weather Correlations\n", subT))
}

# Define key numeric variables
coreNum <- c("TempC", "TempF", "DewC", "DewF", "Altimeter", "modSLP", "WindSpeed", "Visibility")

# Run the correlations function
corMETAR(parseMETAR, numVars=coreNum)


# Create a function for plotting two variables against each other
plotNumCor <- function(met, var1, var2, title=NULL, subT="") {
    if (is.null(title)) 
        { title <- paste0("Hourly Correlations of ", var1, " and ", var2) }
    p <- met %>%
        group_by_at(vars(all_of(c(var1, var2)))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=var1, y=var2)) + 
        geom_point(alpha=0.5, aes_string(size="n")) + 
        geom_smooth(method="lm", aes_string(weight="n")) + 
        labs(x=var1, y=var2, title=title, subtitle=subT)
    print(p)
}

var1List <- c("TempC", "DewC", "Altimeter", "TempF", "TempF",     "TempF",  "Altimeter")
var2List <- c("TempF", "DewF", "modSLP",    "DewF",  "Altimeter", "modSLP", "WindSpeed")

for (n in 1:length(var1List)) {
    plotNumCor(parseMETAR, var1List[n], var2List[n])
}


# Function for linear regressions on METAR data
lmMETAR <- function(met, y, x, yName, subT="Lincoln, NE (2016) Hourly METAR") {
    
    # Convert to formula
    myChar <- paste0(y, " ~ ", x)
    cat("\n *** Regression call is:", myChar, "***\n")
    
    # Run regression
    regr <- lm(formula(myChar), data=met)
    
    # Summarize regression
    print(summary(regr))
    
    # Predict the new values
    pred <- predict(regr, newdata=met)
    
    # Plot the predictions
    p <- met %>%
        select_at(vars(all_of(y))) %>%
        mutate(pred=pred) %>%
        group_by_at(vars(all_of(c(y, "pred")))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=y, y="pred")) + 
        geom_point(aes(size=n), alpha=0.25) + 
        geom_smooth(aes(weight=n), method="lm") + 
        labs(title=paste0("Predicted vs. Actual ", yName, " - ", x, " as Predictor"), 
             subtitle=subT, x=paste0("Actual ", yName), y=paste0("Predicted ", yName)
             )
    print(p)
}

lmMETAR(parseMETAR, "modSLP", "Altimeter", yName="Sea Level Pressure")
lmMETAR(parseMETAR, "modSLP", "Altimeter + TempF", yName="Sea Level Pressure")

```

#### _Example #20: Functional Form for Extracting Cloud Data from METAR_  
Cloud data can also be extracted using the functional form.

Example code includes:  
```{r}

extractClouds <- function(met, metVar, subT="Lincoln, NE (2016) Hourly METAR") {

    metAll <- met %>%
        pull(metVar)
    
    # Extract the CLR records
    mtxCLR <- str_extract_all(metAll, pattern=" CLR ", simplify=TRUE)
    if (dim(mtxCLR)[[2]] != 1) { stop("Extracted 2+ CLR from some METAR; investigate") }
    isCLR <- ifelse(mtxCLR[, 1] == "", 0, 1)

    # Extract the VV records
    mtxVV <- str_extract_all(metAll, pattern="VV(\\d{3})", simplify=TRUE)
    if (dim(mtxVV)[[2]] > 1) { stop("Extracted 2+ VV from some METAR; investigate") }
    if ((dim(mtxVV))[[2]] == 0) {
        cat("\nNo Records with a cloud type of vertical visibility (VV)\n")
        isVV <- rep(0, times=length(isCLR))
        htVV <- rep(NA, times=length(isCLR))
    } else {
        isVV <- ifelse(mtxVV[, 1] == "", 0, 1)
        htVV <- ifelse(mtxVV[, 1] == "", NA, as.integer(str_replace(mtxVV[, 1], "VV", ""))*100)
    }

    # Extract the FEW records
    mtxFEW <- str_extract_all(metAll, pattern="FEW(\\d{3})", simplify=TRUE)
    numFEW <- apply(mtxFEW, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the SCT records
    mtxSCT <- str_extract_all(metAll, pattern="SCT(\\d{3})", simplify=TRUE)
    numSCT <- apply(mtxSCT, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the BKN records
    mtxBKN <- str_extract_all(metAll, pattern="BKN(\\d{3})", simplify=TRUE)
    numBKN <- apply(mtxBKN, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the OVC records
    mtxOVC <- str_extract_all(metAll, pattern="OVC(\\d{3})", simplify=TRUE)
    numOVC <- apply(mtxOVC, 1, FUN=function(x) { sum((x!=""))} )

    # Summarize as a data frame
    tblClouds <- tibble::tibble(isCLR=isCLR, isVV=isVV, htVV=htVV, numFEW=numFEW, 
                                numSCT=numSCT, numBKN=numBKN, numOVC=numOVC
                                )

    # Get the counts
    cat("\n*** Counts by number of layers of each cloud type ***\n")
    tblClouds %>% 
        count(isCLR, isVV, numFEW, numSCT, numBKN, numOVC) %>%
        as.data.frame() %>%
        print()

    # Investigate the problem data
    cat("\n*** METAR records where no clouds were extracted ***\n")
    metAll[rowSums(tblClouds, na.rm=TRUE)==0] %>%
        print()
    
    # Plot the counts of most obscuration
    p <- tblClouds %>%
        filter(rowSums(., na.rm=TRUE) > 0) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            )
               ) %>%
        ggplot(aes(x=wType, y=..count../sum(..count..))) + 
        geom_bar() + 
        labs(title="Highest Obscuration by Cloud", subtitle=subT, 
             x="Cloud Type", y="Proportion of Hourly Measurements"
             )
    print(p)
    
    # Integrate the clouds data
    mtxCloud <- cbind(mtxVV, mtxOVC, mtxBKN, mtxSCT, mtxFEW, mtxCLR)
    cat("\n*** Dimensions for the cloud matrix ***\n")
    print(dim(mtxCloud))
    
    list(tblClouds=tblClouds, mtxCloud=mtxCloud)
}

# Run the initial cloud extraction
initClouds <- extractClouds(parseMETAR, metVar="origMETAR")
str(initClouds)


# Cycle through to find levels of a given type
ckClouds <- function(cloudType, mtx) {
    isKey <- which(apply(mtx, 2, FUN=function(x) {sum(str_detect(x, cloudType))}) > 0)
    as.integer(str_replace(mtx[, min(isKey)], cloudType, "")) * 100
}


# Function to create the lowest cloud levels
findLowestClouds <- function(mtxCloud, subT="Lincoln, NE (2016) Hourly METAR") {

    # Find the lowest clouds by cloud type
    lowOVC <- ckClouds("OVC", mtx=mtxCloud)
    lowVV <- ckClouds("VV", mtx=mtxCloud)
    lowBKN <- ckClouds("BKN", mtx=mtxCloud)
    lowSCT <- ckClouds("SCT", mtx=mtxCloud)
    lowFEW <- ckClouds("FEW", mtx=mtxCloud)

    # Integrate the lowest cloud type by level
    lowCloud <- tibble::tibble(lowVV, lowOVC, lowBKN, lowSCT, lowFEW)
    cat("\n*** Lowest clouds by type tibble ***\n")
    print(lowCloud)

    # Get the lowest cloud level
    minCloud <- lowCloud
    minCloud[is.na(minCloud)] <- 999999
    minCloudLevel <- apply(minCloud, 1, FUN=min)
    minCeilingLevel <- apply(minCloud[, c("lowVV", "lowOVC", "lowBKN")], 1, FUN=min)

    noCloudPct <- mean(minCloudLevel == 999999)
    noCeilingPct <- mean(minCeilingLevel == 999999)

    # Plot the minimum cloud level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCloudLevel != 999999) %>%
        ggplot(aes(x=minCloudLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCloudPct), "% of obs. have no clouds")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Cloud Height (when some clouds exist)", subtitle=subT
             )
    print(p)

    # Plot the minimum ceiling level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCeilingLevel != 999999) %>%
        ggplot(aes(x=minCeilingLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCeilingPct), "% of obs. have no ceiling")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Ceiling Height (when a ceiling exists)", subtitle=subT
             )
    print(p)
    
    list(lowCloud=lowCloud, minCeilingLevel=minCeilingLevel, minCloudLevel=minCloudLevel)
}

processedClouds <-findLowestClouds(initClouds$mtxCloud)
str(processedClouds)

```

#### _Example #21: Functional Form for Plotting by factor variables_  
The month of the year is an interesting data point for plotting against.

Example code includes:  
```{r}

# Function to bind the existing parsed METAR data with the cloud data
bindMETAR <- function(dfParse, tblClouds, lowCloud) {

    # Integrate the cloud data and convert month to a factor
    dfFull <- cbind(dfParse, tblClouds, lowCloud) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            ), 
               month=factor(lubridate::month(dtime), levels=1:12, labels=month.abb)
               )
    
    dfFull <- tibble::as_tibble(dfFull)
    str(dfFull)
    
    dfFull
}

fullMETAR <- bindMETAR(dfParse=parseMETAR, 
                       tblClouds=initClouds$tblClouds, 
                       lowCloud=processedClouds$lowCloud
                       )


# Updated function for plotting numeric by factor
plotFactorNumeric <- function(met, fctVar, numVar, title=NULL, subT) {
    if (is.null(title)) { title <- paste0("Hourly Weather - ", numVar, " vs. ", fctVar) }
    p <- met %>%
        filter(!is.na(get(fctVar)), !is.na(get(numVar))) %>%
        ggplot(aes_string(x=fctVar, y=numVar)) + 
        geom_boxplot(fill="lightblue") + 
        labs(title=title, subtitle=subT)
    print(p)
}

# Function for creating cloud plots
makeFactorPlots <- function(met, 
                            fctVar=c("month", "wType"), 
                            keyVar=c("WindSpeed", "Visibility", "Altimeter", "TempF", "DewF"), 
                            desc="Lincoln, NE (2016) Hourly METAR"
                            ) {

    # Run for all of the key variables against wind speed and cloud type
    for (varF in fctVar) {
        for (varK in keyVar) { 
            plotFactorNumeric(met, fctVar=varF, numVar=varK, subT=desc) 
        }
    }

    # Create stacked bars for cloud type by month
    # dfFull %>%
    #     filter(!is.na(wType), wType!="Error") %>%
    #     ggplot(aes(x=month, fill=wType)) + 
    #     geom_bar(position="fill") + 
    #     labs(title="Lincoln, NE (2016)", x="", y="Proportion of Month")
}

makeFactorPlots(fullMETAR)

```

#### _Example #22: Combining Functional Forms for METAR Processing_  
The functions can be combined in to a single routine for reading, parsing, and running EDA on METAR data..

Example code includes:  
```{r}

# Function to run the full process
runAllMETAR <- function(fname, timeZ, expMin, expDays, locMET, shortMET, longMET, valMet, 
                        labsMet=c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
                                  "TempC", "DewC", "Altimeter", "SLP", "FahrC"
                                  ), 
                        keyMetric=c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
                                    "DewC", "Altimeter", "SLP", "TempF", "DewF"
                                    ), 
                        coreNum=c("TempC", "TempF", "DewC", "DewF", 
                                  "Altimeter", "modSLP", "WindSpeed", "Visibility"
                                  ), 
                        var1List=c("TempC", "DewC", "Altimeter", "TempF", "TempF", "TempF", "Altimeter"), 
                        var2List=c("TempF", "DewF", "modSLP", "DewF", "Altimeter", "modSLP", "WindSpeed")
                        ) {
    
    # Read in the METAR data
    funcMETAR <- readMETAR(fileName=fname, timeZ=timeZ, expMin=expMin, expDays=expDays)
    # funcMETAR

    # Extract wind data from METAR
    windMETAR <- extractWind(funcMETAR)
    # windMETAR

    # Run basic wind plots
    basicWindPlots(windMETAR, desc=locMET, gran=shortMET)


    # Run the METAR parsing on the raw data
    initMETAR <- initialParseMETAR(funcMETAR, val=valMet, labs=labsMet)
    # initMETAR

    # Parse and convert the METAR data
    convMETAR <- convertMETAR(initMETAR, metrics=keyMetric)
    # convMETAR

    # Fix problems with visibility, wind gusts, and SLP
    parseMETAR <- getVisibility(convMETAR, origMet=funcMETAR)
    parseMETAR <- getWindGusts(parseMETAR, origMet=funcMETAR)
    parseMETAR <- fixSLP(parseMETAR)

    # Check updated plots
    plotcountsByMetric(parseMETAR, mets=c("WindGust", "Visibility", "modSLP"))


    # Run the correlations function
    corMETAR(parseMETAR, numVars=coreNum, subT=longMET)

    # Plot correlations
    for (n in 1:length(var1List)) {
        plotNumCor(parseMETAR, var1List[n], var2List[n], subT=longMET)
    }

    # Run lm models for SLP vs Altimeter and (optionally) Temperature
    lmMETAR(parseMETAR, "modSLP", "Altimeter", yName="Sea Level Pressure", subT=longMET)
    lmMETAR(parseMETAR, "modSLP", "Altimeter + TempF", yName="Sea Level Pressure", subT=longMET)


    # Run the initial cloud extraction
    initClouds <- extractClouds(parseMETAR, metVar="origMETAR", subT=longMET)
    str(initClouds)

    # Find the lowest cloud levels and lowest ceilings
    processedClouds <-findLowestClouds(initClouds$mtxCloud, subT=longMET)
    str(processedClouds)


    # Bind the processed METAR and the cloud data
    fullMETAR <- bindMETAR(dfParse=parseMETAR, 
                           tblClouds=initClouds$tblClouds, 
                           lowCloud=processedClouds$lowCloud
                           )

    # Create box plots for key weather elements against month and cloud type
    makeFactorPlots(fullMETAR, desc=longMET)
    
    # Return all of the elements
    list(fullMETAR=fullMETAR, funcMETAR=funcMETAR, windMETAR=windMETAR, 
         initMETAR=initMETAR, convMETAR=convMETAR, parseMETAR=parseMETAR, 
         initClouds=initClouds, processedClouds=processedClouds
         )
}
```
  
Followed by caching the run of the function for Lincoln, NE:  
```{r cache=TRUE}
# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_klnk_2016.txt"  # file name for raw METAR data
timeZ <- "54Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:54:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Lincoln, NE"  # Description of city or location
shortMET <- "KLNK METAR (2016)"  # Station code and timing
longMET <- "Lincoln, NE Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Lincoln, NE
klnk2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(klnk2016METAR)

```

#### _Example #23: Running for a Different Station_  
The functions can be run for a different station.

Example code includes:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kord_2016.txt"  # file name for raw METAR data
timeZ <- "51Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:51:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Chicago, IL"  # Description of city or location
shortMET <- "KORD METAR (2016)"  # Station code and timing
longMET <- "Chicago, IL O'Hare Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Lincoln, NE
kord2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kord2016METAR)

```

#### _Example #24: Downloading the Data by Function_  
The [Iowa State Mesonet website](https://mesonet.agron.iastate.edu/request/download.phtml) provides instructions for automating the download process using Python or R, including links to a GitHub repository with instructions for [R download](https://github.com/realmiketalbot/R-scripts/blob/master/iem_scraper_example.r).  The code can be adapted for use here.

Example code includes:  
```{r}

# Function to get ASOS data from Iowa State
getASOSData <- function(faaID, startDate, endDate, suffix,
                        dirDownload = "./RInputFiles/", getAgain=FALSE) {

    # Create the file name and location for saving data
    fileName <- paste0("metar_k", str_to_lower(faaID), "_", suffix, ".txt")
    fileLoc <- paste0(dirDownload, fileName)

    # Check whether the file already exists, stop if so and getAgain is FALSE, otherwise download the data
    if (file.exists(fileLoc) & !getAgain) {
        cat("\nFile already exists - ", fileLoc)
        cat("\nStopping download routine\n")
        return(FALSE)
    } else {
        # Get the year, day, and hour of the key dates
        y1 <- lubridate::year(startDate)
        m1 <- lubridate::month(startDate)
        d1 <- lubridate::day(startDate)
    
        y2 <- lubridate::year(endDate)
        m2 <- lubridate::month(endDate)
        d2 <- lubridate::day(endDate)

        # Mimic the string shown below
        # https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LAS&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2
    
        baseURL <- "https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?"  # base URL
        useURL <- paste0(baseURL, "station=", faaID)  # add the desired station
        useURL <- paste0(useURL, "&data=all")  # request all data fields
        useURL <- paste0(useURL, "&year1=", y1, "&month1=", m1, "&day1=", d1)  # Specify start ymd
        useURL <- paste0(useURL, "&year2=", y2, "&month2=", m2, "&day2=", d2)  # Specify end ymd
        useURL <- paste0(useURL, "&tz=Etc%2FUTC&format=onlycomma&latlon=no")  # Formatting
        useURL <- paste0(useURL, "&missing=M&trace=T&direct=no&report_type=2")  # Formatting
    
        # Download the file
        download.file(useURL, destfile=fileLoc, method="curl")
        
        return(TRUE)
    }
}

```

And then cache the actual download step to minimize utilization of the Iowa State server (though the code is set to not download to an existing file anyway):
```{r cache=TRUE}

# Specify the FAA ID and Analysis Year
useFAAID <- "LAS"
analysisYear <- 2016

# Specify the start and end dates based on the analysis year
startDate <- ISOdate(analysisYear-1, 12, 31, hour=0)
endDate <- ISOdate(analysisYear+1, 1, 2, hour=0)

# Get the relevant data
getASOSData(faaID=useFAAID, startDate=startDate, endDate=endDate, suffix=analysisYear)

```

And then explore the data to set the key parameters for a full run of the METAR process:
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR based on above inputs
fname <- paste0("./RInputFiles/metar_k", str_to_lower(useFAAID), "_", analysisYear, ".txt")

# Find the most common Zulu time (this will be the METAR)
zTimes <- readr::read_csv(fname) %>%
    pull(metar) %>%
    str_match(pattern="\\d{2}Z") %>%
    as.vector() %>%
    table() %>%
    sort(decreasing=TRUE)

cat("\nThe most common Zulu time is", names(zTimes)[1], "\nFrequency is",
    round(100*zTimes[1]/sum(zTimes), 1), "% (", zTimes[1], "of", sum(zTimes), ")"
    )

# Use the most common Zulu time and start/end dates to set key parameters
timeZ <- names(zTimes)[1]  # Zulu time that METAR is recorded at this station
expMin <- ISOdate(analysisYear-1, 12, 31, hour=0, min=as.integer(str_replace(timeZ, "Z", "")))
expDays <- as.integer(endDate - startDate)  # Expected total days read

# Provide a descriptive name
locMET <- "Las Vegas, NV"  # Description of city or location
shortMET <- "KLAS METAR (2016)"  # Station code and timing
longMET <- "Las Vegas, NV Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Lincoln, NE
klas2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(klas2016METAR)

```

#### _Example #25: More Generic Downloading the Data by Function_  
The download function can be more generic, with just a few key arguments passed to it.  Those arguments can then be derived in a helper function, with the generic function called from inside that.

Example code includes:  
```{r}

genericGetASOSData <- function(fileLoc, 
                               stationID,
                               startDate, 
                               endDate,
                               downloadMethod="curl", 
                               baseURL="https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?", 
                               dataFields="all", 
                               dataTZ="Etc%2FUTC", 
                               dataFormat="onlycomma", 
                               dataLatLon="no", 
                               dataMissing="M", 
                               dataTrace="T", 
                               dataDirect="no", 
                               dataType=2
                               ) {
    
    # Get the year, day, and hour of the key dates
    y1 <- lubridate::year(startDate)
    m1 <- lubridate::month(startDate)
    d1 <- lubridate::day(startDate)
    
    y2 <- lubridate::year(endDate)
    m2 <- lubridate::month(endDate)
    d2 <- lubridate::day(endDate)

    # Mimic the string shown below
    # https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LAS&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2
    
    useURL <- paste0(baseURL, "station=", stationID)  # add the desired station
    useURL <- paste0(useURL, "&data=", dataFields)  # default is "all
    useURL <- paste0(useURL, "&year1=", y1, "&month1=", m1, "&day1=", d1)  # Specify start ymd
    useURL <- paste0(useURL, "&year2=", y2, "&month2=", m2, "&day2=", d2)  # Specify end ymd
    useURL <- paste0(useURL, "&tz=", dataTZ)  # time zone (default UTC)
    useURL <- paste0(useURL, "&format=", dataFormat)  # file format (default CSV)
    useURL <- paste0(useURL, "&latlon=", dataLatLon)  # Whether to include lat-lon (default no)
    useURL <- paste0(useURL, "&missing=", dataMissing)  # How to handle missing data (default is 'M')
    useURL <- paste0(useURL, "&trace=", dataTrace)  # How to handle trace data (default is 'T')
    useURL <- paste0(useURL, "&direct=", dataDirect)  # Whether to directly get the data (default is 'no')
    useURL <- paste0(useURL, "&report_type=", dataType)  # Whether to get just METAR (2, default)
    
    # Download the file
    cat("\nDownloading from:", useURL, "\nDownloading to:", fileLoc, "\n")
    download.file(useURL, destfile=fileLoc, method=downloadMethod)
        
    return(TRUE)
}

```
  
Which can then be called by a function that checks whether the file already exists:
```{r}

getASOSStationTime <- function(stationID, 
                               startDate=NULL, 
                               endDate=NULL, 
                               analysisYears=NULL, 
                               fileLoc=NULL,
                               ovrWrite=FALSE,
                               ...) {
    
    # Get the relevant time period for the data
    if (is.null(analysisYears) & (is.null(startDate) | is.null(endDate))) {
        stop("Must provide either analysisYears or both of startDate and endDate")
    }
    if (!is.null(startDate) & !is.null(endDate) & !is.null(analysisYears)) {
        stop("Should specify EITHER both of startDate and endDate OR analysisYears BUT NOT both")
    }
    if (is.null(startDate)) {
        startDate <- ISOdate(min(analysisYears)-1, 12, 31, hour=0)
        endDate <- ISOdate(max(analysisYears)+1, 1, 2, hour=0)
    }
    
    # Create the file name
    if (!is.null(analysisYears)) {
        if (length(analysisYears) == 1) { timeDesc <- analysisYears }
        else { timeDesc <- paste0(min(analysisYears), "-", max(analysisYears)) }
    } else {
        timeDesc <- paste0(lubridate::year(startDate), 
                           str_pad(lubridate::month(startDate), 2, pad="0"),
                           str_pad(lubridate::day(startDate), 2, pad="0"), 
                           "-", 
                           lubridate::year(endDate), 
                           str_pad(lubridate::month(endDate), 2, pad="0"), 
                           str_pad(lubridate::day(endDate), 2, pad="0")
                           )
    }
    
    if (is.null(fileLoc)) {
        fileLoc <- paste0("./RInputFiles/", "metar_k", str_to_lower(stationID), "_", timeDesc, ".txt")
    }
    
    cat("\nData for station", stationID, "from", as.character(startDate), "to", 
        as.character(endDate), "will download to", fileLoc, "\n"
        )
    
    if (file.exists(fileLoc) & !ovrWrite) {
        stop("File already exists, aborting")
    }
    
    genericGetASOSData(fileLoc=fileLoc, stationID=stationID, startDate=startDate, endDate=endDate, ...)
    
}

```
  
And the files can then be run (cached to avoid multiple hits to the Iowa State server):  
```{r cache=TRUE}

# Get data for EWR for 2016
getASOSStationTime(stationID="EWR", analysisYears=2016)

# Get data for ATL for 2016-2018
getASOSStationTime(stationID="ATL", analysisYears=2016:2018)

# Get data for DFW for 2016-03-31 to 2017-03-01
getASOSStationTime(stationID="DFW", 
                   startDate=ISOdate(2016, 03, 31, hour=0), 
                   endDate=ISOdate(2017, 3, 1, hour=0)
                   )

```

#### _Example #26: Comparative Wind Directions by Location_  
The processed data files can be compared, with wind directions being one of the comparison sets.

Example code includes:  
```{r}

# Extract the wind direction data from a processed METAR file
getWindDirGroup <- function(keyList, src) {
    
    # Use the fullMETAR data and extract WindDir, WindSpeed, month
    windPlotData <- get(keyList)[["fullMETAR"]] %>%
        select(WindDir, WindSpeed, month) %>%
        mutate(windDirGroup=factor(case_when(WindSpeed==0 ~ "No Wind", 
                                             WindDir=="VRB" ~ "Variable", 
                                             WindDir %in% c("030", "040", "050", "060") ~ "NE", 
                                             WindDir %in% c("070", "080", "090", "100", "110") ~ "E", 
                                             WindDir %in% c("120", "130", "140", "150") ~ "SE", 
                                             WindDir %in% c("160", "170", "180", "190", "200") ~ "S", 
                                             WindDir %in% c("210", "220", "230", "240") ~ "SW", 
                                             WindDir %in% c("250", "260", "270", "280", "290") ~ "W", 
                                             WindDir %in% c("300", "310", "320", "330") ~ "NW", 
                                             WindDir %in% c("340", "350", "360", "010", "020") ~ "N", 
                                            TRUE ~ "Error"
                                            ) , levels=c("No Wind", "Variable", "Error", 
                                                         "N", "NE", "E", "SE", "S", "SW", "W", "NW"
                                                         )
                                   )
               )
    
    # Rempve the errors and calculate percentages by month for the remainder
    processedWindData <- windPlotData %>%
        filter(windDirGroup != "Error") %>%
        group_by(month, windDirGroup) %>%
        summarize(n=n()) %>%
        ungroup() %>%
        group_by(month) %>%
        mutate(pct=n/sum(n)) %>%
        ungroup() %>%
        mutate(src=src)
    
    processedWindData

}


# Consolidate and plot wind data
consolidatePlotWind <- function(files, names) {

    consFun <- function(x, y) { getWindDirGroup(keyList=x, src=y) }
    boundByRows <- map2_dfr(.x=files, .y=names, .f=consFun)

    p <- boundByRows %>%
        ggplot(aes(x=month, y=pct, color=src)) + 
        geom_line(aes(group=src)) + 
        facet_wrap(~windDirGroup) + 
        labs(title="Wind Direction Frequency by Month", x="Month", y="Frequency of Wind Observations") +
        theme(axis.text.x=element_text(angle=90))
    print(p)
    
    boundByRows
}

# Run for 2016 with Lincoln, NE; Las Vegas, NV; and Chicago, IL
cpWind <- consolidatePlotWind(files=c("klnk2016METAR", "klas2016METAR", "kord2016METAR"), 
                              names=c("Lincoln, NE (2016)", "Las Vegas, NV (2016)", "Chicago, IL (2016)")
                              )
cpWind

```

#### _Example #27: Extracting Precipitation Information from METAR_  
METAR data include descriptions of the precipitation occuring at any given time.  Two of the most common precipitation forms are rain (RA) and SN().  These can occur together, denoted as RASN or SNRA in the METAR.

Further, the precipitation type can be classified using a prefix as light (-), moderate (no prefix), or heavy (+).  So, RA would be moderate rain, -SNRA would be a light snow-rain mix, +RA would be heavy rain.

Additionally, the timing of the precipitation event is captured in the remarks using B (begin) and E (end).  So, an hourly METAR of RAB20E35B50 would mean rain started at 20 past the hour, ended at 35 past the hour, and began again at 50 past the hour.  Since METAR are often taken just before the top of the hour, a four-digit time is used if it is in the 'previous' hour; for example, RAB1959E36 in the 2053Z METAR.

We can use the remarks to see when it was raining in the given METAR.

Example code includes:  
```{r}

# Extract the METAR and the date-time from a processed list
procMET <- klas2016METAR$fullMETAR %>% 
    select(origMETAR, dtime)

# Check whether there are comments for either rain regins (RAB) or rain ends (RAE) and pull all the data
procMET <- procMET %>% 
    mutate(isRain=grepl("RA[B|E]", origMETAR), 
           rainData=str_extract(origMETAR, pattern="(RA[B|E]\\d+[0-9BE]*)"), 
           nBegin=pmax(0, str_count(rainData, "B"), na.rm=TRUE), 
           nEnd=pmax(0, str_count(rainData, "E"), na.rm=TRUE), 
           dateUTC=lubridate::date(dtime), 
           hourUTC=lubridate::hour(dtime)
           )
str(procMET)

# Check the counts of rain beginning and rain ending
procMET %>%
    count(isRain, nBegin, nEnd)

# Extract all the times when rain began (can be multiple per METAR)
rainMET <- procMET %>%
    select(dateUTC, hourUTC, rainData, nBegin, nEnd)

# Confirm the rainMET is unique by dateUTC and hourUTC
dupAns <- rainMET %>%
    select(dateUTC, hourUTC) %>%
    duplicated() %>%
    any()
cat("\nAre there any problems with duplicated keys?", dupAns, "\n")

# Extract the matrix of rain beginning data
rainBegin <- rainMET %>% 
    pull(rainData) %>% 
    str_extract_all("B\\d+", simplify=TRUE) %>%
    as.data.frame(stringsAsFactors=FALSE)

# Extract the matrix of rain ending data
rainEnd <- rainMET %>% 
    pull(rainData) %>% 
    str_extract_all("E\\d+", simplify=TRUE) %>%
    as.data.frame(stringsAsFactors=FALSE)

# Convert the rain begins data to the hour and minute associated to the UTC
extractTime <- function(x, var, sym="B") {
    if (is.na(x[var]) | x[var]=="") {
        utcUse <- NA
    }
    else {
        utcUse <- str_replace(x[var], sym, "")
        if (str_length(utcUse)==4) {
            utcUse <- paste0(x["dateUTC"], " ", utcUse)
        } else if (str_length(utcUse)==2) {
            utcUse <- paste0(x["dateUTC"], " ", x["hourChar"], utcUse)
        } else {
            cat("\nCannot parse data: ", x, "\n", x[var], "\n", var, sym, utcUse)
            stop()
        }
    }
}

# Extract the begin times from V1
beginTime1 <- rainBegin %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V1", sym="B")

# Extract the begin times from V2
beginTime2 <- rainBegin %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V2", sym="B")

# Extract the end times from V1
endTime1 <- rainEnd %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V1", sym="E")

# Extract the end times from V2
endTime2 <- rainEnd %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V2", sym="E")

allBegins <- 
    c(beginTime1[!is.na(beginTime1)], beginTime2[!is.na(beginTime2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

allEnds <- 
    c(endTime1[!is.na(endTime1)], endTime2[!is.na(endTime2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

# Check the each ending is after its associated beginning
endMinusBeg <- allEnds - allBegins

# Rain is listed as ending at 2016-05-01 00:42 and 2016-04-30 17:38 both for start 2016-04-30 11:20
endMinusBeg <- allEnds[allEnds != lubridate::ymd_hm("2016-05-01 0042")] - allBegins

# Plot the rain durations in minutes
data.frame(minutesRain=as.numeric(endMinusBeg), month=lubridate::month(allBegins)) %>%
    ggplot(aes(x=minutesRain)) +
    geom_histogram()

# Plot the rain totals (in hours) by month
data.frame(minutesRain=as.numeric(endMinusBeg), month=lubridate::month(allBegins)) %>%
    group_by(month) %>%
    summarize(minutesRain=sum(minutesRain), nRain=n()) %>%
    ggplot(aes(x=factor(month.abb[month], levels=month.abb[1:12]), y=minutesRain/60)) +
    geom_col() + 
    labs(title="Las Vegas, NV Rainfall (hours) in 2016", y="Hours of Rain", x="Month") + 
    geom_text(aes(y=2+minutesRain/60, label=round(minutesRain/60, 1)))

```

#### _Example #28: Function for Extracting Precipitation Information from METAR_  
The precipitation extraction process can be converted to functions.  In addition to being more modular, several additional features can be included:

* Allow to search for other precipitation types, specifically snow (SN) or drizzle (DZ)  
* Correct for the isue that a time of 2016-01-04 2359Z is placed in the 206-01-05 bucket by the code above  
* Identify areas where the intervals are not sensible or raise significant questions (negative durations, durations longer than 24 hours)  
* Check that intervals overlap with the time periods in the METAR that show that precipitation event  
  
Example code includes:  

* Extract the precipitation data from a processed METAR file  
* Find the begin and end times from this file  
* Check for intervals of questionable length and correct as needed  
  
```{r}

# Extract the precipitation data from a processed METAR file
extractPrecipData <- function(processedFile, pType="RA") {

    # Extract the METAR and the date-time from a processed list
    procMET <- processedFile[["fullMETAR"]] %>% 
        select(origMETAR, dtime)

    # Check whether there are comments for the desired pType either beginning or ending
    keyPattern <- paste0("(", pType, "[B|E]\\d+[0-9BE]*)")
    cat("\nRegex search code is:", keyPattern, "\n\n")
    
    procMET <- procMET %>% 
        mutate(precipData=str_extract(origMETAR, pattern=keyPattern), 
               isPrecip=grepl(paste0(pType, "[B|E]"), origMETAR, perl=TRUE), 
               nBegin=pmax(0, str_count(precipData, "B"), na.rm=TRUE), 
               nEnd=pmax(0, str_count(precipData, "E"), na.rm=TRUE), 
               dateUTC=lubridate::date(dtime), 
               hourUTC=lubridate::hour(dtime)
               )
    str(procMET)
    cat("\n")

    # Check the counts of precipitation beginning and rain ending
    procMET %>%
        count(isPrecip, nBegin, nEnd) %>%
        print()

    # Check that the file is unique by time
    dupAns <- procMET %>%
        select(dateUTC, hourUTC) %>%
        duplicated %>%
        any()

    cat("\nAre there any problems with duplicated keys?", dupAns, "\n")
    
    procMET
}

testFileProc <-extractPrecipData(klas2016METAR, pType="RA")

```
  
Next, a function for extrating beginning and ending times can be written:  
```{r}

# Helper function to extract the beginning and ending times using str_extract_all
getBeginEndTimeMatrix <- function(file, pullVar="precipData", pState="B") {
    file %>%
        pull(pullVar) %>%
        str_extract_all(paste0(pState, "\\d+"), simplify=TRUE) %>%
        as.data.frame(stringsAsFactors=FALSE)    
}

testBegin <- getBeginEndTimeMatrix(testFileProc, pState="B")
testEnd <- getBeginEndTimeMatrix(testFileProc, pState="E")

if (ncol(testBegin) != 2 | ncol(testEnd) != 2) { 
    stop("Hard-coded for 2 columns each of begin/end- Fix") 
}

# Helper function to convert begin and end time using date and hour
extractTime <- function(x, var, sym="B") {
    if (is.na(x[var]) | x[var]=="") {
        utcReturn <- NA
    }
    else {
        utcUse <- str_replace(x[var], sym, "")
        if (str_length(utcUse)==4) {
            utcReturn <- paste0(x["dateUTC"], " ", utcUse)
            # If a 4-digit time starts with 23 and is in the 0Z METAR, it is part of the previous day
            if(str_sub(utcUse, 1, 2)=="23" & as.numeric(x["hourChar"])==0) {
                utcReturn <- paste0(as.Date(x["dateUTC"]) - 1, " ", utcUse)
            }
        } else if (str_length(utcUse)==2) {
            utcReturn <- paste0(x["dateUTC"], " ", x["hourChar"], utcUse)
        } else {
            cat("\nCannot parse data: ", x, "\n", x[var], "\n", var, sym, utcUse)
            stop()
        }
    }
    utcReturn
}

getBeginEndTimeVector <- function(timeExtractFile, origFullFile, extractVar, extractSym) {
    timeExtractFile %>%
        cbind(origFullFile[, c("dateUTC", "hourUTC")]) %>%
        mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
        apply(1, FUN=extractTime, var=extractVar, sym=extractSym)
}

testBT1 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V1", extractSym="B")
testBT2 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V2", extractSym="B")
testET1 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V1", extractSym="E")
testET2 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V2", extractSym="E")

testAllBegins <- 
    c(testBT1[!is.na(testBT1)], testBT2[!is.na(testBT2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

testAllEnds <- 
    c(testET1[!is.na(testET1)], testET2[!is.na(testET2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

```
  
Next, create the time intervals vector, with the capability to change the start state, check the intervals, exclude times as needed, and compare to raw METAR.
  
Example code includes:  
```{r}

# Function to create the time intervals data
createPrecipInterval <- function(endVector, beginVector, endExclude=c(), beginExclude=c(), 
                                 sState=FALSE, nMinPrint=1, maxProb=1000, nMaxPrint=1
                                 ) {
    
    # If the starting state is one of precipitation, allow it to 'burn in' by deleting the first end time
    if(sState) { endVector <- endVector[2:length(endVector)] }
    
    # Create the interval data
    endsUse <- endVector[!(endVector %in% lubridate::ymd_hm(endExclude))]
    beginsUse <- beginVector[!(beginVector %in% lubridate::ymd_hm(beginExclude))]
    intervalData <- endsUse - beginsUse
    
    # Show a summary of the interval data
    print(summary(as.numeric(intervalData)))
    
    # If there are any non-positive intervals, print the data causing the first of them
    if (min(as.numeric(intervalData)) <= 0) {
        cat("\nProblem Detected - Intervals are not positive.  Data to help investigate\n")
        posns <- which(as.numeric(intervalData) <= 0)
        posns <- max(1, posns[1]-5):min(length(beginsUse), length(endsUse), posns[nMinPrint]+5)
        cat("\nVector of Begins\n")
        print(lubridate::as_datetime(beginsUse[posns]))
        cat("\nVector of Ends\n")
        print(lubridate::as_datetime(endsUse[posns]))
        cat("\n")
    }
    
    # If there are any very long positive intervals, print the data causing the first five of them
    if (max(as.numeric(intervalData)) >= maxProb) {
        cat("\nPotential problem Detected - very long.  Data to help investigate\n")
        posns <- which(as.numeric(intervalData) >= maxProb)
        cat("\nPositions with problems are:", posns)
        posns <- max(1, posns[1]-5):min(length(beginsUse), length(endsUse), posns[min(length(posns), nMaxPrint)]+5)
        cat("\nVector of Begins\n")
        print(lubridate::as_datetime(beginsUse[posns]))
        cat("\nVector of Ends\n")
        print(lubridate::as_datetime(endsUse[posns]))
        cat("\n")
    }
    
    # Return the interval data
    intervalData
}

# Run a full pass for Las Vegas Rain
testIntervals <- createPrecipInterval(testAllEnds, testAllBegins)
testIntervals <- createPrecipInterval(testAllEnds, testAllBegins, endExclude=c("2016-05-01 0042"))


createPrecipIntervalPlots <- function(intervalData, beginsData, titleText, yAxisText, 
                                      beginExclude=c(), returnPlotsAndData=FALSE
                                      ) {

    # Exclude any data from begins as needed
    beginsData <- beginsData[!(beginsData %in% lubridate::ymd_hm(beginExclude))]
    
    # Create a plotting data frame
    histFrame <- data.frame(minutesPrecip=as.numeric(intervalData), 
                            month=lubridate::month(beginsData), 
                            rainDate=lubridate::date(beginsData)
                            ) %>%
        mutate(hoursPrecip=minutesPrecip/60)
    
    # Plot the precipitation durations in hours
    p1 <- histFrame %>%
        ggplot(aes(x=hoursPrecip)) +
        geom_histogram() + 
        labs(title=titleText, x=yAxisText, 
             subtitle="Distribution of hours per unique precipitation event"
             )
    print(p1)

    # Plot the precipitation by day in hours
    p2 <- histFrame %>%
        group_by(rainDate) %>%
        summarize(hoursPrecip=sum(hoursPrecip)) %>%
        ggplot(aes(x=hoursPrecip)) +
        geom_histogram() + 
        labs(title=titleText, x=yAxisText, 
             subtitle="Distribution of hours per day of precipitation (on days when 1+ minutes occurred)"
             )
    print(p2)
    
    # Plot the rain totals (in hours) by month
    # Create a data frame of all months and merge in precipitation data as an where available (0 otherwise)
    monthFrame <- histFrame %>%
        group_by(month) %>%
        summarize(minutesPrecip=sum(minutesPrecip), hoursPrecip=sum(hoursPrecip), nPrecip=n()) %>%
        right_join(data.frame(month=1:12, monthName=month.abb[1:12]), by="month") %>%
        tidyr::replace_na(list(minutesPrecip=0, hoursPrecip=0, nPrecip=0))
    # print(monthFrame)
    
    p3 <- monthFrame %>%
        ggplot(aes(x=factor(monthName, levels=month.abb[1:12]), y=hoursPrecip)) +
        geom_col() + 
        labs(title=titleText, y=yAxisText, x="") + 
        geom_text(aes(y=2 + hoursPrecip, label=round(hoursPrecip, 1)))
    print(p3)
    
    if (returnPlotsAndData) {
        list(histFrame=histFrame, monthFrame=monthFrame, p1=p1, p2=p2, p3=p3)
    } else {
        NULL
    }
}

# Plots for Las Vegas Rain
createPrecipIntervalPlots(testIntervals, testAllBegins, 
                          titleText="Las Vegas, NV Rainfall (hours) in 2016", yAxisText="Hours of Rain"
                          )

```

#### _Example #29: Combining Functions and Extending to Other Locales and Precipitation Types_  
The functions can be combined, and the approach then extended to other locales and precipitation types.

Example code includes:  
```{r}

# Combining all the functions in one place
runFullPrecipExtraction <- function(df, 
                                    pType, 
                                    titleText, 
                                    yAxisText,
                                    endExclude=c(), 
                                    beginExclude=c(), 
                                    endAdd=c(),
                                    beginAdd=c(),
                                    maxProb=1000, 
                                    sState=FALSE, 
                                    makePlots=TRUE, 
                                    returnPlotsAndData=FALSE
                                    ) {
    
    # Extract the precipitation data from a specified processed METAR file
    testFileProc <-extractPrecipData(df, pType=pType)
    
    # Confirm that the two-column specification is met (should relax hard-coding on this)
    testBegin <- getBeginEndTimeMatrix(testFileProc, pState="B")
    testEnd <- getBeginEndTimeMatrix(testFileProc, pState="E")

    if (!(ncol(testBegin) %in% c(1, 2, 3)) | !(ncol(testEnd) %in% c(1, 2, 3))) { 
        cat("\nBegin columns:", ncol(testBegin), "\t\tEndcolumns:", ncol(testEnd))
        stop("Hard-coded for 1-3 columns each of begin/end- Fix")
    }
    
    # Extract the beginning and ending information
    testBT1 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V1", extractSym="B")
    testET1 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V1", extractSym="E")
    
    if (ncol(testBegin) >= 2) {
        testBT2 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V2", extractSym="B")
    } else {
        testBT2 <- c()
    }
    if (ncol(testEnd) >= 2) {
        testET2 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V2", extractSym="E")
    } else {
        testET2 <- c()
    }

    if (ncol(testBegin) >= 3) {
        testBT3 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V3", extractSym="B")
    } else {
        testBT3 <- c()
    }
    if (ncol(testEnd) >= 3) {
        testET3 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V3", extractSym="E")
    } else {
        testET3 <- c()
    }
    
    testAllBegins <- 
        c(testBT1[!is.na(testBT1)], testBT2[!is.na(testBT2)], testBT3[!is.na(testBT3)], beginAdd) %>%
        lubridate::ymd_hm() %>%
        sort()

    testAllEnds <- 
        c(testET1[!is.na(testET1)], testET2[!is.na(testET2)], testET3[!is.na(testET3)], endAdd) %>%
        lubridate::ymd_hm() %>%
        sort()
    
    # Create the intervals
    testIntervals <- createPrecipInterval(testAllEnds, testAllBegins, 
                                          endExclude=endExclude, beginExclude=beginExclude, 
                                          maxProb=maxProb, sState=sState
                                          )
    
    # Create the precipitation plots
    plotOut <- NULL
    if (makePlots) {
        plotOut <- createPrecipIntervalPlots(testIntervals, testAllBegins, titleText=titleText, 
                                             yAxisText=yAxisText, beginExclude=beginExclude, 
                                             returnPlotsAndData=returnPlotsAndData
                                             )
    }
    
    if (!returnPlotsAndData) { plotOut <- NULL }
    
    # Return all of the key files, along with the parameters used
    keyParams <- list(fileName=deparse(substitute(df)), pType=pType, 
                      endExclude=endExclude, beginExclude=beginExclude, 
                      endAdd=endAdd, beginAdd=beginAdd,
                      maxProb=maxProb, sState=sState
                      )
    list(keyParams=keyParams, 
         testFileProc=testFileProc, 
         testAllBegins=testAllBegins[!(testAllBegins %in% lubridate::ymd_hm(beginExclude))], 
         testAllEnds=testAllEnds[!(testAllEnds %in% lubridate::ymd_hm(endExclude))], 
         testIntervals=testIntervals,
         plotOut=plotOut
         )
}

```
  
The full function can then be run to replicate the Las Vegas, NV 2016 rain plots:  
```{r}

# Run for Las Vegas, NV 2016 rainfall
klasRain2016 <- runFullPrecipExtraction(klas2016METAR, 
                                        pType="RA", 
                                        titleText="Las Vegas, NV Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-05-01 0042"),
                                        beginExclude=c(), 
                                        maxProb=1000, 
                                        sState=FALSE
                                        )

# Confirm that results are the same as when run outside the function
identical(testIntervals, klasRain2016$testIntervals)
identical(testFileProc, klasRain2016$testFileProc)
identical(testAllBegins, klasRain2016$testAllBegins)
identical(testAllEnds, klasRain2016$testAllEnds)

# Show the key parameters used
klasRain2016$keyParams

```

The full functions can then be explored for Chicago, IL 2016 rain plots:  
```{r}

# Run for Chicago, IL 2016 rainfall - run with no plots while debugging begin/end exclude
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c(),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the first problem end time - 2016-03-01 04:14:00 UTC
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414"),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the second problem end time - 2016-03-24 22:14:00 UTC
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414", "2016-03-24 2214"),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the begin time that creates a 3-day continuous rainfall - 2016-07-13 2347
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414", "2016-03-24 2214"),
                                        beginExclude=c("2016-07-13 2347"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the third problem end time - 2016-08-29 20:48:00 UTC
# As this is the final data integrity issue, show the plots
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414", "2016-03-24 2214", 
                                                     "2016-08-29 2048"
                                                     ),
                                        beginExclude=c("2016-07-13 2347"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Show the key parameters used
kordRain2016$keyParams

```

The full functions can then be explored for Chicago, IL 2016 snow plots:  
```{r}

# Run for Chicago, IL 2016 snowfall - run with no plots while debugging begin/end exclude
kordSnow2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="SN", 
                                        titleText="Chicago, IL Snowfall (hours) in 2016", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c(),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# After investigation, there are five extraneous end times in the Chicago snowfall data - exclude them
# Create the plots
kordSnow2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="SN", 
                                        titleText="Chicago, IL Snowfall (hours) in 2016", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c("2016-02-10 0216", "2016-03-24 2331", 
                                                     "2016-04-08 2300", "2016-04-09 0446",
                                                     "2016-12-24 0309"
                                                     ),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Show the key parameters used
kordSnow2016$keyParams

```

#### _Example #30: Checking Intervals for Consistency_  
The intervals created above can be compared against the hourly METAR observations for consistency.  In general, RAE53 would lead to no precipitation recorded at 53Z while RAB53 would lead to precipitation recorded at 53Z.  So, the intervals created should be reduced by 1 for purposes of overlap analysis.

Example code includes:  
```{r}

library(lubridate)  # lubridate has the %within% function and creates and checks intervals


intervalConsistency <- function(lst, pType){

    # Extract the beginning and interval times
    begins <- lst[["testAllBegins"]]
    ends <- lst[["testAllEnds"]]
    durs <- lst[["testIntervals"]]


    # Create intervals from the raw list file
    precipInts <- interval(begins, begins + durs - 1)

    
    # Extract the METAR and date-time information
    metar <- lst[["testFileProc"]][["origMETAR"]]
    dtime <- lst[["testFileProc"]][["dtime"]]
    
    # Take each METAR observation and check two factors
    # Is the precipitation type recorded in that METAR?
    # Does that METAR fall in any of the intervals?
    precipMETAR <- grepl(paste0(pType, ".*RMK"), metar, perl=TRUE)
    intMETAR <- sapply(dtime, FUN=function(x) {x %within% precipInts %>% any()})


    # Check for the consistency of the observations and print the mismatches
    print(table(precipMETAR, intMETAR))

    mism <- which(precipMETAR != intMETAR)
    if (length(mism) == 0) {
        cat("\nFull matches between METAR observations and intervals\n")
    } else {
        for (x in mism) {
            cat("\nMismatch at time", strftime(dtime[x], format="%Y-%m-%d %H:%M", tz="UTC"), "UTC\n")
            print(metar[max(1, x-2):min(length(metar), x+2)])
        }
    }
    
    list(precipInts=precipInts, mismatches=mism, mismatchTimes=dtime[mism])
}

# Mismatch due to TSRA without RAB on 2016-04-30
tmp <- intervalConsistency(klasRain2016, pType="RA")

# Mismatches due to malformatted METAR with missing data on 2016-07-13
# Mismatch due to RA vs FZRA on 2016-02-29
# Mismatch due to -RASN without RAB on 2016-03-24
# Mismatch due to TSRA without RAB on 2016-08-29
tmp <- intervalConsistency(kordRain2016, pType="RA")

# Mismatches due to SN without SNB on 2016-02-09, 2016-03-24, 2016-04-08, 2016-12-23
tmp <- intervalConsistency(kordSnow2016, pType="SN")

```
  
#### _Example #31: All Precipitation Types_  
The METAR data can be explored to find all the precipitation types it contains.  Broadly, a precipitation type should meet five criteria:

* Occurs prior to RMK (the remarks section has letter-only data that is not precipitation)  
* Preceded by a space  
* Optionally, begins with + (heavy) or - (light)  
* Contains only capital letters (no numbers) after the optional +/-  
* Trailed by a space  
  
This will extract some non-precipitation types such as CLR and AUTO, so some iteration is expected.

Example code includes:  
```{r}

# Function to find precipitation types
findPrecipTypes <- function(lst, precipRegex="(?<= )[+-]?[A-Z]+(?= )", priorTo="RMK", exclTypes=NULL) {

    metar <- lst[["testFileProc"]][["origMETAR"]]
    
    # Keep only everything prior to priorTo
    if (!is.null(priorTo)) {
        rmkLoc <- str_locate(metar, priorTo)
        rmkLoc[is.na(rmkLoc)] <- -1  # keep everything if NA
        metar <- str_sub(metar, start=1, end=pmax(-1, rmkLoc[, 1]-1))
    }
    
    # Exclude any items from the METAR
    if (!is.null(exclTypes)) {
        for (excl in exclTypes) {
            metar <- str_replace(metar, excl, "")
        }
    }
    
    # Is there a possible precipitation type in the file?
    pExists <- grepl(precipRegex, metar, perl=TRUE)
    
    cat("\nPrecipitation data status by METAR record\n\n")
    print(table(pExists))
    cat("\n")
    
    # Find the precipitation matches
    precipMatches <- str_match_all(metar, pattern=precipRegex)
    
    # Confirm that there are a maximum of two precipitation types per METAR
    listLengths <- sapply(precipMatches, FUN=length)
    if (max(listLengths) > 4) {
        cat("\nMaximum combinations observed is:", max(listLengths), "\n")
        stop("Hard-coded for at most 4 precipitation matches, please investigate")
    } else if (max(listLengths)==0) {
        cat("\nNo precipitation detected in this file\n")
    }
    
    if (max(listLengths) %in% c(2, 3, 4)) {
        cat("\nMultiple Precipitation types in the same record include\n")
        sapply(precipMatches[listLengths %in% c(2, 3, 4)], FUN=paste, collapse=" ") %>% 
            table() %>% 
            sort(decreasing=TRUE) %>%
            print()
    }
    
    if (sum(listLengths==1) > 0) {
        cat("\nSingle Precipitation types in the same record include\n")
        sapply(precipMatches[listLengths==1], FUN=c) %>% 
            table() %>% 
            sort(decreasing=TRUE) %>%
            print()
    }    

    # Extract the second column and summarize precipitation types
    # table(precipMatches[, 2]) %>% sort(decreasing=TRUE) %>% print()
    
    precipMatches
}

# Original pass for Chicago, IL
x1 <- findPrecipTypes(kordRain2016)

# Exclude AUTO and CLR as they are a record state and cloud type, not precipitation
x2 <- findPrecipTypes(kordRain2016, exclTypes=c("CLR", "AUTO"))


# Original pass for Las Vegas, NV
x3 <- findPrecipTypes(klasRain2016)

# Exclude AUTO and CLR as they are a record state and cloud type, not precipitation
x4 <- findPrecipTypes(klasRain2016, exclTypes=c("CLR", "AUTO"))

```
  
#### _Example #32: Extracting Precipitation Amounts_  
The METAR also contains summary of precipitation amounts that have occurred in the past hour(s).  Broadly, the format is:
  
* Pdddd - the amount of liquid precipitation equivalent that has fallen in the past hour  
* 6dddd - the amount of liquid precipitation equivalent that has fallen in the past 3/6 hours (for 0Z, 6Z, 12Z, and 18Z it is 6 hours; and for 3Z, 9Z, 15Z, and 21Z it is 3 hours)  
* 7dddd - the amount of liquid precipitation equivalent that has fallen in the past 24 hours  
  
The data as such can be extracted from the METAR.
  
Example code includes:  
```{r}

# Helper function to extract and convert data
extractConvertPrecipData <- function(var, regPattern) {
    
    mtxPrecip <- str_match(var, pattern=regPattern)
    vecPrecip <- mtxPrecip[, 2]
    vecPrecip[is.na(vecPrecip)] <- "0000"
    vecPrecip <- as.integer(vecPrecip) / 100
    
    vecPrecip
}

# Function to extract key precipitation information
extractLiquidPrecipAmounts <- function(lst) {
    
    # Pull the metar and the dtime
    metar <- lst[["testFileProc"]][["origMETAR"]]
    dtime <- lst[["testFileProc"]][["dtime"]]
    
    # Extract the Pdddd amounts (only available if liquid precipitation equivalent non-zero)
    pAmounts1Hour <- extractConvertPrecipData(metar, regPattern="RMK.* P(\\d{4})")
    cat("\nHourly totals for liquid precipitation equivalents:\n\n")
    table(pAmounts1Hour) %>% print()
    
    # Extract the 6dddd amounts
    pAmounts6Hour <- extractConvertPrecipData(metar, regPattern="RMK.* 6(\\d{4})")
    cat("\n3/6-hourly totals for liquid precipitation equivalents:\n\n")
    table(pAmounts6Hour) %>% print()
    
    # Extract the 7dddd amounts
    pAmounts24Hour <- extractConvertPrecipData(metar, regPattern="RMK.* 7(\\d{4})")
    cat("\n3/24-hourly totals for liquid precipitation equivalents:\n\n")
    table(pAmounts24Hour) %>% print()
    
    # Create a tibble and add the Zulu time
    tbl <- tibble::tibble(metar=metar, 
                          dtime=dtime, 
                          p1Hour=pAmounts1Hour, 
                          p3or6Hour=pAmounts6Hour, 
                          p24Hour=pAmounts24Hour
                          ) %>%
        mutate(zTime=(lubridate::hour(dtime) + ifelse(lubridate::minute(dtime)==0, 0, 1)) %% 24, 
               p6Hour=ifelse((zTime %% 6)==0, p3or6Hour, 0), 
               p3Hour=ifelse((zTime %% 3)==0, p3or6Hour-p6Hour, 0)
               )
    
    # Summarize the key amounts by Zulu time
    tbl %>%
        group_by(zTime) %>%
        summarize_if(is.numeric, sum) %>%
        as.data.frame() %>%
        print()
    
    tbl
    
}

klasPrecip <- extractLiquidPrecipAmounts(klasRain2016)
kordPrecip <- extractLiquidPrecipAmounts(kordRain2016)

```
  
Precipitation summarize can then be plotted by various time intervals:
```{r}

plotPrecipHistogram <- function(df, var, xlab, title, mod=1, rem=0) {

    # Create a variable for whether the modulo matches the desired remainder
    df <- df %>%
        mutate(isMod=(df$zTime %% mod)==rem)
    
    # Separate in to records for further proceesing and records to discard
    dfUse <- df %>%
        filter(isMod)
    dfDiscard <- df %>%
        filter(!isMod)
    
    # Summarize the discarded records
    cat(nrow(dfDiscard), "records have been discarded due to not matching the modulo rules")
    cat("\nThese discarded rows have", sum(dfDiscard %>% pull(var)), "inches of precipitation\n")
    
    numZero <- sum(dfUse[, var]==0)
    numTotal <- nrow(dfUse)
    pZero <- round(numZero/numTotal, 3)

    p <- dfUse %>%
        filter_at(vars(var), any_vars(. > 0)) %>%
        ggplot(aes_string(x=var)) + 
        geom_histogram() + 
        labs(x=xlab, y="Frequency", title=title, 
             subtitle=paste0("Includes only the ", 100*(1-pZero), "% of non-zero observations (", 
                             numZero, " of ", numTotal, " observations are zero)"
                             )
         )
    
    print(p)

}

plotPrecipHistogram(klasPrecip, 
                    var="p1Hour", 
                    xlab="1-hour precipitation (inches)", 
                    title="Las Vegas, NV (2016) Hourly METAR"
                    )

plotPrecipHistogram(klasPrecip, 
                    var="p6Hour", 
                    xlab="6-hour precipitation (inches)", 
                    title="Las Vegas, NV (2016) Hourly METAR", 
                    mod=6
                    )

plotPrecipHistogram(klasPrecip, 
                    var="p24Hour", 
                    xlab="24-hour precipitation (inches)", 
                    title="Las Vegas, NV (2016) Hourly METAR", 
                    mod=24, 
                    rem=12
                    )

```

#### _Example #33: Validating Consistency of Precipitation Amounts_  
The precipitation amounts per day summed from the 1-hour, 6-hour, and 24-hour columns should be nearly identical give or take rounding errors.

Example code includes:  
```{r}

checkPrecipConsistency <- function(df, title, subT="", hour24=12, maxDelta=0.02,
                                   sumVars=c("p24Hour", "p6Hour", "p1Hour"), 
                                   yearsUse=NULL
                                   ) {

    # Create the data file for analysis - updated time, summarized by month and day
    dfPrecip <- df %>% 
        mutate(dtUse=dtime-lubridate::hours(hour24), 
               year=lubridate::year(dtUse), 
               month=lubridate::month(dtUse), 
               day=lubridate::day(dtUse), 
               n=1
               ) %>% 
        group_by(year, month, day) %>% 
        summarize_if(is.numeric, sum) %>%
        ungroup()
    
    head(dfPrecip) %>% 
        select_at(vars(all_of(c("year", "month", "day", "n", sumVars)))) %>%
        print()
    
    
    # Plot the monthly totals
    p1Data <- dfPrecip %>%
        mutate(ym=paste0(year, "-", str_pad(month, width=2, side="left", pad="0"))) %>%
        select_at(vars(all_of(c("ym", sumVars)))) %>%
        group_by(ym) %>%
        summarize_all(sum)
    
    # Filter to only the desired years if specified
    if (!is.null(yearsUse)) {
        p1Data <- p1Data %>%
            filter(as.integer(str_sub(ym, 1, 4)) %in% yearsUse)
    }
    
    print(p1Data)
    print(p1Data %>% select(-ym) %>% colSums())
    
    p1 <- p1Data %>%
        pivot_longer(-ym, names_to="timePeriod", values_to="inchesPrecip") %>%
        ggplot(aes(x=factor(ym), y=inchesPrecip, group=timePeriod, color=timePeriod)) + 
            geom_line(lwd=1.5) + 
        labs(x="Month", y="Inches of Liquid Precipitation", title=title, subtitle=subT) + 
        ylim(0, NA)
    print(p1)
    
    # Output any days that have differences of more than maxDelta inches
    mismPrecip <- dfPrecip %>%
        select_at(vars(c("month", "day", all_of(sumVars)))) 
    
    mismMinMax <- mismPrecip %>%
        select_at(vars(all_of(sumVars))) %>%
        apply(1, FUN=function(x) { c(max(x), min(x), diff(range(x))) }) %>%
        t() %>%
        as.data.frame()
    names(mismMinMax) <- c("maxPrecip", "minPrecip", "delta")
    
    mismPrecip <- bind_cols(mismPrecip, mismMinMax)

    cat("\nMismatch precipitation amounts by day are:\n")
    round(mismPrecip$delta, 2) %>% table() %>% print()
    
    cat("\n\nMismatch days of worse than maxDelta inches include\n")
    mismPrecip %>%
        filter(delta > maxDelta) %>%
        as.data.frame() %>%
        print()
    
}

checkPrecipConsistency(klasPrecip, title="Las Vegas, NV 2016 Precipitation by Month", yearsUse=2016)
checkPrecipConsistency(kordPrecip, title="Chicago, IL 2016 Precipitation by Month", yearsUse=2016)

```
  
While the Las Vegas, NV precipitation data are consistent on the 1-hour, 6-hour, and 24-hour measurements, there are significanr differences for Chicago, IL (particularly 1-hour outliers in Jan-Feb-Mar).

For further exploration, another cold weather city (Lincoln, NE) is assessed:
```{r}

# Run for Lincoln, NE 2016 rainfall
klnkRain2016 <- runFullPrecipExtraction(klnk2016METAR, 
                                        pType="RA", 
                                        titleText="Lincoln, NE Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-09-16 1011", "2016-08-29 2329"),
                                        beginExclude=c("2016-07-13 1301"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Run for Lincoln, NE 2016 snowfall
klnkSnow2016 <- runFullPrecipExtraction(klnk2016METAR, 
                                        pType="SN", 
                                        titleText="Lincoln, NE Snowfall (hours) in 2016", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c(),
                                        beginExclude=c("2016-01-19 1644"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Check for interval consistency in the Lincoln, NE 2016 rainfall data
# Weird issue on 2016-07-13 1354 RAB without RAE
# 2016-08-29 2254 TSRA without RAB
# 2016-09-16 0654 many mismatches due to missing RAB here
tmp <- intervalConsistency(klnkRain2016, pType="RA")

# Check for interval consistency in the Lincoln, NE 2016 rainfall data
tmp <- intervalConsistency(klnkSnow2016, pType="SN")

# Original pass for Lincoln, NE
x5 <- findPrecipTypes(klnkRain2016)

# Exclude AUTO and CLR as they are a record state and cloud type, not precipitation
x6 <- findPrecipTypes(klnkRain2016, exclTypes=c("CLR", "AUTO"))

# Get the Lincoln, NE 2016 liquid precipitation
klnkPrecip <- extractLiquidPrecipAmounts(klnkRain2016)

# Check for consistency in the Lincoln, NE 2016 precipitation data
checkPrecipConsistency(klnkPrecip, title="Lincoln, NE 2016 Precipitation by Month", yearsUse=2016)

```
  
There are mismatches on 4 days of 2016 in the Lincoln, NE data though these appear to be manual error rather than systematic error due to cold weather.  Perhaps consulting official weather records can help determine the true liquid precipitation o the days in question.
  
#### _Example #34: Checking METAR Data for Gaps and Problems_  
Perhaps some of the inconsistencies in METAR data are driven by missing observations or sensor anomalies.  Missing observations can be detected as records expected but not detected, while sensor anomalies are flagged by a trailing '$' in the METAR record.

Example code includes:  
```{r}

checkGapsAnomalies <- function(lst, minDay, maxDay, loc, hour24=12) {
    
    # Pull the METAR data and datetime
    metar <- lst[["testFileProc"]][["origMETAR"]]
    dtime <- lst[["testFileProc"]][["dtime"]]
    
    # Create analysis data frame
    dfUse <- tibble::tibble(metar=metar, dtime=dtime) %>%
        mutate(dtUse=dtime-lubridate::hours(hour24), 
               year=lubridate::year(dtUse), 
               month=lubridate::month(dtUse), 
               day=lubridate::day(dtUse), 
               ym=paste0(year, "-", str_pad(month, width=2, side="left", pad="0")),
               isAnomaly=grepl("\\$$", metar),
               n=1
               )
    
    cat("\nData file with new time and anomaly variable\n")
    dim(dfUse) %>% print()
    names(dfUse) %>% print()
    
    # Keep only days between minDay and maxDay inclusive
    dfUse <- dfUse %>%
        filter(lubridate::date(dtUse) >= as.Date(minDay), lubridate::date(dtUse) <= as.Date(maxDay))
    
    cat("\nData file filtered to include only desired times\n")
    dim(dfUse) %>% print()
    names(dfUse) %>% print()
    
    # Sum n and isAnomaly by day
    dfIssue <- dfUse %>%
        group_by(ym, day) %>%
        summarize(n=sum(n), anomaly=sum(isAnomaly)) %>%
        ungroup() %>%
        mutate(missObs=24-n)
    summary(dfIssue) %>% print()
    
    # Plot summary by month
    dfPlot <- dfIssue %>%
        group_by(ym) %>%
        summarize(missObsDays=sum(missObs > 0), missObs=sum(missObs), 
                  anomalyDays=sum(anomaly > 0), anomaly=sum(anomaly), 
                  nDays=n()
                  ) %>%
        ungroup()
    
    # Create plot of missing observations
    p1 <- dfPlot %>%
        ggplot(aes(x=factor(ym), y=missObs)) + 
        geom_line(aes(group=1), lwd=2, color="red") + 
        geom_text(aes(y=missObs+0.5, label=ifelse(missObs>0, missObs, "")), color="red") + 
        labs(x="", y="Missing METAR Observations in Month", 
             title=paste0(loc, " Missing METAR Observations by Month")
             ) + 
        ylim(0, NA)
    print(p1)
    
    # Create plot of missing observations
    p2 <- dfPlot %>%
        ggplot(aes(x=factor(ym), y=anomaly)) + 
        geom_line(aes(group=1), lwd=2, color="blue") + 
        labs(x="", y="Potentially Anomalous (trailing $) METAR Observations", 
             title=paste0(loc, " Potentially Anomalous METAR Observations by Month")
             ) + 
        ylim(0, NA)
    print(p2)
    
    # Return the plotting frame
    dfPlot
}

checkGapsAnomalies(klasRain2016, minDay="2016-01-01", maxDay="2016-12-31", loc="Las Vegas, NV (2016)")
checkGapsAnomalies(kordRain2016, minDay="2016-01-01", maxDay="2016-12-31", loc="Chicago, IL (2016)")
checkGapsAnomalies(klnkRain2016, minDay="2016-01-01", maxDay="2016-12-31", loc="Lincoln, NE (2016)")

```
  
The Chicago, IL and Las Vegas, NV sensors self-identify as anomalous roughly an order of magnitude more often than the Lincoln, NE sensor.  There are no obvious spikes for Chicago, IL in Q1 2016, suggesting mismatched precipitation may be due to causes other than anomalous sensors and missing data.
  
#### _Example #35: Checking Other Time Periods and Locales_  
The precipitation discrepancies in the Lincoln. NE data appear to be one-off while there are no material preipitation discrepancies in the Las Vegas, NV data.  However, Q1 2016 is filled with precipitation discrepancies in the Chicago, IL data.  Is this just a Q1 2016 problem, or something related to large midwestern cities more generally, or a specific recurring Q1 issue in Chicago?

Pulling data for Minneapolis, MN and Detroit, MI for 2016 can help address whether there is a general Q1 2016 issue in large midwestern cities.  Pulling data for Chicago, IL in 2015 and 2017 can help address whether there is something specific and recurring to Chicago in Q1.

Example code includes:  
```{r cache=TRUE}

# Data pulls are cached to avoid over-using the Iowa State servers

# Get data for ORD for 2015
getASOSStationTime(stationID="ORD", analysisYears=2015, ovrWrite=TRUE)

# Get data for ORD for 2017
getASOSStationTime(stationID="ORD", analysisYears=2017, ovrWrite=TRUE)

# Get data for MSP for 2016
getASOSStationTime(stationID="MSP", analysisYears=2016, ovrWrite=TRUE)

# Get data for DTW for 2016
getASOSStationTime(stationID="DTW", analysisYears=2016, ovrWrite=TRUE)

```

Create the base METAR file for Chicago, IL 2015 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kord_2015.txt"  # file name for raw METAR data
timeZ <- "51Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2014-12-31 00:51:00", tz="UTC")  # Expected first time read
expDays <- 367  # Expected total days read
locMET <- "Chicago, IL"  # Description of city or location
shortMET <- "KORD METAR (2015)"  # Station code and timing
longMET <- "Chicago, IL O'Hare Hourly METAR (2015)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Chicago, IL (2015)
kord2015METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kord2015METAR)

```

Create the base METAR file for Chicago, IL 2017 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kord_2017.txt"  # file name for raw METAR data
timeZ <- "51Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2016-12-31 00:51:00", tz="UTC")  # Expected first time read
expDays <- 367  # Expected total days read
locMET <- "Chicago, IL"  # Description of city or location
shortMET <- "KORD METAR (2017)"  # Station code and timing
longMET <- "Chicago, IL O'Hare Hourly METAR (2017)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Chicago, IL (2017)
kord2017METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kord2017METAR)

```

Create the base METAR file for Minneapolis, MN 2016 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kmsp_2016.txt"  # file name for raw METAR data
timeZ <- "53Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:53:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Minneapolis, MN"  # Description of city or location
shortMET <- "KMSP METAR (2016)"  # Station code and timing
longMET <- "Minneapolis, MN Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Minneapolis, MN (2016)
kmsp2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kmsp2016METAR)

```

Create the base METAR file for Detroit, MI 2016 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kdtw_2016.txt"  # file name for raw METAR data
timeZ <- "53Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:53:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Detroit, MI"  # Description of city or location
shortMET <- "KDTW METAR (2016)"  # Station code and timing
longMET <- "Detroit, MI Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Detroit, MI (2016)
kdtw2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kdtw2016METAR)

```

Run a wind comparison for these new observations:  
```{r}

# Run for newly downloaded and processed data files
cpWind <- consolidatePlotWind(files=c("kord2015METAR", "kord2017METAR", 
                                      "kmsp2016METAR", "kdtw2016METAR"
                                      ), 
                              names=c("Chicago, IL (2015)", "Chicago, IL (2017)", 
                                      "Minneapolis, MN (2016)", "Detroit, MI (2016)"
                                      )
                              )
cpWind

```

The data can be examined for Chicago, IL 2015:  
```{r}

# Run for Chicago, IL 2015 rainfall
kordRain2015 <- runFullPrecipExtraction(kord2015METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2015", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2015-02-08 2308", "2015-05-17 1336", 
                                                     "2015-05-11 2020", "2015-06-29 0215",
                                                     "2015-12-28 1700"
                                                     ),
                                        beginExclude=c("2015-01-03 1022", "2015-01-03 1249", 
                                                       "2015-04-02 1125", "2015-06-21 0156",
                                                       "2015-08-15 0209"
                                                       ), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Run for Chicago, IL 2015 snowfall
kordSnow2015 <- runFullPrecipExtraction(kord2015METAR, 
                                        pType="SN", 
                                        titleText="Chicago, IL Snowfall (hours) in 2015", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c("2015-01-08 1829", "2015-01-08 1845"),
                                        beginExclude=c("2015-01-09 0630"), 
                                        beginAdd=c("2015-02-25 2251", "2015-01-03 1151"),
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Check for interval consistency in the Chicago, IL 2015 rainfall data
# FZRA on 2015-01-03 (4) and 2015-02-08 (1)
# 2015-05-11 1951 -RA without RAB (1)
# 2015-06-21 0251 -TSRA and RAB0156 but no following RAE (1)
# 2015-06-29 0151 -RA without RAB (1)
# 2015-08-15 best solution to miscoded RAEO6 rather than RAE06 (2)
# FZRA on 2015-12-28 (4)
tmp <- intervalConsistency(kordRain2015, pType="RA")

# Check for interval consistency in the Chicago, IL 2015 snowfall data
# 2015-01-09 0651 BLSN but thinks it is SN (interval OK)
tmp <- intervalConsistency(kordSnow2015, pType="SN")

# Original pass for Chicago, IL
x7 <- findPrecipTypes(kordRain2015)

# Exclude CLR as it is a cloud type, not precipitation
x8 <- findPrecipTypes(kordRain2015, exclTypes=c("CLR"))

# Get the Chicago, IL 2015 liquid precipitation
kord2015Precip <- extractLiquidPrecipAmounts(kordRain2015)

# Check for consistency in the Chicago, IL 2015 precipitation data
checkPrecipConsistency(kord2015Precip, title="Chicago, IL 2015 Precipitation by Month", yearsUse=2015)

# Check for missing data and sensor anomalies
checkGapsAnomalies(kordRain2015, minDay="2015-01-01", maxDay="2015-12-31", loc="Chicago, IL (2015)")

```
  
There appears to be a large issue on February 1-2, 2015 where precipitation data are off by several inches.  In addition to a few small one-off, there is also an extended issue from October 20-31, 2015 as well as a missing 24-hour precipitation value (1.25 inches) from December 28, 2015.
  
#### _Example #36: Automating Suggestiong for Precipitation Begin and End_  
At each time period in a METAR, there is a precipitation state that is either "on" or "off".  The state can be checked for consistency against the begin and end data as follows.  The general assumption is that recording will be more accuract for "is there precipitation now" then for all of the possible begins and ends for a specified precipitation type:
  
* If the previous state is "on", there should not be a "begin" prior to the next "end" - flag these as potential beginExclude items  
* If the previous state is "on" and the current state is "on", then there should not be any "end" items without an associated "begin" item following - flag these as potential endExclude items  
* If the previous state is "on" and the current state is "off", then there should be an "end" event in the remarks data - flag these as potential endAdd items  
* If the previous state is "off", there should not be an "end" event prior to the next "begin" - flag these as potential endExclude items  
* If the previous state is "off" and the current state is "off", then there should not be any "begin" items without an associated "end" item following - flag these as potential beginExclude items  
* If the previous state is "off" and the current state is "on", then there should be a "begin" event in the remarks data - flag these as potential beginAdd items  
  
Example code includes:  
```{r}

# Look for RA that is not preceded by FZ in the 2016 Chicago, IL data
regMatch <- "(?<!FZ)RA"

# Pull the Chicago, IL 2016 data and check for the specified precipitation pattern and lags
kordStates <- kordRain2016$testFileProc %>%
    select(dtime, origMETAR) %>%
    mutate(curPrecip=str_detect(origMETAR, paste0(".*", regMatch, ".*RMK")), 
           lagPrecip=lag(curPrecip, 1)
           )

# Use the analysis data to look for begins and ends flagged in the remarks
kordBE <- extractPrecipData(list(fullMETAR=kordStates), pType=regMatch)

# Inner join the data by dtime
kordStates <- kordStates %>%
    inner_join(kordBE %>% select(dtime, precipData, chgPrecip=isPrecip, dateUTC, hourUTC), by="dtime")
kordStates

# Get the beginning and end times data for the desired precipitation type
kordBegin <- getBeginEndTimeMatrix(kordStates, pState="B")
kordEnd <- getBeginEndTimeMatrix(kordStates, pState="E")

# Hard-code for 2-column files (relax later)
# Extract the begin and end times
if (ncol(kordBegin) != 2 | ncol(kordEnd) != 2) { stop("Hard-coded for 2 columns, fix") }
testBT1 <- getBeginEndTimeVector(kordBegin, kordStates, extractVar="V1", extractSym="B")
testET1 <- getBeginEndTimeVector(kordEnd, kordStates, extractVar="V1", extractSym="E")
testBT2 <- getBeginEndTimeVector(kordBegin, kordStates, extractVar="V2", extractSym="B")
testET2 <- getBeginEndTimeVector(kordEnd, kordStates, extractVar="V2", extractSym="E")

# Integrate to a single file
kordExceptions <- kordStates %>%
    mutate(b1=testBT1, e1=testET1, b2=testBT2, e2=testET2, 
           begins=ifelse(is.na(b1), 0, 1) + ifelse(is.na(b2), 0, 1), 
           ends=ifelse(is.na(e1), 0, 1) + ifelse(is.na(e2), 0, 1),
           etob=begins > ends, 
           btoe=ends > begins, 
           needBegin=curPrecip & !lagPrecip & !etob, 
           needEnd=!curPrecip & lagPrecip & !btoe, 
           overBegin=etob & (lagPrecip | !curPrecip), 
           overEnd=btoe & (curPrecip | !lagPrecip)
           )

# Flag potential issues
cat("\nNeed Begin time\n")
kordExceptions %>%
    filter(needBegin) %>%
    select(dtime, origMETAR)

cat("\nNeed End time\n")
kordExceptions %>%
    filter(needEnd) %>%
    select(dtime, origMETAR)

cat("\nExtraneous Begin time\n")
kordExceptions %>%
    filter(overBegin) %>%
    select(dtime, origMETAR)

cat("\nExtraneous End time\n")
kordExceptions %>%
    filter(overEnd) %>%
    select(dtime, origMETAR)

# Attempt to use on the kord2016 data - initial run
kordRain2016Test <- runFullPrecipExtraction(kord2016METAR, 
                                            pType=regMatch, 
                                            titleText="Chicago, IL Rainfall (hours) in 2016", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c(),
                                            beginExclude=c(),
                                            endAdd=c(), 
                                            beginAdd=c(),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=FALSE
                                            )

# Attempt to use on the kord2016 data - with adds and excludes
kordRain2016Test <- runFullPrecipExtraction(kord2016METAR, 
                                            pType=regMatch, 
                                            titleText="Chicago, IL Rainfall (hours) in 2016", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c(),
                                            beginExclude=c(),
                                            endAdd=c("2016-07-13 1851"), 
                                            beginAdd=c("2016-03-24 2151", "2016-08-29 1951"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordRain2016Test, pType=regMatch)

```

#### _Example #37: Function for Automating Suggestions for Precipitation Begin and End_  
The precipitation suggestions can be converted to a function, then applied to several of the other datasets.  
Example code includes:  
```{r}

suggestBeginEndTimes <- function(lst, regMatch, listExtract="fullMETAR") {

    # Pull the data and check for the specified precipitation pattern and lags
    sugStates <- lst[[listExtract]] %>%
        select(dtime, origMETAR) %>%
        mutate(curPrecip=str_detect(origMETAR, paste0(".*", regMatch, ".*RMK")), 
               lagPrecip=lag(curPrecip, 1)
               )

    # Use the analysis data to look for begins and ends flagged in the remarks
    sugBE <- extractPrecipData(list(fullMETAR=sugStates), pType=regMatch)

    # Inner join the data by dtime
    sugStates <- sugStates %>%
        inner_join(sugBE %>% select(dtime, precipData, chgPrecip=isPrecip, dateUTC, hourUTC), by="dtime")

    # Get the beginning and end times data for the desired precipitation type
    sugBegin <- getBeginEndTimeMatrix(sugStates, pState="B")
    sugEnd <- getBeginEndTimeMatrix(sugStates, pState="E")

    # Hard-code for 2-column files (relax later)
    # Extract the begin and end times
    if (ncol(sugBegin) > 3 | ncol(sugEnd) > 3) { stop("Hard-coded for 0-3 columns, fix") }
    testBT1 <- NA
    testBT2 <- NA
    testBT3 <- NA
    testET1 <- NA
    testET2 <- NA
    testET3 <- NA
    
    if (ncol(sugBegin) >= 1) {
        testBT1 <- getBeginEndTimeVector(sugBegin, sugStates, extractVar="V1", extractSym="B")
    }
    if (ncol(sugBegin) >= 2) {
        testBT2 <- getBeginEndTimeVector(sugBegin, sugStates, extractVar="V2", extractSym="B")
    }
    if (ncol(sugBegin) >= 3) {
        testBT3 <- getBeginEndTimeVector(sugBegin, sugStates, extractVar="V3", extractSym="B")
    }

    if (ncol(sugEnd) >= 1) {
        testET1 <- getBeginEndTimeVector(sugEnd, sugStates, extractVar="V1", extractSym="E")
    }
    if (ncol(sugEnd) >= 2) {
        testET2 <- getBeginEndTimeVector(sugEnd, sugStates, extractVar="V2", extractSym="E")
    }
    if (ncol(sugEnd) >= 3) {
        testET3 <- getBeginEndTimeVector(sugEnd, sugStates, extractVar="V3", extractSym="E")
    }
    
    # Integrate to a single file
    sugExceptions <- sugStates %>%
        mutate(b1=testBT1, e1=testET1, b2=testBT2, e2=testET2, b3=testBT3, e3=testET3,
               begins=ifelse(is.na(b1), 0, 1) + ifelse(is.na(b2), 0, 1) + ifelse(is.na(b3), 0, 1), 
               ends=ifelse(is.na(e1), 0, 1) + ifelse(is.na(e2), 0, 1) + ifelse(is.na(e3), 0, 1),
               etob=begins > ends, 
               btoe=ends > begins, 
               needBegin=curPrecip & !lagPrecip & !etob, 
               needEnd=!curPrecip & lagPrecip & !btoe, 
               overBegin=etob & (lagPrecip | !curPrecip), 
               overEnd=btoe & (curPrecip | !lagPrecip)
               )
    colSums(is.na(sugExceptions)) %>% print()

    # Flag potential issues
    cat("\nNeed Begin time\n")
    sugExceptions %>%
        filter(needBegin) %>%
        select(dtime, origMETAR) %>%
        print()

    cat("\nNeed End time\n")
    sugExceptions %>%
        filter(needEnd) %>%
        select(dtime, origMETAR) %>%
        print()

    cat("\nExtraneous Begin time\n")
    sugExceptions %>%
        filter(overBegin) %>%
        select(dtime, b1, b2, b3) %>%
        print()

    cat("\nExtraneous End time\n")
    sugExceptions %>%
        filter(overEnd) %>%
        select(dtime, e1, e2, e3) %>%
        print()

    cat("\nWrong amount of begins or ends\n")
    sugExceptions %>%
        mutate(absMatch=abs(begins-ends), absLag=lag(absMatch, 1), absLead=lead(absMatch, 1)) %>%
        filter(pmax(absMatch, absLag, absLead) > 1) %>%
        select(dtime, e1, e2, e3, b1, b2, b3) %>%
        print()
    
    sugExceptions
    
}


# Look for RA that is not preceded by FZ in the 2016 Chicago, IL data
kord2016ExceptRA <- suggestBeginEndTimes(kord2016METAR, regMatch="(?<!FZ)RA")

# Attempt to use on the kord2016 rain data
kordRain2016Test <- runFullPrecipExtraction(kord2016METAR, 
                                            pType="(?<!FZ)RA", 
                                            titleText="Chicago, IL Rainfall (hours) in 2016", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c(),
                                            beginExclude=c(),
                                            endAdd=c("2016-07-13 1851"), 
                                            beginAdd=c("2016-03-24 2151", "2016-08-29 1951"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordRain2016Test, pType="(?<!FZ)RA")



# Look for SN that is not preceded by BL in the 2016 Chicago, IL data
kord2016ExceptSN <- suggestBeginEndTimes(kord2016METAR, regMatch="(?<!BL)SN")

# Attempt to use on the kord2016 snow data
kordSnow2016Test <- runFullPrecipExtraction(kord2016METAR, 
                                            pType="(?<!BL)SN", 
                                            titleText="Chicago, IL Snowfall (hours) in 2016", 
                                            yAxisText="Hours of Snow", 
                                            endExclude=c("2016-04-09 0446"),
                                            beginExclude=c(),
                                            endAdd=c(), 
                                            beginAdd=c("2016-02-10 0051", "2016-03-24 2151", 
                                                       "2016-04-08 2251", "2016-12-24 0151"
                                                       ),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordSnow2016Test, pType="(?<!BL)SN")

```
  
This algorithm can then be used to process the Chicago, IL 2015 and 2017 data:  
```{r}

# Look for RA that is not preceded by FZ in the 2015 Chicago, IL data
kord2015ExceptRA <- suggestBeginEndTimes(kord2015METAR, regMatch="(?<!FZ)RA")

# Attempt to use on the kord2015 rain data
kordRain2015Test <- runFullPrecipExtraction(kord2015METAR, 
                                            pType="(?<!FZ)RA", 
                                            titleText="Chicago, IL Rainfall (hours) in 2015", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c("2015-05-17 1336"),
                                            beginExclude=c("2015-04-02 1125"),
                                            endAdd=c("2015-06-21 0551", "2015-08-15 0451"), 
                                            beginAdd=c("2015-05-11 1951", "2015-06-29 0151"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordRain2015Test, pType="(?<!FZ)RA")


# Look for SN that is not preceded by BL in the 2015 Chicago, IL data
kord2015ExceptSN <- suggestBeginEndTimes(kord2015METAR, regMatch="(?<!BL)SN")

# Attempt to use on the kord2015 snow data
kordSnow2015Test <- runFullPrecipExtraction(kord2015METAR, 
                                            pType="(?<!BL)SN", 
                                            titleText="Chicago, IL Snowfall (hours) in 2015", 
                                            yAxisText="Hours of Snow", 
                                            endExclude=c(),
                                            beginExclude=c("2015-01-08 1845"),
                                            endAdd=c(), 
                                            beginAdd=c("2015-01-03 1151", "2015-02-25 2251"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordSnow2015Test, pType="(?<!BL)SN")



# Look for RA that is not preceded by FZ in the 2017 Chicago, IL data
kord2017ExceptRA <- suggestBeginEndTimes(kord2017METAR, regMatch="(?<!FZ)RA")

# Attempt to use on the kord2017 rain data
kordRain2017Test <- runFullPrecipExtraction(kord2017METAR, 
                                            pType="(?<!FZ)RA", 
                                            titleText="Chicago, IL Rainfall (hours) in 2017", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c("2017-03-01 0201"),
                                            beginExclude=c("2017-01-25 0359", "2017-03-17 1025"),
                                            endAdd=c(), 
                                            beginAdd=c("2017-05-18 0451", "2017-07-20 0351", 
                                                       "2017-10-14 0851"
                                                       ),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordRain2017Test, pType="(?<!FZ)RA")



# Look for SN that is not preceded by BL in the 2017 Chicago, IL data
kord2017ExceptSN <- suggestBeginEndTimes(kord2017METAR, regMatch="(?<!BL)SN")

# Attempt to use on the kord2017 snow data
kordSnow2017Test <- runFullPrecipExtraction(kord2017METAR, 
                                            pType="(?<!BL)SN", 
                                            titleText="Chicago, IL Snowfall (hours) in 2017", 
                                            yAxisText="Hours of Snow", 
                                            endExclude=c(),
                                            beginExclude=c(),
                                            endAdd=c(), 
                                            beginAdd=c("2017-03-02 0251"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordSnow2017Test, pType="(?<!BL)SN")

```

And, the checks for precipitation consistency can then be run on the Chicago 2015 and 2017 data:  
```{r}

# Get the Chicago, IL 2015 liquid precipitation
kord2015PrecipTest <- extractLiquidPrecipAmounts(kordRain2015Test)

# Check for consistency in the Chicago, IL 2015 precipitation data
checkPrecipConsistency(kord2015PrecipTest, title="Chicago, IL 2015 Precipitation by Month", yearsUse=2015)

# Check for missing data and sensor anomalies
checkGapsAnomalies(kordRain2015Test, minDay="2015-01-01", maxDay="2015-12-31", loc="Chicago, IL (2015)")



# Get the Chicago, IL 2017 liquid precipitation
kord2017PrecipTest <- extractLiquidPrecipAmounts(kordRain2017Test)

# Check for consistency in the Chicago, IL 2017 precipitation data
checkPrecipConsistency(kord2017PrecipTest, title="Chicago, IL 2017 Precipitation by Month", yearsUse=2017)

# Check for missing data and sensor anomalies
checkGapsAnomalies(kordRain2017Test, minDay="2017-01-01", maxDay="2017-12-31", loc="Chicago, IL (2017)")

```
  
The 2017 precipitation data in Chicago show much better alignment among the 1-hour, 6-hour, and 24-hour totals summed by day or month.  There is heavy precipitation on July 12, 2017 that merits further exploration and an April 2-3, 2017 rain event that has about the same sum but differs by day.  This data is suggestive of occasional one-off problems rather than a seemingly systemic issue as in parts of 2015-2016.
  
#### _Example #38: Extending Suggestions to Other Locales_  
The suggestions can be extended to the 2016 data for Minneapolis, MN and Detroit, MI.

Example code includes:  
```{r}

# Look for RA that is not preceded by FZ in the 2015 Minneapolis, MN data
kmsp2016ExceptRA <- suggestBeginEndTimes(kmsp2016METAR, regMatch="(?<!FZ)RA")

# Attempt to use on the kmsp2016 rain data
kmspRain2016Test <- runFullPrecipExtraction(kmsp2016METAR, 
                                            pType="(?<!FZ)RA", 
                                            titleText="Minneapolis, MN Rainfall (hours) in 2016", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c("2016-04-05 1522", "2016-10-18 0435"),
                                            beginExclude=c("2016-05-29 1931", "2016-06-20 0411", 
                                                           "2016-09-06 0744", "2016-09-16 0817", 
                                                           "2016-11-29 0718"
                                                           ),
                                            endAdd=c("2016-09-06 0853", "2016-11-22 1352"), 
                                            beginAdd=c("2016-09-06 0753"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kmspRain2016Test, pType="(?<!FZ)RA")


# Look for SN that is not preceded by BL in the 2016 Minneapolis, MN data
kmsp2016ExceptSN <- suggestBeginEndTimes(kmsp2016METAR, regMatch="(?<!BL)SN")

# Attempt to use on the kmsp2016 snow data
kmspSnow2016Test <- runFullPrecipExtraction(kmsp2016METAR, 
                                            pType="(?<!BL)SN", 
                                            titleText="Minneapolis, MN Snowfall (hours) in 2016", 
                                            yAxisText="Hours of Snow", 
                                            endExclude=c(),
                                            beginExclude=c(),
                                            endAdd=c("2016-12-07 1453"), 
                                            beginAdd=c("2016-01-12 0053", "2016-12-07 1553", 
                                                       "2016-12-08 2253"
                                                       ),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kmspSnow2016Test, pType="(?<!BL)SN")



# Look for RA that is not preceded by FZ in the 2016 Detroit, MI data
kdtw2016ExceptRA <- suggestBeginEndTimes(kdtw2016METAR, regMatch="(?<!FZ)RA")

# Attempt to use on the kdtw2016 rain data
kdtwRain2016Test <- runFullPrecipExtraction(kdtw2016METAR, 
                                            pType="(?<!FZ)RA", 
                                            titleText="Detroit, MI Rainfall (hours) in 2016", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c("2016-07-14 0030"),
                                            beginExclude=c(),
                                            endAdd=c("2016-04-01 0053", "2016-04-07 2253", 
                                                     "2016-08-15 0053", "2016-08-16 1453"
                                                     ), 
                                            beginAdd=c("2016-02-24 1153", "2016-04-07 2353"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kdtwRain2016Test, pType="(?<!FZ)RA")



# Look for SN that is not preceded by BL in the 2016 Detroit, MI data
kdtw2016ExceptSN <- suggestBeginEndTimes(kdtw2016METAR, regMatch="(?<!BL)SN")

# Attempt to use on the kdtw2016 snow data
kdtwSnow2016Test <- runFullPrecipExtraction(kdtw2016METAR, 
                                            pType="(?<!BL)SN", 
                                            titleText="Detroit, MI Snowfall (hours) in 2016", 
                                            yAxisText="Hours of Snow", 
                                            endExclude=c(),
                                            beginExclude=c("2016-02-24 1112"),
                                            endAdd=c(), 
                                            beginAdd=c("2016-02-09 1053"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kdtwSnow2016Test, pType="(?<!BL)SN")

```

And, the checks for precipitation consistency can then be run on the Minneapolis and Detroit 2016 data:  
```{r}

# Get the Minneapolis, MN 2016 liquid precipitation
kmsp2016PrecipTest <- extractLiquidPrecipAmounts(kmspRain2016Test)

# Check for consistency in the Minneapolis, MN 2016 precipitation data
checkPrecipConsistency(kmsp2016PrecipTest, title="Minneapolis, MN 2016 Precipitation by Month", yearsUse=2016)

# Check for missing data and sensor anomalies
checkGapsAnomalies(kmspRain2016Test, minDay="2016-01-01", maxDay="2016-12-31", loc="Minneapolis, MN (2016)")



# Get the Detroit, MI 2016 liquid precipitation
kdtw2016PrecipTest <- extractLiquidPrecipAmounts(kdtwRain2016Test)

# Check for consistency in the Detroit, MI 2016 precipitation data
checkPrecipConsistency(kdtw2016PrecipTest, title="Detroit, MI 2016 Precipitation by Month", yearsUse=2016)

# Check for missing data and sensor anomalies
checkGapsAnomalies(kdtwRain2016Test, minDay="2016-01-01", maxDay="2016-12-31", loc="Chicago, IL (2016)")

```
  
There are just a few one-off mismatches among 1-hour, 6-hour, and 24-hour precipitation sums by day/month in the 2016 data for Minneapolis, MN and Detroit, MI.  So the more systemic issues observed in the 2016 Chicago, IL data appear to be specific to Chicago 2016 rather than a general issue with cold-weather cities or reporting issues associated with 2016.
  
#### _Example #39: Consistency Checks for Precipitation_  
Automating checks for precipitation may help find the areas where the 1-hour totals, 3/6-hour totals, and 24-hour totals diverge.  These are areas for further exploration that can help better assess the monthly liquid precipitation totals.
  
Example code includes:  
```{r}

# Check for consistency in the Chicago, IL 2015 precipitation data
checkPrecipConsistency(kord2015PrecipTest, title="Chicago, IL 2015 Precipitation by Month", yearsUse=2015)

# Check for consistency by time interval in the Chicago, IL 2015 precipitation data

# Code copied from checkPrecipConsistency
hrStart <- 12
dfPrecipHourly <- kord2015PrecipTest %>% 
    mutate(dtUse=dtime-lubridate::hours(hrStart), 
           year=lubridate::year(dtUse), 
           month=lubridate::month(dtUse), 
           day=lubridate::day(dtUse), 
           tod=ifelse(((zTime-hrStart) %% 24)==0, 24, (zTime-hrStart) %% 24),
           h3Block=((tod-1) %/% 3) + 1, 
           h6Block=((tod-1) %/% 6) + 1,
           h24Block=((tod-1) %/% 24) + 1, 
           n=1
           ) %>%
    group_by(year, month, day, h3Block) %>%
    mutate(h3SumHourly=sum(p1Hour)) %>%
    ungroup() %>%
    group_by(year, month, day, h6Block) %>%
    mutate(h6SumHourly=sum(p1Hour)) %>%
    ungroup() %>%
    group_by(year, month, day, h24Block) %>%
    mutate(h24SumHourly=sum(p1Hour), h24Sum6Hourly=sum(p6Hour)) %>%
    ungroup()
    
# Three-hourly sum does not equal sum of hourly sums
threeVone <- dfPrecipHourly %>%
    group_by(year, month, day, h3Block) %>%
    summarize(n=sum(n), p1Hour=sum(p1Hour), p3Hour=sum(p3Hour)) %>%
    filter((h3Block %% 2) != 0, abs(p1Hour-p3Hour) > 0.025)

# Six-hourly sum does not equal sum of hourly sums
sixVone <- dfPrecipHourly %>%
    group_by(year, month, day, h6Block) %>%
    summarize(n=sum(n), p1Hour=sum(p1Hour), p6Hour=sum(p6Hour)) %>%
    filter(abs(p1Hour-p6Hour) > 0.025)

# 24-hourly sum does not equal sum of hourly sums
twentyfourVone <- dfPrecipHourly %>%
    group_by(year, month, day, h24Block) %>%
    summarize(n=sum(n), p1Hour=sum(p1Hour), p24Hour=sum(p24Hour)) %>%
    filter(abs(p24Hour-p1Hour) > 0.025)

# 24-hourly sum does not equal sum of six-hourly sums
twentyfourVsix <- dfPrecipHourly %>%
    group_by(year, month, day, h24Block) %>%
    summarize(n=sum(n), p6Hour=sum(p6Hour), p24Hour=sum(p24Hour)) %>%
    filter(abs(p24Hour-p6Hour) > 0.025)

# For each day, flag whether there is a 6-1, 24-6, and 24-1 issue
sixOneIssue <- sixVone %>%
    group_by(year, month, day) %>%
    summarize(n6_1=n(), p6_1=abs(sum(p1Hour)-sum(p6Hour)))

twentyfourSixIssue <- twentyfourVsix %>%
    group_by(year, month, day) %>%
    summarize(n6_24=n(), p6_24=abs(sum(p24Hour)-sum(p6Hour)))

twentyfourOneIssue <- twentyfourVone %>%
    group_by(year, month, day) %>%
    summarize(n1_24=n(), p1_24=abs(sum(p24Hour)-sum(p1Hour)))

allIssues <- sixOneIssue %>%
    full_join(twentyfourSixIssue) %>%
    full_join(twentyfourOneIssue) %>%
    arrange(year, month, day) %>%
    select(year, month, day, n6_1, n6_24, n1_24, p6_1, p6_24, p1_24)
allIssues

dfPrecipHourly %>%
    filter(year==2015, month==2, day==1) %>%
    select(tod, p1Hour, p6Hour, p24Hour, h6SumHourly, h24SumHourly, h24Sum6Hourly) %>%
    as.data.frame()

dfPrecipHourly %>%
    filter(year==2015, month==2, day==2) %>%
    select(tod, p1Hour, p6Hour, p24Hour, h6SumHourly, h24SumHourly, h24Sum6Hourly) %>%
    as.data.frame()

dfPrecipHourly %>%
    filter(year==2015, month==10, day==20) %>%
    select(tod, p1Hour, p6Hour, p24Hour, h6SumHourly, h24SumHourly, h24Sum6Hourly) %>%
    as.data.frame()

dfPrecipHourly %>%
    filter(year==2015, month==10, day==21) %>%
    select(tod, p1Hour, p6Hour, p24Hour, h6SumHourly, h24SumHourly, h24Sum6Hourly) %>%
    as.data.frame()

allIssues %>%
    filter(is.na(p6_1) | is.na(p6_24) | is.na(p1_24))

allIssues %>%
    filter(!(is.na(p6_1) | is.na(p6_24) | is.na(p1_24)))

```
  
Looking at February 1, 2015 as an example shows the issue may not be easy to track down.  There is no obvious single entry error driving the large disconnects.  Rather, the 6-hourly observations seem somewhat tethered to the hourly observations while the 24-hour observation is very different than the sum of either the 1-hourly observations or the 6-hourly observations.

Looking at February 2, 2015, there is a single anomalous record in the p1Hour column that likely accounts for the full disconnect.  Checking official precipitation records may be helpful to vet that there was no precipitation on this day.

Looking at October 20, 2015, there is a possible anomaly around hour 16 (not in 24-hour total) and hour 18 (sums to 0.1 greater than hourly totals) that drive the disconnect.

Looking at Octiber 21, 2015, there is a possible anomaly at hours 10 and 12 where precipitations are recorded (with 1-hour and 6-hour not aligned) despite nothing in the 24-hour total.

Further investigation such as checking the official precipitation records may be needed.
  
#### _Example #40: External Source for Precipitation Checking_  
Data are maintained for Chicago precipitation totals by month at https://www.weather.gov/lot/

While the table is not ideally formatted, it appears possible to pull down the data.

Example code includes:  
```{r cache=TRUE}

# cached to avoid repeated hits against the weather.gov server

# # Define the website and xpath for the table
# webSite <- 'https://www.weather.gov/lot/July_Precip_Rankings_Chicago'
# webXPath <- '//*[@id=\"pagebody\"]/div[3]/div/table[2]'
# 
# # Pull down the table
# webData <- webSite %>% 
#     xml2::read_html() %>% 
#     rvest::html_nodes(xpath=webXPath) %>% 
#     rvest::html_table(fill=TRUE)
# 
# # Data appear to be stored in a list containing a frame
# webDF <- webData[[1]]
# 
# # Row 1 appears to have the data, but the first two columns are filler
# webUse <- webDF[1, 3:ncol(webDF)]
# webUse
# 
# # The order of the data are a recurrence of (Precip - Year - NA) (Precip - Year - NA) (Precip - Year)
# # So, for each 8 columns, precipitation will be 1-4-7 and year will be 2-5-8
# maxRead <- ncol(webUse) %/% 8
# precipBase <- 8*(1:maxRead) - 7
# yearBase <- 8*(1:maxRead) - 6
# 
# precipData <- webUse[, c(precipBase, precipBase+3, precipBase+6)]
# yearData <- webUse[, c(yearBase, yearBase+3, yearBase+6)]
# 
# dfPrecip <- tibble::tibble(precipData=as.vector(as.matrix(precipData)), 
#                            yearData=as.vector(as.matrix(yearData))
#                            )
# dfPrecip


# Extend to Function for all 12 months
getPrecipMonthlyData <- function(webSite, webXPath) {
    
    # Pull down the table
    webData <- webSite %>% 
        xml2::read_html() %>% 
        rvest::html_nodes(xpath=webXPath) %>% 
        rvest::html_table(fill=TRUE)

    # Data appear to be stored in a list containing a frame
    webDF <- webData[[1]]

    # Row 1 appears to have the data, but the first two columns are filler
    webUse <- webDF[1, 3:ncol(webDF)]

    # The order of the data are a recurrence of (Precip - Year - NA) (Precip - Year - NA) (Precip - Year)
    # So, for each 8 columns, precipitation will be 1-4-7 and year will be 2-5-8
    maxRead <- ncol(webUse) %/% 8
    precipBase <- 8*(1:maxRead) - 7
    yearBase <- 8*(1:maxRead) - 6

    precipData <- webUse[, c(precipBase, precipBase+3, precipBase+6)]
    yearData <- webUse[, c(yearBase, yearBase+3, yearBase+6)]

    dfPrecip <- tibble::tibble(precipData=as.vector(as.matrix(precipData)), 
                               yearData=as.vector(as.matrix(yearData))
                               )
    dfPrecip

}

getPrecipMonthlyData(webSite='https://www.weather.gov/lot/January_Precip_Rankings_Chicago', 
                     webXPath='//*[@id=\"pagebody\"]/div[3]/div/table[2]'
                     )

listPrecip <- vector("list", 12)
for (x in 1:12) {
    webSite <- paste0("https://www.weather.gov/lot/", month.name[x], "_Precip_Rankings_Chicago")
    if (x %in% c(3, 5, 7)) { 
        listPrecip[[x]] <- NA
        cat("\nPassed on pulling data for x = ", x)
    }
    else {
        listPrecip[[x]] <- getPrecipMonthlyData(webSite, 
                                                webXPath='//*[@id=\"pagebody\"]/div[3]/div/table[2]'
                                                )
        cat("\nPulled data for x =", x)
    }
}

```
  
Data for March, May, and July have a different formatting and could not parse with the automated routine.  The remaining data can be bound in to a single data frame:
```{r}

listPrecip[[3]] <- data.frame(precipData=NA, yearData=NA)
listPrecip[[5]] <- data.frame(precipData=NA, yearData=NA)
listPrecip[[7]] <- data.frame(precipData=NA, yearData=NA)

officialPrecip <- bind_rows(listPrecip, .id="month")

officialPrecip %>%
    filter(yearData==2016)
```
  
#### _Example #41: Examining Wind Direction by Season_  
Winds tend to change with month as can be seen in the METAR data.
  
Example code includes:  
```{r}

wdTypes <- c("Error", "None", "Variable", 
             "NNW", "N", "NNE", 
             "ENE", "E", "ESE", 
             "SSE", "S", "SSW", 
             "WSW", "W", "WNW"
             )

plotWindData <- function(lst, subT, extraVars=NULL, showPlots=TRUE) {

    baseVars <- c("valid", "dirW", "spdW")
    if (!(is.null(extraVars))) {
        allVars <- c(baseVars, extraVars)
    } else {
        allVars <- baseVars
    }
    
    # Extract the wind data
    # Pull the wind speed and wind direction, and classify accordingly
    windData <- lst[["fullMETAR"]] %>%
        rename(valid=dtime, dirW=WindDir, spdW=WindSpeed) %>%
        select_at(vars(all_of(allVars))) %>%
        filter(complete.cases(.)) %>%
        mutate(spdBucket=factor(case_when(spdW==0 ~ "None", 
                                          spdW<=6 ~ "Light", 
                                          spdW<=12 ~ "Moderate", 
                                          TRUE ~ "Strong"
                                          ), 
                                levels=c("Strong", "Moderate", "Light", "None")
                                ), 
               wd=factor(case_when(dirW=="VRB" ~ "Variable", 
                                   dirW=="000" ~ "None", 
                                   dirW %in% c("350", "360", "010") ~ "N", 
                                   dirW %in% c("020", "030", "040") ~ "NNE", 
                                   dirW %in% c("050", "060", "070") ~ "ENE", 
                                   dirW %in% c("080", "090", "100") ~ "E",
                                   dirW %in% c("110", "120", "130") ~ "ESE", 
                                   dirW %in% c("140", "150", "160") ~ "SSE", 
                                   dirW %in% c("170", "180", "190") ~ "S", 
                                   dirW %in% c("200", "210", "220") ~ "SSW", 
                                   dirW %in% c("230", "240", "250") ~ "WSW", 
                                   dirW %in% c("260", "270", "280") ~ "W", 
                                   dirW %in% c("290", "300", "310") ~ "WNW", 
                                   dirW %in% c("320", "330", "340") ~ "NNW", 
                                   TRUE ~ "Error"
                                   ), 
                         levels=wdTypes
                         ), 
               predomDir=factor(case_when(wd=="Variable" ~ "Variable", 
                                          wd=="None" ~ "None", 
                                          wd %in% c("NNW", "N", "NNE") ~ "North",
                                          wd %in% c("ENE", "E", "ESE") ~ "East", 
                                          wd %in% c("SSE", "S", "SSW") ~ "South", 
                                          wd %in% c("WSW", "W", "WNW") ~ "West"
                                          ), 
                                levels=c("North", "East", "South", "West", "Variable", "None")
                                ),
               month=lubridate::month(valid), 
               monthfct=factor(month.abb[month], levels=month.abb[1:12])
               )

    # Summary of wind speeds and directions
    windData %>% 
        count(spdBucket, wd) %>% 
        pivot_wider(id_cols=c("wd"), names_from="spdBucket", values_from="n") %>%
        print()

    # Plot of wind speeds and directions - stacked bars
    p1 <- windData %>%
        ggplot(aes(x=wd, fill=spdBucket)) + 
        geom_bar() + 
        labs(x="Wind Direction", y="# Hourly Observations", 
             title="Wind Speed by Wind Direction", subtitle=subT
             ) + 
        scale_fill_discrete("Wind Speed")
    if (showPlots) print(p1)
    
    # Plot of wind speeds and directions - filled bars
    p2 <- windData %>%
        ggplot(aes(x=wd, fill=spdBucket)) + 
        geom_bar(position="fill") + 
        labs(x="Wind Direction", y="% Hourly Observations", 
             title="Wind Speed by Wind Direction", subtitle=subT
             ) + 
        scale_fill_discrete("Wind Speed")
    if (showPlots) print(p2)

    # Plot of wind speeds and predominant directions - filled bars
    p3 <- windData %>%
        filter(spdBucket != "None") %>%
        ggplot(aes(x=spdBucket, fill=predomDir)) + 
        geom_bar(position="fill") + 
        labs(x="Wind Speed", y="% Hourly Observations", 
             title="Wind Speed by Wind Direction", subtitle=subT
             ) + 
        scale_fill_discrete("Wind Direction")
    if (showPlots) print(p3)
    
    # Plot of months and wind speeds - filled bars
    p4 <- windData %>%
        ggplot(aes(x=monthfct, fill=spdBucket)) + 
        geom_bar(position="fill") + 
        labs(x="", y="% Hourly Observations", 
             title="Wind Speed by Month", subtitle=subT
             ) + 
        scale_fill_discrete("Wind Speed")
    if (showPlots) print(p4)
    
    # Plot of months and wind directions - filled bars
    p5 <- windData %>%
        ggplot(aes(x=monthfct, fill=predomDir)) + 
        geom_bar(position="fill") + 
        labs(x="", y="% Hourly Observations", 
             title="Wind Direction by Month", subtitle=subT
             ) + 
        scale_fill_discrete("Wind Direction")
    if (showPlots) print(p5)
    
    windData    
}

# Run for Lincoln, NE (2016)
klnk2016Wind <- plotWindData(klnk2016METAR, subT="Lincoln, NE (2016)")

# Run for Las Vegas, NV (2016)
klas2016Wind <- plotWindData(klas2016METAR, subT="Las Vegas, NV (2016)")

# Run for Chicago, IL (2016)
kord2016Wind <- plotWindData(kord2016METAR, subT="Chicago, IL (2016)")

```
  
Lincoln, NE 2016 findings:  
  
* Winds are frequently from the North (NNW, N) and South (SSE, S), particularly with Strong winds  
* Winds are more often strong in late winter and early spring, peaking around April  
* North winds are more common in February while South winds are more common in summer  
  
Las Vegas, NV 2016 findings:  
  
* Winds are predominatly either calm or from the South (S, SSW, WSW)  
* Winds from the NNW, while rare, were strong nearly half the time they occurred  
* Winds are generally calm or light from November to February, with moderate winds becoming more common during the summer months  
* Winds from the south are especially common in May to July  
  
Chicago, IL 2016 findings:  
  
* Winds are faily evenly distributed by direction, though with troughs near East and spikes near West  
* Strong winds are less likely to be from the East and more likely to be from the West than moderate or light winds  
* Winds are more likely to be strong in winter and to be light in summer  
* Wind directions change meaningfully, with East being rare in the winter and more common in spring-summer; and with West being more common in winter  
  
#### _Example #42: Predicting Locale Based on Wind Data_  
Are there sufficient differences in wind by month to make predictions as to which locale the data are sampled from?  A random forest can provide a first cut at assessing that.

Example code includes:  
```{r cache=TRUE}

windData <- bind_rows(klnk=klnk2016Wind, klas=klas2016Wind, kord=kord2016Wind, .id="source") %>%
    mutate(source=factor(source))
windData


set.seed(2005061331)
trainIdx <- sample(1:nrow(windData), round(0.7*nrow(windData), 0), replace=FALSE) %>% sort()

trainData <- windData[trainIdx, ]
testData <- windData[-trainIdx, ]

# Attempt a basic random forest on just the speed bucket, wind direction, and month
rfInit <- trainData %>%
    select(source, spdBucket, wd, monthfct) %>%
    randomForest::randomForest(source ~ spdBucket + wd + monthfct, data=.)
rfInit

# Attempt a basic random forest on just the speed bucket, wind direction, and month
rfmtry2 <- trainData %>%
    select(source, spdBucket, wd, monthfct) %>%
    randomForest::randomForest(source ~ spdBucket + wd + monthfct, data=., mtry=2)
rfmtry2

# Predictions and confusion matrices on test data
evalWindPredictions <- function(model, 
                                testData, 
                                printAll=TRUE,
                                printCM=printAll, 
                                printConfSummary=printAll, 
                                printConfTable=printAll, 
                                showPlots=TRUE
                                ) {
    
    # Get the predicted class and probabilities
    testClass <- predict(model, newdata=testData)
    testProbs <- predict(model, newdata=testData, type="prob")
    if (printCM) {
        print(caret::confusionMatrix(testClass, testData$source))
    }
    
    # Create a tibble containing class prediction, maximum probability, and individual predictions
    tblProbs <- tibble::as_tibble(testProbs) %>%
        mutate(maxProb=apply(., 1, FUN=max), 
               sumProb=apply(., 1, FUN=sum), 
               predClass=testClass, 
               source=testData$source, 
               accurate=(predClass==source)
               )
    
    # Describe the maximum probability by source
    if (printConfSummary) {
        tblProbs %>%
            group_by(source) %>%
            summarize(meanMax=mean(maxProb), medianMax=median(maxProb), 
                      pct90Plus=mean(maxProb > 0.9), pct50Minus=mean(maxProb < 0.5)
                      ) %>%
        print()
    }
    
    # Create a table of accuracy by source and prediction confidence
    p1Data <- tblProbs %>%
        mutate(predProb=0.5 * round(2*maxProb, 1)) %>%
        group_by(predProb, source) %>%
        summarize(pctCorrect=mean(accurate), nCorrect=sum(accurate), nObs=n())
    
    p1Print <- p1Data %>%
        group_by(predProb) %>%
        summarize(nCorrect=sum(nCorrect), nObs=sum(nObs)) %>%
        mutate(pctCorrect=nCorrect/nObs)
    if (printConfTable) {
        print(p1Print)
    }
    
    cat("\nMean Error-Squared Between Confidence of Prediction and Accuracy of Precition\n")
    p1Print %>%
        mutate(err2=nObs*(pctCorrect-predProb)**2) %>%
        summarize(meanError2=sum(err2)/sum(nObs)) %>%
        print()
    
    # Plot the maximum probability forecasted by row
    p1 <- p1Data %>%
        ggplot(aes(x=predProb)) +
        geom_col(aes(y=nObs, fill=source)) + 
        labs(x="Maximum probability predicted", y="# Observations", 
             title="Count of Maximum Probability Predicted by Locale"
             )
    p2 <- p1Data %>%
        ggplot(aes(x=predProb)) +
        geom_line(aes(y=pctCorrect, group=source, color=source)) + 
        geom_abline(aes(intercept=0, slope=1), lty=2) +
        ylim(c(0, 1)) + 
        labs(x="Maximum probability predicted", y="Actual Probability Correct", 
             title="Accuracy of Maximum Probability Predicted by Locale"
             )
    
    if (showPlots) {
        print(p1)
        print(p2)
    }
    
    tblProbs
    
}

rfInitAccData <- evalWindPredictions(rfInit, testData=testData)
rfmtry2AccData <- evalWindPredictions(rfmtry2, testData=testData)

```
  
So, a basic random forest gets to 54% accuracy with the null accuracy being 34%.  However, the model is very over-confident in many of its predictions, perhaps driven by so many permutations of the factors (12 months, 12 wind directions, 3 wind speeds) with so few days.  So, rather than learning light and strong winds, the model may just be memorizing the single data point in certain grids.

Can accuracy or at least over-confidence be addressed by allowing the model to use the actual numeric wind speed?
```{r cache=TRUE}

# Attempt a basic random forest on just the speed bucket, wind direction, and month
rfmtry2Num <- trainData %>%
    select(source, spdW, wd, monthfct) %>%
    randomForest::randomForest(source ~ spdW + wd + monthfct, data=., mtry=2)
rfmtry2Num

# Print accuracy and confidence
rfmtry2NumAccData <- evalWindPredictions(rfmtry2Num, testData=testData)

```
  
The model is a touch less over-confident, though overall prediction accuracy remains at 54%.

So, still much work to do to make more meaningful predictions.  There is a lot of overlap among the wind data by city, so it is surprising that the model has so many high-confidence predictions that do not actually work out.

Further avenues to explore include 1) a lot more data for each city, and 2) further compressing the levels for the factors so that trends are learned rather than individual data points being memorized.
  
#### _Example #43: Further Predicting Locale Based on Wind Data_  
Is the model appropriately confident on the training data?

Example code includes:  
```{r}

library(caret)
library(randomForest)

rfInitAccDataTrain <- evalWindPredictions(rfInit, testData=trainData)
rfmtry2AccDataTrain <- evalWindPredictions(rfmtry2, testData=trainData)
rfmtry2NumAccDataTrain <- evalWindPredictions(rfmtry2Num, testData=trainData)

```
  
The accuracy is somewhat greater with the training data, and the confidence of the predictions is visually somewhat more aligned with the accuracy, though the model is over-confident even on the training data.

Forcing the trees to be less complex is an option to avoid over-fitting and over-confidence.  As an example, with fewer trees and larger nodes:  
```{r cache=TRUE}

# Attempt a basic random forest on just the speed bucket, wind direction, and month
rfSmaller <- trainData %>%
    select(source, spdW, wd, monthfct) %>%
    randomForest::randomForest(source ~ spdW + wd + monthfct, data=., mtry=2, nodesize=5)
rfSmaller

# Print accuracy and confidence
rfSmallerAccData <- evalWindPredictions(rfSmaller, testData=trainData)
rfSmallerAccData <- evalWindPredictions(rfSmaller, testData=testData)

```

The impact is modest.  Perhaps restricting the maximum number of nodes can improve performance:  
```{r cache=TRUE}

# Attempt a basic random forest on just the speed bucket, wind direction, and month
rfSmaller <- trainData %>%
    select(source, spdW, wd, monthfct) %>%
    randomForest::randomForest(source ~ spdW + wd + monthfct, data=., mtry=2, nodesize=25, ntree=100)
rfSmaller

# Print accuracy and confidence
rfSmallerAccData <- evalWindPredictions(rfSmaller, testData=trainData)
rfSmallerAccData <- evalWindPredictions(rfSmaller, testData=testData)

```

It is encouraging that the accuracy stayed flat or even slightly improved, while the confidence in the predictions is reduced.

Perhaps the caret::train() functionality can help drill-down on some better parameters.  Since there are very few variables, there are only 3 possible combinations with mtry=2.  So, even ntree=50 should provide plenty of opportunity to explore the variable and observations spaces.  Modelling is switched to ranger::ranger() with slight change in parameter names.

```{r cache=TRUE}

trGrid <- expand.grid(min.node.size=c(1, 5, 10, 25, 100), mtry=c(1, 2), splitrule=c("gini"))

caretModel <- caret::train(source ~ spdW + wd + monthfct, 
                           data=trainData,
                           method="ranger",
                           tuneGrid=trGrid,
                           trControl=caret::trainControl(method="cv", number=5),
                           num.trees=50
                           )
caretModel

# Run the best parameters from ranger in randomForest
caretBest <- randomForest::randomForest(source ~ spdW + wd + monthfct, 
                                        data=trainData,
                                        ntree=50, 
                                        nodesize=10, 
                                        mtry=2
                                        )

caretAccData <- evalWindPredictions(caretBest, testData=trainData)
caretAccData <- evalWindPredictions(caretBest, testData=testData)

```
  
There is still no significant gain in accuracy, and the overconfidence issue persists.  Ultimtately, wind speed and direction by month provides only modest predictive power for whether the data are taken from Las Vegas, Lincoln, or Chicago.  The sensitivities suggest better ability to correctly classify Las Vegas (63%), which is reasonable given that winds in Lincoln and Chicago will both tend to be wintry for chunks of the year.
  
#### _Example #43: Predicting Locale Based on Additional METAR Data_  
Winds provide some differentiation between Las Vegas, Chicago, and Lincoln.  Two additional areas to explore include:  
  
1.  Can the model perform better if it only needs to differentiate two locales with the most differences in wind - Las Vegas and Chicago?  
2.  Does adding temperature and dewpoint help further differentiate the locales?  
  
Example code includes:  
```{r}

# Differentiating Las Vegas and Chicago
windData <- bind_rows(klas=klas2016Wind, kord=kord2016Wind, .id="source") %>%
    mutate(source=factor(source))
windData


set.seed(2005071256)
trainIdx <- sample(1:nrow(windData), round(0.7*nrow(windData), 0), replace=FALSE) %>% sort()

trainData <- windData[trainIdx, ]
testData <- windData[-trainIdx, ]


# Create a tuning grid and run the models
trGrid <- expand.grid(min.node.size=c(1, 5, 10, 25, 100), mtry=c(1, 2), splitrule=c("gini"))

caretModel <- caret::train(source ~ spdW + wd + monthfct, 
                           data=trainData,
                           method="ranger",
                           tuneGrid=trGrid,
                           trControl=caret::trainControl(method="cv", number=5),
                           num.trees=50
                           )
caretModel


# Run the best parameters from ranger in randomForest
caretBest <- randomForest::randomForest(source ~ spdW + wd + monthfct, 
                                        data=trainData,
                                        ntree=50, 
                                        nodesize=25, 
                                        mtry=2
                                        )
caretBest


caretAccData <- evalWindPredictions(caretBest, testData=trainData)
caretAccData <- evalWindPredictions(caretBest, testData=testData)

```
  
Accuracy increases to 71%, compared with a baseline accuracy of 51% based on majority class.  So, the model is taking about 40% of the baseline errors and making accuract predictions for them.

The previous accuracy in predicting among 3 cities was 54%, compared with a baselne accurcay of 34% for the majority class.  So, the old models were taking about 30% of the baseline erros and making accuract predictions for them.
  
Is it in fact harder to differentiate Lincoln and Chicago?
```{r}

# Differentiating Lincoln and Chicago
windData <- bind_rows(klnk=klnk2016Wind, kord=kord2016Wind, .id="source") %>%
    mutate(source=factor(source))
windData


set.seed(2005071306)
trainIdx <- sample(1:nrow(windData), round(0.7*nrow(windData), 0), replace=FALSE) %>% sort()

trainData <- windData[trainIdx, ]
testData <- windData[-trainIdx, ]


# Create a tuning grid and run the models
trGrid <- expand.grid(min.node.size=c(1, 5, 10, 25, 100), mtry=c(1, 2), splitrule=c("gini"))

caretModel <- caret::train(source ~ spdW + wd + monthfct, 
                           data=trainData,
                           method="ranger",
                           tuneGrid=trGrid,
                           trControl=caret::trainControl(method="cv", number=5),
                           num.trees=50
                           )
caretModel


# Run the best parameters from ranger in randomForest
caretBest <- randomForest::randomForest(source ~ spdW + wd + monthfct, 
                                        data=trainData,
                                        ntree=50, 
                                        nodesize=100, 
                                        mtry=2
                                        )
caretBest


caretAccData <- evalWindPredictions(caretBest, testData=trainData)
caretAccData <- evalWindPredictions(caretBest, testData=testData)

```
  
Accuracy dips to 67%, so it is marginally harder to differentiate Lincoln and Chicago (both wintry cities near the east-west center of the US) than it is to differentiate from Las Vegas (a western desert city that experiences a different form of winter).
  
Can adding temperature and dewpoint help with the differentiation?  This factor will also tend to be much different in Las Vegas than in Lincoln or Chicago.  
```{r}

klnk2016WindTempDew <- plotWindData(klnk2016METAR, 
                                    subT="Lincoln, NE (2016)", 
                                    extraVars=c("TempF", "DewF"), 
                                    showPlots=FALSE
                                    )

klas2016WindTempDew <- plotWindData(klas2016METAR, 
                                    subT="Las Vegas, NV (2016)", 
                                    extraVars=c("TempF", "DewF"), 
                                    showPlots=FALSE
                                    )

kord2016WindTempDew <- plotWindData(kord2016METAR, 
                                    subT="Chicago, IL (2016)", 
                                    extraVars=c("TempF", "DewF"), 
                                    showPlots=FALSE
                                    )

```

```{r cache=TRUE}

# Full data for Lincoln, Chicago, Las Vegas
windTempDewData <- bind_rows(klnk=klnk2016WindTempDew,
                             kord=kord2016WindTempDew, 
                             klas=klas2016WindTempDew, 
                             .id="source"
                             ) %>%
    mutate(source=factor(source))
windTempDewData


set.seed(2005071329)
trainIdx <- sample(1:nrow(windTempDewData), round(0.7*nrow(windTempDewData), 0), replace=FALSE) %>% sort()

trainData <- windTempDewData[trainIdx, ]
testData <- windTempDewData[-trainIdx, ]


# Create a tuning grid and run the models
trGrid <- expand.grid(min.node.size=c(1, 5, 10, 25), mtry=c(1, 2, 3, 4, 5), splitrule=c("gini"))

caretModel <- caret::train(source ~ spdW + wd + monthfct + TempF + DewF, 
                           data=trainData,
                           method="ranger",
                           tuneGrid=trGrid,
                           trControl=caret::trainControl(method="cv", number=5),
                           num.trees=50
                           )
caretModel


# Run the best parameters from ranger in randomForest
caretBest <- randomForest::randomForest(source ~ spdW + wd + monthfct + TempF + DewF, 
                                        data=trainData,
                                        ntree=50, 
                                        nodesize=1, 
                                        mtry=5
                                        )
caretBest


caretAccData <- evalWindPredictions(caretBest, testData=trainData)
caretAccData <- evalWindPredictions(caretBest, testData=testData)

```
  
The model performs well even with parameters that would tend to lead to high variance - using all the variables in mtry and allowing a minimum node size of 5.  Despite this, confidence in classifying the test data is roughly in line with actual predictive power.
  
The overall prediction accuracy in the training data is 100% due to parameters that drive high variance and lack of any CV.  The ranger::ranger() call was used to establish the parameters, so this is not, in and of itself, overly concerning.

The overall prediction accuracy in the test data is 83%, consistent with the randomForest::randomForest() estimate of 18% OOB error.  The model is spectacular with Las Vegas, achieving 94% sensitivity and 97% specificity.  The model also does well differentiating Lincoln and Chicago, achieving sensitivities in the 75%-80% range and specificities just under 90%.  
```{r}

caretBest %>%
    varImp() %>%
    rownames_to_column() %>%
    mutate(var=factor(rowname)) %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(title="Temperature and Dew Point Best Separate Las Vegas, Lincoln, and Chicago", 
         subtitle="Variable Importance in Random Forest", 
         x="", 
         y="VarImp"
         )

```
  
So, temperature and dewpoint by month are significant helps in differentiating among the three cities.
  
#### _Example #44: Function for Locale Predictions_  
The locale predictions can be converted to functional form.  Functions include:  
  
* integrateLocaleData - bind rows for existing locale data  
* createTrainTestData - split a full dataset in to training and testing data  
* predictLocale - test user-specified options for mtry and min.node.size, pick the best, and report accuracy when applied to the training and testing data  

Example code includes:  
```{r}

# Function to bind rows of any number of processed METAR files
integrateLocaleData <- function(...) {

    localeData <- bind_rows(..., .id="source") %>%
        mutate(source=factor(source))
    
    localeData
    
}


# Function to split an integrated file in to test and train
createTrainTestData <- function(df, trainSize=0.7, seed=NULL) {
    
    if (!is.null(seed)) { 
        set.seed(seed)
    }
    
    trainIdx <- sample(1:nrow(df), round(trainSize*nrow(df), 0), replace=FALSE) %>% 
        sort()
    
    trainData <- df[trainIdx, ]
    testData <- df[-trainIdx, ]
    
    list(trainData=trainData, testData=testData, trainSize=trainSize, seed=seed)
}

predictLocale <- function(predFormula,
                          listTestTrain, 
                          mns=c(1, 5, 10, 25, 100), 
                          mtry=NULL,
                          ntree=50, 
                          seed=NULL
                          ) {

    # Test all the possible variables in mtry if passed as NULL
    if (is.null(mtry)) {
        mtry <- 1:length(labels(terms(predFormula)))
    }
    
    # Create a tuning grid and run the models
    trGrid <- expand.grid(min.node.size=mns, mtry=mtry, splitrule=c("gini"))

    # If the seed has been provided, apply it before running the first model
    if (!is.null(seed)) {
        set.seed(seed)
    }
    
    # Run the training process using the training data
    caretModel <- caret::train(predFormula, 
                               data=listTestTrain$trainData,
                               method="ranger",
                               tuneGrid=trGrid,
                               trControl=caret::trainControl(method="cv", number=5),
                               num.trees=ntree
                               )
    print(caretModel)
    
    # Extract the best parameters
    cat("\nThe best parameters will be used:\n")
    print(caretModel$bestTune)
    
    # Run the best parameters from ranger in randomForest
    caretBest <- randomForest::randomForest(predFormula, 
                                            data=listTestTrain$trainData,
                                            ntree=ntree, 
                                            nodesize=caretModel$bestTune$min.node.size, 
                                            mtry=caretModel$bestTune$mtry
                                            )
    print(caretBest)

    # Evaluate prediction power on training data and testing data
    trainAccData <- evalWindPredictions(caretBest, testData=listTestTrain$trainData, 
                                        printConfSummary=FALSE, printConfTable=FALSE
                                        )
    testAccData <- evalWindPredictions(caretBest, testData=listTestTrain$testData, 
                                       printConfSummary=FALSE, printConfTable=FALSE
                                       )
    
    trainAcc <- round(mean(trainAccData$accurate), 4)
    testAcc <- round(mean(testAccData$accurate), 4)
    
    # Print the overall accuracy of the predictions
    cat("\nTraining Accuracy:", trainAcc)
    cat("\nTesting Accuracy:", testAcc, "\n")
    
    list(formula=predFormula, 
         seed=seed,
         tuningModel=caretModel, 
         finalModel=caretBest, 
         trainAccData=trainAccData, 
         testAccData=testAccData,
         trainAcc=trainAcc,
         testAcc=testAcc
         )
    
}

```
  
The relevant dataset components can be created (no need to cache):  
```{r}

# Create the component datasets
klas2016 <- plotWindData(klas2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
klnk2016 <- plotWindData(klnk2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
kord2016 <- plotWindData(kord2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
kdtw2016 <- plotWindData(kdtw2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
kmsp2016 <- plotWindData(kmsp2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
kord2015 <- plotWindData(kord2015METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
kord2017 <- plotWindData(kord2017METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)

```
  
Subsets of the components are then integrated to full datasets, with training and testing data created (no need to cache):  
``` {r}

# Create the full 2016 dataset for Lincoln, Chicago, Las Vegas
fullDF001 <- integrateLocaleData(klnk=klnk2016,
                                 kord=kord2016, 
                                 klas=klas2016
                                 )

# Create the train data and the test data for the full 2016 dataset for Lincoln, Chicago, Las Vegas
listTestTrain001 <- createTrainTestData(fullDF001, trainSize=0.7, seed=2005071329)


# Create the full 2016 dataset for Chicago, Detroit, Minneapolis
fullDF002 <- integrateLocaleData(kord=kord2016,
                                 kdtw=kdtw2016, 
                                 kmsp=kmsp2016
                                 )

# Create the train data and the test data for the full 2016 dataset for Chicago, Detroit, Minneapolis
listTestTrain002 <- createTrainTestData(fullDF002, trainSize=0.7, seed=2005081434)


# Create the full dataset for Chicago 2015, 2016, 2017
fullDF003 <- integrateLocaleData(kord2015=kord2015,
                                 kord2016=kord2016, 
                                 kord2017=kord2017
                                 )

# Create the train data and the test data for the full 2016 dataset for Chicago, Detroit, Minneapolis
listTestTrain003 <- createTrainTestData(fullDF003, trainSize=0.7, seed=2005081436)

```

The modeling is re-run using the function for Chicago, Las Vegas, Lincoln, with accuracies on the training and testing data reported (cached):  
```{r cache=TRUE}

# Run the locale predictions
pL001 <- predictLocale(source ~ spdW + wd + monthfct + TempF + DewF, 
                       listTestTrain=listTestTrain001, 
                       seed=2005071329
                       )

```
  
As before, accuracy on the testing dataset is 83%, with very strong model performance in predicting Las Vegas and good but less accurate differentiation of Lincoln and Chicago.

Is the model largely pulling out that Las Vegas has a different winter?  Running on Minneapolis, Chicago, and Detroit is interesting since they are all wintry cities with somewhat different dynamics.  Minneapolis has the coldest and longest winters.  Cached code includes:  
```{r cache=TRUE}

# Run the locale predictions
pL002 <- predictLocale(source ~ spdW + wd + monthfct + TempF + DewF, 
                       listTestTrain=listTestTrain002, 
                       seed=2005081444
                       )

```
  
Accuracy on the testing dataset dips to 63%, with the model being roughly as good in predicting each of the cities (sensitivity for Minneapolis is 5% higher than the others, specificity is largely the same for all).  This is consistent with the 38% OOB error estimate in the final random forest model.  Prediction confidence on the training data is well aligned with actual prediction accuracy.

It is also interesting to consider whether the model has any accuracy in distinguishing among the years for the same city.  Chicago has a variable climate over the medium-term, so the model is run on the full-year 2015, 2016, and 2017 data for Chicago.  Cached code includes:  
```{r cache=TRUE}

# Run the locale predictions
pL003 <- predictLocale(source ~ spdW + wd + monthfct + TempF + DewF, 
                       listTestTrain=listTestTrain003, 
                       seed=2005081447
                       )

```

Interestingly, the model is 69% accurate in classifying Chicago 2015-2016-2017, which is 5% higher than the model accuracy in classifying Chicago, Detroit, Minneapolis all in the same year (2016).  Further extensions to look at cities with much less medium-term climate variation (perhaps coastal cities like San Diego or desert cities like Las Vegas) could be interesting as well.

Of note, the model does NOT have access to the day or the hour of observations, only to the month.  So, the model is not currently using things like February 12, 2016 was very warm in city A for predictions.  It only has access to Month, Wind Direction, Wind Speed, Temperature, and Dew Point.
  
#### _Example #45: Relationship Between Prediction Accuracy and Variables_  
The variable importance suggest the temperature and dewpoint are the most important for differentiating among Chicago, Las Vegas, and Lincoln.  Is this true for Chicago, Detroit, Minneapolis and also for Chicago 2015, 2016, 2017?

Example code includes:  
```{r}

pL001$finalModel %>%
    varImp() %>%
    rownames_to_column() %>%
    mutate(var=factor(rowname)) %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(title="Temperature and Dew Point Best Separate Las Vegas, Lincoln, and Chicago", 
         subtitle="Variable Importance in Random Forest", 
         x="", 
         y="VarImp"
         )


pL002$finalModel %>%
    varImp() %>%
    rownames_to_column() %>%
    mutate(var=factor(rowname)) %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(title="Temperature and Dew Point Best Separate Chicago, Detroit, and Minneapolis", 
         subtitle="Variable Importance in Random Forest", 
         x="", 
         y="VarImp"
         )


pL003$finalModel %>%
    varImp() %>%
    rownames_to_column() %>%
    mutate(var=factor(rowname)) %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(title="All 5 Variables Help Differentiate Chicago 2015-2016-2017", 
         subtitle="Variable Importance in Random Forest", 
         x="", 
         y="VarImp"
         )

```
  
So, temperature and dewpoint appear to be key variables in each of the three models, with the Chicago 2015-2016-2017 modeling being somewhat more evenly influenced by all 5 factors.

The prediction accuracies for Chicago, Las Vegas, Lincoln can then be assessed across each of the underlying variables:  
```{r}

assessAccuracy1D <- function(accList, 
                             ttList, 
                             numVars, 
                             fctVars,
                             mapVarNames
                             ) {

    # Integrate the test set predictions and the raw data file
    fullTestData <- bind_cols(accList[["testAccData"]] %>% rename(accSource=source),
                              ttList[["testData"]]
                              )

    # Grab the test set prediction accuracy
    testAcc <- accList[["testAcc"]]

    # Confirm that sources are the same across the files
    fullTestData %>%
        count(accSource, source) %>%
        print()

    # Create point plots with smooths for accuracy by numeric variables
    for (numVar in numVars) {
        pl <- fullTestData %>%
            mutate(accurate=as.integer(accurate)) %>%
            group_by_at(vars(all_of(c(numVar, "accurate")))) %>%
            summarize(n=n()) %>%
            mutate(nAcc=n*accurate) %>%
            group_by_at(vars(all_of(numVar))) %>%
            summarize(n=sum(n), nAcc=sum(nAcc)) %>%
            mutate(pctAcc=nAcc/n) %>%
            ggplot(aes_string(y="pctAcc", x=numVar)) + 
            geom_point(aes(size=n)) +
            geom_smooth() +
            ylim(c(0, 1)) + 
            geom_hline(yintercept=testAcc, lty=2) +
            labs(y="% Accuracy", x=mapVarNames[numVar], 
                 subtitle=paste0("Overall Accuracy on Test Data: ", round(100*testAcc, 1), "%"),
                 title=paste0("Accuracy of Classifications on Test Data By ", mapVarNames[numVar])
                 )
        print(pl)
    }
    
    
    # Create bar plots for accuracy by factor variables
    for (fctVar in fctVars) {
        pl <- fullTestData %>%
            ggplot(aes_string(fill="accurate", x=fctVar)) + 
            geom_bar(position="fill") +
            geom_hline(yintercept=testAcc, lty=2) +
            labs(y="% Accuracy", x=mapVarNames[fctVar], 
                 subtitle=paste0("Overall Accuracy on Test Data: ", round(100*testAcc, 1), "%"),
                 title=paste0("Accuracy of Classifications on Test Data By ", mapVarNames[fctVar])
                 )
        print(pl)
    }
    
}


# Create a mapping for the variable names to better plotting names
mapVarNames <- c(DewF="Dew Point (F)", 
                 TempF="Temperature (F)", 
                 spdW="Wind Speed (kts)",
                 wd="Prevailing Wind Direction", 
                 monthfct="Month"
                 )

# Assess Accuracy by Dimension for Chicago, Las Vegas, Lincoln
assessAccuracy1D(pL001, 
                 listTestTrain001, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
Prediction accuracy is high for all values of the key modeled variables.  There is some variation:  
  
* Model accuracy is slightly higher for wind speeds above 15 knots, though these are rare  
* Model accuracy is higher for extreme temperatures (under 20F and above 90F) than for modest temperatures  
* Model accuracy is higher for dew points under 20F and lower for dew points above 40F  
* Model accuracy is very similar by month and wind direction  
  
It is interesting then to look at a 2D plot of temperatures and dew points:  
```{r}

listTestTrain001[["testData"]] %>%
    count(TempF, DewF, source) %>%
    ggplot(aes(x=TempF, y=DewF, size=n, color=source)) + 
    geom_point(alpha=0.3)

```
  
Las Vegas is very clearly differentiated from Chicago and Lincoln when looking at both dew point and temperature.  In fact, how well would a 3-cluster k-means do in separating the data?
  
```{r}

# Pull key data from the test-train file
keyData <- listTestTrain001[["testData"]] %>%
    select(TempF, DewF, source)

# Scale the numeric data for k-means
keyMatrix <- keyData %>%
    select(-source) %>%
    scale()

set.seed(2005091500)

# Run k-means with 3 centers
kmModel <- kmeans(keyMatrix, centers=3)

# Augment the key data with the cluster results
keyData <- keyData %>%
    mutate(cluster=factor(kmModel$cluster))

# Summarize the clusters
keyData %>%
    group_by(cluster) %>%
    summarize(meanDewF=mean(DewF), meanTempF=mean(TempF), n=n(), 
              pctLAS=mean(source=="klas"), pctORD=mean(source=="kord"), pctLNK=mean(source=="klnk")
              )

# Plot the clusters
keyData %>%
    count(cluster, TempF, DewF) %>%
    ggplot(aes(x=TempF, y=DewF, size=n, color=cluster)) + 
    geom_point()

```
  
Much of the model's success is from the observation that high temperature with low dew point occurs ~25% of the time and is almost always Las Vegas.  Chicago and Lincoln tend to have temperatures and dew points that travel together, and a very simple k-means struggles to pull apart which is which.
  
Suppose that a different number of clusters were attempted - where is the elbow point?  
```{r}

kVals <- 1:20
bSS <- numeric(length(kVals))
wSS <- numeric(length(kVals))
silh <- numeric(length(kVals))
mtxDist <- dist(keyMatrix)

for (x in seq_along(kVals)) {

    kmModel <- kmeans(keyMatrix, centers=kVals[x])
    bSS[x] <- kmModel$betweenss
    wSS[x] <- kmModel$tot.withinss
    if (kVals[x]==1) { 
        silh[x] <- 0
    } else {
        sData <- cluster::silhouette(kmModel$cluster, mtxDist)
        silh[x] <- apply(sData, 2, FUN=mean)[3]
    }
    
}

data.frame(centers=kVals, withinSS=wSS) %>%
    ggplot(aes(x=centers, y=withinSS)) + 
    geom_point() + 
    geom_line() + 
    labs(x="# Clusters", y="Total Within Sum-Squares") + 
    ylim(c(0, NA))

data.frame(centers=kVals, sWidth=silh) %>%
    ggplot(aes(x=centers, y=sWidth)) + 
    geom_point() + 
    geom_line() + 
    labs(x="# Clusters", y="Silhouette Width (mean)") + 
    ylim(c(0, NA))

```
  
While there is no sharp elbow, there is some meaningful improvement out to 5 clusters.  The best silhouette width is at 2 or 3 (sensibly, as there are 3 cities and the splitting is largely Las Vegas vs. not Las Vegas).  For curiosity, the model is run again for 5 clusters:  
```{r}

# Run k-means with 5 centers
kmModel <- kmeans(keyMatrix, centers=5)

# Augment the key data with the cluster results
keyData <- keyData %>%
    mutate(cluster=factor(kmModel$cluster))

# Summarize the clusters
keyData %>%
    group_by(cluster) %>%
    summarize(meanDewF=mean(DewF), meanTempF=mean(TempF), n=n(), 
              pctLAS=mean(source=="klas"), pctORD=mean(source=="kord"), pctLNK=mean(source=="klnk")
              )

# Plot the clusters
keyData %>%
    count(cluster, TempF, DewF) %>%
    ggplot(aes(x=TempF, y=DewF, size=n, color=cluster)) + 
    geom_point()

```
  
Expanding to 5 clusters does little to help break apart Lincoln and Chicago.  There are just more buckets of temperature-dewpoint (high-high, low-low, medium-medium).  And, Las Vegas has two clusters of very high temperature with low dew point (representing ~25% of the data in total) almost all to itself.

This suggests that while temperature and dew point are important, the random forest (which is about 75% accurate in separating Chicago and Lincoln) either takes advantage of the month and wind data also, or using sophisticated tree splits to tease out more granular relationships between temperature and dew point.
  
#### _Example #46: Splitting Two Similar Cities_  
There are many similarities between Chicago and Lincoln on the variables of interest for this analysis.  A dataset for just these two cities is created and explored.
  
Example code includes:  
```{r}

# Create the full 2016 dataset for Lincoln and Chicago
fullDF004 <- integrateLocaleData(klnk=klnk2016,
                                 kord=kord2016
                                 )

# Create the train data and the test data for the full 2016 dataset for Lincoln and Chicago
listTestTrain004 <- createTrainTestData(fullDF004, trainSize=0.7, seed=2005101431)

# Plot the temperature, dewpoint, and wind speed by city and month
fullDF001 %>%
    ggplot(aes(x=monthfct, y=TempF)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Temperature (F) Boxplots by Month")

fullDF001 %>%
    ggplot(aes(x=monthfct, y=DewF)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Dew Point (F) Boxplots by Month")

fullDF001 %>%
    ggplot(aes(x=monthfct, y=spdW)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Wind Speed (knots) Boxplots by Month")

```
  
Las Vegas stands out significantly as having hotter temperatures and more stable dew points (leading to the cluster of high temperature, low dew point).  Lincoln and Chicago have fairly similar profiles of wintry cities - temperatures and dew points both soar in the summer and plummet in the winter.
  
Next, suppose that the model can use all of the information to split apart Chicago and Lincoln:  
```{r cache=TRUE}

# Run the locale predictions
pL004 <- predictLocale(source ~ spdW + wd + monthfct + TempF + DewF, 
                       listTestTrain=listTestTrain004, 
                       seed=2005101441
                       )

pL004$finalModel %>%
    varImp() %>%
    rownames_to_column() %>%
    mutate(var=factor(rowname)) %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(title="All 5 Variables Help Differentiate Chicago and Lincoln (2016)", 
         subtitle="Variable Importance in Random Forest", 
         x="", 
         y="VarImp"
         )

# Assess Accuracy by Dimension for Chicago and Lincoln (2016)
assessAccuracy1D(pL004, 
                 listTestTrain004, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
The model is 79% accurate in classifying Chicago and Lincoln.  The most important variables continue to be Temperature and Dew Point, though it is notable that prediction accuracy is worse with calm or variable winds and best with strong winds (not many data points here).

Suppose the model were restricted to just the two most important variables - temperature and dew point.  
```{r cache=TRUE}

# Run the locale predictions
pL004td <- predictLocale(source ~ TempF + DewF, 
                       listTestTrain=listTestTrain004, 
                       seed=2005101441
                       )

# Assess Accuracy by Dimension for Chicago and Lincoln (2016)
assessAccuracy1D(pL004td, 
                 listTestTrain004, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
Accuracy falls to just under 60%, and the model becomes overly confident in its predictions.  The model also very slightly prefers mtry=2 and a large node size, so it self-selects parameters that are not overly likely to lead to overconfidence.  
  
Now suppose that the model can bring in the additional variable of wind speed:  
```{r cache=TRUE}

# Run the locale predictions
pL004tds <- predictLocale(source ~ TempF + DewF + spdW, 
                          listTestTrain=listTestTrain004, 
                          seed=2005101441
                          )

# Assess Accuracy by Dimension for Chicago and Lincoln (2016)
assessAccuracy1D(pL004tds, 
                 listTestTrain004, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
Accuracy increases only very slightly to just over 60%.  There are not enough windy days for the predictive power there to drive much overall improvement.

Suppose instead that the model is augmented with wind direction:  
```{r cache=TRUE}

# Run the locale predictions
pL004tdw <- predictLocale(source ~ TempF + DewF + wd, 
                          listTestTrain=listTestTrain004, 
                          seed=2005101441
                          )

# Assess Accuracy by Dimension for Chicago and Lincoln (2016)
assessAccuracy1D(pL004tdw, 
                 listTestTrain004, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
Wind direction is much more helpful than wind speed, with accuracy increasing to 69%.  Is this more valuable than knowing the month (much of the wind speed difference is Chicago having predominantly W winds rather than the N/S winds of Lincoln in the winter)?  
```{r cache=TRUE}

# Run the locale predictions
pL004tdm <- predictLocale(source ~ TempF + DewF + monthfct, 
                          listTestTrain=listTestTrain004, 
                          seed=2005101441
                          )

# Assess Accuracy by Dimension for Chicago and Lincoln (2016)
assessAccuracy1D(pL004tdm, 
                 listTestTrain004, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```

Knowing the month and the wind direction are roughly equally as helpful, with each driving accuracy to 65%-70%.  Assuming that the model already has access to temperature, dew point, and wind direction (accuracy 69%), how helpful is it to add wind speed?  
```{r cache=TRUE}

# Run the locale predictions
pL004tdws <- predictLocale(source ~ TempF + DewF + spdW + wd, 
                          listTestTrain=listTestTrain004, 
                          seed=2005101441
                          )

# Assess Accuracy by Dimension for Chicago and Lincoln (2016)
assessAccuracy1D(pL004tdws, 
                 listTestTrain004, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
Adding wind speed has only a minor impact.  If instead month is added:  
```{r cache=TRUE}

# Run the locale predictions
pL004tdwm <- predictLocale(source ~ TempF + DewF + spdW + monthfct, 
                          listTestTrain=listTestTrain004, 
                          seed=2005101441
                          )

# Assess Accuracy by Dimension for Chicago and Lincoln (2016)
assessAccuracy1D(pL004tdwm, 
                 listTestTrain004, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```

Adding month has the same minor impact.  So, in summary:

* Temperature and Dew Point - 59% accuracy  
* Temperature and Dew Point and Wind Speed - 62% accuracy  
* Temperature and Dew Point and Month - 66% accuracy  
* Temperature and Dew Point and Wind Direction - 69% accuracy  
* Temperature and Dew Point and Wind Direction and Wind Speed - 68% accuracy  
* Temperature and Dew Point and Wind Speed and Month - 70% accuracy  
* Temperature and Dew Point and Wind Direction and Wind Speed and Month - 79% accuracy  
  
This shows the power of the random forest in teasing out small gains from incremental patterns observed in the interactions of the variables.  With just wind speed and dew point, the model behaves similarly to something that is correct 20% of the time and guessing a coin flip 80% of the time.  With all 5 variables included, the model behaves similarly to something that is correct 60% of the time and guessing a coin flip 40% of the time.  So, the power has in a sense tripled even though there is not a ton of one-dimensional differentiation on the variables.
  
#### _Example #47: Applying Predictions to Other Years_  
The model shows good ability to differentiate Chicago 2016 and Lincoln 2016.  The model has previously shown good ability to differentiate Chicago 2015, Chicago 2016, and Chicago 2017.  To what extent is the model learning what weather is like generally in Chicago and Lincoln, and to what extent is the model learning specific vagaries of the year 2016?

Data already have been pulled for Chicago 2015 and Chicago 2017, and these can be run through the prediction process.

Example code includes:  
```{r}

# Integrate the data for Chicago 2015-2017 (available in listTestTrain003)
# Include the predictions on this integrated data from pL004$finalModel
chiData1517 <- bind_rows(train=listTestTrain003$trainData, 
                         test=listTestTrain003$testData, 
                         .id="testTrain"
                         ) %>%
    mutate(origSource=as.character(source), 
           source="kord", 
           year=str_sub(origSource, 5, -1), 
           pred=predict(pL004$finalModel, newdata=.)
           ) %>%
    filter(year %in% c("2015", "2017"))


# Integrate the data for Chicago 2016 (available in listTestTrain004)
# Include the predictions on this integrated data from pL004$finalModel
chiData16 <- bind_rows(train=listTestTrain004$trainData, 
                       test=listTestTrain004$testData, 
                       .id="testTrain"
                       ) %>%
    mutate(origSource=paste0(source, "2016"), 
           year=str_sub(origSource, 5, -1), 
           pred=predict(pL004$finalModel, newdata=.), 
           source=as.character(source),
           ) %>%
    filter(source=="kord")


# Integrate the full dataset
chiData <- bind_rows(chiData1517, chiData16)
str(chiData)


# Assess the probability of success based on year and test-train data
overallAccuracy <- chiData %>%
    summarize(overallAccuracy=mean(pred=="kord")) %>%
    pull(overallAccuracy)
overallAccuracy

chiData %>%
    group_by(testTrain, year) %>%
    summarize(annualAccuracy=mean(pred=="kord")) %>%
    ggplot(aes(x=year, y=annualAccuracy)) +
    geom_col(fill="lightblue") + 
    labs(title="Percent of Accurately Predicting Chicago on Data by Year", x="Year", y="Accuracy", 
         subtitle=paste0("Overall Accuracy: ", round(100*overallAccuracy, 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(0.5, overallAccuracy), lty=2) + 
    facet_wrap(~ testTrain) + 
    geom_text(y=0.25, aes(label=paste0(round(100*annualAccuracy, 0), "%")))

```
  
The model has "seen" the 2016 training data, so it fits 95% to that.  Per previous, the model is ~75% accurate in predicting unseen 2016 data from Chicago or Lincoln.

The model clearly learned best by memorizing the training data for Chicago 2016.  Model accuracy on unseen data is ~75% in 2016, ~65% in 2015, and ~60% in 2017.  So, the model has partially learned to separate Chicago from Lincoln generally, and partially learned features specific to the 2016 weather in each locale.  This is consistent with the model being able to pull apart Chicago 2015-2016-2017, which would not be possible if Chicago weather were "exactly the same" every year.

Does this apply also when the model looks at two very different cities such as Chicago and Las Vegas?  
```{r}

# Create the full 2016 dataset for Las Vegas and Chicago
fullDF005 <- integrateLocaleData(klas=klas2016,
                                 kord=kord2016
                                 )

# Create the train data and the test data for the full 2016 dataset for Las Vegas and Chicago
listTestTrain005 <- createTrainTestData(fullDF005, trainSize=0.7, seed=2005111509)

# Plot the temperature, dewpoint, and wind speed by city and month
fullDF005 %>%
    ggplot(aes(x=monthfct, y=TempF)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Temperature (F) Boxplots by Month")

fullDF005 %>%
    ggplot(aes(x=monthfct, y=DewF)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Dew Point (F) Boxplots by Month")

fullDF005 %>%
    ggplot(aes(x=monthfct, y=spdW)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Wind Speed (knots) Boxplots by Month")

```
  
There are some significant differences, with Las Vegas running warmer and with more stable dew points.  A random forest is modeled:  
```{r cache=TRUE}

# Run the locale predictions
pL005 <- predictLocale(source ~ spdW + wd + monthfct + TempF + DewF, 
                       listTestTrain=listTestTrain005, 
                       seed=2005101441
                       )

pL005$finalModel %>%
    varImp() %>%
    rownames_to_column() %>%
    mutate(var=factor(rowname)) %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(title="Temperature and Dew Point Differentiate Chicago and Las Vegas (2016)", 
         subtitle="Variable Importance in Random Forest", 
         x="", 
         y="VarImp"
         )

# Assess Accuracy by Dimension for Chicago and Las Vegas (2016)
assessAccuracy1D(pL005, 
                 listTestTrain005, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
The model achieves a 96% accuracy in splitting Chicago and Las Vegas in 2016.  This is driven by the significantly different patterns in temperature and dew point.

K-means clustering would not pick this up so quickly:  
```{r}

# Pull key data from the test-train file
keyData <- listTestTrain005[["testData"]] %>%
    select(TempF, DewF, source)

# Scale the numeric data for k-means
keyMatrix <- keyData %>%
    select(-source) %>%
    scale()

set.seed(2005111417)

# Run k-means with 2 centers
kmModel <- kmeans(keyMatrix, centers=2)

# Augment the key data with the cluster results
keyData <- keyData %>%
    mutate(cluster=factor(kmModel$cluster))

# Summarize the clusters
keyData %>%
    group_by(cluster) %>%
    summarize(meanDewF=mean(DewF), meanTempF=mean(TempF), n=n(), 
              pctLAS=mean(source=="klas"), pctORD=mean(source=="kord")
              )

# Plot the clusters
keyData %>%
    count(cluster, TempF, DewF) %>%
    ggplot(aes(x=TempF, y=DewF, size=n, color=cluster)) + 
    geom_point()

```
  
The two-cluster k-means is insufficient, as there is too much of a linear relationship between dewpoint and temperature that is the first cut.  What does the silhouette width suggest?  
```{r}

kVals <- 1:20
bSS <- numeric(length(kVals))
wSS <- numeric(length(kVals))
silh <- numeric(length(kVals))
mtxDist <- dist(keyMatrix)

for (x in seq_along(kVals)) {

    kmModel <- kmeans(keyMatrix, centers=kVals[x])
    bSS[x] <- kmModel$betweenss
    wSS[x] <- kmModel$tot.withinss
    if (kVals[x]==1) { 
        silh[x] <- 0
    } else {
        sData <- cluster::silhouette(kmModel$cluster, mtxDist)
        silh[x] <- apply(sData, 2, FUN=mean)[3]
    }
    
}

data.frame(centers=kVals, withinSS=wSS) %>%
    ggplot(aes(x=centers, y=withinSS)) + 
    geom_point() + 
    geom_line() + 
    labs(x="# Clusters", y="Total Within Sum-Squares") + 
    ylim(c(0, NA))

data.frame(centers=kVals, sWidth=silh) %>%
    ggplot(aes(x=centers, y=sWidth)) + 
    geom_point() + 
    geom_line() + 
    labs(x="# Clusters", y="Silhouette Width (mean)") + 
    ylim(c(0, NA))

```
  
Interestingly, while there are only two cities, the silhouette width suggests 3 clusters:  
```{r}

set.seed(2005111421)

# Run k-means with 2 centers
kmModel <- kmeans(keyMatrix, centers=3)

# Augment the key data with the cluster results
keyData <- keyData %>%
    mutate(cluster=factor(kmModel$cluster))

# Summarize the clusters
keyData %>%
    group_by(cluster) %>%
    summarize(meanDewF=mean(DewF), meanTempF=mean(TempF), n=n(), 
              pctLAS=mean(source=="klas"), pctORD=mean(source=="kord")
              )

# Plot the clusters
keyData %>%
    count(cluster, TempF, DewF) %>%
    ggplot(aes(x=TempF, y=DewF, size=n, color=cluster)) + 
    geom_point()

```
  
The k-means picks out a 97% Las Vegas cluster, an 80% Chicago cluster, and a 60% Chicago cluster.  So, nowhere near the performance of the random forest.

How well does the random forest model project to 2015-2017 Chicago data?  
```{r}

# Integrate the data for Chicago 2015-2017 (available in listTestTrain003)
# Include the predictions on this integrated data from pL005$finalModel
chiData1517 <- bind_rows(train=listTestTrain003$trainData, 
                         test=listTestTrain003$testData, 
                         .id="testTrain"
                         ) %>%
    mutate(origSource=as.character(source), 
           source="kord", 
           year=str_sub(origSource, 5, -1), 
           pred=predict(pL005$finalModel, newdata=.)
           ) %>%
    filter(year %in% c("2015", "2017"))


# Integrate the data for Chicago 2016 (available in listTestTrain005)
# Include the predictions on this integrated data from pL005$finalModel
chiData16 <- bind_rows(train=listTestTrain005$trainData, 
                       test=listTestTrain005$testData, 
                       .id="testTrain"
                       ) %>%
    mutate(origSource=paste0(source, "2016"), 
           year=str_sub(origSource, 5, -1), 
           pred=predict(pL005$finalModel, newdata=.), 
           source=as.character(source),
           ) %>%
    filter(source=="kord")


# Integrate the full dataset
chiData <- bind_rows(chiData1517, chiData16)
str(chiData)


# Assess the probability of success based on year and test-train data
overallAccuracy <- chiData %>%
    summarize(overallAccuracy=mean(pred=="kord")) %>%
    pull(overallAccuracy)
overallAccuracy

chiData %>%
    group_by(testTrain, year) %>%
    summarize(annualAccuracy=mean(pred=="kord")) %>%
    ggplot(aes(x=year, y=annualAccuracy)) +
    geom_col(fill="lightblue") + 
    labs(title="Percent of Accurately Predicting Chicago on Data by Year", x="Year", y="Accuracy", 
         subtitle=paste0("Overall Accuracy: ", round(100*overallAccuracy, 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(0.5, overallAccuracy), lty=2) + 
    facet_wrap(~ testTrain) + 
    geom_text(y=0.25, aes(label=paste0(round(100*annualAccuracy, 0), "%")))

```
  
This model generalizes very nicely.  While it is 100% accurate on data that it has seen, it is over 90% accurate in predicting Chicago even on data and in years that it has never had access to.

So, the random model in this case is learning the general, recurring differences between weather in Chicago and Las Vegas.
  
#### _Example #48: Predictions Across Locales Including Cities with Similar Climates_  
The model appears to be learning a bit about generally differentiating two wintry cities (Chicago and Lincoln) and a lot about generally differentiating two cities from very different climates (Chicago and Las Vegas).

Suppose that the modeling is extended to look at eight cities from five relatively distinct climates:  
  
* Chicago - wintry mid-continental  
* Lincoln - wintry mid-continental  
* Detroit - wintry mid-continental  
* Las Vegas - desert  
* Minneapolis - cold  
* Houston - hot and humid  
* New Orleans - hot and humid  
* San Diego - marine influenced  
  
Processed data are already available for 2016 in Chicago, Lincoln, Detroit, Las Vegas, and Minneapolis.  As such, data need to be downloaded and processed for Houston 2016, New Orleans 2016, and San Diego 2016.  The download process is cached to avoid multiple hits to the Iowa State server:  
```{r cache=TRUE}

# Get data for IAH for 2016
getASOSStationTime(stationID="IAH", analysisYears=2016, ovrWrite=TRUE)

# Get data for MSY for 2016
getASOSStationTime(stationID="MSY", analysisYears=2016, ovrWrite=TRUE)

# Get data for SAN for 2016
getASOSStationTime(stationID="SAN", analysisYears=2016, ovrWrite=TRUE)

```

The data are then processed for Houston 2016:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kiah_2016.txt"  # file name for raw METAR data
timeZ <- "53Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:53:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Houston, TX"  # Description of city or location
shortMET <- "KIAH METAR (2016)"  # Station code and timing
longMET <- "Houston, TX (IAH) Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Chicago, IL (2015)
kiah2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kiah2016METAR)

```

The data are then processed for New Orleans 2016:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kmsy_2016.txt"  # file name for raw METAR data
timeZ <- "53Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:53:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "New Orleans, LA"  # Description of city or location
shortMET <- "KMSY METAR (2016)"  # Station code and timing
longMET <- "New Orleans, LA Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Chicago, IL (2015)
kmsy2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kmsy2016METAR)

```

The data are then processed for San Diego 2016:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_ksan_2016.txt"  # file name for raw METAR data
timeZ <- "51Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:51:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "San Diego, CA"  # Description of city or location
shortMET <- "KSAN METAR (2016)"  # Station code and timing
longMET <- "San Diego, CA Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Chicago, IL (2015)
ksan2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(ksan2016METAR)

```

The integrated database is then created:  
```{r}

# Create the full 2016 dataset for Chicago, Detroit, Lincoln, Minneapolis, 
# Las Vegas, Houston, New Orleans, San Diego

# Use plotWindData to extract properly formatted data for the new cities
kiah2016 <- plotWindData(kiah2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
kmsy2016 <- plotWindData(kmsy2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)
ksan2016 <- plotWindData(ksan2016METAR, subT="", extraVars=c("TempF", "DewF"), showPlots=FALSE)

# Create the full integrated dataset
fullDF006 <- integrateLocaleData(kord2016=kord2016,
                                 kdtw2016=kdtw2016,
                                 klnk2016=klnk2016,
                                 kmsp2016=kmsp2016,
                                 klas2016=klas2016, 
                                 kiah2016=kiah2016,
                                 kmsy2016=kmsy2016,
                                 ksan2016=ksan2016
                                 )

# Create the train data and the test data for the full 2016 dataset for eight cities
listTestTrain006 <- createTrainTestData(fullDF006, trainSize=0.7, seed=2005121344)


# Plot the temperature, dewpoint, and wind speed by city and month
fullDF006 %>%
    ggplot(aes(x=monthfct, y=TempF)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Temperature (F) Boxplots by Month")

fullDF006 %>%
    ggplot(aes(x=monthfct, y=DewF)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Dew Point (F) Boxplots by Month")

fullDF006 %>%
    ggplot(aes(x=monthfct, y=spdW)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~source) + 
    labs(title="Wind Speed (knots) Boxplots by Month")

```
  
There are some similarities and differences as expected based on the characteristics of the cities:  
  
* San Diego has a very similar temperature, dew point, and wind speed across all months  
* Las Vegas runs warm with dew points largely stable by month but with seasonal temperature fluctuations  
* Houston and New Orleans do not experience much of a temperature or dew point dip in winter and run warm to hot for most of the year  
* Chicago, Detroit, Lincoln, and Minneapolis all look similar at a first glance  
  
An initial random forest is attempted to break apart the cities:  
```{r cache=TRUE}

# Run the locale predictions
pL006 <- predictLocale(source ~ spdW + wd + monthfct + TempF + DewF, 
                       listTestTrain=listTestTrain006, 
                       ntree=100,
                       seed=2005121353
                       )

pL006$finalModel %>%
    varImp() %>%
    rownames_to_column() %>%
    mutate(var=factor(rowname)) %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(title="Temperature and Dew Point Best Differentiate 8 Cities (2016)", 
         subtitle="Variable Importance in Random Forest", 
         x="", 
         y="VarImp"
         )

# Assess Accuracy by Dimension for 8 Cities (2016)
assessAccuracy1D(pL006, 
                 listTestTrain006, 
                 numVars=c("spdW", "TempF", "DewF"), 
                 fctVars=c("wd", "monthfct"),
                 mapVarNames=mapVarNames
                 )

```
  
The model achieves a 64% accuracy on the testing data, compared with a 13% no-information accuracy. The probabilities derived from the model roughly match the overall accuracies, even as the model chooses the higher variance parameters (mtry=5, min.node.size=1).  Temperature and Dew Point are both strong variables for separating the cities.  

The confusion matrix can be augmented graphically for a look at accuracy by source city:  
```{r}

# Integrate the data for 8 cities 2016 (available in listTestTrain006)
# Include the predictions on this integrated data from pL006$finalModel
allData16 <- bind_rows(train=listTestTrain006$trainData, 
                       test=listTestTrain006$testData, 
                       .id="testTrain"
                       ) %>%
    mutate(origSource=as.character(source), 
           year=str_sub(origSource, 5, -1), 
           pred=as.character(predict(pL006$finalModel, newdata=.))
           ) %>%
    filter(year %in% c("2016"))

str(allData16)


# Assess the probability of success based on only the test data
overallAccuracy <- allData16 %>%
    filter(testTrain=="test") %>%
    summarize(overallAccuracy=mean(pred==origSource)) %>%
    pull(overallAccuracy)
overallAccuracy


cityNames <- c(kdtw2016="Detroit", kiah2016="Houston", klas2016="Las Vegas", klnk2016="Lincoln", 
               kmsp2016="Minneapolis", kmsy2016="New Orleans", kord2016="Chicago", ksan2016="San Diego"
               )

# Plot the accuracy on test data by locale
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource) %>%
    summarize(sourceAccuracy=mean(pred==origSource)) %>%
    ggplot(aes(x=fct_reorder(cityNames[origSource], -sourceAccuracy), y=sourceAccuracy)) +
    geom_col(fill="lightblue") + 
    labs(title="Percent of Accurately Predicting by Source City", x="City", y="Accuracy", 
         subtitle=paste0("Overall Test Set Accuracy: ", round(100*overallAccuracy, 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(0.125, overallAccuracy), lty=2) + 
    geom_text(y=0.25, aes(label=paste0(round(100*sourceAccuracy, 0), "%")))

```
  
On a per-city basis, the following are noted:  
  
* The model does very well predicting Las Vegas (87% sensitivity) and San Diego (81% sensitivity), with balanced accuracy of ~90% for each  
* The model does well predicting New Orleans (69% sensitivity) and Houston (65% sensitivity), with balanced accuracy of ~80% for each  
* The model does reasonably well on the remaining wintry cities, achieving sensitivities just above 50% and balanced accuracies just above 60% for all four  
  
When errors are made, which are the most common mis-classifications?  
```{r}

# Plot the mispredictions for test data by locale
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource, pred) %>%
    summarize(n=n()) %>%
    mutate(nTotal=sum(n), pctTotal=n/nTotal, class=ifelse(origSource==pred, "Correct", pred)) %>%
    filter(class!="Correct") %>%
    mutate(pctWrong=sum(pctTotal)) %>%
    ggplot(aes(x=fct_reorder(cityNames[origSource], -pctWrong), y=pctTotal, fill=class)) +
    geom_col(position="stack") + 
    labs(title="Prediction Error Frequency by Source City", x="City", y="% Predicted Errors", 
         subtitle=paste0("Overall Test Set Error Rate: ", round(100*(1-overallAccuracy), 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(1-overallAccuracy), lty=2)

```

The confusion matrix and plots show errors of the type that might be expected:  
  
* Las Vegas and San Diego are rarely mis-classified  
* Houston and New Orleans are most commonly mis-classified as each other  
* Chicago, Detroit, Lincoln, and Minneapolis are all mis-classified 10%-15% as each of the other cities  
  
This is suggestive that there are only four weather types in the data - Las Vegas, San Diego, Houston/New Orleans, and Chicago/Detroit/Lincoln/Minneapolis.  
  
#### _Example #49: Additional Diagnostics of Accuracy_  
Suppose that the classifications are graded as:  
  
* Correct  
* Incorrect, but similar  
* Incorrect  
  
What is the model performance by locale in this case?  
```{r}

localeCategory <- c(kord2016="Wintry", 
                    kdtw2016="Wintry", 
                    klnk2016="Wintry", 
                    kmsp2016="Wintry", 
                    klas2016="Desert", 
                    kiah2016="Hot-Humid", 
                    kmsy2016="Hot-Humid", 
                    ksan2016="Marine"
                    )


# Plot the predictions for test data by locale
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource, pred) %>%
    summarize(n=n()) %>%
    mutate(origCat=localeCategory[origSource], predCat=localeCategory[pred], 
           nTotal=sum(n), pctTotal=n/nTotal, 
           class=ifelse(origSource==pred, "Correct", ifelse(origCat==predCat, "Similar", "Different")), 
           pctCorrect=sum(ifelse(class=="Correct", pctTotal, 0)), 
           pctSimilar=sum(ifelse(class=="Similar", pctTotal, 0)),
           pctDifferent=sum(ifelse(class=="Different", pctTotal, 0))
           ) %>%
    ggplot(aes(x=fct_reorder(cityNames[origSource], -pctCorrect), y=pctTotal, 
               fill=factor(class, levels=c("Different", "Similar", "Correct"))
               )
           ) +
    geom_col(position="stack") + 
    labs(title="Predictions by Source City", x="City", y="% Predicted", 
         subtitle=paste0("Overall Test Set Accuracy: ", round(100*(overallAccuracy), 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(overallAccuracy), lty=2) + 
    scale_fill_manual("", values=c("red", "orange", "seagreen")) + 
    geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect, 0), "%")), color="white") +
    geom_text(aes(y=1-pctDifferent/2, label=paste0(round(100*pctDifferent, 0), "%")), color="white") + 
    geom_text(aes(y=pctCorrect + pctSimilar/2, 
                  label=ifelse(pctSimilar==0, "", paste0(round(100*pctSimilar, 0), "%")))
              )

```
  
Full misclassifications to a locale with a different climate occur ~10% of the time, with the most frequently misclassified city being San Diego and the least misclassified cities being Minneapolis and Chicago.
  
Are there any patterns to when cities are misclassified?
```{r}

# Plot the predictions for test data by month
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource, pred, month) %>%
    summarize(n=n()) %>%
    mutate(origCat=localeCategory[origSource], 
           predCat=localeCategory[pred], 
           class=ifelse(origSource==pred, "Correct", ifelse(origCat==predCat, "Similar", "Different"))
           ) %>%
    group_by(month, class) %>%
    summarize(n=sum(n)) %>%
    mutate(pctTotal=n/sum(n)) %>%
    ggplot(aes(x=factor(month.abb[month], levels=month.abb[1:12]), y=pctTotal, 
               fill=factor(class, levels=c("Different", "Similar", "Correct"))
               )
           ) +
    geom_col(position="stack") + 
    labs(title="Predictions by Month", x="Month", y="% Predicted", 
         subtitle=paste0("Overall Test Set Accuracy: ", round(100*(overallAccuracy), 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(overallAccuracy), lty=2) + 
    scale_fill_manual("", values=c("red", "orange", "seagreen"))

```
  
The predictions do not get meaningfully better or worse by month.  

```{r}

# Plot the predictions for test data by wind direction
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource, pred, wd) %>%
    summarize(n=n()) %>%
    mutate(origCat=localeCategory[origSource], 
           predCat=localeCategory[pred], 
           class=ifelse(origSource==pred, "Correct", ifelse(origCat==predCat, "Similar", "Different"))
           ) %>%
    group_by(wd, class) %>%
    summarize(n=sum(n)) %>%
    mutate(pctTotal=n/sum(n), 
           pctDifferent=sum(ifelse(class=="Different", pctTotal, 0)), 
           pctCorrect=sum(ifelse(class=="Correct", pctTotal, 0))) %>%
    ggplot(aes(x=wd, y=pctTotal, 
               fill=factor(class, levels=c("Different", "Similar", "Correct"))
               )
           ) +
    geom_col(position="stack") + 
    labs(title="Predictions by Wind Direction", x="", y="% Predicted", 
         subtitle=paste0("Overall Test Set Accuracy: ", round(100*(overallAccuracy), 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(overallAccuracy), lty=2) + 
    scale_fill_manual("", values=c("red", "orange", "seagreen")) + 
    geom_text(aes(y=1-pctDifferent/2, label=paste0(round(100*pctDifferent, 0), "%")), color="white") + 
    geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect, 0), "%")), color="white")

```
  
Prediction classifications improve when there is a direction to the wind (winds are other than calm or variable)
  
```{r}

# Plot the predictions for test data by wind speed bucket
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource, pred, spdBucket) %>%
    summarize(n=n()) %>%
    mutate(origCat=localeCategory[origSource], 
           predCat=localeCategory[pred], 
           class=ifelse(origSource==pred, "Correct", ifelse(origCat==predCat, "Similar", "Different"))
           ) %>%
    group_by(spdBucket, class) %>%
    summarize(n=sum(n)) %>%
    mutate(pctTotal=n/sum(n), 
           pctDifferent=sum(ifelse(class=="Different", pctTotal, 0)), 
           pctCorrect=sum(ifelse(class=="Correct", pctTotal, 0))) %>%
    ggplot(aes(x=spdBucket, y=pctTotal, 
               fill=factor(class, levels=c("Different", "Similar", "Correct"))
               )
           ) +
    geom_col(position="stack") + 
    labs(title="Predictions by Wind Speed", x="", y="% Predicted", 
         subtitle=paste0("Overall Test Set Accuracy: ", round(100*(overallAccuracy), 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(overallAccuracy), lty=2) + 
    scale_fill_manual("", values=c("red", "orange", "seagreen")) + 
    geom_text(aes(y=1-pctDifferent/2, label=paste0(round(100*pctDifferent, 0), "%")), color="white") + 
    geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect, 0), "%")), color="white")

```
  
This shows clearly that strong winds help the model make correct classifications and avoid classifications to completely different climates.  
  
```{r}

# Plot the predictions for test data by temperature
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource, pred, TempF) %>%
    summarize(n=n()) %>%
    mutate(origCat=localeCategory[origSource], 
           predCat=localeCategory[pred], 
           class=ifelse(origSource==pred, "Correct", ifelse(origCat==predCat, "Similar", "Different")), 
           TempF=10*round(0.1*TempF, 0)
           ) %>%
    group_by(TempF, class) %>%
    summarize(n=sum(n)) %>%
    mutate(pctTotal=n/sum(n), 
           pctDifferent=sum(ifelse(class=="Different", pctTotal, 0)), 
           pctCorrect=sum(ifelse(class=="Correct", pctTotal, 0))) %>%
    ggplot(aes(x=TempF, y=pctTotal, 
               fill=factor(class, levels=c("Different", "Similar", "Correct"))
               )
           ) +
    geom_col(position="stack") + 
    labs(title="Predictions by Temperature", x="Temperature (F)", y="% Predicted", 
         subtitle=paste0("Overall Test Set Accuracy: ", round(100*(overallAccuracy), 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(overallAccuracy), lty=2) + 
    scale_fill_manual("", values=c("red", "orange", "seagreen")) + 
    geom_text(aes(y=1-pctDifferent/2, 
                  label=ifelse(pctDifferent==0, "", paste0(round(100*pctDifferent, 0), "%"))
                  ), 
              color="white"
              ) + 
    geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect, 0), "%")), color="white")

```
  
Prediction accuracy is highest at extreme temperatured, and prediction full mis-classification is greatest when there are moderate temperatures (50-90 F).

```{r}

# Plot the predictions for test data by dew point
allData16 %>%
    filter(testTrain=="test") %>%
    group_by(origSource, pred, DewF) %>%
    summarize(n=n()) %>%
    mutate(origCat=localeCategory[origSource], 
           predCat=localeCategory[pred], 
           class=ifelse(origSource==pred, "Correct", ifelse(origCat==predCat, "Similar", "Different")), 
           DewF=10*round(0.1*DewF, 0)
           ) %>%
    group_by(DewF, class) %>%
    summarize(n=sum(n)) %>%
    mutate(pctTotal=n/sum(n), 
           pctDifferent=sum(ifelse(class=="Different", pctTotal, 0)), 
           pctCorrect=sum(ifelse(class=="Correct", pctTotal, 0))) %>%
    ggplot(aes(x=DewF, y=pctTotal, 
               fill=factor(class, levels=c("Different", "Similar", "Correct"))
               )
           ) +
    geom_col(position="stack") + 
    labs(title="Predictions by Dew Point", x="Dew Point (F)", y="% Predicted", 
         subtitle=paste0("Overall Test Set Accuracy: ", round(100*(overallAccuracy), 1), "%")
         ) + 
    ylim(c(0, 1)) + 
    geom_hline(yintercept=c(overallAccuracy), lty=2) + 
    scale_fill_manual("", values=c("red", "orange", "seagreen")) + 
    geom_text(aes(y=1-pctDifferent/2, 
                  label=ifelse(pctDifferent==0, "", paste0(round(100*pctDifferent, 0), "%"))
                  ), 
              color="white"
              ) + 
    geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect, 0), "%")), color="white")

```
  
Full prediction misclassifications tend to occur most frequently for moderate-to-high dew points (30-70).  So, the model tends to perform better when there is at least some form of extreme condition - strong wind, high or low temperature, high or low dew point.  And, the model tends to perform worse when conditions are pleasant - calm or variable winds, moderate temperature, moderate dew point.

Suppose we create a test dataset of various temperatures and dew point for the month of June, assuming that the winds are calm.  What city would the model predict?  
```{r}

sampleTDData <- tibble::tibble(monthfct=factor(month.abb[6], levels=month.abb[1:12]), 
                               spdW=0, 
                               wd=factor("Variable", levels=levels(allData16$wd)),
                               TempF=rep(seq(40, 120, by=2), each=41),
                               DewF=rep(seq(0, 80, by=2), times=41)
                               )

sampleVotes <- predict(pL006$finalModel, newdata=sampleTDData, type="prob")
sampleClass <- predict(pL006$finalModel, newdata=sampleTDData)

sampleTDData <- sampleTDData %>%
    mutate(x=sampleClass, 
           pred=localeCategory[as.character(sampleClass)], 
           maxProb=apply(sampleVotes, 1, FUN=max)
           )

tdCounts <- allData16 %>%
    filter(month==6, spdW==0) %>%
    mutate(TempF=2*round(TempF/2, 0), DewF=2*round(DewF/2, 0)) %>%
    count(TempF, DewF)

sampleTDData %>%
    left_join(tdCounts, by=c("TempF", "DewF")) %>%
    mutate(n=ifelse(is.na(n), 0, n)) %>%
    filter(TempF >= DewF, n > 0) %>%
    ggplot(aes(x=TempF, y=DewF)) + 
    geom_point(aes(alpha=maxProb, color=pred, size=n)) + 
    geom_abline(intercept=0, slope=1, lty=2) + 
    labs(x="Temperature (F)", y="Dew Point (F)", title="Predictions by Temperature and Dew Point", 
         subtitle="Simulated Data for June, No Wind"
         ) + 
    scale_color_discrete("Prediction") + 
    scale_alpha_continuous("% Votes") + 
    scale_size_continuous("Prevalence\nin 2016")

```
  
At a glance, the decision boundaries seem reasonable:  
  
* The marine segment has a very tight range for both temperature and dew point.  If another climate type has this profile, it will likely be misclassified as "marine" in the absense of compelling wind data  
* The wintry cities generally have lower dew points than the hot-humid cities and higher dew points than the desert cities  
* Hot-humid and desert appear to be apt descriptions of the June temperatures and dew points for these clusters  
* Model confidence appears higher in the more differentiated portions of the boundary  
  
#### _Example #50: Data by Month by Locale_  
There is a seasonal nature to the weather in many locales, and these seasonal changes cause some variation in the data that may be useful for predicting.

To begin, a look at temperatures and dew points by month:  
```{r}

# Look at the 'Marine' locales by month
allData16 %>% 
    mutate(localeType=localeCategory[origSource], 
           TempF=round(0.2*TempF, 0)*5, 
           DewF=round(0.2*DewF, 0)*5
           ) %>% 
    count(month, localeType, TempF, DewF) %>% 
    arrange(month, TempF, DewF, -n) %>% 
    group_by(month, TempF, DewF) %>% 
    mutate(mainType=first(localeType), pctType=n/sum(n), nTotal=sum(n)) %>% 
    filter(localeType=="Marine") %>% 
    ggplot(aes(x=TempF, y=DewF)) + 
    geom_point(aes(color=mainType, alpha=pctType, size=nTotal)) + 
    facet_wrap(~ month) + 
    scale_alpha_continuous("% Marine at\nTemp and Dew") + 
    scale_color_discrete("Main Weather Type") + 
    scale_size_continuous("# Marine Obs") + 
    labs(title="Marine Temperature and Dew Point Observations by Month", 
         subtitle="Primary Locale Type Observed at Each Dew Point and Temperature", 
         x="Temperature (F)", 
         y="Dew Point (F)"
         )

```
  
The marine temperatures and dew points are reasonable distinct from the other groups in iDecember through May, though in June to September there are more wintry observations at these temperatures and dew points.
  
This is converted to functional form so that it can be run multiple times:  
```{r}

plotLocaleMonthFrequency <- function(df, var1, var2, locType) {

    # Create the analysis dataset
    dfPlot <- df %>%
        group_by_at(vars(all_of(c("month", "localeType", var1, var2)))) %>%
        summarize(n=n(), negN=-n()) %>%
        arrange_at(vars(all_of(c("month", var1, var2, "negN")))) %>% 
        group_by_at(vars(all_of(c("month", var1, var2)))) %>% 
        mutate(mainType=first(localeType), pctType=n/sum(n), nTotal=sum(n), maxPct=max(pctType)) %>% 
        filter(localeType==locType)
    
    # Plot using the analysis dataset
    p1 <- dfPlot %>%
        ggplot(aes_string(x=var1, y=var2)) + 
        geom_point(aes(color=mainType, alpha=pctType, size=nTotal)) + 
        facet_wrap(~ month) + 
        scale_alpha_continuous(paste0("% ", locType, " at\n", var1, " and ", var2)) + 
        scale_color_discrete("Main Locale Type") + 
        scale_size_continuous("# Marine Obs") + 
        labs(title=paste0(locType, 
                          " ", 
                          titleNames[var1], 
                          " and ", 
                          titleNames[var2], 
                          " Observations by Month"
                          ), 
             subtitle=paste0("Primary Locale Type Observed at Each ", 
                             titleNames[var1], 
                             " and ", 
                             titleNames[var2]
                             ), 
             x=axisNames[var1], 
             y=axisNames[var2]
             )
    print(p1)
    
    # Return the dfPlot data
    dfPlot
    
}


# Create a mapping of variables to names
titleNames <- c(TempF="Temperature", 
                DewF="Dew Point"
                )
axisNames <- c(TempF="Temperature (F)", 
               DewF="Dew Point (F)"
               )

# Create an analysis database
modData16 <- allData16 %>% 
    mutate(localeType=localeCategory[origSource], 
           TempF=round(0.2*TempF, 0)*5, 
           DewF=round(0.2*DewF, 0)*5
           )


# Create plot for 'Marine'
plotLocaleMonthFrequency(modData16, 
                         var1="TempF", 
                         var2="DewF", 
                         locType="Marine"
                         )

# Create plot for 'Desert'
plotLocaleMonthFrequency(modData16, 
                         var1="TempF", 
                         var2="DewF", 
                         locType="Desert"
                         )

# Create plot for 'Hot-Humid'
plotLocaleMonthFrequency(modData16, 
                         var1="TempF", 
                         var2="DewF", 
                         locType="Hot-Humid"
                         )

# Create plot for 'Wintry'
plotLocaleMonthFrequency(modData16, 
                         var1="TempF", 
                         var2="DewF", 
                         locType="Wintry"
                         )

```
  
There are some nice trends where each of the four locale types is the predominant locale in many of its combinations of temperature and dew point
  
An extension is carried out to look at data prevalence by locale and month:  
```{r}

# Create dummy data for calm winds
sampleTDData <- expand.grid(DewF=seq(-40, 100, by=2), 
                            TempF=seq(-40, 120, by=2), 
                            monthfct=factor(month.abb[1:12], levels=month.abb[1:12]), 
                            spdW=0, 
                            wd=factor("Variable", levels=levels(allData16$wd))
                            ) %>%
    tibble::as_tibble()

sampleVotes <- predict(pL006$finalModel, newdata=sampleTDData, type="prob")
sampleClass <- predict(pL006$finalModel, newdata=sampleTDData)

sampleTDData <- sampleTDData %>%
    mutate(x=sampleClass, 
           pred=localeCategory[as.character(sampleClass)], 
           maxProb=apply(sampleVotes, 1, FUN=max)
           )

tdCounts <- allData16 %>%
    filter(spdW==0) %>%
    mutate(TempF=2*round(TempF/2, 0), DewF=2*round(DewF/2, 0)) %>%
    count(monthfct, TempF, DewF)

sampleTDData %>%
    left_join(tdCounts, by=c("monthfct", "TempF", "DewF")) %>%
    mutate(n=ifelse(is.na(n), 0, n)) %>%
    filter(TempF >= DewF, n > 0) %>%
    ggplot(aes(x=TempF, y=DewF)) + 
    geom_point(aes(alpha=maxProb, color=pred, size=n)) + 
    geom_abline(intercept=0, slope=1, lty=2) + 
    labs(x="Temperature (F)", y="Dew Point (F)", title="Predictions by Temperature and Dew Point", 
         subtitle="Simulated Data, No Wind"
         ) + 
    scale_color_discrete("Prediction") + 
    scale_alpha_continuous("% Votes") + 
    scale_size_continuous("Prevalence\nin 2016") + 
    facet_wrap(~ monthfct)

```
  
The wintry cities separate nicely during the cold months while the Hot-Humid and Desert cities separate nicely during the warm months.  The Marine segment is compact, but frequently overlapping with at least one other weather type.

Are there any misclassifications that are more likely in a given month than on annual average?  
```{r}

plotLocaleAccuracyByMonth <- function(locType="All") {

    # Create the database
    plotData <- allData16 %>%
        filter(testTrain=="test") %>%
        group_by(origSource, pred, monthfct) %>%
        summarize(n=n()) %>%
        mutate(origCat=localeCategory[origSource], 
               predCat=localeCategory[pred], 
               class=ifelse(origSource==pred, "Correct", ifelse(origCat==predCat, "Similar", "Different"))
               ) %>%
        group_by(monthfct, origCat, class) %>%
        summarize(n=sum(n)) %>%
        mutate(pctTotal=n/sum(n), 
               pctDifferent=sum(ifelse(class=="Different", pctTotal, 0)), 
               pctCorrect=sum(ifelse(class=="Correct", pctTotal, 0)))
    
    if (locType!="All") {
        plotData <- plotData %>%
            filter(origCat==locType)
    }
    
    # Plot the predictions for test data by month
    p1 <- plotData %>%
        ggplot(aes(x=monthfct, y=pctTotal, 
                   fill=factor(class, levels=c("Different", "Similar", "Correct"))
                   )
               ) +
        geom_col(position="stack") + 
        ylim(c(0, 1)) + 
        scale_fill_manual("", values=c("red", "orange", "seagreen")) + 
        geom_text(aes(y=1-pctDifferent/2, 
                      label=ifelse(pctDifferent==0, "", paste0(round(100*pctDifferent, 0), "%"))
                      ), 
                  color="white"
                  ) + 
        geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect, 0), "%")), color="white")
    if (locType=="All") {
        p1 <- p1 + 
            facet_wrap(~ origCat) + 
            labs(title=paste0("Prediction Accuracy by Month for Locales"), x="", y="% Predicted") 
    } else {
        p1 <- p1 + 
            labs(title=paste0("Prediction Accuracy by Month for ", locType), x="", y="% Predicted")
    }
    print(p1)
    
    # Return the plotting dataset
    plotData
    
}


# Create plot for all types
plotLocaleAccuracyByMonth(locType="All")
```
  
While the previous diagnostic showed that predictions were on average as good in any given month, there is a notable pattern by type of locale:

* Desert cities are best classified by the model in the summer and fall, with lower prediction accuracy observed in January to May  
* Hot-Humid cities are best classified by the model in the summer, with prediction accuracy falling off during cooler times of the year  
* Marine cities are best classified in the late winter and spring, with prediction accuracy falling off during fall and early winter  
* Wintry cities are best classified in the winer, with prediction accuracy falling off during summer  
  
What locale types are predicted when prediction errors are made?  
```{r}

plotLocaleVsPredictionByMonth <- function(locType="All") {

    # Create the database
    plotData <- allData16 %>%
        filter(testTrain=="test") %>%
        group_by(origSource, pred, monthfct) %>%
        summarize(n=n()) %>%
        mutate(origCat=localeCategory[origSource], 
               predCat=localeCategory[pred], 
               class=ifelse(origCat==predCat, "Correct Type", predCat)
               ) %>%
        group_by(monthfct, origCat, class) %>%
        summarize(n=sum(n)) %>%
        mutate(pctTotal=n/sum(n), 
               pctCorrect=sum(ifelse(class=="Correct Type", pctTotal, 0))
               )
    
    if (locType!="All") {
        plotData <- plotData %>%
            filter(origCat==locType)
    }
    
    # Plot the predictions for test data by month
    p1 <- plotData %>%
        ggplot(aes(x=monthfct, y=pctTotal, 
                   fill=factor(class, levels=c("Wintry", "Marine", "Hot-Humid", "Desert", "Correct Type"))
                   )
               ) +
        geom_col(position="stack") + 
        ylim(c(0, 1)) + 
        geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect, 0), "%")), color="white")
    
    if (locType=="All") {
        p1 <- p1 + 
            scale_fill_manual("", values=c("darkblue", "lightblue", "orange", "red", "seagreen")) + 
            facet_wrap(~ origCat) + 
            labs(title=paste0("Prediction Accuracy by Month for Locales"), x="", y="% Predicted") 
    } else {
        p1 <- p1 + 
            scale_fill_manual("", values=c("darkblue", "lightblue", "orange", "seagreen")) + 
            labs(title=paste0("Prediction Accuracy by Month for ", locType), x="", y="% Predicted")
    }
    print(p1)
    
    # Return the plotting dataset
    plotData
    
}

plotLocaleVsPredictionByMonth(locType="Wintry")
plotLocaleVsPredictionByMonth(locType="Marine")
plotLocaleVsPredictionByMonth(locType="Hot-Humid")
plotLocaleVsPredictionByMonth(locType="Desert")
plotLocaleVsPredictionByMonth(locType="All")

```
  
There are interesting patterns in the misclasssifications by month:  
  
* Desert cities are most often misclassified as Hot-Humid in January-February and as Wintry in March-May  
* Hot-Humid cities are most frequently mis-classified as Marine during the cooler months  
* Marine cities are most frequently mis-classified as Wintry during the summer months and as Hot-Humid during the winter months  
* Wintry cities are most frequently mis-classified as Marine during the summer months  
  
Perhaps there are some time of day effects as well.  The findings above could suggest, for example, that evenings in the desert and daytimes in Wintry cities are somewhat similar in March-May  
  
#### _Example #51: Data by Month by Locale_  
Are there any common features as to when the model makes poor predictions for San Diego?  
```{r}

# Create the database for San Diego 'test' classifications
mismData <- allData16 %>%
    filter(testTrain=="test", source=="ksan2016") %>%
    mutate(mism=ifelse(pred==source, "Correct", "Incorrect"), 
           hour=lubridate::hour(valid)
           )
mismData

# Assess the classifications by month
mismData %>%
    group_by(monthfct, mism) %>%
    summarize(n=n()) %>%
    mutate(nTot=sum(n), nWrong=sum(ifelse(mism=="Incorrect", n, 0)), pctWrong=nWrong/nTot) %>%
    ggplot(aes(x=monthfct, y=n, fill=factor(mism, levels=c("Incorrect", "Correct")))) + 
    geom_col(position="stack") + 
    labs(x="", 
         y="# Test Observations", 
         title="San Diego 2016 Test Dataset", 
         subtitle="Accuracy by Month"
         ) + 
    scale_fill_discrete("") + 
    geom_text(aes(y=nTot-0.5*nWrong, 
                  label=paste0(nWrong, "\n(", round(100*pctWrong, 0), "%)")
                  ), 
              color="black"
              )

```
  
And this is then converted to a function so it can be run for multiple variables:  
```{r}

# Function to calculate mismatches by city by a key variable
mismatchByCityVariable <- function(loc, keyVar, locName, varName, roundTo=NULL) {

    # Create the database for specified city 'test' classifications
    mismData <- allData16 %>%
        filter(testTrain=="test", source %in% loc) %>%
        mutate(mism=ifelse(pred==source, "Correct", "Incorrect"), 
               hour=lubridate::hour(valid)
               )
    
    if (!is.null(roundTo)) {
        mismData <- mismData %>%
            mutate_at(vars(all_of(keyVar)), ~roundTo * round(./roundTo, 0))
    }

    # Assess the classifications by key variable
    p1 <- mismData %>%
        group_by_at(vars(all_of(c(keyVar, "mism")))) %>%
        summarize(n=n()) %>%
        mutate(nTot=sum(n), 
               nWrong=sum(ifelse(mism=="Incorrect", n, 0)), 
               pctWrong=nWrong/nTot, 
               mismFactor=factor(mism, levels=c("Incorrect", "Correct"))
               ) %>%
        ggplot(aes_string(x=keyVar, y="n", fill="mismFactor")) + 
        geom_col(position="stack") + 
        labs(x="", 
             y="# Test Observations", 
             title=paste0(locName, " Test Dataset"), 
             subtitle=paste0("Accuracy by ", varName)
             ) + 
        scale_fill_discrete("") + 
        geom_text(aes(y=nTot-0.5*nWrong, 
                      label=paste0(nWrong, "\n(", round(100*pctWrong, 0), "%)")
                      ), 
                  color="black"
                  )
    print(p1)
    
    mismData

}

# Run for San Diego 2016 by monthfct, wd, predomDir, spdBucket
mismatchByCityVariable(loc="ksan2016", keyVar="monthfct", locName="San Diego 2016", varName="Month")
mismatchByCityVariable(loc="ksan2016", keyVar="wd", locName="San Diego 2016", varName="Wind Direction")
mismatchByCityVariable(loc="ksan2016", keyVar="predomDir", 
                       locName="San Diego 2016", varName="Wind Direction"
                       )
mismatchByCityVariable(loc="ksan2016", keyVar="spdBucket", locName="San Diego 2016", varName="Wind Speed")

# Run for San Diego 2016 by hour, TempF, DewF
mismatchByCityVariable(loc="ksan2016", keyVar="hour", locName="San Diego 2016", 
                       varName="Hour (Zulu Time)", roundTo=4
                       )
mismatchByCityVariable(loc="ksan2016", keyVar="TempF", locName="San Diego 2016", 
                       varName="Temperature (F)", roundTo=5
                       )
mismatchByCityVariable(loc="ksan2016", keyVar="DewF", locName="San Diego 2016", 
                       varName="Dew Point (F)", roundTo=5
                       )

```
  
Accuracy by month for San Diego 2016 has been discussed previously.  There are some interesting findings on the other dimensions:  
  
* The two most common wind types in San Diego are WNW (6% error rate) and None (28% error rate), suggesting that the model may be teasing apart San Diego and other cities that sometimes experience a temperate climate by using existence of a WNW wind  
* Further evidence for this hypothesis is that accuracy gets better when winds are Moderate (12% error) or strong (9% error), though Light winds are the most common  
* The test set is somewhat over-represented with 6-10Z and 14-18Z and under-represented with 10-14Z, based on random sampling  
* Prediction accuracy is higher for 18Z-24Z (12% and 10% error), representing roughly 10 am - 4 pm San Diego time  
* Prediction accuracy falls during the rare times when San Diego has a temperature other than 55-75 F or a dew point other than 40-60 F  
  
