---
title: "Mapping and Plotting"
author: "davegoblue"
date: "3/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

This document is to provide some plotting examples for reference.

***

#### _Example #1: US State and County Maps_ 
The package usmap contains maps of US states and counties.  There is also some associated data available about state and county demographics.

Example code includes:  
```{r}

library(tidyverse)

# Population by county
data(countypop, package="usmap")
# Population by state
data(statepop, package="usmap")
# Poverty rate by county
data(countypov, package="usmap")
# Poverty rate by state
data(statepov, package="usmap")
# Population of largest city by state
data(citypop, package="usmap")
# Location of earthquakes
data(earthquakes, package="usmap")


# Included datasets
countypop
statepop
countypov
statepov
citypop
earthquakes

# Basic, empty US maps
usmap::plot_usmap(regions="states")
usmap::plot_usmap(regions="counties")

# Basic, empty US maps subsetted to an area
usmap::plot_usmap(regions="states", 
                  include=c("WA", "OR", "CA", "NV", "ID", "MT", "WY", "UT", "CO", "AZ", "NM")
                  )
usmap::plot_usmap(regions="counties", include=c("MN", "WI", "MI", "OH", "PA", "NY", "IN", "IL"))

# Basic, subsetted state map with poverty rates included
usmap::plot_usmap(regions="states", 
                  include=c("WA", "OR", "CA", "NV", "ID", "MT", "WY", "UT", "CO", "AZ", "NM"), 
                  values="pct_pov_2014", data=statepov, labels=TRUE
                  ) + 
    scale_fill_continuous(low="lightblue", high="darkblue", "Poverty Rate (%)") + 
    labs(title="Poverty Rates by Western and Mountain States")

# Basic, subsetted county map with poverty rates included
usmap::plot_usmap(regions="counties", include=c("MN", "WI", "MI", "OH", "PA", "NY", "IN", "IL"),
                  values="pct_pov_2014", data=countypov, labels=FALSE
                  ) + 
    scale_fill_continuous(low="lightblue", high="darkblue", "Poverty Rate (%)") + 
    labs(title="Poverty Rates by County in Great Lakes States")

```

#### _Example #2: Converting and adding lat/lon data_ 
The latitude and longitude data can be converted to a form suitable for usmap by using the usmap_transform function.

Example code includes:  
```{r}

# Transform the earthquakes data
trQuakes <- usmap::usmap_transform(earthquakes)
str(trQuakes)

# Add as a layer to the state map
usmap::plot_usmap(regions="states") + 
    geom_point(data=trQuakes, aes(x=lon.1, y=lat.1, size=mag), alpha=0.4) + 
    labs(title="Earthquakes of Magnitude 2.5+ (H1 2019)")

# Transform the largest city data
trCity <- usmap::usmap_transform(citypop)
str(trCity)

# Add as a layer to the state map
usmap::plot_usmap(regions="states") + 
    geom_point(data=trCity, aes(x=lon.1, y=lat.1, size=city_pop)) + 
    labs(title="Largest City by State")

```
  
#### _Example #3: Filtering and coloring by region_ 
The census region definitions are included, and can be used to filter or color the maps.

Example code includes:  
```{r}

# Filter the map to include only new_england, mid_atlantic, and south_atlantic
usmap::plot_usmap(regions="states", 
                  include=c(usmap::.new_england, usmap::.mid_atlantic, usmap::.south_atlantic)
                  )

# Create regions data for US states
regionData <- usmap::statepop %>%
    mutate(region=as.factor(ifelse(abbr %in% usmap::.midwest_region, 1, 0)))
usmap::plot_usmap(regions="states", data=regionData, values="region") + 
    scale_fill_discrete("Midwest") + 
    labs(title="Midwest Region US States")

# Enhanced Coloring and Labelling
usmap::plot_usmap(regions="states", data=regionData, values="region") + 
    scale_fill_manual(values=c("lightgray", "lightblue"), "", labels=c("Other", "Midwest")) + 
    labs(title="Midwest Region US States")

```

#### _Example #4: Labelling geographies_ 
Since usmap is built on ggplot2, the standard techniques from ggplot2 can be used to enhance the geography labelling.  Further, centroids for the geographies are available in loadable files.

Example code includes:  
```{r}

# Base state map labelled with defaults
usmap::plot_usmap(regions="states", labels=TRUE, label_color="red")

# Base county map labelled with defaults
usmap::plot_usmap(regions="counties", include=c("TX", "OK"), labels=TRUE, label_color="grey")

# Load state centroid data
stCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "states", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 3)), stringsAsFactors = FALSE
                            )

# Load county centroid data
ctCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "counties", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 4)), stringsAsFactors = FALSE
                            )

# Add state labels using geom_text
regionData <- usmap::statepop %>%
    mutate(region=as.factor(ifelse(abbr %in% usmap::.midwest_region, 1, 0))) %>%
    left_join(stCenter %>% select(x, y, full, fips) %>% rename(fname=full)) %>%
    mutate(fname=ifelse(fname=="District of Columbia", "DC", str_replace_all(fname, " ", "\n")))

usmap::plot_usmap(regions="states", data=regionData[, c("fips", "region")], values="region") + 
    scale_fill_manual(values=c("lightgray", "lightblue"), "", labels=c("Other", "Midwest")) + 
    labs(title="Midwest Region US States") + 
    geom_text(data=filter(regionData, region==1), aes(x=x, y=y, label=fname), size=2.5)


# Add county labels using geom_text
regionData <- usmap::countypop %>%
    mutate(region=as.factor(case_when(abbr=="OK" ~ 1, abbr=="TX" ~ 2, TRUE ~ 0))) %>%
    left_join(ctCenter %>% select(x, y, county, fips) %>% rename(cname=county)) %>%
    mutate(cname=str_replace_all(str_replace(cname, " County", ""), " ", "\n"))

usmap::plot_usmap(regions="counties", include=c("TX", "OK"), 
                  data=regionData[, c("fips", "region")], values="region") + 
    scale_fill_manual(values=c("red", "orange"), "", labels=c("Oklahoma", "Texas")) + 
    labs(title="Texas and Oklahoma Counties") + 
    geom_text(data=filter(regionData, abbr %in% c("TX", "OK")), 
              aes(x=x, y=y, label=cname), size=2.5, 
              color=ifelse(pull(filter(regionData, abbr %in% c("TX", "OK")), abbr)=="OK", "white", "black")
              )

```

#### _Example #5: Adding population centers_ 
Separate data exists for key population centers, which can be loaded and then added to maps.

Example code includes:  
```{r}

# Transform the largest city data
str(maps::us.cities)
trCity <- usmap::usmap_transform(select(maps::us.cities, long, lat, everything())) %>% 
    mutate(useName=str_replace_all(str_sub(name, 1, -4), " ", "\n"))
str(trCity)

# Define a key region for plotting
rgnPlot <- c(usmap::.west_south_central, usmap::.east_south_central)
popFilter <- 100000

# Add cities as a layer to the state map
usmap::plot_usmap(regions="states", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("South Central Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               )

# Plot the full nation for cities of 250k +
rgnPlot <- c(usmap::.midwest_region, usmap::.northeast_region, 
             usmap::.south_region, usmap::.west_region
             )
popFilter <- 250000

# Add cities as a layer to the state map
usmap::plot_usmap(regions="states", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("US Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               )

# Plot cities by name for the Four Corners region
rgnPlot <- c("UT", "CO", "NM", "AZ")
popFilter <- 125000

# Add cities as a layer to the state map (points)
usmap::plot_usmap(regions="counties", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("Four Corners Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               )

# Add cities as a layer to the state map (text)
usmap::plot_usmap(regions="counties", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("Four Corners Cities with Population >= ", popFilter/1000, "k")) + 
    geom_text(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop, label=useName)
               )

popFilter <- 50000
popFilter2 <- 250000

# Add cities as a layer to the state map (points and text)
usmap::plot_usmap(regions="counties", include=rgnPlot, fill="lightblue") + 
    labs(title=paste0("Four Corners Cities with Population >= ", popFilter/1000, "k")) + 
    geom_point(data=filter(trCity, pop >= popFilter, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop), alpha=0.5
               ) + 
    geom_text(data=filter(trCity, pop >= popFilter2, country.etc %in% rgnPlot), 
               aes(x=long.1, y=lat.1, size=pop, label=useName), color="red", show.legend=FALSE
               )

```
  
#### _Example #6: Custom coloring geographies_  
Using scale_fill_manual(), custom colors can be created by geography.

Example code includes:  
```{r}

# Basic county population map with continuous colors
usmap::countypop %>% 
    filter(abbr %in% c("OH", "IN", "KY")) %>% 
    mutate(pop=pop_2015/1000, name=str_replace(str_replace(county, " County", ""), " ", "\n")) %>%
    usmap::plot_usmap(regions="counties", include=c("OH", "IN", "KY"), data=., values="pop") +
    scale_fill_continuous(low="lightblue", high="darkblue", "Pop. (000)") + 
    labs(title="Indiana, Ohio, and Kentucky - Population by County")

# Custom county population map with colors - red for Indiana, blue for Kentucky, grey for Ohio
popBucket <- c(0, 100, 500)
popLabels <- sapply(1:(length(popBucket)-1), FUN=function(x){paste0(popBucket[x], "-", popBucket[x+1])})
popLabels <- c(popLabels, paste0(popBucket[length(popBucket)], "+"))
guideLabels <- paste(rep(c("OH", "KY", "IN"), each=3), popLabels)

usmap::countypop %>% 
    filter(abbr %in% c("OH", "KY", "IN")) %>% 
    mutate(pop=pop_2015/1000, name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           pBucket=findInterval(pop, popBucket), 
           pColor=rgb(abbr=="IN", 0, abbr=="KY", pBucket/length(popBucket))
           ) %>%
    usmap::plot_usmap(regions="counties", include=c("OH", "IN", "KY"), data=., values="pColor") +
    scale_fill_identity(guide="legend", "Pop. (000)", labels=guideLabels) + 
    labs(title="Indiana, Ohio, and Kentucky - Population by County") + 
    theme(legend.position = "bottom") + 
    guides(fill=guide_legend(nrow=3))

# Custom county poverty rate map with colors - red for Indiana, blue for Kentucky, grey for Ohio
povBucket <- c(0, 15, 30)
povLabels <- sapply(1:(length(povBucket)-1), FUN=function(x){paste0(povBucket[x], "-", povBucket[x+1])})
povLabels <- c(povLabels, paste0(povBucket[length(povBucket)], "+"))
guideLabels <- paste(rep(c("OH", "KY", "IN"), each=3), povLabels)

usmap::countypov %>% 
    filter(abbr %in% c("OH", "KY", "IN")) %>% 
    mutate(name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           pBucket=findInterval(pct_pov_2014, povBucket), 
           pColor=rgb(abbr=="IN", 0, abbr=="KY", pBucket/length(povBucket))
           ) %>%
    usmap::plot_usmap(regions="counties", include=c("OH", "IN", "KY"), data=., values="pColor") +
    scale_fill_identity(guide="legend", "Poverty Rate (%)", labels=guideLabels) + 
    labs(title="Indiana, Ohio, and Kentucky - Poverty Rate by County") + 
    theme(legend.position = "bottom") + 
    guides(fill=guide_legend(nrow=3))

```

#### _Example #7: Custom labeling of key geographies_  
The above techniques can be combined for custom labeling of key geographies.

Example code includes:  
```{r}

# Basic state population data
stateData <- usmap::statepop %>% 
    mutate(pop=round(pop_2015/1000000, 1), 
           name=ifelse(full=="District of Columbia", "DC", str_replace(full, " ", "\n")), 
           lab=paste0(abbr, "\n(", pop, ")\n")
           )

# Load state centroid data
stCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "states", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 3)), stringsAsFactors = FALSE
                            )

# Grab centroids for top 5 states
top5State <- stateData %>%
    top_n(5, pop) %>% 
    left_join(select(stCenter, x, y, fips))

# Plot state population with continuous colors and custom labels
stateData %>% 
    usmap::plot_usmap(regions="states", data=., values="pop") +
    scale_fill_continuous(low="lightblue", high="darkblue", "Pop. (millions)") + 
    labs(title="Population by State", subtitle="Top 5 in millions") + 
    geom_text(data=top5State, aes(x=x, y=y, label=lab), color="white", size=4, fontface="bold")


# Load county centroid data
ctCenter <- utils::read.csv(system.file("extdata", 
                                        paste0("us_", "counties", "_centroids.csv"), package = "usmap"
                                        ),
                            colClasses = c(rep("numeric", 2), rep("character", 4)), stringsAsFactors = FALSE
                            )

# Custom county population map with colors - red for Wisconsin, blue for Michigan
popBucket <- c(0, 100, 500)
popLabels <- sapply(1:(length(popBucket)-1), FUN=function(x){paste0(popBucket[x], "-", popBucket[x+1])})
popLabels <- c(popLabels, paste0(popBucket[length(popBucket)], "+"))
guideLabels <- paste(rep(c("MI", "WI"), each=3), popLabels)

# Grab county data for counties exceeding the top popBucket
ctyData <- usmap::countypop %>%
    filter(abbr %in% c("MI", "WI")) %>%
    mutate(pop=round(pop_2015/1000, 0), 
           name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           lab=paste0(name, "\n(", pop, ")\n")
           )

topCounty <- ctyData %>%
    filter(pop >= max(popBucket)) %>%
    left_join(select(ctCenter, x, y, fips))

# Create county population map
usmap::countypop %>% 
    filter(abbr %in% c("MI", "WI")) %>% 
    mutate(pop=pop_2015/1000, name=str_replace(str_replace(county, " County", ""), " ", "\n"), 
           pBucket=findInterval(pop, popBucket), 
           pColor=rgb(abbr=="WI", 0, abbr=="MI", pBucket/length(popBucket))
           ) %>%
    usmap::plot_usmap(regions="counties", include=c("WI", "MI"), data=., values="pColor") +
    scale_fill_identity(guide="legend", "Pop. (000)", labels=guideLabels) + 
    geom_text(data=topCounty, aes(x=x, y=y, label=lab), size=3, fontface="bold", color="white") +
    labs(title="Michigan and Wisconsin - Population by County", subtitle="Labelled Pop. (000) for 500k+") + 
    theme(legend.position = "bottom") + 
    guides(fill=guide_legend(nrow=3)) + 
    theme(panel.background=element_rect(color="black", fill="lightgrey"))

```

#### _Example #8: Plotting Weather Data (Temperatures and Dew Points)_  
The ggridges package has weather data for Lincoln, NE in the data file 'lincoln_weather'.  The data are captured once per day for 366 days of 2016.  Simple plots can be made of the average temperatures and dew points.

Example code includes:  
```{r}

data(lincoln_weather, package="ggridges")
str(lincoln_weather, give.attr=FALSE)

# Extract temperature and dew point data
tdData <- lincoln_weather %>%
    select(CST, maxT=`Max Temperature [F]`, minT=`Min Temperature [F]`, meanT=`Mean Temperature [F]`, 
           maxD=`Max Dew Point [F]`, minD=`Min Dewpoint [F]`, meanD=`Mean Dew Point [F]`
           ) %>%
    mutate(date=as.Date(CST))
str(tdData)

# Plot temperatures by day
tdData %>%
    select(date, maxT, meanT, minT) %>%
    pivot_longer(-date) %>%
    ggplot(aes(x=date, y=value, group=name)) + 
    geom_line(aes(color=name))

# Plot dew points by day
tdData %>%
    select(date, maxD, meanD, minD) %>%
    pivot_longer(-date) %>%
    ggplot(aes(x=date, y=value, group=name)) + 
    geom_line(aes(color=name))


library(xts)

# Create an XTS for temperature data
tdXTS <- xts(select(tdData, minT, meanT, maxT), order.by=tdData$date)

# Create and plot weekly and monthly averages
tdXTS %>%
    apply.weekly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Weekly Temperature Average (Lincoln, NE 2016)")

tdXTS %>%
    apply.monthly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Monthly Temperature Average (Lincoln, NE 2016)")


# Create an XTS for dew-point data
tdXTS <- xts(select(tdData, minD, meanD, maxD), order.by=tdData$date)

# Create and plot weekly and monthly averages
tdXTS %>%
    apply.weekly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Weekly Dew Point Average (Lincoln, NE 2016)")

tdXTS %>%
    apply.monthly(FUN=mean, na.rm=TRUE) %>%
    plot(main="Monthly Dew Point Average (Lincoln, NE 2016)")

```

#### _Example #9: Combining xts and ggplot2_  
The xts package is good for working with time series data, while ggplot2 is strong for customizing plots.  The packages can be combined in using the weather data.

Example code includes:  
```{r}

# Create an XTS for temperature and dewpoint data
tdXTS <- xts(select(tdData, minT, meanT, maxT, minD, meanD, maxD), order.by=tdData$date)

# Use xts for monthly average and ggplot2 for plotting
basePlot <- tdXTS %>%
    apply.monthly(FUN=mean, na.rm=TRUE) %>% 
    data.frame(date=index(.), row.names=NULL) %>% 
    ggplot(aes(x=date-lubridate::days(15))) + 
    geom_ribbon(aes(ymin=minT, ymax=maxT), color="lightblue", fill="lightblue", alpha=0.5) +
    geom_line(aes(y=meanT), color="blue", lwd=1) + 
    labs(x="Month", y="Avg. Temperature (F)", title="Lincoln, NE Weather (2016)", 
         subtitle="Monthly Avg. Temperature (F)"
         )
basePlot

# Add labelling for the three elements
hiMonth <- index(tdXTS %>% apply.monthly(FUN=mean))[3]
loMonth <- index(tdXTS %>% apply.monthly(FUN=mean))[9]
muMonth <- index(tdXTS %>% apply.monthly(FUN=mean))[6]
hiPoint <- c(60, 75)
loPoint <- c(45, 25)
muPoint <- c(72.5, 45)

labFrame <- data.frame(x=c(hiMonth, loMonth, muMonth), 
                       yend=c(hiPoint[1], loPoint[1], muPoint[1]),
                       y=c(hiPoint[2], loPoint[2], muPoint[2]), 
                       text=c("Avg. Monthly High", "Avg. Monthly Low", "Avg. Monthly Mean")) %>%
    mutate(xend=x)

basePlot + 
    geom_segment(data=labFrame, aes(x=x, y=y, xend=xend, yend=yend), arrow=arrow()) + 
    geom_text(data=labFrame, aes(x=x, y=y+c(5, -5, -5), label=text), fontface="bold", size=4)


# Can also create and plot a custom rolling average
baseData <- tdXTS %>%
    data.frame(date=index(.), row.names=NULL)

base7Day <- rollapply(tdXTS, 7, FUN=mean, na.rm=TRUE) %>% 
    data.frame(date=index(.), row.names=NULL)

base30Day <- rollapply(tdXTS, 30, FUN=mean, na.rm=TRUE) %>% 
    data.frame(date=index(.), row.names=NULL)

plotFrame <- bind_rows(baseData, base7Day, base30Day, .id="rolling") %>%
    mutate(rollLabel=case_when(rolling==1 ~ "Daily", 
                               rolling==2 ~ "7 Day Rolling", 
                               rolling==3 ~ "30 Day Rolling",
                               TRUE ~ "ERROR"
                               )
           )
            
plotFrame %>%
    ggplot(aes(x=date)) + 
    geom_line(aes(y=meanT, color=rollLabel, group=rollLabel), lwd=1) + 
    labs(x="Month", y="Avg. Temperature (F)", title="Lincoln, NE Weather (2016)", 
         subtitle="Daily Avg. Temperature (F)"
         )

```

#### _Example #10: Plotting Weather Data (Humidity)_  
Humidity data are also available in the lincoln_weather dataset.  There is a relationship between temperature, dewpoint, and humidity.

Example code includes:  
```{r}

htdData <- lincoln_weather %>%
    select(CST, meanT=`Mean Temperature [F]`, meanD=`Mean Dew Point [F]`, meanH=`Mean Humidity`) %>%
    mutate(date=as.Date(CST), month=lubridate::month(date))
str(htdData)

# Histogram for average humidity
htdData %>%
    ggplot(aes(x=meanH)) + 
    geom_histogram() + 
    labs(title="Mean Humidity Histogram", subtitle="Lincoln, NE (2016)", x="Mean Humidity (%)", y="Count")

htdData %>%
    filter((meanH < 25) | is.na(meanH))

htdData <- htdData %>%
    filter(!((meanH < 25) | is.na(meanH)))

# Updated Histogram for average humidity
htdData %>%
    ggplot(aes(x=meanH)) + 
    geom_histogram() + 
    labs(title="Mean Humidity Histogram", subtitle="Lincoln, NE (2016)", x="Mean Humidity (%)", y="Count")

# Histogram for dewpoint depression (T - D)
htdData %>%
    ggplot(aes(x=meanT-meanD)) + 
    geom_histogram() + 
    labs(title="Mean Dewpoint Depression Histogram", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint Depression (F)", y="Count")

htdData %>% 
    filter(meanD >= meanT)

htdData <- htdData %>%
    filter(meanT >= meanD)

# Updated Histogram for dewpoint depression (T - D)
htdData %>%
    ggplot(aes(x=meanT-meanD, y=..density..)) + 
    geom_histogram(binwidth=1) + 
    geom_density(color="red") + 
    labs(title="Mean Dewpoint Depression Histogram", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint Depression (F)", y="Proportion")

# Average humidity by month
htdData %>%
    group_by(month) %>%
    summarize(meanH=mean(meanH, na.rm=TRUE)) %>%
    ggplot(aes(x=as.factor(month), y=meanH)) + 
    geom_col() + 
    labs(title="Average Humidity by Month", subtitle="Lincoln, NE (2016)", x="Month", y="Mean Humidity (%)")

# Relationship between temperature and dewpoint
htdData %>%
    ggplot(aes(x=meanD, y=meanT)) + 
    geom_point() + 
    geom_abline(slope=1, intercept=0) + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint (F)", y="Mean Temperature (F)"
         )

# Relationship between dewpoint depression and humidity
htdData %>%
    mutate(dpD=meanT-meanD) %>%
    ggplot(aes(x=dpD, y=meanH)) + 
    geom_point() + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint Depression (F)", y="Mean Humidity (%)"
         )

# Relationship between temperature and dewpoint and humidity
humInts <- c(0, 50, 60, 70, 80)
humLabel <- sapply(1:(length(humInts)-1), FUN=function(x) { paste0(humInts[x], "-", humInts[x+1]) })
humLabel <- c(humLabel, paste0(humInts[length(humInts)], "+"))

htdData %>%
    mutate(humBin=factor(findInterval(meanH, humInts), levels=1:length(humInts), labels=humLabel)) %>%
    ggplot(aes(x=meanD, y=meanT, color=humBin)) + 
    geom_point() + 
    geom_smooth(se=FALSE) +
    geom_abline(slope=1, intercept=0) + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint (F)", y="Mean Temperature (F)"
         )

# Expressed using dewpoint depression vs. dewpoint
htdData %>%
    mutate(humBin=factor(findInterval(meanH, humInts), levels=1:length(humInts), labels=humLabel)) %>%
    ggplot(aes(x=meanD, y=meanT-meanD, color=humBin)) + 
    geom_point() + 
    geom_smooth() +
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Dewpoint (F)", y="Dewpoint Depression (F)"
         )

# Expressed using dewpoint depression vs. temperature
htdData %>%
    mutate(humBin=factor(findInterval(meanH, humInts), levels=1:length(humInts), labels=humLabel)) %>%
    ggplot(aes(x=meanT, y=meanT-meanD, color=humBin)) + 
    geom_point() + 
    geom_smooth(se=FALSE, method="lm") +
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Mean Temperature (F)", y="Dewpoint Depression (F)"
         )

# Linear regression for temperature, dewpoint, and humidity
htdReg <- htdData %>%
    mutate(dpD=meanT-meanD) %>%
    lm(meanH ~ meanT + dpD, data=.)
summary(htdReg)

htdData %>%
    mutate(dpD=meanT-meanD) %>%
    mutate(predH=predict(htdReg, newdata=.)) %>%
    ggplot(aes(x=predH, y=meanH)) + 
    geom_point() + 
    geom_abline(slope=1, intercept=0) + 
    labs(title="Daily Averages", subtitle="Lincoln, NE (2016)", 
         x="Predicted Humidity (%)", y="Actual Humidity (%)"
         )

```

#### _Example #11: Plotting Weather Data (Wind)_  
Wind data (speed, gust, direction) are also available in the lincoln_weather dataset..

Example code includes:  
```{r}

# Extract wind data
wdData <- lincoln_weather %>%
    select(CST, maxW=`Max Wind Speed [MPH]`, maxG=`Max Gust Speed [MPH]`, meanW=`Mean Wind Speed[MPH]`, 
           dirW=`WindDir [Degrees]`
           ) %>%
    mutate(date=as.Date(CST))
str(wdData)

# Manage missing data
wdData[!complete.cases(wdData), ]

wdData <- wdData %>%
    filter(dirW != -1, !is.na(dirW)) %>%
    mutate(maxG=ifelse(is.na(maxG), maxW, maxG))
summary(wdData)

# Manage very high wind data
wdData[wdData$maxG >= 60, ]
wdData <- wdData %>%
    filter(maxG <= 80)
summary(wdData)

# Density of wind speeds
wdData %>%
    select(date, meanW, maxW, maxG) %>%
    pivot_longer(-date) %>%
    ggplot(aes(x=value, fill=name)) + 
    geom_density(alpha=0.5) + 
    scale_fill_discrete(name="Wind Speed [MPH]", labels=c("Max Gust", "Max", "Mean")) + 
    labs(title="Lincoln, NE (2016) Wind Speeds", y="Density", x="Wind Speed [MPH]")

# Density of wind direction
wdData %>%
    select(date, dirW) %>%
    ggplot(aes(x=dirW)) + 
    geom_density(alpha=0.5, fill="blue") + 
    labs(title="Winds are mainly from the S and NW", subtitle="Lincoln, NE (2016)", 
         y="Density", x="Wind Direction"
         )

# Wind speed and direction
wdData %>% 
    ggplot(aes(x=meanW, y=dirW)) + 
    geom_point(alpha=0.25) + 
    coord_polar(theta="y") + 
    labs(title="Lincoln, NE (2016)", subtitle="Direction vs. Mean Wind Speed", x="Mean Wind Speed [MPH]") + 
    scale_y_continuous(limits=c(0, 360), breaks=c(0, 90, 180, 270, 360)) + 
    scale_x_continuous(limits=c(0, 30), breaks=c(0, 5, 10, 15, 20, 25, 30)) + 
    geom_point(aes(x=0, y=0), color="red", size=2)

# Wind speed and direction as factors
windDirs <- c("N", "NE", "E", "SE", "S", "SW", "W", "NW")
windSpeeds <- c(0, 5, 10, 15)
windLabels <- sapply(1:(length(windSpeeds)-1), FUN=function(x){ paste0(windSpeeds[x], "-", windSpeeds[x+1]) })
windLabels <- c(windLabels, paste0(windSpeeds[length(windSpeeds)], "+"))
wdData <- wdData %>% 
    mutate(wd=factor(floor(((dirW+22.5)/45) %% 8), levels=0:7, labels=windDirs), 
           ws=factor(findInterval(meanW, windSpeeds), levels=length(windSpeeds):1, labels=rev(windLabels))
           )

# Summary of interaction between wind speed and wind direction 
wdData %>%
    group_by(wd) %>% 
    summarize(n=n(), avgMean=mean(meanW), avgMax=mean(maxW), avgGust=mean(maxG))
table(wdData$wd, wdData$ws)

# Graph of wind speed and wind direction
wdData %>%
    ggplot(aes(x=wd, fill=ws)) + 
    geom_bar() + 
    scale_fill_discrete(name="Wind Speed [MPH]") + 
    labs(title="Lincoln, NE (2016) Wind Speeds and Directions", y="# Days", x="Wind Direction")

# Graph of wind speed and wind direction (polar coordinates)
wdData %>%
    ggplot(aes(x=wd, fill=ws)) + 
    geom_bar() + 
    scale_fill_discrete(name="Wind Speed [MPH]") + 
    labs(title="Lincoln, NE (2016) Wind Speeds and Directions", y="# Days", x="Wind Direction") + 
    coord_polar(start=-0.4)
```

#### _Example #12: Archived granular weather Data (METAR)_  
Iowa State has a great database of archived weather data, including the historical METAR data (meteorological aerodrome report) for a number of reporting stations.

[METAR](https://en.wikipedia.org/wiki/METAR) include information on visibility, wind, temperature, dew point, precipitation, clouds, barometric pressure, and other features that may impact safe aviation.

The data for station KLNK (Lincoln, NE airport) was saved as a CSV from [Iowa State](https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LNK&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2)

Some processing is required before using the METAR data:

Example code includes:  
```{r}

# Load METAR data
klnk <- readr::read_csv("./RInputFiles/metar_klnk_2016.txt", na=c("", "NA", "M"))
str(klnk, give.attr=FALSE)

# Filter to only data that ends with times ending in 54Z
metarKLNK <- klnk %>%
    filter(str_detect(metar, "54Z"))
dim(metarKLNK)


# There should be 24*368=8832 records, so there are a handful (19) of missing METAR observations
minDate <- min(metarKLNK$valid)
expDate <- minDate + lubridate::hours(0:(24*368 - 1))

# Observations expected but not recorded
as.POSIXct(setdiff(expDate, metarKLNK$valid), origin="1970-01-01", tz="UTC")

# Observations recorded but not expected
setdiff(metarKLNK$valid, expDate)

# Confirmation of uniqueness
length(unique(metarKLNK$valid)) == length(metarKLNK$valid)


# Extract wind speeds and direction
# The general wind format is dddssGssKT where ddd is the direction (VRB meaning variable), the main ss is the speed, and the Gss is the gust speed (optional and not always displayed)

mtxWind <- metarKLNK %>%
    pull(metar) %>%
    str_match(pattern="(\\d{3}|VRB)(\\d{2})(G\\d{2})?KT")
head(mtxWind)

table(mtxWind[, 2], useNA="ifany")
table(mtxWind[, 3], useNA="ifany")
table(mtxWind[, 4], useNA="ifany")

# Verify that winds not captured are in fact missing from the METAR
metarKLNK[which(is.na(mtxWind[, 2])), "metar"]

metarKLNK <- metarKLNK %>%
    mutate(dirW=mtxWind[, 2], 
           spdW=as.numeric(mtxWind[, 3]), 
           gustW=as.numeric(str_replace(mtxWind[, 4], "G", ""))
           )

# Plot for the wind direction
metarKLNK %>%
    ggplot(aes(x=dirW)) + 
    geom_bar() + 
    labs(title="Lincoln, NE Wind Direction", subtitle="KLNK METAR (2016)", 
         y="# Hourly Observations", x="Wind Direction"
         )

# Plot for the minimum, average, and maximum wind speed by wind direction
# Wind direction 000 is reserved for 0 KT wind, while VRB is reserved for 3-6 KT variable winds
metarKLNK %>%
    filter(!is.na(dirW)) %>%
    group_by(dirW) %>%
    summarize(minWind=min(spdW), meanWind=mean(spdW), maxWind=max(spdW)) %>%
    ggplot(aes(x=dirW)) + 
    geom_point(aes(y=meanWind), color="red", size=2) + 
    geom_errorbar(aes(ymin=minWind, ymax=maxWind)) + 
    labs(title="Lincoln, NE Wind Direction", subtitle="KLNK METAR (2016)", 
         y="Wind Speed [KT]", x="Wind Direction"
         )

# Plot for the wind speed
# Roughly 10% of the time, there is no wind in Lincoln
metarKLNK %>%
    ggplot(aes(x=spdW)) + 
    geom_bar(aes(y=..count../sum(..count..))) + 
    labs(title="Roughly 10% of wind speeds in Lincoln, NE measure 0 Knots", subtitle="KLNK METAR (2016)", 
         y="% Hourly Observations", x="Wind Speed {KT]"
         )

metarKLNK %>% 
    filter(!is.na(dirW), dirW != "VRB", dirW != "000") %>%
    mutate(dirW=as.numeric(dirW)) %>%
    group_by(dirW, spdW) %>%
    summarize(n=n()) %>%
    ggplot(aes(x=spdW, y=dirW)) + 
    geom_point(alpha=0.1, aes(size=n)) + 
    coord_polar(theta="y") + 
    labs(title="Lincoln, NE (2016)", subtitle="Direction vs. Wind Speed", x="Wind Speed [KT]") + 
    scale_y_continuous(limits=c(0, 360), breaks=c(0, 90, 180, 270, 360)) + 
    scale_x_continuous(limits=c(0, 30), breaks=c(0, 5, 10, 15, 20, 25, 30)) + 
    geom_point(aes(x=0, y=0), color="red", size=2)

```

#### _Example #13: Extracting Key Elements from METAR_  
A properly formatted METAR includes the following information in order, though with variable amounts of other information in between.

dddd54Z ddddd[Gdd]KT dSM [M]dd/[M]dd Adddd RMK SLPddd Tdddddddd

* dddd54Z is the two-digit date and four-digit Zulu time (KLNK METAR are taken at 54 minutes past the hour)  
* ddddd[Gdd]KT is the three-digit wind direction (can be VRB), two digit wind speed in knots, and sometimes the two digit maximum gust in knots  
* dSM is the visibility in statute miles.  This is somewhat tricky in that d can be any of 0-10 but can also be 1/4, 1/2, 3/4, 1 1/4, 1 1/2, 1 3/4, 2 1/2  
* [M]dd/[M]dd is the temperature in celsius and dewpoint in celsius.  M means negative  
* Adddd is the four-digit altimeter reading  
* RMK notes that the remarks are beginning  
* SLPddd notes the three-digit sea-level pressure  
* Tdddddddd notes the four digit temperature in Celsius and the four digit dewpoint in celsius.  If it begins with 1 it is negative.  The fourth digit is the decimal.  It will always be a Celsius reading that best corresponds to integer degrees of Fahrenheit  
  
Example code includes:  
```{r}

metAll <- metarKLNK %>%
    pull(metar)

# Create a search string for METAR
valMet <- "54Z.*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})"

# Find the number of matching elements
str_detect(metAll, pattern=valMet) %>% table()

# The strings that do not match have errors in the raw data (typically, missing wind speed)
metAll[!str_detect(metAll, pattern=valMet)]

# A matrix of string matches can be obtained
mtxParse <- str_match(metAll, pattern=valMet)
head(mtxParse)

# Create a data frame
dfParse <- data.frame(mtxParse, stringsAsFactors=FALSE)
names(dfParse) <- c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
                    "TempC", "DewC", "Altimeter", "SLP", "FahrC"
                    )
dfParse <- tibble::as_tibble(dfParse)
str(dfParse)

# Convert to numeric where appropriate
dfParse <- dfParse %>%
    mutate(WindSpeed = as.integer(WindSpeed), 
           WindGust = as.numeric(WindGust), 
           Visibility = as.numeric(str_replace(Visibility, "SM", "")),
           TempC = as.integer(str_replace(TempC, "M", "-")), 
           DewC = as.integer(str_replace(DewC, "M", "-")), 
           Altimeter = as.integer(str_replace(Altimeter, "A", "")), 
           SLP = as.integer(str_replace(SLP, "SLP", "")), 
           TempF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 2, 5), pattern="^1", "-"))/10, 
           DewF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 6, 9), pattern="^1", "-"))/10
           )

# Investigate the data
set.seed(2003211416)
str(dfParse)
head(dfParse)
tail(dfParse)
dfParse %>% 
    sample_n(20)

# Check for NA values
colSums(is.na(dfParse))

# Plot of counts by key metric
keyMetric <- c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
               "DewC", "Altimeter", "SLP", "TempF", "DewF"
               )

for (x in keyMetric) {
    p <- dfParse %>%
        group_by_at(vars(all_of(x))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=x, y="n")) + 
        geom_col() + 
        labs(title=x, y="Count")
    print(p)
}

# There are three obvious issues
# Visibility is not correctly picked up when there is a / such as 1/2 SM
# Wind gusts are never picked up
# Sea Level Pressure is missing a digit

# Correct for visibility
# Areas that have \\d \\d/\\dSM
sm1 <- which(str_detect(metAll, pattern=" \\d/\\dSM"))
sm2 <- which(str_detect(metAll, pattern=" \\d \\d/\\dSM"))

valSM1 <- str_match(metAll, pattern="\\d/\\dSM")[sm1]
valSM1 <- str_replace(valSM1, "SM", "")
valSM1 <- as.integer(str_sub(valSM1, 1, 1)) / as.integer(str_sub(valSM1, 3, 3))

valSM2 <- str_match(metAll, pattern=" \\d \\d/\\dSM")[sm2]
valSM2 <- as.integer(str_sub(valSM2, 2, 2))

dfParse[sm1, "Visibility"] <- valSM1
dfParse[sm2, "Visibility"] <- dfParse[sm2, "Visibility"] + valSM2

dfParse %>% 
    count(Visibility)


# Correct for wind gusts
gustCheck <- which(str_detect(metAll, pattern="\\d{5}G\\d{2}KT"))
valGust <- str_match(metAll, pattern="\\d{5}G\\d{2}KT")[gustCheck]
valGust <- as.integer(str_sub(valGust, 7, 8))

dfParse[gustCheck, "WindGust"] <- valGust

dfParse %>% 
    count(WindGust) %>% 
    as.data.frame

# Correct for SLP
dfParse <- dfParse %>%
    mutate(modSLP=ifelse(dfParse$SLP < 500, 1000 + dfParse$SLP/10, 900 + dfParse$SLP/10))

dfParse %>%
    group_by(SLP, modSLP) %>%
    summarize(n=n()) %>%
    ggplot(aes(x=SLP, y=modSLP, size=n)) + 
    geom_point(alpha=0.3)

# Check updated plots
keyMetric <- c("WindGust", "Visibility", "modSLP")
for (x in keyMetric) {
    p <- dfParse %>%
        group_by_at(vars(all_of(x))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=x, y="n")) + 
        geom_col() + 
        labs(title=x, y="Count")
    print(p)
}

```

#### _Example #14: Relationships Between METAR Variables_  
Many of the METAR variables are correlated/associated to one another.

Example code includes:  
```{r}

# Define key numeric variables
coreNum <- c("TempC", "TempF", "DewC", "DewF", "Altimeter", "modSLP", "WindSpeed", "Visibility")

# Add the date back to the file (should edit the above instead)
dfParse <- dfParse %>%
    mutate(month=lubridate::month(metarKLNK$valid))
str(dfParse)

# Keep only complete cases and find correlations
mtxCorr <- dfParse %>%
    mutate(month=lubridate::month(metarKLNK$valid)) %>%
    select_at(vars(all_of(coreNum))) %>%
    filter(complete.cases(.)) %>%
    cor()

# Print the correlations and show a heatmap
mtxCorr %>%
    round(2)

corrplot::corrplot(mtxCorr, method="color", title="Lincoln, NE Hourly Weather Correlations (2016)")

# Create a function for plotting two variables against each other
plotNumCor <- function(var1, var2, title=NULL) {
    if (is.null(title)) 
        { title <- paste0("Lincoln, NE (2016) Hourly Correlations of ", var1, " and ", var2) }
    p <- dfParse %>%
        group_by_at(vars(all_of(c(var1, var2)))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=var1, y=var2)) + 
        geom_point(alpha=0.5, aes_string(size="n")) + 
        geom_smooth(method="lm", aes_string(weight="n")) + 
        labs(x=var1, y=var2, title=title)
    print(p)
}

# The three linear or almost linear relationships
plotNumCor("TempC", "TempF")
plotNumCor("DewC", "DewF")
plotNumCor("Altimeter", "modSLP")

# Strongly and positively related
plotNumCor("TempF", "DewF")

# Moderately negatively correlated
plotNumCor("TempF", "Altimeter")
plotNumCor("TempF", "modSLP")
plotNumCor("Altimeter", "WindSpeed")

# Predict modSLP from Altimeter
lmSLP1 <- lm(modSLP ~ Altimeter, data=dfParse)
lmSLP2 <- lm(modSLP ~ Altimeter + TempF, data=dfParse)
summary(lmSLP1)
summary(lmSLP2)

# Plot predictions vs. actual (model 1)
dfParse %>%
    filter(!is.na(modSLP)) %>%
    mutate(pred1=predict(lmSLP1)) %>%
    count(modSLP, pred1) %>%
    ggplot(aes(x=modSLP, y=pred1)) + 
    geom_point(alpha=0.25, aes(size=n)) + 
    geom_smooth(method="lm", aes(weight=n)) + 
    labs(title="Predicted vs. Actual Sea Level Pressure - Altitude Only as Predictor", 
         subtitle="Lincoln, NE (2016) Hourly METAR", x="Sea Level Pressure", y="Predicted"
         )

# Plot predictions vs. actual (model 2)
dfParse %>%
    filter(!is.na(modSLP)) %>%
    mutate(pred2=predict(lmSLP2)) %>%
    count(modSLP, pred2) %>%
    ggplot(aes(x=modSLP, y=pred2)) + 
    geom_point(alpha=0.25, aes(size=n)) + 
    geom_smooth(method="lm", aes(weight=n)) + 
    labs(title="Predicted vs. Actual Sea Level Pressure - Altitude and Temperature as Predictor", 
         subtitle="Lincoln, NE (2016) Hourly METAR", x="Sea Level Pressure", y="Predicted"
         )

```

#### _Example #15: Extracting Cloud Data from METAR_  
[Cloud data](https://en.wikipedia.org/wiki/METAR#Cloud_reporting) is also included in the METAR, with the type of clouds being described as:

* CLR - there are no clouds below 12,000 feet 
* VVddd - there is a vertical visibility of ddd hundred feet (cannot tell where the clouds are above that)  
* FEWddd - there are clouds with bases at ddd feet, and they obscure 25% or less of the sky  
* SCTddd - there are clouds with bases at ddd feet, and they obscure 25%-50% of the sky  
* BKNddd - there are clouds with bases at ddd feet, and they obscure 50%-99% of the sky  
* OVCddd - there is a full overcast with base at ddd feet  
  
The ceiling is considered the lowest height that is measured as any of OVC, BKN, or VV.

Example code includes:  
```{r}

# Extract the CLR records
mtxCLR <- str_extract_all(metarKLNK$metar, pattern=" CLR ", simplify=TRUE)
if (dim(mtxCLR)[[2]] != 1) { stop("Extracted 2+ CLR from some METAR; investigate") }
isCLR <- ifelse(mtxCLR[, 1] == "", 0, 1)

# Extract the VV records
mtxVV <- str_extract_all(metarKLNK$metar, pattern="VV(\\d{3})", simplify=TRUE)
if (dim(mtxVV)[[2]] != 1) { stop("Extracted 2+ VV from some METAR; investigate") }
isVV <- ifelse(mtxVV[, 1] == "", 0, 1)
htVV <- ifelse(mtxVV[, 1] == "", NA, as.integer(str_replace(mtxVV[, 1], "VV", ""))*100)

# Extract the FEW records
mtxFEW <- str_extract_all(metarKLNK$metar, pattern="FEW(\\d{3})", simplify=TRUE)
numFEW <- apply(mtxFEW, 1, FUN=function(x) { sum((x!=""))} )

# Extract the SCT records
mtxSCT <- str_extract_all(metarKLNK$metar, pattern="SCT(\\d{3})", simplify=TRUE)
numSCT <- apply(mtxSCT, 1, FUN=function(x) { sum((x!=""))} )

# Extract the BKN records
mtxBKN <- str_extract_all(metarKLNK$metar, pattern="BKN(\\d{3})", simplify=TRUE)
numBKN <- apply(mtxBKN, 1, FUN=function(x) { sum((x!=""))} )

# Extract the OVC records
mtxOVC <- str_extract_all(metarKLNK$metar, pattern="OVC(\\d{3})", simplify=TRUE)
numOVC <- apply(mtxOVC, 1, FUN=function(x) { sum((x!=""))} )

# Summarize as a data frame
tblClouds <- tibble::tibble(isCLR=isCLR, isVV=isVV, htVV=htVV, numFEW=numFEW, 
                            numSCT=numSCT, numBKN=numBKN, numOVC=numOVC
                            )

# Get the counts
# As expected, if isCLR then nothing else, and if isVV then nothing else
tblClouds %>% 
    count(isCLR, isVV, numFEW, numSCT, numBKN, numOVC) %>%
    as.data.frame()

# Investigate the problem data
metarKLNK$metar[rowSums(tblClouds, na.rm=TRUE)==0]

# Get the counts of most obscuration
tblClouds %>%
    filter(rowSums(., na.rm=TRUE) > 0) %>%
    mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                  numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                  TRUE ~ "Error"
                                  ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                  )
           ) %>%
    ggplot(aes(x=wType, y=..count../sum(..count..))) + 
    geom_bar() + 
    labs(title="Highest Obscuration by Cloud - Lincoln, NE (2016)", x="Cloud Type", 
         y="Proportion of Hourly Measurements"
         )

# Integrate the clouds data
mtxCloud <- cbind(mtxVV, mtxOVC, mtxBKN, mtxSCT, mtxFEW, mtxCLR)

# Cycle through to find levels of a given type
ckClouds <- function(cloudType) {
    isKey <- which(apply(mtxCloud, 2, FUN=function(x) {sum(str_detect(x, cloudType))}) > 0)
    as.integer(str_replace(mtxCloud[, min(isKey)], cloudType, "")) * 100
}
lowOVC <- ckClouds("OVC")
lowVV <- ckClouds("VV")
lowBKN <- ckClouds("BKN")
lowSCT <- ckClouds("SCT")
lowFEW <- ckClouds("FEW")

# Integrate the lowest cloud type by level
lowCloud <- tibble::tibble(lowVV, lowOVC, lowBKN, lowSCT, lowFEW)
lowCloud

# Get the lowest cloud level
minCloud <- lowCloud
minCloud[is.na(minCloud)] <- 999999
minCloudLevel <- apply(minCloud, 1, FUN=min)
minCeilingLevel <- apply(minCloud[, c("lowVV", "lowOVC", "lowBKN")], 1, FUN=min)

noCloudPct <- mean(minCloudLevel == 999999)
noCeilingPct <- mean(minCeilingLevel == 999999)

# Plot the minimum cloud level (where it exists)
data.frame(minCloudLevel, minCeilingLevel) %>%
    filter(minCloudLevel != 999999) %>%
    ggplot(aes(x=minCloudLevel)) + 
    geom_bar(aes(y=..count../sum(..count..))) + 
    geom_text(aes(x=2500, y=0.04, 
                  label=paste0(round(100*noCloudPct), "% of obs. have no clouds")
                  )
              ) + 
    labs(x="Height [ft]", y="Proportion", title="Minimum Cloud Height (when some clouds exist)", 
         subtitle="Lincoln, NE (2016)"
         )

# Plot the minimum ceiling level (where it exists)
data.frame(minCloudLevel, minCeilingLevel) %>%
    filter(minCeilingLevel != 999999) %>%
    ggplot(aes(x=minCeilingLevel)) + 
    geom_bar(aes(y=..count../sum(..count..))) + 
    geom_text(aes(x=2500, y=0.04, 
                  label=paste0(round(100*noCeilingPct), "% of obs. have no ceiling")
                  )
              ) + 
    labs(x="Height [ft]", y="Proportion", title="Minimum Ceiling Height (when a ceiling exists)", 
         subtitle="Lincoln, NE (2016)"
         )

```

#### _Example #16: Plotting by factor variables_  
The month of the year is an interesting data point for plotting against.

Example code includes:  
```{r}

# Integrate the cloud data and convert month to a factor
dfFull <- cbind(dfParse, tblClouds, lowCloud) %>%
    mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                  numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                  TRUE ~ "Error"
                                  ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                  ), 
           month=factor(month, levels=1:12, labels=month.abb)
           )
dfFull <- tibble::as_tibble(dfFull)
str(dfFull)

# Run the boxplot of a factor against a numeric variable
plotFactorNumeric <- function(fctVar, numVar, title=NULL) {
    if (is.null(title)) { title <- paste0("Lincoln, NE (2016) Hourly Weather - ", numVar, " vs. ", fctVar) }
    p <- dfFull %>%
        filter(!is.na(get(fctVar)), !is.na(get(numVar))) %>%
        ggplot(aes_string(x=fctVar, y=numVar)) + 
        geom_boxplot(fill="lightblue") + 
        labs(title=title)
    print(p)
}

# Run for all of the key variables against wind speed and cloud type
keyVar <- c("WindSpeed", "Visibility", "Altimeter", "TempF", "DewF")
for (var in keyVar) { plotFactorNumeric("month", var) }
for (var in keyVar) { plotFactorNumeric("wType", var) }

# Create stacked bars for cloud type by month
dfFull %>%
    filter(!is.na(wType), wType!="Error") %>%
    ggplot(aes(x=month, fill=wType)) + 
    geom_bar(position="fill") + 
    labs(title="Lincoln, NE (2016)", x="", y="Proportion of Month")

```

#### _Example #17: Functional Form - METAR download and initial wind processing_  
Example 12 can be converted to functional form so that the process can be applied to other reporting stations and time periods.

Example code includes:  
```{r}

# Function to make an initial read of the data, filter to METAR records, and check date-times
readMETAR <- function(fileName="./RInputFiles/metar_klnk_2016.txt", timeZ="54Z",
                      expMin=as.POSIXct("2015-12-31 00:54:00", tz="UTC"), expDays=365
                      ) {

    # Load METAR data
    initRead <- readr::read_csv(fileName, na=c("", "NA", "M"))
    str(initRead, give.attr=FALSE)

    # Filter to only data that ends with times ending in 54Z
    filterRead <- initRead %>%
        filter(str_detect(metar, timeZ))
    dim(filterRead)

    # Check that the dates and times included are as expected
    expDate <- expMin + lubridate::hours(0:(24*expDays - 1))
    
    # Observations expected but not recorded
    cat("\n*** OBSERVATIONS EXPECTED BUT NOT RECORDED ***\n")
    print(as.POSIXct(setdiff(expDate, filterRead$valid), origin="1970-01-01", tz="UTC"))

    # Observations recorded but not expected
    cat("\n*** OBSERVATIONS RECORDED BUT NOT EXPECTED ***\n")
    print(as.POSIXct(setdiff(filterRead$valid, expDate), origin="1970-01-01", tz="UTC"))

    # Confirmation of uniqueness
    cat("\n*** Are the extracted records unique? ***\n")
    print(length(unique(filterRead$valid)) == length(filterRead$valid))
    cat("\n")
    
    # Return the dataset as a tibble
    tibble::as_tibble(filterRead)
}
funcMETAR <- readMETAR(expDays=368)
funcMETAR


# Extract wind speeds and direction
# The general wind format is dddssGssKT where ddd is the direction (VRB meaning variable), the main ss is the speed, and the Gss is the gust speed (optional and not always displayed)
extractWind <- function(met) {

    mtxWind <- met %>%
        pull(metar) %>%
        str_match(pattern="(\\d{3}|VRB)(\\d{2})(G\\d{2})?KT")
    cat("\n*** First 6 winds and parsing ***\n")
    print(head(mtxWind))

    cat("\n*** Table of WIND DIRECTION ***\n")
    print(table(mtxWind[, 2], useNA="ifany"))
    cat("\n*** Table of WIND SPEED ***\n")
    print(table(mtxWind[, 3], useNA="ifany"))
    cat("\n*** Table of WIND GUST ***\n")
    print(table(mtxWind[, 4], useNA="ifany"))

    # Verify that winds not captured are in fact missing from the METAR
    cat("\n *** WIND DATA WAS NOT CAPTURED FROM: *** \n")
    print(met[which(is.na(mtxWind[, 2])), "metar"])
    cat("\n")

    met %>%
        mutate(dirW=mtxWind[, 2], 
               spdW=as.numeric(mtxWind[, 3]), 
               gustW=as.numeric(str_replace(mtxWind[, 4], "G", ""))
               )
}
windMETAR <- extractWind(funcMETAR)
windMETAR


# Generate basic wind plots
basicWindPlots <- function(met, desc="Lincoln, NE", gran="KLNK METAR (2016)") {

    # Plot for the wind direction
    wDir <- met %>%
        ggplot(aes(x=dirW)) + 
        geom_bar() + 
        labs(title=paste0(desc, " Wind Direction"), subtitle=gran, 
             y="# Hourly Observations", x="Wind Direction"
             )
    print(wDir)

    # Plot for the minimum, average, and maximum wind speed by wind direction
    # Wind direction 000 is reserved for 0 KT wind, while VRB is reserved for 3-6 KT variable winds
    wSpeedByDir <- met %>%
        filter(!is.na(dirW)) %>%
        group_by(dirW) %>%
        summarize(minWind=min(spdW), meanWind=mean(spdW), maxWind=max(spdW)) %>%
        ggplot(aes(x=dirW)) + 
        geom_point(aes(y=meanWind), color="red", size=2) + 
        geom_errorbar(aes(ymin=minWind, ymax=maxWind)) + 
        labs(title=paste0(desc, " Wind Speed (Max, Mean, Min) By Wind Direction"), subtitle=gran, 
             y="Wind Speed [KT]", x="Wind Direction"
             )
    print(wSpeedByDir)

    # Plot for the wind speed
    pctZero <- sum(met$spdW==0, na.rm=TRUE) / length(met$spdW)
    wSpeed <- met %>%
        ggplot(aes(x=spdW)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        labs(title=paste0(round(100*pctZero), "% of wind speeds in ", desc, " measure 0 Knots"), 
             subtitle=gran, 
             y="% Hourly Observations", x="Wind Speed {KT]"
             )
    print(wSpeed)

    wPolarDirSpeed <- met %>% 
        filter(!is.na(dirW), dirW != "VRB", dirW != "000") %>%
        mutate(dirW=as.numeric(dirW)) %>%
        group_by(dirW, spdW) %>%
        summarize(n=n()) %>%
        ggplot(aes(x=spdW, y=dirW)) + 
        geom_point(alpha=0.1, aes(size=n)) + 
        coord_polar(theta="y") + 
        labs(title=paste0(desc, " Direction vs. Wind Speed"), subtitle=gran, x="Wind Speed [KT]") + 
        scale_y_continuous(limits=c(0, 360), breaks=c(0, 90, 180, 270, 360)) + 
        scale_x_continuous(limits=c(0, 30), breaks=c(0, 5, 10, 15, 20, 25, 30)) + 
        geom_point(aes(x=0, y=0), color="red", size=2)
    print(wPolarDirSpeed)
}
basicWindPlots(windMETAR)

```

#### _Example #18: Functional Form for Extracting Key Elements from METAR_  
METAR parsing can also be converted to a functional form.  This will need to be modified to be more general, since the codes used for a few things like clouds can vary from station to station.
  
Example code includes:  
```{r}

# Code for the initial METAR parsing
initialParseMETAR <- function(met, val, labs) {
    
    # Pull the METAR data
    metAll <- met %>%
        pull(metar)
    
    # Find the number of matching elements
    cat("\n*** Tentative Summary of Element Parsing *** \n")
    str_detect(metAll, pattern=val) %>% 
        table() %>%
        print()

    # The strings that do not match have errors in the raw data (typically, missing wind speed)
    cat("\n*** Data Not Matched *** \n")
    print(metAll[!str_detect(metAll, pattern=val)])

    # A matrix of string matches can be obtained
    mtxParse <- str_match(metAll, pattern=val)
    cat("\n*** Parsing matrix summary *** \n")
    print(dim(mtxParse))
    print(head(mtxParse))

    # Create a data frame
    dfParse <- data.frame(mtxParse, stringsAsFactors=FALSE) %>%
        mutate(dtime=met$valid, origMETAR=met$metar)
    names(dfParse) <- c(labs, "dtime", "origMETAR")
    dfParse <- tibble::as_tibble(dfParse)
    cat("\n*** Summary of the parsed data *** \n")
    glimpse(dfParse)
    
    dfParse
}


# Create a search string for METAR
valMet <- "54Z.*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})"

# Create the names for the search string to parse in to
labsMet <- c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
             "TempC", "DewC", "Altimeter", "SLP", "FahrC"
             )

# Run the METAR parsing on the raw data
initMETAR <- initialParseMETAR(funcMETAR, val=valMet, labs=labsMet)
initMETAR


# Helper function for generating plots by key variables
plotcountsByMetric <- function(df, mets) {
    
    # Plot of counts by key metric
    for (x in mets) {
        p <- df %>%
            group_by_at(vars(all_of(x))) %>%
            summarize(n=n()) %>%
            ggplot(aes_string(x=x, y="n")) + 
            geom_col() + 
            labs(title=x, y="Count")
        print(p)
    }
}


# Code for the conversion of METAR to meaningful numeric
# Should make this much more general later
convertMETAR <- function(met, metrics, seed=2003211416) {
    
    # Convert to numeric where appropriate
    dfParse <- met %>%
        mutate(WindSpeed = as.integer(WindSpeed), 
               WindGust = as.numeric(WindGust), 
               Visibility = as.numeric(str_replace(Visibility, "SM", "")),
               TempC = as.integer(str_replace(TempC, "M", "-")), 
               DewC = as.integer(str_replace(DewC, "M", "-")), 
               Altimeter = as.integer(str_replace(Altimeter, "A", "")), 
               SLP = as.integer(str_replace(SLP, "SLP", "")), 
               TempF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 2, 5), pattern="^1", "-"))/10, 
               DewF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 6, 9), pattern="^1", "-"))/10
               )

    # Investigate the data
    cat("\n *** Parsed data structure, head, tail, and random sample *** \n")
    str(dfParse)
    print(head(dfParse))
    print(tail(dfParse))
    set.seed(seed)
    dfParse %>% 
        sample_n(20) %>%
        print()

    # Check for NA values
    cat("\n *** Number of NA values *** \n")
    print(colSums(is.na(dfParse)))

    # Plot of counts by key metric
    plotcountsByMetric(dfParse, mets=metrics)
    
    # Return the parsed dataset
    dfParse
}

keyMetric <- c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
               "DewC", "Altimeter", "SLP", "TempF", "DewF"
               )
convMETAR <- convertMETAR(initMETAR, metrics=keyMetric)


# There are three obvious issues
# Visibility is not correctly picked up when there is a / such as 1/2 SM
# Wind gusts are never picked up
# Sea Level Pressure is missing a digit

# Address the visibility issues
getVisibility <- function(curMet, origMet) {
    
    # Get the original METAR data
    metAll <- origMet %>%
        pull(metar)
    
    # Correct for visibility
    # Areas that have \\d \\d/\\dSM
    sm1 <- which(str_detect(metAll, pattern=" \\d/\\dSM"))
    sm2 <- which(str_detect(metAll, pattern=" \\d \\d/\\dSM"))

    valSM1 <- str_match(metAll, pattern="\\d/\\dSM")[sm1]
    valSM1 <- str_replace(valSM1, "SM", "")
    valSM1 <- as.integer(str_sub(valSM1, 1, 1)) / as.integer(str_sub(valSM1, 3, 3))

    valSM2 <- str_match(metAll, pattern=" \\d \\d/\\dSM")[sm2]
    valSM2 <- as.integer(str_sub(valSM2, 2, 2))

    curMet[sm1, "Visibility"] <- valSM1
    curMet[sm2, "Visibility"] <- curMet[sm2, "Visibility"] + valSM2

    curMet %>% 
        count(Visibility) %>%
        print()
    
    curMet
}
parseMETAR <- getVisibility(convMETAR, origMet=funcMETAR)


# Correct for wind gusts
getWindGusts <- function(curMet, origMet) {

    metAll <- origMet %>%
        pull(metar)
    
    gustCheck <- which(str_detect(metAll, pattern="\\d{5}G\\d{2}KT"))
    valGust <- str_match(metAll, pattern="\\d{5}G\\d{2}KT")[gustCheck]
    valGust <- as.integer(str_sub(valGust, 7, 8))

    curMet[gustCheck, "WindGust"] <- valGust

    curMet %>% 
        count(WindGust) %>% 
        as.data.frame %>%
        print()
    
    curMet
}
parseMETAR <- getWindGusts(parseMETAR, origMet=funcMETAR)


# Correct for SLP
fixSLP <- function(curMet) {

    dfParse <- curMet %>%
        mutate(modSLP=ifelse(curMet$SLP < 500, 1000 + curMet$SLP/10, 900 + curMet$SLP/10))

    p <- dfParse %>%
        group_by(SLP, modSLP) %>%
        summarize(n=n()) %>%
        ggplot(aes(x=SLP, y=modSLP, size=n)) + 
        geom_point(alpha=0.3)
    print(p)
    
    dfParse
}
parseMETAR <- fixSLP(parseMETAR)


# Check updated plots
plotcountsByMetric(parseMETAR, mets=c("WindGust", "Visibility", "modSLP"))

```

#### _Example #19: Functional Form For Relationships Between METAR Variables_  
The relationships between METAR variables can also be converted to functional form.

Example code includes:  
```{r}

# Function to calculate, display, and plot variable correlations
corMETAR <- function(met, numVars, subT="") {

    # Keep only complete cases and report on data kept
    dfUse <- met %>%
        select_at(vars(all_of(numVars))) %>%
        filter(complete.cases(.))
    
    nU <- nrow(dfUse)
    nM <- nrow(met)
    myPct <- round(100*nU/nM, 1)
    cat("\n *** Correlations use ", nU, " complete cases (", myPct, "% of ", nM, " total) ***\n", sep="")
    
    # Create the correlation matrix
    mtxCorr <- dfUse %>%
        cor()

    # Print the correlations
    mtxCorr %>%
        round(2) %>%
        print()

    # Display a heat map
    corrplot::corrplot(mtxCorr, method="color", title=paste0("Hourly Weather Correlations\n", subT))
}

# Define key numeric variables
coreNum <- c("TempC", "TempF", "DewC", "DewF", "Altimeter", "modSLP", "WindSpeed", "Visibility")

# Run the correlations function
corMETAR(parseMETAR, numVars=coreNum)


# Create a function for plotting two variables against each other
plotNumCor <- function(met, var1, var2, title=NULL, subT="") {
    if (is.null(title)) 
        { title <- paste0("Hourly Correlations of ", var1, " and ", var2) }
    p <- met %>%
        group_by_at(vars(all_of(c(var1, var2)))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=var1, y=var2)) + 
        geom_point(alpha=0.5, aes_string(size="n")) + 
        geom_smooth(method="lm", aes_string(weight="n")) + 
        labs(x=var1, y=var2, title=title, subtitle=subT)
    print(p)
}

var1List <- c("TempC", "DewC", "Altimeter", "TempF", "TempF",     "TempF",  "Altimeter")
var2List <- c("TempF", "DewF", "modSLP",    "DewF",  "Altimeter", "modSLP", "WindSpeed")

for (n in 1:length(var1List)) {
    plotNumCor(parseMETAR, var1List[n], var2List[n])
}


# Function for linear regressions on METAR data
lmMETAR <- function(met, y, x, yName, subT="Lincoln, NE (2016) Hourly METAR") {
    
    # Convert to formula
    myChar <- paste0(y, " ~ ", x)
    cat("\n *** Regression call is:", myChar, "***\n")
    
    # Run regression
    regr <- lm(formula(myChar), data=met)
    
    # Summarize regression
    print(summary(regr))
    
    # Predict the new values
    pred <- predict(regr, newdata=met)
    
    # Plot the predictions
    p <- met %>%
        select_at(vars(all_of(y))) %>%
        mutate(pred=pred) %>%
        group_by_at(vars(all_of(c(y, "pred")))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x=y, y="pred")) + 
        geom_point(aes(size=n), alpha=0.25) + 
        geom_smooth(aes(weight=n), method="lm") + 
        labs(title=paste0("Predicted vs. Actual ", yName, " - ", x, " as Predictor"), 
             subtitle=subT, x=paste0("Actual ", yName), y=paste0("Predicted ", yName)
             )
    print(p)
}

lmMETAR(parseMETAR, "modSLP", "Altimeter", yName="Sea Level Pressure")
lmMETAR(parseMETAR, "modSLP", "Altimeter + TempF", yName="Sea Level Pressure")

```

#### _Example #20: Functional Form for Extracting Cloud Data from METAR_  
Cloud data can also be extracted using the functional form.

Example code includes:  
```{r}

extractClouds <- function(met, metVar, subT="Lincoln, NE (2016) Hourly METAR") {

    metAll <- met %>%
        pull(metVar)
    
    # Extract the CLR records
    mtxCLR <- str_extract_all(metAll, pattern=" CLR ", simplify=TRUE)
    if (dim(mtxCLR)[[2]] != 1) { stop("Extracted 2+ CLR from some METAR; investigate") }
    isCLR <- ifelse(mtxCLR[, 1] == "", 0, 1)

    # Extract the VV records
    mtxVV <- str_extract_all(metAll, pattern="VV(\\d{3})", simplify=TRUE)
    if (dim(mtxVV)[[2]] > 1) { stop("Extracted 2+ VV from some METAR; investigate") }
    if ((dim(mtxVV))[[2]] == 0) {
        cat("\nNo Records with a cloud type of vertical visibility (VV)\n")
        isVV <- rep(0, times=length(isCLR))
        htVV <- rep(NA, times=length(isCLR))
    } else {
        isVV <- ifelse(mtxVV[, 1] == "", 0, 1)
        htVV <- ifelse(mtxVV[, 1] == "", NA, as.integer(str_replace(mtxVV[, 1], "VV", ""))*100)
    }

    # Extract the FEW records
    mtxFEW <- str_extract_all(metAll, pattern="FEW(\\d{3})", simplify=TRUE)
    numFEW <- apply(mtxFEW, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the SCT records
    mtxSCT <- str_extract_all(metAll, pattern="SCT(\\d{3})", simplify=TRUE)
    numSCT <- apply(mtxSCT, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the BKN records
    mtxBKN <- str_extract_all(metAll, pattern="BKN(\\d{3})", simplify=TRUE)
    numBKN <- apply(mtxBKN, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the OVC records
    mtxOVC <- str_extract_all(metAll, pattern="OVC(\\d{3})", simplify=TRUE)
    numOVC <- apply(mtxOVC, 1, FUN=function(x) { sum((x!=""))} )

    # Summarize as a data frame
    tblClouds <- tibble::tibble(isCLR=isCLR, isVV=isVV, htVV=htVV, numFEW=numFEW, 
                                numSCT=numSCT, numBKN=numBKN, numOVC=numOVC
                                )

    # Get the counts
    cat("\n*** Counts by number of layers of each cloud type ***\n")
    tblClouds %>% 
        count(isCLR, isVV, numFEW, numSCT, numBKN, numOVC) %>%
        as.data.frame() %>%
        print()

    # Investigate the problem data
    cat("\n*** METAR records where no clouds were extracted ***\n")
    metAll[rowSums(tblClouds, na.rm=TRUE)==0] %>%
        print()
    
    # Plot the counts of most obscuration
    p <- tblClouds %>%
        filter(rowSums(., na.rm=TRUE) > 0) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            )
               ) %>%
        ggplot(aes(x=wType, y=..count../sum(..count..))) + 
        geom_bar() + 
        labs(title="Highest Obscuration by Cloud", subtitle=subT, 
             x="Cloud Type", y="Proportion of Hourly Measurements"
             )
    print(p)
    
    # Integrate the clouds data
    mtxCloud <- cbind(mtxVV, mtxOVC, mtxBKN, mtxSCT, mtxFEW, mtxCLR)
    cat("\n*** Dimensions for the cloud matrix ***\n")
    print(dim(mtxCloud))
    
    list(tblClouds=tblClouds, mtxCloud=mtxCloud)
}

# Run the initial cloud extraction
initClouds <- extractClouds(parseMETAR, metVar="origMETAR")
str(initClouds)


# Cycle through to find levels of a given type
ckClouds <- function(cloudType, mtx) {
    isKey <- which(apply(mtx, 2, FUN=function(x) {sum(str_detect(x, cloudType))}) > 0)
    as.integer(str_replace(mtx[, min(isKey)], cloudType, "")) * 100
}


# Function to create the lowest cloud levels
findLowestClouds <- function(mtxCloud, subT="Lincoln, NE (2016) Hourly METAR") {

    # Find the lowest clouds by cloud type
    lowOVC <- ckClouds("OVC", mtx=mtxCloud)
    lowVV <- ckClouds("VV", mtx=mtxCloud)
    lowBKN <- ckClouds("BKN", mtx=mtxCloud)
    lowSCT <- ckClouds("SCT", mtx=mtxCloud)
    lowFEW <- ckClouds("FEW", mtx=mtxCloud)

    # Integrate the lowest cloud type by level
    lowCloud <- tibble::tibble(lowVV, lowOVC, lowBKN, lowSCT, lowFEW)
    cat("\n*** Lowest clouds by type tibble ***\n")
    print(lowCloud)

    # Get the lowest cloud level
    minCloud <- lowCloud
    minCloud[is.na(minCloud)] <- 999999
    minCloudLevel <- apply(minCloud, 1, FUN=min)
    minCeilingLevel <- apply(minCloud[, c("lowVV", "lowOVC", "lowBKN")], 1, FUN=min)

    noCloudPct <- mean(minCloudLevel == 999999)
    noCeilingPct <- mean(minCeilingLevel == 999999)

    # Plot the minimum cloud level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCloudLevel != 999999) %>%
        ggplot(aes(x=minCloudLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCloudPct), "% of obs. have no clouds")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Cloud Height (when some clouds exist)", subtitle=subT
             )
    print(p)

    # Plot the minimum ceiling level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCeilingLevel != 999999) %>%
        ggplot(aes(x=minCeilingLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCeilingPct), "% of obs. have no ceiling")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Ceiling Height (when a ceiling exists)", subtitle=subT
             )
    print(p)
    
    list(lowCloud=lowCloud, minCeilingLevel=minCeilingLevel, minCloudLevel=minCloudLevel)
}

processedClouds <-findLowestClouds(initClouds$mtxCloud)
str(processedClouds)

```

#### _Example #21: Functional Form for Plotting by factor variables_  
The month of the year is an interesting data point for plotting against.

Example code includes:  
```{r}

# Function to bind the existing parsed METAR data with the cloud data
bindMETAR <- function(dfParse, tblClouds, lowCloud) {

    # Integrate the cloud data and convert month to a factor
    dfFull <- cbind(dfParse, tblClouds, lowCloud) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            ), 
               month=factor(lubridate::month(dtime), levels=1:12, labels=month.abb)
               )
    
    dfFull <- tibble::as_tibble(dfFull)
    str(dfFull)
    
    dfFull
}

fullMETAR <- bindMETAR(dfParse=parseMETAR, 
                       tblClouds=initClouds$tblClouds, 
                       lowCloud=processedClouds$lowCloud
                       )


# Updated function for plotting numeric by factor
plotFactorNumeric <- function(met, fctVar, numVar, title=NULL, subT) {
    if (is.null(title)) { title <- paste0("Hourly Weather - ", numVar, " vs. ", fctVar) }
    p <- met %>%
        filter(!is.na(get(fctVar)), !is.na(get(numVar))) %>%
        ggplot(aes_string(x=fctVar, y=numVar)) + 
        geom_boxplot(fill="lightblue") + 
        labs(title=title, subtitle=subT)
    print(p)
}

# Function for creating cloud plots
makeFactorPlots <- function(met, 
                            fctVar=c("month", "wType"), 
                            keyVar=c("WindSpeed", "Visibility", "Altimeter", "TempF", "DewF"), 
                            desc="Lincoln, NE (2016) Hourly METAR"
                            ) {

    # Run for all of the key variables against wind speed and cloud type
    for (varF in fctVar) {
        for (varK in keyVar) { 
            plotFactorNumeric(met, fctVar=varF, numVar=varK, subT=desc) 
        }
    }

    # Create stacked bars for cloud type by month
    # dfFull %>%
    #     filter(!is.na(wType), wType!="Error") %>%
    #     ggplot(aes(x=month, fill=wType)) + 
    #     geom_bar(position="fill") + 
    #     labs(title="Lincoln, NE (2016)", x="", y="Proportion of Month")
}

makeFactorPlots(fullMETAR)

```

#### _Example #22: Combining Functional Forms for METAR Processing_  
The functions can be combined in to a single routine for reading, parsing, and running EDA on METAR data..

Example code includes:  
```{r}

# Function to run the full process
runAllMETAR <- function(fname, timeZ, expMin, expDays, locMET, shortMET, longMET, valMet, 
                        labsMet=c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
                                  "TempC", "DewC", "Altimeter", "SLP", "FahrC"
                                  ), 
                        keyMetric=c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
                                    "DewC", "Altimeter", "SLP", "TempF", "DewF"
                                    ), 
                        coreNum=c("TempC", "TempF", "DewC", "DewF", 
                                  "Altimeter", "modSLP", "WindSpeed", "Visibility"
                                  ), 
                        var1List=c("TempC", "DewC", "Altimeter", "TempF", "TempF", "TempF", "Altimeter"), 
                        var2List=c("TempF", "DewF", "modSLP", "DewF", "Altimeter", "modSLP", "WindSpeed")
                        ) {
    
    # Read in the METAR data
    funcMETAR <- readMETAR(fileName=fname, timeZ=timeZ, expMin=expMin, expDays=expDays)
    # funcMETAR

    # Extract wind data from METAR
    windMETAR <- extractWind(funcMETAR)
    # windMETAR

    # Run basic wind plots
    basicWindPlots(windMETAR, desc=locMET, gran=shortMET)


    # Run the METAR parsing on the raw data
    initMETAR <- initialParseMETAR(funcMETAR, val=valMet, labs=labsMet)
    # initMETAR

    # Parse and convert the METAR data
    convMETAR <- convertMETAR(initMETAR, metrics=keyMetric)
    # convMETAR

    # Fix problems with visibility, wind gusts, and SLP
    parseMETAR <- getVisibility(convMETAR, origMet=funcMETAR)
    parseMETAR <- getWindGusts(parseMETAR, origMet=funcMETAR)
    parseMETAR <- fixSLP(parseMETAR)

    # Check updated plots
    plotcountsByMetric(parseMETAR, mets=c("WindGust", "Visibility", "modSLP"))


    # Run the correlations function
    corMETAR(parseMETAR, numVars=coreNum, subT=longMET)

    # Plot correlations
    for (n in 1:length(var1List)) {
        plotNumCor(parseMETAR, var1List[n], var2List[n], subT=longMET)
    }

    # Run lm models for SLP vs Altimeter and (optionally) Temperature
    lmMETAR(parseMETAR, "modSLP", "Altimeter", yName="Sea Level Pressure", subT=longMET)
    lmMETAR(parseMETAR, "modSLP", "Altimeter + TempF", yName="Sea Level Pressure", subT=longMET)


    # Run the initial cloud extraction
    initClouds <- extractClouds(parseMETAR, metVar="origMETAR", subT=longMET)
    str(initClouds)

    # Find the lowest cloud levels and lowest ceilings
    processedClouds <-findLowestClouds(initClouds$mtxCloud, subT=longMET)
    str(processedClouds)


    # Bind the processed METAR and the cloud data
    fullMETAR <- bindMETAR(dfParse=parseMETAR, 
                           tblClouds=initClouds$tblClouds, 
                           lowCloud=processedClouds$lowCloud
                           )

    # Create box plots for key weather elements against month and cloud type
    makeFactorPlots(fullMETAR, desc=longMET)
    
    # Return all of the elements
    list(fullMETAR=fullMETAR, funcMETAR=funcMETAR, windMETAR=windMETAR, 
         initMETAR=initMETAR, convMETAR=convMETAR, parseMETAR=parseMETAR, 
         initClouds=initClouds, processedClouds=processedClouds
         )
}
```
  
Followed by caching the run of the function for Lincoln, NE:  
```{r cache=TRUE}
# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_klnk_2016.txt"  # file name for raw METAR data
timeZ <- "54Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:54:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Lincoln, NE"  # Description of city or location
shortMET <- "KLNK METAR (2016)"  # Station code and timing
longMET <- "Lincoln, NE Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Lincoln, NE
klnk2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(klnk2016METAR)

```

#### _Example #23: Running for a Different Station_  
The functions can be run for a different station.

Example code includes:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kord_2016.txt"  # file name for raw METAR data
timeZ <- "51Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:51:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Chicago, IL"  # Description of city or location
shortMET <- "KORD METAR (2016)"  # Station code and timing
longMET <- "Chicago, IL O'Hare Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Lincoln, NE
kord2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kord2016METAR)

```

#### _Example #24: Downloading the Data by Function_  
The [Iowa State Mesonet website](https://mesonet.agron.iastate.edu/request/download.phtml) provides instructions for automating the download process using Python or R, including links to a GitHub repository with instructions for [R download](https://github.com/realmiketalbot/R-scripts/blob/master/iem_scraper_example.r).  The code can be adapted for use here.

Example code includes:  
```{r}

# Function to get ASOS data from Iowa State
getASOSData <- function(faaID, startDate, endDate, suffix,
                        dirDownload = "./RInputFiles/", getAgain=FALSE) {

    # Create the file name and location for saving data
    fileName <- paste0("metar_k", str_to_lower(faaID), "_", suffix, ".txt")
    fileLoc <- paste0(dirDownload, fileName)

    # Check whether the file already exists, stop if so and getAgain is FALSE, otherwise download the data
    if (file.exists(fileLoc) & !getAgain) {
        cat("\nFile already exists - ", fileLoc)
        cat("\nStopping download routine\n")
        return(FALSE)
    } else {
        # Get the year, day, and hour of the key dates
        y1 <- lubridate::year(startDate)
        m1 <- lubridate::month(startDate)
        d1 <- lubridate::day(startDate)
    
        y2 <- lubridate::year(endDate)
        m2 <- lubridate::month(endDate)
        d2 <- lubridate::day(endDate)

        # Mimic the string shown below
        # https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LAS&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2
    
        baseURL <- "https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?"  # base URL
        useURL <- paste0(baseURL, "station=", faaID)  # add the desired station
        useURL <- paste0(useURL, "&data=all")  # request all data fields
        useURL <- paste0(useURL, "&year1=", y1, "&month1=", m1, "&day1=", d1)  # Specify start ymd
        useURL <- paste0(useURL, "&year2=", y2, "&month2=", m2, "&day2=", d2)  # Specify end ymd
        useURL <- paste0(useURL, "&tz=Etc%2FUTC&format=onlycomma&latlon=no")  # Formatting
        useURL <- paste0(useURL, "&missing=M&trace=T&direct=no&report_type=2")  # Formatting
    
        # Download the file
        download.file(useURL, destfile=fileLoc, method="curl")
        
        return(TRUE)
    }
}

```

And then cache the actual download step to minimize utilization of the Iowa State server (though the code is set to not download to an existing file anyway):
```{r cache=TRUE}

# Specify the FAA ID and Analysis Year
useFAAID <- "LAS"
analysisYear <- 2016

# Specify the start and end dates based on the analysis year
startDate <- ISOdate(analysisYear-1, 12, 31, hour=0)
endDate <- ISOdate(analysisYear+1, 1, 2, hour=0)

# Get the relevant data
getASOSData(faaID=useFAAID, startDate=startDate, endDate=endDate, suffix=analysisYear)

```

And then explore the data to set the key parameters for a full run of the METAR process:
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR based on above inputs
fname <- paste0("./RInputFiles/metar_k", str_to_lower(useFAAID), "_", analysisYear, ".txt")

# Find the most common Zulu time (this will be the METAR)
zTimes <- readr::read_csv(fname) %>%
    pull(metar) %>%
    str_match(pattern="\\d{2}Z") %>%
    as.vector() %>%
    table() %>%
    sort(decreasing=TRUE)

cat("\nThe most common Zulu time is", names(zTimes)[1], "\nFrequency is",
    round(100*zTimes[1]/sum(zTimes), 1), "% (", zTimes[1], "of", sum(zTimes), ")"
    )

# Use the most common Zulu time and start/end dates to set key parameters
timeZ <- names(zTimes)[1]  # Zulu time that METAR is recorded at this station
expMin <- ISOdate(analysisYear-1, 12, 31, hour=0, min=as.integer(str_replace(timeZ, "Z", "")))
expDays <- as.integer(endDate - startDate)  # Expected total days read

# Provide a descriptive name
locMET <- "Las Vegas, NV"  # Description of city or location
shortMET <- "KLAS METAR (2016)"  # Station code and timing
longMET <- "Las Vegas, NV Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Lincoln, NE
klas2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(klas2016METAR)

```

#### _Example #25: More Generic Downloading the Data by Function_  
The download function can be more generic, with just a few key arguments passed to it.  Those arguments can then be derived in a helper function, with the generic function called from inside that.

Example code includes:  
```{r}

genericGetASOSData <- function(fileLoc, 
                               stationID,
                               startDate, 
                               endDate,
                               downloadMethod="curl", 
                               baseURL="https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?", 
                               dataFields="all", 
                               dataTZ="Etc%2FUTC", 
                               dataFormat="onlycomma", 
                               dataLatLon="no", 
                               dataMissing="M", 
                               dataTrace="T", 
                               dataDirect="no", 
                               dataType=2
                               ) {
    
    # Get the year, day, and hour of the key dates
    y1 <- lubridate::year(startDate)
    m1 <- lubridate::month(startDate)
    d1 <- lubridate::day(startDate)
    
    y2 <- lubridate::year(endDate)
    m2 <- lubridate::month(endDate)
    d2 <- lubridate::day(endDate)

    # Mimic the string shown below
    # https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LAS&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2
    
    useURL <- paste0(baseURL, "station=", stationID)  # add the desired station
    useURL <- paste0(useURL, "&data=", dataFields)  # default is "all
    useURL <- paste0(useURL, "&year1=", y1, "&month1=", m1, "&day1=", d1)  # Specify start ymd
    useURL <- paste0(useURL, "&year2=", y2, "&month2=", m2, "&day2=", d2)  # Specify end ymd
    useURL <- paste0(useURL, "&tz=", dataTZ)  # time zone (default UTC)
    useURL <- paste0(useURL, "&format=", dataFormat)  # file format (default CSV)
    useURL <- paste0(useURL, "&latlon=", dataLatLon)  # Whether to include lat-lon (default no)
    useURL <- paste0(useURL, "&missing=", dataMissing)  # How to handle missing data (default is 'M')
    useURL <- paste0(useURL, "&trace=", dataTrace)  # How to handle trace data (default is 'T')
    useURL <- paste0(useURL, "&direct=", dataDirect)  # Whether to directly get the data (default is 'no')
    useURL <- paste0(useURL, "&report_type=", dataType)  # Whether to get just METAR (2, default)
    
    # Download the file
    cat("\nDownloading from:", useURL, "\nDownloading to:", fileLoc, "\n")
    download.file(useURL, destfile=fileLoc, method=downloadMethod)
        
    return(TRUE)
}

```
  
Which can then be called by a function that checks whether the file already exists:
```{r}

getASOSStationTime <- function(stationID, 
                               startDate=NULL, 
                               endDate=NULL, 
                               analysisYears=NULL, 
                               fileLoc=NULL,
                               ovrWrite=FALSE,
                               ...) {
    
    # Get the relevant time period for the data
    if (is.null(analysisYears) & (is.null(startDate) | is.null(endDate))) {
        stop("Must provide either analysisYears or both of startDate and endDate")
    }
    if (!is.null(startDate) & !is.null(endDate) & !is.null(analysisYears)) {
        stop("Should specify EITHER both of startDate and endDate OR analysisYears BUT NOT both")
    }
    if (is.null(startDate)) {
        startDate <- ISOdate(min(analysisYears)-1, 12, 31, hour=0)
        endDate <- ISOdate(max(analysisYears)+1, 1, 2, hour=0)
    }
    
    # Create the file name
    if (!is.null(analysisYears)) {
        if (length(analysisYears) == 1) { timeDesc <- analysisYears }
        else { timeDesc <- paste0(min(analysisYears), "-", max(analysisYears)) }
    } else {
        timeDesc <- paste0(lubridate::year(startDate), 
                           str_pad(lubridate::month(startDate), 2, pad="0"),
                           str_pad(lubridate::day(startDate), 2, pad="0"), 
                           "-", 
                           lubridate::year(endDate), 
                           str_pad(lubridate::month(endDate), 2, pad="0"), 
                           str_pad(lubridate::day(endDate), 2, pad="0")
                           )
    }
    
    if (is.null(fileLoc)) {
        fileLoc <- paste0("./RInputFiles/", "metar_k", str_to_lower(stationID), "_", timeDesc, ".txt")
    }
    
    cat("\nData for station", stationID, "from", as.character(startDate), "to", 
        as.character(endDate), "will download to", fileLoc, "\n"
        )
    
    if (file.exists(fileLoc) & !ovrWrite) {
        stop("File already exists, aborting")
    }
    
    genericGetASOSData(fileLoc=fileLoc, stationID=stationID, startDate=startDate, endDate=endDate, ...)
    
}

```
  
And the files can then be run (cached to avoid multiple hits to the Iowa State server):  
```{r cache=TRUE}

# Get data for EWR for 2016
getASOSStationTime(stationID="EWR", analysisYears=2016)

# Get data for ATL for 2016-2018
getASOSStationTime(stationID="ATL", analysisYears=2016:2018)

# Get data for DFW for 2016-03-31 to 2017-03-01
getASOSStationTime(stationID="DFW", 
                   startDate=ISOdate(2016, 03, 31, hour=0), 
                   endDate=ISOdate(2017, 3, 1, hour=0)
                   )

```

#### _Example #26: Comparative Wind Directions by Location_  
The processed data files can be compared, with wind directions being one of the comparison sets.

Example code includes:  
```{r}

# Extract the wind direction data from a processed METAR file
getWindDirGroup <- function(keyList, src) {
    
    # Use the fullMETAR data and extract WindDir, WindSpeed, month
    windPlotData <- get(keyList)[["fullMETAR"]] %>%
        select(WindDir, WindSpeed, month) %>%
        mutate(windDirGroup=factor(case_when(WindSpeed==0 ~ "No Wind", 
                                             WindDir=="VRB" ~ "Variable", 
                                             WindDir %in% c("030", "040", "050", "060") ~ "NE", 
                                             WindDir %in% c("070", "080", "090", "100", "110") ~ "E", 
                                             WindDir %in% c("120", "130", "140", "150") ~ "SE", 
                                             WindDir %in% c("160", "170", "180", "190", "200") ~ "S", 
                                             WindDir %in% c("210", "220", "230", "240") ~ "SW", 
                                             WindDir %in% c("250", "260", "270", "280", "290") ~ "W", 
                                             WindDir %in% c("300", "310", "320", "330") ~ "NW", 
                                             WindDir %in% c("340", "350", "360", "010", "020") ~ "N", 
                                            TRUE ~ "Error"
                                            ) , levels=c("No Wind", "Variable", "Error", 
                                                         "N", "NE", "E", "SE", "S", "SW", "W", "NW"
                                                         )
                                   )
               )
    
    # Rempve the errors and calculate percentages by month for the remainder
    processedWindData <- windPlotData %>%
        filter(windDirGroup != "Error") %>%
        group_by(month, windDirGroup) %>%
        summarize(n=n()) %>%
        ungroup() %>%
        group_by(month) %>%
        mutate(pct=n/sum(n)) %>%
        ungroup() %>%
        mutate(src=src)
    
    processedWindData

}


# Consolidate and plot wind data
consolidatePlotWind <- function(files, names) {

    consFun <- function(x, y) { getWindDirGroup(keyList=x, src=y) }
    boundByRows <- map2_dfr(.x=files, .y=names, .f=consFun)

    p <- boundByRows %>%
        ggplot(aes(x=month, y=pct, color=src)) + 
        geom_line(aes(group=src)) + 
        facet_wrap(~windDirGroup) + 
        labs(title="Wind Direction Frequency by Month", x="Month", y="Frequency of Wind Observations") +
        theme(axis.text.x=element_text(angle=90))
    print(p)
    
    boundByRows
}

# Run for 2016 with Lincoln, NE; Las Vegas, NV; and Chicago, IL
cpWind <- consolidatePlotWind(files=c("klnk2016METAR", "klas2016METAR", "kord2016METAR"), 
                              names=c("Lincoln, NE (2016)", "Las Vegas, NV (2016)", "Chicago, IL (2016)")
                              )
cpWind

```

#### _Example #27: Extracting Precipitation Information from METAR_  
METAR data include descriptions of the precipitation occuring at any given time.  Two of the most common precipitation forms are rain (RA) and SN().  These can occur together, denoted as RASN or SNRA in the METAR.

Further, the precipitation type can be classified using a prefix as light (-), moderate (no prefix), or heavy (+).  So, RA would be moderate rain, -SNRA would be a light snow-rain mix, +RA would be heavy rain.

Additionally, the timing of the precipitation event is captured in the remarks using B (begin) and E (end).  So, an hourly METAR of RAB20E35B50 would mean rain started at 20 past the hour, ended at 35 past the hour, and began again at 50 past the hour.  Since METAR are often taken just before the top of the hour, a four-digit time is used if it is in the 'previous' hour; for example, RAB1959E36 in the 2053Z METAR.

We can use the remarks to see when it was raining in the given METAR.

Example code includes:  
```{r}

# Extract the METAR and the date-time from a processed list
procMET <- klas2016METAR$fullMETAR %>% 
    select(origMETAR, dtime)

# Check whether there are comments for either rain regins (RAB) or rain ends (RAE) and pull all the data
procMET <- procMET %>% 
    mutate(isRain=grepl("RA[B|E]", origMETAR), 
           rainData=str_extract(origMETAR, pattern="(RA[B|E]\\d+[0-9BE]*)"), 
           nBegin=pmax(0, str_count(rainData, "B"), na.rm=TRUE), 
           nEnd=pmax(0, str_count(rainData, "E"), na.rm=TRUE), 
           dateUTC=lubridate::date(dtime), 
           hourUTC=lubridate::hour(dtime)
           )
str(procMET)

# Check the counts of rain beginning and rain ending
procMET %>%
    count(isRain, nBegin, nEnd)

# Extract all the times when rain began (can be multiple per METAR)
rainMET <- procMET %>%
    select(dateUTC, hourUTC, rainData, nBegin, nEnd)

# Confirm the rainMET is unique by dateUTC and hourUTC
dupAns <- rainMET %>%
    select(dateUTC, hourUTC) %>%
    duplicated() %>%
    any()
cat("\nAre there any problems with duplicated keys?", dupAns, "\n")

# Extract the matrix of rain beginning data
rainBegin <- rainMET %>% 
    pull(rainData) %>% 
    str_extract_all("B\\d+", simplify=TRUE) %>%
    as.data.frame(stringsAsFactors=FALSE)

# Extract the matrix of rain ending data
rainEnd <- rainMET %>% 
    pull(rainData) %>% 
    str_extract_all("E\\d+", simplify=TRUE) %>%
    as.data.frame(stringsAsFactors=FALSE)

# Convert the rain begins data to the hour and minute associated to the UTC
extractTime <- function(x, var, sym="B") {
    if (is.na(x[var]) | x[var]=="") {
        utcUse <- NA
    }
    else {
        utcUse <- str_replace(x[var], sym, "")
        if (str_length(utcUse)==4) {
            utcUse <- paste0(x["dateUTC"], " ", utcUse)
        } else if (str_length(utcUse)==2) {
            utcUse <- paste0(x["dateUTC"], " ", x["hourChar"], utcUse)
        } else {
            cat("\nCannot parse data: ", x, "\n", x[var], "\n", var, sym, utcUse)
            stop()
        }
    }
}

# Extract the begin times from V1
beginTime1 <- rainBegin %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V1", sym="B")

# Extract the begin times from V2
beginTime2 <- rainBegin %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V2", sym="B")

# Extract the end times from V1
endTime1 <- rainEnd %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V1", sym="E")

# Extract the end times from V2
endTime2 <- rainEnd %>%
    cbind(rainMET[, c("dateUTC", "hourUTC")]) %>%
    mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
    apply(1, FUN=extractTime, var="V2", sym="E")

allBegins <- 
    c(beginTime1[!is.na(beginTime1)], beginTime2[!is.na(beginTime2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

allEnds <- 
    c(endTime1[!is.na(endTime1)], endTime2[!is.na(endTime2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

# Check the each ending is after its associated beginning
endMinusBeg <- allEnds - allBegins

# Rain is listed as ending at 2016-05-01 00:42 and 2016-04-30 17:38 both for start 2016-04-30 11:20
endMinusBeg <- allEnds[allEnds != lubridate::ymd_hm("2016-05-01 0042")] - allBegins

# Plot the rain durations in minutes
data.frame(minutesRain=as.numeric(endMinusBeg), month=lubridate::month(allBegins)) %>%
    ggplot(aes(x=minutesRain)) +
    geom_histogram()

# Plot the rain totals (in hours) by month
data.frame(minutesRain=as.numeric(endMinusBeg), month=lubridate::month(allBegins)) %>%
    group_by(month) %>%
    summarize(minutesRain=sum(minutesRain), nRain=n()) %>%
    ggplot(aes(x=factor(month.abb[month], levels=month.abb[1:12]), y=minutesRain/60)) +
    geom_col() + 
    labs(title="Las Vegas, NV Rainfall (hours) in 2016", y="Hours of Rain", x="Month") + 
    geom_text(aes(y=2+minutesRain/60, label=round(minutesRain/60, 1)))

```

#### _Example #28: Function for Extracting Precipitation Information from METAR_  
The precipitation extraction process can be converted to functions.  In addition to being more modular, several additional features can be included:

* Allow to search for other precipitation types, specifically snow (SN) or drizzle (DZ)  
* Correct for the isue that a time of 2016-01-04 2359Z is placed in the 206-01-05 bucket by the code above  
* Identify areas where the intervals are not sensible or raise significant questions (negative durations, durations longer than 24 hours)  
* Check that intervals overlap with the time periods in the METAR that show that precipitation event  
  
Example code includes:  

* Extract the precipitation data from a processed METAR file  
* Find the begin and end times from this file  
* Check for intervals of questionable length and correct as needed  
  
```{r}

# Extract the precipitation data from a processed METAR file
extractPrecipData <- function(processedFile, pType="RA") {

    # Extract the METAR and the date-time from a processed list
    procMET <- processedFile[["fullMETAR"]] %>% 
        select(origMETAR, dtime)

    # Check whether there are comments for the desired pType either beginning or ending
    keyPattern <- paste0("(", pType, "[B|E]\\d+[0-9BE]*)")
    cat("\nRegex search code is:", keyPattern, "\n\n")
    
    procMET <- procMET %>% 
        mutate(precipData=str_extract(origMETAR, pattern=keyPattern), 
               isPrecip=grepl(paste0(pType, "[B|E]"), origMETAR, perl=TRUE), 
               nBegin=pmax(0, str_count(precipData, "B"), na.rm=TRUE), 
               nEnd=pmax(0, str_count(precipData, "E"), na.rm=TRUE), 
               dateUTC=lubridate::date(dtime), 
               hourUTC=lubridate::hour(dtime)
               )
    str(procMET)
    cat("\n")

    # Check the counts of precipitation beginning and rain ending
    procMET %>%
        count(isPrecip, nBegin, nEnd) %>%
        print()

    # Check that the file is unique by time
    dupAns <- procMET %>%
        select(dateUTC, hourUTC) %>%
        duplicated %>%
        any()

    cat("\nAre there any problems with duplicated keys?", dupAns, "\n")
    
    procMET
}

testFileProc <-extractPrecipData(klas2016METAR, pType="RA")

```
  
Next, a function for extrating beginning and ending times can be written:  
```{r}

# Helper function to extract the beginning and ending times using str_extract_all
getBeginEndTimeMatrix <- function(file, pullVar="precipData", pState="B") {
    file %>%
        pull(pullVar) %>%
        str_extract_all(paste0(pState, "\\d+"), simplify=TRUE) %>%
        as.data.frame(stringsAsFactors=FALSE)    
}

testBegin <- getBeginEndTimeMatrix(testFileProc, pState="B")
testEnd <- getBeginEndTimeMatrix(testFileProc, pState="E")

if (ncol(testBegin) != 2 | ncol(testEnd) != 2) { 
    stop("Hard-coded for 2 columns each of begin/end- Fix") 
}

# Helper function to convert begin and end time using date and hour
extractTime <- function(x, var, sym="B") {
    if (is.na(x[var]) | x[var]=="") {
        utcReturn <- NA
    }
    else {
        utcUse <- str_replace(x[var], sym, "")
        if (str_length(utcUse)==4) {
            utcReturn <- paste0(x["dateUTC"], " ", utcUse)
            # If a 4-digit time starts with 23 and is in the 0Z METAR, it is part of the previous day
            if(str_sub(utcUse, 1, 2)=="23" & as.numeric(x["hourChar"])==0) {
                utcReturn <- paste0(as.Date(x["dateUTC"]) - 1, " ", utcUse)
            }
        } else if (str_length(utcUse)==2) {
            utcReturn <- paste0(x["dateUTC"], " ", x["hourChar"], utcUse)
        } else {
            cat("\nCannot parse data: ", x, "\n", x[var], "\n", var, sym, utcUse)
            stop()
        }
    }
    utcReturn
}

getBeginEndTimeVector <- function(timeExtractFile, origFullFile, extractVar, extractSym) {
    timeExtractFile %>%
        cbind(origFullFile[, c("dateUTC", "hourUTC")]) %>%
        mutate(hourChar=str_pad(str_trim(as.character(hourUTC)), width=2, pad="0")) %>%
        apply(1, FUN=extractTime, var=extractVar, sym=extractSym)
}

testBT1 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V1", extractSym="B")
testBT2 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V2", extractSym="B")
testET1 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V1", extractSym="E")
testET2 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V2", extractSym="E")

testAllBegins <- 
    c(testBT1[!is.na(testBT1)], testBT2[!is.na(testBT2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

testAllEnds <- 
    c(testET1[!is.na(testET1)], testET2[!is.na(testET2)]) %>%
    lubridate::ymd_hm() %>%
    sort()

```
  
Next, create the time intervals vector, with the capability to change the start state, check the intervals, exclude times as needed, and compare to raw METAR.
  
Example code includes:  
```{r}

# Function to create the time intervals data
createPrecipInterval <- function(endVector, beginVector, endExclude=c(), beginExclude=c(), 
                                 sState=FALSE, nMinPrint=1, maxProb=1000, nMaxPrint=1
                                 ) {
    
    # If the starting state is one of precipitation, allow it to 'burn in' by deleting the first end time
    if(sState) { endVector <- endVector[2:length(endVector)] }
    
    # Create the interval data
    endsUse <- endVector[!(endVector %in% lubridate::ymd_hm(endExclude))]
    beginsUse <- beginVector[!(beginVector %in% lubridate::ymd_hm(beginExclude))]
    intervalData <- endsUse - beginsUse
    
    # Show a summary of the interval data
    print(summary(as.numeric(intervalData)))
    
    # If there are any non-positive intervals, print the data causing the first of them
    if (min(as.numeric(intervalData)) <= 0) {
        cat("\nProblem Detected - Intervals are not positive.  Data to help investigate\n")
        posns <- which(as.numeric(intervalData) <= 0)
        posns <- max(1, posns[1]-5):min(length(beginsUse), length(endsUse), posns[nMinPrint]+5)
        cat("\nVector of Begins\n")
        print(lubridate::as_datetime(beginsUse[posns]))
        cat("\nVector of Ends\n")
        print(lubridate::as_datetime(endsUse[posns]))
        cat("\n")
    }
    
    # If there are any very long positive intervals, print the data causing the first five of them
    if (max(as.numeric(intervalData)) >= maxProb) {
        cat("\nPotential problem Detected - very long.  Data to help investigate\n")
        posns <- which(as.numeric(intervalData) >= maxProb)
        cat("\nPositions with problems are:", posns)
        posns <- max(1, posns[1]-5):min(length(beginsUse), length(endsUse), posns[min(length(posns), nMaxPrint)]+5)
        cat("\nVector of Begins\n")
        print(lubridate::as_datetime(beginsUse[posns]))
        cat("\nVector of Ends\n")
        print(lubridate::as_datetime(endsUse[posns]))
        cat("\n")
    }
    
    # Return the interval data
    intervalData
}

# Run a full pass for Las Vegas Rain
testIntervals <- createPrecipInterval(testAllEnds, testAllBegins)
testIntervals <- createPrecipInterval(testAllEnds, testAllBegins, endExclude=c("2016-05-01 0042"))


createPrecipIntervalPlots <- function(intervalData, beginsData, titleText, yAxisText, 
                                      beginExclude=c(), returnPlotsAndData=FALSE
                                      ) {

    # Exclude any data from begins as needed
    beginsData <- beginsData[!(beginsData %in% lubridate::ymd_hm(beginExclude))]
    
    # Create a plotting data frame
    histFrame <- data.frame(minutesPrecip=as.numeric(intervalData), 
                            month=lubridate::month(beginsData), 
                            rainDate=lubridate::date(beginsData)
                            ) %>%
        mutate(hoursPrecip=minutesPrecip/60)
    
    # Plot the precipitation durations in hours
    p1 <- histFrame %>%
        ggplot(aes(x=hoursPrecip)) +
        geom_histogram() + 
        labs(title=titleText, x=yAxisText, 
             subtitle="Distribution of hours per unique precipitation event"
             )
    print(p1)

    # Plot the precipitation by day in hours
    p2 <- histFrame %>%
        group_by(rainDate) %>%
        summarize(hoursPrecip=sum(hoursPrecip)) %>%
        ggplot(aes(x=hoursPrecip)) +
        geom_histogram() + 
        labs(title=titleText, x=yAxisText, 
             subtitle="Distribution of hours per day of precipitation (on days when 1+ minutes occurred)"
             )
    print(p2)
    
    # Plot the rain totals (in hours) by month
    # Create a data frame of all months and merge in precipitation data as an where available (0 otherwise)
    monthFrame <- histFrame %>%
        group_by(month) %>%
        summarize(minutesPrecip=sum(minutesPrecip), hoursPrecip=sum(hoursPrecip), nPrecip=n()) %>%
        right_join(data.frame(month=1:12, monthName=month.abb[1:12]), by="month") %>%
        tidyr::replace_na(list(minutesPrecip=0, hoursPrecip=0, nPrecip=0))
    # print(monthFrame)
    
    p3 <- monthFrame %>%
        ggplot(aes(x=factor(monthName, levels=month.abb[1:12]), y=hoursPrecip)) +
        geom_col() + 
        labs(title=titleText, y=yAxisText, x="") + 
        geom_text(aes(y=2 + hoursPrecip, label=round(hoursPrecip, 1)))
    print(p3)
    
    if (returnPlotsAndData) {
        list(histFrame=histFrame, monthFrame=monthFrame, p1=p1, p2=p2, p3=p3)
    } else {
        NULL
    }
}

# Plots for Las Vegas Rain
createPrecipIntervalPlots(testIntervals, testAllBegins, 
                          titleText="Las Vegas, NV Rainfall (hours) in 2016", yAxisText="Hours of Rain"
                          )

```

#### _Example #29: Combining Functions and Extending to Other Locales and Precipitation Types_  
The functions can be combined, and the approach then extended to other locales and precipitation types.

Example code includes:  
```{r}

# Combining all the functions in one place
runFullPrecipExtraction <- function(df, 
                                    pType, 
                                    titleText, 
                                    yAxisText,
                                    endExclude=c(), 
                                    beginExclude=c(), 
                                    endAdd=c(),
                                    beginAdd=c(),
                                    maxProb=1000, 
                                    sState=FALSE, 
                                    makePlots=TRUE, 
                                    returnPlotsAndData=FALSE
                                    ) {
    
    # Extract the precipitation data from a specified processed METAR file
    testFileProc <-extractPrecipData(df, pType=pType)
    
    # Confirm that the two-column specification is met (should relax hard-coding on this)
    testBegin <- getBeginEndTimeMatrix(testFileProc, pState="B")
    testEnd <- getBeginEndTimeMatrix(testFileProc, pState="E")

    if (!(ncol(testBegin) %in% c(1, 2, 3)) | !(ncol(testEnd) %in% c(1, 2, 3))) { 
        cat("\nBegin columns:", ncol(testBegin), "\t\tEndcolumns:", ncol(testEnd))
        stop("Hard-coded for 1-3 columns each of begin/end- Fix")
    }
    
    # Extract the beginning and ending information
    testBT1 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V1", extractSym="B")
    testET1 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V1", extractSym="E")
    
    if (ncol(testBegin) >= 2) {
        testBT2 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V2", extractSym="B")
    } else {
        testBT2 <- c()
    }
    if (ncol(testEnd) >= 2) {
        testET2 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V2", extractSym="E")
    } else {
        testET2 <- c()
    }

    if (ncol(testBegin) >= 3) {
        testBT3 <- getBeginEndTimeVector(testBegin, testFileProc, extractVar="V3", extractSym="B")
    } else {
        testBT3 <- c()
    }
    if (ncol(testEnd) >= 3) {
        testET3 <- getBeginEndTimeVector(testEnd, testFileProc, extractVar="V3", extractSym="E")
    } else {
        testET3 <- c()
    }
    
    testAllBegins <- 
        c(testBT1[!is.na(testBT1)], testBT2[!is.na(testBT2)], testBT3[!is.na(testBT3)], beginAdd) %>%
        lubridate::ymd_hm() %>%
        sort()

    testAllEnds <- 
        c(testET1[!is.na(testET1)], testET2[!is.na(testET2)], testET3[!is.na(testET3)], endAdd) %>%
        lubridate::ymd_hm() %>%
        sort()
    
    # Create the intervals
    testIntervals <- createPrecipInterval(testAllEnds, testAllBegins, 
                                          endExclude=endExclude, beginExclude=beginExclude, 
                                          maxProb=maxProb, sState=sState
                                          )
    
    # Create the precipitation plots
    plotOut <- NULL
    if (makePlots) {
        plotOut <- createPrecipIntervalPlots(testIntervals, testAllBegins, titleText=titleText, 
                                             yAxisText=yAxisText, beginExclude=beginExclude, 
                                             returnPlotsAndData=returnPlotsAndData
                                             )
    }
    
    if (!returnPlotsAndData) { plotOut <- NULL }
    
    # Return all of the key files, along with the parameters used
    keyParams <- list(fileName=deparse(substitute(df)), pType=pType, 
                      endExclude=endExclude, beginExclude=beginExclude, 
                      endAdd=endAdd, beginAdd=beginAdd,
                      maxProb=maxProb, sState=sState
                      )
    list(keyParams=keyParams, 
         testFileProc=testFileProc, 
         testAllBegins=testAllBegins[!(testAllBegins %in% lubridate::ymd_hm(beginExclude))], 
         testAllEnds=testAllEnds[!(testAllEnds %in% lubridate::ymd_hm(endExclude))], 
         testIntervals=testIntervals,
         plotOut=plotOut
         )
}

```
  
The full function can then be run to replicate the Las Vegas, NV 2016 rain plots:  
```{r}

# Run for Las Vegas, NV 2016 rainfall
klasRain2016 <- runFullPrecipExtraction(klas2016METAR, 
                                        pType="RA", 
                                        titleText="Las Vegas, NV Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-05-01 0042"),
                                        beginExclude=c(), 
                                        maxProb=1000, 
                                        sState=FALSE
                                        )

# Confirm that results are the same as when run outside the function
identical(testIntervals, klasRain2016$testIntervals)
identical(testFileProc, klasRain2016$testFileProc)
identical(testAllBegins, klasRain2016$testAllBegins)
identical(testAllEnds, klasRain2016$testAllEnds)

# Show the key parameters used
klasRain2016$keyParams

```

The full functions can then be explored for Chicago, IL 2016 rain plots:  
```{r}

# Run for Chicago, IL 2016 rainfall - run with no plots while debugging begin/end exclude
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c(),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the first problem end time - 2016-03-01 04:14:00 UTC
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414"),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the second problem end time - 2016-03-24 22:14:00 UTC
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414", "2016-03-24 2214"),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the begin time that creates a 3-day continuous rainfall - 2016-07-13 2347
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414", "2016-03-24 2214"),
                                        beginExclude=c("2016-07-13 2347"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# Exclude the third problem end time - 2016-08-29 20:48:00 UTC
# As this is the final data integrity issue, show the plots
kordRain2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-03-01 0414", "2016-03-24 2214", 
                                                     "2016-08-29 2048"
                                                     ),
                                        beginExclude=c("2016-07-13 2347"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Show the key parameters used
kordRain2016$keyParams

```

The full functions can then be explored for Chicago, IL 2016 snow plots:  
```{r}

# Run for Chicago, IL 2016 snowfall - run with no plots while debugging begin/end exclude
kordSnow2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="SN", 
                                        titleText="Chicago, IL Snowfall (hours) in 2016", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c(),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=FALSE
                                        )

# After investigation, there are five extraneous end times in the Chicago snowfall data - exclude them
# Create the plots
kordSnow2016 <- runFullPrecipExtraction(kord2016METAR, 
                                        pType="SN", 
                                        titleText="Chicago, IL Snowfall (hours) in 2016", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c("2016-02-10 0216", "2016-03-24 2331", 
                                                     "2016-04-08 2300", "2016-04-09 0446",
                                                     "2016-12-24 0309"
                                                     ),
                                        beginExclude=c(), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Show the key parameters used
kordSnow2016$keyParams

```

#### _Example #30: Checking Intervals for Consistency_  
The intervals created above can be compared against the hourly METAR observations for consistency.  In general, RAE53 would lead to no precipitation recorded at 53Z while RAB53 would lead to precipitation recorded at 53Z.  So, the intervals created should be reduced by 1 for purposes of overlap analysis.

Example code includes:  
```{r}

library(lubridate)  # lubridate has the %within% function and creates and checks intervals


intervalConsistency <- function(lst, pType){

    # Extract the beginning and interval times
    begins <- lst[["testAllBegins"]]
    ends <- lst[["testAllEnds"]]
    durs <- lst[["testIntervals"]]


    # Create intervals from the raw list file
    precipInts <- interval(begins, begins + durs - 1)

    
    # Extract the METAR and date-time information
    metar <- lst[["testFileProc"]][["origMETAR"]]
    dtime <- lst[["testFileProc"]][["dtime"]]
    
    # Take each METAR observation and check two factors
    # Is the precipitation type recorded in that METAR?
    # Does that METAR fall in any of the intervals?
    precipMETAR <- grepl(paste0(pType, ".*RMK"), metar, perl=TRUE)
    intMETAR <- sapply(dtime, FUN=function(x) {x %within% precipInts %>% any()})


    # Check for the consistency of the observations and print the mismatches
    print(table(precipMETAR, intMETAR))

    mism <- which(precipMETAR != intMETAR)
    if (length(mism) == 0) {
        cat("\nFull matches between METAR observations and intervals\n")
    } else {
        for (x in mism) {
            cat("\nMismatch at time", strftime(dtime[x], format="%Y-%m-%d %H:%M", tz="UTC"), "UTC\n")
            print(metar[max(1, x-2):min(length(metar), x+2)])
        }
    }
    
    list(precipInts=precipInts, mismatches=mism, mismatchTimes=dtime[mism])
}

# Mismatch due to TSRA without RAB on 2016-04-30
tmp <- intervalConsistency(klasRain2016, pType="RA")

# Mismatches due to malformatted METAR with missing data on 2016-07-13
# Mismatch due to RA vs FZRA on 2016-02-29
# Mismatch due to -RASN without RAB on 2016-03-24
# Mismatch due to TSRA without RAB on 2016-08-29
tmp <- intervalConsistency(kordRain2016, pType="RA")

# Mismatches due to SN without SNB on 2016-02-09, 2016-03-24, 2016-04-08, 2016-12-23
tmp <- intervalConsistency(kordSnow2016, pType="SN")

```
  
#### _Example #31: All Precipitation Types_  
The METAR data can be explored to find all the precipitation types it contains.  Broadly, a precipitation type should meet five criteria:

* Occurs prior to RMK (the remarks section has letter-only data that is not precipitation)  
* Preceded by a space  
* Optionally, begins with + (heavy) or - (light)  
* Contains only capital letters (no numbers) after the optional +/-  
* Trailed by a space  
  
This will extract some non-precipitation types such as CLR and AUTO, so some iteration is expected.

Example code includes:  
```{r}

# Function to find precipitation types
findPrecipTypes <- function(lst, precipRegex="(?<= )[+-]?[A-Z]+(?= )", priorTo="RMK", exclTypes=NULL) {

    metar <- lst[["testFileProc"]][["origMETAR"]]
    
    # Keep only everything prior to priorTo
    if (!is.null(priorTo)) {
        rmkLoc <- str_locate(metar, priorTo)
        rmkLoc[is.na(rmkLoc)] <- -1  # keep everything if NA
        metar <- str_sub(metar, start=1, end=pmax(-1, rmkLoc[, 1]-1))
    }
    
    # Exclude any items from the METAR
    if (!is.null(exclTypes)) {
        for (excl in exclTypes) {
            metar <- str_replace(metar, excl, "")
        }
    }
    
    # Is there a possible precipitation type in the file?
    pExists <- grepl(precipRegex, metar, perl=TRUE)
    
    cat("\nPrecipitation data status by METAR record\n\n")
    print(table(pExists))
    cat("\n")
    
    # Find the precipitation matches
    precipMatches <- str_match_all(metar, pattern=precipRegex)
    
    # Confirm that there are a maximum of two precipitation types per METAR
    listLengths <- sapply(precipMatches, FUN=length)
    if (max(listLengths) > 4) {
        cat("\nMaximum combinations observed is:", max(listLengths), "\n")
        stop("Hard-coded for at most 4 precipitation matches, please investigate")
    } else if (max(listLengths)==0) {
        cat("\nNo precipitation detected in this file\n")
    }
    
    if (max(listLengths) %in% c(2, 3, 4)) {
        cat("\nMultiple Precipitation types in the same record include\n")
        sapply(precipMatches[listLengths %in% c(2, 3, 4)], FUN=paste, collapse=" ") %>% 
            table() %>% 
            sort(decreasing=TRUE) %>%
            print()
    }
    
    if (sum(listLengths==1) > 0) {
        cat("\nSingle Precipitation types in the same record include\n")
        sapply(precipMatches[listLengths==1], FUN=c) %>% 
            table() %>% 
            sort(decreasing=TRUE) %>%
            print()
    }    

    # Extract the second column and summarize precipitation types
    # table(precipMatches[, 2]) %>% sort(decreasing=TRUE) %>% print()
    
    precipMatches
}

# Original pass for Chicago, IL
x1 <- findPrecipTypes(kordRain2016)

# Exclude AUTO and CLR as they are a record state and cloud type, not precipitation
x2 <- findPrecipTypes(kordRain2016, exclTypes=c("CLR", "AUTO"))


# Original pass for Las Vegas, NV
x3 <- findPrecipTypes(klasRain2016)

# Exclude AUTO and CLR as they are a record state and cloud type, not precipitation
x4 <- findPrecipTypes(klasRain2016, exclTypes=c("CLR", "AUTO"))

```
  
#### _Example #32: Extracting Precipitation Amounts_  
The METAR also contains summary of precipitation amounts that have occurred in the past hour(s).  Broadly, the format is:
  
* Pdddd - the amount of liquid precipitation equivalent that has fallen in the past hour  
* 6dddd - the amount of liquid precipitation equivalent that has fallen in the past 3/6 hours (for 0Z, 6Z, 12Z, and 18Z it is 6 hours; and for 3Z, 9Z, 15Z, and 21Z it is 3 hours)  
* 7dddd - the amount of liquid precipitation equivalent that has fallen in the past 24 hours  
  
The data as such can be extracted from the METAR.
  
Example code includes:  
```{r}

# Helper function to extract and convert data
extractConvertPrecipData <- function(var, regPattern) {
    
    mtxPrecip <- str_match(var, pattern=regPattern)
    vecPrecip <- mtxPrecip[, 2]
    vecPrecip[is.na(vecPrecip)] <- "0000"
    vecPrecip <- as.integer(vecPrecip) / 100
    
    vecPrecip
}

# Function to extract key precipitation information
extractLiquidPrecipAmounts <- function(lst) {
    
    # Pull the metar and the dtime
    metar <- lst[["testFileProc"]][["origMETAR"]]
    dtime <- lst[["testFileProc"]][["dtime"]]
    
    # Extract the Pdddd amounts (only available if liquid precipitation equivalent non-zero)
    pAmounts1Hour <- extractConvertPrecipData(metar, regPattern="RMK.* P(\\d{4})")
    cat("\nHourly totals for liquid precipitation equivalents:\n\n")
    table(pAmounts1Hour) %>% print()
    
    # Extract the 6dddd amounts
    pAmounts6Hour <- extractConvertPrecipData(metar, regPattern="RMK.* 6(\\d{4})")
    cat("\n3/6-hourly totals for liquid precipitation equivalents:\n\n")
    table(pAmounts6Hour) %>% print()
    
    # Extract the 7dddd amounts
    pAmounts24Hour <- extractConvertPrecipData(metar, regPattern="RMK.* 7(\\d{4})")
    cat("\n3/24-hourly totals for liquid precipitation equivalents:\n\n")
    table(pAmounts24Hour) %>% print()
    
    # Create a tibble and add the Zulu time
    tbl <- tibble::tibble(metar=metar, 
                          dtime=dtime, 
                          p1Hour=pAmounts1Hour, 
                          p3or6Hour=pAmounts6Hour, 
                          p24Hour=pAmounts24Hour
                          ) %>%
        mutate(zTime=(lubridate::hour(dtime) + ifelse(lubridate::minute(dtime)==0, 0, 1)) %% 24, 
               p6Hour=ifelse((zTime %% 6)==0, p3or6Hour, 0), 
               p3Hour=ifelse((zTime %% 3)==0, p3or6Hour-p6Hour, 0)
               )
    
    # Summarize the key amounts by Zulu time
    tbl %>%
        group_by(zTime) %>%
        summarize_if(is.numeric, sum) %>%
        as.data.frame() %>%
        print()
    
    tbl
    
}

klasPrecip <- extractLiquidPrecipAmounts(klasRain2016)
kordPrecip <- extractLiquidPrecipAmounts(kordRain2016)

```
  
Precipitation summarize can then be plotted by various time intervals:
```{r}

plotPrecipHistogram <- function(df, var, xlab, title, mod=1, rem=0) {

    # Create a variable for whether the modulo matches the desired remainder
    df <- df %>%
        mutate(isMod=(df$zTime %% mod)==rem)
    
    # Separate in to records for further proceesing and records to discard
    dfUse <- df %>%
        filter(isMod)
    dfDiscard <- df %>%
        filter(!isMod)
    
    # Summarize the discarded records
    cat(nrow(dfDiscard), "records have been discarded due to not matching the modulo rules")
    cat("\nThese discarded rows have", sum(dfDiscard %>% pull(var)), "inches of precipitation\n")
    
    numZero <- sum(dfUse[, var]==0)
    numTotal <- nrow(dfUse)
    pZero <- round(numZero/numTotal, 3)

    p <- dfUse %>%
        filter_at(vars(var), any_vars(. > 0)) %>%
        ggplot(aes_string(x=var)) + 
        geom_histogram() + 
        labs(x=xlab, y="Frequency", title=title, 
             subtitle=paste0("Includes only the ", 100*(1-pZero), "% of non-zero observations (", 
                             numZero, " of ", numTotal, " observations are zero)"
                             )
         )
    
    print(p)

}

plotPrecipHistogram(klasPrecip, 
                    var="p1Hour", 
                    xlab="1-hour precipitation (inches)", 
                    title="Las Vegas, NV (2016) Hourly METAR"
                    )

plotPrecipHistogram(klasPrecip, 
                    var="p6Hour", 
                    xlab="6-hour precipitation (inches)", 
                    title="Las Vegas, NV (2016) Hourly METAR", 
                    mod=6
                    )

plotPrecipHistogram(klasPrecip, 
                    var="p24Hour", 
                    xlab="24-hour precipitation (inches)", 
                    title="Las Vegas, NV (2016) Hourly METAR", 
                    mod=24, 
                    rem=12
                    )

```

#### _Example #33: Validating Consistency of Precipitation Amounts_  
The precipitation amounts per day summed from the 1-hour, 6-hour, and 24-hour columns should be nearly identical give or take rounding errors.

Example code includes:  
```{r}

checkPrecipConsistency <- function(df, title, subT="", hour24=12, maxDelta=0.02,
                                   sumVars=c("p24Hour", "p6Hour", "p1Hour")
                                   ) {

    # Create the data file for analysis - updated time, summarized by month and day
    dfPrecip <- df %>% 
        mutate(dtUse=dtime-lubridate::hours(hour24), 
               year=lubridate::year(dtUse), 
               month=lubridate::month(dtUse), 
               day=lubridate::day(dtUse), 
               n=1
               ) %>% 
        group_by(year, month, day) %>% 
        summarize_if(is.numeric, sum) %>%
        ungroup()
    
    head(dfPrecip) %>% 
        select_at(vars(all_of(c("year", "month", "day", "n", sumVars)))) %>%
        print()
    
    
    # Plot the monthly totals
    p1Data <- dfPrecip %>%
        mutate(ym=paste0(year, "-", str_pad(month, width=2, side="left", pad="0"))) %>%
        select_at(vars(all_of(c("ym", sumVars)))) %>%
        group_by(ym) %>%
        summarize_all(sum)
    print(p1Data)
    
    p1 <- p1Data %>%
        pivot_longer(-ym, names_to="timePeriod", values_to="inchesPrecip") %>%
        ggplot(aes(x=factor(ym), y=inchesPrecip, group=timePeriod, color=timePeriod)) + 
            geom_line(lwd=1.5) + 
        labs(x="Month", y="Inches of Liquid Precipitation", title=title, subtitle=subT)
    print(p1)
    
    # Output any days that have differences of more than maxDelta inches
    mismPrecip <- dfPrecip %>%
        select_at(vars(c("month", "day", all_of(sumVars)))) 
    
    mismMinMax <- mismPrecip %>%
        select_at(vars(all_of(sumVars))) %>%
        apply(1, FUN=function(x) { c(max(x), min(x), diff(range(x))) }) %>%
        t() %>%
        as.data.frame()
    names(mismMinMax) <- c("maxPrecip", "minPrecip", "delta")
    
    mismPrecip <- bind_cols(mismPrecip, mismMinMax)

    cat("\nMismatch precipitation amounts by day are:\n")
    table(mismPrecip$delta) %>% round(2) %>% print()
    
    cat("\n\nMismatch days of worse than maxDelta inches include\n")
    mismPrecip %>%
        filter(delta > maxDelta) %>%
        as.data.frame() %>%
        print()
    
}

checkPrecipConsistency(klasPrecip, title="Las Vegas, NV 2016 Precipitation by Month")
checkPrecipConsistency(kordPrecip, title="Chicago, IL 2016 Precipitation by Month")

```
  
While the Las Vegas, NV precipitation data are consistent on the 1-hour, 6-hour, and 24-hour measurements, there are significanr differences for Chicago, IL (particularly 1-hour outliers in Jan-Feb-Mar).

For further exploration, another cold weather city (Lincoln, NE) is assessed:
```{r}

# Run for Lincoln, NE 2016 rainfall
klnkRain2016 <- runFullPrecipExtraction(klnk2016METAR, 
                                        pType="RA", 
                                        titleText="Lincoln, NE Rainfall (hours) in 2016", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2016-09-16 1011", "2016-08-29 2329"),
                                        beginExclude=c("2016-07-13 1301"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Run for Lincoln, NE 2016 snowfall
klnkSnow2016 <- runFullPrecipExtraction(klnk2016METAR, 
                                        pType="SN", 
                                        titleText="Lincoln, NE Snowfall (hours) in 2016", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c(),
                                        beginExclude=c("2016-01-19 1644"), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Check for interval consistency in the Lincoln, NE 2016 rainfall data
# Weird issue on 2016-07-13 1354 RAB without RAE
# 2016-08-29 2254 TSRA without RAB
# 2016-09-16 0654 many mismatches due to missing RAB here
tmp <- intervalConsistency(klnkRain2016, pType="RA")

# Check for interval consistency in the Lincoln, NE 2016 rainfall data
tmp <- intervalConsistency(klnkSnow2016, pType="SN")

# Original pass for Lincoln, NE
x5 <- findPrecipTypes(klnkRain2016)

# Exclude AUTO and CLR as they are a record state and cloud type, not precipitation
x6 <- findPrecipTypes(klnkRain2016, exclTypes=c("CLR", "AUTO"))

# Get the Lincoln, NE 2016 liquid precipitation
klnkPrecip <- extractLiquidPrecipAmounts(klnkRain2016)

# Check for consistency in the Lincoln, NE 2016 precipitation data
checkPrecipConsistency(klnkPrecip, title="Lincoln, NE 2016 Precipitation by Month")

```
  
There are mismatches on 4 days of 2016 in the Lincoln, NE data though these appear to be manual error rather than systematic error due to cold weather.  Perhaps consulting official weather records can help determine the true liquid precipitation o the days in question.
  
#### _Example #34: Checking METAR Data for Gaps and Problems_  
Perhaps some of the inconsistencies in METAR data are driven by missing observations or sensor anomalies.  Missing observations can be detected as records expected but not detected, while sensor anomalies are flagged by a trailing '$' in the METAR record.

Example code includes:  
```{r}

checkGapsAnomalies <- function(lst, minDay, maxDay, loc, hour24=12) {
    
    # Pull the METAR data and datetime
    metar <- lst[["testFileProc"]][["origMETAR"]]
    dtime <- lst[["testFileProc"]][["dtime"]]
    
    # Create analysis data frame
    dfUse <- tibble::tibble(metar=metar, dtime=dtime) %>%
        mutate(dtUse=dtime-lubridate::hours(hour24), 
               year=lubridate::year(dtUse), 
               month=lubridate::month(dtUse), 
               day=lubridate::day(dtUse), 
               ym=paste0(year, "-", str_pad(month, width=2, side="left", pad="0")),
               isAnomaly=grepl("\\$$", metar),
               n=1
               )
    
    cat("\nData file with new time and anomaly variable\n")
    dim(dfUse) %>% print()
    names(dfUse) %>% print()
    
    # Keep only days between minDay and maxDay inclusive
    dfUse <- dfUse %>%
        filter(lubridate::date(dtUse) >= as.Date(minDay), lubridate::date(dtUse) <= as.Date(maxDay))
    
    cat("\nData file filtered to include only desired times\n")
    dim(dfUse) %>% print()
    names(dfUse) %>% print()
    
    # Sum n and isAnomaly by day
    dfIssue <- dfUse %>%
        group_by(ym, day) %>%
        summarize(n=sum(n), anomaly=sum(isAnomaly)) %>%
        ungroup() %>%
        mutate(missObs=24-n)
    summary(dfIssue) %>% print()
    
    # Plot summary by month
    dfPlot <- dfIssue %>%
        group_by(ym) %>%
        summarize(missObsDays=sum(missObs > 0), missObs=sum(missObs), 
                  anomalyDays=sum(anomaly > 0), anomaly=sum(anomaly), 
                  nDays=n()
                  ) %>%
        ungroup()
    
    # Create plot of missing observations
    p1 <- dfPlot %>%
        ggplot(aes(x=factor(ym), y=missObs)) + 
        geom_line(aes(group=1), lwd=2, color="red") + 
        geom_text(aes(y=missObs+0.5, label=ifelse(missObs>0, missObs, "")), color="red") + 
        labs(x="", y="Missing METAR Observations in Month", 
             title=paste0(loc, " Missing METAR Observations by Month")
             ) + 
        ylim(0, NA)
    print(p1)
    
    # Create plot of missing observations
    p2 <- dfPlot %>%
        ggplot(aes(x=factor(ym), y=anomaly)) + 
        geom_line(aes(group=1), lwd=2, color="blue") + 
        labs(x="", y="Potentially Anomalous (trailing $) METAR Observations", 
             title=paste0(loc, " Potentially Anomalous METAR Observations by Month")
             ) + 
        ylim(0, NA)
    print(p2)
    
    # Return the plotting frame
    dfPlot
}

checkGapsAnomalies(klasRain2016, minDay="2016-01-01", maxDay="2016-12-31", loc="Las Vegas, NV (2016)")
checkGapsAnomalies(kordRain2016, minDay="2016-01-01", maxDay="2016-12-31", loc="Chicago, IL (2016)")
checkGapsAnomalies(klnkRain2016, minDay="2016-01-01", maxDay="2016-12-31", loc="Lincoln, NE (2016)")

```
  
The Chicago, IL and Las Vegas, NV sensors self-identify as anomalous roughly an order of magnitude more often than the Lincoln, NE sensor.  There are no obvious spikes for Chicago, IL in Q1 2016, suggesting mismatched precipitation may be due to causes other than anomalous sensors and missing data.
  
#### _Example #35: Checking Other Time Periods and Locales_  
The precipitation discrepancies in the Lincoln. NE data appear to be one-off while there are no material preipitation discrepancies in the Las Vegas, NV data.  However, Q1 2016 is filled with precipitation discrepancies in the Chicago, IL data.  Is this just a Q1 2016 problem, or something related to large midwestern cities more generally, or a specific recurring Q1 issue in Chicago?

Pulling data for Minneapolis, MN and Detroit, MI for 2016 can help address whether there is a general Q1 2016 issue in large midwestern cities.  Pulling data for Chicago, IL in 2015 and 2017 can help address whether there is something specific and recurring to Chicago in Q1.

Example code includes:  
```{r cache=TRUE}

# Data pulls are cached to avoid over-using the Iowa State servers

# Get data for ORD for 2015
getASOSStationTime(stationID="ORD", analysisYears=2015, ovrWrite=TRUE)

# Get data for ORD for 2017
getASOSStationTime(stationID="ORD", analysisYears=2017, ovrWrite=TRUE)

# Get data for MSP for 2016
getASOSStationTime(stationID="MSP", analysisYears=2016, ovrWrite=TRUE)

# Get data for DTW for 2016
getASOSStationTime(stationID="DTW", analysisYears=2016, ovrWrite=TRUE)

```

Create the base METAR file for Chicago, IL 2015 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kord_2015.txt"  # file name for raw METAR data
timeZ <- "51Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2014-12-31 00:51:00", tz="UTC")  # Expected first time read
expDays <- 367  # Expected total days read
locMET <- "Chicago, IL"  # Description of city or location
shortMET <- "KORD METAR (2015)"  # Station code and timing
longMET <- "Chicago, IL O'Hare Hourly METAR (2015)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Chicago, IL (2015)
kord2015METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kord2015METAR)

```

Create the base METAR file for Chicago, IL 2017 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kord_2017.txt"  # file name for raw METAR data
timeZ <- "51Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2016-12-31 00:51:00", tz="UTC")  # Expected first time read
expDays <- 367  # Expected total days read
locMET <- "Chicago, IL"  # Description of city or location
shortMET <- "KORD METAR (2017)"  # Station code and timing
longMET <- "Chicago, IL O'Hare Hourly METAR (2017)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Chicago, IL (2017)
kord2017METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kord2017METAR)

```

Create the base METAR file for Minneapolis, MN 2016 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kmsp_2016.txt"  # file name for raw METAR data
timeZ <- "53Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:53:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Minneapolis, MN"  # Description of city or location
shortMET <- "KMSP METAR (2016)"  # Station code and timing
longMET <- "Minneapolis, MN Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Minneapolis, MN (2016)
kmsp2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kmsp2016METAR)

```

Create the base METAR file for Detroit, MI 2016 data:  
```{r cache=TRUE}

# Set key parameters for reading and interpreting METAR
fname <- "./RInputFiles/metar_kdtw_2016.txt"  # file name for raw METAR data
timeZ <- "53Z"  # Zulu time that METAR is recorded at this station
expMin <- as.POSIXct("2015-12-31 00:53:00", tz="UTC")  # Expected first time read
expDays <- 368  # Expected total days read
locMET <- "Detroit, MI"  # Description of city or location
shortMET <- "KDTW METAR (2016)"  # Station code and timing
longMET <- "Detroit, MI Hourly METAR (2016)"  # Description of city or location and timing

# Extraction format for METAR - paste the expected Zulu time at the front
valMet <- paste0(timeZ, ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})")

# Run the process for Detroit, MI (2016)
kdtw2016METAR <- runAllMETAR(fname=fname, timeZ=timeZ, expMin=expMin, expDays=expDays, 
                             locMET=locMET, shortMET=shortMET, longMET=longMET, valMet=valMet
                             )
str(kdtw2016METAR)

```

Run a wind comparison for these new observations:  
```{r}

# Run for newly downloaded and processed data files
cpWind <- consolidatePlotWind(files=c("kord2015METAR", "kord2017METAR", 
                                      "kmsp2016METAR", "kdtw2016METAR"
                                      ), 
                              names=c("Chicago, IL (2015)", "Chicago, IL (2017)", 
                                      "Minneapolis, MN (2016)", "Detroit, MI (2016)"
                                      )
                              )
cpWind

```

The data can be examined for Chicago, IL 2015:  
```{r}

# Run for Chicago, IL 2015 rainfall
kordRain2015 <- runFullPrecipExtraction(kord2015METAR, 
                                        pType="RA", 
                                        titleText="Chicago, IL Rainfall (hours) in 2015", 
                                        yAxisText="Hours of Rain", 
                                        endExclude=c("2015-02-08 2308", "2015-05-17 1336", 
                                                     "2015-05-11 2020", "2015-06-29 0215",
                                                     "2015-12-28 1700"
                                                     ),
                                        beginExclude=c("2015-01-03 1022", "2015-01-03 1249", 
                                                       "2015-04-02 1125", "2015-06-21 0156",
                                                       "2015-08-15 0209"
                                                       ), 
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Run for Chicago, IL 2015 snowfall
kordSnow2015 <- runFullPrecipExtraction(kord2015METAR, 
                                        pType="SN", 
                                        titleText="Chicago, IL Snowfall (hours) in 2015", 
                                        yAxisText="Hours of Snow", 
                                        endExclude=c("2015-01-08 1829", "2015-01-08 1845"),
                                        beginExclude=c("2015-01-09 0630"), 
                                        beginAdd=c("2015-02-25 2251", "2015-01-03 1151"),
                                        maxProb=1440, 
                                        sState=FALSE, 
                                        makePlots=TRUE
                                        )

# Check for interval consistency in the Chicago, IL 2015 rainfall data
# FZRA on 2015-01-03 (4) and 2015-02-08 (1)
# 2015-05-11 1951 -RA without RAB (1)
# 2015-06-21 0251 -TSRA and RAB0156 but no following RAE (1)
# 2015-06-29 0151 -RA without RAB (1)
# 2015-08-15 best solution to miscoded RAEO6 rather than RAE06 (2)
# FZRA on 2015-12-28 (4)
tmp <- intervalConsistency(kordRain2015, pType="RA")

# Check for interval consistency in the Chicago, IL 2015 snowfall data
# 2015-01-09 0651 BLSN but thinks it is SN (interval OK)
tmp <- intervalConsistency(kordSnow2015, pType="SN")

# Original pass for Chicago, IL
x7 <- findPrecipTypes(kordRain2015)

# Exclude CLR as it is a cloud type, not precipitation
x8 <- findPrecipTypes(kordRain2015, exclTypes=c("CLR"))

# Get the Chicago, IL 2015 liquid precipitation
kord2015Precip <- extractLiquidPrecipAmounts(kordRain2015)

# Check for consistency in the Chicago, IL 2015 precipitation data
checkPrecipConsistency(kord2015Precip, title="Chicago, IL 2015 Precipitation by Month")

# Check for missing data and sensor anomalies
checkGapsAnomalies(kordRain2015, minDay="2015-01-01", maxDay="2015-12-31", loc="Chicago, IL (2015)")

```
  
There appears to be a large issue on February 1-2, 2015 where precipitation data are off by several inches.  In addition to a few small one-off, there is also an extended issue from October 20-31, 2015 as well as a missing 24-hour precipitation value (1.25 inches) from December 28, 2015.
  
#### _Example #36: Automating Suggestiong for Precipitation Begin and End_  
At each time period in a METAR, there is a precipitation state that is either "on" or "off".  The state can be checked for consistency against the begin and end data as follows.  The general assumption is that recording will be more accuract for "is there precipitation now" then for all of the possible begins and ends for a specified precipitation type:
  
* If the previous state is "on", there should not be a "begin" prior to the next "end" - flag these as potential beginExclude items  
* If the previous state is "on" and the current state is "on", then there should not be any "end" items without an associated "begin" item following - flag these as potential endExclude items  
* If the previous state is "on" and the current state is "off", then there should be an "end" event in the remarks data - flag these as potential endAdd items  
* If the previous state is "off", there should not be an "end" event prior to the next "begin" - flag these as potential endExclude items  
* If the previous state is "off" and the current state is "off", then there should not be any "begin" items without an associated "end" item following - flag these as potential beginExclude items  
* If the previous state is "off" and the current state is "on", then there should be a "begin" event in the remarks data - flag these as potential beginAdd items  
  
Example code includes:  
```{r}

# Look for RA that is not preceded by FZ in the 2016 Chicago, IL data
regMatch <- "(?<!FZ)RA"

# Pull the Chicago, IL 2016 data and check for the specified precipitation pattern and lags
kordStates <- kordRain2016$testFileProc %>%
    select(dtime, origMETAR) %>%
    mutate(curPrecip=str_detect(origMETAR, paste0(".*", regMatch, ".*RMK")), 
           lagPrecip=lag(curPrecip, 1)
           )

# Use the analysis data to look for begins and ends flagged in the remarks
kordBE <- extractPrecipData(list(fullMETAR=kordStates), pType=regMatch)

# Inner join the data by dtime
kordStates <- kordStates %>%
    inner_join(kordBE %>% select(dtime, precipData, chgPrecip=isPrecip, dateUTC, hourUTC), by="dtime")
kordStates

# Get the beginning and end times data for the desired precipitation type
kordBegin <- getBeginEndTimeMatrix(kordStates, pState="B")
kordEnd <- getBeginEndTimeMatrix(kordStates, pState="E")

# Hard-code for 2-column files (relax later)
# Extract the begin and end times
if (ncol(kordBegin) != 2 | ncol(kordEnd) != 2) { stop("Hard-coded for 2 columns, fix") }
testBT1 <- getBeginEndTimeVector(kordBegin, kordStates, extractVar="V1", extractSym="B")
testET1 <- getBeginEndTimeVector(kordEnd, kordStates, extractVar="V1", extractSym="E")
testBT2 <- getBeginEndTimeVector(kordBegin, kordStates, extractVar="V2", extractSym="B")
testET2 <- getBeginEndTimeVector(kordEnd, kordStates, extractVar="V2", extractSym="E")

# Integrate to a single file
kordExceptions <- kordStates %>%
    mutate(b1=testBT1, e1=testET1, b2=testBT2, e2=testET2, 
           begins=ifelse(is.na(b1), 0, 1) + ifelse(is.na(b2), 0, 1), 
           ends=ifelse(is.na(e1), 0, 1) + ifelse(is.na(e2), 0, 1),
           etob=begins > ends, 
           btoe=ends > begins, 
           needBegin=curPrecip & !lagPrecip & !etob, 
           needEnd=!curPrecip & lagPrecip & !btoe, 
           overBegin=etob & (lagPrecip | !curPrecip), 
           overEnd=btoe & (curPrecip | !lagPrecip)
           )

# Flag potential issues
cat("\nNeed Begin time\n")
kordExceptions %>%
    filter(needBegin) %>%
    select(dtime, origMETAR)

cat("\nNeed End time\n")
kordExceptions %>%
    filter(needEnd) %>%
    select(dtime, origMETAR)

cat("\nExtraneous Begin time\n")
kordExceptions %>%
    filter(overBegin) %>%
    select(dtime, origMETAR)

cat("\nExtraneous End time\n")
kordExceptions %>%
    filter(overEnd) %>%
    select(dtime, origMETAR)

# Attempt to use on the kord2016 data - initial run
kordRain2016Test <- runFullPrecipExtraction(kord2016METAR, 
                                            pType=regMatch, 
                                            titleText="Chicago, IL Rainfall (hours) in 2016", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c(),
                                            beginExclude=c(),
                                            endAdd=c(), 
                                            beginAdd=c(),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=FALSE
                                            )

# Attempt to use on the kord2016 data - with adds and excludes
kordRain2016Test <- runFullPrecipExtraction(kord2016METAR, 
                                            pType=regMatch, 
                                            titleText="Chicago, IL Rainfall (hours) in 2016", 
                                            yAxisText="Hours of Rain", 
                                            endExclude=c(),
                                            beginExclude=c(),
                                            endAdd=c("2016-07-13 1851"), 
                                            beginAdd=c("2016-03-24 2151", "2016-08-29 1951"),
                                            maxProb=1440, 
                                            sState=FALSE, 
                                            makePlots=TRUE
                                            )

# Intervals match
tmp <- intervalConsistency(kordRain2016Test, pType=regMatch)

```

