---
title: "Data Camp Insights"
author: "davegoblue"
date: "September 24, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)

```

## Background and Overview  
This document is a complement to DataCamp_Insights_v003_c.Rmd to be merged and integrated later.
  

***
  
###_Survey and Measure Development in R_  
  
Chapter 1 - Preparing to Analyze Survey Data  
  
Surveys in Marketing Research:  
  
* Surveys can consist of items rated on the Likert scale (commonly, but not necessarily, 1-5 scales)  
	* Step 1 - item generation (expert review, SME, etc.)  
    * Step 2 - questionnaire administration  
    * Setp 3 - initial item reduction  
    * Step 4 - confirmatory factor analysis  
    * Step 5 - Convergent/Discriminant Validity  
    * Step 6 - Replication  
* Example for inter-rater reliability  
	* library(irr)  
    * agree(experts)  
    * cohen.kappa(experts)  # 0-0.4 is poor, 0.4-0.6 is mediocre, 0.6-0.8 is substantial, 0.8+ is very strong  
* Example for content validity ratios  
	* Lawshe's Content Validity Ratio (CVR): what percent of experts judge that item essential to what's being measured  
    * CVR = [ E - (N/2) ] / (N/2)  where N is the total number of experts and E is the number who rated the item as essential  
    * psychometric::CVratio(NTOTAL=, NESSENTIAL=)  # -1 is consensus againt, +1 is consensus in favor  
  
Measurement, Validity, and Reliability:  
  
* Measurement is the process of observing and recording events  
	* Requires a measurement device and a calibration standard  
* Reliability can be assessed on three prongs  
	* Equivalence (inter-rater)  
    * Internal consistency (coefficient alpha, split-half)  
    * Stability (test-retest)  
* Validity checks whether measurments are as-claimed  
	* Content  
    * Construct (convergent, discriminant)  
    * Criterion (concurrent, predictive)  
* Exploratory Data Analysis can be considered as step 2.5 - between questionnaire administration and initial item reduction  
	* c_sat_likert <- c_sat %>% mutate_if(is.integer, as.factor) %>% likert()  
    * plot(c_sat_likert)  
* Can use EDA to check for items that are reverse coded (where 5 would be bad and 1 would be good)  
	* car::recode(myVar, "1=5; 2=4; 3=3; 4=2; 5=1")  
  
Describing Survey Results:  
  
* Generally, if there are less than 5% missing values and with equal distribution, then just omit them  
	* Hmisc::naclus(bad_survey))  
* Item correlations can be valuable - high correlations with each other, but not outside their own group  
	* corr.test(myDF)  
    * corrplot(cor(myDF), method="circle")  
  
Example code includes:  
```{r}

library(lavaan)


file001 <- readr::read_csv("./RInputFiles/brandrep-cleansurvey-extraitem.csv")
file002 <- readr::read_csv("./RInputFiles/brandquall11-recodedbutextraitem.csv")
file003 <- readr::read_csv("./RInputFiles/customersatisfactionclean.xls")
file004 <- readr::read_csv("./RInputFiles/brandloyalty.xls")


sme <- data.frame(Rater_A=c(1, 2, 3, 2, 1, 1, 1, 2, 3, 3, 2, 1, 1), 
                  Rater_B=c(1, 2, 2, 3, 3, 1, 1, 1, 2, 3, 3, 3, 1)
                  )


# Print beginning of sme data frame
head(sme)

# Correlation matrix of expert ratings
cor(sme)

# Percentage agreement of experts
irr::agree(sme)


# Check inter-rater reliability
psych::cohen.kappa(sme)

# While our Cohen's kappa and Pearson correlation happen to be similar in value, these are not measuring the same thing
# We are interested in agreement between the pairs of expert ratings on each item rather than a linear relationship between the item ratings in total
# In the next exercise, we'll look at content validity
# This is a measure of the assessment by a panel of experts (not just two, as in in Cohen's kappa) about the strength of an individual item


lawshe <- data.frame(item=rep(1:5, each=3), 
                     expert=rep(LETTERS[1:3], times=5), 
                     rating=factor(c("Essential", "Useful", "Not necessary", "Useful", "Not necessary", 
                                     "Useful",  "Not necessary", "Not necessary", "Essential", "Essential", 
                                     "Useful", "Essential", "Essential", "Essential", "Essential"
                                     )
                                   )
                     )


# Calculate the CVR for each unique item in the data frame
lawshe %>% 
    group_by(item) %>% 
    summarize(CVR = psychometric::CVratio(NTOTAL = length(unique(expert)), 
                                          NESSENTIAL = sum(rating == 'Essential')
                                          )
              )


brand_rep <- file001
glimpse(brand_rep)


# Convert items to factor
b_rep_likert <- brand_rep %>% 
    mutate(poor_workman_r=6-poor_workman_r) %>%
    mutate_if(is.double, as.factor) %>%
    as.data.frame()

# Response frequencies - base R
summary(b_rep_likert)

# Plot response frequencies
result <- likert::likert(b_rep_likert)
plot(result)


brand_qual <- file002 %>%
    mutate(tired=6-tired_r) %>%
    select(-innovator) %>%
    as.data.frame()
glimpse(brand_qual)

brand_qual_items <- c('trendy = This brand is trendy.', 'latest = This brand offers the latest products.', 'tired = This is a tired brand.', "happy_pay = I am happy paying what I do for this brand's products.", "reason_price = This brand's products are reasonably priced.", "good_deal = This brand's products are a good deal.", "strong_perform = This brand's products are strong performers.", 'leader = This brand is a leader in its field.', 'serious = This brand takes its product quality seriously.')
brand_qual_items


# Get response frequencies from psych
psych::response.frequencies(brand_qual)

# Print item descriptions
brand_qual_items

# Reverse code the "opposite" item
brand_qual$tired_r <- car::recode(brand_qual$tired, "1 = 5; 2 = 4; 4 = 2; 5 = 1")

# Check recoding frequencies
brand_qual %>% 
    select(tired, tired_r) %>%
    psych::response.frequencies() %>%
    round(2)


missing_lots <- file002 %>%
    mutate(tired=6-tired_r) %>%
    select(-tired_r)
set.seed(1908181013)
naRow <- sample(1:nrow(missing_lots), round(0.2*nrow(missing_lots)*ncol(missing_lots)), replace=TRUE)
naCol <- sample(1:ncol(missing_lots), round(0.2*nrow(missing_lots)*ncol(missing_lots)), replace=TRUE)
for (j in seq_along(naRow)) { missing_lots[naRow[j], naCol[j]] <- NA }
glimpse(missing_lots)
    

# Total number of rows
nrow(missing_lots)

# Total number of incomplete cases
sum(!complete.cases(missing_lots))

# Number of incomplete cases by variable
colSums(is.na(missing_lots))

# Hierarchical plot -- what values are missing together?
plot(Hmisc::naclus(missing_lots))


brand_qual_9 <- file002
glimpse(brand_qual_9)


# View significance of item correlations
psych::corr.test(brand_qual_9)

# Visualize item correlations -- corrplot
corrplot::corrplot(cor(brand_qual_9), method = "circle")


b_rep_items <- c("well_made: Crunchola's products are well-made.", 'consistent: Crunchola offers consistently high-quality products.', 'poor_workman: Crunchola suffers from poor workmanship in its products.', 'higher_price: I am willing to pay a higher price for Crunchola products.', 'lot_more: I am willing to pay a lot more for Crunchola products.', 'go_up: The price of Crunchola products would have to go up quite a bit before I would switch to another brand.', "stands_out: Crunchola's brand really stands out from its competitors.", "unique: Crunchola's brand is unique from other brands.", "one_of_a_kind: Crunchola's brand is truly one of a kind.")
b_rep_items


brand_rep_9 <- file001 %>%
    mutate(poor_workman = 6-poor_workman_r) %>%
    select(-poor_workman_r) %>%
    as.data.frame()
glimpse(brand_rep_9)


# Get response frequencies
psych::response.frequencies(brand_rep_9)

# Recode the appropriate item 
brand_rep_9$poor_workman_r <- car::recode(brand_rep_9$poor_workman, "1 = 5; 2 = 4; 4 = 2; 5 = 1")

# Adjust brandrep 9 dataset
brand_rep_9_new <- select(brand_rep_9, -poor_workman)

# Visualize item correlation
corrplot::corrplot(cor(brand_rep_9_new), method = "circle")

```
  
  
  
***
  
Chapter 2 - Exploratory Factor Analysis and Survey Development  
  
Latent Variables:  
  
* Latent variables are inferred from manifest variables - for example, brand loyalty  
* Parsimony is a general goal of survey development - how do the manifest variables help identify the latent variables  
	* psych::fa.parallel(myDF)  # scree of dataset against random dataset  
    * myEFA <- psych::fa(myDF, nfactors=3)  
    * myEFA$loadings  
    * psych::scree(myDF)  
  
EFA and Item Refinement:  
  
* Factor loadings are a valuable statistic - relationship between manifest variables and latent variables  
	* Ideally, only one latent variable per manifest variables  
    * 0 - 0.4 are poor  
    * 0.7+ are strong  
    * 0.4 - 0.7 are equivocal  
* What makes for a strong EFA?  
	* c_sat_11_EFA_3$e.values  # check that the number of eigenvalues >1 is the same as the number of factors (rule of thumb)  
    * Factor score correlations of 0.6 and under are "not too similar"  
    * myEFA$score.cor  
* If results are not favorable can either 1) drop poorly performing items, or 2) revist the number of factors  
  
Assessing Internal Reliability:  
  
* Internal consistency is a measure of survey reliability - consistency within itself  
* Split-half reliability checks whether all parts of the survey contribute equally  
	* psych::splitHalf(mySurvey)  # generally, 0.8+ indicates internal reliability  
* Coefficient (Cornbach) alpha measures the consistency of measures of the construct  
	* psych::alpha(mySurvey)  # std.alpha is considered a more reliable metric due to standardization  
    * Target is 0.8 - 0.9 with 0.7 - 0.8 also respectable and 0.65 - 0.7 minimally acceptable  
    * Values > 0.9 may suggest collinearity problems; drop items  
    * Values < 0.65 are undesirable and/or unacceptable; drop items as they may not be measuring the same construct  
  
Example code includes:  
```{r}

b_loyal_10 <- file004
glimpse(b_loyal_10)


# Print correlation matrix
psych::corr.test(b_loyal_10)

# Visualize b_loyal_10 correlation matrix
corrplot::corrplot(cor(b_loyal_10))

# Parallel analysis
psych::fa.parallel(b_loyal_10)


brand_rep_9 <- file001 %>%
    mutate(poor_workman = 6-poor_workman_r) %>%
    select(-poor_workman) %>%
    as.data.frame()
glimpse(brand_rep_9)


# Scree plot
psych::scree(brand_rep_9)

# Conduct three-factor EFA
brand_rep_9_EFA <- psych::fa(brand_rep_9, nfactors = 3)

# Print output of EFA
names(brand_rep_9_EFA)


# Summarize results of three-factor EFA
summary(brand_rep_9_EFA)

# Build and print loadings for a two-factor EFA
brand_rep_9_EFA_2 <- psych::fa(brand_rep_9, nfactors = 2)
brand_rep_9_EFA_2$loadings

# Build and print loadings for a four-factor EFA
brand_rep_9_EFA_4 <- psych::fa(brand_rep_9, nfactors = 4)
brand_rep_9_EFA_4$loadings


# (Factor loadings greater than 1, while rare, are not necessarily an issue.)

# Three factor EFA - brand_rep_9
brand_rep_9_EFA_3 <- psych::fa(brand_rep_9, nfactors = 3)

# Eigenvalues
brand_rep_9_EFA_3$e.values

# Factor score correlations
brand_rep_9_EFA_3$score.cor

# Factor loadings
brand_rep_9_EFA_3$loadings


# Create brand_rep_8 data frame
brand_rep_8 <- brand_rep_9 %>% select(-one_of_a_kind)

# Create three-factor EFA
brand_rep_8_EFA_3 <- psych::fa(brand_rep_8, nfactors=3)

# Factor loadings
brand_rep_8_EFA_3$loadings

# Factor correlations -- 9 versus 8 item model
brand_rep_8_EFA_3$score.cor
brand_rep_9_EFA_3$score.cor


# Three factor EFA loadings
brand_rep_8_EFA_3$loadings

# Two factor EFA & loadings
brand_rep_8_EFA_2 <- psych::fa(brand_rep_8, nfactors = 2)
brand_rep_8_EFA_2$loadings

# Four factor EFA & loadings
brand_rep_8_EFA_4 <- psych::fa(brand_rep_8, nfactors = 4)
brand_rep_8_EFA_4$loadings

# Scree plot of brand_rep_8
psych::scree(brand_rep_8)


# Standardized coefficient alpha
psych::alpha(brand_rep_9)$total$std.alpha

# 3-factor EFA
brand_rep_9_EFA_3 <- psych::fa(brand_rep_9, nfactors = 3)
brand_rep_9_EFA_3$loadings

# Standardized coefficient alpha - refined scale
psych::alpha(brand_rep_8)$total$std.alpha

# A survey with poorly-loading items can still be reliable – that's why we do EFA first
# Remember that a reliable survey in itself is not the goal of measurement – it is necessary but not sufficient


# Get names of survey items
names(brand_rep_8)

# Create new data frames for each of three dimensions
p_quality <- brand_rep_8 %>% select(1:3)
p_willingness <- brand_rep_8 %>% select(4:6)
# p_difference <- brand_rep_8 %>% select(7:8)

# Check the standardized alpha for each dimension
psych::alpha(p_quality)$total$std.alpha
psych::alpha(p_willingness)$total$std.alpha
# psych::alpha(p_difference)$total$std.alpha
psych::alpha(brand_rep_8)$total$std.alpha


# Get split-half reliability 
psych::splitHalf(brand_rep_8)

# Get averages of even and odd row scores
odd_scores <- rowMeans(brand_rep_8[c(TRUE, FALSE), ])
even_scores <- rowMeans(brand_rep_8[c(FALSE,TRUE), ])

# Correlate scores from even and odd items
cor(odd_scores[1:length(even_scores)], even_scores)


# 3 factor EFA
b_loyal_10_EFA_3 <- psych::fa(b_loyal_10, nfactors = 3)

# Factor loadings, eigenvalues and factor score correlations
b_loyal_10_EFA_3$loadings
b_loyal_10_EFA_3$e.values
b_loyal_10_EFA_3$score.cor

# 2 factor EFA
b_loyal_10_EFA_2 <- psych::fa(b_loyal_10, nfactors = 2)

# Factor loadings, eigenvalues and factor score correlations
b_loyal_10_EFA_2$loadings
b_loyal_10_EFA_2$e.values
b_loyal_10_EFA_2$score.cor

```
  
  
  
***
  
Chapter 3 - Confirmatory Factor Analysis and Construct Validation  
  
CFA and EFA:  
  
* Confirmatory Factor Analysis (CFA) is a means of construct validation  
	* Do the number of factors reflected in the data match theory and hypotheses  
* Can use the lavaan package for latent variable analysis  
	* la(tent) va(riable) an(alaysis)  
    * Use =~ to assign items to factors  
* Example for using lavaan on the 9-item survey  
	* bq_9_CFAModel <- "VAL =~ reason_price + happy_pay + good deal PERF =~ serious + leader +  strong_perform FUN =~ trendy + latest + tired_r"  
    * bq_9_CFA <- cfa(model = bq_9_CFAModel, data=bq_9)  
    * summary(bq_9_CFA, fit.measures=TRUE, standardized=TRUE)  
    * inspect(bq_9_CFA, "std")$lambda  
    * semPlot::semPaths(bq_9_CFA)  
  
CFA Assumptions and Interpretation:  
  
* Can test for multivariate normality - p-values for skewness and kurtosis  
	* psych::mardia(myData)  
* The default for lavaan is maximum likelihood, which assumes normality  
	* Can instead use MLR to mitigate non-normality  
    * bq_cfa <- cfa(model=myModel, data=myData, estimator="MLR")  
* Can look at fit measures and assess model performance  
	* CFI (comparative fit index) - should be 0.9+  
    * TLI (Tucker Lewis Index) - should be 0.9+  
    * Chi-squared - should be < 0.05 (though often will be for large sample sizes, even with a bad model)  
    * RMSEA - ideally less than 0.05  
* Can use the fit measures function to get 42 fit measures  
	* fitMeasures(myModel)  
    * fitMeasures(myModel, fit.measures=c("cfi", "tli"))  
* Can inspect estimates using standardizedSolution()  
	* standardizedSolution(myModel)  
  
Construct Validity:  
  
* Construct validity is the extent to which the actual measurements and the claims of what is being measured are congruent  
	* Validity is like being centered on a bullseye  
    * Reliability is based on being tightly clustered around the mean  
* If two dimensions are measuring the same things, then they should be combined in the interests of parsimony  
	* semTools::reliability(myModel)  
* Discriminant validity means that items should be distinct, but not unrelated  
	* avevar should be 0.5+  
    * CR (omega) should be 0.7+  
    * alpha (Cronbach) should be 0.7+  
  
Example code includes:  
```{r}

brand_rep_EFA <- brand_rep_8_EFA_3
brand_rep_8_model <- 'F1 =~ well_made + consistent + poor_workman_r
F2 =~ higher_price + lot_more + go_up
F3 =~ stands_out + unique'
brand_rep_CFA <- lavaan::cfa(model=brand_rep_8_model, data=brand_rep_8)


# Factor loadings -- EFA
brand_rep_EFA$loadings

# Factor loadings -- CFA
lavaan::inspect(brand_rep_CFA, what = "std")$lambda

# Plot diagram -- EFA
psych::fa.diagram(brand_rep_EFA)

# Plot diagram -- CFA
semPlot::semPaths(brand_rep_CFA)


# Rename items based on proposed dimensions
colnames(b_loyal_10) <- c("ID1", "ID2", "ID3", "PV1", "PV2", "PV3", "BT1", "BT2", "BT3", "BT4")

# Define the model
b_loyal_cfa_model <- 'ID =~ ID1 + ID2 + ID3
                    PV =~ PV1 + PV2 + PV3
                    BT =~ BT1 + BT2 + BT3 + BT4'
                        
# Fit the model to the data
b_loyal_cfa <- lavaan::cfa(model=b_loyal_cfa_model, data=b_loyal_10)

# Check the summary statistics -- include fit measures and standardized estimates
summary(b_loyal_cfa, fit.measures=TRUE, standardized=TRUE)


# Two dimensions: odd- versus even-numbered items
bad_model <- 'ODD =~ CS1 + CS3 + CS5 + CS7 + CS9
              EVEN =~ CS2 + CS4 + CS6 + CS8 + CS10'
                
# Fit the model to the data
c_sat_bad_CFA <- cfa(model=bad_model, data=file003)

# Summary measures
summary(c_sat_bad_CFA, fit.measures=TRUE, standardized=TRUE)


c_sat_model <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10'
c_sat_50 <- file003[1:50, ]


# Mardia's test for multivarite normality
psych::mardia(c_sat_50)

# Fit model to the data using robust standard errors
c_sat_cfa_mlr <- cfa(model=c_sat_model, data=c_sat_50, estimator="MLR")

# Summary including standardized estimates and fit measures
summary(c_sat_cfa_mlr, fit.measures=TRUE, standardized=TRUE)


c_sat_model_a <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10'
c_sat_model_b <- 'F1 =~ CS1 + CS3 + CS5 + CS7 + CS9
F2 =~ CS2 + CS4 + CS6 + CS8 + CS10'


# Fit the models to the data
c_sat_cfa_a <- cfa(model = c_sat_model_a, data = file003)
c_sat_cfa_b <- cfa(model = c_sat_model_b, data = file003)

# Print the model definitions
cat(c_sat_model_a)
cat(c_sat_model_b)

# Calculate the desired model fit statistics
fitMeasures(c_sat_cfa_a, fit.measures=c("cfi", "tli"))
fitMeasures(c_sat_cfa_b, fit.measures=c("cfi", "tli"))


c_sat <- file003
names(c_sat) <- c("CSU1", "CSU2", "CSU3", "CSU4", "EU1", "EU2", "EU3", "PS1", "PS2", "PS3")


# Add EU1 to the CSU factor
c_sat_model_a <- 'CSU =~ CSU1 + CSU2 + CSU3 + CSU4
                EU =~ EU1 + EU2 + EU3
                PS =~ PS1 + PS2 + PS3'

# View current c_sat model
cat(c_sat_model_a)

# Add EU1 to the CSU factor
c_sat_model_b <- 'CSU =~ CSU1 + CSU2 + CSU3 + CSU4 + EU1
                EU =~ EU1 + EU2 + EU3
                PS =~ PS1 + PS2 + PS3'

# Fit Models A and B to the data
c_sat_cfa_a <- cfa(model = c_sat_model_a, data = c_sat)
c_sat_cfa_b <- cfa(model = c_sat_model_b, data = c_sat)

# Compare the nested models
anova(c_sat_cfa_a, c_sat_cfa_b)


# Fit the model to the data 
# c_sat_cfa <- cfa(model = c_sat_model, data = c_sat_group, group = "COUNTRY")

# Summarize results -- include fit measures and standardized estimates
# summary(c_sat_cfa, fit.measures=TRUE, standardized=TRUE)

# Get average estimate for both groups
# standardized_solution <- standardizedSolution(c_sat_cfa)
# standardized_solution %>%
#   filter(op == "=~") %>%
#   group_by(group) %>% 
#   summarize(mean(est.std))


c_sat_cfa_model_3 <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10'
c_sat_cfa_model_2 <- 'F1 =~ CS1 + CS2 + CS3 + CS4 + CS5 + CS6 + CS7
F2 =~ CS8 + CS9 + CS10'

# Fit three-factor CFA
c_sat_cfa_3 <- cfa(model = c_sat_cfa_model_3, data = file003)

# Inspect key fit measures - three-factor CFA
fitMeasures(c_sat_cfa_3, fit.measures = c("cfi","tli","rmsea"))

# Fit two-factor CFA
c_sat_cfa_2 <- cfa(model = c_sat_cfa_model_2, data = file003)

# Inspect key fit measures - two-factor CFA
fitMeasures(c_sat_cfa_2, fit.measures = c("cfi","tli","rmsea"))

# Compare measures of construct validity for three- versus two-factor models
semTools::reliability(c_sat_cfa_3)
semTools::reliability(c_sat_cfa_2)


brand_rep_CFA_model <- 'F1 =~ well_made + consistent + poor_workman_r
F2 =~ higher_price + lot_more + go_up
F3 =~ stands_out + unique'
brand_rep_CFA <- lavaan::cfa(model=brand_rep_8_model, data=brand_rep_8)


# Print CFA model
cat(brand_rep_CFA_model)

# semTools reliability measures
semTools::reliability(brand_rep_CFA)

# psych standardized coefficient alpha measure
psych::alpha(brand_rep_9)$total$std.alpha


# Store F1 estimates as object loadings
loadings <- standardizedSolution(c_sat_cfa_3) %>%
    filter(op == "=~", lhs == "F1") %>% 
    select(est.std)

# Composite reliability
re <- 1 - loadings ^ 2
result <- sum(loadings) ^ 2 / ((sum(loadings)^ 2)  + sum(re))
result

# Average variance extracted
l2 <- loadings ^ 2
avg_var <- sum(l2) / nrow(loadings)
avg_var

# Compare versus semTools
semTools::reliability(c_sat_cfa_3)


# Print brand_rep_factors
# brand_rep_factors

# Build model for lavaan
brand_rep_8_cfa_model <- "QUAL =~ consistent + well_made + poor_workman_r
PRICE =~ go_up + lot_more + higher_price
UNIQUE =~ stands_out + unique"

# Summarize results with fit measures and standardized estimates
# summary(brand_rep_8_CFA, standardized = TRUE, fit.measures = TRUE)

# Construct validity
# semTools::reliability(brand_rep_8_CFA)

```
  
  
  
***
  
Chapter 4 - Criterion Validity and Replication  
  
Concurrent Validity and Model Diagrams:  
  
* Criterion validity is a measure of relationship between the construct and external variable of interest  
* Variables are not always on the 1-5 scale, and differences in units can negatively impact model validity  
	* describe(myData)  # check means and standard deviations (ideally, everything is N(0, 1)  
* Can latentize a variable by adding it with =~  
	* myModel <- '… age_fact =~ age'  # latentizes age to age_fact  
* Can correlate manifest and latent variable with ~~  
	* myModel <- '… age_fact =~ age\n age_fact ~~ F1 + F2 + F3'  # latentizes age to age_fact and gets latent statistics (F1, F2, F3 already defined in the model)  
    * mySEM <- sem(myModel, data=myData, estimator="MLR")  
    * summary(mySEM, fit.measures=TRUE, standardized=TRUE)  
* Diagrams are sometimes called "spaghetti and meatballs", representing that they can be busy diagrams  
  
Predictive Validity and Factor Scores:  
  
* Predictive validity assesses the degree to which models predict future outcomes  
* Linear regression can be used for this task  
	* Begin by binding and scaling all of the relevant variables  
* Can run regression in lavaan using ~  
	* c_sat_model = "… spend ~ F1 + F2 + F3"  # assumes F1, F2, F3 each defined as usual using =~  
    * semPaths(c_sat_sem, rotation=2)  
    * standardizedSolution(c_sat_sem) %>% filter(op == "~") %>% mutate_if(is.numeric, round, digits=3)  
    * inspect(c_sat_rem, "r2")  # pull the R-squared  
* Factors scores are numerical scores reflecting relative standings on the latent factor  
	* csat_cfa <- cfa(model = csat_model, data = c_sat)  
    * csat_scores <- as.data.frame(predict(csat_cfa))  
    * describe(csat_scores)  
    * multi.hist(csat_scores)  
  
Repeated Measures, Replication, and Factor Scores:  
  
* Stability is a third form of reliability measurement  
	* Does an instrument get similar responses if measuring the same population near the same time?  
    * "Test-retest reliability"  
    * survey_test_retest <- testRetest(t1 = survey_t_1, t2 = survey_t_2, id = "id")  
    * Generally, scores of 0.9+ are very good and scores of 0.7- are unreliable  
* Replication is a different step that can be taken in the event that it is not possible to get people to retake the survey  
	* Split the data by rows - odd vs. even  
  
Wrap Up:  
  
* Six step process for building and testing models  
  
Example code includes:  
```{r}

spendData <- c(94.5, 715, 145.5, 772.5, 133.5, 350, 75.5, 304.5, 117, 81, 234.5, 102, 152.5, 295, 145, 222, 121.5, 142, 82.5, 144, 130, 141, 545, 142.5, 175, 154, 130, 148.5, 255, 139.5, 420, 373.5, 197.5, 487.5, 337.5, 133.5, 114, 84, 255.5, 129, 114, 275, 297, 84, 87, 109.5, 123, 405, 123, 158, 145, 139.5, 112.5, 458, 138, 91.5, 190, 257.5, 155, 259, 120, 84, 84, 755, 84, 412, 270, 134, 285, 227.5, 133.5, 123, 127.5, 825, 418, 103.5, 144, 124, 120, 445.5, 150.5, 75, 129, 312, 330, 182.5, 282, 91.5, 218, 245, 157.5, 118.5, 148.5, 505, 87, 182, 111, 294, 110, 325.5, 115.5, 312, 120, 510, 91.5, 139.5, 85.5, 189, 152, 141, 138, 387, 114, 84, 213, 120, 115.5, 231, 78, 85.5, 354, 142.5, 128, 212, 547.5, 145, 103.5, 294, 354, 182.5, 185, 212, 97.5, 103.5, 235, 395, 105.5, 283.5, 155, 91.5, 94.5, 297.5, 283.5, 125, 159, 139.5, 95, 198, 104, 138, 155, 200, 97.5, 224, 588, 108, 100.5, 183, 350, 153, 150, 155, 91.5, 138, 117, 135.5, 138, 202, 257, 103.5, 114, 282, 112, 198, 159, 420, 315, 402, 507, 259.5, 81, 127.5, 144, 225, 141, 84, 150, 150.5, 455, 75, 294, 102, 199.5, 385, 155, 144, 135, 142.5, 172, 390, 94.5, 153, 472.5, 105, 123, 188, 325, 504, 99, 111, 151.5, 78, 545, 170, 123, 93, 381.5, 735, 100.5, 97.5, 155, 252.5, 192, 132, 252.5, 121.5, 90, 257, 151.5, 94.5, 153.5, 311.5, 79.5, 284, 151.5, 95, 78, 480, 102, 215, 115.5, 330, 592.5, 79.5, 355.5, 195, 105.5, 142.5, 154, 155, 312, 321, 75.5, 185, 324, 155, 530, 127.5, 148.5, 152, 111, 157.5, 151.5, 772.5, 123, 115.5, 145.5, 84, 515, 82.5, 108, 130.5, 138, 279, 151.5, 207, 150, 109.5, 150, 153.5, 152.5, 150.5, 88.5, 185, 115.5, 123, 150, 114, 321, 144, 142.5, 152, 82.5, 187.5, 97.5, 145, 257, 435, 250, 310.5, 78, 105.5, 102, 138, 303, 285, 155, 124.5, 240, 204, 118.5, 241.5, 147, 118.5, 105, 591.5, 180, 93, 252, 103.5, 287, 575, 75, 238, 189, 204, 210, 153, 145.5, 117, 559, 153.5, 79.5, 222.5, 145.5, 88.5, 159, 155, 255, 127.5, 300, 154, 213, 135, 84, 151.5, 127.5, 99, 200, 135, 522.5, 297, 152, 127.5, 203, 103.5, 178.5, 130.5, 255, 100.5, 213, 185, 228, 115.5, 109.5, 75.5, 273, 511, 414, 152, 217, 150, 102, 537.5, 282, 440, 288, 172.5, 112.5, 577.5, 140, 291.5, 152, 582.5, 210, 318.5, 185, 145.5, 148.5, 324, 145, 105.5, 132, 85.5, 135, 152, 135, 324, 200, 155, 247.5, 197.5, 95, 304.5, 215, 577.5, 111, 495, 141, 139.5, 112.5, 110, 135.5, 97.5, 157.5, 243, 159, 155, 185, 155, 114, 395, 130.5, 238, 345.5, 597.5, 210, 220, 210, 222.5, 124.5, 158, 150, 490.5, 270, 88.5, 205, 135, 90, 152, 309, 153, 105, 111, 78, 123, 159, 95, 115, 435, 235, 292.5, 155, 304.5, 114, 104, 135, 397.5, 93, 257, 102, 204, 252, 152, 215, 108, 148.5, 79.5, 155, 114, 94.5, 118.5, 178.5, 111, 150.5, 195, 85.5, 84, 93, 575, 148.5, 757.5, 155, 87, 112.5, 88.5, 255, 358, 84, 405, 153, 127.5, 81, 135.5, 154.5, 247.5, 182, 79.5, 373.5, 95, 147, 145.5, 152.5, 294, 259.5, 354, 103.5, 187.5, 124.5, 218, 227.5, 481.5, 125, 123, 450, 129, 318, 170, 319.5, 91.5, 183, 154, 391.5, 458, 303, 114, 111, 112.5, 222, 742.5, 234, 120, 81, 312, 335, 135, 133.5, 118.5, 390, 518, 215, 373.5, 118.5, 195, 111, 205, 94.5, 123, 99, 75.5, 102, 244, 380, 357.5, 254, 227.5, 198, 192.5, 151.5)
brand_rep_spend <- tibble::tibble(spend=spendData)
brand_rep <- file001 %>%
    mutate(poorworkman = 6-poor_workman_r) %>%
    select(-poorworkman)
brand_rep
brand_rep_spend


# Check if brand_rep and brand_rep_spend have the same number of rows
same_rows <- nrow(brand_rep) == nrow(brand_rep_spend)
same_rows

# Append spend column to brand_rep
brand_rep <- cbind(brand_rep, brand_rep_spend)

# Scale the data
brand_rep_scaled <- scale(brand_rep)

# Get summary statistics of scaled dataframe
psych::describe(brand_rep)
psych::describe(brand_rep_scaled)


# Correlate F1, F2 and F3 to spend_f, the 'latentized' spend
brand_rep_model <- 'F1 =~ well_made + consistent + poor_workman_r
F2 =~ higher_price + lot_more + go_up
F3 =~ stands_out + unique
spend_f =~ spend
spend_f ~~ F1 + F2 + F3'

# Fit the model to the data -- sem()
brand_rep_cv <- lavaan::sem(data = brand_rep_scaled, model = brand_rep_model)

# Print the standardized covariances b/w spend_f and other factors
lavaan::standardizedSolution(brand_rep_cv) %>% 
    filter(rhs == "spend_f")

# Plot the model with standardized estimate labels
semPlot::semPaths(brand_rep_cv, whatLabels = "est.std", edge.label.cex = .8)


c_sat <- file003
c_sat_recommend <- tibble::tibble(Rec_1=c(4, 3, 3, 3, 4, 3, 2, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 4, 3, 2, 3, 3, 3, 4, 4, 3, 4, 4, 5, 3, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 2, 3, 3, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 1, 3, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 2, 4, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 4, 2, 4, 3, 4, 3, 3, 4, 3, 2, 3, 2, 3, 3, 4, 3, 4, 2, 3, 3, 2, 4, 3, 4, 2, 3, 4, 3, 3, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 4, 3, 2, 3, 3, 4, 2, 5, 3, 4, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 3, 3, 3, 4, 2, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 2, 4, 4, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 5, 4, 4, 3, 4, 2, 4, 4, 4, 4, 3, 4, 3, 3, 3, 2, 3, 3, 3, 4, 3, 3, 4, 5, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 1, 4, 3, 4, 3, 3, 3, 4, 3, 5, 4, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 2, 3, 4, 3, 4, 3, 4, 4, 4, 3, 3, 3, 5, 3, 3, 4, 3, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 4, 2, 4, 3, 2, 3, 3, 5, 4, 2, 5, 3, 5, 3, 2, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 5, 4, 4, 3, 3, 5, 3, 4, 3, 5, 4, 3, 2, 2))
c_sat
c_sat_recommend


# Bind & scale the variables
c_sat_rec_scale <- c_sat %>% 
    bind_cols(c_sat_recommend) %>% scale()

# Define the model - Rec_f covaries with F1, F2, F3
c_sat_rec_model <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10
Rec_f =~ Rec_1
Rec_f ~~ F1 + F2 + F3'

# Fit the model to the data 
c_sat_rec_sem <- lavaan::sem(model = c_sat_rec_model, data = c_sat_rec_scale)

# Look up standardized covariances
lavaan::standardizedSolution(c_sat_rec_sem) %>% 
    filter(rhs == "Rec_f")


# Define the model
b_q_model <- 'HIP =~ trendy + latest + tired_r
            VALUE =~ happy_pay + reason_price + good_deal
            PERFORM =~ strong_perform + leader + serious
            spend ~ HIP + VALUE + PERFORM'

# Fit the model to the data
# b_q_pv <- lavaan::sem(data = b_q_scale, model = b_q_model)

# Check fit, r-square, standardized estimates
# summary(b_q_pv, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)

# Plot the model -- rotate from left to right
# semPlot::semPaths(b_q_pv, rotation = 2, whatLabels = "est.std", edge.label.cex = 0.8)


# Plot the new model
# semPlot::semPaths(brand_rep_sem, rotation = 2)

# Get the coefficient information
# lavaan::standardizedSolution(brand_rep_sem) %>% filter(op == "~")

# Get the r-squared
# r_squared <- car::inspect(brand_rep_sem, "r2")["F2"]
# r_squared


# Compute factor scores in lavaan -- store as data frame
brand_rep_scores <- as.data.frame(predict(brand_rep_CFA))

# Summary statistics of our factor scores
psych::describe(brand_rep_scores)

# Plot histograms for each variable
psych::multi.hist(brand_rep_scores)

# Are they normally distributed? Check using map()
map(brand_rep_scores, shapiro.test)


# Linear regression of standardized spending and factor scores
# bq_fs_reg <- lm(spend ~ F1 + F2 + F3, data = bq_fs_spend)

# Summarize results, round estimates
# rounded_summary <- round(coef(bq_fs_reg), 3)
# rounded_summary

# Summarize the results of CFA model
# summary(brand_qual_pv)

# Compare the r-squared of each
# inspect_rsq <- car::inspect(brand_qual_pv, "r2")["spend"]
# inspect_rsq
# summary(bq_fs_reg)$r.squared


# Descriptive statistics grouped by 'time'
# psych::describeBy(brand_rep_t1_t2, "time")

# Test retest: time == 1 versus time == 2 by id = "id"
# brand_rep_test_retest <- psych::testRetest(t1 = filter(brand_rep_t1_t2, time == 1), t2 = filter(brand_rep_t1_t2, time == 2), id = "id")

# brand_rep_test_retest$r12


brand_rep <- file001 %>%
    select(-one_of_a_kind)
brand_rep


# Split data into odd and even halves
brand_rep_efa_data <- as.data.frame(brand_rep)[c(TRUE,FALSE),]
brand_rep_cfa_data <- as.data.frame(brand_rep)[c(FALSE,TRUE),]

# Get factor loadings of brand_rep_efa_data EFA
efa <- psych::fa(brand_rep_efa_data, nfactors = 3)
efa$loadings

# Confirm the data that the model was fit to
# car::inspect(brand_rep_cfa, what = "call")

# Check fit measures
# fitmeasures(brand_rep_cfa)[c("cfi", "tli", "rmsea")]

```
  
  
  
***
  
###_R for SAS Users_  
  
Chapter 1 - Getting Started with R  
  
Get Help and Load Data in R:  
  
* R Functionality includes packages and global environment  
	* ls() is similar to PROC DATASETS  
    * load() is similar to LIBNAME or DATA + SET  
* Generally, an R session begins without any objects loaded in memory  
* Abalone dataset is available from UC Irvine  
	* load("abalone.Rdata")  
    * library(myPkg) will load myPkg  
    * sessionInfo() shows the currently loaded packages  
  
Dataset Contents and Descriptive Statistics:  
  
* Can use readr::read_csv() to load a CSV file in to the environment  
	* myObject <- readr::read_csv("myFile.csv")  
    * str(myObject)  # look at the variables and dimensions of the dataset  
    * dim(myObject)  # dimensions of the object  
    * names(myObject)  # variable names  
    * head(myObject)  # first 6 rows  
    * tail(myObject)  # last 6 rows  
* The dplyr functions are helpful for manipulating data frames  
	* arrange(myDF, myCol)  # sort by ascending myCol  
    * myDF %>% arrange(myCol)  # sort by ascending myCol (same as above)  
    * myDF %>% pull(myCol)  # extract myCol as a vector  
    * myDF %>% pull(myCol) %>% mean() # extract myCol as a vector and take the mean()  
    * myDF %>% select(a, b) %>% summary()  # keep only columns "a" and "b", then create a summary of that frame  
  
Graphical Visualizations:  
  
* The ggplot2 package is useful for plotting data in R  
	* "grammar of graphics" - layering approach to graphics  
    * Build a base layer and then add to it  
    * ggplot(data=myDF, aes(x=x, y=y)) + geom_boxplot() + theme_bw()  # build a boxplot of y by x using the bw theme  
    * Can include options like color= or fill=  
    * Can customize axes using xlab(), ylab(), and ggtitle()  
  
Example code includes:  
```{r}

# List the objects in global environment
# ls()

# Load the "abalone.RData" dataset
# load("abalone.RData")

# List objects in global environment again 
# ls()

# Learn more about the load function
# help(load)

# Learn more about the ls function
# help(ls)


# Run sessionInfo() see packages available to this session
# sessionInfo()

# Load Hmisc package
# library(Hmisc)

# Run sessionInfo() again to see updated package list
# sessionInfo()


# Load abalone.csv dataset assign output to abalone
# abalone <- readr::read_csv("abalone.csv")

data(abalone, package = "AppliedPredictiveModeling")
abalone <- tibble::as_tibble(abalone)

# Get the dimensions of the abalone dataset object
dim(abalone)

# Get variable names in the abalone dataset object
names(abalone)

chgName <- function(x) {
    paste0(stringr::str_to_lower(stringr::str_sub(x, 1, 1)), stringr::str_sub(x, 2))
}

tmpNames <- sapply(names(abalone), FUN=chgName, USE.NAMES=FALSE)
names(abalone) <- tmpNames
names(abalone)


# View top 3 rows of abalone dataset using head()
head(abalone, 3)

# View bottom 3 rows of abalone dataset using tail()
tail(abalone, 3)

# Run arrange function from dplyr to sort the data by rings
arrange(abalone, rings)

# Rewrite the line of code above using the %>% notation
abalone %>% 
    arrange(rings)


# Find mean length of abalones using pull() and mean() 
abalone %>% 
    pull(longestShell) %>% 
    mean()

# Find the median wholeWeight of the abalones
abalone %>% 
    pull(wholeWeight) %>% 
    median()

# Get descriptive statistics of diameter and shellWeight
abalone %>% 
    select(diameter, shellWeight) %>% 
    summary()


# Add a title and labels for the axes
ggplot(abalone, aes(shellWeight)) + 
    geom_histogram(color = "blue", fill = "yellow") +
    xlab("Shell Weight") + 
    ylab("Frequency Counts") + 
    ggtitle("Shell Weights Histogram")


# Change the boxplots to the violin geom
ggplot(data = abalone, aes(x=type, y=shuckedWeight)) + 
    geom_violin() + 
    theme_bw()


# Create panel plot of scatterplot for sex categories
ggplot(abalone, aes(diameter, wholeWeight)) + 
    geom_point() + 
    geom_smooth() + 
    facet_wrap(~type)

```
  
  
  
***
  
Chapter 2 - Data Wrangling  
  
Objects - Building Blocks of R:  
  
* Objects are the buidling blocks of R - "everything in R is an object"  
	* The assignment operator is <-  
    * The c() operator is the combine function, which can be used to create a vector  
* Can create matrices using matrix() and data frames using data.frame()  
* Can get the class of an objects using class() or see it as one of the outputs of str()  
  
Selecting Elements from Objects:  
  
* Can select elements from a matrix using myMtx[myRow, myCol] - if myRow is blank it means "all rows" and if myCol is blank it means "all columns"  
	* Same approach for data frames, though names for columns can be used rather than numbers  
* Can select variables using a:b, which will pull all variables from a through b  
* Can grab rows using dplyr::slice()  
	* slice(2:3) will pull rows 2 and 3  
  
Manipulating Datasets and Data Objects:  
  
* Can use the mutate() function to create new variables, similar to data a; set a; myVar=myOldVar/100; in SAS  
* Can use the ifelse() function to create new variables based on a boolean condition  
* Can use is.numeric(), is.vector(), is.character(), is.data.frame(), is.matrix()  
* Can coerce object types to new types using as.xxx()  
	* as.matrix()  
    * as.numeric()  
    * as.integer()  
  
Data Quality and Cleaning:  
  
* Can use the summary() function to check for minima, maxima, mean, median, etc.  
* The geom_dotplot() can be helpful in identifying outliers  
* Can filter out cases with errors, then resume the analysis  
  
Example code includes:  
```{r}

# Assign the value of 1 to an object x
x <- 1

# Create object w by adding the value 4 to x
w <- x + 4

# Create vector object y 
y <- c(5, 2, 3)

# Create a vector z 
z <- c("red", "green", "blue")

# Create a logical object v 
v <- c(TRUE, TRUE, FALSE)


numvec1 <- 1:3
numvec2 <- c(5, 2, 3)
charvec <- c("red", "green", "blue")
logicvec <- c(TRUE, TRUE, FALSE)

# Create matrix with numvec1 and numvec2 with 2 columns, 3 rows
nummtx <- matrix(c(numvec1, numvec2), ncol=2, nrow=3)

# Create dataframe from numvec1, numvec2, charvec and logicvec
dataframe <- data.frame(numvec1, numvec2, charvec, logicvec)

# Get the structure of dataframe
str(dataframe)


# Select the 2nd element in vector charvec
charvec[2]

# Select the 2nd column of matrix nummtx
nummtx[, 2]

# Pull column numvec1 from dataframe assign it to id
id <- dataframe %>% 
    pull(numvec1)

# Pull column charvec from dataframe and determine the class of charvec
dataframe %>% 
    pull(charvec) %>% 
    class()


# Create abaloneMod from abalone, add new variable age
abaloneMod <- abalone %>%
    mutate(age = rings + 1.5)

# Add three more variables to abaloneMod
abaloneMod <- abaloneMod %>%
    mutate(pctShucked = shuckedWeight * 100 / wholeWeight) %>%
    mutate(pctViscera = visceraWeight * 100 / wholeWeight) %>%
    mutate(pctShell = shellWeight * 100 / wholeWeight)

# Select age, pctShucked, pctViscera, pctShell, run summary()
abaloneMod %>%
    select(age, pctShucked, pctViscera, pctShell) %>%
    summary()


# Add new character variable agecat to abaloneMod
abaloneMod <- abaloneMod %>%
    mutate(agecat = factor(ifelse(test = age < 10.5, "< 10.5", "10.5 and older")))

# Add new logical variable adult to abaloneMod
abaloneMod <- abaloneMod %>%
    mutate(adult = (type != "I"))

# Select age, agecat, sex, adult and view top 6 rows
abaloneMod %>%
    select(age, agecat, type, adult) %>%
    head(6) 


# Convert adult to numeric and confirm class type
abaloneMod %>% 
    mutate(adult = as.numeric(adult)) %>% 
    pull(adult) %>% 
    class()

# Select three dimensions, save as abalonedim
abalonedim <- abaloneMod %>%
    select(longestShell, diameter, height)

# Convert abalonedim to a matrix and confirm class type
abalonedim %>% 
    as.matrix() %>% 
    class()


# Pull height from abaloneMod and run summary()
abaloneMod %>% 
    pull(height) %>%
    summary()

# Keep cases with height > 0 assign to abaloneKeep
abaloneKeep <- abaloneMod %>%
    filter(height > 0)

# Make histogram of updated heights in abaloneKeep
ggplot(abaloneKeep, aes(x=height)) +
    geom_histogram()


# Sort abaloneKeep by pctShucked, view largest 6 pctShucked
abaloneKeep %>%
    arrange(pctShucked) %>%
    pull(pctShucked) %>%
    tail(6)

# Scatterplot of shuckedWeight by wholeWeight add y=x line
ggplot(abaloneKeep, aes(x=wholeWeight, y=shuckedWeight)) +
    geom_point() +
    geom_abline(intercept=0, slope=1)

# Keep cases where shuckedWeight is less than wholeWeight
abaloneKeep <- abaloneKeep %>%
    filter(shuckedWeight < wholeWeight)


# Make scatterplot of height by length add y=x line
ggplot(abaloneKeep, aes(x=longestShell, y=height)) +
    geom_point() + 
    geom_abline(intercept=0, slope=1)

# Make scatterplot of diameter by length add y=x line
ggplot(abaloneKeep, aes(x=longestShell, y=diameter)) +
    geom_point() + 
    geom_abline(intercept=0, slope=1)

# Keep abalones with length > both height and diameter
abaloneKeep <- abaloneKeep %>%
    filter((longestShell > height) & (longestShell > diameter))


# Dimensions of final dataset with samples in abaloneKeep
dim(abaloneKeep)

# Get summary statistics of all variables in abaloneKeep
summary(abaloneKeep)

# Scatterplot of shuckedWeight by wholeWeight add y=x line
ggplot(abaloneKeep, aes(x=wholeWeight, y=shuckedWeight)) +
    geom_point() +
    geom_abline(intercept=0, slope=1)

# Scatterplot of length by diameter add y=x line
ggplot(abaloneKeep, aes(x=diameter, y=longestShell)) +
    geom_point() +
    geom_abline(intercept=0, slope=1)

```
  
  
  
***
  
Chapter 3 - Data Exploration  
  
Exploratory Data Analysis:  
  
* Can get summary statistics in R using several different packages and functions  
	* summary()  
    * Hmisc::describe()  
    * psych::describe()  
* Can use the dplyr::summarize() to create new names for statistics  
* Can use the dplyr::summarize_all() to get key statistic  
	* myDF %>% select(a, b) %>% summarize_all(funs(mean, sd))  
* Can also use the dplyr::group_by() to get the output summarized by values of a key variable  
  
Correlations and T-tests:  
  
* Can assess the strenght of correlations using psych::corr.test()  
* Can use Ggally::ggpairs() to get the pairs plot, and density plots for all of the key variables  
	* Can add aes(color=a) to have the charts colored by levels of variable a  
* Can add the counts in several ways, including use of an extra group_by statement  
	* myDF %>% select(a, b) %>% group_by(a) %>% group_by(N = n(), add=TRUE) %>% summarize_all(funs(mean, sd))  
* Can perform F tests for equal variances, and t-tests for equal means  
	* var.test(a ~ b, data = myDF)  
    * t.test(a ~ b, data = myDF)  # unpooled t-test is the default  
    * t.test(a ~ b, data = myDF, var.equal = TRUE)  # pooled t-test to over-ride defaults  
  
Categorical Data: Analyze and Visualize:  
  
* Can use the table() command inside the with() command  
	* myDF %>% with(table(a))  
* Can create contingency tables using either table() or gmodels::CrossTable()  
	* gmodels::CrossTable(chisq=TRUE)  # will run a Chi-squared test on the table resulting from the gmodels call  
    * Can also include prop.r=, prop.t=, prop.chisq=, expected= options inside gmodels::CrossTable()  
* Can get the mosaicplot using mosaicplot()  
	* mosaicplot(a ~ b, data=myDF, color=c("red", "blue"), main="myTitle")  
  
Example code includes:  
```{r}

# Run describe() from Hmisc for sex, length, diameter, height
abaloneKeep %>% 
    select(type, longestShell, diameter, height) %>% 
    Hmisc::describe()


# Run describe() from psych for length, diameter, height
abaloneKeep %>% 
    select(longestShell, diameter, height) %>% 
    psych::describe()


# Run summary() for shuckedWeight and wholeWeight
abaloneKeep %>%
    select(shuckedWeight, wholeWeight) %>%
    summary()

# Get mean and sd for length
abaloneKeep %>%
    summarize(mean_length = mean(longestShell), sd_length = sd(longestShell))

# Get mean and sd for height and diameter
abaloneKeep %>%
    select(height, diameter) %>%
    summarise_all(list(~mean(.), ~sd(.)))


# Get min, mean, sd, median, and max for age by sex
abaloneKeep %>% 
    group_by(type) %>% 
    select(type, age) %>%
    summarise_all(list(~min(.), ~mean(.), ~sd(.), ~median(.), ~max(.)))

# Get median, 25th, 75th percentiles for wholeWeight by adult
abaloneKeep %>% 
    group_by(adult) %>%
    select(adult, wholeWeight) %>%
    summarise(median_wweight = median(wholeWeight), 
              q1_wweight = quantile(wholeWeight, probs = 0.25), 
              q3_wweight = quantile(wholeWeight, probs = 0.75)
              )


# Get correlations age and weights with corr.test()
abaloneKeep %>% 
    select(age, wholeWeight, shuckedWeight, shellWeight, visceraWeight) %>%
    psych::corr.test()

# Get correlations age and dimensions with corr.test()
abaloneKeep %>% 
    select(age, longestShell, diameter, height) %>%
    psych::corr.test()


# Make ggpairs plot of age, wholeWeight, shellWeight
abaloneKeep %>% 
    select(age, wholeWeight, shellWeight) %>%
    GGally::ggpairs()

# Make ggpairs plot of age, wholeWeight, shellWeight
abaloneKeep %>% 
    select(type, age, wholeWeight, shellWeight) %>%
    GGally::ggpairs(aes(color=type))


# Get n, mean, sd for length by adult groups
abaloneKeep %>% 
    select(longestShell, adult) %>%
    group_by(adult) %>%
    group_by(N = n(), add = TRUE) %>%
    summarise_all(list(~mean(.), ~sd(.)))

# Run equal variance and pooled t-tests of length by adult 
var.test(longestShell ~ adult, data = abaloneKeep)
t.test(longestShell ~ adult, data = abaloneKeep, var.equal = TRUE)

# Make boxplots of length by adult
ggplot(abaloneKeep, aes(x=adult, y=longestShell)) +
    geom_boxplot()


# Create frequency table of sex by agecat
tablesexage <- abaloneKeep %>% 
    with(table(type, agecat))

# Run chi-square test using tablesexage
chisq.test(tablesexage)
  
# Use CrossTabs, run chisq test
abaloneKeep %>%
    with(gmodels::CrossTable(type, agecat, chisq=TRUE, prop.c = FALSE, prop.t = FALSE, 
                             prop.chisq = FALSE, expected = TRUE
                             )
         )


# Make mosaicplot of sex by agecat
mosaicplot(type ~ agecat, data = abaloneKeep, 
           color = c("light blue", "dark grey"), main = "Abalone Sex by Age"
           )

# Make mosaicplot of adult by agecat
mosaicplot(adult ~ agecat, data = abaloneKeep, 
           color = c("light green", "purple"), main = "Abalone Adult by Age"
           )


# Add shellcat to abaloneKeep
abaloneKeep <- abaloneKeep %>%
    mutate(shellcat = ifelse(shellWeight <= 0.235, "<= 0.235g", "> 0.235g"))

# Run chisq.test of agecat by shellcat
tableageshell <- abaloneKeep %>% 
    with(table(agecat, shellcat))

chisq.test(tableageshell)

# Make mosaicplot of agecat by shellcat
mosaicplot(agecat ~ shellcat, data = abaloneKeep, 
           color = c("light blue", "grey"), main = "Mosaicplot of age by shell categories"
           )

```
  
  
  
***
  
Chapter 4 - Models and Presentation  
  
Working with Output Objects:  
  
* Can use summary() to get the summary of various types of objects  
	* class(summary(x)) # matrix, which means that you can use column and row filtering  
* The dplyr::summarize_all() creates a data.frame object  
	* davissmall <- daviskeep %>% select(weight, height) %>% summarize_all(funs(mean, sd))  
    * str(davissmall)  # 1 obs of 4 variables  
* The psych::describe() function produces a data.frame that is also 'psych' and 'describe'  
  
Working with Lists:  
  
* Lists are more flexible than data frames; can combine objects of different lengths and data types  
* Can set the names of objects in the list using names()  
* Can extract objects out of the list using the $ operator  
	* myList$myVar  
* The Hmisc::describe() outputs lists of lists  
* Many times, the output of a function will be a list of lists  
	* names(myOutput)  # get the names  
    * myOutput$myName  # extract one of the elements  
  
ANOVA and Linear Models:  
  
* Can add na.rm=TRUE to eliminate NA and NaN from the calculations and functions  
* Can run ANOVA in R using a combination of aov() and TukeyHSD()  
	* myAOV <- aov(y ~ x, data=myDF)  
    * TukeyHSD(myAOV)  
    * Note that the aov() function will automatically remove missing values; no need for na.rm=TRUE  
* Linear regression in R can be run using the lm() function  
	* myLM <- lm(y ~ x, data=myDF)  
  
Final Models Evaluation:  
  
* Can run multiple linear models and run summary() on each to compare statistics  
* Can also run AIC using AIC(modA, modB)  
* Can run regressions on a subset of the data using subset=  
	* lm(y ~ x, data=myDF, subset=(myCol=="myVar"))  
  
Wrap Up:  
  
* R environment, packages, and plotting  
* Objects in R  
* Exploratory data analysis, statistical tests  
* Saving output objects and running analyses  
  
Example code includes:  
```{r}

# Save summary stats output for 3 requested weights
absummary <- abaloneKeep %>% 
    select(wholeWeight, shuckedWeight, shellWeight) %>% 
    summary()

# Pull columns 1 to 2 out of absummary
absummary[, 1:2]

# Save describe from psych output for weights as abpsych
abpsych <- abaloneKeep %>%
    select(wholeWeight, shuckedWeight, shellWeight) %>%
    psych::describe()

# Display the mad element from abpsych
data.frame(abpsych)[,"mad",drop=FALSE]


# Save output from summarise_all() for diameter
abdiam <- abaloneKeep %>% 
    select(diameter) %>%
    summarise_all(list(~mean(.), ~sd(.), ~median(.), ~min(.), ~max(.)))

# Use str() to see class of abdiam and names of elements
str(abdiam)

# Use $ selector to display sd 
abdiam$sd


# Save output from summarise_all() for diameter by sex
absexdiam <- abaloneKeep %>% 
    group_by(type) %>%
    select(type, diameter) %>%
    summarize_all(list(~mean(.), ~sd(.), ~median(.), ~min(.), ~max(.)))

# Get structure of absexdiam
str(absexdiam)

# Use filter() function to extract row for sex == "I"
absexdiam %>% filter(type == "I")


# Save output from Hmisc::describe(), view class of output
abhmisc <- abaloneKeep %>% 
    select(wholeWeight, shuckedWeight, shellWeight) %>% 
    Hmisc::describe()

# Get structure of abhmisc
str(abhmisc)

# View shuckedWeight statistics from abhmisc
abhmisc$shuckedWeight

# View shuckedWeight extremes from abhmisc
abhmisc$shuckedWeight$extremes


# Save correlation from psych::corr.test(), run str()
abcorr <- abaloneKeep %>% 
    select(age, wholeWeight, shuckedWeight, shellWeight, visceraWeight) %>%
    psych::corr.test()

# Display Pearson's r's and p-values from abage
abcorr$r
abcorr$p


# Perform pooled t-test of length by adult, save as abttest
abttest <- t.test(longestShell ~ adult, data = abaloneKeep, var.equal=TRUE)

# Display statistic, parameter and conf.int from abttest
abttest$statistic
abttest$parameter
abttest$conf.int


# Create frequency table of sex by agecat
tablesexage <- abaloneKeep %>% 
    with(table(type, agecat))

# Chi-square test using tablesexage
cssexage <- chisq.test(tablesexage)

# Display expected values and p.value from cssexage
cssexage$expected
cssexage$p.value


# Compute n(), mean(), sd() and var() of age by sex
abaloneKeep %>%
    group_by(type) %>%
    group_by(N=n(), add=TRUE) %>%
    select(type, age) %>%
    summarise_all(list(~mean(.), ~sd(.), ~var(.)))

# Run aov() of age by sex, save as abaov
abaov <- aov(age ~ type, data = abaloneKeep)

# Run summary() of abaov
summary(abaov)

# Perform TukeyHSD posthoc pairwise tests on abaov
TukeyHSD(abaov)


# Run lm() of age by shuckedWeight, save output as lmshucked
lmshucked <- lm(age ~ shuckedWeight, data = abaloneKeep)

# Display coefficients element from lmshucked
lmshucked$coefficients

# Save summary() output of lmshucked as smrylmshucked
smrylmshucked <- summary(lmshucked)

# Show r.squared and adj.r.squared elements of smrylmshucked
smrylmshucked$r.squared
smrylmshucked$adj.r.squared


# Run lm() for age by shuckedWeight and by shellWeight
lmshucked <- lm(age ~ shuckedWeight, data = abaloneKeep)
lmshell <- lm(age ~ shellWeight, data = abaloneKeep)

# Run summary() for each model fit and save results
smrylmshucked <- summary(lmshucked)
smrylmshell <- summary(lmshell)

# Display r.squared for both models
smrylmshucked$r.squared
smrylmshell$r.squared

# Compare AICs for both models
AIC(lmshucked, lmshell)


# Save model output and output of model summary for infants
modelInfants <- lm(age ~ shellWeight, data = abaloneKeep, subset = (type == "I"))
summaryInfants <- summary(modelInfants)

# Save model output and output of model summary for females
modelFemales <- lm(age ~ shellWeight, data = abaloneKeep, subset = (type == "F"))
summaryFemales <- summary(modelFemales)

# Save model output and output of model summary for males
modelMales <- lm(age ~ shellWeight, data = abaloneKeep, subset = (type == "M"))
summaryMales <- summary(modelMales)

# Display each model r.squared values
t(data.frame(summaryInfants$r.squared, summaryFemales$r.squared, summaryMales$r.squared))

```
  
  
  
***
  
***
  
###_Quantitative Risk Management in R_  
  
Chapter 1 - Exploring Market Risk-Factor Data  
  
Introduction - Alexander McNeil, author of book "Quantitative Risk Management" (contains theory behind the course):  
  
* Can also review on qrmtutorial.org (helper materials to the book)  
* Packages qrmdata (large data sets) and qrmtools (useful functions) are available as R packages  
* The goal of QRM is to measure and then manage the risk of a portfolio  
	* Can manage risk by selling assets, buying assets (diversify), maintaining sufficient capital, etc.  
* Value at risk (VaR) is an important concept in this course and in this space  
	* Dependent on many risk factors - interest rates, index prices, exchange rates, etc.  
  
Risk-factor returns:  
  
* Changes in risk factors are risk-factor returns or returns  
* Suppose that Zt is a time series of risk factor values  
	* Simple returns: Xt = Zt - Z(t-1)  # used for values that are very closest to zero  
    * Relative returns: Xt = (Zt - Z(t-1)) / Z(t-1)  # easiest to interpret  
    * Log returns: Xt = ln(Zt) - ln(Z(t-1))  # most commonly used (desirable feature of non-negativity, very close to relative returns for small values, particularly easy to aggregate, geometrics Brownian motion)  
* In practice, log returns tend not to be normally distributed  
	* logR <- diff(log(mySeries))[-1]  # first value will be NA and is not helpful for plotting  
  
Aggregating log returns:  
  
* Suppose you have daily log-returns Xt - can just sum the log returns over the time period of interest  
	* ln(Z(t+5)) - ln(Zt) = sum-of-relevant-Zt  
    * Can use apply.weekly() or apply.monthly() from the xts package  # need the FUN=sum for each of these  
  
Exploring other risk factors - commodities prices and zero-coupon bonds data:  
  
* Suppose p(t, T) is the price at time t of a bond paying back 1 unit at time T  
    * The yield is frequently considered to be the risk factor - comparable across different maturities, T  
    * y(t, T) = -ln(p(t, T)) / (T - t)  
    * The mapping T for y(t, T) defines the yield curve - some arguments about whether to use simple returns or log returns on yields  
  
Example code includes:  
```{r}

library(xts)
library(zoo)

# Load DJ index
data(DJ, package='qrmdata')

# Show head() and tail() of DJ index
head(DJ)
tail(DJ)

# Plot DJ index
plot(DJ)

# Extract 2008-2009 and assign to dj0809
dj0809 <- DJ["2008/2009"]

# Plot dj0809
plot(dj0809)


# Load DJ constituents data
data("DJ_const", package='qrmdata')

# Apply names() and head() to DJ_const
names(DJ_const)
head(DJ_const)

# Extract AAPL and GS in 2008-09 and assign to stocks
stocks <- DJ_const["2008/2009", c("AAPL", "GS")]

# Plot stocks with plot.zoo()
plot.zoo(stocks)


# Load exchange rate data
data("GBP_USD", package='qrmdata')
data("EUR_USD", package='qrmdata')

# Plot the two exchange rates
plot(GBP_USD)
plot(EUR_USD)

# Plot a USD_GBP exchange rate
plot(1/GBP_USD)

# Merge the two exchange rates GBP_USD and EUR_USD
fx <- merge(GBP_USD, EUR_USD, all = TRUE)

# Extract 2010-15 data from fx and assign to fx0015
fx0015 <- fx["2010/2015"]

# Plot the exchange rates in fx0015
plot.zoo(fx0015)


# Compute the log-returns of dj0809 and assign to dj0809_x
dj0809_x <- diff(log(dj0809))

# Plot the log-returns
plot(dj0809_x)

# Compute the log-returns of djstocks and assign to djstocks_x
djstocks_x <- diff(log(stocks))

# Plot the two share returns
plot.zoo(djstocks_x)

par(mfrow=c(1, 1))
par(mfcol=c(1, 1))

# Compute the log-returns of GBP_USD and assign to erate_x
erate_x <- diff(log(GBP_USD))

# Plot the log-returns
plot(erate_x)


# You already know that you can use plot.zoo() to plot multiple time series
# For a four-dimensional time series data, the call plot.zoo(data) creates four separate plots by default, unless you include the parameter plot.type = "single" to plot all four series in one plot
# You can also add even more parameters such as col to specify different colors and type = "h" to get vertical bars instead of joining points, which can sometimes be a better way of displaying returns.
# plot.zoo(x, plot.type, col = 1, type = "l", ...)


djstocks <- DJ_const["2008/2009", c("AAPL", "AXP", "BA", "CAT")]
str(djstocks)

# Plot djstocks in four separate plots
plot.zoo(djstocks)

# Plot djstocks in one plot and add legend
plot.zoo(djstocks, plot.type="single", col=1:4)
legend(julian(x = as.Date("2009-01-01")), y = 70, legend = names(DJ_const)[1:4], fill = 1:4)

# Compute log-returns and assign to djstocks_x
djstocks_x <- diff(log(djstocks))

# Plot djstocks_x in four separate plots
plot.zoo(djstocks_x)

# Plot djstocks_x with vertical bars
plot.zoo(djstocks_x, type="h")


djx <- DJ["2000/2015"]
str(djx)

par(mfrow=c(1, 1))
par(mfcol=c(1, 1))

# Plot djx
plot(djx)

# Plot weekly log-returns of djx
plot(apply.weekly(djx, FUN=sum), type="h")

# Plot monthly log-returns of djx
plot(apply.monthly(djx, FUN=sum), type="h")

# Plot djreturns
plot.zoo(djstocks_x)

# Plot monthly log-returns of djreturns
plot.zoo(apply.monthly(djstocks_x, FUN=colSums), type="h")


data(GOLD, package='qrmdata')
data(OIL_Brent, package='qrmdata')

gold <- GOLD['1990/2015']
oil <- OIL_Brent['1990/2015']
str(gold)
str(oil)

par(mfrow=c(1, 1))
par(mfcol=c(1, 1))

# Plot gold and oil prices
plot(gold)
plot(oil)

# Calculate daily log-returns
goldx <- diff(log(gold))
oilx <- diff(log(oil))

# Calculate monthly log-returns
goldx_m <- apply.monthly(goldx, FUN=sum)
oilx_m <- apply.monthly(oilx, FUN=sum)

# Merge goldx_m and oilx_m into coms
coms <- merge(goldx_m, oilx_m)

# Plot coms with vertical bars
plot.zoo(coms, type="h")

# Make a pairwise scatterplot of coms
pairs(as.zoo(coms))


data(ZCB_CA, package="qrmdata")
zcb <- ZCB_CA['2006/2015']
str(zcb)

# Identify and create vector containing column names for 1, 5, 10 year yields
names(zcb)
yield_cols <- c("1.00y", "5.00y", "10.00y")

# Compute log-returns as zcb_x and plot them for same maturities
zcb_x <- diff(log(zcb))
plot.zoo(zcb_x[, yield_cols])

# Compute simple returns as zcb_x2 and plot them for same maturities
zcb_x2 <- diff(zcb)
plot.zoo(zcb_x2[, yield_cols])

# Make a vector containing the maturities                             
maturity <- (1:120)/4

# Plot the yield curve for the first day of zcb
plot(maturity, zcb[1, ], ylim = range(zcb), type = "l", ylab = "yield (%)", col = "red")

# Add a line for the last day of zcb
lines(maturity, zcb[nrow(zcb), ])

```
  
  
  
***
  
Chapter 2 - Real World Returns are Riskier than Normal  
  
Normal distribution - if risk returns follow GBM, than log-returns would be normal:  
  
* The normal distribution (bell curve) depends on a well known formula containing mu and sigma - N(mu, sigma-squared)  
	* The sum of independent normal variables is itself normally dsitributed  
* The Central Limit Theorem (CLT) says that the dsitribution of the sum of many iid trials will itself by normally dsitributed (even if the underlying iid distribution is decidedly non-normal)  
* The "method of moments" is used to estimate the sample mean and the sample variance  
	* The sigma-squared-hat with a mu subscript is the unbiased estimator (divide by n-1)  
    * The sigma-squared-hat with no subscript is the maximum likelihood estimator (divide by n)  
  
Testing for normality:  
  
* The Q-Q test is a graphical test for normality, so the reference distribution is the standard normal  
	* Q-Q plots can use other reference distributions  
    * qqnorm(myData)  
    * qqline(myData)  # qqnorm should tend to fall along the line at all quantiles  
  
Skewness, kurtosis, Jarque-Bera test:  
  
* The Jarque-Bera test is based on skewness and kurtosis  
	* The skewness (b) is the third moment, a measure of its asymmetry (b=0 for normal distribution) - moments::skewness()  
    * The kurtosis (k) is the fourth moment, a measure of its heavy tails (k=3 for normal distribution) - moments::kurtosis()  
* The Jarque-Bera test compares both skewness and kurtosis using a single test statistic, T  
	* T = N/6 * (b**2 + 0.25*(k-3)**2)  # uses a chi-squared with 2 degrees of freedom  
    * moments::jarque.test(myData)  
* Daily returns tend to be very non-normal, but the CLT suggests that the sum of these returns may become increasingly normal  
  
Student t-distribution:  
  
* The Student t-distribution has mu, sigma, and df and tends to be better for smaller data sets  
	* With df at infinite, the student t-distribution becomes the normal distribution (df is also called nu) ; for nu less than 4, the kurtosis is infinite  
    * The maximum likelihood method can be found using QRM::fit.st() - the $par.ests will have the parameters  
* Due to some vagaries in how the dt() function is implemented in R, converting for a known mu, sigma, and nu  
	* dt((myData - mu)/sigma, df=nu) / sigma  
  
Example code includes:  
```{r}

par(mfcol=c(1, 1))
par(mfrow=c(1, 1))

# Calculate average and standard deviation of djx
djx <- as.vector(diff(log(DJ['2008/2009'])))
djx <- sort(djx[2:length(djx)])
str(djx)

mu <- mean(djx)
sigma <- sd(djx)

# Plot histogram of djx
hist(djx, nclass=20, probability=TRUE)

# Add the normal density as a red line to histogram
lines(djx, dnorm(djx, mean=mu, sd=sigma), col = "red")

# Plot non-parametric KDE of djx
plot(density(djx))

# Add the normal density as red line to KDE
lines(djx, dnorm(djx, mean=mu, sd=sigma), col = "red")


# Make a Q-Q plot of djx and add a red line
qqnorm(djx)
qqline(djx, col="red")

# Calculate the length of djx as n
n <- length(djx)

# Generate n standard normal variables, make a Q-Q plot, add a red line
x1 <- rnorm(n)
qqnorm(x1)
qqline(x1, col="red")

# Generate n Student t variables, make a Q-Q plot, add a red line
x2 <- rt(n, df = 4)
qqnorm(x2)
qqline(x2, col="red")

# Generate n standard uniform variables, make a Q-Q plot, add red line
x3 <- runif(n)
qqnorm(x3)
qqline(x3, col="red")


dj0811 <- DJ_const["2008/2011"]
djreturns <- diff(log(dj0811))[2:nrow(dj0811), !(names(dj0811) %in% c("V"))]
str(djreturns)

# Calculate average and standard deviation of djx
djx <- as.vector(diff(log(DJ['2008/2011'])))
djx <- sort(djx[2:length(djx)])
str(djx)

# Calculate skewness and kurtosis of djx
moments::skewness(djx)
moments::kurtosis(djx)

# Carry out a Jarque-Bera test for djx
moments::jarque.test(djx)

# Calculate skewness and kurtosis of djreturns 
s <- apply(djreturns, 2, FUN=moments::skewness)
k <- apply(djreturns, 2, FUN=moments::kurtosis)

# Plot k against s and add text labels to identify stocks
plot(s, k, type="n")
text(s, k, names(s), cex = 0.6)

# Carry out Jarque-Bera tests for each constituent in djreturns
apply(djreturns, 2, FUN=moments::jarque.test)


dj0015 <- DJ_const["2000/2015"]
djx_d <- diff(log(dj0015))[2:nrow(dj0015), !(names(dj0015) %in% c("V"))]
str(djx_d)

# Calculate weekly and monthly log-returns from djx_d
djx_w <- apply.weekly(djx_d, FUN=colSums)
djx_m <- apply.monthly(djx_d, FUN=colSums)

# Calculate the p-value for each series in djx_d
apply(djx_d, 2, function(v){moments::jarque.test(v)$p.value})

# Calculate the p-value for each series in djx_w
apply(djx_w, 2, function(v){moments::jarque.test(v)$p.value})

# Calculate the p-value for each series in djx_m
apply(djx_m, 2, function(v){moments::jarque.test(v)$p.value})


# Calculate average and standard deviation of djx
djx <- diff(log(DJ))['2008/2011']
str(djx)

# When you aggregate series by summing daily log-returns into longer intervals, you analyze a smaller amount of observations
# To preserve the quantity of data, you can calculate overlapping returns with the rollapplyr() function; this also creates strong correlations between observations

# There are 5 trading days in the average calendar week
# By computing the 5-day moving sums of the log-returns of daily index data, you obtain approximate overlapping weekly returns ending on each calendar week
# Similarly, calculating 21-day moving sums gives approximate overlapping monthly returns, and calculating 63-day moving sums gives approximate overlapping quarterly returns

# Calculate a 21-day moving sum of djx
djx21 <- rollapplyr(djx, width=21, FUN=sum)[-(1:20)]

# Calculate a 63-day moving sum of djx
djx63 <- rollapplyr(djx, width=63, FUN=sum)[-(1:62)]

# Merge the three series and plot
djx2 <- merge(djx, djx21, djx63, all=FALSE)
plot.zoo(djx2)

# Compute the skewness and kurtosis for each series in djx2
apply(djx2, 2, FUN=moments::skewness)
apply(djx2, 2, FUN=moments::kurtosis)

# Conduct the Jarque-Bera test to each series in djx2
apply(djx2, 2, FUN=moments::jarque.test)


djx <- sort(as.vector(djx))
str(djx)

# Fit a Student t distribution to djx
tfit <- QRM::fit.st(djx)

# Define tpars, nu, mu, and sigma
tpars <- tfit$par.ests
nu <- tpars["nu"]
mu <- tpars["mu"]
sigma <- tpars["sigma"]

# Plot a histogram of djx
hist(djx, nclass = 20, probability = TRUE, ylim = range(0, 40))

# Compute the fitted t density at the values djx
yvals <- dt((djx - mu)/sigma, df = nu)/sigma

# Superimpose a red line to show the fitted t density
lines(djx, yvals, col = "red")


data(JPY_USD, package="qrmdata")
USD_JPY <- 1/JPY_USD
fx_d <- diff(log(merge(GBP_USD, EUR_USD, USD_JPY, all = TRUE)))['2001/2015']
str(fx_d)

# Plot the daily log-return series in fx_d
plot.zoo(fx_d)

# Apply the Jarque-Bera test to each of the series in fx_d
apply(fx_d, 2, FUN=moments::jarque.test)

fx_m <- apply.monthly(fx_d, FUN=colSums)
str(fx_m)

# Plot the monthly log-return series in fx_m
plot.zoo(fx_m, type="h")

# Apply the Jarque-Bera test to each of the series in fx_m
apply(fx_m, 2, FUN=moments::jarque.test)

# Fit a Student t distribution to each of the series in fx_m
apply(fx_m, 2, function(v){QRM::fit.st(v)$par.ests})


zcbx_m <- apply.monthly(zcb_x['2006/2015', c("1.00y", "5.00y", "10.00y")], 2, FUN=colSums)
str(zcbx_m)
zcb_x2 <- 100 * diff(zcb)
zcbx2_m <- apply.monthly(zcb_x2['2006/2015', c("1.00y", "5.00y", "10.00y")], 2, FUN=colSums)
str(zcbx2_m)

# Plot the interest-rate return series zcbx_m and zcbx2_m
plot.zoo(zcbx_m, type="h")
plot.zoo(zcbx2_m, type="h")

# Make Q-Q plots of the 3rd component series of zcbx_m and zcbx2_m
qqnorm(zcbx_m[, 3])
qqnorm(zcbx2_m[, 3])

# Compute the kurtosis of each series in zcbx_m and zcbx2_m
apply(zcbx_m, 2, FUN=moments::kurtosis)
apply(zcbx2_m, 2, FUN=moments::kurtosis)

# Conduct the Jarque-Bera test on each series in zcbx_m and zcbx2_m
apply(zcbx_m, 2, FUN=moments::jarque.test)
apply(zcbx2_m, 2, FUN=moments::jarque.test)

```
  
  
  
***
  
Chapter 3 - Real World Returns are Volatile and Correlated  
  
Characteristics of volatile return series:  
  
* Investigation of whether financial returns can be modeled as iid - "random walk model" (implication that past prices cannot be used to predict future behavior)  
* Can compare real returns with iid data derived from either normal distributions or t-distributions  
	* Clear differences between real data and modeled data - real data have "volatility clustering"  
    * The "volatility clustering" is valuable for risk management as well as for prediction of future volatility  
  
Estimating serial correlations:  
  
* With serial dependencies, existing volatility is suggestive of continued volatility  
	* The sample acf with lag k can help to estimate the behavior -- denoted as rho-hat(k)  
    * The sample acf assumes that the series has the property of stationarity  
* The sample acf plot (coorelogram) gives the acf for various lags of k - blue dashed lines are 95% confidence bounds for the correlation estimates  
	* Note that k=0 will trivially have correlation of 1  
    * acf(abs(x)) may reveal things that are not shown by acf(x) - sign changes can hide the serial dependencies  
  
The Ljung-Box test - numerical test of squared sample autocorrelations up to a value of k-lags:  
  
* When including k lags, the value of the test statistic is compared with the chi-squared distribution of the same degree of freedom  
	* Should be carried out on either the returns or the abosolute value of the returns (which are often where the volatility are clustered)  
    * Box.test(myData, lag=, type="Ljung")  # the type="Ljung" gets Box-Ljung while the type="Box" gets the Pierce test  
* Generally, the closer the data get to normal (e.g., quarterly or monthly or perhaps weekly rather than daily), the significance of the Box-Ljung test will go away  
  
Looking at extremes in volatile return series:  
  
* Can be interesting just to look at days with extreme returns - for example, where the log-returns are less than -0.025  
	* Tend to be clusters of extreme values on these dimensions around the times of large stock market changes - very different behavior from iid  
  
Stylised facts of return series:  
  
* Properties observed in equities are broadly consistent across other asset classes - commonly called the "stylized facts"  
	* Return series tend to be heavier-tailed than normal distributions (leptokurtic)  
    * Volatility of return series appears to vary over time  
    * Return series tend to show very little serial correlation  
    * Absolute values of return series tend to show protfound serial correlation  
    * Extreme returns appear in clusters  
    * Returns aggregated over longer time periods tend to become more normally distributed and less serially dependent  
  
Example code includes:  
```{r}

djx <- diff(log(DJ))['2008/2011']
str(djx)

npars <- c(mu=-8.150765e-05, sigma=1.643593e-02)

# Compute the length n of djx 
n <- length(djx)

#  Generate a normal sample of size n with parameters given by npars
ndata <- rnorm(n)*npars[2] + npars[1]

# Generate a t-distributed sample of size n with paramaters given by tpars
tdata <- rt(n, df = tpars[1])*tpars[3] + tpars[2]

# Make ndata and tdata into xts objects
ndatax <- xts(ndata, time(djx))
tdatax <- xts(tdata, time(djx))

# Merge djx, ndatax, and tdatax and plot
alldata <- merge(djx, ndatax, tdatax)
plot.zoo(alldata, type="h", ylim = range(alldata))


# Set up a plot region to show 3 plots at a time
par(mfrow = c(3, 1))

# Plot the acfs of djx, ndata and tdata
acf(djx)
acf(ndatax)
acf(tdatax)

# Plot the acfs of the absolute values
acf(abs(djx))
acf(abs(ndatax))
acf(abs(tdatax))

# Plot the acfs of the squares of the values
acf(djx**2)
acf(ndatax**2)
acf(tdatax**2)

par(mfrow = c(1, 1))


dj0715 <- DJ_const["2007/2015"]
djall <- diff(log(dj0715))[2:nrow(dj0715), !(names(dj0715) %in% c("V"))]
str(djall)

# Apply the Ljung-Box test to djx
Box.test(djx, lag = 10, type = "Ljung")

# Apply the Ljung-Box test to absolute values of djx
Box.test(abs(djx), lag = 10, type = "Ljung")

# Apply the Ljung-Box test to all return series in djall
apply(djall, 2, Box.test, lag = 10, type = "Ljung")

# Apply the Ljung-Box test to absolute values of all returns in djall
apply(abs(djall), 2, Box.test, lag = 10, type = "Ljung")


# Create monthly log-returns from djx
djx_m <- apply.monthly(djx, FUN=sum)

# Apply Ljung-Box tests to raw and absolute values of djx_m
Box.test(djx_m, lag = 10, type = "Ljung")
Box.test(abs(djx_m), lag = 10, type = "Ljung")

# Create monthly log-returns from djall
djall_m <- apply.monthly(djall, FUN=colSums)

# Apply Ljung-Box tests to raw and absolute values of djall_m
apply(djall_m, 2, Box.test, lag = 10, type = "Ljung")
apply(abs(djall_m), 2, Box.test, lag = 10, type = "Ljung")


# When you take a long series of iid data, such as several thousand observations, and select a small subset of the most extreme observations, like less than 100, then these extremes appear at random and the spaces or gaps between the extremes follow a distribution that is very close to exponential
# When we carry out the same exercise for a volatile financial log-return series then the extremes appear in clusters during periods of high volatility
# This is another feature of real log-return data that we need to take account of when building models

# In this exercise, you will investigate the irregular time series djx_extremes which contains the 100 most extreme negative log-returns of the Dow Jones index between 1985 and 2015
# You will compare it with iid_extremes which contains the 100 most extreme values in an iid series of the same length as djx_extremes
# To do this, you will use the object exp_quantiles, which contains 100 theoretical quantiles of the standard exponential distribution
# These can be used to construct a Q-Q plot of each dataset against the exponential reference distribution

djx_extremes <- diff(log(DJ))
keyCut <- sort(as.vector(djx_extremes), decreasing=TRUE)[100]
djx_extremes <- djx_extremes[as.vector(djx_extremes) >= keyCut]
str(djx_extremes)
summary(djx_extremes)

exp_quantiles <- c(0.005013, 0.015114, 0.025318, 0.035627, 0.046044, 0.05657, 0.067209, 0.077962, 0.088831, 0.09982, 0.110932, 0.122168, 0.133531, 0.145026, 0.156654, 0.168419, 0.180324, 0.192372, 0.204567, 0.216913, 0.229413, 0.242072, 0.254892, 0.267879, 0.281038, 0.294371, 0.307885, 0.321584, 0.335473, 0.349557, 0.363843, 0.378336, 0.393043, 0.407968, 0.42312, 0.438505, 0.45413, 0.470004, 0.486133, 0.502527, 0.519194, 0.536143, 0.553385, 0.57093, 0.588787, 0.606969, 0.625489, 0.644357, 0.663588, 0.683197, 0.703198, 0.723606, 0.74444, 0.765718, 0.787458, 0.809681, 0.832409, 0.855666, 0.879477, 0.903868, 0.92887, 0.954512, 0.980829, 1.007858, 1.035637, 1.064211, 1.093625, 1.12393, 1.155183, 1.187444, 1.22078, 1.255266, 1.290984, 1.328025, 1.366492, 1.406497, 1.44817, 1.491655, 1.537117, 1.584745, 1.634756, 1.687399, 1.742969, 1.80181, 1.86433, 1.931022, 2.002481, 2.079442, 2.162823, 2.253795, 2.353878, 2.465104, 2.590267, 2.733368, 2.900422, 3.101093, 3.352407, 3.688879, 4.199705, 5.298317)
iid_extremes <- xts(c(2.485179, 2.335036, 2.348495, 2.609534, 2.571431, 2.505418, 3.436557, 2.306443, 2.406384, 2.249789, 2.699147, 2.384733, 2.462003, 2.825771, 2.658806, 2.696008, 2.484846, 2.253926, 2.390248, 2.753181, 3.026417, 2.305134, 2.365227, 2.499355, 2.844757, 2.290955, 3.048307, 2.906714, 2.536778, 2.574244, 2.740777, 2.259236, 2.247154, 2.338442, 2.597085, 2.435009, 2.905871, 2.211387, 2.807309, 2.76335, 2.505434, 2.512933, 2.655662, 2.362114, 2.300621, 2.635403, 2.554976, 2.255644, 2.397592, 2.214528, 2.391973, 2.72426, 2.926669, 2.213251, 2.576326, 2.249265, 2.402427, 2.678113, 2.243226, 2.513481, 2.961663, 2.597378, 2.498836, 2.983134, 2.234179, 2.769461, 2.288106, 2.465069, 2.236144, 2.732343, 2.610239, 2.432895, 2.319882, 2.215825, 2.606343, 3.282729, 2.437939, 3.132531, 2.231877, 2.391024, 2.441499, 2.389186, 2.319438, 2.926213, 2.457484, 3.396832, 2.288507, 2.240025, 2.696893, 2.541371, 2.410939, 2.51565, 2.660812, 2.378425, 2.274061, 2.544918, 2.236294, 2.569099, 3.256515, 2.428861), order.by=as.Date(c(5569, 5626, 5702, 6047, 6052, 6216, 6405, 6532, 6570, 6641, 6734, 6873, 7061, 7223, 7239, 7328, 7474, 7484, 7621, 7644, 7972, 8048, 8103, 8202, 8617, 9049, 9175, 9244, 9456, 9482, 9527, 9618, 9631, 9735, 9776, 9794, 9944, 10071, 10114, 10347, 10378, 10424, 10450, 10834, 10990, 11052, 11103, 11235, 11410, 11421, 11473, 11515, 11557, 11571, 11620, 11845, 11913, 11919, 12116, 12118, 12430, 12454, 12628, 12653, 13266, 13278, 13430, 13458, 13614, 13635, 13747, 13780, 13907, 13958, 14202, 14322, 14355, 14698, 14806, 14866, 15100, 15104, 15156, 15247, 15350, 15496, 15966, 15967, 16001, 16016, 16128, 16140, 16226, 16261, 16279, 16484, 16637, 16651, 16770, 16797))
                    )
str(exp_quantiles)
str(iid_extremes)


# Partition plotting area into 3 pieces
par(mfrow = c(1, 3))

# Plot djx_extremes
plot(djx_extremes, type="h")

# Compute the spaces between the times of the extremes
djx_spaces <- diff(time(djx_extremes))

# Make a histogram of these spaces
hist(as.numeric(djx_spaces))

# Make a Q-Q plot of djx_spaces against exp_quantiles
qqplot(exp_quantiles, djx_spaces)

# Carry out the previous 4 steps for iid_extremes
plot(iid_extremes, type="h")
iid_spaces <- diff(time(iid_extremes))
hist(as.numeric(iid_spaces))
qqplot(exp_quantiles, iid_spaces)

par(mfrow = c(1, 1))


# When the function acf() is applied to a multivariate time series, we obtain a matrix of plots with the usual sample acf plots on the diagonal, and plots of the correlations between different series at different lags off the diagonal.

data(FTSE, package='qrmdata')
data(SMI, package='qrmdata')

dj_i <- diff(log(DJ))['2005/2015']
ftse_i <- diff(log(FTSE))['2005/2015']
smi_i <- diff(log(SMI))['2005/2015']

indexes <- merge(dj_i, ftse_i, smi_i, all = FALSE)
str(indexes)


# Make a time series plot of indexes with plot.zoo and a pairwise scatterplot with pairs
plot.zoo(indexes)
pairs(as.zoo(indexes))

# Calculate the sample correlation matrix of indexes
cor(indexes)

# Plot the sample acfs and cross-correlation functions for the returns in indexes
acf(indexes)

# Plots the sample acfs and cross-correlations functions for the absolute values of indexes
acf(abs(indexes))


data(CHF_USD, package="qrmdata")
fx <- diff(log(merge(GBP_USD, EUR_USD, USD_JPY, CHF_USD, all = TRUE)))['2011/2015']
str(fx)
fx_w <- apply.weekly(fx, FUN=colSums)
str(fx_w)

# Plot fx and fx_w
plot.zoo(fx, type="h")
plot.zoo(fx_w, type="h")

# Make acf plots of fx and the absolute values of fx
acf(fx)
acf(abs(fx))

# Apply the Ljung-Box test to the components of fx and their absolute values
apply(fx, 2, Box.test, lag = 10, type = "Ljung")
apply(abs(fx), 2, Box.test, lag = 10, type = "Ljung")

# Make acf plots of fx_w and the absolute values of fx_w
acf(fx_w)
acf(abs(fx_w))

# Apply the Ljung-Box test to the components of fx_w and their absolute values
apply(fx_w, 2, Box.test, lag = 10, type = "Ljung")
apply(abs(fx_w), 2, Box.test, lag = 10, type = "Ljung")


# Make acf plots of zcb_x and the absolute values of zcb_x
acf(zcb_x[-1, c("1.00y", "5.00y", "10.00y")])
acf(abs(zcb_x[-1, c("1.00y", "5.00y", "10.00y")]))

# Apply the Ljung-Box test to the components of zcb_x and their absolute values
apply(zcb_x[-1, c("1.00y", "5.00y", "10.00y")], 2, FUN=Box.test, lag=10, type="Ljung")
apply(abs(zcb_x[-1, c("1.00y", "5.00y", "10.00y")]), 2, FUN=Box.test, lag=10, type="Ljung")

# Make acf plots of zcbx_m and the absolute values of zcbx_m
acf(zcbx_m)
acf(abs(zcbx_m))

# Apply the Ljung-Box test to the components of zcbx_m and their absolute values
apply(zcbx_m, 2, FUN=Box.test, lag=10, type="Ljung")
apply(abs(zcbx_m), 2, FUN=Box.test, lag=10, type="Ljung")

```
  
  
  
***
  
Chapter 4 - Estimating Portfolio value-at-risk (VaR)  
  
Value-at-risk and expected shortfall:  
  
* Consider the distribution of losses over a fixed time period (such as two weeks) with no rebalancing  
	* The risk measure is just a statistic for the loss distribution - common to take a quantile (99.5% or 99% or 95%) for the tails  
    * The alpha-VaR is the alpha-quantile for the loss distribution (alpha is also known as the confidence level)  
    * The loss distribution is estimated based on historical data  
* Expected shortfall (ES) is becoming increasingly important in banking  
	* Tail VaR (TVaR), conditional VaR (cVaR), or expected shortfall (ES)  
    * alpha-ES is the expected loss given that the loss exceeds alpha-VaR  
  
International equity portfolio:  
  
* Example of UK investor with 30% FTSE, 40% S&P 500, 30% SMI (Swiss)  
	* 5 risk factors - FTSE, S&P 500, SMI, GBP/USD, GBP/CHF  
    * Can look at each of the individualized risk factors first, using the log-returns method  
* Historical simulation allows for estimating data - non-parametric approach involving resampling of past data  
	* Resample historical risk-factor returns and examine their impact on the current portfolio  
    * Loss operator shows effect of different risk-factor returns on the portfolio - for the specific portfolio and at the specific point in time  
    * VaR and ES can then be estimated based on the sample quantiles of the resampled data  
  
Option portfolio and Black-Scholes:  
  
* European options have a strike price K at a future time T - call is the right to buy at price K, put is the right to sell at price K  
	* Price depends on current price S, interest rate r, time to maturity T-t, and annualized volatility sigma  
    * Black-Scholes was designed to price options assuming normal distributions over returns  
    * Option is said to be "in the money" if it would be exercised at its current price  
* Estimating volatility for options pricing models is challenging - typically, an "implied volatility" is assumed  
	* The quoted option prices suggest how the market is pricing volatility at the current point in time  
    * The VIX index from the CBOE is one of the measures of applied volatility for options traded on the S&P 500  
    * The implied volatility is itself a very volatile measurement  
  
Historical simulation for the option example:  
  
* When an option is valued using Black-Scholes, changes in any of S, sigma, or r will change the price of the underlying option  
	* For this example, assume that r remains constant and that a single day has passed  
  
Wrap up:  
  
* Enhnacements include serial dependence for risk calculations (improve VaR and ES through volatility clustering) - filtered historical simulation with GARCH or EWMA  
* Improvements are available for both calculating VaR and ES - small sample sizes are especially risky, and can be improved using parametric-tailed methods (extreme value theory)  
  
Example code includes:  
```{r}

# The standard function qnorm() calculates quantiles of a normal distribution from the probability p, the mean, and standard deviation, and thus can be used to calculate value-at-risk (VaR)
# The function ESnorm() from the QRM package calculates the expected shortfall (ES) for a normal distribution from the probability p, location parameter mu, and scale parameter sd:
# qnorm(p, mean = 0, sd = 1)
# ESnorm(p, mu = 0, sd = 1)

par(mfrow=c(1, 1))
par(mfcol=c(1, 1))

mu <- mean(diff(log(DJ))['2008/2009'])
sigma <- sd(diff(log(DJ))['2008/2009'])

# Make a sequence of 100 x-values going from -4*sigma to 4*sigma
xvals <- seq(from = -4*sigma, to = 4*sigma, length.out = 100)

# Compute the density of a N(mu, sigma^2) distribution at xvals
ndens <- dnorm(xvals, mean = mu, sd = sigma)

# Plot ndens against xvals
plot(xvals, ndens, type="l")

# Compute the 99% VaR and 99% ES of a N(mu, sigma^2) distribution
VaR99 <- qnorm(0.99, mean=mu, sd=sigma)
ES99 <- QRM::ESnorm(0.99, mu=mu, sd=sigma)

# Draw vertical lines at VaR99 and ES99 in red and green
abline(v = VaR99, col = "red")
abline(v = ES99, col = "green")


data(SP500, package="qrmdata")
data(CHF_GBP, package="qrmdata")
data(USD_GBP, package="qrmdata")
ftse_r <- FTSE['2000/2012']
smi_r <- SMI['2000/2012']
sp500_r <- SP500['2000/2012']
usdgbp_r <- USD_GBP['2000/2012']
chfgbp_r <- CHF_GBP['2000/2012']
riskfactors <- merge(ftse_r, sp500_r, smi_r, usdgbp_r, chfgbp_r, all=FALSE)
str(riskfactors)


# Plot the risk-factor data
plot.zoo(riskfactors)

# Calculate the log-returns, assign to returns, and plot
returns <- diff(log(riskfactors))[-1, ]
plot.zoo(returns)

# Use apply() to carry out the Jarque-Bera test for all 5 series
apply(returns, 2, FUN=moments::jarque.test)

# Make a Q-Q plot against normal for the 5th return series and add a reference line
qqnorm(returns[, 5])
qqline(returns[, 5])

# Make a picture of the sample acfs for returns and their absolute values
acf(returns)
acf(abs(returns))


# For different vectors of log-returns for the 5 risk factors, the function lossop() computes the loss or gain incurred by the investor when her total wealth is 1
# The function can also be applied to a 5-dimensional time series of log-returns to obtain a time series of historically-simulated losses and gains corresponding to each vector of log-returns in the time series.
# The function lossop() is the so-called loss operator for the portfolio and has been specially written for this exercise
# In general, for each new portfolio, a specific function has to be written to compute portfolio losses and gains.

lossop <- function(xseries,wts=c(0.3,0.4,0.3)){
    if (is.xts(xseries)) x <- coredata(xseries)
    else if (is.matrix(xseries)) x <- xseries
    else x <- matrix(xseries,nrow=1)
    ll <- apply(x, 1, 
                function(x, wts) {
                    1-(wts[1]*exp(x[1]) + wts[2]*exp(x[2]+x[4]) + wts[3]*exp(x[3]+x[5]))
                    }, 
                wts=wts
                )
    if (is.xts(xseries)) ll <- xts(ll,time(xseries))
    ll
}


# Calculate the loss from a log-return of -0.1 for all risk factors
lossop(rep(-0.1, 5))

# Apply lossop() to returns and plot hslosses
hslosses <- lossop(returns)
plot(hslosses)

# Form a Q-Q plot of hslosses against normal
qqnorm(hslosses)

# Plot the sample acf of hslosses and their absolute values
acf(hslosses)
acf(abs(hslosses))


# Estimate the 99th sample percentile of the distribution of hslosses
quantile(hslosses, 0.99)

# Estimate the 99% ES
mean(hslosses[hslosses >= quantile(hslosses, 0.99)])

# Estimate the mean and standard deviation of hslosses
mu <- mean(hslosses)
sigma <- sd(hslosses)

# Compute the 99% quantile of a normal distribution
qnorm(p=0.99, mean=mu, sd=sigma)

# Compute the 99% ES of a normal distribution
QRM::ESnorm(p=0.99, mu=mu, sd=sigma)


# DO NOT HAVE package qrmtools
# Set the interest rate r to be 0.01, the volatility sigma to be 0.2 and the strike K to be 100
# r <- 0.01
# sigma <- 0.2
# K <- 100

# Look at the arguments of the Black_Scholes function
# args(qrmtools::Black_Scholes)

# Price a European call option that matures in one year if the current stock price is 80
# qrmtools::Black_Scholes(0, S=80, r, sigma, K, 1, "call")

# Price a European call option that matures in one year if the current stock price is 120
# qrmtools::Black_Scholes(0, S=120, r, sigma, K, 1, "call")

# Price a European put option that matures in one year if the current stock price is 80
# qrmtools::Black_Scholes(0, S=80, r, sigma, K, 1,"put")

# Price a European put option that matures in one year if the current stock price is 120
# qrmtools::Black_Scholes(0, S=120, r, sigma, K, 1,"put")


data(VIX, package="qrmdata")
sp500_r <- SP500['1990/2010']
vix_r <- VIX['1990/2010']
riskfactors <- merge(sp500_r, vix_r, all=FALSE)
str(riskfactors)
returns <- diff(log(riskfactors))[-1, ]
str(returns)


# Plot the risk factors and the log-returns
plot.zoo(riskfactors)
plot.zoo(returns)

# Make a scatterplot of the two return series
plot(as.matrix(returns))

# Apply the Jarque-Bera test to the returns and make a Q-Q plot of the volatility log-returns
apply(returns, 2, FUN=moments::jarque.test)
qqnorm(returns[, "X.VIX"])

# Create the sample acf of the returns and absolute returns
acf(returns)
acf(abs(returns))

# Calculate the correlation between the log-returns
cor(returns)


# DO NOT HAVE package qrmtools
# lossop <- function(xseries,r=0.01, K=100, T=1, sigma=0.2,S=100){
#   if (is.xts(xseries))
#     x <- coredata(xseries)
#   else if (is.matrix(xseries))
#     x <- xseries
#   else
#     x <- matrix(xseries,nrow=1)
#   ll <- apply(x,1,function(x,r,K,T,sigma,S){
#     deltat <- 1/250
#     V_t0 <- Black_Scholes(0, S, r, sigma, K, T, "call")
#     V_t1 = Black_Scholes(deltat, exp(log(S)+x[1]), r, exp(log(sigma)+x[2]), K, T, "call")
#     - (V_t1 - V_t0)/V_t0
#   },
#               r=r,K=K,T=T,sigma=sigma,S=S)
#   if (is.xts(xseries))
#     ll <- xts(ll,time(xseries))
#   ll
# }


# Calculate the first loss
# lossop(c(-0.1, -0.1), S=80, sigma=0.2)

# Calculate the second loss
# lossop(c(-0.1, 0.1), S=100, sigma=0.2)

# Create and plot hslosses
# hslosses <- lossop(returns, S=100, sigma=0.2)
# plot.zoo(hslosses)

# Form a Q-Q plot of hslosses against normal
# qqnorm(hslosses)

# Plot the sample acf of raw data and absolute values in hslosses
# acf(hslosses)
# acf(abs(hslosses))


# Estimate the 99.5% percentile of the distribution
# quantile(hslosses, 0.995)

# Estimate the 99.5% ES
# mean(hslosses[hslosses >= quantile(hslosses, 0.995)])

# Estimate the mean and standard deviation of hslosses
# mu <- mean(hslosses)
# sigma = sd(hslosses)

# Compute the 99.5% quantile of a normal distribution
# qnorm(p=0.995, mean=mu, sd=sigma)

# Compute the 99.5% ES of a normal distribution
# QRM::ESnorm(p=0.995, mu=mu, sd=sigma)

```
  
