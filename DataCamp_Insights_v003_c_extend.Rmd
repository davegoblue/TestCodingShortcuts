---
title: "Data Camp Insights"
author: "davegoblue"
date: "September 24, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)

```

## Background and Overview  
This document is a complement to DataCamp_Insights_v003_c.Rmd to be merged and integrated later.
  

***
  
###_Survey and Measure Development in R_  
  
Chapter 1 - Preparing to Analyze Survey Data  
  
Surveys in Marketing Research:  
  
* Surveys can consist of items rated on the Likert scale (commonly, but not necessarily, 1-5 scales)  
	* Step 1 - item generation (expert review, SME, etc.)  
    * Step 2 - questionnaire administration  
    * Setp 3 - initial item reduction  
    * Step 4 - confirmatory factor analysis  
    * Step 5 - Convergent/Discriminant Validity  
    * Step 6 - Replication  
* Example for inter-rater reliability  
	* library(irr)  
    * agree(experts)  
    * cohen.kappa(experts)  # 0-0.4 is poor, 0.4-0.6 is mediocre, 0.6-0.8 is substantial, 0.8+ is very strong  
* Example for content validity ratios  
	* Lawshe's Content Validity Ratio (CVR): what percent of experts judge that item essential to what's being measured  
    * CVR = [ E - (N/2) ] / (N/2)  where N is the total number of experts and E is the number who rated the item as essential  
    * psychometric::CVratio(NTOTAL=, NESSENTIAL=)  # -1 is consensus againt, +1 is consensus in favor  
  
Measurement, Validity, and Reliability:  
  
* Measurement is the process of observing and recording events  
	* Requires a measurement device and a calibration standard  
* Reliability can be assessed on three prongs  
	* Equivalence (inter-rater)  
    * Internal consistency (coefficient alpha, split-half)  
    * Stability (test-retest)  
* Validity checks whether measurments are as-claimed  
	* Content  
    * Construct (convergent, discriminant)  
    * Criterion (concurrent, predictive)  
* Exploratory Data Analysis can be considered as step 2.5 - between questionnaire administration and initial item reduction  
	* c_sat_likert <- c_sat %>% mutate_if(is.integer, as.factor) %>% likert()  
    * plot(c_sat_likert)  
* Can use EDA to check for items that are reverse coded (where 5 would be bad and 1 would be good)  
	* car::recode(myVar, "1=5; 2=4; 3=3; 4=2; 5=1")  
  
Describing Survey Results:  
  
* Generally, if there are less than 5% missing values and with equal distribution, then just omit them  
	* Hmisc::naclus(bad_survey))  
* Item correlations can be valuable - high correlations with each other, but not outside their own group  
	* corr.test(myDF)  
    * corrplot(cor(myDF), method="circle")  
  
Example code includes:  
```{r}

library(lavaan)


file001 <- readr::read_csv("./RInputFiles/brandrep-cleansurvey-extraitem.csv")
file002 <- readr::read_csv("./RInputFiles/brandquall11-recodedbutextraitem.csv")
file003 <- readr::read_csv("./RInputFiles/customersatisfactionclean.xls")
file004 <- readr::read_csv("./RInputFiles/brandloyalty.xls")


sme <- data.frame(Rater_A=c(1, 2, 3, 2, 1, 1, 1, 2, 3, 3, 2, 1, 1), 
                  Rater_B=c(1, 2, 2, 3, 3, 1, 1, 1, 2, 3, 3, 3, 1)
                  )


# Print beginning of sme data frame
head(sme)

# Correlation matrix of expert ratings
cor(sme)

# Percentage agreement of experts
irr::agree(sme)


# Check inter-rater reliability
psych::cohen.kappa(sme)

# While our Cohen's kappa and Pearson correlation happen to be similar in value, these are not measuring the same thing
# We are interested in agreement between the pairs of expert ratings on each item rather than a linear relationship between the item ratings in total
# In the next exercise, we'll look at content validity
# This is a measure of the assessment by a panel of experts (not just two, as in in Cohen's kappa) about the strength of an individual item


lawshe <- data.frame(item=rep(1:5, each=3), 
                     expert=rep(LETTERS[1:3], times=5), 
                     rating=factor(c("Essential", "Useful", "Not necessary", "Useful", "Not necessary", 
                                     "Useful",  "Not necessary", "Not necessary", "Essential", "Essential", 
                                     "Useful", "Essential", "Essential", "Essential", "Essential"
                                     )
                                   )
                     )


# Calculate the CVR for each unique item in the data frame
lawshe %>% 
    group_by(item) %>% 
    summarize(CVR = psychometric::CVratio(NTOTAL = length(unique(expert)), 
                                          NESSENTIAL = sum(rating == 'Essential')
                                          )
              )


brand_rep <- file001
glimpse(brand_rep)


# Convert items to factor
b_rep_likert <- brand_rep %>% 
    mutate(poor_workman_r=6-poor_workman_r) %>%
    mutate_if(is.double, as.factor) %>%
    as.data.frame()

# Response frequencies - base R
summary(b_rep_likert)

# Plot response frequencies
result <- likert::likert(b_rep_likert)
plot(result)


brand_qual <- file002 %>%
    mutate(tired=6-tired_r) %>%
    select(-innovator) %>%
    as.data.frame()
glimpse(brand_qual)

brand_qual_items <- c('trendy = This brand is trendy.', 'latest = This brand offers the latest products.', 'tired = This is a tired brand.', "happy_pay = I am happy paying what I do for this brand's products.", "reason_price = This brand's products are reasonably priced.", "good_deal = This brand's products are a good deal.", "strong_perform = This brand's products are strong performers.", 'leader = This brand is a leader in its field.', 'serious = This brand takes its product quality seriously.')
brand_qual_items


# Get response frequencies from psych
psych::response.frequencies(brand_qual)

# Print item descriptions
brand_qual_items

# Reverse code the "opposite" item
brand_qual$tired_r <- car::recode(brand_qual$tired, "1 = 5; 2 = 4; 4 = 2; 5 = 1")

# Check recoding frequencies
brand_qual %>% 
    select(tired, tired_r) %>%
    psych::response.frequencies() %>%
    round(2)


missing_lots <- file002 %>%
    mutate(tired=6-tired_r) %>%
    select(-tired_r)
set.seed(1908181013)
naRow <- sample(1:nrow(missing_lots), round(0.2*nrow(missing_lots)*ncol(missing_lots)), replace=TRUE)
naCol <- sample(1:ncol(missing_lots), round(0.2*nrow(missing_lots)*ncol(missing_lots)), replace=TRUE)
for (j in seq_along(naRow)) { missing_lots[naRow[j], naCol[j]] <- NA }
glimpse(missing_lots)
    

# Total number of rows
nrow(missing_lots)

# Total number of incomplete cases
sum(!complete.cases(missing_lots))

# Number of incomplete cases by variable
colSums(is.na(missing_lots))

# Hierarchical plot -- what values are missing together?
plot(Hmisc::naclus(missing_lots))


brand_qual_9 <- file002
glimpse(brand_qual_9)


# View significance of item correlations
psych::corr.test(brand_qual_9)

# Visualize item correlations -- corrplot
corrplot::corrplot(cor(brand_qual_9), method = "circle")


b_rep_items <- c("well_made: Crunchola's products are well-made.", 'consistent: Crunchola offers consistently high-quality products.', 'poor_workman: Crunchola suffers from poor workmanship in its products.', 'higher_price: I am willing to pay a higher price for Crunchola products.', 'lot_more: I am willing to pay a lot more for Crunchola products.', 'go_up: The price of Crunchola products would have to go up quite a bit before I would switch to another brand.', "stands_out: Crunchola's brand really stands out from its competitors.", "unique: Crunchola's brand is unique from other brands.", "one_of_a_kind: Crunchola's brand is truly one of a kind.")
b_rep_items


brand_rep_9 <- file001 %>%
    mutate(poor_workman = 6-poor_workman_r) %>%
    select(-poor_workman_r) %>%
    as.data.frame()
glimpse(brand_rep_9)


# Get response frequencies
psych::response.frequencies(brand_rep_9)

# Recode the appropriate item 
brand_rep_9$poor_workman_r <- car::recode(brand_rep_9$poor_workman, "1 = 5; 2 = 4; 4 = 2; 5 = 1")

# Adjust brandrep 9 dataset
brand_rep_9_new <- select(brand_rep_9, -poor_workman)

# Visualize item correlation
corrplot::corrplot(cor(brand_rep_9_new), method = "circle")

```
  
  
  
***
  
Chapter 2 - Exploratory Factor Analysis and Survey Development  
  
Latent Variables:  
  
* Latent variables are inferred from manifest variables - for example, brand loyalty  
* Parsimony is a general goal of survey development - how do the manifest variables help identify the latent variables  
	* psych::fa.parallel(myDF)  # scree of dataset against random dataset  
    * myEFA <- psych::fa(myDF, nfactors=3)  
    * myEFA$loadings  
    * psych::scree(myDF)  
  
EFA and Item Refinement:  
  
* Factor loadings are a valuable statistic - relationship between manifest variables and latent variables  
	* Ideally, only one latent variable per manifest variables  
    * 0 - 0.4 are poor  
    * 0.7+ are strong  
    * 0.4 - 0.7 are equivocal  
* What makes for a strong EFA?  
	* c_sat_11_EFA_3$e.values  # check that the number of eigenvalues >1 is the same as the number of factors (rule of thumb)  
    * Factor score correlations of 0.6 and under are "not too similar"  
    * myEFA$score.cor  
* If results are not favorable can either 1) drop poorly performing items, or 2) revist the number of factors  
  
Assessing Internal Reliability:  
  
* Internal consistency is a measure of survey reliability - consistency within itself  
* Split-half reliability checks whether all parts of the survey contribute equally  
	* psych::splitHalf(mySurvey)  # generally, 0.8+ indicates internal reliability  
* Coefficient (Cornbach) alpha measures the consistency of measures of the construct  
	* psych::alpha(mySurvey)  # std.alpha is considered a more reliable metric due to standardization  
    * Target is 0.8 - 0.9 with 0.7 - 0.8 also respectable and 0.65 - 0.7 minimally acceptable  
    * Values > 0.9 may suggest collinearity problems; drop items  
    * Values < 0.65 are undesirable and/or unacceptable; drop items as they may not be measuring the same construct  
  
Example code includes:  
```{r}

b_loyal_10 <- file004
glimpse(b_loyal_10)


# Print correlation matrix
psych::corr.test(b_loyal_10)

# Visualize b_loyal_10 correlation matrix
corrplot::corrplot(cor(b_loyal_10))

# Parallel analysis
psych::fa.parallel(b_loyal_10)


brand_rep_9 <- file001 %>%
    mutate(poor_workman = 6-poor_workman_r) %>%
    select(-poor_workman) %>%
    as.data.frame()
glimpse(brand_rep_9)


# Scree plot
psych::scree(brand_rep_9)

# Conduct three-factor EFA
brand_rep_9_EFA <- psych::fa(brand_rep_9, nfactors = 3)

# Print output of EFA
names(brand_rep_9_EFA)


# Summarize results of three-factor EFA
summary(brand_rep_9_EFA)

# Build and print loadings for a two-factor EFA
brand_rep_9_EFA_2 <- psych::fa(brand_rep_9, nfactors = 2)
brand_rep_9_EFA_2$loadings

# Build and print loadings for a four-factor EFA
brand_rep_9_EFA_4 <- psych::fa(brand_rep_9, nfactors = 4)
brand_rep_9_EFA_4$loadings


# (Factor loadings greater than 1, while rare, are not necessarily an issue.)

# Three factor EFA - brand_rep_9
brand_rep_9_EFA_3 <- psych::fa(brand_rep_9, nfactors = 3)

# Eigenvalues
brand_rep_9_EFA_3$e.values

# Factor score correlations
brand_rep_9_EFA_3$score.cor

# Factor loadings
brand_rep_9_EFA_3$loadings


# Create brand_rep_8 data frame
brand_rep_8 <- brand_rep_9 %>% select(-one_of_a_kind)

# Create three-factor EFA
brand_rep_8_EFA_3 <- psych::fa(brand_rep_8, nfactors=3)

# Factor loadings
brand_rep_8_EFA_3$loadings

# Factor correlations -- 9 versus 8 item model
brand_rep_8_EFA_3$score.cor
brand_rep_9_EFA_3$score.cor


# Three factor EFA loadings
brand_rep_8_EFA_3$loadings

# Two factor EFA & loadings
brand_rep_8_EFA_2 <- psych::fa(brand_rep_8, nfactors = 2)
brand_rep_8_EFA_2$loadings

# Four factor EFA & loadings
brand_rep_8_EFA_4 <- psych::fa(brand_rep_8, nfactors = 4)
brand_rep_8_EFA_4$loadings

# Scree plot of brand_rep_8
psych::scree(brand_rep_8)


# Standardized coefficient alpha
psych::alpha(brand_rep_9)$total$std.alpha

# 3-factor EFA
brand_rep_9_EFA_3 <- psych::fa(brand_rep_9, nfactors = 3)
brand_rep_9_EFA_3$loadings

# Standardized coefficient alpha - refined scale
psych::alpha(brand_rep_8)$total$std.alpha

# A survey with poorly-loading items can still be reliable – that's why we do EFA first
# Remember that a reliable survey in itself is not the goal of measurement – it is necessary but not sufficient


# Get names of survey items
names(brand_rep_8)

# Create new data frames for each of three dimensions
p_quality <- brand_rep_8 %>% select(1:3)
p_willingness <- brand_rep_8 %>% select(4:6)
# p_difference <- brand_rep_8 %>% select(7:8)

# Check the standardized alpha for each dimension
psych::alpha(p_quality)$total$std.alpha
psych::alpha(p_willingness)$total$std.alpha
# psych::alpha(p_difference)$total$std.alpha
psych::alpha(brand_rep_8)$total$std.alpha


# Get split-half reliability 
psych::splitHalf(brand_rep_8)

# Get averages of even and odd row scores
odd_scores <- rowMeans(brand_rep_8[c(TRUE, FALSE), ])
even_scores <- rowMeans(brand_rep_8[c(FALSE,TRUE), ])

# Correlate scores from even and odd items
cor(odd_scores[1:length(even_scores)], even_scores)


# 3 factor EFA
b_loyal_10_EFA_3 <- psych::fa(b_loyal_10, nfactors = 3)

# Factor loadings, eigenvalues and factor score correlations
b_loyal_10_EFA_3$loadings
b_loyal_10_EFA_3$e.values
b_loyal_10_EFA_3$score.cor

# 2 factor EFA
b_loyal_10_EFA_2 <- psych::fa(b_loyal_10, nfactors = 2)

# Factor loadings, eigenvalues and factor score correlations
b_loyal_10_EFA_2$loadings
b_loyal_10_EFA_2$e.values
b_loyal_10_EFA_2$score.cor

```
  
  
  
***
  
Chapter 3 - Confirmatory Factor Analysis and Construct Validation  
  
CFA and EFA:  
  
* Confirmatory Factor Analysis (CFA) is a means of construct validation  
	* Do the number of factors reflected in the data match theory and hypotheses  
* Can use the lavaan package for latent variable analysis  
	* la(tent) va(riable) an(alaysis)  
    * Use =~ to assign items to factors  
* Example for using lavaan on the 9-item survey  
	* bq_9_CFAModel <- "VAL =~ reason_price + happy_pay + good deal PERF =~ serious + leader +  strong_perform FUN =~ trendy + latest + tired_r"  
    * bq_9_CFA <- cfa(model = bq_9_CFAModel, data=bq_9)  
    * summary(bq_9_CFA, fit.measures=TRUE, standardized=TRUE)  
    * inspect(bq_9_CFA, "std")$lambda  
    * semPlot::semPaths(bq_9_CFA)  
  
CFA Assumptions and Interpretation:  
  
* Can test for multivariate normality - p-values for skewness and kurtosis  
	* psych::mardia(myData)  
* The default for lavaan is maximum likelihood, which assumes normality  
	* Can instead use MLR to mitigate non-normality  
    * bq_cfa <- cfa(model=myModel, data=myData, estimator="MLR")  
* Can look at fit measures and assess model performance  
	* CFI (comparative fit index) - should be 0.9+  
    * TLI (Tucker Lewis Index) - should be 0.9+  
    * Chi-squared - should be < 0.05 (though often will be for large sample sizes, even with a bad model)  
    * RMSEA - ideally less than 0.05  
* Can use the fit measures function to get 42 fit measures  
	* fitMeasures(myModel)  
    * fitMeasures(myModel, fit.measures=c("cfi", "tli"))  
* Can inspect estimates using standardizedSolution()  
	* standardizedSolution(myModel)  
  
Construct Validity:  
  
* Construct validity is the extent to which the actual measurements and the claims of what is being measured are congruent  
	* Validity is like being centered on a bullseye  
    * Reliability is based on being tightly clustered around the mean  
* If two dimensions are measuring the same things, then they should be combined in the interests of parsimony  
	* semTools::reliability(myModel)  
* Discriminant validity means that items should be distinct, but not unrelated  
	* avevar should be 0.5+  
    * CR (omega) should be 0.7+  
    * alpha (Cronbach) should be 0.7+  
  
Example code includes:  
```{r}

brand_rep_EFA <- brand_rep_8_EFA_3
brand_rep_8_model <- 'F1 =~ well_made + consistent + poor_workman_r
F2 =~ higher_price + lot_more + go_up
F3 =~ stands_out + unique'
brand_rep_CFA <- lavaan::cfa(model=brand_rep_8_model, data=brand_rep_8)


# Factor loadings -- EFA
brand_rep_EFA$loadings

# Factor loadings -- CFA
lavaan::inspect(brand_rep_CFA, what = "std")$lambda

# Plot diagram -- EFA
psych::fa.diagram(brand_rep_EFA)

# Plot diagram -- CFA
semPlot::semPaths(brand_rep_CFA)


# Rename items based on proposed dimensions
colnames(b_loyal_10) <- c("ID1", "ID2", "ID3", "PV1", "PV2", "PV3", "BT1", "BT2", "BT3", "BT4")

# Define the model
b_loyal_cfa_model <- 'ID =~ ID1 + ID2 + ID3
                    PV =~ PV1 + PV2 + PV3
                    BT =~ BT1 + BT2 + BT3 + BT4'
                        
# Fit the model to the data
b_loyal_cfa <- lavaan::cfa(model=b_loyal_cfa_model, data=b_loyal_10)

# Check the summary statistics -- include fit measures and standardized estimates
summary(b_loyal_cfa, fit.measures=TRUE, standardized=TRUE)


# Two dimensions: odd- versus even-numbered items
bad_model <- 'ODD =~ CS1 + CS3 + CS5 + CS7 + CS9
              EVEN =~ CS2 + CS4 + CS6 + CS8 + CS10'
                
# Fit the model to the data
c_sat_bad_CFA <- cfa(model=bad_model, data=file003)

# Summary measures
summary(c_sat_bad_CFA, fit.measures=TRUE, standardized=TRUE)


c_sat_model <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10'
c_sat_50 <- file003[1:50, ]


# Mardia's test for multivarite normality
psych::mardia(c_sat_50)

# Fit model to the data using robust standard errors
c_sat_cfa_mlr <- cfa(model=c_sat_model, data=c_sat_50, estimator="MLR")

# Summary including standardized estimates and fit measures
summary(c_sat_cfa_mlr, fit.measures=TRUE, standardized=TRUE)


c_sat_model_a <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10'
c_sat_model_b <- 'F1 =~ CS1 + CS3 + CS5 + CS7 + CS9
F2 =~ CS2 + CS4 + CS6 + CS8 + CS10'


# Fit the models to the data
c_sat_cfa_a <- cfa(model = c_sat_model_a, data = file003)
c_sat_cfa_b <- cfa(model = c_sat_model_b, data = file003)

# Print the model definitions
cat(c_sat_model_a)
cat(c_sat_model_b)

# Calculate the desired model fit statistics
fitMeasures(c_sat_cfa_a, fit.measures=c("cfi", "tli"))
fitMeasures(c_sat_cfa_b, fit.measures=c("cfi", "tli"))


c_sat <- file003
names(c_sat) <- c("CSU1", "CSU2", "CSU3", "CSU4", "EU1", "EU2", "EU3", "PS1", "PS2", "PS3")


# Add EU1 to the CSU factor
c_sat_model_a <- 'CSU =~ CSU1 + CSU2 + CSU3 + CSU4
                EU =~ EU1 + EU2 + EU3
                PS =~ PS1 + PS2 + PS3'

# View current c_sat model
cat(c_sat_model_a)

# Add EU1 to the CSU factor
c_sat_model_b <- 'CSU =~ CSU1 + CSU2 + CSU3 + CSU4 + EU1
                EU =~ EU1 + EU2 + EU3
                PS =~ PS1 + PS2 + PS3'

# Fit Models A and B to the data
c_sat_cfa_a <- cfa(model = c_sat_model_a, data = c_sat)
c_sat_cfa_b <- cfa(model = c_sat_model_b, data = c_sat)

# Compare the nested models
anova(c_sat_cfa_a, c_sat_cfa_b)


# Fit the model to the data 
# c_sat_cfa <- cfa(model = c_sat_model, data = c_sat_group, group = "COUNTRY")

# Summarize results -- include fit measures and standardized estimates
# summary(c_sat_cfa, fit.measures=TRUE, standardized=TRUE)

# Get average estimate for both groups
# standardized_solution <- standardizedSolution(c_sat_cfa)
# standardized_solution %>%
#   filter(op == "=~") %>%
#   group_by(group) %>% 
#   summarize(mean(est.std))


c_sat_cfa_model_3 <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10'
c_sat_cfa_model_2 <- 'F1 =~ CS1 + CS2 + CS3 + CS4 + CS5 + CS6 + CS7
F2 =~ CS8 + CS9 + CS10'

# Fit three-factor CFA
c_sat_cfa_3 <- cfa(model = c_sat_cfa_model_3, data = file003)

# Inspect key fit measures - three-factor CFA
fitMeasures(c_sat_cfa_3, fit.measures = c("cfi","tli","rmsea"))

# Fit two-factor CFA
c_sat_cfa_2 <- cfa(model = c_sat_cfa_model_2, data = file003)

# Inspect key fit measures - two-factor CFA
fitMeasures(c_sat_cfa_2, fit.measures = c("cfi","tli","rmsea"))

# Compare measures of construct validity for three- versus two-factor models
semTools::reliability(c_sat_cfa_3)
semTools::reliability(c_sat_cfa_2)


brand_rep_CFA_model <- 'F1 =~ well_made + consistent + poor_workman_r
F2 =~ higher_price + lot_more + go_up
F3 =~ stands_out + unique'
brand_rep_CFA <- lavaan::cfa(model=brand_rep_8_model, data=brand_rep_8)


# Print CFA model
cat(brand_rep_CFA_model)

# semTools reliability measures
semTools::reliability(brand_rep_CFA)

# psych standardized coefficient alpha measure
psych::alpha(brand_rep_9)$total$std.alpha


# Store F1 estimates as object loadings
loadings <- standardizedSolution(c_sat_cfa_3) %>%
    filter(op == "=~", lhs == "F1") %>% 
    select(est.std)

# Composite reliability
re <- 1 - loadings ^ 2
result <- sum(loadings) ^ 2 / ((sum(loadings)^ 2)  + sum(re))
result

# Average variance extracted
l2 <- loadings ^ 2
avg_var <- sum(l2) / nrow(loadings)
avg_var

# Compare versus semTools
semTools::reliability(c_sat_cfa_3)


# Print brand_rep_factors
# brand_rep_factors

# Build model for lavaan
brand_rep_8_cfa_model <- "QUAL =~ consistent + well_made + poor_workman_r
PRICE =~ go_up + lot_more + higher_price
UNIQUE =~ stands_out + unique"

# Summarize results with fit measures and standardized estimates
# summary(brand_rep_8_CFA, standardized = TRUE, fit.measures = TRUE)

# Construct validity
# semTools::reliability(brand_rep_8_CFA)

```
  
  
  
***
  
Chapter 4 - Criterion Validity and Replication  
  
Concurrent Validity and Model Diagrams:  
  
* Criterion validity is a measure of relationship between the construct and external variable of interest  
* Variables are not always on the 1-5 scale, and differences in units can negatively impact model validity  
	* describe(myData)  # check means and standard deviations (ideally, everything is N(0, 1)  
* Can latentize a variable by adding it with =~  
	* myModel <- '… age_fact =~ age'  # latentizes age to age_fact  
* Can correlate manifest and latent variable with ~~  
	* myModel <- '… age_fact =~ age\n age_fact ~~ F1 + F2 + F3'  # latentizes age to age_fact and gets latent statistics (F1, F2, F3 already defined in the model)  
    * mySEM <- sem(myModel, data=myData, estimator="MLR")  
    * summary(mySEM, fit.measures=TRUE, standardized=TRUE)  
* Diagrams are sometimes called "spaghetti and meatballs", representing that they can be busy diagrams  
  
Predictive Validity and Factor Scores:  
  
* Predictive validity assesses the degree to which models predict future outcomes  
* Linear regression can be used for this task  
	* Begin by binding and scaling all of the relevant variables  
* Can run regression in lavaan using ~  
	* c_sat_model = "… spend ~ F1 + F2 + F3"  # assumes F1, F2, F3 each defined as usual using =~  
    * semPaths(c_sat_sem, rotation=2)  
    * standardizedSolution(c_sat_sem) %>% filter(op == "~") %>% mutate_if(is.numeric, round, digits=3)  
    * inspect(c_sat_rem, "r2")  # pull the R-squared  
* Factors scores are numerical scores reflecting relative standings on the latent factor  
	* csat_cfa <- cfa(model = csat_model, data = c_sat)  
    * csat_scores <- as.data.frame(predict(csat_cfa))  
    * describe(csat_scores)  
    * multi.hist(csat_scores)  
  
Repeated Measures, Replication, and Factor Scores:  
  
* Stability is a third form of reliability measurement  
	* Does an instrument get similar responses if measuring the same population near the same time?  
    * "Test-retest reliability"  
    * survey_test_retest <- testRetest(t1 = survey_t_1, t2 = survey_t_2, id = "id")  
    * Generally, scores of 0.9+ are very good and scores of 0.7- are unreliable  
* Replication is a different step that can be taken in the event that it is not possible to get people to retake the survey  
	* Split the data by rows - odd vs. even  
  
Wrap Up:  
  
* Six step process for building and testing models  
  
Example code includes:  
```{r}

spendData <- c(94.5, 715, 145.5, 772.5, 133.5, 350, 75.5, 304.5, 117, 81, 234.5, 102, 152.5, 295, 145, 222, 121.5, 142, 82.5, 144, 130, 141, 545, 142.5, 175, 154, 130, 148.5, 255, 139.5, 420, 373.5, 197.5, 487.5, 337.5, 133.5, 114, 84, 255.5, 129, 114, 275, 297, 84, 87, 109.5, 123, 405, 123, 158, 145, 139.5, 112.5, 458, 138, 91.5, 190, 257.5, 155, 259, 120, 84, 84, 755, 84, 412, 270, 134, 285, 227.5, 133.5, 123, 127.5, 825, 418, 103.5, 144, 124, 120, 445.5, 150.5, 75, 129, 312, 330, 182.5, 282, 91.5, 218, 245, 157.5, 118.5, 148.5, 505, 87, 182, 111, 294, 110, 325.5, 115.5, 312, 120, 510, 91.5, 139.5, 85.5, 189, 152, 141, 138, 387, 114, 84, 213, 120, 115.5, 231, 78, 85.5, 354, 142.5, 128, 212, 547.5, 145, 103.5, 294, 354, 182.5, 185, 212, 97.5, 103.5, 235, 395, 105.5, 283.5, 155, 91.5, 94.5, 297.5, 283.5, 125, 159, 139.5, 95, 198, 104, 138, 155, 200, 97.5, 224, 588, 108, 100.5, 183, 350, 153, 150, 155, 91.5, 138, 117, 135.5, 138, 202, 257, 103.5, 114, 282, 112, 198, 159, 420, 315, 402, 507, 259.5, 81, 127.5, 144, 225, 141, 84, 150, 150.5, 455, 75, 294, 102, 199.5, 385, 155, 144, 135, 142.5, 172, 390, 94.5, 153, 472.5, 105, 123, 188, 325, 504, 99, 111, 151.5, 78, 545, 170, 123, 93, 381.5, 735, 100.5, 97.5, 155, 252.5, 192, 132, 252.5, 121.5, 90, 257, 151.5, 94.5, 153.5, 311.5, 79.5, 284, 151.5, 95, 78, 480, 102, 215, 115.5, 330, 592.5, 79.5, 355.5, 195, 105.5, 142.5, 154, 155, 312, 321, 75.5, 185, 324, 155, 530, 127.5, 148.5, 152, 111, 157.5, 151.5, 772.5, 123, 115.5, 145.5, 84, 515, 82.5, 108, 130.5, 138, 279, 151.5, 207, 150, 109.5, 150, 153.5, 152.5, 150.5, 88.5, 185, 115.5, 123, 150, 114, 321, 144, 142.5, 152, 82.5, 187.5, 97.5, 145, 257, 435, 250, 310.5, 78, 105.5, 102, 138, 303, 285, 155, 124.5, 240, 204, 118.5, 241.5, 147, 118.5, 105, 591.5, 180, 93, 252, 103.5, 287, 575, 75, 238, 189, 204, 210, 153, 145.5, 117, 559, 153.5, 79.5, 222.5, 145.5, 88.5, 159, 155, 255, 127.5, 300, 154, 213, 135, 84, 151.5, 127.5, 99, 200, 135, 522.5, 297, 152, 127.5, 203, 103.5, 178.5, 130.5, 255, 100.5, 213, 185, 228, 115.5, 109.5, 75.5, 273, 511, 414, 152, 217, 150, 102, 537.5, 282, 440, 288, 172.5, 112.5, 577.5, 140, 291.5, 152, 582.5, 210, 318.5, 185, 145.5, 148.5, 324, 145, 105.5, 132, 85.5, 135, 152, 135, 324, 200, 155, 247.5, 197.5, 95, 304.5, 215, 577.5, 111, 495, 141, 139.5, 112.5, 110, 135.5, 97.5, 157.5, 243, 159, 155, 185, 155, 114, 395, 130.5, 238, 345.5, 597.5, 210, 220, 210, 222.5, 124.5, 158, 150, 490.5, 270, 88.5, 205, 135, 90, 152, 309, 153, 105, 111, 78, 123, 159, 95, 115, 435, 235, 292.5, 155, 304.5, 114, 104, 135, 397.5, 93, 257, 102, 204, 252, 152, 215, 108, 148.5, 79.5, 155, 114, 94.5, 118.5, 178.5, 111, 150.5, 195, 85.5, 84, 93, 575, 148.5, 757.5, 155, 87, 112.5, 88.5, 255, 358, 84, 405, 153, 127.5, 81, 135.5, 154.5, 247.5, 182, 79.5, 373.5, 95, 147, 145.5, 152.5, 294, 259.5, 354, 103.5, 187.5, 124.5, 218, 227.5, 481.5, 125, 123, 450, 129, 318, 170, 319.5, 91.5, 183, 154, 391.5, 458, 303, 114, 111, 112.5, 222, 742.5, 234, 120, 81, 312, 335, 135, 133.5, 118.5, 390, 518, 215, 373.5, 118.5, 195, 111, 205, 94.5, 123, 99, 75.5, 102, 244, 380, 357.5, 254, 227.5, 198, 192.5, 151.5)
brand_rep_spend <- tibble::tibble(spend=spendData)
brand_rep <- file001 %>%
    mutate(poorworkman = 6-poor_workman_r) %>%
    select(-poorworkman)
brand_rep
brand_rep_spend


# Check if brand_rep and brand_rep_spend have the same number of rows
same_rows <- nrow(brand_rep) == nrow(brand_rep_spend)
same_rows

# Append spend column to brand_rep
brand_rep <- cbind(brand_rep, brand_rep_spend)

# Scale the data
brand_rep_scaled <- scale(brand_rep)

# Get summary statistics of scaled dataframe
psych::describe(brand_rep)
psych::describe(brand_rep_scaled)


# Correlate F1, F2 and F3 to spend_f, the 'latentized' spend
brand_rep_model <- 'F1 =~ well_made + consistent + poor_workman_r
F2 =~ higher_price + lot_more + go_up
F3 =~ stands_out + unique
spend_f =~ spend
spend_f ~~ F1 + F2 + F3'

# Fit the model to the data -- sem()
brand_rep_cv <- lavaan::sem(data = brand_rep_scaled, model = brand_rep_model)

# Print the standardized covariances b/w spend_f and other factors
lavaan::standardizedSolution(brand_rep_cv) %>% 
    filter(rhs == "spend_f")

# Plot the model with standardized estimate labels
semPlot::semPaths(brand_rep_cv, whatLabels = "est.std", edge.label.cex = .8)


c_sat <- file003
c_sat_recommend <- tibble::tibble(Rec_1=c(4, 3, 3, 3, 4, 3, 2, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 4, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3, 3, 3, 4, 3, 4, 4, 3, 2, 3, 3, 3, 4, 4, 3, 4, 4, 5, 3, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 2, 3, 3, 4, 4, 3, 4, 3, 3, 4, 3, 3, 3, 1, 3, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 2, 4, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 4, 2, 4, 3, 4, 3, 3, 4, 3, 2, 3, 2, 3, 3, 4, 3, 4, 2, 3, 3, 2, 4, 3, 4, 2, 3, 4, 3, 3, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 4, 4, 3, 2, 3, 3, 4, 2, 5, 3, 4, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 3, 3, 3, 4, 2, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 2, 4, 4, 3, 3, 3, 4, 3, 3, 4, 4, 3, 3, 5, 4, 4, 3, 4, 2, 4, 4, 4, 4, 3, 4, 3, 3, 3, 2, 3, 3, 3, 4, 3, 3, 4, 5, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 1, 4, 3, 4, 3, 3, 3, 4, 3, 5, 4, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 2, 3, 4, 3, 4, 3, 4, 4, 4, 3, 3, 3, 5, 3, 3, 4, 3, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3, 4, 2, 4, 3, 2, 3, 3, 5, 4, 2, 5, 3, 5, 3, 2, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 5, 4, 4, 3, 3, 5, 3, 4, 3, 5, 4, 3, 2, 2))
c_sat
c_sat_recommend


# Bind & scale the variables
c_sat_rec_scale <- c_sat %>% 
    bind_cols(c_sat_recommend) %>% scale()

# Define the model - Rec_f covaries with F1, F2, F3
c_sat_rec_model <- 'F1 =~ CS1 + CS2 + CS3 + CS4
F2 =~ CS5 + CS6 + CS7
F3 =~ CS8 + CS9 + CS10
Rec_f =~ Rec_1
Rec_f ~~ F1 + F2 + F3'

# Fit the model to the data 
c_sat_rec_sem <- lavaan::sem(model = c_sat_rec_model, data = c_sat_rec_scale)

# Look up standardized covariances
lavaan::standardizedSolution(c_sat_rec_sem) %>% 
    filter(rhs == "Rec_f")


# Define the model
b_q_model <- 'HIP =~ trendy + latest + tired_r
            VALUE =~ happy_pay + reason_price + good_deal
            PERFORM =~ strong_perform + leader + serious
            spend ~ HIP + VALUE + PERFORM'

# Fit the model to the data
# b_q_pv <- lavaan::sem(data = b_q_scale, model = b_q_model)

# Check fit, r-square, standardized estimates
# summary(b_q_pv, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)

# Plot the model -- rotate from left to right
# semPlot::semPaths(b_q_pv, rotation = 2, whatLabels = "est.std", edge.label.cex = 0.8)


# Plot the new model
# semPlot::semPaths(brand_rep_sem, rotation = 2)

# Get the coefficient information
# lavaan::standardizedSolution(brand_rep_sem) %>% filter(op == "~")

# Get the r-squared
# r_squared <- car::inspect(brand_rep_sem, "r2")["F2"]
# r_squared


# Compute factor scores in lavaan -- store as data frame
brand_rep_scores <- as.data.frame(predict(brand_rep_CFA))

# Summary statistics of our factor scores
psych::describe(brand_rep_scores)

# Plot histograms for each variable
psych::multi.hist(brand_rep_scores)

# Are they normally distributed? Check using map()
map(brand_rep_scores, shapiro.test)


# Linear regression of standardized spending and factor scores
# bq_fs_reg <- lm(spend ~ F1 + F2 + F3, data = bq_fs_spend)

# Summarize results, round estimates
# rounded_summary <- round(coef(bq_fs_reg), 3)
# rounded_summary

# Summarize the results of CFA model
# summary(brand_qual_pv)

# Compare the r-squared of each
# inspect_rsq <- car::inspect(brand_qual_pv, "r2")["spend"]
# inspect_rsq
# summary(bq_fs_reg)$r.squared


# Descriptive statistics grouped by 'time'
# psych::describeBy(brand_rep_t1_t2, "time")

# Test retest: time == 1 versus time == 2 by id = "id"
# brand_rep_test_retest <- psych::testRetest(t1 = filter(brand_rep_t1_t2, time == 1), t2 = filter(brand_rep_t1_t2, time == 2), id = "id")

# brand_rep_test_retest$r12


brand_rep <- file001 %>%
    select(-one_of_a_kind)
brand_rep


# Split data into odd and even halves
brand_rep_efa_data <- as.data.frame(brand_rep)[c(TRUE,FALSE),]
brand_rep_cfa_data <- as.data.frame(brand_rep)[c(FALSE,TRUE),]

# Get factor loadings of brand_rep_efa_data EFA
efa <- psych::fa(brand_rep_efa_data, nfactors = 3)
efa$loadings

# Confirm the data that the model was fit to
# car::inspect(brand_rep_cfa, what = "call")

# Check fit measures
# fitmeasures(brand_rep_cfa)[c("cfi", "tli", "rmsea")]

```
  
  
  
***
  
###_R for SAS Users_  
  
Chapter 1 - Getting Started with R  
  
Get Help and Load Data in R:  
  
* R Functionality includes packages and global environment  
	* ls() is similar to PROC DATASETS  
    * load() is similar to LIBNAME or DATA + SET  
* Generally, an R session begins without any objects loaded in memory  
* Abalone dataset is available from UC Irvine  
	* load("abalone.Rdata")  
    * library(myPkg) will load myPkg  
    * sessionInfo() shows the currently loaded packages  
  
Dataset Contents and Descriptive Statistics:  
  
* Can use readr::read_csv() to load a CSV file in to the environment  
	* myObject <- readr::read_csv("myFile.csv")  
    * str(myObject)  # look at the variables and dimensions of the dataset  
    * dim(myObject)  # dimensions of the object  
    * names(myObject)  # variable names  
    * head(myObject)  # first 6 rows  
    * tail(myObject)  # last 6 rows  
* The dplyr functions are helpful for manipulating data frames  
	* arrange(myDF, myCol)  # sort by ascending myCol  
    * myDF %>% arrange(myCol)  # sort by ascending myCol (same as above)  
    * myDF %>% pull(myCol)  # extract myCol as a vector  
    * myDF %>% pull(myCol) %>% mean() # extract myCol as a vector and take the mean()  
    * myDF %>% select(a, b) %>% summary()  # keep only columns "a" and "b", then create a summary of that frame  
  
Graphical Visualizations:  
  
* The ggplot2 package is useful for plotting data in R  
	* "grammar of graphics" - layering approach to graphics  
    * Build a base layer and then add to it  
    * ggplot(data=myDF, aes(x=x, y=y)) + geom_boxplot() + theme_bw()  # build a boxplot of y by x using the bw theme  
    * Can include options like color= or fill=  
    * Can customize axes using xlab(), ylab(), and ggtitle()  
  
Example code includes:  
```{r}

# List the objects in global environment
# ls()

# Load the "abalone.RData" dataset
# load("abalone.RData")

# List objects in global environment again 
# ls()

# Learn more about the load function
# help(load)

# Learn more about the ls function
# help(ls)


# Run sessionInfo() see packages available to this session
# sessionInfo()

# Load Hmisc package
# library(Hmisc)

# Run sessionInfo() again to see updated package list
# sessionInfo()


# Load abalone.csv dataset assign output to abalone
# abalone <- readr::read_csv("abalone.csv")

data(abalone, package = "AppliedPredictiveModeling")
abalone <- tibble::as_tibble(abalone)

# Get the dimensions of the abalone dataset object
dim(abalone)

# Get variable names in the abalone dataset object
names(abalone)

chgName <- function(x) {
    paste0(stringr::str_to_lower(stringr::str_sub(x, 1, 1)), stringr::str_sub(x, 2))
}

tmpNames <- sapply(names(abalone), FUN=chgName, USE.NAMES=FALSE)
names(abalone) <- tmpNames
names(abalone)


# View top 3 rows of abalone dataset using head()
head(abalone, 3)

# View bottom 3 rows of abalone dataset using tail()
tail(abalone, 3)

# Run arrange function from dplyr to sort the data by rings
arrange(abalone, rings)

# Rewrite the line of code above using the %>% notation
abalone %>% 
    arrange(rings)


# Find mean length of abalones using pull() and mean() 
abalone %>% 
    pull(longestShell) %>% 
    mean()

# Find the median wholeWeight of the abalones
abalone %>% 
    pull(wholeWeight) %>% 
    median()

# Get descriptive statistics of diameter and shellWeight
abalone %>% 
    select(diameter, shellWeight) %>% 
    summary()


# Add a title and labels for the axes
ggplot(abalone, aes(shellWeight)) + 
    geom_histogram(color = "blue", fill = "yellow") +
    xlab("Shell Weight") + 
    ylab("Frequency Counts") + 
    ggtitle("Shell Weights Histogram")


# Change the boxplots to the violin geom
ggplot(data = abalone, aes(x=type, y=shuckedWeight)) + 
    geom_violin() + 
    theme_bw()


# Create panel plot of scatterplot for sex categories
ggplot(abalone, aes(diameter, wholeWeight)) + 
    geom_point() + 
    geom_smooth() + 
    facet_wrap(~type)

```
  
  
  
***
  
Chapter 2 - Data Wrangling  
  
Objects - Building Blocks of R:  
  
* Objects are the buidling blocks of R - "everything in R is an object"  
	* The assignment operator is <-  
    * The c() operator is the combine function, which can be used to create a vector  
* Can create matrices using matrix() and data frames using data.frame()  
* Can get the class of an objects using class() or see it as one of the outputs of str()  
  
Selecting Elements from Objects:  
  
* Can select elements from a matrix using myMtx[myRow, myCol] - if myRow is blank it means "all rows" and if myCol is blank it means "all columns"  
	* Same approach for data frames, though names for columns can be used rather than numbers  
* Can select variables using a:b, which will pull all variables from a through b  
* Can grab rows using dplyr::slice()  
	* slice(2:3) will pull rows 2 and 3  
  
Manipulating Datasets and Data Objects:  
  
* Can use the mutate() function to create new variables, similar to data a; set a; myVar=myOldVar/100; in SAS  
* Can use the ifelse() function to create new variables based on a boolean condition  
* Can use is.numeric(), is.vector(), is.character(), is.data.frame(), is.matrix()  
* Can coerce object types to new types using as.xxx()  
	* as.matrix()  
    * as.numeric()  
    * as.integer()  
  
Data Quality and Cleaning:  
  
* Can use the summary() function to check for minima, maxima, mean, median, etc.  
* The geom_dotplot() can be helpful in identifying outliers  
* Can filter out cases with errors, then resume the analysis  
  
Example code includes:  
```{r}

# Assign the value of 1 to an object x
x <- 1

# Create object w by adding the value 4 to x
w <- x + 4

# Create vector object y 
y <- c(5, 2, 3)

# Create a vector z 
z <- c("red", "green", "blue")

# Create a logical object v 
v <- c(TRUE, TRUE, FALSE)


numvec1 <- 1:3
numvec2 <- c(5, 2, 3)
charvec <- c("red", "green", "blue")
logicvec <- c(TRUE, TRUE, FALSE)

# Create matrix with numvec1 and numvec2 with 2 columns, 3 rows
nummtx <- matrix(c(numvec1, numvec2), ncol=2, nrow=3)

# Create dataframe from numvec1, numvec2, charvec and logicvec
dataframe <- data.frame(numvec1, numvec2, charvec, logicvec)

# Get the structure of dataframe
str(dataframe)


# Select the 2nd element in vector charvec
charvec[2]

# Select the 2nd column of matrix nummtx
nummtx[, 2]

# Pull column numvec1 from dataframe assign it to id
id <- dataframe %>% 
    pull(numvec1)

# Pull column charvec from dataframe and determine the class of charvec
dataframe %>% 
    pull(charvec) %>% 
    class()


# Create abaloneMod from abalone, add new variable age
abaloneMod <- abalone %>%
    mutate(age = rings + 1.5)

# Add three more variables to abaloneMod
abaloneMod <- abaloneMod %>%
    mutate(pctShucked = shuckedWeight * 100 / wholeWeight) %>%
    mutate(pctViscera = visceraWeight * 100 / wholeWeight) %>%
    mutate(pctShell = shellWeight * 100 / wholeWeight)

# Select age, pctShucked, pctViscera, pctShell, run summary()
abaloneMod %>%
    select(age, pctShucked, pctViscera, pctShell) %>%
    summary()


# Add new character variable agecat to abaloneMod
abaloneMod <- abaloneMod %>%
    mutate(agecat = factor(ifelse(test = age < 10.5, "< 10.5", "10.5 and older")))

# Add new logical variable adult to abaloneMod
abaloneMod <- abaloneMod %>%
    mutate(adult = (type != "I"))

# Select age, agecat, sex, adult and view top 6 rows
abaloneMod %>%
    select(age, agecat, type, adult) %>%
    head(6) 


# Convert adult to numeric and confirm class type
abaloneMod %>% 
    mutate(adult = as.numeric(adult)) %>% 
    pull(adult) %>% 
    class()

# Select three dimensions, save as abalonedim
abalonedim <- abaloneMod %>%
    select(longestShell, diameter, height)

# Convert abalonedim to a matrix and confirm class type
abalonedim %>% 
    as.matrix() %>% 
    class()


# Pull height from abaloneMod and run summary()
abaloneMod %>% 
    pull(height) %>%
    summary()

# Keep cases with height > 0 assign to abaloneKeep
abaloneKeep <- abaloneMod %>%
    filter(height > 0)

# Make histogram of updated heights in abaloneKeep
ggplot(abaloneKeep, aes(x=height)) +
    geom_histogram()


# Sort abaloneKeep by pctShucked, view largest 6 pctShucked
abaloneKeep %>%
    arrange(pctShucked) %>%
    pull(pctShucked) %>%
    tail(6)

# Scatterplot of shuckedWeight by wholeWeight add y=x line
ggplot(abaloneKeep, aes(x=wholeWeight, y=shuckedWeight)) +
    geom_point() +
    geom_abline(intercept=0, slope=1)

# Keep cases where shuckedWeight is less than wholeWeight
abaloneKeep <- abaloneKeep %>%
    filter(shuckedWeight < wholeWeight)


# Make scatterplot of height by length add y=x line
ggplot(abaloneKeep, aes(x=longestShell, y=height)) +
    geom_point() + 
    geom_abline(intercept=0, slope=1)

# Make scatterplot of diameter by length add y=x line
ggplot(abaloneKeep, aes(x=longestShell, y=diameter)) +
    geom_point() + 
    geom_abline(intercept=0, slope=1)

# Keep abalones with length > both height and diameter
abaloneKeep <- abaloneKeep %>%
    filter((longestShell > height) & (longestShell > diameter))


# Dimensions of final dataset with samples in abaloneKeep
dim(abaloneKeep)

# Get summary statistics of all variables in abaloneKeep
summary(abaloneKeep)

# Scatterplot of shuckedWeight by wholeWeight add y=x line
ggplot(abaloneKeep, aes(x=wholeWeight, y=shuckedWeight)) +
    geom_point() +
    geom_abline(intercept=0, slope=1)

# Scatterplot of length by diameter add y=x line
ggplot(abaloneKeep, aes(x=diameter, y=longestShell)) +
    geom_point() +
    geom_abline(intercept=0, slope=1)

```
  
  
  
***
  
Chapter 3 - Data Exploration  
  
Exploratory Data Analysis:  
  
* Can get summary statistics in R using several different packages and functions  
	* summary()  
    * Hmisc::describe()  
    * psych::describe()  
* Can use the dplyr::summarize() to create new names for statistics  
* Can use the dplyr::summarize_all() to get key statistic  
	* myDF %>% select(a, b) %>% summarize_all(funs(mean, sd))  
* Can also use the dplyr::group_by() to get the output summarized by values of a key variable  
  
Correlations and T-tests:  
  
* Can assess the strenght of correlations using psych::corr.test()  
* Can use Ggally::ggpairs() to get the pairs plot, and density plots for all of the key variables  
	* Can add aes(color=a) to have the charts colored by levels of variable a  
* Can add the counts in several ways, including use of an extra group_by statement  
	* myDF %>% select(a, b) %>% group_by(a) %>% group_by(N = n(), add=TRUE) %>% summarize_all(funs(mean, sd))  
* Can perform F tests for equal variances, and t-tests for equal means  
	* var.test(a ~ b, data = myDF)  
    * t.test(a ~ b, data = myDF)  # unpooled t-test is the default  
    * t.test(a ~ b, data = myDF, var.equal = TRUE)  # pooled t-test to over-ride defaults  
  
Categorical Data: Analyze and Visualize:  
  
* Can use the table() command inside the with() command  
	* myDF %>% with(table(a))  
* Can create contingency tables using either table() or gmodels::CrossTable()  
	* gmodels::CrossTable(chisq=TRUE)  # will run a Chi-squared test on the table resulting from the gmodels call  
    * Can also include prop.r=, prop.t=, prop.chisq=, expected= options inside gmodels::CrossTable()  
* Can get the mosaicplot using mosaicplot()  
	* mosaicplot(a ~ b, data=myDF, color=c("red", "blue"), main="myTitle")  
  
Example code includes:  
```{r eval=FALSE}

# Run describe() from Hmisc for sex, length, diameter, height
abaloneKeep %>% 
    select(type, longestShell, diameter, height) %>% 
    Hmisc::describe()


# Run describe() from psych for length, diameter, height
abaloneKeep %>% 
    select(longestShell, diameter, height) %>% 
    psych::describe()


# Run summary() for shuckedWeight and wholeWeight
abaloneKeep %>%
    select(shuckedWeight, wholeWeight) %>%
    summary()

# Get mean and sd for length
abaloneKeep %>%
    summarize(mean_length = mean(longestShell), sd_length = sd(longestShell))

# Get mean and sd for height and diameter
abaloneKeep %>%
    select(height, diameter) %>%
    summarise_all(list(~mean(.), ~sd(.)))


# Get min, mean, sd, median, and max for age by sex
abaloneKeep %>% 
    group_by(type) %>% 
    select(type, age) %>%
    summarise_all(list(~min(.), ~mean(.), ~sd(.), ~median(.), ~max(.)))

# Get median, 25th, 75th percentiles for wholeWeight by adult
abaloneKeep %>% 
    group_by(adult) %>%
    select(adult, wholeWeight) %>%
    summarise(median_wweight = median(wholeWeight), 
              q1_wweight = quantile(wholeWeight, probs = 0.25), 
              q3_wweight = quantile(wholeWeight, probs = 0.75)
              )


# Get correlations age and weights with corr.test()
abaloneKeep %>% 
    select(age, wholeWeight, shuckedWeight, shellWeight, visceraWeight) %>%
    psych::corr.test()

# Get correlations age and dimensions with corr.test()
abaloneKeep %>% 
    select(age, longestShell, diameter, height) %>%
    psych::corr.test()


# Make ggpairs plot of age, wholeWeight, shellWeight
abaloneKeep %>% 
    select(age, wholeWeight, shellWeight) %>%
    GGally::ggpairs()

# Make ggpairs plot of age, wholeWeight, shellWeight
abaloneKeep %>% 
    select(type, age, wholeWeight, shellWeight) %>%
    GGally::ggpairs(aes(color=type))


# Get n, mean, sd for length by adult groups
abaloneKeep %>% 
    select(longestShell, adult) %>%
    group_by(adult) %>%
    group_by(N = n(), add = TRUE) %>%
    summarise_all(list(~mean(.), ~sd(.)))

# Run equal variance and pooled t-tests of length by adult 
var.test(longestShell ~ adult, data = abaloneKeep)
t.test(longestShell ~ adult, data = abaloneKeep, var.equal = TRUE)

# Make boxplots of length by adult
ggplot(abaloneKeep, aes(x=adult, y=longestShell)) +
    geom_boxplot()


# Create frequency table of sex by agecat
tablesexage <- abaloneKeep %>% 
    with(table(type, agecat))

# Run chi-square test using tablesexage
chisq.test(tablesexage)
  
# Use CrossTabs, run chisq test
abaloneKeep %>%
    with(gmodels::CrossTable(type, agecat, chisq=TRUE, prop.c = FALSE, prop.t = FALSE, 
                             prop.chisq = FALSE, expected = TRUE
                             )
         )


# Make mosaicplot of sex by agecat
mosaicplot(type ~ agecat, data = abaloneKeep, 
           color = c("light blue", "dark grey"), main = "Abalone Sex by Age"
           )

# Make mosaicplot of adult by agecat
mosaicplot(adult ~ agecat, data = abaloneKeep, 
           color = c("light green", "purple"), main = "Abalone Adult by Age"
           )


# Add shellcat to abaloneKeep
abaloneKeep <- abaloneKeep %>%
    mutate(shellcat = ifelse(shellWeight <= 0.235, "<= 0.235g", "> 0.235g"))

# Run chisq.test of agecat by shellcat
tableageshell <- abaloneKeep %>% 
    with(table(agecat, shellcat))

chisq.test(tableageshell)

# Make mosaicplot of agecat by shellcat
mosaicplot(agecat ~ shellcat, data = abaloneKeep, 
           color = c("light blue", "grey"), main = "Mosaicplot of age by shell categories"
           )

```
  
  
  
***
  
Chapter 4 - Models and Presentation  
  
Working with Output Objects:  
  
* Can use summary() to get the summary of various types of objects  
	* class(summary(x)) # matrix, which means that you can use column and row filtering  
* The dplyr::summarize_all() creates a data.frame object  
	* davissmall <- daviskeep %>% select(weight, height) %>% summarize_all(funs(mean, sd))  
    * str(davissmall)  # 1 obs of 4 variables  
* The psych::describe() function produces a data.frame that is also 'psych' and 'describe'  
  
Working with Lists:  
  
* Lists are more flexible than data frames; can combine objects of different lengths and data types  
* Can set the names of objects in the list using names()  
* Can extract objects out of the list using the $ operator  
	* myList$myVar  
* The Hmisc::describe() outputs lists of lists  
* Many times, the output of a function will be a list of lists  
	* names(myOutput)  # get the names  
    * myOutput$myName  # extract one of the elements  
  
ANOVA and Linear Models:  
  
* Can add na.rm=TRUE to eliminate NA and NaN from the calculations and functions  
* Can run ANOVA in R using a combination of aov() and TukeyHSD()  
	* myAOV <- aov(y ~ x, data=myDF)  
    * TukeyHSD(myAOV)  
    * Note that the aov() function will automatically remove missing values; no need for na.rm=TRUE  
* Linear regression in R can be run using the lm() function  
	* myLM <- lm(y ~ x, data=myDF)  
  
Final Models Evaluation:  
  
* Can run multiple linear models and run summary() on each to compare statistics  
* Can also run AIC using AIC(modA, modB)  
* Can run regressions on a subset of the data using subset=  
	* lm(y ~ x, data=myDF, subset=(myCol=="myVar"))  
  
Wrap Up:  
  
* R environment, packages, and plotting  
* Objects in R  
* Exploratory data analysis, statistical tests  
* Saving output objects and running analyses  
  
Example code includes:  
```{r eval=FALSE}

# Save summary stats output for 3 requested weights
absummary <- abaloneKeep %>% 
    select(wholeWeight, shuckedWeight, shellWeight) %>% 
    summary()

# Pull columns 1 to 2 out of absummary
absummary[, 1:2]

# Save describe from psych output for weights as abpsych
abpsych <- abaloneKeep %>%
    select(wholeWeight, shuckedWeight, shellWeight) %>%
    psych::describe()

# Display the mad element from abpsych
data.frame(abpsych)[,"mad",drop=FALSE]


# Save output from summarise_all() for diameter
abdiam <- abaloneKeep %>% 
    select(diameter) %>%
    summarise_all(list(~mean(.), ~sd(.), ~median(.), ~min(.), ~max(.)))

# Use str() to see class of abdiam and names of elements
str(abdiam)

# Use $ selector to display sd 
abdiam$sd


# Save output from summarise_all() for diameter by sex
absexdiam <- abaloneKeep %>% 
    group_by(type) %>%
    select(type, diameter) %>%
    summarize_all(list(~mean(.), ~sd(.), ~median(.), ~min(.), ~max(.)))

# Get structure of absexdiam
str(absexdiam)

# Use filter() function to extract row for sex == "I"
absexdiam %>% filter(type == "I")


# Save output from Hmisc::describe(), view class of output
abhmisc <- abaloneKeep %>% 
    select(wholeWeight, shuckedWeight, shellWeight) %>% 
    Hmisc::describe()

# Get structure of abhmisc
str(abhmisc)

# View shuckedWeight statistics from abhmisc
abhmisc$shuckedWeight

# View shuckedWeight extremes from abhmisc
abhmisc$shuckedWeight$extremes


# Save correlation from psych::corr.test(), run str()
abcorr <- abaloneKeep %>% 
    select(age, wholeWeight, shuckedWeight, shellWeight, visceraWeight) %>%
    psych::corr.test()

# Display Pearson's r's and p-values from abage
abcorr$r
abcorr$p


# Perform pooled t-test of length by adult, save as abttest
abttest <- t.test(longestShell ~ adult, data = abaloneKeep, var.equal=TRUE)

# Display statistic, parameter and conf.int from abttest
abttest$statistic
abttest$parameter
abttest$conf.int


# Create frequency table of sex by agecat
tablesexage <- abaloneKeep %>% 
    with(table(type, agecat))

# Chi-square test using tablesexage
cssexage <- chisq.test(tablesexage)

# Display expected values and p.value from cssexage
cssexage$expected
cssexage$p.value


# Compute n(), mean(), sd() and var() of age by sex
abaloneKeep %>%
    group_by(type) %>%
    group_by(N=n(), add=TRUE) %>%
    select(type, age) %>%
    summarise_all(list(~mean(.), ~sd(.), ~var(.)))

# Run aov() of age by sex, save as abaov
abaov <- aov(age ~ type, data = abaloneKeep)

# Run summary() of abaov
summary(abaov)

# Perform TukeyHSD posthoc pairwise tests on abaov
TukeyHSD(abaov)


# Run lm() of age by shuckedWeight, save output as lmshucked
lmshucked <- lm(age ~ shuckedWeight, data = abaloneKeep)

# Display coefficients element from lmshucked
lmshucked$coefficients

# Save summary() output of lmshucked as smrylmshucked
smrylmshucked <- summary(lmshucked)

# Show r.squared and adj.r.squared elements of smrylmshucked
smrylmshucked$r.squared
smrylmshucked$adj.r.squared


# Run lm() for age by shuckedWeight and by shellWeight
lmshucked <- lm(age ~ shuckedWeight, data = abaloneKeep)
lmshell <- lm(age ~ shellWeight, data = abaloneKeep)

# Run summary() for each model fit and save results
smrylmshucked <- summary(lmshucked)
smrylmshell <- summary(lmshell)

# Display r.squared for both models
smrylmshucked$r.squared
smrylmshell$r.squared

# Compare AICs for both models
AIC(lmshucked, lmshell)


# Save model output and output of model summary for infants
modelInfants <- lm(age ~ shellWeight, data = abaloneKeep, subset = (type == "I"))
summaryInfants <- summary(modelInfants)

# Save model output and output of model summary for females
modelFemales <- lm(age ~ shellWeight, data = abaloneKeep, subset = (type == "F"))
summaryFemales <- summary(modelFemales)

# Save model output and output of model summary for males
modelMales <- lm(age ~ shellWeight, data = abaloneKeep, subset = (type == "M"))
summaryMales <- summary(modelMales)

# Display each model r.squared values
t(data.frame(summaryInfants$r.squared, summaryFemales$r.squared, summaryMales$r.squared))

```
  
  
  
***
  