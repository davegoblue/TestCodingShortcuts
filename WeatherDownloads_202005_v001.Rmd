---
title: "Weather Data Downloads"
author: "davegoblue"
date: "5/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
The file 'AdditionalCoding_202003_v002.Rmd' contains code for dowloading, processing, integrating, and analyzing historical weather data as contained in METAR archives.

[METAR](https://en.wikipedia.org/wiki/METAR) are hourly weather data collected at airports, including  visibility, wind, temperature, dew point, precipitation, clouds, barometric pressure, and other features that may impact safe aviation.

Iowa State University has a great database of archived METAR data, stored in a manner that makes for easy, automated downloads in CSV format.

The objective of this file is to isolate the portions of code that download data from the Iowa State archives and run initial processing to verify data integrity and produce/save tibbles for further exploration.
  
#### _Example #1: Functions for Downloading METAR Data_  
There are two key functions for downloading METAR data:  
  
* genericGetASOSData() downloads METAR data from a specified station with a specified start and end date to a specified file location.  Defaults are set as per Iowa State's html code when requesting a CSV.    
  
* getASOSStationTime() automates parameter management for calling genericGetASOSData(), with specific focus on converting a user's desired station and time to a sensible filename, and converting target year to a sensible start and end time (METAR are in Zulu/UTC time zone, so it is helpful to start early and end late to have full data even if later converting to a local time zone)  
  
The code for function genericGetASOSData:  
```{r}

genericGetASOSData <- function(fileLoc, 
                               stationID,
                               startDate, 
                               endDate,
                               downloadMethod="curl", 
                               baseURL="https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?", 
                               dataFields="all", 
                               dataTZ="Etc%2FUTC", 
                               dataFormat="onlycomma", 
                               dataLatLon="no", 
                               dataMissing="M", 
                               dataTrace="T", 
                               dataDirect="no", 
                               dataType=2
                               ) {
    
    # Get the year, day, and hour of the key dates
    y1 <- lubridate::year(startDate)
    m1 <- lubridate::month(startDate)
    d1 <- lubridate::day(startDate)
    
    y2 <- lubridate::year(endDate)
    m2 <- lubridate::month(endDate)
    d2 <- lubridate::day(endDate)

    # Mimic the string shown below
    # https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LAS&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2
    
    useURL <- paste0(baseURL, "station=", stationID)  # add the desired station
    useURL <- paste0(useURL, "&data=", dataFields)  # default is "all
    useURL <- paste0(useURL, "&year1=", y1, "&month1=", m1, "&day1=", d1)  # Specify start ymd
    useURL <- paste0(useURL, "&year2=", y2, "&month2=", m2, "&day2=", d2)  # Specify end ymd
    useURL <- paste0(useURL, "&tz=", dataTZ)  # time zone (default UTC)
    useURL <- paste0(useURL, "&format=", dataFormat)  # file format (default CSV)
    useURL <- paste0(useURL, "&latlon=", dataLatLon)  # Whether to include lat-lon (default no)
    useURL <- paste0(useURL, "&missing=", dataMissing)  # How to handle missing data (default is 'M')
    useURL <- paste0(useURL, "&trace=", dataTrace)  # How to handle trace data (default is 'T')
    useURL <- paste0(useURL, "&direct=", dataDirect)  # Whether to directly get the data (default is 'no')
    useURL <- paste0(useURL, "&report_type=", dataType)  # Whether to get just METAR (2, default)
    
    # Download the file
    cat("\nDownloading from:", useURL, "\nDownloading to:", fileLoc, "\n")
    download.file(useURL, destfile=fileLoc, method=downloadMethod)
        
    return(TRUE)
}

```
  
The code for getASOSStationTime:  
```{r}

getASOSStationTime <- function(stationID, 
                               startDate=NULL, 
                               endDate=NULL, 
                               analysisYears=NULL, 
                               fileLoc=NULL,
                               ovrWrite=FALSE,
                               ...) {
    
    # Get the relevant time period for the data
    if (is.null(analysisYears) & (is.null(startDate) | is.null(endDate))) {
        stop("Must provide either analysisYears or both of startDate and endDate")
    }
    if (!is.null(startDate) & !is.null(endDate) & !is.null(analysisYears)) {
        stop("Should specify EITHER both of startDate and endDate OR analysisYears BUT NOT both")
    }
    if (is.null(startDate)) {
        startDate <- ISOdate(min(analysisYears)-1, 12, 31, hour=0)
        endDate <- ISOdate(max(analysisYears)+1, 1, 2, hour=0)
    }
    
    # Create the file name
    if (!is.null(analysisYears)) {
        if (length(analysisYears) == 1) { timeDesc <- analysisYears }
        else { timeDesc <- paste0(min(analysisYears), "-", max(analysisYears)) }
    } else {
        timeDesc <- paste0(lubridate::year(startDate), 
                           stringr::str_pad(lubridate::month(startDate), 2, pad="0"),
                           stringr::str_pad(lubridate::day(startDate), 2, pad="0"), 
                           "-", 
                           lubridate::year(endDate), 
                           stringr::str_pad(lubridate::month(endDate), 2, pad="0"), 
                           stringr::str_pad(lubridate::day(endDate), 2, pad="0")
                           )
    }
    
    if (is.null(fileLoc)) {
        fileLoc <- paste0("./RInputFiles/", 
                          "metar_k", 
                          stringr::str_to_lower(stationID), 
                          "_", 
                          timeDesc, 
                          ".txt"
                          )
    }
    
    cat("\nData for station", stationID, "from", as.character(startDate), "to", 
        as.character(endDate), "will download to", fileLoc, "\n"
        )
    
    if (file.exists(fileLoc) & !ovrWrite) {
        stop("File already exists, aborting")
    }
    
    genericGetASOSData(fileLoc=fileLoc, stationID=stationID, startDate=startDate, endDate=endDate, ...)
    
}

```
  
Example downloads are created (cached to avoid multiple hits to Iowa State's server), with focus on additional 2016 data for Wintry cities to build on the later analyses in 'AdditionalCoding_202003_v002.Rmd':  
```{r cache=TRUE}
# Download 2016 weather data for Milwaukee, WI
getASOSStationTime(stationID="MKE", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Madison, WI
getASOSStationTime(stationID="MSN", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Green Bay, WI
getASOSStationTime(stationID="GRB", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Escanaba, MI
getASOSStationTime(stationID="ESC", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Traverse City, MI
getASOSStationTime(stationID="TVC", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Grand Rapids, MI
getASOSStationTime(stationID="GRR", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Indianapolis, IN
getASOSStationTime(stationID="IND", analysisYears=2016, ovrWrite=TRUE)
```
  
Confirmation can be obtained that the files downloaded as expected:  
```{r}

for (airport in c("kmke", "kmsn", "kgrb", "kesc", "ktvc", "kgrr", "kind")) {
    
    fileCheck <- paste0("./RInputFiles/metar_", airport, "_2016.txt")
    print(file.info(fileCheck)[c("size", "isdir", "mode", "mtime")])
    
}

```
  
The files are all in the 2-3 MB range, which is appropriate for a year of METAR data.
  
#### _Example #2: Finding the Hourly Reporting Time for METAR Data_  
The METAR are downloaded in raw CSV format, and require some data wrangling to extract a clean file in a common format.

One component of wrangling a METAR file is to specify the columns and expected data types.  The Iowa State data are contained in the below format, which is not necessarily standard to how other archives may store the information (Iowa State has run processing on the METAR file):  
```{r}

metType <- readr::cols(station=readr::col_character(), 
                       valid=readr::col_datetime(), 
                       tmpf=readr::col_double(),
                       dwpf=readr::col_double(),
                       relh=readr::col_double(),
                       drct=readr::col_double(),
                       sknt=readr::col_double(),
                       p01i=readr::col_character(),  # needs to handle 'T' for trace
                       alti=readr::col_double(),
                       mslp=readr::col_double(),
                       vsby=readr::col_double(),
                       gust=readr::col_double(),
                       skyc1=readr::col_character(),
                       skyc2=readr::col_character(), 
                       skyc3=readr::col_character(), 
                       skyc4=readr::col_character(),
                       skyl1=readr::col_double(),
                       skyl2=readr::col_double(),
                       skyl3=readr::col_double(),
                       skyl4=readr::col_double(),
                       wxcodes=readr::col_character(),
                       ice_accretion_1hr=readr::col_character(), # needs to handle 'T' for trace
                       ice_accretion_3hr=readr::col_character(), # needs to handle 'T' for trace
                       ice_accretion_6hr=readr::col_character(), # needs to handle 'T' for trace
                       peak_wind_gust=readr::col_double(),
                       peak_wind_drct=readr::col_double(),
                       peak_wind_time=readr::col_datetime(),
                       feel=readr::col_double(),
                       metar=readr::col_character()
                       )

```

Another component of wrangling a METAR file is to extract the hourly observations.  Since the primary use of a METAR is aviation safety, non-hourly updates are common when changes in clouds, winds, precipiation, and the like need to be communicated.  For analysis, though, it is more useful to have just the hourly summaries which contain the most salient information about the last hour in an easily digestable format.

Each airport reports hourly at the same number of minutes past the hour (each airport may have a different hourly reporting time, but the hourly reporting time for a given airport is constant all year).  In the US, this is commonly some time between 50-55 minutes past the hour.

The function findHourlyReportingTime() reads a METAR file and analyzes the most commonly occuring time:  
```{r}

findHourlyReportingTimes <- function(metFile=NULL, 
                                     metStation=NULL, 
                                     metTime=NULL, 
                                     metPath="./RInputFiles/metar_k", 
                                     metSep="_", 
                                     metExt=".txt",
                                     stationToLower=TRUE, 
                                     returnAll=FALSE,
                                     naValues=c("", "NA", "M"), 
                                     colTypes=metType
                                     ) {
    
    # Find the file name if it has not been passed
    if (is.null(metFile)) {
        if (is.null(metStation) | is.null(metPath) | is.null(metSep) | is.null(metTime) | is.null(metExt)) {
            stop("If metFile is not passed, all other parameters must be passed so filename can be created")
        }
        if (stationToLower) { metStation <- stringr::str_to_lower(metStation) }
        metFile <- paste0(metPath, metStation, metSep, metTime, metExt)
    }
    
    # Find the most common Zulu time (this will be the METAR)
    zTimes <- readr::read_csv(metFile, na=naValues, col_types=colTypes) %>%
        pull(metar) %>%
        stringr::str_match(pattern="\\d{2}Z") %>%
        as.vector() %>%
        table() %>%
        sort(decreasing=TRUE)

    # Print the most common 10 times    
    cat("\nMost common 10 times for file:", metFile, "\n")
    print(zTimes[1:10])

    # Print the estimated Zulu time
    cat("\nThe most common Zulu time for", metFile, "is", names(zTimes)[1], "\nFrequency is",
        round(100*zTimes[1]/sum(zTimes), 1), "% (", zTimes[1], "of", sum(zTimes), ")\n"
        )

    # Return the fill vector if requested (likely only if a problem is detected), otherwise return the time
    if (returnAll) {
        return(zTimes)
    } else {
        return(names(zTimes)[1])
    }
    
}

# The tidvverse library is needed for the remainder of the process
library(tidyverse)

# The most common hours are assessed
findHourlyReportingTimes(metStation="MKE", metTime=2016)
findHourlyReportingTimes(metStation="MSN", metTime=2016)
findHourlyReportingTimes(metStation="GRB", metTime=2016)
findHourlyReportingTimes(metStation="ESC", metTime=2016)
findHourlyReportingTimes(metStation="TVC", metTime=2016)
findHourlyReportingTimes(metStation="GRR", metTime=2016)
findHourlyReportingTimes(metStation="IND", metTime=2016)

```
  
With the exception of Escanaba, MI (ESC), the most common times seeem reasonable - greater than 75% of the observations are at a specific, recurring time past the hour in the early-mid 50s:  
  
* 52Z - Milwaukee (83%)  
* 53Z - Madison (79%), Green Bay (78%), Traverse City (79%), Grand Rapids (79%)  
* 54Z - Indianapolis (83%)  
  
Escanaba appears to have multiple reporting times, which may be driven by Escanaba being a much smaller airport that sees little if any scheduled air service.  As there are sufficient Wintry cities already for further analysis, and as Green Bay is less than 100 miles to the south, Escanaba will be excluded from any follow-on analysis.
  
The remaining 6 cities all have roughly 8,800 observations at the most common time.  There are 8,784 hours in a 366-day leap-year (2016), 1-2 extra days were pulled on either side of the year, an observation is expected every hour, and these are real-world systems where occasional missing archive data is not unexpected.  So, to a first order, data quantities of about 8,800 observations appear accurate.
  
#### _Example #3: Processing a METAR File_  
The METAR files can be processed, with data extracted based on the hourly time identified above.  The two key fields in the files are:  
  
* valid - the UTC date-time associated to the METAR  
* metar - the raw METAR data  
  
The other fields represent processed data which may be useful for other purposes.  But, the main purpose of this analysis is to extract and analyze METAR data.
  
Broadly speaking, there are several steps required to convert downloaded data for use in further analysis:  
  
* readMETAR() - extract only hourly observations, check for expected times and record uniqueness  
* initialParseMETAR() - parse the raw METAR data based on an expected regex pattern  
* convertMETAR() - converts characters from the parsed METAR file to meaningful numbers for analysis  
* fixMETAR() - corrects for issues with extracting and converting visibility, wind gusts, and SLP  
* getclouds() - extract cloud data from the METAR and determine cloud types, minimum heights, and minimum ceilings  
* bindMETAR() - put everything together for further analysis  
  
The readMETAR() function is designed to extract only the hourly observations.  It also checks that all of the expected times are found in the data and flags any differences.  If any unexpected times are found in the data, or if any of the data are not unique, an error is thrown:  
```{r}

# Function to make an initial read of the data, filter to METAR records, and check date-times
readMETAR <- function(fileName, 
                      timeZ,
                      expMin, 
                      expDays,
                      colTypes=NULL,
                      printSTR=FALSE,
                      errUnexpected=TRUE,
                      errNonUnique=TRUE
                      ) {

    # Read METAR data
    initRead <- readr::read_csv(fileName, na=c("", "NA", "M"), col_types=colTypes)
    
    # Provide descriptions of the METAR read (str only if flag is set)
    if (printSTR) { str(initRead, give.attr=FALSE) }
    dim(initRead)

    # Filter to only data that ends with times ending in timeZ
    filterRead <- initRead %>%
        filter(str_detect(metar, timeZ))
    dim(filterRead)

    # Check that the dates and times included are as expected
    expDate <- expMin + lubridate::hours(0:(24*expDays - 1))
    
    # Observations expected but not recorded
    cat("\n*** OBSERVATIONS EXPECTED BUT NOT RECORDED ***\n")
    print(as.POSIXct(setdiff(expDate, filterRead$valid), origin="1970-01-01", tz="UTC"))

    # Observations recorded but not expected
    unexpected <- as.POSIXct(setdiff(filterRead$valid, expDate), origin="1970-01-01", tz="UTC")
    cat("\n*** OBSERVATIONS RECORDED BUT NOT EXPECTED ***\n")
    print(unexpected)
    if (errUnexpected) {
        stopifnot(length(unexpected)==0)
    }

    # Confirmation of uniqueness
    allUnique <- length(unique(filterRead$valid)) == length(filterRead$valid)
    cat("\n*** Are the extracted records unique? ***\n", allUnique, "\n")
    if (errNonUnique) {
        stopifnot(allUnique)
    }
    
    # Return the dataset as a tibble
    tibble::as_tibble(filterRead)
}

```
  
As an example, readMETAR() is run for the Green Bay data:  
```{r}

readGRB <- readMETAR("./RInputFiles/metar_kgrb_2016.txt", 
                     timeZ="53Z", 
                     expMin=as.POSIXct("2015-12-31 00:53:00", tz="UTC"), 
                     expDays=368, 
                     colTypes=metType
                     )
str(readGRB, give.attr=FALSE)

```
  
The initialParseMETAR() function is designed to parse a METAR file based on an expected regex string.  The regex passed is specific to the parameters of interest for this project; different regex could be passed if different portions of the METAR were desired to be extracted:  
```{r}

# Code for the initial METAR parsing
initialParseMETAR <- function(met, val, labs, showParseSummary=FALSE, glimpseFinal=FALSE) {
    
    # Pull the METAR data
    metAll <- met %>%
        pull(metar)
    
    # Find the number of matching elements
    cat("\n*** Tentative Summary of Element Parsing *** \n")
    str_detect(metAll, pattern=val) %>% 
        table() %>%
        print()

    # The strings that do not match have errors in the raw data (typically, missing wind speed)
    cat("\n*** Data Not Matched *** \n")
    print(metAll[!str_detect(metAll, pattern=val)])

    # A matrix of string matches can be obtained
    mtxParse <- str_match(metAll, pattern=val)
    if (showParseSummary) {
        cat("\n*** Parsing matrix summary *** \n")
        print(dim(mtxParse))
        print(head(mtxParse))
    }

    # Create a data frame
    dfParse <- data.frame(mtxParse, stringsAsFactors=FALSE) %>%
        mutate(dtime=met$valid, origMETAR=met$metar)
    names(dfParse) <- c(labs, "dtime", "origMETAR")
    dfParse <- tibble::as_tibble(dfParse)
    if (glimpseFinal) {
        cat("\n*** Summary of the parsed data *** \n")
        glimpse(dfParse)
    }
    
    dfParse
}

```
  
The function can then be run against the Green Bay data:  
```{r}

# Create a search string for METAR
valMet <- "53Z.*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})"

# Create the names for the search string to parse in to
labsMet <- c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
             "TempC", "DewC", "Altimeter", "SLP", "FahrC"
             )

# Run the METAR parsing on the raw data
initGRB <- initialParseMETAR(readGRB, val=valMet, labs=labsMet, glimpseFinal=TRUE)

```
  
The convertMETAR() function is designed to convert the parsed data to numeric entries for further analysis.  Among the steps are:  
  
* WindSpeed - convert from character to numeric  
* WindGust - convert from character to numeric  
* Visibility - remove 'SM' and convert remainder to numeric  
* TempC - change 'M' (means negative) to '-' and convert to numeric
* DewC - change 'M' (means negative) to '-' and convert to numeric  
* Altimeter - remove 'A' and convert remainder to numeric  
* SLP - remove 'SLP' and convert remainder to numeric  
* TempF - take characters 2-5 of T(dddd)dddd, convert to -ddd if leading d is 1, convert to numeric, divide by 10, multiply by 1.8 and add 32  
* DewF - take characters 6-9 of Tdddd(dddd), convert to -ddd if leading d is 1, convert to numeric, divide by 10, multiply by 1.8 and add 32  
  
Function code below:  
```{r}

# Code for the conversion of METAR to meaningful numeric
# Function is hard-coded to work on metrics passed as default; potentially generalize later
convertMETAR <- function(met, 
                         metrics=c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
                                   "DewC", "Altimeter", "SLP", "TempF", "DewF"
                                   ), 
                         seed=2003211416, 
                         showInvestigate=FALSE, 
                         showNACounts=TRUE,
                         showMetricPlots=FALSE
                         
                         ) {
    
    # Convert to numeric where appropriate
    dfParse <- met %>%
        mutate(WindSpeed = as.integer(WindSpeed), 
               WindGust = as.numeric(WindGust), 
               Visibility = as.numeric(str_replace(Visibility, "SM", "")),
               TempC = as.integer(str_replace(TempC, "M", "-")), 
               DewC = as.integer(str_replace(DewC, "M", "-")), 
               Altimeter = as.integer(str_replace(Altimeter, "A", "")), 
               SLP = as.integer(str_replace(SLP, "SLP", "")), 
               TempF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 2, 5), pattern="^1", "-"))/10, 
               DewF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 6, 9), pattern="^1", "-"))/10
               )

    # Investigate the data
    if (showInvestigate) {
        cat("\n *** Parsed data structure, head, tail, and random sample *** \n")
        str(dfParse)
        print(head(dfParse))
        print(tail(dfParse))
        set.seed(seed)
        dfParse %>% 
            sample_n(20) %>%
            print()
    }

    if (showNACounts) {
        # Check for NA values
        cat("\n *** Number of NA values *** \n")
        print(colSums(is.na(dfParse)))
    }
    
    # Plot of counts by key metric
    # NOTE - function has NOT YET been created in this module - placeholder
    if (showMetricPlots) {
        plotcountsByMetric(dfParse, mets=metrics)
    }
    
    # Return the parsed dataset
    dfParse
    
}

```
  
The function is run for the initial Green Bay data:  
```{r}

convGRB <- convertMETAR(initGRB)
glimpse(convGRB)

```

There are then three fixes required to the converted data:  
  
* Visibility is recorded as a fraction when visibility is low, such as 1/2SM - these cases need to be identified and converted properly, since the base regex would extract 2SM from the above  
* Wind gusts are never correctly extracted by convertMETAR(); a separate function is needed to identify only the records with dddddGddKT and to parse out the wind gusts from these  
* SLP is recorded as three digits which are know to be either the 900s for a high SLP or the 1000s for a low SLP; this needs to be modified accordingly  
  
The function fixMETAR() runs the functions getVisibility(), getWindGusts(), and fixSLP(), and integrated the updates in a consolidated file:  
```{r}

# Address the visibility issues
getVisibility <- function(curMet, origMet, var="metar") {
    
    # Get the original METAR data
    metAll <- origMet %>%
        pull(var)
    
    # Correct for visibility
    # Areas that have \\d \\d/\\dSM
    sm1 <- which(str_detect(metAll, pattern=" \\d/\\dSM"))
    sm2 <- which(str_detect(metAll, pattern=" \\d \\d/\\dSM"))

    valSM1 <- str_match(metAll, pattern="\\d/\\dSM")[sm1]
    valSM1 <- str_replace(valSM1, "SM", "")
    valSM1 <- as.integer(str_sub(valSM1, 1, 1)) / as.integer(str_sub(valSM1, 3, 3))

    valSM2 <- str_match(metAll, pattern=" \\d \\d/\\dSM")[sm2]
    valSM2 <- as.integer(str_sub(valSM2, 2, 2))

    curMet[sm1, "Visibility"] <- valSM1
    curMet[sm2, "Visibility"] <- curMet[sm2, "Visibility"] + valSM2

    visCounts <- curMet %>% 
        count(Visibility)
    visVector <- visCounts$n
    names(visVector) <- visCounts$Visibility
    cat("\nVisibilities after correcting for fractions:\n")
    print(visVector)
    cat("\n")
    
    curMet
}

# Correct for wind gusts
getWindGusts <- function(curMet, origMet, var="metar") {

    metAll <- origMet %>%
        pull(var)
    
    gustCheck <- which(str_detect(metAll, pattern="\\d{5}G\\d{2}KT"))
    valGust <- str_match(metAll, pattern="\\d{5}G\\d{2}KT")[gustCheck]
    valGust <- as.integer(str_sub(valGust, 7, 8))

    curMet[gustCheck, "WindGust"] <- valGust

    gustCounts <- curMet %>% 
        count(WindGust)
    gustVector <- gustCounts$n
    names(gustVector) <- gustCounts$WindGust
    cat("\nCounts of wind gusts extracted:\n")
    print(gustVector)
    cat("\n")
    
    curMet
}

# Correct for SLP
fixSLP <- function(curMet) {

    dfParse <- curMet %>%
        mutate(modSLP=ifelse(curMet$SLP < 500, 1000 + curMet$SLP/10, 900 + curMet$SLP/10))

    p <- dfParse %>%
        group_by(SLP, modSLP) %>%
        summarize(n=n()) %>%
        ggplot(aes(x=SLP, y=modSLP, size=n)) + 
        geom_point(alpha=0.3) + 
        labs(title="Correction of SLP in raw METAR to modSLP for further analysis", 
             x="SLP: Original Data", 
             y="modSLP: Converted for Analysis Data"
             )
    print(p)
    
    dfParse
}

fixMETAR <- function(met, 
                     origMet=met, 
                     var="origMETAR",
                     modifyVisibility=TRUE, 
                     modifyWindGusts=TRUE, 
                     modifySLP=TRUE
                     ) {
    
    if (modifyVisibility) {
        met <- getVisibility(met, origMet, var=var)
    }
    
    if (modifyWindGusts) {
        met <- getWindGusts(met, origMet, var=var)
    }
    
    if (modifySLP) {
        met <- fixSLP(met)
    }
    
    met
    
}

```
  
The function is then run to fix the processed Green Bay data:  
```{r}

fixGRB <- fixMETAR(convGRB)

```
  
Lastly, cloud data can be extracted from the raw METAR file.  The process is split in to two steps:  
  
* extractClouds() extracts all of the cloud data from a METAR file - clouds can be flagged as clear skies (CLR or SKC), indeterminate due to low vertical visibility (VV), FEW, SCT, BKN, OVC.  When the cloud type is any of VV, FEW, SCT, BKN, OVC, it is followed by ddd which is the height in hundreds of feet.  There can be more than one cloud layer, for example FEW008 SCT025 BKN100  
* findLowestClouds() extracts the lowest cloud level for each cloud type - for example, SCT025 SCT050 BKN100 has a lowest SCT (scattered) of 2500 feet and a lowest BKN (broken) of 10000 feet  
  
The function getClouds() runs these two functions:  
```{r}

extractClouds <- function(met, 
                          metVar, 
                          subT,
                          showInitCloudTable=TRUE,
                          showInitCloudMissing=TRUE
                          ) {

    metAll <- met %>%
        pull(metVar)
    
    # Extract the CLR records
    mtxCLR <- str_extract_all(metAll, pattern=" CLR ", simplify=TRUE)
    if (dim(mtxCLR)[[2]] != 1) { stop("Extracted 2+ CLR from some METAR; investigate") }
    isCLR <- ifelse(mtxCLR[, 1] == "", 0, 1)

    # Extract the VV records
    mtxVV <- str_extract_all(metAll, pattern="VV(\\d{3})", simplify=TRUE)
    if (dim(mtxVV)[[2]] > 1) { stop("Extracted 2+ VV from some METAR; investigate") }
    if ((dim(mtxVV))[[2]] == 0) {
        cat("\nNo Records with a cloud type of vertical visibility (VV)\n")
        isVV <- rep(0, times=length(isCLR))
        htVV <- rep(NA, times=length(isCLR))
    } else {
        isVV <- ifelse(mtxVV[, 1] == "", 0, 1)
        htVV <- ifelse(mtxVV[, 1] == "", NA, as.integer(str_replace(mtxVV[, 1], "VV", ""))*100)
    }

    # Extract the FEW records
    mtxFEW <- str_extract_all(metAll, pattern="FEW(\\d{3})", simplify=TRUE)
    numFEW <- apply(mtxFEW, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the SCT records
    mtxSCT <- str_extract_all(metAll, pattern="SCT(\\d{3})", simplify=TRUE)
    numSCT <- apply(mtxSCT, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the BKN records
    mtxBKN <- str_extract_all(metAll, pattern="BKN(\\d{3})", simplify=TRUE)
    numBKN <- apply(mtxBKN, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the OVC records
    mtxOVC <- str_extract_all(metAll, pattern="OVC(\\d{3})", simplify=TRUE)
    numOVC <- apply(mtxOVC, 1, FUN=function(x) { sum((x!=""))} )

    # Summarize as a data frame
    tblClouds <- tibble::tibble(isCLR=isCLR, isVV=isVV, htVV=htVV, numFEW=numFEW, 
                                numSCT=numSCT, numBKN=numBKN, numOVC=numOVC
                                )

    # Get the counts
    if (showInitCloudTable) {
        cat("\n*** Counts by number of layers of each cloud type ***\n")
        tblClouds %>% 
            count(isCLR, isVV, numFEW, numSCT, numBKN, numOVC) %>%
            as.data.frame() %>%
            print()
    }
    
    # Investigate the problem data
    if (showInitCloudMissing) {
        cat("\n*** METAR records where no clouds were extracted ***\n")
        metAll[rowSums(tblClouds, na.rm=TRUE)==0] %>%
            print()
    }
    
    # Plot the counts of most obscuration
    p <- tblClouds %>%
        filter(rowSums(., na.rm=TRUE) > 0) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            )
               ) %>%
        ggplot(aes(x=wType, y=..count../sum(..count..))) + 
        geom_bar() + 
        labs(title="Highest Obscuration by Cloud", subtitle=subT, 
             x="Cloud Type", y="Proportion of Hourly Measurements"
             )
    print(p)
    
    # Integrate the clouds data
    mtxCloud <- cbind(mtxVV, mtxOVC, mtxBKN, mtxSCT, mtxFEW, mtxCLR)
    cat("\n*** Dimensions for the cloud matrix ***\n")
    print(dim(mtxCloud))
    
    list(tblClouds=tblClouds, mtxCloud=mtxCloud)
}


# Helper function to cycle through to find levels of a given type
ckClouds <- function(cloudType, mtx) {
    isKey <- which(apply(mtx, 2, FUN=function(x) {sum(str_detect(x, cloudType))}) > 0)
    as.integer(str_replace(mtx[, min(isKey)], cloudType, "")) * 100
}


# Function to create the lowest cloud levels
findLowestClouds <- function(mtxCloud, 
                             subT="Lincoln, NE (2016) Hourly METAR", 
                             showExample=TRUE, 
                             showPlots=TRUE
                             ) {

    # Find the lowest clouds by cloud type
    lowOVC <- ckClouds("OVC", mtx=mtxCloud)
    lowVV <- ckClouds("VV", mtx=mtxCloud)
    lowBKN <- ckClouds("BKN", mtx=mtxCloud)
    lowSCT <- ckClouds("SCT", mtx=mtxCloud)
    lowFEW <- ckClouds("FEW", mtx=mtxCloud)

    # Integrate the lowest cloud type by level
    lowCloud <- tibble::tibble(lowVV, lowOVC, lowBKN, lowSCT, lowFEW)
    if (showExample) {
        cat("\n*** Lowest clouds by type tibble ***\n")
        print(lowCloud)
    }

    # Get the lowest cloud level
    minCloud <- lowCloud
    minCloud[is.na(minCloud)] <- 999999
    minCloudLevel <- apply(minCloud, 1, FUN=min)
    minCeilingLevel <- apply(minCloud[, c("lowVV", "lowOVC", "lowBKN")], 1, FUN=min)

    noCloudPct <- mean(minCloudLevel == 999999)
    noCeilingPct <- mean(minCeilingLevel == 999999)

    # Plot the minimum cloud level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCloudLevel != 999999) %>%
        ggplot(aes(x=minCloudLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCloudPct), "% of obs. have no clouds")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Cloud Height (when some clouds exist)", subtitle=subT
             )
    if (showPlots) print(p)

    # Plot the minimum ceiling level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCeilingLevel != 999999) %>%
        ggplot(aes(x=minCeilingLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCeilingPct), "% of obs. have no ceiling")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Ceiling Height (when a ceiling exists)", subtitle=subT
             )
    if (showPlots) print(p)
    
    list(lowCloud=lowCloud, minCeilingLevel=minCeilingLevel, minCloudLevel=minCloudLevel)
}

# Calls extractClouds() and findLowestClouds()
getClouds <- function(met, 
                      cityName, 
                      metVar="origMETAR",
                      showInitCloudTable=FALSE, 
                      showInitCloudMissing=TRUE, 
                      showExample=FALSE,
                      showPlots=TRUE
                      ) {
    
    # Run the initial cloud extraction
    initClouds <- extractClouds(met, 
                                metVar=metVar, 
                                subT=paste0(cityName, " Hourly METAR"), 
                                showInitCloudTable=showInitCloudTable, 
                                showInitCloudMissing=showInitCloudMissing
                                )
    
    # Find the lowest cloud levels of each cloud type
    processedClouds <-findLowestClouds(initClouds$mtxCloud, 
                                       subT=paste0(cityName, " Hourly METAR"), 
                                       showExample=showExample, 
                                       showPlots=showPlots
                                       )

    list(initClouds=initClouds, processedClouds=processedClouds)
    
}

```
  
The functions are then run for the Green Bay data:  
```{r}

cloudsGRB <- getClouds(fixGRB, cityName="Green Bay, WI (2016)")

```
  
The bindMETAR() function then puts everything together:  
```{r}

# Function to bind the existing parsed METAR data with the cloud data
bindMETAR <- function(dfParse, tblClouds, lowCloud) {

    # Integrate the cloud data and convert month to a factor
    dfFull <- cbind(dfParse, tblClouds, lowCloud) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", 
                                      isVV==1 ~ "VV", 
                                      numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", 
                                      numSCT > 0 ~ "SCT", 
                                      numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), 
                            levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            ), 
               month=factor(lubridate::month(dtime), levels=1:12, labels=month.abb)
               )
    
    dfFull <- tibble::as_tibble(dfFull)
    glimpse(dfFull)
    
    dfFull
}

```
  
The function is run for the Green Bay data:  
```{r}

fullGRB <- bindMETAR(fixGRB, 
                     tblClouds=cloudsGRB$initClouds$tblClouds, 
                     lowCloud=cloudsGRB$processedClouds$lowCloud
                     )

```
  
A fully processed file for Green Bay, WI (2016) is now available for further analysis.
  
#### _Example #4: Cloud Processing Update_  
The cloud extraction code above is verbose and mingles analysis with data wrangling.  An improved approach is to focus on extracting and validating cloud data as a single function.  Investigation of things like ceiling height and minimum cloud height can be left for the analysis stage.
  
Broadly speaking, there are seven types of clouds that can be reported in a METAR:  
  
* SKC - skies are clear as determined by an automated sensor and confirmed by a human observer  
* CLR - skies are clear as determined by an automated sensor (the sensor has a maximum height, and there may be clouds, including full overcast, above the maximum sensor height)  
* VVddd - vertical visbility is obscured to ddd hundred feet, thus cloud levels are indeterminate  
* OVCddd - there is an overcast at ddd hundred feet  
* BKNddd - there is a broken (5/8 to 7/8) layer of clouds at ddd hundred feet  
* SCTddd - there is a scattered (3/8 to 4/8) layer of clouds at ddd hundred feet  
* FEWddd - there are a few (1/8 to 2/8) clouds at ddd hundred feet  
  
Cloud layers are always graded according to the total obscuration up to and including that level.  As such, it is impossible for FEW to be recorded at any level higher than the first recording of SCT/BKN/OVC/VV.
  
The cloud processing function is enahnced to perform the following:  
  
* Determine whether SKC or CLR has been reported  
* Extract up to 6 observations of (VV|OVC|BKN|SCT|FEW)\\d{3}, and split their data in to cloud type and cloud height  
* Confirm that every record has exactly 1 of SKC, CLR, or 1+ cloud type recorded (flag exceptions)  
* Confirm that cloud types are strictly non-descending across FEW<SCT<BKN<OVC<VV  
* Confirm that cloud heights are strictly ascending by observation  
* Create wtype as the highest level of obscuration recorded in the cloud data  
  
Example code includes:  
```{r}

# Helper function to check if data are always ascending
# Subject to the rules that NA may follow anything but may only precede NA
ascVectorChecker <- function(y, strictly=TRUE) {

    # Checks a vector for being ascending (strictly so if strictly=TRUE)
    # Allows for NA provided that they never precede a numeric value
    
    if (all(is.na(y))) {
        # This is OK, if there are no numeric values then they are all NA
    } else if (all(is.na(y[2:length(y)]))) {
        # This is OK, single value is by definition ascending
    } else {
        # Confirm that NA are non-descending and that values are non-descending
        if (any(is.na(y) != cummax(is.na(y)))) {
            cat("\nIssue with NA preceding value -", y)
        } else {
            nonNA <- y[!is.na(y)]
            if (strictly) {
                if (min(diff(nonNA)) <= 0) {
                    cat("\nIssue with following values same or lower then preceding:", y, "\n")
                }
            } else {
                if (min(diff(nonNA)) < 0) {
                    cat("\nIssue with following values lower then preceding:", y, "\n")
                }
            }
        }
    }
}
    
# Helper function to verify non-descending cloud types, ascending cloud heights, return highest cloud type
validateClouds <- function(tbl, 
                           cloudStates,
                           nCols=6, 
                           var1="cloud", 
                           var2="cType", 
                           var3="cLevel"
                           ) {
    
    # STEP 1: Validate non-descending cloud types
    ascStates <- c(cloudStates, "")  # "" can be considered the highest cloud state; nothing should follow it
    cTypes <- tbl %>%
        select_at(vars(all_of(paste0(var2, 1:nCols)))) %>%
        mutate_all(~match(., ascStates))
    
    for (intCtr in 1:(nCols-1)) {
        descClouds <- as.vector(cTypes[, intCtr+1] - cTypes[, intCtr])
        mism <- sum(descClouds < 0)
        if (mism != 0) {
            cat("\nIssue with descending cloud types - ", mism, "records in column", intCtr+1, "\n")
            stop()
        }
    }
    
    
    # STEP 2: Validate ascending cloud types (NA is always fine, but cannot be followed by a number)
    cHeights <- tbl %>%
        select_at(vars(all_of(paste0(var3, 1:nCols))))
    
    apply(cHeights, 1, FUN=ascVectorChecker)
    
    
    # STEP 3: Extract the most obscured cloud type
    obscureTypes <- cTypes %>%
        mutate_all(~ifelse(. > length(cloudStates), 0, .)) %>%
        apply(1, FUN=max)
    
    obscureTypes <- ifelse(obscureTypes==0, length(cloudStates)+1, obscureTypes)
    ascStates[obscureTypes]
    
}


# Function to extract and wrangle cloud data contained in a METAR
extractAndWrangleClouds <- function(met, 
                                    metVar, 
                                    maxClouds=6,
                                    timeVar="dtime",
                                    soloStates=c("SKC", "CLR"), 
                                    numStates=c("FEW", "SCT", "BKN", "OVC", "VV"), 
                                    printDetails=FALSE
                                    ) {
    
    # Check that maxClouds <= 9
    if ((maxClouds != round(maxClouds, 0)) | (maxClouds < 1) | (maxClouds > 9)) {
        cat("\nThe maxClouds parameter must be an integer from 1-9, invalid entry of:", maxClouds)
        stop("Investigate and fix")
    }
    
    # Extract the metar record
    metAll <- met %>%
        pull(metVar)
    
    # Search for soloStates and bind them by column to the existing data
    for (soloState in soloStates) {
        ss <- str_extract_all(metAll, pattern=paste0(" ", soloState, " "), simplify=TRUE)
        if (dim(ss)[2] == 0) {
            resVec <- rep(0L, dim(ss)[1])
        } else {
            resVec <- apply(ss, 1, FUN=function(x) { sum(x!="") })
        }
        origNames <- names(met)
        met <- bind_cols(met, data.frame(resVec))
        names(met) <- c(origNames, paste0("n", soloState))
    }
    
    # Search and extract for numStates
    numPasted <- paste0("(", paste0(numStates, collapse="|"), ")")
    numRecords <- str_extract_all(metAll, pattern=paste0(numPasted, "\\d{3}"), simplify=TRUE)

    # Allow for up to maxClouds cloud types
    if (dim(numRecords)[2] > maxClouds) {
        cat("Too many clouds relative to passed paremeter maximum of", maxClouds, ": Dimensions -",
            dim(numRecords)
            )
        stop("Investigate and fix")
    }
    
    # Create columns for cType1-cType(maxClouds) and cLevel1-cLevel(maxClouds)
    for (intCtr in 1:maxClouds) {
        if (intCtr > dim(numRecords)[2]) {
            x <- rep("", dim(numRecords)[1])
        } else {
            x <- numRecords[, intCtr]
        }
        
        # Hard-coding that the heights are always three digits (as per the extraction)
        cType <- str_sub(x, 1, str_length(x)-3)
        cLevel <- str_sub(x, str_length(x)-2, -1)
        cLevel <- ifelse(cLevel=="", NA_integer_, 100*as.integer(cLevel))
        
        origNames <- names(met)
        met <- bind_cols(met, tibble::tibble(x, cType, cLevel))
        names(met) <- c(origNames, paste0(c("cloud", "cType", "cLevel"), intCtr))
    }
    
    # Validate clouds are sensible and return the most obscured state
    wType <- validateClouds(met, cloudStates=numStates, nCols=maxClouds)
    
    # Create a table of soloStates and wType
    tmp <- met %>%
        select_at(vars(all_of(paste0("n", soloStates)))) %>%
        mutate(obsc=wType)
    
    # Confirm that values in tmp are as expected
    # Should be either 0 or 1 total of soloStates counts
    # If 1 soloStates counts, should be wType=""
    # If 0 soloStates counts, should be wType!=""
    tmpSolos <- tmp %>%
        select_at(vars(all_of(paste0("n", soloStates)))) %>%
        apply(1, FUN=sum)
    
    tmpObsc <- ifelse(wType!="", 1, 0)
    tmpProbs <- (tmpSolos + tmpObsc != 1)
    
    if (sum(tmpProbs)==0) {
        cat("\nwType as created from clouds data is consistent")
    } else {
        cat("\nIssues with clouds data at records:\n")
        met %>%
            select_at(vars(all_of(c(timeVar, metVar)))) %>%
            filter(tmpProbs) %>%
            print()
    }
    
    # Print the summaries by key type
    if (printDetails) {
        tmp %>%
            group_by_all() %>%
            summarize(n=n()) %>%
            print()
    }

    # Create the final wType variable, attach to the met data, and convert to a factor
    # Take the wType variable and overwrite in the soloStates if they exist
    for (soloVar in rev(soloStates)) {
        soloVec <- tmp %>%
            pull(paste0("n", soloVar))
        wType <- ifelse(soloVec > 0, soloVar, wType)
    }
    wType <- ifelse(wType=="", "Error", wType)
    
    met <- met %>%
        mutate(wType=factor(wType, levels=c(rev(numStates), rev(soloStates), "Error")))
    
    # Show count by wType
    met %>%
        count(wType) %>%
        print()
    
    # Return the database
    met
    
}

```
  
And the routine can then be run to create fullGRB2:  
```{r}

fullGRB2 <- extractAndWrangleClouds(fixGRB, metVar="origMETAR")

# Variables in fullGRB but not in fullGRB2
setdiff(names(fullGRB), names(fullGRB2))

# Variables in fullGRB2 but not in fullGRB
setdiff(names(fullGRB2), names(fullGRB))

# Variables in both fullGRB2 and fullGRB
namesBoth <- intersect(names(fullGRB2), names(fullGRB))
namesBoth

# Convert fullGRB to have same levels as fullGRB2
testGRB <- fullGRB %>%
    mutate(wType=factor(wType, levels=levels(fullGRB2$wType)))

cat("\n")
# Comparisons for equality for variables in both
for (colName in namesBoth) {
    ae <- all.equal(testGRB[, colName], fullGRB2[, colName])
    if (isTRUE(ae)) {
        cat("100% Match for Variable:", colName, "\n")
    } else {
        cat("Mismatch for variable:", colName, "-", ae, "\n")
    }
}

```
  
The routine seems to be working well and is extracting a greater portion of cloud data which may be useful for later analysis:  
  
* Every cloud record  
* Every cloud type  
* Every cloud height  
  
Further, checks have been automated to be sure that cloud types are non-descending and cloud heights are strictly ascending (with the exception that NA can follow anything but can only precede NA).
  
The wType variable is created as a factor and reflects that SKC/CLR are acceptable states.  Further the functions check to ensure that there is exactly 1 of SKC, CLR, or wType!="".

Month is also a valuable variable for analysis, though it is contained in 'dtime' already.  Further checks should be run to confirm that 'dtime' is aligned to origMETAR.
  
#### _Example #5: Checking Time Properties_  
At various times, the 'dtime' variable is used to classify information that is pulled from 'origMETAR'.  It will be useful to confirm that the variables are well aligned as follows:  
  
* The origMETAR will have ddddddZ where the first dd are the two-digit day, the second dd are the two-digit hour, the third dd are the two-digit minutes, and the Z reflects UTC/Zulu time  
* The corresponding data from dtime should match, and year, month, and day can also be extracted during this process  
  
Example code includes:  
```{r}

# Function to check alignment of Zulu time in a METAR with an associated date-time variable
checkTimeProperties <- function(met, 
                                metVar, 
                                timeVar
                                ) {
    
    # Extract the Zulu days and times from metVar
    zTime <- met %>%
        pull(metVar) %>%
        str_extract(pattern="\\d{6}Z")
    
    # Extract the day, hour, and minute from timeVar
    dTime <- met %>%
        pull(timeVar)
    
    # Create the expected Zulu string from dTime
    expTime <- paste0(str_pad(lubridate::day(dTime), width=2, pad="0"), 
                      str_pad(lubridate::hour(dTime), width=2, pad="0"), 
                      str_pad(lubridate::minute(dTime), width=2, pad="0"), 
                      "Z"
                      )
    
    # Check for consistency
    mism <- (zTime != expTime)
    if (sum(mism) == 0) {
        cat("\nAll Zulu times in the METAR are consistent with the datetime variable\n")
    } else {
        cat("\nThere are", 
            sum(mism), 
            "inconsistencies between Zulu times in the METAR and the datetime variable\n",
            "First 10 examples include:\n"
            )
        data.frame(zTime, dTime, expTime, mism) %>%
            filter(mism) %>%
            head(10)
    }
    
    # Create key variables and return them
    met <- met %>%
        mutate(year=lubridate::year(dTime), 
               monthint=lubridate::month(dTime), 
               month=factor(monthint, levels=1:12, labels=month.abb[1:12]),
               day=lubridate::day(dTime)
               )
    
    met
        
}

fullGRB3 <- checkTimeProperties(fullGRB2, metVar="origMETAR", timeVar="dtime")

# Variables in fullGRB but not in fullGRB3
setdiff(names(fullGRB), names(fullGRB3))

# Variables in fullGRB3 but not in fullGRB
setdiff(names(fullGRB3), names(fullGRB))

# Variables in both fullGRB3 and fullGRB
namesBoth <- intersect(names(fullGRB3), names(fullGRB))
namesBoth

# Convert fullGRB to have same levels as fullGRB3
testGRB <- fullGRB %>%
    mutate(wType=factor(wType, levels=levels(fullGRB3$wType)))

cat("\n")
# Comparisons for equality for variables in both
for (colName in namesBoth) {
    ae <- all.equal(testGRB[, colName], fullGRB3[, colName])
    if (isTRUE(ae)) {
        cat("100% Match for Variable:", colName, "\n")
    } else {
        cat("Mismatch for variable:", colName, "-", ae, "\n")
    }
}

```
  
The times are aligned, allowing for confident use of the 'dtime' variable in analysis.  Further, the year, month (as both integer and factor), and day have been extracted for easy availability in future analysis.
  
Next steps are to integrate the full download and processing, and to save a final file to the local drive so that processing need only be run once.
  

