---
title: "Weather Data Downloads"
author: "davegoblue"
date: "5/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
The file 'AdditionalCoding_202003_v002.Rmd' contains code for dowloading, processing, integrating, and analyzing historical weather data as contained in METAR archives.

[METAR](https://en.wikipedia.org/wiki/METAR) are hourly weather data collected at airports, including  visibility, wind, temperature, dew point, precipitation, clouds, barometric pressure, and other features that may impact safe aviation.

Iowa State University has a great database of archived METAR data, stored in a manner that makes for easy, automated downloads in CSV format.

The objective of this file is to isolate the portions of code that download data from the Iowa State archives and run initial processing to verify data integrity and produce/save tibbles for further exploration.
  
#### _Example #1: Functions for Downloading METAR Data_  
There are two key functions for downloading METAR data:  
  
* genericGetASOSData() downloads METAR data from a specified station with a specified start and end date to a specified file location.  Defaults are set as per Iowa State's html code when requesting a CSV.    
  
* getASOSStationTime() automates parameter management for calling genericGetASOSData(), with specific focus on converting a user's desired station and time to a sensible filename, and converting target year to a sensible start and end time (METAR are in Zulu/UTC time zone, so it is helpful to start early and end late to have full data even if later converting to a local time zone)  
  
The code for function genericGetASOSData:  
```{r}

genericGetASOSData <- function(fileLoc, 
                               stationID,
                               startDate, 
                               endDate,
                               downloadMethod="curl", 
                               baseURL="https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?", 
                               dataFields="all", 
                               dataTZ="Etc%2FUTC", 
                               dataFormat="onlycomma", 
                               dataLatLon="no", 
                               dataMissing="M", 
                               dataTrace="T", 
                               dataDirect="no", 
                               dataType=2
                               ) {
    
    # Get the year, day, and hour of the key dates
    y1 <- lubridate::year(startDate)
    m1 <- lubridate::month(startDate)
    d1 <- lubridate::day(startDate)
    
    y2 <- lubridate::year(endDate)
    m2 <- lubridate::month(endDate)
    d2 <- lubridate::day(endDate)

    # Mimic the string shown below
    # https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?station=LAS&data=all&year1=2015&month1=12&day1=31&year2=2017&month2=1&day2=2&tz=Etc%2FUTC&format=onlycomma&latlon=no&missing=M&trace=T&direct=no&report_type=2
    
    useURL <- paste0(baseURL, "station=", stationID)  # add the desired station
    useURL <- paste0(useURL, "&data=", dataFields)  # default is "all
    useURL <- paste0(useURL, "&year1=", y1, "&month1=", m1, "&day1=", d1)  # Specify start ymd
    useURL <- paste0(useURL, "&year2=", y2, "&month2=", m2, "&day2=", d2)  # Specify end ymd
    useURL <- paste0(useURL, "&tz=", dataTZ)  # time zone (default UTC)
    useURL <- paste0(useURL, "&format=", dataFormat)  # file format (default CSV)
    useURL <- paste0(useURL, "&latlon=", dataLatLon)  # Whether to include lat-lon (default no)
    useURL <- paste0(useURL, "&missing=", dataMissing)  # How to handle missing data (default is 'M')
    useURL <- paste0(useURL, "&trace=", dataTrace)  # How to handle trace data (default is 'T')
    useURL <- paste0(useURL, "&direct=", dataDirect)  # Whether to directly get the data (default is 'no')
    useURL <- paste0(useURL, "&report_type=", dataType)  # Whether to get just METAR (2, default)
    
    # Download the file
    cat("\nDownloading from:", useURL, "\nDownloading to:", fileLoc, "\n")
    download.file(useURL, destfile=fileLoc, method=downloadMethod)
        
    return(TRUE)
}

```
  
The code for getASOSStationTime:  
```{r}

getASOSStationTime <- function(stationID, 
                               startDate=NULL, 
                               endDate=NULL, 
                               analysisYears=NULL, 
                               fileLoc=NULL,
                               ovrWrite=FALSE,
                               ...) {
    
    # Get the relevant time period for the data
    if (is.null(analysisYears) & (is.null(startDate) | is.null(endDate))) {
        stop("Must provide either analysisYears or both of startDate and endDate")
    }
    if (!is.null(startDate) & !is.null(endDate) & !is.null(analysisYears)) {
        stop("Should specify EITHER both of startDate and endDate OR analysisYears BUT NOT both")
    }
    if (is.null(startDate)) {
        startDate <- ISOdate(min(analysisYears)-1, 12, 31, hour=0)
        endDate <- ISOdate(max(analysisYears)+1, 1, 2, hour=0)
    }
    
    # Create the file name
    if (!is.null(analysisYears)) {
        if (length(analysisYears) == 1) { timeDesc <- analysisYears }
        else { timeDesc <- paste0(min(analysisYears), "-", max(analysisYears)) }
    } else {
        timeDesc <- paste0(lubridate::year(startDate), 
                           stringr::str_pad(lubridate::month(startDate), 2, pad="0"),
                           stringr::str_pad(lubridate::day(startDate), 2, pad="0"), 
                           "-", 
                           lubridate::year(endDate), 
                           stringr::str_pad(lubridate::month(endDate), 2, pad="0"), 
                           stringr::str_pad(lubridate::day(endDate), 2, pad="0")
                           )
    }
    
    if (is.null(fileLoc)) {
        fileLoc <- paste0("./RInputFiles/", 
                          "metar_k", 
                          stringr::str_to_lower(stationID), 
                          "_", 
                          timeDesc, 
                          ".txt"
                          )
    }
    
    cat("\nData for station", stationID, "from", as.character(startDate), "to", 
        as.character(endDate), "will download to", fileLoc, "\n"
        )
    
    if (file.exists(fileLoc) & !ovrWrite) {
        stop("File already exists, aborting")
    }
    
    genericGetASOSData(fileLoc=fileLoc, stationID=stationID, startDate=startDate, endDate=endDate, ...)
    
}

```
  
Example downloads are created (cached to avoid multiple hits to Iowa State's server), with focus on additional 2016 data for Wintry cities to build on the later analyses in 'AdditionalCoding_202003_v002.Rmd':  
```{r cache=TRUE}
# Download 2016 weather data for Milwaukee, WI
getASOSStationTime(stationID="MKE", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Madison, WI
getASOSStationTime(stationID="MSN", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Green Bay, WI
getASOSStationTime(stationID="GRB", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Escanaba, MI
getASOSStationTime(stationID="ESC", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Traverse City, MI
getASOSStationTime(stationID="TVC", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Grand Rapids, MI
getASOSStationTime(stationID="GRR", analysisYears=2016, ovrWrite=TRUE)
```

```{r cache=TRUE}
# Download 2016 weather data for Indianapolis, IN
getASOSStationTime(stationID="IND", analysisYears=2016, ovrWrite=TRUE)
```
  
Confirmation can be obtained that the files downloaded as expected:  
```{r}

for (airport in c("kmke", "kmsn", "kgrb", "kesc", "ktvc", "kgrr", "kind")) {
    
    fileCheck <- paste0("./RInputFiles/metar_", airport, "_2016.txt")
    print(file.info(fileCheck)[c("size", "isdir", "mode", "mtime")])
    
}

```
  
The files are all in the 2-3 MB range, which is appropriate for a year of METAR data.
  
#### _Example #2: Finding the Hourly Reporting Time for METAR Data_  
The METAR are downloaded in raw CSV format, and require some data wrangling to extract a clean file in a common format.

One component of wrangling a METAR file is to specify the columns and expected data types.  The Iowa State data are contained in the below format, which is not necessarily standard to how other archives may store the information (Iowa State has run processing on the METAR file):  
```{r}

metType <- readr::cols(station=readr::col_character(), 
                       valid=readr::col_datetime(), 
                       tmpf=readr::col_double(),
                       dwpf=readr::col_double(),
                       relh=readr::col_double(),
                       drct=readr::col_double(),
                       sknt=readr::col_double(),
                       p01i=readr::col_character(),  # needs to handle 'T' for trace
                       alti=readr::col_double(),
                       mslp=readr::col_double(),
                       vsby=readr::col_double(),
                       gust=readr::col_double(),
                       skyc1=readr::col_character(),
                       skyc2=readr::col_character(), 
                       skyc3=readr::col_character(), 
                       skyc4=readr::col_character(),
                       skyl1=readr::col_double(),
                       skyl2=readr::col_double(),
                       skyl3=readr::col_double(),
                       skyl4=readr::col_double(),
                       wxcodes=readr::col_character(),
                       ice_accretion_1hr=readr::col_character(), # needs to handle 'T' for trace
                       ice_accretion_3hr=readr::col_character(), # needs to handle 'T' for trace
                       ice_accretion_6hr=readr::col_character(), # needs to handle 'T' for trace
                       peak_wind_gust=readr::col_double(),
                       peak_wind_drct=readr::col_double(),
                       peak_wind_time=readr::col_datetime(),
                       feel=readr::col_double(),
                       metar=readr::col_character()
                       )

```

Another component of wrangling a METAR file is to extract the hourly observations.  Since the primary use of a METAR is aviation safety, non-hourly updates are common when changes in clouds, winds, precipiation, and the like need to be communicated.  For analysis, though, it is more useful to have just the hourly summaries which contain the most salient information about the last hour in an easily digestable format.

Each airport reports hourly at the same number of minutes past the hour (each airport may have a different hourly reporting time, but the hourly reporting time for a given airport is constant all year).  In the US, this is commonly some time between 50-55 minutes past the hour.

The function findHourlyReportingTime() reads a METAR file and analyzes the most commonly occuring time:  
```{r}

findHourlyReportingTimes <- function(metFile=NULL, 
                                     metStation=NULL, 
                                     metTime=NULL, 
                                     metPath="./RInputFiles/metar_k", 
                                     metSep="_", 
                                     metExt=".txt",
                                     stationToLower=TRUE, 
                                     returnAll=FALSE,
                                     naValues=c("", "NA", "M"), 
                                     colTypes=metType
                                     ) {
    
    # Find the file name if it has not been passed
    if (is.null(metFile)) {
        if (is.null(metStation) | is.null(metPath) | is.null(metSep) | is.null(metTime) | is.null(metExt)) {
            stop("If metFile is not passed, all other parameters must be passed so filename can be created")
        }
        if (stationToLower) { metStation <- stringr::str_to_lower(metStation) }
        metFile <- paste0(metPath, metStation, metSep, metTime, metExt)
    }
    
    # Find the most common Zulu time (this will be the METAR)
    zTimes <- readr::read_csv(metFile, na=naValues, col_types=colTypes) %>%
        pull(metar) %>%
        stringr::str_match(pattern="\\d{2}Z") %>%
        as.vector() %>%
        table() %>%
        sort(decreasing=TRUE)

    # Print the most common 10 times    
    cat("\nMost common 10 times for file:", metFile, "\n")
    print(zTimes[1:10])

    # Print the estimated Zulu time
    cat("\nThe most common Zulu time for", metFile, "is", names(zTimes)[1], "\nFrequency is",
        round(100*zTimes[1]/sum(zTimes), 1), "% (", zTimes[1], "of", sum(zTimes), ")\n"
        )

    # Return the fill vector if requested (likely only if a problem is detected), otherwise return the time
    if (returnAll) {
        return(zTimes)
    } else {
        return(names(zTimes)[1])
    }
    
}

# The tidvverse library is needed for the remainder of the process
library(tidyverse)

# The most common hours are assessed
findHourlyReportingTimes(metStation="MKE", metTime=2016)
findHourlyReportingTimes(metStation="MSN", metTime=2016)
findHourlyReportingTimes(metStation="GRB", metTime=2016)
findHourlyReportingTimes(metStation="ESC", metTime=2016)
findHourlyReportingTimes(metStation="TVC", metTime=2016)
findHourlyReportingTimes(metStation="GRR", metTime=2016)
findHourlyReportingTimes(metStation="IND", metTime=2016)

```
  
With the exception of Escanaba, MI (ESC), the most common times seeem reasonable - greater than 75% of the observations are at a specific, recurring time past the hour in the early-mid 50s:  
  
* 52Z - Milwaukee (83%)  
* 53Z - Madison (79%), Green Bay (78%), Traverse City (79%), Grand Rapids (79%)  
* 54Z - Indianapolis (83%)  
  
Escanaba appears to have multiple reporting times, which may be driven by Escanaba being a much smaller airport that sees little if any scheduled air service.  As there are sufficient Wintry cities already for further analysis, and as Green Bay is less than 100 miles to the south, Escanaba will be excluded from any follow-on analysis.
  
The remaining 6 cities all have roughly 8,800 observations at the most common time.  There are 8,784 hours in a 366-day leap-year (2016), 1-2 extra days were pulled on either side of the year, an observation is expected every hour, and these are real-world systems where occasional missing archive data is not unexpected.  So, to a first order, data quantities of about 8,800 observations appear accurate.
  
#### _Example #3: Processing a METAR File_  
The METAR files can be processed, with data extracted based on the hourly time identified above.  The two key fields in the files are:  
  
* valid - the UTC date-time associated to the METAR  
* metar - the raw METAR data  
  
The other fields represent processed data which may be useful for other purposes.  But, the main purpose of this analysis is to extract and analyze METAR data.
  
Broadly speaking, there are several steps required to convert downloaded data for use in further analysis:  
  
* readMETAR() - extract only hourly observations, check for expected times and record uniqueness  
* initialParseMETAR() - parse the raw METAR data based on an expected regex pattern  
* convertMETAR() - converts characters from the parsed METAR file to meaningful numbers for analysis  
* fixMETAR() - corrects for issues with extracting and converting visibility, wind gusts, and SLP  
* getclouds() - extract cloud data from the METAR and determine cloud types, minimum heights, and minimum ceilings  
* bindMETAR() - put everything together for further analysis  
  
The readMETAR() function is designed to extract only the hourly observations.  It also checks that all of the expected times are found in the data and flags any differences.  If any unexpected times are found in the data, or if any of the data are not unique, an error is thrown:  
```{r}

# Function to make an initial read of the data, filter to METAR records, and check date-times
readMETAR <- function(fileName, 
                      timeZ,
                      expMin, 
                      expDays,
                      colTypes=NULL,
                      printSTR=FALSE,
                      errUnexpected=TRUE,
                      errNonUnique=TRUE
                      ) {

    # Read METAR data
    initRead <- readr::read_csv(fileName, na=c("", "NA", "M"), col_types=colTypes)
    
    # Provide descriptions of the METAR read (str only if flag is set)
    if (printSTR) { str(initRead, give.attr=FALSE) }
    dim(initRead)

    # Filter to only data that ends with times ending in timeZ
    filterRead <- initRead %>%
        filter(str_detect(metar, timeZ))
    dim(filterRead)

    # Check that the dates and times included are as expected
    expDate <- expMin + lubridate::hours(0:(24*expDays - 1))
    
    # Observations expected but not recorded
    cat("\n*** OBSERVATIONS EXPECTED BUT NOT RECORDED ***\n")
    print(as.POSIXct(setdiff(expDate, filterRead$valid), origin="1970-01-01", tz="UTC"))

    # Observations recorded but not expected
    unexpected <- as.POSIXct(setdiff(filterRead$valid, expDate), origin="1970-01-01", tz="UTC")
    cat("\n*** OBSERVATIONS RECORDED BUT NOT EXPECTED ***\n")
    print(unexpected)
    if (errUnexpected) {
        stopifnot(length(unexpected)==0)
    }

    # Confirmation of uniqueness
    allUnique <- length(unique(filterRead$valid)) == length(filterRead$valid)
    cat("\n*** Are the extracted records unique? ***\n", allUnique, "\n")
    if (errNonUnique) {
        stopifnot(allUnique)
    }
    
    # Return the dataset as a tibble
    tibble::as_tibble(filterRead)
}

```
  
As an example, readMETAR() is run for the Green Bay data:  
```{r}

readGRB <- readMETAR("./RInputFiles/metar_kgrb_2016.txt", 
                     timeZ="53Z", 
                     expMin=as.POSIXct("2015-12-31 00:53:00", tz="UTC"), 
                     expDays=368, 
                     colTypes=metType
                     )
str(readGRB, give.attr=FALSE)

```
  
The initialParseMETAR() function is designed to parse a METAR file based on an expected regex string.  The regex passed is specific to the parameters of interest for this project; different regex could be passed if different portions of the METAR were desired to be extracted:  
```{r}

# Code for the initial METAR parsing
initialParseMETAR <- function(met, val, labs, showParseSummary=FALSE, glimpseFinal=FALSE) {
    
    # Pull the METAR data
    metAll <- met %>%
        pull(metar)
    
    # Find the number of matching elements
    cat("\n*** Tentative Summary of Element Parsing *** \n")
    str_detect(metAll, pattern=val) %>% 
        table() %>%
        print()

    # The strings that do not match have errors in the raw data (typically, missing wind speed)
    cat("\n*** Data Not Matched *** \n")
    print(metAll[!str_detect(metAll, pattern=val)])

    # A matrix of string matches can be obtained
    mtxParse <- str_match(metAll, pattern=val)
    if (showParseSummary) {
        cat("\n*** Parsing matrix summary *** \n")
        print(dim(mtxParse))
        print(head(mtxParse))
    }

    # Create a data frame
    dfParse <- data.frame(mtxParse, stringsAsFactors=FALSE) %>%
        mutate(dtime=met$valid, origMETAR=met$metar)
    names(dfParse) <- c(labs, "dtime", "origMETAR")
    dfParse <- tibble::as_tibble(dfParse)
    if (glimpseFinal) {
        cat("\n*** Summary of the parsed data *** \n")
        glimpse(dfParse)
    }
    
    dfParse
}

```
  
The function can then be run against the Green Bay data:  
```{r}

# Create a search string for METAR
valMet <- "53Z.*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})"

# Create the names for the search string to parse in to
labsMet <- c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
             "TempC", "DewC", "Altimeter", "SLP", "FahrC"
             )

# Run the METAR parsing on the raw data
initGRB <- initialParseMETAR(readGRB, val=valMet, labs=labsMet, glimpseFinal=TRUE)

```
  
The convertMETAR() function is designed to convert the parsed data to numeric entries for further analysis.  Among the steps are:  
  
* WindSpeed - convert from character to numeric  
* WindGust - convert from character to numeric  
* Visibility - remove 'SM' and convert remainder to numeric  
* TempC - change 'M' (means negative) to '-' and convert to numeric
* DewC - change 'M' (means negative) to '-' and convert to numeric  
* Altimeter - remove 'A' and convert remainder to numeric  
* SLP - remove 'SLP' and convert remainder to numeric  
* TempF - take characters 2-5 of T(dddd)dddd, convert to -ddd if leading d is 1, convert to numeric, divide by 10, multiply by 1.8 and add 32  
* DewF - take characters 6-9 of Tdddd(dddd), convert to -ddd if leading d is 1, convert to numeric, divide by 10, multiply by 1.8 and add 32  
  
Function code below:  
```{r}

# Code for the conversion of METAR to meaningful numeric
# Function is hard-coded to work on metrics passed as default; potentially generalize later
convertMETAR <- function(met, 
                         metrics=c("WindDir", "WindSpeed", "WindGust", "Visibility", "TempC", 
                                   "DewC", "Altimeter", "SLP", "TempF", "DewF"
                                   ), 
                         seed=2003211416, 
                         showInvestigate=FALSE, 
                         showNACounts=TRUE,
                         showMetricPlots=FALSE
                         
                         ) {
    
    # Convert to numeric where appropriate
    dfParse <- met %>%
        mutate(WindSpeed = as.integer(WindSpeed), 
               WindGust = as.numeric(WindGust), 
               Visibility = as.numeric(str_replace(Visibility, "SM", "")),
               TempC = as.integer(str_replace(TempC, "M", "-")), 
               DewC = as.integer(str_replace(DewC, "M", "-")), 
               Altimeter = as.integer(str_replace(Altimeter, "A", "")), 
               SLP = as.integer(str_replace(SLP, "SLP", "")), 
               TempF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 2, 5), pattern="^1", "-"))/10, 
               DewF = 32 + 1.8 * as.integer(str_replace(str_sub(FahrC, 6, 9), pattern="^1", "-"))/10
               )

    # Investigate the data
    if (showInvestigate) {
        cat("\n *** Parsed data structure, head, tail, and random sample *** \n")
        str(dfParse)
        print(head(dfParse))
        print(tail(dfParse))
        set.seed(seed)
        dfParse %>% 
            sample_n(20) %>%
            print()
    }

    if (showNACounts) {
        # Check for NA values
        cat("\n *** Number of NA values *** \n")
        print(colSums(is.na(dfParse)))
    }
    
    # Plot of counts by key metric
    # NOTE - function has NOT YET been created in this module - placeholder
    if (showMetricPlots) {
        plotcountsByMetric(dfParse, mets=metrics)
    }
    
    # Return the parsed dataset
    dfParse
    
}

```
  
The function is run for the initial Green Bay data:  
```{r}

convGRB <- convertMETAR(initGRB)
glimpse(convGRB)

```

There are then three fixes required to the converted data:  
  
* Visibility is recorded as a fraction when visibility is low, such as 1/2SM - these cases need to be identified and converted properly, since the base regex would extract 2SM from the above  
* Wind gusts are never correctly extracted by convertMETAR(); a separate function is needed to identify only the records with dddddGddKT and to parse out the wind gusts from these  
* SLP is recorded as three digits which are know to be either the 900s for a high SLP or the 1000s for a low SLP; this needs to be modified accordingly  
  
The function fixMETAR() runs the functions getVisibility(), getWindGusts(), and fixSLP(), and integrated the updates in a consolidated file:  
```{r}

# Address the visibility issues
getVisibility <- function(curMet, origMet, var="metar") {
    
    # Get the original METAR data
    metAll <- origMet %>%
        pull(var)
    
    # Correct for visibility
    # Areas that have \\d \\d/\\dSM
    sm1 <- which(str_detect(metAll, pattern=" \\d/\\dSM"))
    sm2 <- which(str_detect(metAll, pattern=" \\d \\d/\\dSM"))

    valSM1 <- str_match(metAll, pattern="\\d/\\dSM")[sm1]
    valSM1 <- str_replace(valSM1, "SM", "")
    valSM1 <- as.integer(str_sub(valSM1, 1, 1)) / as.integer(str_sub(valSM1, 3, 3))

    valSM2 <- str_match(metAll, pattern=" \\d \\d/\\dSM")[sm2]
    valSM2 <- as.integer(str_sub(valSM2, 2, 2))

    curMet[sm1, "Visibility"] <- valSM1
    curMet[sm2, "Visibility"] <- curMet[sm2, "Visibility"] + valSM2

    visCounts <- curMet %>% 
        count(Visibility)
    visVector <- visCounts$n
    names(visVector) <- visCounts$Visibility
    cat("\nVisibilities after correcting for fractions:\n")
    print(visVector)
    cat("\n")
    
    curMet
}

# Correct for wind gusts
getWindGusts <- function(curMet, origMet, var="metar") {

    metAll <- origMet %>%
        pull(var)
    
    gustCheck <- which(str_detect(metAll, pattern="\\d{5}G\\d{2}KT"))
    valGust <- str_match(metAll, pattern="\\d{5}G\\d{2}KT")[gustCheck]
    valGust <- as.integer(str_sub(valGust, 7, 8))

    curMet[gustCheck, "WindGust"] <- valGust

    gustCounts <- curMet %>% 
        count(WindGust)
    gustVector <- gustCounts$n
    names(gustVector) <- gustCounts$WindGust
    cat("\nCounts of wind gusts extracted:\n")
    print(gustVector)
    cat("\n")
    
    curMet
}

# Correct for SLP
fixSLP <- function(curMet) {

    dfParse <- curMet %>%
        mutate(modSLP=ifelse(curMet$SLP < 500, 1000 + curMet$SLP/10, 900 + curMet$SLP/10))

    p <- dfParse %>%
        group_by(SLP, modSLP) %>%
        summarize(n=n()) %>%
        ggplot(aes(x=SLP, y=modSLP, size=n)) + 
        geom_point(alpha=0.3) + 
        labs(title="Correction of SLP in raw METAR to modSLP for further analysis", 
             x="SLP: Original Data", 
             y="modSLP: Converted for Analysis Data"
             )
    print(p)
    
    dfParse
}

fixMETAR <- function(met, 
                     origMet=met, 
                     var="origMETAR",
                     modifyVisibility=TRUE, 
                     modifyWindGusts=TRUE, 
                     modifySLP=TRUE
                     ) {
    
    if (modifyVisibility) {
        met <- getVisibility(met, origMet, var=var)
    }
    
    if (modifyWindGusts) {
        met <- getWindGusts(met, origMet, var=var)
    }
    
    if (modifySLP) {
        met <- fixSLP(met)
    }
    
    met
    
}

```
  
The function is then run to fix the processed Green Bay data:  
```{r}

fixGRB <- fixMETAR(convGRB)

```
  
Lastly, cloud data can be extracted from the raw METAR file.  The process is split in to two steps:  
  
* extractClouds() extracts all of the cloud data from a METAR file - clouds can be flagged as clear skies (CLR or SKC), indeterminate due to low vertical visibility (VV), FEW, SCT, BKN, OVC.  When the cloud type is any of VV, FEW, SCT, BKN, OVC, it is followed by ddd which is the height in hundreds of feet.  There can be more than one cloud layer, for example FEW008 SCT025 BKN100  
* findLowestClouds() extracts the lowest cloud level for each cloud type - for example, SCT025 SCT050 BKN100 has a lowest SCT (scattered) of 2500 feet and a lowest BKN (broken) of 10000 feet  
  
The function getClouds() runs these two functions:  
```{r}

extractClouds <- function(met, 
                          metVar, 
                          subT,
                          showInitCloudTable=TRUE,
                          showInitCloudMissing=TRUE
                          ) {

    metAll <- met %>%
        pull(metVar)
    
    # Extract the CLR records
    mtxCLR <- str_extract_all(metAll, pattern=" CLR ", simplify=TRUE)
    if (dim(mtxCLR)[[2]] != 1) { stop("Extracted 2+ CLR from some METAR; investigate") }
    isCLR <- ifelse(mtxCLR[, 1] == "", 0, 1)

    # Extract the VV records
    mtxVV <- str_extract_all(metAll, pattern="VV(\\d{3})", simplify=TRUE)
    if (dim(mtxVV)[[2]] > 1) { stop("Extracted 2+ VV from some METAR; investigate") }
    if ((dim(mtxVV))[[2]] == 0) {
        cat("\nNo Records with a cloud type of vertical visibility (VV)\n")
        isVV <- rep(0, times=length(isCLR))
        htVV <- rep(NA, times=length(isCLR))
    } else {
        isVV <- ifelse(mtxVV[, 1] == "", 0, 1)
        htVV <- ifelse(mtxVV[, 1] == "", NA, as.integer(str_replace(mtxVV[, 1], "VV", ""))*100)
    }

    # Extract the FEW records
    mtxFEW <- str_extract_all(metAll, pattern="FEW(\\d{3})", simplify=TRUE)
    numFEW <- apply(mtxFEW, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the SCT records
    mtxSCT <- str_extract_all(metAll, pattern="SCT(\\d{3})", simplify=TRUE)
    numSCT <- apply(mtxSCT, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the BKN records
    mtxBKN <- str_extract_all(metAll, pattern="BKN(\\d{3})", simplify=TRUE)
    numBKN <- apply(mtxBKN, 1, FUN=function(x) { sum((x!=""))} )

    # Extract the OVC records
    mtxOVC <- str_extract_all(metAll, pattern="OVC(\\d{3})", simplify=TRUE)
    numOVC <- apply(mtxOVC, 1, FUN=function(x) { sum((x!=""))} )

    # Summarize as a data frame
    tblClouds <- tibble::tibble(isCLR=isCLR, isVV=isVV, htVV=htVV, numFEW=numFEW, 
                                numSCT=numSCT, numBKN=numBKN, numOVC=numOVC
                                )

    # Get the counts
    if (showInitCloudTable) {
        cat("\n*** Counts by number of layers of each cloud type ***\n")
        tblClouds %>% 
            count(isCLR, isVV, numFEW, numSCT, numBKN, numOVC) %>%
            as.data.frame() %>%
            print()
    }
    
    # Investigate the problem data
    if (showInitCloudMissing) {
        cat("\n*** METAR records where no clouds were extracted ***\n")
        metAll[rowSums(tblClouds, na.rm=TRUE)==0] %>%
            print()
    }
    
    # Plot the counts of most obscuration
    p <- tblClouds %>%
        filter(rowSums(., na.rm=TRUE) > 0) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", isVV==1 ~ "VV", numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", numSCT > 0 ~ "SCT", numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            )
               ) %>%
        ggplot(aes(x=wType, y=..count../sum(..count..))) + 
        geom_bar() + 
        labs(title="Highest Obscuration by Cloud", subtitle=subT, 
             x="Cloud Type", y="Proportion of Hourly Measurements"
             )
    print(p)
    
    # Integrate the clouds data
    mtxCloud <- cbind(mtxVV, mtxOVC, mtxBKN, mtxSCT, mtxFEW, mtxCLR)
    cat("\n*** Dimensions for the cloud matrix ***\n")
    print(dim(mtxCloud))
    
    list(tblClouds=tblClouds, mtxCloud=mtxCloud)
}


# Helper function to cycle through to find levels of a given type
ckClouds <- function(cloudType, mtx) {
    isKey <- which(apply(mtx, 2, FUN=function(x) {sum(str_detect(x, cloudType))}) > 0)
    as.integer(str_replace(mtx[, min(isKey)], cloudType, "")) * 100
}


# Function to create the lowest cloud levels
findLowestClouds <- function(mtxCloud, 
                             subT="Lincoln, NE (2016) Hourly METAR", 
                             showExample=TRUE, 
                             showPlots=TRUE
                             ) {

    # Find the lowest clouds by cloud type
    lowOVC <- ckClouds("OVC", mtx=mtxCloud)
    lowVV <- ckClouds("VV", mtx=mtxCloud)
    lowBKN <- ckClouds("BKN", mtx=mtxCloud)
    lowSCT <- ckClouds("SCT", mtx=mtxCloud)
    lowFEW <- ckClouds("FEW", mtx=mtxCloud)

    # Integrate the lowest cloud type by level
    lowCloud <- tibble::tibble(lowVV, lowOVC, lowBKN, lowSCT, lowFEW)
    if (showExample) {
        cat("\n*** Lowest clouds by type tibble ***\n")
        print(lowCloud)
    }

    # Get the lowest cloud level
    minCloud <- lowCloud
    minCloud[is.na(minCloud)] <- 999999
    minCloudLevel <- apply(minCloud, 1, FUN=min)
    minCeilingLevel <- apply(minCloud[, c("lowVV", "lowOVC", "lowBKN")], 1, FUN=min)

    noCloudPct <- mean(minCloudLevel == 999999)
    noCeilingPct <- mean(minCeilingLevel == 999999)

    # Plot the minimum cloud level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCloudLevel != 999999) %>%
        ggplot(aes(x=minCloudLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCloudPct), "% of obs. have no clouds")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Cloud Height (when some clouds exist)", subtitle=subT
             )
    if (showPlots) print(p)

    # Plot the minimum ceiling level (where it exists)
    p <- data.frame(minCloudLevel, minCeilingLevel) %>%
        filter(minCeilingLevel != 999999) %>%
        ggplot(aes(x=minCeilingLevel)) + 
        geom_bar(aes(y=..count../sum(..count..))) + 
        geom_text(aes(x=2500, y=0.04, 
                      label=paste0(round(100*noCeilingPct), "% of obs. have no ceiling")
                      )
                  ) + 
        labs(x="Height [ft]", y="Proportion", 
             title="Minimum Ceiling Height (when a ceiling exists)", subtitle=subT
             )
    if (showPlots) print(p)
    
    list(lowCloud=lowCloud, minCeilingLevel=minCeilingLevel, minCloudLevel=minCloudLevel)
}

# Calls extractClouds() and findLowestClouds()
getClouds <- function(met, 
                      cityName, 
                      metVar="origMETAR",
                      showInitCloudTable=FALSE, 
                      showInitCloudMissing=TRUE, 
                      showExample=FALSE,
                      showPlots=TRUE
                      ) {
    
    # Run the initial cloud extraction
    initClouds <- extractClouds(met, 
                                metVar=metVar, 
                                subT=paste0(cityName, " Hourly METAR"), 
                                showInitCloudTable=showInitCloudTable, 
                                showInitCloudMissing=showInitCloudMissing
                                )
    
    # Find the lowest cloud levels of each cloud type
    processedClouds <-findLowestClouds(initClouds$mtxCloud, 
                                       subT=paste0(cityName, " Hourly METAR"), 
                                       showExample=showExample, 
                                       showPlots=showPlots
                                       )

    list(initClouds=initClouds, processedClouds=processedClouds)
    
}

```
  
The functions are then run for the Green Bay data:  
```{r}

cloudsGRB <- getClouds(fixGRB, cityName="Green Bay, WI (2016)")

```
  
The bindMETAR() function then puts everything together:  
```{r}

# Function to bind the existing parsed METAR data with the cloud data
bindMETAR <- function(dfParse, tblClouds, lowCloud) {

    # Integrate the cloud data and convert month to a factor
    dfFull <- cbind(dfParse, tblClouds, lowCloud) %>%
        mutate(wType=factor(case_when(isCLR==1 ~ "CLR", 
                                      isVV==1 ~ "VV", 
                                      numOVC > 0 ~ "OVC", 
                                      numBKN > 0 ~ "BKN", 
                                      numSCT > 0 ~ "SCT", 
                                      numFEW > 0 ~ "FEW", 
                                      TRUE ~ "Error"
                                      ), 
                            levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR", "Error")
                            ), 
               month=factor(lubridate::month(dtime), levels=1:12, labels=month.abb)
               )
    
    dfFull <- tibble::as_tibble(dfFull)
    glimpse(dfFull)
    
    dfFull
}

```
  
The function is run for the Green Bay data:  
```{r}

fullGRB <- bindMETAR(fixGRB, 
                     tblClouds=cloudsGRB$initClouds$tblClouds, 
                     lowCloud=cloudsGRB$processedClouds$lowCloud
                     )

```
  
A fully processed file for Green Bay, WI (2016) is now available for further analysis.
  
