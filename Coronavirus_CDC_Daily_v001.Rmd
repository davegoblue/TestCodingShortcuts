---
title: "CDC Daily by State"
author: "davegoblue"
date: "4/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
This file is designed to use CDC data to assess coronavirus disease burden by state, including creating and analyzing state-level cluters.

Through March 7, 2021, [The COVID Tracking Project](https://covidtracking.com/) collected and integrated data on tests, cases, hospitalizations, deaths, and the like by state and date.  The latest code for using this data is available in Coronavirus_Statistics_CTP_v004.Rmd.

The COVID Tracking Project suggest that [US federal data sources](https://covidtracking.com/analysis-updates/federal-covid-data-101-how-to-find-data) are now sufficiently robust to be used for analyses that previously relied on COVID Tracking Project.  This code is an attempt to update modules in Coronavirus_Statistics_CTP_v004.Rmd to leverage US federal data.

The code leverages tidyverse and a variable mapping file throughout:  
```{r}

# All functions assume that tidyverse and its components are loaded and available
# Other functions are declared in the sourcing files or use library::function()
library(tidyverse)

# For future use, source key modules from a separate .R file

# Create a variable mapping file - this is not yest updated for federal data
varMapper <- c("cases"="Cases",
               "tot_cases"="Total cases",
               "newCases"="Increase in cases, most recent 30 days",
               "casesroll7"="Rolling 7-day mean cases", 
               "deaths"="Deaths", 
               "tot_deaths"="Total deaths",
               "newDeaths"="Increase in deaths, most recent 30 days",
               "deathsroll7"="Rolling 7-day mean deaths", 
               "cpm"="Cases per million",
               "cpm7"="Cases per day (7-day rolling mean) per million", 
               "tcpm"="Total cases per million",
               "newcpm"="Increase in cases, most recent 30 days, per million",
               "dpm"="Deaths per million", 
               "dpm7"="Deaths per day (7-day rolling mean) per million", 
               "tdpm"="Total deaths per million", 
               "newdpm"="Increase in deaths, most recent 30 days, per million", 
               "hpm7"="Currently Hospitalized per million (7-day rolling mean)", 
               "tpm"="Tests per million", 
               "tpm7"="Tests per million per day (7-day rolling mean)"
               )

# Helper functions used early in the process
# Function for saving an R object to RDS, including a check for whether the object already exists
saveToRDS <- function(obj, 
                      file=paste0(deparse(substitute(obj)), ".RDS"), 
                      dir="./RInputFiles/Coronavirus/", 
                      ovrWrite=FALSE, 
                      ovrWriteError=TRUE,
                      makeReadOnly=TRUE
                      ) {
    
    # FUNCTION ARGUMENTS:
    # obj: the R object to save
    # file: the file name to save as
    # dir: the directory to save in (file path will be paste0(dir, file))
    # ovrWrite: boolean, should the file be overwritten if it already exists?
    # ovrWriteError: boolean, should an error be thrown if an attempt is made to overwrite the file?
    # makeReadOnly: boolean, should the output file be made read-only?
    
    # Create the file name
    locFile <- paste0(dir, file)
    
    # Check if the file already exists and proceed as per options
    if (file.exists(locFile)) {
        cat("\nFile already exists:", locFile, "\n")
        if (!ovrWrite & ovrWriteError) stop("\nAborting due to ovrWrite=FALSE and ovrWriteError=TRUE")
        if (!ovrWrite) {
            cat("\nNot replacing the existing file since ovrWrite=FALSE\n")
            return(NULL)
        }
    }
    
    # Save the file and update the permissions to read-only (if flag is set)
    saveRDS(obj, file=locFile)
    if (makeReadOnly) Sys.chmod(locFile, mode="0555", use_umask = FALSE)
    
}



# Function for reading an R object from RDS
readFromRDS <- function(file, 
                        dir="./RInputFiles/Coronavirus/", 
                        addSuffix=".RDS", 
                        deparseSub=FALSE
                        ) {
    
    # FUNCTION ARGUMENTS:
    # file: the file name to read in
    # dir: the directory the file is in
    # addSuffix: the suffix that should be added to file (file path will be paste0(dir, file, addSuffix))
    # deparseSub: whether to deparse and substitute file (use it as the text name)
    
    # Convert file if needed
    if (deparseSub) file <- deparse(substitute(file))
    
    # Ensure that file is of type character
    if (!isTRUE(all.equal(class(file), "character"))) {
        stop("\nUnable to read since file is not a character\n")
    }
    
    # Create the file name
    locFile <- paste0(dir, file, addSuffix)
    
    # Read the file (will be the return)
    readRDS(locFile)
    
}


```
  
### Downloading and Integrating US Federal Data  
#### _Cases and Deaths by State-Date_
CDC case and death data by state and date are available for download on the [CDC website](https://data.cdc.gov/api/views/9mfq-cb36/rows.csv?accessType=DOWNLOAD).  The previously written function downloadCOVIDbyState() can be used to acquire the data by updating the API:  
```{r}

# NO CHANGES MADE TO FUNCTION - default API is from COVID Tracking Project
# Function to download data for COVID Tracking Project
downloadCOVIDbyState <- function(fileName, 
                                 api="https://api.covidtracking.com/v1/states/daily.csv", 
                                 ovrWrite=FALSE
                                 ) {
    
    # FUNCTION ARGUMENTS:
    # fileName: the filename that the data will be saved to
    # api: The API link for data downloads
    # ovrWrite: whether to allow overwriting of the existing fileName
    
    # Check whether fileName already exists
    if (file.exists(fileName)) {
        cat("\nFile already exists at:", fileName, "\n")
        if (ovrWrite) cat("Will over-write with current data from", api, "\n")
        else stop("Exiting due to ovrWrite=FALSE and a duplicate fileName\n")
    }
    
    # Download the file 
    download.file(api, destfile=fileName)
    
    # Show statistics on downloaded file
    file.info(fileName)
    
}


```

The data are downloaded and the process cached to avoid repeated hits against the CDC website:  
```{r cache=TRUE}

downloadCOVIDbyState("./RInputFiles/Coronavirus/CDC_dc_downloaded_210414.csv",
                     api="https://data.cdc.gov/api/views/9mfq-cb36/rows.csv?accessType=DOWNLOAD", 
                     ovrWrite=FALSE
                     )

```

Key fields from the documentation include:  
  
* submission_date - date of counts  
* state - state (includes 50 states, DC, NYC, and 8 territories/federations)  
* tot_cases - cumulative number of cases (confirmed and probable)  
* new_case - new cases (confirmed and probable)  
* tot_death - cumulative number of deaths (confirmed and probable)  
* new_death - new deaths (confirmed and probable)  
* consent_cases - boolean tracked as "Agree" (confirmed and probable tracked separately) or "Not Agree" (only total tracked)  
* consent_deaths - boolean tracked as "Agree" or "Not Agree" (same as for consent_cases)  
  
Basic formatting and QC is run on the downloaded data (this can later be converted to functional form):  
```{r}

# Read and glimpse downloaded CDC file
cdcRaw <- readr::read_csv("./RInputFiles/Coronavirus/CDC_dc_downloaded_210414.csv")
glimpse(cdcRaw)

# Check for variables in the consent_ fields
cdcRaw %>% 
    count(consent_cases, consent_deaths)

# Function to convert N/A, Agree, and Not agree to boolean
cdcToBool <- function(x) {
    y <- case_when(is.na(x) ~ "NA", 
                   x=="N/A" ~ "NA", 
                   x=="Agree" ~ "TRUE", 
                   x=="Not agree" ~ "FALSE", 
                   TRUE ~ "Problem"
                   )
    if (sum(y=="Problem") != 0) stop("Problem with the boolean conversion")
    y[y=="NA"] <- NA
    as.logical(y)
}

# Format fields as desired types
cdcProcessed <- cdcRaw %>%
    mutate(date=lubridate::mdy(submission_date), 
           creation_date=lubridate::mdy_hms(created_at), 
           bool_c_cases=cdcToBool(consent_cases), 
           bool_c_deaths=cdcToBool(consent_deaths)
           )
glimpse(cdcProcessed)

# Get control totals by state for numeric fields
cdcControl <- cdcProcessed %>%
    group_by(state) %>%
    summarize_if(is.numeric, .funs=function(x) sum(x, na.rm=TRUE))

# Plot control totals by state for numeric fields
for (keyVar in names(cdcControl)[2:ncol(cdcControl)]) {
    p1 <- cdcControl %>%
        select_at(vars(all_of(c("state", keyVar)))) %>%
        purrr::set_names(c("state", "y")) %>%
        ggplot(aes(x=fct_reorder(state, y), y=y)) + 
        geom_col(fill="lightblue") + 
        geom_text(aes(y=y/2, label=format(y, big.mark=",")), size=3, hjust=0) +
        labs(x="", y="", title=paste0("Control totals by state for ", keyVar)) + 
        coord_flip()
    print(p1)
}

```
  
Not every state reports on every metric.  In particular, some jurisdictions break cases and deaths in to probable and confirmed while others do not.  All states appear to report both total (cumulative) and new case.  Comparisons of these fields are run, since restatements of history can lead to total and cumsum(new) being different:  
```{r}

# Check for alignment of total and sum(new)
cdcMism <- cdcProcessed %>%
    arrange(state, date) %>%
    group_by(state) %>%
    mutate(ck_tot_case=cumsum(ifelse(is.na(new_case), 0, new_case)), 
           ck_tot_death=cumsum(ifelse(is.na(new_death), 0, new_death))
           ) %>%
    select(date, state, tot_cases, ck_tot_case, tot_death, ck_tot_death, everything()) %>%
    mutate(mism_case=tot_cases != ck_tot_case, mism_death=tot_death != ck_tot_death) %>%
    group_by(state) %>%
    summarize(mism_case=sum(mism_case), mism_death=sum(mism_death), .groups="drop") %>%
    filter(mism_death > 0 | mism_case > 0) %>%
    arrange(-mism_case, -mism_death)
cdcMism

```

Many states have data that are aligned throughout.  In some states, there are differences between the total and new fields that should be explored further.  A function is written to assess mismatches in the data:  
```{r}

# Function to report totals and plot trends by state
assessMismatch <- function(keyStates, 
                           keyMetric="cases",
                           df=cdcProcessed
                           ) {
    
    # FUNCTION ARGUMENTS
    # keyStates: states to be explored for differences
    # keyMetric: metric to be explored
    # df: the data frame containing data by state-date
    
    # Create a main database for comparisons
    dfUse <- cdcProcessed %>%
        arrange(state, date) %>%
        group_by(state) %>%
        mutate(ck_tot_case=cumsum(ifelse(is.na(new_case), 0, new_case)), 
               ck_tot_death=cumsum(ifelse(is.na(new_death), 0, new_death)), 
               d_cases=ck_tot_case-tot_cases,
               d_deaths=ck_tot_death-tot_death
               ) %>%
        select(date, 
               state, 
               cases=tot_cases, 
               ck_cases=ck_tot_case, 
               d_cases,
               deaths=tot_death, 
               ck_deaths=ck_tot_death, 
               d_deaths,
               everything()
               )
    
    # Show the discrepancies in the final data for each state
    dfUse %>%
        filter(state %in% all_of(keyStates)) %>%
        group_by(state) %>%
        filter(row_number()==n()) %>% 
        select(date, state, cases, ck_cases, d_cases, deaths, ck_deaths, d_deaths) %>%
        print()
    
    # Create plot of metric evolution
    dfPlot <- dfUse %>%
        filter(state %in% all_of(keyStates)) %>%
        select(date, state, cases, ck_cases, d_cases, deaths, ck_deaths, d_deaths) %>%
        pivot_longer(-c(date, state)) %>%
        filter(str_detect(name, keyMetric)) 
    p1 <- dfPlot %>%
        filter(!str_detect(name, "d_")) %>%
        mutate(useName=ifelse(str_detect(name, "ck_"), "cumsum(new)", "reported total")) %>%
        ggplot(aes(x=date, y=value)) + 
        geom_line(aes(group=useName, color=useName)) +
        labs(x="", 
             y="Cumulative reported", 
             title=paste0("Discrepancies in cumulative total ", keyMetric)
             ) +
        scale_color_discrete("Source") +
        facet_wrap(~state, scales="free_y")
    print(p1)
    p2 <- dfPlot %>%
        filter(str_detect(name, "d_")) %>%
        ggplot(aes(x=date, y=value)) + 
        geom_line(aes(group=state, color=state)) +
        labs(x="", 
             y="Cumulative discrepancy", 
             title=paste0("Discrepancies in cumulative total ", keyMetric)
             ) +
        scale_color_discrete("State")
    print(p2)
    
}

```
  
The function can then be applied to the case and death data with mismatches:  
```{r}

assessMismatch(keyStates=cdcMism %>% filter(mism_case > 0) %>% pull(state), keyMetric="cases")
assessMismatch(keyStates=cdcMism %>% filter(mism_death > 0) %>% pull(state), keyMetric="deaths")

```

The mismathes appear to arise at discrete points in time, likely reflecting a reclassification of many previous cases and deaths.  The total field is always greater than or equal to the sum of the new field.  This sugests using 'new' for shape of the curve and 'total' for overall disease burden.

A function is then written to rename and split columns appropriately:  
```{r}

# Create list of expected variables and renames (NA means keep as-is)
cdcVarNames <- c("date"=NA, 
                 "state"=NA, 
                 "tot_cases"=NA, 
                 "conf_cases"=NA, 
                 "prob_cases"=NA, 
                 "new_case"="new_cases", 
                 "pnew_case"="pnew_cases", 
                 "tot_death"="tot_deaths", 
                 "conf_death"="conf_deaths", 
                 "prob_death"="prob_deaths", 
                 "new_death"="new_deaths", 
                 "pnew_death"="pnew_deaths",
                 "bool_c_cases"="bool_cases", 
                 "bool_c_deaths"="bool_deaths"
                 )

# Function to rename variables, split, and pivot for easier analysis
renamePivotProcessed <- function(df, 
                                 selectRename=cdcVarNames
                                 ) {
    
    # FUNCTION ARGUMENTS
    # df: a processed CDC data file
    # selectRename: a list of variable -> new name (NA means keep as-is) as c('original'='new')
    
    # Check alignment of variables in df and selectRename
    dfNames <- names(df)
    selNames <- names(selectRename)
    
    # Create the vector of renamed variables after selection
    selRenames <- unname(selectRename[selNames])
    selRenames[is.na(selRenames)] <- selNames[is.na(selRenames)]
    
    # Names in one but not the other
    cat("\n*** Variables that will be dropped (not in selectRename vector) include:", 
        setdiff(dfNames, selNames), 
        sep="\n"
        )
    cat("\n\n*** Variables passed in selectRename that are not in the data include:", 
        setdiff(selNames, dfNames), 
        sep="\n"
        )
    
    # Select the key variables and rename
    df <- df %>%
        select_at(vars(all_of(selNames))) %>%
        purrr::set_names(selRenames)

    # Pivot and separate the data, keeping unique by date-state
    # Requires that selectRename have every variable as modifier_type
    df <- df %>%
        pivot_longer(-c(date, state)) %>%
        tidyr::separate(name, into=c("modifier", "metric"), sep="_")

    # Summary NA statistics of the new dataset
    cat("\nSummary statistics for processed and pivoted data\n")
    df %>%
        group_by(modifier, metric) %>%
        summarize(n=n(), nna=sum(is.na(value)), sum=sum(value, na.rm=TRUE), .groups="drop") %>%
        print()
    cat("\n")
    
    # Return the data frane
    df
    
}

```
  
The function is applied to create cdcPivotLong:  
```{r}

cdcPivotLong <- renamePivotProcessed(cdcProcessed)
glimpse(cdcPivotLong)

```

Next steps are to combine the NYS/NYC data as NY and to filter to the states and DC.  A function is written to combine states (will be applied only to NY and NYC for these data):  
```{r}

# Function to combine states
combineStates <- function(df, 
                          mapper=comboStates,
                          sortBy=c("date", "state", "modifier", "metric"),
                          boolMod=c("bool")
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: a data frame resulting from renamePivotProcessed() that includes date-state-modifier-metric-value
    # mapper: a vector listing the states to be remapped
    # sortBy: order that the resulting file should be sorted (and unique) by
    # boolMod: modifier indicating value is a boolean rather than number (cannot be summed)
    
    # Split the file in to stand-alone and combined
    df <- df %>%
        mutate(combined=state %in% names(mapper))
    dfAlone <- df %>% filter(!combined)
    dfCombo <- df %>% filter(combined)
    
    # Process the combined file to give it the new state name
    # Group by the final sorting variables
    dfCombo <- dfCombo %>%
        mutate(state=unname(comboStates[state])) %>%
        group_by_at(vars(all_of(sortBy))) %>%
        arrange_at(vars(all_of(sortBy)))
    
    # Numeric variables should be summed, with NA+NA=NA and NA+number=number
    specNASum <- function(x) if (any(!is.na(x))) sum(x, na.rm=TRUE) else NA

    # Sum the value field, leave the combined field as-is
    dfCombo <- dfCombo %>%
        summarize(value=specNASum(value), combined=first(combined), .groups="drop")
    
    # Boolean variables should be output as NA since there is no sensible combining based on what it means
    dfCombo <- dfCombo %>%
        mutate(value=ifelse(modifier %in% all_of(boolMod), NA, value))
    
    # Combine the data, sort, check for uniqueness, and return
    dfAll <- bind_rows(dfCombo, dfAlone) %>%
        arrange_at(vars(all_of(sortBy)))
    if (nrow(distinct(dfAll %>% select_at(vars(all_of(sortBy))))) != nrow(dfAll)) stop("\nRow mismatch\n")
    dfAll
    
}

```
  
The function is then applied to the cdcPivotLong data:  
```{r}

# List of states to be changed 
# Also include any state that is mapped to itself with some other state mapping to it
# Format is 'currentState'='combinedState'
comboStates <- c("NYC"="NY", 
                 "NY"="NY"
                 )

# Run function
cdcAnalysis <- combineStates(cdcPivotLong)

# Check for prevalenece of NA by metric-modifier
cdcAnalysis %>%
    group_by(metric, modifier) %>%
    summarize(sumValue=sum(value, na.rm=TRUE), n=n(), nna=sum(is.na(value)), nComb=sum(combined))

```
  
The fields for tot/new by cases/deaths appear appropriate for further usage.  Next steps are to compare the totals to existing data from the CTP (final date 2021-03-07).

The final CTP data are pulled, with cumulative data through 2021-03-07 calculated.  Cumulative data through 2021-03-07 from the CDC file are also extracted:  
```{r}

# Read in existing data from CTP
ctp_hier6_210401 <- readFromRDS("ctp_hier6_210401")

# Totals through 2021-03-07 for CTP (final date of collection)
ctp_cum_210307 <- ctp_hier6_210401$dfFiltered %>%
    group_by(state) %>%
    summarize_if(is.numeric, sum, na.rm=TRUE) %>%
    pivot_longer(-c(state), names_to="metric") %>%
    mutate(modifier="ctp") %>%
    select(state, modifier, metric, value, everything())
ctp_cum_210307

# Totals through 2021-03-07 for CDC (to match CTP data)
# Uses final tot-cases, final tot-deaths, cumsum new-cases, cumsum new-deaths
cdc_cum_210307 <- cdcAnalysis %>%
    filter(date <= as.Date("2021-03-07"), 
           metric %in% c("cases", "deaths"), 
           modifier %in% c("new", "tot"), 
           state %in% c(state.abb, "DC")
           ) %>%
    pivot_wider(c(state, date, metric), names_from="modifier", values_from="value") %>%
    group_by(state, metric) %>%
    summarize(new=sum(new, na.rm=TRUE), tot=last(tot, order_by=date), .groups="drop") %>%
    pivot_longer(-c(state, metric), names_to="modifier") %>%
    select(state, modifier, metric, value, everything())
cdc_cum_210307

```

Next steps are to write a function to pull a specific metric and plot the differences in a 'base' modifier and any other values for modifier.

A function is written to compare data from different sources:  
```{r}

# Function to assess variables new_x, tot_x, and x for percentage differences
checkStateCompare <- function(..., 
                              met="cases", 
                              baseModifier="ctp"
                              ) {
    
    # FUNCTION ARGUMENTS:
    # ...: one or more data frames of the form state-modifier-metric-value
    # met: the metric to be used, "cases" or "deaths"
    # baseModifier: value of the modifier field that signifies the baseline for comparison
    
    # Bind the data frames, and calculate as a percentage of the "base" modifier
    df <- bind_rows(...) %>%
        filter(metric %in% all_of(met)) %>%
        arrange(state, modifier) %>%
        group_by(state) %>%
        mutate(pct_of=value/sum(ifelse(modifier==all_of(baseModifier), value, 0))) %>%
        ungroup()

    # Plot the percentage differences for fields other than reference
    p1 <- df %>%
        filter(modifier != all_of(baseModifier)) %>%
        ggplot(aes(x=fct_reorder(state, pct_of), 
                   y=pct_of, 
                   color=modifier
                   )
               ) + 
        geom_point() + 
        coord_flip() + 
        labs(x="", 
             y=paste0("Reported ", met, " as function of baseline data from ", baseModifier), 
             title=paste0("Comparison to baseline data for ", met)
             ) + 
        scale_color_discrete("Comparison metric") + 
        geom_hline(yintercept=1, lty=2)
    print(p1)
    
    # Return the data frame
    df
    
}

```

The function can then be applied to the cases and deaths data:  
```{r}

# Run comparisons for cases and deaths
caseCompare <- checkStateCompare(ctp_cum_210307, cdc_cum_210307, met="cases")
deathCompare <- checkStateCompare(ctp_cum_210307, cdc_cum_210307, met="deaths")

# Print cases and deaths that are at least 3% different
caseCompare %>%
    filter(abs(1-pct_of) >= 0.03)
deathCompare %>%
    filter(abs(1-pct_of) >= 0.03)

```

Data are well aligned between CTP and CDC as of 2021-03-07.  Primary differences include:  
  
* CDC reports significantly greater cases for Iowa (+20%) and Missouri (+17%)  
* CDC reports modestly lower cases for Massachusetts (-4%) and Hawaii (-6%)  
* CDC reports significantly more deaths for Oklahoma (+46%) and New York (+13% on sum of 'new', +23% on 'tot)  
* CDC reports modestly more deaths for Texas (+5%)  
* CDC reports significantly lower deaths for Indiana (-11% using 'new'), Ohio (-21% using 'new'), and New Jersey (-8% using 'new'), though sums of 'tot' for these states appear well aligned to the CTP data  
  
In general, the CDC 'tot' field appears well aligned to the cumulative totals from CTP, and can likely be used on a go-forward as a measure of cumulative disease impact in each state.

Next steps are to check the curve evolutions, particularly since states with differences in 'new' and 'tot' have large corrections all reported at once.

A function is written to compare the shapes of the curves.  Curve shape will be assessed as the percentage distribution of 'new' cases and deaths by time:  
```{r}

curvePercent <- function(df, 
                         keyMetric, 
                         keyModifiers=c("new", "ctp"), 
                         dateMax=as.Date("2021-03-07")
                         ) {
  
 
    # FUNCTION ARGUMENTS:
    # df: the data frame containing date-state-modifier-metric-value
    
    # Filter for the metric and modifiers of interest and only through dateMax
    df <- df %>%
        filter(metric %in% all_of(keyMetric), 
               modifier %in% all_of(keyModifiers), 
               date <= all_of(dateMax)
               )
    
    # Calculate the cumulative percentage by date
    df <- df %>%
        mutate(value=ifelse(is.na(value), 0, value)) %>%
        arrange(date) %>%
        group_by(state, metric, modifier) %>%
        mutate(pct=cumsum(value)/sum(value)) %>%
        ungroup()
    
    # Pivot for percentages by state-date-metric
    df <- df %>%
        pivot_wider(c(state, date, metric), names_from="modifier", values_from="pct") %>%
        mutate_at(vars(all_of(keyModifiers)), .funs=function(x) ifelse(is.na(x), 0, x))
    
    # Calculate RMSE for each state-metric (assumes only two columns - can expand later)
    stateRMSE <- df %>%
        group_by(state, metric) %>%
        summarize(rmse=sqrt(mean((get(keyModifiers[1])-get(keyModifiers[2]))**2)), .groups="drop")

    # Create plots of RMSE by state
    p1 <- stateRMSE %>%
        ggplot(aes(x=fct_reorder(state, rmse), y=rmse)) + 
        geom_point() +
        geom_text(aes(label=round(rmse, 3), y=rmse+0.00025), hjust=0, size=3) +
        coord_flip() + 
        labs(x="", 
             y="RMSE for cumulative percentage by date", 
             title=paste0("Curve comparison for ", keyMetric)
             )
    print(p1)
    
    # Return the data frame
    df
    
}

```
  
The CTP data are formatted for use with the function, integrated with cdcPivotLong, and assessed:  
```{r}

fullPivotLong <- ctp_hier6_210401$dfFiltered %>%
    pivot_longer(-c(date, state), names_to="metric") %>%
    mutate(modifier="ctp") %>%
    bind_rows(cdcPivotLong) %>%
    filter(state %in% c(state.abb, "DC"))

curveCases <- curvePercent(fullPivotLong, keyMetric="cases")
curveDeath <- curvePercent(fullPivotLong, keyMetric="deaths")

```

Many of the same locales that have differences between the 'new' and 'total' fields in CDC also have differences in curve shape with CTP.  Next steps are to explore some of the larger differences.

A function is written to explore curves for a speciic metric and set of states:  
```{r}

exploreCurve <- function(df, 
                         keyMetric=NULL, 
                         keyStates=NULL
                         ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame containing state-date-metric-[1 column per metric type]
    # keyMetric: the metric to filter in column 'metric' (NULL means determine from data)
    # keyStates: the list of states to be plotted (NULL means pick the top 9 from data)
    
    # Get keyMetric if it is passed as NULL
    if (is.null(keyMetric)) {
        keyMetric <- df %>% count(metric) %>% arrange(-n) %>% head(1) %>% pull(metric)
        cat("\nKey metric will be: ", keyMetric)
    }
    
    # Get the numerical columns for plotting
    colPlot <- df %>% select_if(is.numeric) %>% names()
    cat("\nColumns to be plotted will be: ", colPlot, "\n")
    
    # Get keyStates if not passed
    if (is.null(keyStates)) {
        keyStates <- df %>%
            filter(metric==all_of(keyMetric)) %>%
            group_by(state) %>%
            summarize(rmse=sqrt(mean((get(colPlot[1])-get(colPlot[2]))**2)), .groups="drop") %>%
            arrange(-rmse) %>%
            head(9) %>%
            pull(state)
        cat("States to be plotted will be: ", keyStates, "\n")
    }
    
    # Create faceted plots for the requested metrics
    dfPlot <- df %>%
        filter(metric==all_of(keyMetric), state %in% all_of(keyStates)) %>%
        select_at(vars(all_of(c("state", "date", colPlot)))) %>%
        mutate(state=factor(state, levels=keyStates)) %>%
        pivot_longer(-c("state", "date")) %>%
        arrange(state, name, date) %>%
        group_by(state, name) %>%
        mutate(delta=ifelse(row_number()==1, value, value-lag(value, 1)))
    p1 <- dfPlot %>%
        ggplot(aes(x=date, y=value)) + 
        geom_line(aes(group=name, color=name)) + 
        facet_wrap(~state) + 
        labs(x="", 
             y="Cumulative percentage of total recorded by date", 
             title=paste0("Curve evolution by percentage for metric: ", keyMetric), 
             subtitle="Cumulative"
             ) + 
        scale_color_discrete("Data source")
    p2 <- dfPlot %>%
        ggplot(aes(x=date, y=delta)) + 
        geom_line(aes(group=name, color=name)) + 
        facet_wrap(~state, scales="free_y") + 
        labs(x="", 
             y="Incremental percentage of total recorded by date", 
             title=paste0("Curve evolution by percentage for metric: ", keyMetric), 
             subtitle="Incremental"
             ) + 
        scale_color_discrete("Data source")
    print(p1)
    print(p2)
    
}

```

The routine is then run for cases and deaths, with a larger plotting area:  
```{r, fig.height=9, fig.width=9}

exploreCurve(curveCases)
exploreCurve(curveDeath)

```

Visually, the shapes of the case curves are nearly overlapping, and segments based using shape will likely be the same whether using CTP or CDC data.  The shapes of the death curves vary meaningfully in NY and modestly in OH, OK, and IN.  While the different curve evolutions are unlikely to drive different segments, it is worth keeping an eye on (particularly for NY).

Next steps are to adapt the CDC data and existing CTP code so they can be used together.

#### _Coding Steps for State Data_
The main function used previously is readRunCOVIDTrackingProject(), which performs multiple tasks:  
  
STEP 1: Extracts a file of population by state (by default uses 2015 population from usmap::statepop)  
STEP 2a^: Downloads the latest data from COVID Tracking Project if requested  
STEP 2b^: Reads in data from a specified local file (may have just been downloaded in step 2a), and checks control total trends against a previous version of the file  
STEP 3^: Processed the loaded data file for keeping proper variables, dropping non-valid states, etc.  
STEP 4^: Adds per-capita metrics for cases, deaths, tests, and hospitalizations  
STEP 5: Adds existing clusters by state if passed as an argument to useClusters=, otherwise creates new segments based on user-defined parameters  
STEP 6^^: Creates assessment plots for the state-level clusters  
STEP 7^^: Creates consolidated plots of cases, hospitalizations, deaths, and tests  
STEP 8^^: Optionally, creates plots of cumulative burden by segments and by state  
STEP 9: Returns a list of key data frames, modeling objects, named cluster vectors, etc.  
  
^ The user can instead specify a previously processed file and skip steps 2a, 2b, 3, and 4.  The previously processed file needs to be formatted and filtered such that it can be used "as is"  
^^ The user can skip the segment-level assessments by setting skipAssessmentPlots=TRUE  
  
The main function and the helper functions were previously updated to allow for using 2021 data.  The main function is copied below and will eventually be adapted for CDC daily data:  
```{r}

# Function to download/load, process, segment, and analyze data for CDC daily
# Needs to be updated
readRunCDCDaily <- function(thruLabel, 
                            downloadTo=NULL, 
                            readFrom=downloadTo, 
                            compareFile=NULL,
                            dateChangePlot=FALSE,
                            dateMetricPrint=TRUE,
                            writeLog=NULL,
                            ovrwriteLog=TRUE,
                            dfPerCapita=NULL,
                            useClusters=NULL,
                            hierarchical=TRUE,
                            returnList=!hierarchical, 
                            kCut=6,
                            reAssignState=vector("list", 0),
                            makeCumulativePlots=TRUE,
                            skipAssessmentPlots=FALSE,
                            ...
                            ) {
    
    # FUNCTION ARGUMENTS:
    # thruLabel: the label for when the data are through (e.g., "Aug 30, 2020")
    # donwloadTo: download the most recent CDC daily data to this location
    #             NULL means do not download any data
    # readFrom: location for reading in the CDC daily data (defaults to donwloadTo)
    # compareFile: name of the file to use for comparisons when reading in raw data (NULL means no comparison)
    # dateChangePlot: boolean, should changes in dates be captured as a plot rather than as a list?
    # dateMetricPrint: boolean, should the changes by date and metric be printed to the main log?
    # writeLog: name of a separate log file for capturing detailed data on changes between files
    #           NULL means no detailed data captured
    # ovrwriteLog: boolean, should the log file be overwritten and started again from scratch?
    # dfPerCapita: file can be passed directly, which bypasses the loading and processing steps
    # useClusters: file containing clusters by state (NULL means make the clusters from the data)
    # hierarchical: boolean, should hierarchical clusters be produced (if FALSE, will be k-means)?
    # returnList: boolean, should a list be returned or just the cluster object?
    #             refers to what is returned by clusterStates(); the main function always returns a list
    # kCut: number of segments when cutting the hierarchical tree
    # reAssignState: mapping file for assigning a state to another state's cluster
    #                format list("stateToChange"="stateClusterToAssign")
    # makeCumulativePlots: whether to make plots of cumulative metrics
    # skipAssessmentPlots: boolean to skip the plots for assessClusters()
    #                      especially useful if just exploring dendrograms or silhouette widths
    # ...: arguments to be passed to clusterStates(), will be used only if useClusters is NULL
    
    
    # STEP 1: Get state data
    stateData <- getStateData()
    
    # Helper function for glimpsing
    glimpseFile <- function(x, txt) {
        cat(txt)
        glimpse(x)
    }
            
    # STEPS 2-4 are run only if dfPerCapita does not exist
    if (is.null(dfPerCapita)) {
        
        # STEP 2a: Download latest CDC daily data (if requested)
        if (!is.null(downloadTo)) {
            downloadCOVIDbyState(fileName=downloadTo, 
                                 api="https://api.covidtracking.com/v1/states/daily.csv"
                                 )
        }
        
        # STEP 2b: Read-in CDC Daily Data
        dfRaw <- readCOViDbyState(readFrom, 
                                  checkFile=compareFile, 
                                  dateChangePlot=dateChangePlot, 
                                  dateMetricPrint=dateMetricPrint, 
                                  writeLog=writeLog, 
                                  ovrwriteLog=ovrwriteLog
                                  )
        if (is.null(writeLog)) glimpseFile(dfRaw, txt="\nRaw data file:\n")
        else capture.output(glimpseFile(dfRaw, txt="\nRaw data file:\n"), file=writeLog, append=TRUE)
        
        # STEP 3: Process the data so that it includes all requested key variables
        varsFilter <- c("date", "state", "positiveIncrease", "deathIncrease", 
                        "hospitalizedCurrently", "totalTestResultsIncrease"
                        )
        dfFiltered <- processCVData(dfRaw, 
                                    varsKeep=varsFilter, 
                                    varsRename=c(positiveIncrease="cases", 
                                                 deathIncrease="deaths", 
                                                 hospitalizedCurrently="hosp", 
                                                 totalTestResultsIncrease="tests"
                                    )
        )
        if (is.null(writeLog)) glimpseFile(dfFiltered, txt="\nFiltered data file:\n")
        else capture.output(glimpseFile(dfFiltered, txt="\nFiltered data file:\n"), file=writeLog, append=TRUE)
        
        # STEP 4: Convert to per capita
        dfPerCapita <- helperMakePerCapita(dfFiltered, 
                                           mapVars=c("cases"="cpm", "deaths"="dpm", 
                                                     "hosp"="hpm", "tests"="tpm"
                                           ), 
                                           popData=stateData
        )
        if (is.null(writeLog)) glimpseFile(dfPerCapita, txt="\nPer capita data file:\n")
        else capture.output(glimpseFile(dfPerCapita, txt="\nPer capita data file:\n"), 
                            file=writeLog, 
                            append=TRUE
                            )
        
    } else {
        dfRaw <- NULL
        dfFiltered <- NULL
    }
    
    
    # STEP 5: Create the clusters (if they have not been passed)
    if (is.null(useClusters)) {
        # Run the clustering process
        clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
        # If hierarchical clusters, cut the tree, otherwise use the output object directly
        if (hierarchical) {
            useClusters <- cutree(clData, k=kCut)
        } else {
            useClusters <- clData$objCluster$cluster
        }
        # If requested, manually assign clusters to the cluster for another state
        for (xNum in seq_len(length(reAssignState))) {
            useClusters[names(reAssignState)[xNum]] <- useClusters[reAssignState[[xNum]]]
        }
        
    }
    
    
    # STEP 5a: Stop the process and return what is available if skipAssessmentPlots is TRUE
    if (skipAssessmentPlots) {
        return(list(stateData=stateData, 
                    dfRaw=dfRaw, 
                    dfFiltered=dfFiltered, 
                    dfPerCapita=dfPerCapita, 
                    useClusters=useClusters, 
                    plotData=NULL, 
                    consolidatedPlotData=NULL, 
                    clCum=NULL
                    )
               )
    }
    
    
    # STEP 6: Create the cluster assessments
    plotData <- assessClusters(useClusters, 
                               dfState=stateData, 
                               dfBurden=dfPerCapita,
                               thruLabel=thruLabel,
                               plotsTogether=TRUE
    )
    
    
    # STEP 7: Plot the consolidated metrics
    subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
    consolidatedPlotData <- plotConsolidatedMetrics(plotData, 
                                                    varMain=c("state", "cluster", "date", "pop",
                                                              "cases", "deaths", "hosp", "tests"
                                                    ), 
                                                    subT=subT, 
                                                    nrowPlot2=2
    )
    
    # STEP 8: Create cumulative metrics if requested
    if (makeCumulativePlots) {
        consPos <- consolidatedPlotData %>%
            ungroup() %>%
            select(state, cluster, date, name, vpm7) %>%
            arrange(state, cluster, date, name) %>%
            pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
            mutate(pctpos=cases/tests) %>%
            pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
            filter(!is.na(vpm7))
        clCum <- makeCumulative(consPos)
        plotCumulativeData(clCum, 
                           keyMetricp2="", 
                           flagsp2="", 
                           makep1=TRUE, 
                           makep2=FALSE
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="deaths", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "deaths")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="cases", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "cases")
        )
        plotCumulativeData(clCum, 
                           keyMetricp2="tests", 
                           flagsp2=findFlagStates(clCum, keyMetricVal = "tests")
        )
    } else {
        clCum <- NULL
    }
    
    
    # STEP 9: Return a list of the key data
    list(stateData=stateData, 
         dfRaw=dfRaw, 
         dfFiltered=dfFiltered, 
         dfPerCapita=dfPerCapita, 
         useClusters=useClusters, 
         plotData=plotData, 
         consolidatedPlotData=consolidatedPlotData, 
         clCum=clCum
    )
    
    
}

```
  
The state data are downloaded and a per capita file created:  
```{r, fig.height=9, fig.width=9}

# Function to extract and format key state data
getStateData <- function(df=readFromRDS("statePop2019"), 
                         renameVars=c("stateAbb"="state", "NAME"="name", "pop_2019"="pop"), 
                         keepVars=c("state", "name", "pop")
                         ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing state data
    # renameVars: variables to be renamed, using named list with format "originalName"="newName"
    # keepVars: variables to be kept in the final file
    
    # Rename variables where appropriate
    names(df) <- ifelse(is.na(renameVars[names(df)]), names(df), renameVars[names(df)])
    
    # Return file with only key variables kept
    df %>%
        select_at(vars(all_of(keepVars)))
    
}

# Run getStateData() as a stand-alone
stateDataCDC <- getStateData()

# Create an analysis file, pivoted wider, for new and total cases/deaths
cdcFiltered <- cdcAnalysis %>%
    filter(metric %in% c("cases", "deaths"), modifier %in% c("new", "tot")) %>%
    mutate(key=paste(modifier, metric, sep="_")) %>%
    pivot_wider(c("date", "state"), names_from="key", values_from="value")


# Helper function to create per capita metrics
helperPerCapita <- function(df, 
                            origVar, 
                            newName,
                            byVar="state",
                            popVar="pop",
                            popData=stateData,
                            mult=1000000
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame currently being processed
    # origVar: the variables to be converted to per capita
    # newName: the new per capita variable name
    # byVar: the variable that will be merged by
    # popVar: the name of the population variable in the popData file
    # popData: the file containing the population data
    # mult: the multiplier, so that the metric is "per mult people"
    
    # Create the per capita variable
    df %>%
        inner_join(select_at(popData, vars(all_of(c(byVar, popVar)))), by=byVar) %>%
        mutate(!!newName:=mult*get(origVar)/get(popVar)) %>%
        select(-all_of(popVar))
    
}


# Helper function to create rolling aggregates
helperRollingAgg <- function(df, 
                             origVar, 
                             newName,
                             func=zoo::rollmean,
                             k=7, 
                             fill=NA, 
                             ...
                             ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing the data
    # origVar: the original data column name
    # newName: the new variable column name
    # func: the function to be applied (zoo::rollmean will be by far the most common)
    # k: the periodicity (k=7 is rolling weekly data)
    # fill: how to fill leading.trailing data to maintain the same vector lengths
    # ...: any other arguments to be passed to func
    
    # Create the appropriate variable
    df %>%
        mutate(!!newName:=func(get(origVar), k=k, fill=fill, ...))
    
}


# Function to add per capita and rolling to the base data frame
helperMakePerCapita <- function(df, 
                                mapVars=c("cases"="cpm", "deaths"="dpm"),
                                popData=stateData,
                                k=7
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: the initial data frame for conversion
    # mapVars: named vector of variables to be converted 'original name'='converted name'
    # k: the rolling time period to use
    
    # Create the variables for per capita
    for (origVar in names(mapVars)) {
        df <- df %>% 
            helperPerCapita(origVar=origVar, newName=mapVars[origVar], popData=popData)
    }
    # Arrange the data by date in preparation for rolling aggregations
    df <- df %>% 
        group_by(state) %>% 
        arrange(date)

    # Create the rolling variables
    for (newVar in mapVars) {
        df <- df %>% 
            helperRollingAgg(origVar=newVar, newName=paste0(newVar, k), k=k)
    }
    
    # Return the updated data frame, ungrouped
    df %>%
        ungroup()
    
}

# Convert to per capita
cdcPerCapita <- helperMakePerCapita(cdcFiltered, 
                                    mapVars=c("new_cases"="cpm", "new_deaths"="dpm", 
                                              "tot_cases"="tcpm", "tot_deaths"="tdpm"
                                              ), 
                                    popData=stateDataCDC
                                    )
cdcPerCapita

cdcPerCapita %>%
    select(date, state, contains("7")) %>%
    pivot_longer(-c("date", "state")) %>%
    filter(!is.na(value)) %>%
    bind_rows(mutate(summarize(group_by(., date, name), value=median(value), .groups="drop"), 
                     state="med"
                     )
              ) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(data=~filter(., state!="med"), aes(group=state), alpha=0.15) +
    geom_line(data=~filter(., state=="med"), aes(group=state), color="blue", size=1.5) + 
    labs(x="", y="Per million", title="Evolution metrics per capita by state", subtitle="Blue is median") +
    facet_wrap(~name, scales="free_y")

```
  
Per capita data appear broadly as expected.  Next steps are to update the segmentation and clustering algorithms.  The function call is copied, with the function then updated:  
```{r, fig.height=9, fig.width=9}

# Clustering code
# STEP 5: Create the clusters (if they have not been passed)
# if (is.null(useClusters)) {
#     # Run the clustering process
#     clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
#     # If hierarchical clusters, cut the tree, otherwise use the output object directly
#     if (hierarchical) {
#         useClusters <- cutree(clData, k=kCut)
#     } else {
#         useClusters <- clData$objCluster$cluster
#     }
#     # If requested, manually assign clusters to the cluster for another state
#     for (xNum in seq_len(length(reAssignState))) {
#         useClusters[names(reAssignState)[xNum]] <- useClusters[reAssignState[[xNum]]]
#     }
# }

# Function to create an elbow plot for various numbers of clusters in the data
helperElbow <- function(mtx, 
                        testCenters, 
                        iter.max, 
                        nstart, 
                        silhouette=FALSE
                        ) {
    
    # FUNCTION ARGUMENTS:
    # mtx: a numeric matrix, or an object that can be coerced to a numeric matrix (no character fields)
    # testCenters: integer vector for the centers to be tested
    # iter.max: parameter passed to kmeans
    # nstart: parameter passed to kmeans
    # silhouette: whether to calculate the silhouette score
    
    # Create an object for storing tot.withinss and silhouetteScore
    totWithin <- vector("numeric", length(testCenters))
    silhouetteScore <- vector("numeric", length(testCenters))
    
    # Create the distancing data (required for silhouette score)
    if (silhouette) distData <- dist(mtx)
    
    # Run k-means for every value in testCenters, and store $tot.withinss (and silhouetteScore, if requested)
    n <- 1
    for (k in testCenters) {
        km <- kmeans(mtx, centers=k, iter.max=iter.max, nstart=nstart)
        totWithin[n] <- km$tot.withinss
        if (silhouette & (k > 1)) silhouetteScore[n] <- mean(cluster::silhouette(km$cluster, distData)[, 3])
        n <- n + 1
    }
    
    # Create the elbow plot
    p1 <- tibble::tibble(n=testCenters, wss=totWithin) %>%
        ggplot(aes(x=n, y=wss)) + 
        geom_point() + 
        geom_line() + 
        geom_text(aes(y=wss + 0.05*max(totWithin), x=n+0.2, label=round(wss, 1))) + 
        labs(x="Number of segments", y="Total Within Sum-Squares", title="Elbow plot") + 
        ylim(c(0, NA))
    
    # Create the silhouette plot if requested
    if (silhouette) {
        p2 <- tibble::tibble(n=testCenters, ss=silhouetteScore) %>%
            ggplot(aes(x=n, y=ss)) + 
            geom_point() + 
            geom_line() + 
            geom_text(aes(y=ss + 0.05*max(silhouetteScore), x=n+0.2, label=round(ss, 1))) + 
            labs(x="Number of segments", y="Mean silhouette width", title="Silhouette plot") + 
            ylim(c(-1, NA))
        gridExtra::grid.arrange(p1, p2, nrow=1)
    } else {
        print(p1)
    }
    
}

# Custom function for creating YYYY-MM for use as the shape of the curve function
customTimeBucket <- function(x) {
    paste0(lubridate::year(x), "-", stringr::str_pad(lubridate::month(x), width=2, side="left", pad="0"))
}

# Updates to the clustering function
clusterStates <- function(df, 
                          caseVar="cpm", 
                          deathVar="dpm",
                          totCaseVar=NULL, 
                          totDeathVar=NULL,
                          shapeFunc=customTimeBucket, 
                          minShape=NULL, 
                          maxShape=NULL,
                          minDeath=0,
                          maxDeath=Inf,
                          minCase=0,
                          maxCase=Inf,
                          ratioTotalvsShape=1, 
                          ratioDeathvsCase=1, 
                          hierarchical=TRUE, 
                          hierMethod="complete", 
                          nCenters=3, 
                          iter.max=10,
                          nstart=1,
                          testCenters=NULL,
                          returnList=FALSE, 
                          hmlSegs=3, 
                          eslSegs=2,
                          seed=NULL
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing cases and deaths data
    # caseVar: the variable containing the daily cases per capita data
    # deathVar: the variable containing the daily deaths per capita data
    # totCaseVar: a variable containing total cases per capita (may differ from sum of new due to bulk adds)
    #             NULL means use sum(caseVar), otherwise use value of totCaseVar on last day of data
    # totDeathVar: a variable containing total deaths per capita (may differ from sum of new due to bulk adds)
    #             NULL means use sum(deathVar), otherwise use value of totDeathVar on last day of data
    # shapeFunc: the function to be used for creating the shape of the curve
    # minShape: the minimum value to be used for shape (to avoid very small amounts of data in Jan/Feb/Mar)
    #           shape is the month, so 4 means start with April data (NULL means keep everything)
    # maxShape: the maximum value to be used for shape (to avoid very small amounts of data in a partial month)
    #           shape is the month, so 9 means end with September data (NULL means keep everything)
    # minDeath: use this value as a floor for the death metric when calculating shape
    # maxDeath: use this value as a maximum when calculating distance using deaths 
    # minCase: use this value as a floor for the case metric when calculating shape
    # maxCase: use this value as a maximum when calculating distance using cases 
    # ratioTotalvsShape: amount of standard deviation to be kept in total variable vs shape variables
    # ratioDeathvsCase: amount of standard deviation to be kept in deaths vs cases 
    #                   (total death data will be scaled to have sd this many times higher than cases)
    #                   (death percentages by time period will be scaled directly by this amount)
    # hierarchical: whether to create hierarchical clusters
    #               TRUE means run hierarchical clustering
    #               FALSE means run kmeans clustering
    #               NA means run rules-based clustering
    # hierMethod: the method for hierarchical clustering (e.g., 'complete' or 'single')
    # nCenters: the number of centers to use for kmeans clustering
    # testCenters: integer vector of centers to test (will create an elbow plot); NULL means do not test
    # iter.max: maximumum number of kmeans iterations (default in kmeans algorithm is 10)
    # nstart: number of random sets chosen for kmeans (default in kmeans algorithm is 1)
    # returnList: boolean, if FALSE just the cluster object is returned
    #                      if TRUE, a list is returned with dfCluster and the cluster object
    # hmlSegs: number of segments to create for volume of burden integrated over time
    # eslSegs: number of segments to create for shape of burden over time
    # seed: set the seed to this value (NULL means no seed)

    # Create the timeBucket field, then filter the data to only the time periods of interest
    df <- df %>%
        mutate(timeBucket=shapeFunc(date))
    
    # Limit to only relevant time buckets if requested
    if (!is.null(minShape)) {
        df <- df %>%
            filter(timeBucket >= minShape)
    }
    if (!is.null(maxShape)) {
        df <- df %>%
            filter(timeBucket <= maxShape)
    }

    # Create an aggregate by state, scaled so that they have the proper ratio
    # If totCaseVar is NULL, use sum(cases), otherwise use max(cases)
    # If totDeathVar is NULL, use sum(cases), otherwise use max(cases)
    dfAgg <- df %>%
        group_by(state) %>%
        summarize(origTotalCases=if(is.null(totCaseVar)) sum(get(caseVar)) else max(get(totCaseVar)), 
                  origTotalDeaths=if(is.null(totDeathVar)) sum(get(deathVar)) else max(get(totDeathVar)), 
                  .groups="drop"
                  ) %>%
        mutate(totalCases=pmin(origTotalCases, maxCase), totalDeaths=pmin(origTotalDeaths, maxDeath)) %>%
        ungroup() %>%
        mutate(totalDeaths=ratioDeathvsCase*totalDeaths*sd(totalCases)/sd(totalDeaths)) %>%
        select(-origTotalCases, -origTotalDeaths)  # fields are just for QC while writing function

    # Create shape of the curve by state
    dfShape <- df %>%
        select_at(vars(all_of(c("timeBucket", "state", caseVar, deathVar)))) %>%
        purrr::set_names(c("timeBucket", "state", "cases", "deaths")) %>%
        group_by(state, timeBucket) %>%
        summarize_if(is.numeric, .funs=sum) %>%
        ungroup() %>%
        pivot_longer(-c(state, timeBucket)) %>%
        group_by(state, name) %>%
        mutate(tot=pmax(sum(value), ifelse(name=="deaths", minDeath, minCase)), 
               value=ifelse(name=="deaths", ratioDeathvsCase, 1) * value / tot) %>%
        select(-tot) %>%
        pivot_wider(state, names_from=c(name, timeBucket), values_from=value) %>%
        ungroup()
    
    # Function to calculate SD of a subset of columns
    calcSumSD <- function(df) {
        df %>% 
            ungroup() %>% 
            select(-state) %>% 
            summarize_all(.funs=sd) %>% 
            as.vector() %>% 
            sum()
    }
    
    # Down-weight the aggregate data so that there is the proper sum of sd in aggregates and shapes
    aggSD <- calcSumSD(dfAgg)
    shapeSD <- calcSumSD(dfShape)
    dfAgg <- dfAgg %>%
        mutate_if(is.numeric, ~. * ratioTotalvsShape * shapeSD / aggSD)
    
    # Combine so there is one row per state
    dfCluster <- dfAgg %>%
        inner_join(dfShape, by="state")
    
    # convert 'state' to rowname
    keyData <- dfCluster %>% 
        column_to_rownames("state")
    
    # Create rules-based segments (NA) or hierarchical segments (TRUE) or kmeans segments (FALSE)
    if (is.na(hierarchical)) {
        # Create pseudo-rules-based segments
        if (!is.null(seed)) set.seed(seed)
        # STEP 1: Classify high-medium-low based on deaths and cases
        hml <- kmeans(select(keyData, starts_with("total")), 
                      centers=hmlSegs, iter.max=iter.max, nstart=nstart
        )
        # STEP 2: Classify early-late based on shape
        esl <- kmeans(select(keyData, -starts_with("total")), 
                      centers=eslSegs, iter.max=iter.max, nstart=nstart
        )
        # STEP 3: Create a final segment
        objCluster <- eslSegs*(hml$cluster-1) + esl$cluster
    } else if (isTRUE(hierarchical)) {
        # Create hierarchical segments
        objCluster <-  hclust(dist(keyData), method=hierMethod)
        plot(objCluster)
    } else {
        # Create k-means segments
        # Create an elbow plot if testCenters is not NULL
        if (!is.null(testCenters)) {
            helperElbow(keyData, testCenters=testCenters, iter.max=iter.max, nstart=nstart, silhouette=TRUE)
        }
        # Create the kmeans cluster object, setting a seed if requested
        if (!is.null(seed)) set.seed(seed)
        objCluster <- kmeans(keyData, centers=nCenters, iter.max=iter.max, nstart=nstart)
        cat("\nCluster means and counts\n")
        n=objCluster$size %>% cbind(objCluster$centers) %>% round(2) %>% t() %>% print()
    }
    
    # Return the data and object is a list if returnList is TRUE, otherwise return only the clustering object
    if (!isTRUE(returnList)) {
        objCluster
    } else {
        list(objCluster=objCluster, dfCluster=dfCluster)
    }
    
}


```

The function can then be tested for various clustering schemes:  
```{r}

# Example for rules-based clustering
clRules <- clusterStates(df=cdcPerCapita, 
                         hierarchical=NA, 
                         returnList=TRUE, 
                         shapeFunc=customTimeBucket, 
                         minShape="2020-04", 
                         maxShape="2021-03", 
                         hmlSegs=3, 
                         eslSegs=3, 
                         seed=2104221520
                         )
tibble::tibble(state=names(clRules$objCluster), cluster=factor(unname(clRules$objCluster))) %>%
    usmap::plot_usmap(regions="states", values="cluster", data=.) + 
    labs(title="Rules-based clusters using CDC daily data") + 
    scale_fill_discrete("Cluster")

# Example for kmeans clustering (elbow plot)
# Return 7 segments
clKMeans <- clusterStates(df=cdcPerCapita, 
                          hierarchical=FALSE, 
                          returnList=TRUE, 
                          shapeFunc=customTimeBucket, 
                          minShape="2020-04", 
                          maxShape="2021-03", 
                          nCenters=7,
                          iter.max=50,
                          nstart=25,
                          testCenters=1:20,
                          seed=2104221530
                          )
tibble::tibble(state=names(clKMeans$objCluster$cluster), 
               cluster=factor(unname(clKMeans$objCluster$cluster))
               ) %>%
    usmap::plot_usmap(regions="states", values="cluster", data=.) + 
    labs(title="K-means clusters using CDC daily data") + 
    scale_fill_discrete("Cluster")

# Example for hierarchical clustering (clusters)
clHier <- clusterStates(cdcPerCapita, 
                        hierarchical=TRUE, 
                        returnList=FALSE, 
                        shapeFunc=customTimeBucket, 
                        minShape="2020-04", 
                        maxShape="2021-01"
                        )
tibble::tibble(state=names(cutree(clHier, k=7)), 
               cluster=factor(unname(cutree(clHier, k=7)))
               ) %>%
    usmap::plot_usmap(regions="states", values="cluster", data=.) + 
    labs(title="Hierarchical clusters using CDC daily data") + 
    scale_fill_discrete("Cluster")

```

There are some meaningful differences in segment membership depending on method, suggestive that there may have been some convergence of outcomes across states.  Generally, the heavy/hard cluster centered around NY and the light/late cluster centered in the NE/NW tend to appear in each approach.  The rules-based segments look reasonably similar to those generated previously.

Next steps are to generate descriptive plots for the segments.  The code for assessClusters() is copied and adapted:  
```{r}

# # STEP 6: Create the cluster assessments
# plotData <- assessClusters(useClusters, 
#                            dfState=stateData, 
#                            dfBurden=dfPerCapita,
#                            thruLabel=thruLabel,
#                            plotsTogether=TRUE
#                            )


# Function to reorder and relabel factors
changeOrderLabel <- function(df, 
                             fctVar="cluster",
                             grpVars=c(),
                             burdenVar="dpm", 
                             wgtVar="pop",
                             exclfct="999"
                             ) {
    
    # FUNCTION ARGUMENTS
    # df: the data frame
    # fctVar: the factor variable
    # grpVars: the variable that the data are aurrently at (e.g., "fipsCounty" for county-level in df)
    # burdenVar: the disease burden variable for sorting
    # wgtVar: the weight variable for sorting
    # exclfct: the factor level to be excluded from analysis
    
    # General approach
    # 1. Data are aggregated to c(fctVar, grpVars) as x=sum(burdenVar*wgtVar) and y=mean(wgtVar)
    # 2. Data are aggregated to fctVar as z=sum(x)/sum(y)
    # 3. Factors are reordered from high to low on z, with the excluded factor added back last (if it exists)
    
    # Check if exclfct exists in the factor variable
    fctDummy <- exclfct %in% levels(df[, fctVar, drop=TRUE])
    
    # Create the summary of impact by segment
    newLevels <- df %>%
        filter(get(fctVar) != exclfct) %>%
        group_by_at(vars(all_of(c(fctVar, grpVars)))) %>%
        summarize(x=sum(get(burdenVar)*get(wgtVar)), y=mean(get(wgtVar)), .groups="drop") %>%
        group_by_at(vars(all_of(fctVar))) %>%
        summarize(z=sum(x)/sum(y), .groups="drop") %>%
        arrange(-z) %>%
        pull(fctVar) %>%
        as.character()
    
    # Add back the dummy factor at the end (if it exists)
    if (fctDummy) newLevels <- c(newLevels, exclfct)
    
    # Reassign the levels in df
    df %>% 
        mutate(!!fctVar:=factor(get(fctVar), levels=newLevels, labels=newLevels))
    
}


# Helper function to make the overall cluster summary statistics
helperClusterSummaryPlots <- function(dfState, 
                                      dfPlot,
                                      showMap, 
                                      clusterPlotsTogether,
                                      weightedMean=TRUE,
                                      mapLevel="states", 
                                      p3Vars=c("cases"="cpm7", "deaths"="dpm7"),
                                      p4Vars=c("cases"="cases", "deaths"="deaths"), 
                                      p4Fun=sum
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # dfState: contains the state/county-level data
    # dfPlot: contains the joined data for plotting
    # showMap: boolean for whether to create a map (if FALSE, segment membership counts are shown instead)
    # clusterPlotsTogether: boolean, whether to put all four plots on the same page
    # weightedMean: boolean, whether to create weighted mean by segment (if FALSE, median by segment is taken)
    # mapLevel: the level to be used on the map
    # p3Vars: the variables to be included in plot 3 (character vector of length 2, plotName=variableName)
    # p4Vars: the variables to be included in plot 4 (character vector of length 2, plotName=variableName)
    # p4Fun: the function to be applied in plot 4 (sum for sum of daily, max for max of cumulative)
    
    # Reorder the cluster levels in dfState to match dfPlot
    # Convert factor order to match dfPlot (can be reordered by argument to the calling function)
    dfState <- dfState %>%
        mutate(cluster=factor(cluster, levels=levels(dfPlot$cluster)))
    
    # Plot the segments on a map or show segment membership
    if (showMap) {
        if (mapLevel=="counties") {
            dfMap <- dfState %>%
                mutate(fips=stringr::str_pad(state, width=5, side="left", pad="0")) %>%
                select(fips, cluster)
        } else {
            dfMap <- dfState
        }
        # Create the map
        p1 <- usmap::plot_usmap(regions=mapLevel, data=dfMap, values="cluster") + 
            scale_fill_discrete("cluster") + 
            theme(legend.position="right")
    } else {
        p1 <- dfState %>%
            count(cluster) %>%
            ggplot(aes(x=fct_rev(cluster), y=n)) + 
            geom_col(aes(fill=cluster)) +
            geom_text(aes(y=n/2, label=n)) +
            coord_flip() + 
            labs(x="", y="# Counties", title="Membership by segment")
    }
    
    # Plot the population totals by segment
    # Show population totals by cluster
    p2 <- dfState %>%
        group_by(cluster) %>%
        summarize(pop=sum(pop)/1000000, .groups="drop") %>%
        ggplot(aes(x=fct_rev(cluster), y=pop)) + 
        geom_col(aes(fill=cluster)) + 
        geom_text(aes(y=pop/2, label=round(pop))) + 
        labs(y="Population (millions)", x="Cluster", title="Population by cluster (millions)") +
        coord_flip()
    
    # Plot the rolling 7-day mean daily disease burden by cluster
    # Create the p3Data to be either median of all elements in cluster or weighted mean
    p3 <- dfPlot %>%        
        select_at(vars(all_of(c("date", "cluster", unname(p3Vars), "pop")))) %>%
        purrr::set_names(c("date", "cluster", names(p3Vars), "pop")) %>%
        pivot_longer((-c(date, cluster, pop))) %>%
        filter(!is.na(value)) %>%
        group_by(date, cluster, name) %>%
        summarize(mdnValue=median(value), wtdValue=sum(pop*value)/sum(pop), .groups="drop") %>%
        ggplot(aes(x=date, y=if(weightedMean) wtdValue else mdnValue)) +
        geom_line(aes(group=cluster, color=cluster)) +
        facet_wrap(~name, scales="free_y") +
        labs(x="",
             y="Rolling 7-day mean, per million",
             title="Rolling 7-day mean daily disease burden, per million",
             subtitle=paste0(if(weightedMean) "Weighted mean" else "Median", 
                             " per day for states assigned to cluster"
             )
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b")
    
    # Plot the total cases and total deaths by cluster
    p4 <- dfPlot %>%
        select_at(vars(all_of(c("cluster", "date", unname(p4Vars))))) %>%
        purrr::set_names(c("cluster", "date", names(p4Vars))) %>%
        group_by(cluster, date) %>%
        summarize_all(.funs=sum) %>% # Get the total by cluster-date so that it can then be summed/maxed
        group_by(cluster) %>%
        summarize_at(vars(names(p4Vars)), .funs=p4Fun) %>%
        ungroup() %>%
        pivot_longer(-cluster) %>%
        ggplot(aes(x=fct_rev(cluster), y=value/1000)) + 
        geom_col(aes(fill=cluster)) + 
        geom_text(aes(y=value/2000, label=round(value/1000))) +
        coord_flip() + 
        facet_wrap(~varMapper[name], scales="free_x") + 
        labs(x="Cluster", y="Burden (000s)", title="Total burden by segment")
    
    # Place the plots together if plotsTogether is TRUE, otherwise just print
    if (isTRUE(clusterPlotsTogether)) {
        gridExtra::grid.arrange(p1, p2, p3, p4, nrow=2, ncol=2)
    } else {
        print(p1); print(p2); print(p3); print(p4)
    }
    
}


# Function to create side-by-side plots for a deaths and cases metric
# Data in df will be aggregated to be unique by byVar using aggFunc
helperBarDeathsCases <- function(df, 
                                 numVars,
                                 title="",
                                 xVar="state",
                                 fillVar=NULL,
                                 aggFunc=sum, 
                                 mapper=varMapper
                                 ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing the data
    # numVars: the relevant numeric variables for plotting
    # title: plot title, default is nothing
    # xVar: the x-axis variable for plotting
    # fillVar: the variable that will color the bars in the final plot (NULL means use "lightblue" for all)
    # aggFunc: the aggregate function (will be applied to create data unique by byVar)
    # mapper: mapping file to convert x/y variables to descriptive axes (named vector "variable"="label")
    
    # OVERALL FUNCTION PROCESS:
    # 1.  Variables in numVar are aggregated by aggFunc to be unique by c(xVar, fillVar)
    # 2.  Data are pivoted longer
    # 3.  Bar charts are created, with coloring by fillVar if provided
    
    # Create the byVar for summing
    byVar <- xVar
    if (!is.null(fillVar)) { byVar <- c(byVar, fillVar) }
    
    # Process the data and create the plot
    p1 <- df %>%
        select_at(vars(all_of(c(byVar, numVars)))) %>%
        group_by_at(vars(all_of(byVar))) %>%
        summarize_all(aggFunc) %>%
        pivot_longer(-all_of(byVar)) %>%
        ggplot(aes(x=fct_reorder(get(xVar), value, .fun=min), y=value)) + 
        coord_flip() + 
        facet_wrap(~mapper[name], scales="free_x") + 
        labs(x="", y="", title=title) + 
        if (is.null(fillVar)) geom_col(fill="lightblue") else geom_col(aes_string(fill=fillVar))
    
    # Print the plot
    print(p1)
    
}


# Helper function to make total and per capita by state (calls its own helper function)
helperCallTotalPerCapita <- function(dfPlot, 
                                     thruLabel, 
                                     isCDC=FALSE
                                     ) {
    
    # FUNCTION ARGUMENTS:
    # dfPlot: the plotting data frame
    # thruLabel: the date that data are through
    # isCDC: boolean, are the data from CDC daily rather than CTP?
    
    # Plot total cases and total deaths by state, colored by cluster
    helperBarDeathsCases(dfPlot, 
                         numVars=if(isTRUE(isCDC)) c("tot_cases", "tot_deaths") else c("cases", "deaths"), 
                         title=paste0("Coronavirus impact by state through ", thruLabel), 
                         xVar=c("state"), 
                         fillVar=c("cluster"), 
                         aggFunc=if(isTRUE(isCDC)) max else sum
                         )
    
    # Plot cases per million and deaths per million by state, colored by cluster
    helperBarDeathsCases(dfPlot, 
                         numVars=if(isTRUE(isCDC)) c("tcpm", "tdpm") else c("cpm", "dpm"), 
                         title=paste0("Coronavirus impact by state through ", thruLabel), 
                         xVar=c("state"), 
                         fillVar=c("cluster"),
                         aggFunc=if(isTRUE(isCDC)) max else sum
                         )
    
}


# Helper function to assess 30-day change vs. total
helperRecentvsTotal <- function(df, 
                                xVar, 
                                yVar,
                                title,
                                recencyDays=30, 
                                ablineSlope=0.5, 
                                mapper=varMapper, 
                                labelPlot=TRUE,
                                printPlot=TRUE, 
                                isCDC=FALSE
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: the tibble containing data by state by day
    # xVar: the x-variable
    # yVar: the y-variable
    # title: the plot title
    # recencyDays: number of days to consider as recent
    # ablineSlope: dashed line will be drawn with this slope and intercept 0
    # mapper: mapping file to convert x/y variables to descriptive axes (named vector "variable"="label")
    # labelPlot: boolean, whether to show the labels for each point
    # printPlot: boolean, whether to display the plot (if FALSE, plot object is returned)
    # isCDC: boolean, are the data from CDC daily rather than CTP?
    
    # Get the most date cutoff
    dateCutoff <- df %>% pull(date) %>% max() - recencyDays + 1
    cat("\nRecency is defined as", format(dateCutoff, "%Y-%m-%d"), "through current\n")
    
    # For CDC data, set tcpm and tdpm to 0 except for the last date
    if(isTRUE(isCDC)) {
        df <- df %>%
            group_by(state) %>%
            mutate(tcpm=ifelse(date==max(date), tcpm, 0), tdpm=ifelse(date==max(date), tdpm, 0)) %>%
            ungroup()
    }
    
    # Create the plot
    p1 <- df %>%
        mutate(newCases=ifelse(date >= dateCutoff, if(isTRUE(isCDC)) new_cases else cases, 0), 
               newDeaths=ifelse(date >= dateCutoff, if (isTRUE(isCDC)) new_deaths else deaths, 0), 
               newcpm=ifelse(date >= dateCutoff, cpm, 0), 
               newdpm=ifelse(date >= dateCutoff, dpm, 0)
        ) %>%
        group_by(state, cluster) %>%
        summarize_if(is.numeric, .funs=sum) %>%
        ungroup() %>%
        ggplot(aes_string(x=xVar, y=yVar)) + 
        labs(x=mapper[xVar], 
             y=mapper[yVar], 
             title=title, 
             subtitle=paste0("Dashed line represents ", 
                             round(100*ablineSlope), 
                             "% of total is new in last ", 
                             recencyDays,
                             " days"
             )
        ) + 
        geom_abline(lty=2, slope=ablineSlope) + 
        lims(x=c(0, NA), y=c(0, NA)) + 
        theme(legend.position = "bottom")
    
    # Add the appropriate geom (scatterplot if labelPlot is FALSE)
    if (labelPlot) p1 <- p1 + geom_text(aes(color=cluster, label=state))
    else p1 <- p1 + geom_point(aes(color=cluster), alpha=0.5)
    
    if (isTRUE(printPlot)) {
        print(p1)
    } else {
        p1
    }
    
}


# Helper function to make recent vs. total plots
helperCallRecentvsTotal <- function(dfPlot, 
                                    thruLabel, 
                                    labelPoints, 
                                    recentTotalTogether, 
                                    isCDC=FALSE
                                    ) {
    
    # FUNCTION ARGUMENTS:
    # dfPlot: the plotting data frame
    # thruLabel: the date that data are through
    # labelPoints: boolean, whether to label the individual points
    # recentTotalTogether: boolean, whether to put these plots together on one page
    # isCDC: boolean, are the data from CDC daily rather than CTP?
    
    # Plot last-30 vs total for cases per million by state, colored by cluster
    p7 <- helperRecentvsTotal(dfPlot, 
                              xVar=if(isTRUE(isCDC)) "tcpm" else "cpm", 
                              yVar="newcpm", 
                              title=paste0("Coronavirus burden through ", thruLabel), 
                              labelPlot=labelPoints,
                              printPlot=FALSE, 
                              isCDC=isCDC
    )
    
    # Plot last-30 vs total for deaths per million by state, colored by cluster
    p8 <- helperRecentvsTotal(dfPlot, 
                              xVar=if(isTRUE(isCDC)) "tdpm" else "dpm", 
                              yVar="newdpm", 
                              title=paste0("Coronavirus burden through ", thruLabel), 
                              labelPlot=labelPoints,
                              printPlot=FALSE,
                              isCDC=isCDC
    )
    
    # Print the plots either as a single page or separately
    if (isTRUE(recentTotalTogether)) {
        gridExtra::grid.arrange(p7, p8, nrow=1)
    } else {
        print(p7); print(p8)
    }    
    
}


# Function to plot cluster vs. individual elements on a key metric
helperTotalvsElements <- function(df, 
                                  keyVar, 
                                  title,
                                  aggAndTotal=TRUE,
                                  pctRibbon=0.8,
                                  aggFunc=if(aggAndTotal) median else mean, 
                                  mapper=varMapper, 
                                  facetScales="free_y", 
                                  printPlot=TRUE
                                  ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing n-day rolling averages
    # keyVar: the variable to be plotted
    # title: the plot title
    # aggAndTotal: boolean, whether to plot every individual observation along with the cluster aggregate
    # pctRibbon: if aggAndTotal is FALSE, a ribbon covering this percentage of the data will be plotted
    # aggFunc: how to aggregate the elements to the segment
    #          CAUTION that this is an aggregate of averages, rather than a population-weighted aggregate
    # mapper: the variable mapping file to get the appropriate label for keyVar
    # facetScales: the scaling for the facets - "free_y" to let them all float, "fixed" to have them the same
    # printPlot: boolean, if TRUE print the plot (otherwise return the plot object)
    
    # Create an appropriate subtitle
    subtitle <- if(facetScales=="free_y") {
        "Caution that each facet has its own y axis with different scales"
    } else if (facetScales=="fixed") { 
        "All facets are on the same scale"
    } else {
        "Update subtitle code in function helperTotalvsElements"
    }
    
    # Create an appropriate caption
    xCap <- "1. Cluster aggregate is mean over all observations (NOT population-weighted)"
    xCap <- paste0(xCap, "\n2. Ribbons reflect range covering ", round(pctRibbon*100), "% of observations")
    caption <- if(aggAndTotal) {
        "Cluster aggregate is median, weighting each observation equally\n(NOT population-weighted)"
    } else {
        xCap
    }
    
    # Create the plots for segment-level data
    p1 <- df %>%
        rbind(mutate(., state="cluster")) %>%
        group_by(state, cluster, date) %>%
        summarize_at(vars(all_of(keyVar)), .funs=aggFunc) %>%
        ungroup() %>%
        filter(!is.na(get(keyVar))) %>%
        ggplot(aes_string(x="date")) + 
        geom_line(data=~filter(., state == "cluster"), 
                  aes(y=get(keyVar), group=state, color=cluster), 
                  lwd=1.5
        ) + 
        facet_wrap(~cluster, scales=facetScales) + 
        labs(x="", 
             y=mapper[keyVar], 
             title=title, 
             subtitle=subtitle,
             caption=caption
        ) + 
        ylim(c(0, NA)) + 
        theme(legend.position="bottom")
    
    # Add an appropriate individual metric (either every observation or quantiles)
    if (aggAndTotal) {
        p1 <- p1 + 
            geom_line(data=~filter(., state != "cluster"), 
                      aes(y=get(keyVar), group=state), 
                      alpha=0.25
            )
    } else {
        dfRibbon <- df %>%
            filter(!is.na(get(keyVar))) %>%
            group_by(cluster, date) %>%
            summarize(ylow=quantile(get(keyVar), 0.5-0.5*pctRibbon), 
                      yhigh=quantile(get(keyVar), 0.5+0.5*pctRibbon), 
                      .groups="drop"
            )
        p1 <- p1 + 
            geom_ribbon(data=dfRibbon, 
                        aes(ymin=ylow, ymax=yhigh), 
                        alpha=0.25
            )
    }
    
    # Print plot if requested, otherwise return it
    if (isTRUE(printPlot)) {
        print(p1)
    } else {
        p1
    }
    
}


# Helper function to create total vs. elements plots
helperCallTotalvsElements <- function(dfPlot, 
                                      aggAndTotal, 
                                      clusterAggTogether,
                                      ...
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # dfPlot: the plotting data frame
    # aggAndTotal: boolean, should each individual line be plotted (if FALSE an 80% CI is plotted instead)
    # clusterAggTogether: boolean, whether to print the plots all on a single page
    # ...: any other arguments to pass to helperTotalvsElements (especially pctRibbon or aggFunc)
    
    # Plot the cases per million on a free y-scale and a fixed y-scale
    p9 <- helperTotalvsElements(dfPlot, 
                                keyVar="cpm7", 
                                aggAndTotal=aggAndTotal,
                                title="Cases per million, 7-day rolling mean", 
                                printPlot=FALSE, 
                                ...
    )
    p10 <- helperTotalvsElements(dfPlot, 
                                 keyVar="cpm7", 
                                 aggAndTotal=aggAndTotal,
                                 title="Cases per million, 7-day rolling mean", 
                                 facetScales="fixed", 
                                 printPlot=FALSE, 
                                 ...
    )
    
    # Plot the deaths per million on a free y-scale and a fixed y-scale
    p11 <- helperTotalvsElements(dfPlot, 
                                 keyVar="dpm7", 
                                 aggAndTotal=aggAndTotal,
                                 title="Deaths per million, 7-day rolling mean", 
                                 printPlot=FALSE, 
                                 ...
    )
    p12 <- helperTotalvsElements(dfPlot, 
                                 keyVar="dpm7", 
                                 aggAndTotal=aggAndTotal,
                                 title="Deaths per million, 7-day rolling mean", 
                                 facetScales="fixed", 
                                 printPlot=FALSE, 
                                 ...
    )
    
    if (isTRUE(clusterAggTogether)) {
        gridExtra::grid.arrange(p9, p11, nrow=1)
        gridExtra::grid.arrange(p10, p12, nrow=1)
    } else {
        print(p9); print(p10); print(p11); print(p12)
    }
    
}


# assessClusters() function
assessClusters <- function(clusters, 
                           dfState=stateData, 
                           dfBurden=cvFilteredPerCapita,
                           thruLabel="Aug 20, 2020",
                           isCounty=FALSE,
                           plotsTogether=FALSE, 
                           clusterPlotsTogether=plotsTogether,
                           recentTotalTogether=plotsTogether, 
                           clusterAggTogether=plotsTogether, 
                           makeSummaryPlots=TRUE, 
                           makeTotalvsPerCapitaPlots=!isCounty, 
                           makeRecentvsTotalPlots=TRUE, 
                           makeTotalvsElementPlots=TRUE, 
                           showMap=!isCounty, 
                           orderCluster=FALSE, 
                           isCDC=FALSE,
                           p3Vars=c("cases"="cpm7", "deaths"="dpm7"),
                           p4Vars=c("cases"="cases", "deaths"="deaths"), 
                           p4Fun=sum
                           ) {
    
    # FUNCTION ARGUMENTS:
    # clusters: the named vector containing the clusters by state
    # dfState: the file containing the states and populations
    # dfBurden: the data containing the relevant per capita burden statistics by state-date
    # thruLabel: label for plots for 'data through'
    # isCounty: boolean, is this a plot of county-level data that have been named 'state'?
    #           FALSE means that it is normal state-level data
    # plotsTogether: boolean, should plots be consolidated on fewer pages?
    # clusterPlotsTogether: boolean, should plots p1-p4 be consolidated?
    # recentTotalTogether: boolean, should recent total plots p7-p8 be consolidated?
    # clusterAggTogether: boolean, should aggregate plots p9/p11 and p10/p12 be consolidated?
    # makeSummaryPlots: boolean, should the summary plots be made?
    # makeTotalvsPerCapitaPlots: boolean, should the total and per capita plots be produced?
    # makeRecentvsTotalPlots: boolean, should the recent vs. total plots be created?
    # makeTotalvsElementPlots: boolean, should the total vs. element plots be created?
    # showMap: boolean, whether to create a map colored by cluster (will show segment counts otherwise)
    # orderCluster: if FALSE, ignore; if TRUE, order by "dpm"; if anything else, order by orderCluster
    # isCDC: boolean, are the data from CDC daily rather than CTP?
    # p3Vars: the variables to be included in plot 3 (character vector of length 2, plotName=variableName)
    # p4Vars: the variables to be included in plot 4 (character vector of length 2, plotName=variableName)
    # p4Fun: the function to be applied in plot 4 (sum for sum of daily, max for max of cumulative)
    # ...: any additional arguments passed from a calling function
    #      most common would be orderCluster=TRUE, a request for the clusters to be ordered by disease burden
    
    # Attach the clusters to the state population data
    dfState <- as.data.frame(clusters) %>%
        set_names("cluster") %>%
        rownames_to_column("state") %>%
        inner_join(dfState, by="state") %>%
        mutate(cluster=factor(cluster))
    
    # Plot the rolling 7-day mean dialy disease burden by cluster
    dfPlot <- dfState %>%
        inner_join(dfBurden, by="state") %>%
        tibble::as_tibble()
    
    # Reorder the clusters if requested
    if (!isFALSE(orderCluster)) {
        if (isTRUE(orderCluster)) burdenParam <- "dpm" else burdenParam <- orderCluster
        dfPlot <- changeOrderLabel(dfPlot, grpVars="state", burdenVar=burdenParam)
    }
    
    # Call the helper function to make the overall summary statistics
    if (makeSummaryPlots) {
        helperClusterSummaryPlots(dfState=dfState, 
                                  dfPlot=dfPlot, 
                                  showMap=showMap, 
                                  clusterPlotsTogether=clusterPlotsTogether, 
                                  mapLevel=if(isCounty) "counties" else "states", 
                                  p3Vars=p3Vars, 
                                  p4Vars=p4Vars, 
                                  p4Fun=p4Fun
                                  )
    }
    
    # These are primarily useful for state-level data and default to FALSE when isCounty is TRUE
    if (makeTotalvsPerCapitaPlots) {
        helperCallTotalPerCapita(dfPlot=dfPlot, thruLabel=thruLabel, isCDC=isCDC)
    }
    
    # Call the helper function to make recent vs. total plots
    if (makeRecentvsTotalPlots) {
        helperCallRecentvsTotal(dfPlot=dfPlot, 
                                thruLabel=thruLabel, 
                                labelPoints=!isCounty, 
                                recentTotalTogether=recentTotalTogether, 
                                isCDC=isCDC
                                )
    }
    
    # Call the total vs. elements helper function
    if (makeTotalvsElementPlots) {
        helperCallTotalvsElements(dfPlot=dfPlot, 
                                  aggAndTotal=!isCounty, 
                                  clusterAggTogether=clusterPlotsTogether
        )
    }
    
    # Return the plotting data frame
    dfPlot
    
}

```
  
The functions is then tested against the CDC daily data:  
```{r, fig.height=9, fig.width=9}

cdcPlotData <- assessClusters(clusters=clRules$objCluster,
                              dfState=stateDataCDC,
                              dfBurden=cdcPerCapita,
                              thruLabel="Apr 12, 2021",
                              plotsTogether=TRUE, 
                              orderCluster="dpm", 
                              p4Vars=c("cases"="tot_cases", "deaths"="tot_deaths"), 
                              p4Fun=max, 
                              isCDC=TRUE
                              )
cdcPlotData

```

Next steps are to adapt the code for plotConsolidatedMetrics() for use with CDC daily data:
```{r}

# STEP 7: Plot the consolidated metrics
# subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
# consolidatedPlotData <- plotConsolidatedMetrics(plotData, 
#                                                 varMain=c("state", "cluster", "date", "pop",
#                                                           "cases", "deaths", "hosp", "tests"
#                                                           ), 
#                                                 subT=subT, 
#                                                 nrowPlot2=2
#                                                 )

# Function to create plots of consolidated metrics
plotConsolidatedMetrics <- function(dfMain, 
                                    dfHosp=NULL, 
                                    varMain=c("state", "cluster", "date", "pop", "cases", "deaths", "hosp"),
                                    varDropHosp=c("n", "pop"), 
                                    joinBy=c("state", "cluster", "date"), 
                                    subT=NULL, 
                                    nrowPlot2=1
                                    ) {
    
    # FUNCTION ARGUMENTS:
    # dfMain: the main file produced by assessClusters(), containing data by state-cluster-date
    # dfHosp: the separate file with hospital data (NULL means data already available in dfMain)
    # varMain: variables to keep from dfMain
    # varDropHosp: variables to drop from dfHosp
    # joinBy: variables for joining dfMain and dfHosp
    # subT: plot subtitle (NULL will use the defaults), 
    # nrowPlot2: number of rows for the facetting to use on plot 2
    
    if (is.null(subT)) {
        subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new)"
    }
    
    # Filter dfMain to include only variables in varMain
    dfMain <- dfMain %>%
        select_at(vars(all_of(varMain)))
    
    # Left join dfMain to dfHosp unless dfHosp is NULL
    if (!is.null(dfHosp)) {
        dfHosp <- dfHosp %>%
            select_at(vars(all_of(names(dfHosp)[!(names(dfHosp) %in% varDropHosp)])))
        dfMain <- dfMain %>%
            left_join(dfHosp, by=all_of(joinBy))
    }
    
    # Check that variables state, cluster, date, pop are all available
    if (!(c("state", "cluster", "date", "pop") %in% names(dfMain) %>% all())) {
        stop("\nFunction requires variables state, cluster, date, and pop after processing\n")
    }
    
    # Create the relevant plotting data
    dfPlot <- dfMain %>%
        pivot_longer(-c(state, cluster, date, pop)) %>%
        filter(!is.na(value)) %>%
        rbind(mutate(., state="cluster")) %>%
        group_by_at(vars(all_of(c(joinBy, "name")))) %>%
        summarize(value=sum(value), pop=sum(pop), .groups="drop") %>%
        mutate(vpm=1000000*value/pop) %>%
        arrange(state, cluster, name, date) %>%
        group_by(state, cluster, name) %>%
        helperRollingAgg(origVar="vpm", newName="vpm7")    
    
    # Create facetted plots for totals by metric by segment
    p1 <- dfPlot %>%
        filter(!is.na(vpm7)) %>%
        ggplot(aes(x=date, y=vpm7)) + 
        geom_line(data=~filter(., state=="cluster"), aes(group=cluster, color=cluster), lwd=1.5) +
        geom_line(data=~filter(., state!="cluster"), aes(group=state), alpha=0.25) + 
        facet_grid(name ~ cluster, scales="free_y") + 
        labs(x="", 
             y="Rolling 7-day mean per million", 
             title="Key metrics by cluster (7-day rolling mean per million)", 
             subtitle=subT
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b") + 
        theme(axis.text.x=element_text(angle=90))
    print(p1)
    
    # Create all-segment plot by metric
    p2 <- dfPlot %>%
        filter(!is.na(vpm7)) %>%
        ggplot(aes(x=date, y=vpm7)) + 
        geom_line(data=~filter(., state=="cluster"), aes(group=cluster, color=cluster), lwd=1.5) +
        facet_wrap(~ name, scales="free_y", nrow=nrowPlot2) + 
        labs(x="", 
             y="Rolling 7-day mean per million", 
             title="Key metrics by cluster (7-day rolling mean per million)", 
             subtitle=subT
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b") + 
        theme(axis.text.x=element_text(angle=90))
    print(p2)
    
    # Create all-metric plot by segment (define 100% as peak for segment-metric)
    p3 <- dfPlot %>%
        filter(!is.na(vpm7)) %>%
        group_by(state, cluster, name) %>%
        mutate(spm7=vpm7/max(vpm7)) %>%
        ggplot(aes(x=date, y=spm7)) + 
        geom_line(data=~filter(., state=="cluster"), aes(group=name, color=name), lwd=1) +
        facet_wrap(~ cluster, scales="free_y") + 
        labs(x="", 
             y="% of Maximum", 
             title="Key metrics by cluster (% of cluster maximum for variable)", 
             subtitle=subT
        ) + 
        scale_x_date(date_breaks="1 months", date_labels="%b") + 
        scale_color_discrete("variable") +
        theme(axis.text.x=element_text(angle=90))
    print(p3)
    
    # Return the plotting data
    dfPlot
    
}

```

The function appears OK as-is provided that appropriate parameters are passed:  
```{r, fig.height=9, fig.width=9}

subT <- "new_cases: new cases, new_deaths: new deaths"
subT <- paste0(subT, ", tot_cases: cumulative cases, tot_deaths: cumulative deaths")
consolidatedCDCPlotData <- plotConsolidatedMetrics(cdcPlotData,
                                                   varMain=c("state", "cluster", "date", "pop",
                                                             "new_cases", "new_deaths", 
                                                             "tot_cases", "tot_deaths"
                                                             ),
                                                   subT=subT,
                                                   nrowPlot2=2
                                                   )

```

Next steps are to adapt the code for making cumulative plots:  
```{r}

# # STEP 8: Create cumulative metrics if requested
# consPos <- consolidatedPlotData %>%
#     ungroup() %>%
#     select(state, cluster, date, name, vpm7) %>%
#     arrange(state, cluster, date, name) %>%
#     pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
#     mutate(pctpos=cases/tests) %>%
#     pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
#     filter(!is.na(vpm7))
# 
# clCum <- makeCumulative(consPos)
# plotCumulativeData(clCum, 
#                    keyMetricp2="", 
#                    flagsp2="", 
#                    makep1=TRUE, 
#                    makep2=FALSE
#                    )
# plotCumulativeData(clCum, 
#                    keyMetricp2="deaths", 
#                    flagsp2=findFlagStates(clCum, keyMetricVal = "deaths")
#                    )
# plotCumulativeData(clCum, 
#                    keyMetricp2="cases", 
#                    flagsp2=findFlagStates(clCum, keyMetricVal = "cases")
#                    )
# plotCumulativeData(clCum, 
#                    keyMetricp2="tests", 
#                    flagsp2=findFlagStates(clCum, keyMetricVal = "tests")
#                    )


# Function to convert a file to cumulative totals
# Function is already OK for CDC daily data (cum7 column will be meaningless in some cases, but code runs)
makeCumulative <- function(df, 
                           typeVar="name", 
                           typeKeep=c("cases", "deaths", "tests"), 
                           valVar="vpm7", 
                           groupVars=c("state", "cluster", "name"), 
                           arrangeVars=c("date"), 
                           newName="cum7"
                           ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame containing the metrics
    # typeVar: the variable holding the metric type (default is 'name')
    # typeKeep: the values of typeVar to be kept
    # valVar: the variable holding the metric value (default is 'vpm7')
    # groupVars: groups for calculating cumulative sum
    # arrangeVars: variables to be sorted by before calculating cumulative sum
    # newName: the name for the new variable
    
    # Create the cumulative data
    dfCum <- df %>%
        filter(get(typeVar) %in% typeKeep, !is.na(get(valVar))) %>%
        arrange_at(vars(all_of(c(groupVars, arrangeVars)))) %>%
        group_by_at(groupVars) %>%
        mutate(!!newName:=cumsum(get(valVar))) %>%
        ungroup()
    
    # Return the processed data
    dfCum
    
}


# Function to find and flag states that are high on a key value or change in key value
findFlagStates <- function(df, 
                           keyMetricVal, 
                           keyMetricVar="name", 
                           cumVar="cum7", 
                           prevDays=30, 
                           nFlag=5
                           ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing the cumulative data
    # keyMetricVal: the metric of interest (e.g., "deaths", "cases", "tests")
    # keyMetricVar: the variable name for the column containing the metric of interest
    # cumVar: variable containing the cumulative totals
    # prevDays: the number of days previous to use for defining growth
    # nFlag: the number of states to flag for either total or growth (top-n of each)
    
    # Find top-5 in either total or recent increase
    dfFlag <- df %>%
        filter(get(keyMetricVar)==keyMetricVal, state!="cluster") %>%
        select_at(vars(all_of(c("state", "date", cumVar)))) %>%
        group_by(state) %>%
        summarize(maxVal=max(get(cumVar)), 
                  tminus=sum(ifelse(date==max(date)-lubridate::days(prevDays), get(cumVar), 0)), 
                  .groups="drop"
        ) %>%
        ungroup() %>%
        mutate(growth=maxVal-tminus, 
               rkTotal=min_rank(-maxVal), 
               rkGrowth=min_rank(-growth), 
               flag=ifelse(pmin(rkTotal, rkGrowth)<=nFlag, 1, 0)
        ) %>%
        arrange(-flag, rkTotal)
    
    # Return the top values as a vector of states
    dfFlag %>%
        filter(flag==1) %>%
        pull(state)
    
}


# Function to plot cumulative data
# Will need to call using vpm7 or cum7 depending on metric
plotCumulativeData <- function(df, 
                               keyMetricp2,
                               flagsp2,
                               p2Desc=keyMetricp2,
                               keyVar="cum7", 
                               makep1=FALSE, 
                               makep2=TRUE, 
                               otherKeyVar="vpm7", 
                               namesOtherKeyVar=""
                               ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame of cumulative data
    # keyMetricp2: the key metric to be plotted in the second plot (e.g., "deaths", "cases", "tests")
    # flagsp2: states to be treated as flagged in the second plot
    # p2Desc: the description to be used in plot 2
    # keyVar: the key variable to plot
    # makep1: boolean, whether to make the first plot
    # makep2: boolean, whether to make the second plot
    # otherKeyVar: other key variable to be used (allows for vpm7 for some variables)
    # namesOtherKeyVar: values for 'name' that should use otherKeyVar
    
    # Plot the cumulative data by cluster (if flag is set for producing this)
    if (isTRUE(makep1)) {
        p1 <- df %>%
            filter(state=="cluster") %>%
            mutate(plotVar=ifelse(name %in% namesOtherKeyVar, get(otherKeyVar), get(keyVar))) %>%
            ggplot(aes(x=date, y=plotVar)) + 
            geom_line(aes(group=cluster, color=cluster)) + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            scale_x_date(date_breaks="1 months", date_labels="%m") + 
            labs(x="Month", 
                 y="Cumulative Burden (per million)", 
                 title="Cumulative burden by segment (per million)"
            )
        print(p1)
    }
    
    
    # Plot the cumulative totals over time for one metric, and flag the highest
    if (isTRUE(makep2)) {
        p2 <- df %>%
            filter(state!="cluster", name==keyMetricp2) %>%
            mutate(bold=ifelse(state %in% flagsp2, 1, 0)) %>%
            ggplot(aes(x=date, y=get(keyVar))) + 
            geom_line(aes(group=state, color=cluster, alpha=0.4+0.6*bold, size=0.5+0.5*bold)) + 
            geom_text(data=~filter(., bold==1, date==max(date)), 
                      aes(x=date+lubridate::days(10), 
                          label=paste0(state, ": ", round(get(keyVar), 0)), 
                          color=cluster
                      ), 
                      size=3, 
                      fontface="bold"
            ) +
            scale_x_date(date_breaks="1 months", date_labels="%m") + 
            scale_alpha_identity() +
            scale_size_identity() +
            labs(x="Month", 
                 y=paste0("Cumulative ", p2Desc, " (per million)"), 
                 title=paste0("Cumulative coronavirus ", p2Desc, " by state (per million)"), 
                 subtitle="Top 5 states for total and growth rate are bolded and labelled"
            )
        print(p2)
    }
    
}


```

The code and function are then run:  
```{r, fig.height=9, fig.width=9}

# Without tests data, this is not particulary useful (removes the NA from early/late vpm7)
consCDCPos <- consolidatedCDCPlotData %>%
    ungroup() %>%
    select(state, cluster, date, name, vpm7) %>%
    arrange(state, cluster, date, name) %>%
    pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
    # mutate(pctpos=cases/tests) %>%  # deleted since there is no 'tests' field
    pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
    filter(!is.na(vpm7))

# Create a cumulative file keeping both new and total cases (cumulative for total will be meaningless)
clCDCCum <- makeCumulative(consCDCPos, 
                           typeKeep=c("new_cases", "new_deaths", "tot_cases", "tot_deaths")
                           )

# Cumulative plot of each variable
plotCumulativeData(clCDCCum,
                   keyMetricp2="",
                   flagsp2="",
                   makep1=TRUE,
                   makep2=FALSE, 
                   namesOtherKeyVar=c("tot_cases", "tot_deaths")
                   )

# Cumulative plot for new_deaths
plotCumulativeData(clCDCCum,
                   keyMetricp2="new_deaths",
                   flagsp2=findFlagStates(clCDCCum, keyMetricVal = "new_deaths")
                   )

# Cumulative pot for new_cases
plotCumulativeData(clCDCCum,
                   keyMetricp2="new_cases",
                   flagsp2=findFlagStates(clCDCCum, keyMetricVal = "new_cases")
                   )

```

Next steps are to write the full function using the updates made above.

The elements of the list are saved, for use in future functions:  
```{r}

cdcDaily_hier7_210414 <- list(stateData=stateDataCDC, 
                              dfRaw=cdcRaw, 
                              dfFiltered=cdcFiltered, 
                              dfPerCapita=cdcPerCapita, 
                              useClusters=clRules, 
                              plotData=cdcPlotData, 
                              consolidatedPlotData=consolidatedCDCPlotData, 
                              clCum=clCDCCum
                              )
saveToRDS(cdcDaily_hier7_210414, ovrWriteError=FALSE)

```

The full function is copied and updated for use with CDC daily data:  
```{r}

# Function to read, convert, and sanity check a downloaded file
readCOViDbyState <- function(fileName, 
                             checkFile=NULL, 
                             controlFields=c("positiveIncrease", "deathIncrease", "hospitalizedCurrently"), 
                             controlBy=c("state"), 
                             dateChangePlot=FALSE, 
                             dateMetricPrint=TRUE, 
                             controlByMetricPrint=TRUE, 
                             writeLog=NULL, 
                             ovrwriteLog=TRUE, 
                             isCDCDaily=FALSE
                             ) {
    
    # FUNCTION ARGUMENTS:
    # fileName: the file name for reading the data
    # checkFile: a file that can be used for comparison purposes (NULL means do not compare to anything)
    # controlFields: fields that will be explicitly checked against checkFile
    # controlBy: level of aggregation at which fields will be explicitly checked against checkFile
    # dateChangePlot: boolean, should the change in dates included be plotte rather than listed?
    # dateMetricPrint: boolean, should the list of date-metric changes be printed?
    # controlByMetricPrint: boolean, should the list of controlBy-metric changes be printed?
    # writeLog: write detailed comparison to log file (NULL means do not write)
    # ovrwriteLog: boolean, should the log be started from scratch with the date comparisons?
    # isCDCDaily: boolean, are the data from CDC (default FALSE means use CTP)
    
    # Helper function to check for similarity of key elements
    helperSimilarity <- function(newData, refData, label, countOnly=FALSE, logFile=NULL, logAppend=TRUE) {
        d1 <- setdiff(refData, newData)
        d2 <- setdiff(newData, refData)
        cat("\n\nChecking for similarity of:", label)
        cat("\nIn reference but not in current:", if(countOnly) length(d1) else d1)
        cat("\nIn current but not in reference:", if(countOnly) length(d2) else d2)
        if (countOnly & !is.null(logFile)) {
            cat("\nDetailed differences available in:", logFile)
            capture.output(cat("\nIn reference but not in current:\n", paste(d1, collapse="\n"), sep=""), 
                           cat("\nIn current but not in reference:\n", paste(d2, collapse="\n"), sep=""), 
                           file=logFile, 
                           append=logAppend
                           )
        }
        if (countOnly) return(list(d1=d1, d2=d2))
    }
    
    # Read in the file and convert the date field for proper usage in remainder of process
    df <- readr::read_csv(fileName)
    if (isTRUE(isCDCDaily)) {
        df <- df %>%
            rename(date=submission_date) %>%
            mutate(date=lubridate::mdy(date))
    } else {
        df <- df %>% 
            mutate(date=lubridate::ymd(date))
    }
    
    # Check that the file is unique by date-state
    if ((df %>% select(date, state) %>% anyDuplicated()) != 0) {
        stop("\nDuplicates by date and state, investigate and fix\n")
    } else {
        cat("\nFile is unique by state and date\n")
    }
    
    # Check for overall control totals in new file
    cat("\n\nOverall control totals in file:\n")
    df %>% 
        summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>% 
        print()
    
    # Get control totals by date for new file
    dfByDate <- df %>% 
        group_by(date) %>%
        summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
        ungroup() %>%
        pivot_longer(-date, values_to="newValue")
    
    # If there is no checkFile, then just produce a plot of the key metrics
    if (is.null(checkFile)) {
        p1 <- dfByDate %>% 
            ggplot(aes(x=date, y=newValue)) + 
            geom_line() + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            labs(title="Control totals by date for new file (no reference file)", x="", y="Summed Value")
        print(p1)
    } else {
        # Check for similarity of fields, dates, and states
        cat("\n*** COMPARISONS TO REFERENCE FILE:", deparse(substitute(checkFile)))
        helperSimilarity(newData=names(df), refData=names(checkFile), label="column names")
        helperSimilarity(newData=df %>% pull(state) %>% unique(), 
                         refData=checkFile %>% pull(state) %>% unique(), 
                         label="states"
        )
        dateChangeList <- helperSimilarity(newData=df %>% 
                                               pull(date) %>% 
                                               unique() %>% 
                                               format("%Y-%m-%d") %>%
                                               sort(), 
                                           refData=checkFile %>% 
                                               pull(date) %>% 
                                               unique() %>% 
                                               format("%Y-%m-%d") %>%
                                               sort(),
                                           label="dates", 
                                           countOnly=dateChangePlot, 
                                           logFile=writeLog, 
                                           logAppend=!ovrwriteLog
                                           )
        
        # Plot date changes if requested
        if (dateChangePlot) {
            pDate <- tibble::tibble(date=as.Date(c(dateChangeList$d1, dateChangeList$d2)), 
                                    type=c(rep("Control File Only", length(dateChangeList$d1)), 
                                           rep("New File Only", length(dateChangeList$d2))
                                           )
                                    ) %>%
                ggplot(aes(x=date, fill=type)) + 
                geom_bar() + 
                coord_flip() + 
                labs(x="", y="", title="Dates in one file and not in the other")
            print(pDate)
        }
        
        # Check for similarity of control totals by date in files
        checkByDate <- checkFile %>% 
            group_by(date) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-date, values_to="oldValue")
        deltaDate <- dfByDate %>% 
            inner_join(checkByDate, by=c("date", "name")) %>%
            filter(abs(newValue-oldValue)>=5, 
                   pmax(newValue, oldValue)>=1.01*pmin(newValue, oldValue)
            ) %>%
            as.data.frame()
        cat("\n\nDifference of 5+ that is at least 1% (summed to date and metric):", nrow(deltaDate))
        if (dateMetricPrint) {
            cat("\n")
            print(deltaDate)
        }
        else if (!is.null(writeLog)) {
            cat("\nDetailed output available in log:", writeLog)
            capture.output(cat("\n\nChange by date:\n"), print(deltaDate), file=writeLog, append=TRUE)
        }
        p1 <- dfByDate %>% 
            full_join(checkByDate, by=c("date", "name")) %>%
            pivot_longer(-c(date, name), names_to="newOld") %>%
            ggplot(aes(x=date, y=value, group=newOld, color=newOld)) + 
            geom_line() + 
            facet_wrap(~name, nrow=1, scales="free_y") + 
            labs(title="Control totals by date for new and reference file", x="", y="Summed Value")
        print(p1)
        
        # Check for similarity of control totals by controlBy in files
        dfByControl <- df %>% 
            semi_join(select(checkFile, date), by="date") %>%
            group_by_at(vars(all_of(controlBy))) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-all_of(controlBy), values_to="newValue")
        checkByControl <- checkFile %>% 
            group_by_at(vars(all_of(controlBy))) %>%
            summarize_at(vars(all_of(controlFields)), .funs=sum, na.rm=TRUE) %>%
            ungroup() %>%
            pivot_longer(-all_of(controlBy), values_to="oldValue")
        deltaBy <- dfByControl %>% 
            inner_join(checkByControl, by=c(controlBy, "name")) %>%
            filter(abs(newValue-oldValue)>=5, 
                   pmax(newValue, oldValue)>=1.01*pmin(newValue, oldValue)
            ) %>%
            as.data.frame()
        cat("\n\nDifference of 5+ that is at least 1% (summed to", 
            controlBy, 
            "and metric):", 
            nrow(deltaBy), 
            "\n"
            )
        if (controlByMetricPrint) print(deltaBy)
    }
    
    # Return the processed data file
    df
    
}


# Function to select relevant variables and observations, and report on control totals
processCVData <- function(dfFull, 
                          varsKeep=c("date", "state", "positiveIncrease", "deathIncrease"), 
                          varsRename=c("positiveIncrease"="cases", "deathIncrease"="deaths"), 
                          stateList=c(state.abb, "DC"), 
                          isCDCDaily=FALSE,
                          comboStates=if(isTRUE(isCDCDaily)) c("NYC"="NY", "NY"="NY") else c(),
                          sumBy=c("state", "date")
                          ) {
    
    # FUNCTION ARGUMENTS
    # dfFull: the full data file originally loaded
    # varsKeep: variables to keep from the full file
    # varsRename: variables to be renamed, using a named vector of form originalName=newName
    # stateList: variables for filtering state (NULL means do not run any filters)
    # isCDCDaily: boolean, are the daily data from CDC
    # comboStates: states to be consolidated ('name in file'='name to use')
    # sumBy: variables to sum by (after changing state names using comboStates)
    
    # Select only the key variables
    df <- dfFull %>%
        select_at(vars(all_of(varsKeep)))
    
    # Apply the renaming of variables
    names(df) <- ifelse(is.na(varsRename[names(df)]), names(df), varsRename[names(df)])
    
    # Apply state name changes and sum
    df <- df %>%
        mutate(state=ifelse(state %in% names(comboStates), unname(comboStates[state]), state)) %>%
        group_by_at(vars(all_of(sumBy))) %>%
        summarize_if(is.numeric, sum, na.rm=TRUE) %>%
        ungroup()
    
    # Designate each record as being either a valid state or not
    if (!is.null(stateList)) {
        df <- df %>%
            mutate(validState=state %in% stateList)
    } else {
        df <- df %>%
            mutate(validState=TRUE)
    }
    
    # Summarize the control totals for the data, based on whether the state is valid
    cat("\n\nControl totals - note that validState other than TRUE will be discarded\n(na.rm=TRUE)\n\n")
    df %>%
        mutate(n=1) %>%
        group_by(validState) %>%
        summarize_if(is.numeric, sum, na.rm=TRUE) %>%
        print()
    
    # Return the file, filtered to where validState is TRUE, and deleting variable validState
    df %>%
        filter(validState) %>%
        select(-validState)
    
}


# Function to download and process data (steps 2-4)
helperReadRunCTP_steps02_04 <- function(downloadTo, 
                                        isCDCDaily, 
                                        readFrom, 
                                        compareFile,
                                        dateChangePlot,
                                        dateMetricPrint,
                                        writeLog,
                                        ovrwriteLog, 
                                        stateData,
                                        perCapitaVarsMap=NULL
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # downloadTo: download the most recent COVID Tracking Project data to this location
    #             NULL means do not download any data
    # isCDCDaily: boolean, are the data from CDC (default FALSE means use CTP)
    # readFrom: location for reading in the COVID Tracking Project data (defaults to donwloadTo)
    # compareFile: name of the file to use for comparisons when reading in raw data (NULL means no comparison)
    # dateChangePlot: boolean, should changes in dates be captured as a plot rather than as a list?
    # dateMetricPrint: boolean, should the changes by date and metric be printed to the main log?
    # writeLog: name of a separate log file for capturing detailed data on changes between files
    #           NULL means no detailed data captured
    # ovrwriteLog: boolean, should the log file be overwritten and started again from scratch?
    # stateData: state population data file
    # perCapitaVarsMap: mapVars to be used in helperMakePerCapita (NULL means create based on isCDCDaily)
    
    # Helper function for glimpsing
    glimpseFile <- function(x, txt) {
        cat(txt)
        glimpse(x)
    }
    
    # STEP 2a: Download latest COVID Tracking Project data (if requested)
    if (!is.null(downloadTo)) {
        
        # Use the proper API depending on whether the data are from CDC
        if (isTRUE(isCDCDaily)) {
            api <- api="https://data.cdc.gov/api/views/9mfq-cb36/rows.csv?accessType=DOWNLOAD"
        }
        else api <- "https://api.covidtracking.com/v1/states/daily.csv"
        
        # Download the relevant data to the proper location
        downloadCOVIDbyState(fileName=downloadTo, api=api)
    }
        
    # STEP 2b: Read-in COVID Tracking Project data
    # Use a different reading process for CDC daily data and CTP data
    if (isTRUE(isCDCDaily)) controlFields <- c("tot_cases", "tot_death", "new_case", "new_death")
    else controlFields <- c("positiveIncrease", "deathIncrease", "hospitalizedCurrently")
    dfRaw <- readCOViDbyState(readFrom, 
                              checkFile=compareFile, 
                              controlFields=controlFields,
                              dateChangePlot=dateChangePlot, 
                              dateMetricPrint=dateMetricPrint, 
                              writeLog=writeLog, 
                              ovrwriteLog=ovrwriteLog, 
                              isCDCDaily=isCDCDaily
                              )
    # Glimpse the raw data, either in the main log file or in a separate log file
    if (is.null(writeLog)) glimpseFile(dfRaw, txt="\nRaw data file:\n")
    else capture.output(glimpseFile(dfRaw, txt="\nRaw data file:\n"), file=writeLog, append=TRUE)
        
        
    # STEP 3: Process the data so that it includes all requested key variables
    # Change variables to keep and renames depending on data source
    if (isTRUE(isCDCDaily)) {
        varsFilter <- c("date", "state", "tot_cases", "tot_death", "new_case", "new_death")
        varsRename <- c(tot_death="tot_deaths", new_case="new_cases", new_death="new_deaths")
    } else {
        varsFilter <- c("date", "state", "positiveIncrease", "deathIncrease", 
                        "hospitalizedCurrently", "totalTestResultsIncrease"
                        )
        varsRename <- c(positiveIncrease="cases", 
                        deathIncrease="deaths", 
                        hospitalizedCurrently="hosp", 
                        totalTestResultsIncrease="tests"
                        )
    }
    # Run the filtering process using the parameters above
    dfFiltered <- processCVData(dfRaw, varsKeep=varsFilter, varsRename=varsRename, isCDCDaily=isCDCDaily)
    
    # Glimpse the processed data, either in the main log file or in a separate log file
    if (is.null(writeLog)) glimpseFile(dfFiltered, txt="\nFiltered data file:\n")
    else capture.output(glimpseFile(dfFiltered, txt="\nFiltered data file:\n"), file=writeLog, append=TRUE)
        
        
    # STEP 4: Convert to per capita
    # Create the appropriate perCapitaVarsMap if it has not been passed
    if (is.null(perCapitaVarsMap)) {
        if(isTRUE(isCDCDaily)) {
            perCapitaVarsMap <- c("new_cases"="cpm", "new_deaths"="dpm", 
                                  "tot_cases"="tcpm", "tot_deaths"="tdpm"
                                  )
        } else {
            perCapitaVarsMap <- c("cases"="cpm", "deaths"="dpm", "hosp"="hpm", "tests"="tpm")
        }
    }
    
    # Convert to per capita
    dfPerCapita <- helperMakePerCapita(dfFiltered, mapVars=perCapitaVarsMap, popData=stateData)
    
    # Glimpse the per-capita data, either in the main log file or in a separate log file
    if (is.null(writeLog)) glimpseFile(dfPerCapita, txt="\nPer capita data file:\n")
    else capture.output(glimpseFile(dfPerCapita, txt="\nPer capita data file:\n"), file=writeLog, append=TRUE)
    
    # Return the three key elements as a list
    list(dfRaw=dfRaw, dfFiltered=dfFiltered, dfPerCapita=dfPerCapita)
    
}


# Function to download/load, process, segment, and analyze data from COVID Tracking Project
readRunCOVIDTrackingProject <- function(thruLabel, 
                                        downloadTo=NULL, 
                                        readFrom=downloadTo, 
                                        compareFile=NULL,
                                        dateChangePlot=FALSE,
                                        dateMetricPrint=TRUE,
                                        writeLog=NULL,
                                        ovrwriteLog=TRUE,
                                        dfPerCapita=NULL,
                                        useClusters=NULL,
                                        hierarchical=TRUE,
                                        returnList=!isTRUE(hierarchical), 
                                        kCut=6,
                                        reAssignState=vector("list", 0),
                                        makeCumulativePlots=TRUE,
                                        skipAssessmentPlots=FALSE,
                                        isCDCDaily=FALSE,
                                        ...
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # thruLabel: the label for when the data are through (e.g., "Aug 30, 2020")
    # downloadTo: download the most recent COVID Tracking Project data to this location
    #             NULL means do not download any data
    # readFrom: location for reading in the COVID Tracking Project data (defaults to donwloadTo)
    # compareFile: name of the file to use for comparisons when reading in raw data (NULL means no comparison)
    # dateChangePlot: boolean, should changes in dates be captured as a plot rather than as a list?
    # dateMetricPrint: boolean, should the changes by date and metric be printed to the main log?
    # writeLog: name of a separate log file for capturing detailed data on changes between files
    #           NULL means no detailed data captured
    # ovrwriteLog: boolean, should the log file be overwritten and started again from scratch?
    # dfPerCapita: file can be passed directly, which bypasses the loading and processing steps
    # useClusters: file containing clusters by state (NULL means make the clusters from the data)
    # hierarchical: boolean, should hierarchical clusters be produced (if FALSE, will be k-means)?
    # returnList: boolean, should a list be returned or just the cluster object?
    #             refers to what is returned by clusterStates(); the main function always returns a list
    # kCut: number of segments when cutting the hierarchical tree
    # reAssignState: mapping file for assigning a state to another state's cluster
    #                format list("stateToChange"="stateClusterToAssign")
    # makeCumulativePlots: whether to make plots of cumulative metrics
    # skipAssessmentPlots: boolean to skip the plots for assessClusters()
    #                      especially useful if just exploring dendrograms or silhouette widths
    # isCDCDaily: boolean, are the data from CDC (default FALSE means use CTP)
    # ...: arguments to be passed to clusterStates(), will be used only if useClusters is NULL
    
    
    # STEP 1: Get state data
    stateData <- getStateData()
    
    
    # STEPS 2-4 are run only if dfPerCapita does not exist
    if (is.null(dfPerCapita)) {
        # Call the helper function and pass the key arguments
        helpList <- helperReadRunCTP_steps02_04(downloadTo=downloadTo, 
                                                isCDCDaily=isCDCDaily, 
                                                readFrom=readFrom, 
                                                compareFile=compareFile, 
                                                dateChangePlot=dateChangePlot, 
                                                dateMetricPrint=dateMetricPrint, 
                                                writeLog=writeLog, 
                                                ovrwriteLog=ovrwriteLog, 
                                                stateData=stateData
                                                )
        # Extract the key data files from the helper list for future processing
        dfRaw <- helpList$dfRaw
        dfFiltered <- helpList$dfFiltered
        dfPerCapita <- helpList$dfPerCapita
    } else {
        # There is no raw or filtered data since a properly formatted dfPerCapita was passed
        dfRaw <- NULL
        dfFiltered <- NULL
    }
    
    # STEP 5: Create the clusters (if they have not been passed)
    if (is.null(useClusters)) {
        # Run the clustering process
        clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
        # If hierarchical clusters, cut the tree, otherwise use the output object directly
        if (isTRUE(hierarchical)) {
            useClusters <- cutree(clData, k=kCut)
        } else {
            useClusters <- if(is.na(hierarchical)) clData$objCluster else clData$objCluster$cluster
        }
        # If requested, manually assign clusters to the cluster for another state
        for (xNum in seq_len(length(reAssignState))) {
            useClusters[names(reAssignState)[xNum]] <- useClusters[reAssignState[[xNum]]]
        }
        
    }
    
    # STEP 5a: Stop the process and return what is available if skipAssessmentPlots is TRUE
    if (skipAssessmentPlots) {
        return(list(stateData=stateData, 
                    dfRaw=dfRaw, 
                    dfFiltered=dfFiltered, 
                    dfPerCapita=dfPerCapita, 
                    useClusters=useClusters, 
                    plotData=NULL, 
                    consolidatedPlotData=NULL, 
                    clCum=NULL
                    )
               )
    }
    
    
    # STEP 6: Create the cluster assessments
    p4Def <- eval(formals(assessClusters)$p4Vars)  # eval(formals(fun)$arg) returns the default arg in fun
    plotData <- assessClusters(useClusters, 
                               dfState=stateData, 
                               dfBurden=dfPerCapita,
                               thruLabel=thruLabel,
                               plotsTogether=TRUE, 
                               p4Vars=if(isCDCDaily) c(cases="tot_cases", deaths="tot_deaths") else p4Def,
                               p4Fun=if(isCDCDaily) max else eval(formals(assessClusters)$p4Fun), 
                               isCDC=isCDCDaily
                               )
    
    
    # STEP 7: Plot the consolidated metrics
    if (isCDCDaily) {
        subT <- "new_cases: new cases, new_deaths: new deaths"
        subT <- paste0(subT, ", tot_cases: cumulative cases, tot_deaths: cumulative deaths")
        varMain=c("state", "cluster", "date", "pop", "new_cases", "new_deaths", "tot_cases", "tot_deaths")
    } else {
        subT <- "Cases: new cases, Deaths: new deaths, Hosp: total in hospital (not new), Tests: new tests"
        varMain <- c("state", "cluster", "date", "pop", "cases", "deaths", "hosp", "tests")
    }
    consolidatedPlotData <- plotConsolidatedMetrics(plotData, varMain=varMain, subT=subT, nrowPlot2=2)
    
    
    # STEP 8: Create cumulative metrics if requested
    if (makeCumulativePlots) {
        consPos <- consolidatedPlotData %>%
            ungroup() %>%
            select(state, cluster, date, name, vpm7) %>%
            arrange(state, cluster, date, name) %>%
            pivot_wider(-vpm7, names_from="name", values_from="vpm7") %>%
            mutate(pctpos=if(isTRUE(isCDCDaily)) 0 else cases/tests) %>%
            pivot_longer(-c(state, cluster, date), values_to="vpm7") %>%
            filter(!is.na(vpm7))
        typeKeep <- eval(formals(makeCumulative)$typeKeep)
        if(isTRUE(isCDCDaily)) typeKeep <- c("new_cases", "new_deaths", "tot_cases", "tot_deaths")
        clCum <- makeCumulative(consPos, typeKeep=typeKeep)
        # Cumulative plot of each variable
        a <- if(isTRUE(isCDCDaily)) c("tot_cases", "tot_deaths") else ""
        plotCumulativeData(clCum, keyMetricp2="", flagsp2="", makep1=TRUE, makep2=FALSE, namesOtherKeyVar=a)
        # Cumulative plots of relevant key variables
        a <- if(isTRUE(isCDCDaily)) c("new_deaths", "new_cases") else c("deaths", "cases", "tests")
        plotCumulativeData(clCum, keyMetricp2=a[1], flagsp2=findFlagStates(clCum, keyMetricVal=a[1]))
        plotCumulativeData(clCum, keyMetricp2=a[2], flagsp2=findFlagStates(clCum, keyMetricVal=a[2]))
        if(!isTRUE(isCDCDaily)) {
            plotCumulativeData(clCum, keyMetricp2=a[3], flagsp2=findFlagStates(clCum, keyMetricVal=a[3]))
        }
    } else {
        clCum <- NULL
    }
    
    
    # STEP 9: Return a list of the key data
    list(stateData=stateData, 
         dfRaw=dfRaw, 
         dfFiltered=dfFiltered, 
         dfPerCapita=dfPerCapita, 
         useClusters=useClusters, 
         plotData=plotData, 
         consolidatedPlotData=consolidatedPlotData, 
         clCum=clCum
    )
    
    
}


```
  
The function is then run using previously downloaded data:  
```{r, fig.height=9, fig.width=9}

# Test the reading process only (works provided date is managed OK)
# readCOViDbyState(fileName="./RInputFiles/Coronavirus/CDC_dc_downloaded_210414.csv", 
#                  checkFile=readFromRDS("cdcDaily_hier7_210414")$dfRaw %>%
#                      mutate(date=lubridate::mdy(submission_date)), 
#                  controlFields=c("tot_cases", "tot_death", "new_case", "new_death"),
#                  dateChangePlot=TRUE,
#                  isCDCDaily=TRUE
#                  )

# Test full process using April 14, 2021 data
locDownload <- "./RInputFiles/Coronavirus/CDC_dc_downloaded_210414.csv"
locRDS <- "cdcDaily_hier7_210414"
cdcDaily_rules7_210421 <- readRunCOVIDTrackingProject(thruLabel="Apr 14, 2021", 
                                                      readFrom=locDownload, 
                                                      compareFile=readFromRDS(locRDS)$dfRaw %>%
                                                          mutate(date=lubridate::mdy(submission_date)), 
                                                      isCDCDaily=TRUE,
                                                      hierarchical=NA,
                                                      shapeFunc=customTimeBucket,
                                                      minShape="2020-04", 
                                                      maxShape="2021-03", 
                                                      ratioDeathvsCase = 5, 
                                                      ratioTotalvsShape = 0.25, 
                                                      hmlSegs=3, 
                                                      eslSegs=3, 
                                                      seed=2104271501,
                                                      minDeath=100, 
                                                      minCase=10000, 
                                                      reAssignState=list("SD"="ND")
                                                      )

```
  
The function is checked to confirm that it still works on legacy CTP data:  
```{r, fig.height=9, fig.width=9}

# Confirm that process still works for final CTP data
test_oldctp <- readRunCOVIDTrackingProject(thruLabel="Mar 31, 2021",
                                           readFrom="./RInputFiles/Coronavirus/CV_downloaded_210401.csv",
                                           compareFile=readFromRDS("ctp_hier6_210301")$dfRaw,
                                           hierarchical=TRUE,
                                           kCut=6,
                                           shapeFunc=customTimeBucket,
                                           minShape="2020-04",
                                           maxShape="2021-03",
                                           ratioDeathvsCase = 5,
                                           ratioTotalvsShape = 0.25,
                                           minDeath=100,
                                           minCase=10000
                                           )

```
  
Next steps are to search for hospital and test data that can be integrated to more closely replicate what was available from CTP.

Hospital data are downloaded, with the process cached to avoid repeated hits against the CDC website:  
```{r cache=TRUE}

downloadCOVIDbyState("./RInputFiles/Coronavirus/CDC_h_downloaded_210429.csv",
                     api="https://beta.healthdata.gov/api/views/g62h-syeh/rows.csv?accessType=DOWNLOAD", 
                     ovrWrite=FALSE
                     )

```

Fields available in the dataset are described at  [healthdata.gov](https://beta.healthdata.gov/Hospital/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/g62h-syeh).  Salient fields include:  
  
* state - state (includes 50 states, DC, PR, VI)  
* date - date of reported data  
* inpatient_beds_used_covid - reported patients currently hospitalized with confirmed or suspected COVID-19  
* total_adult_patients_hospitalized_confirmed_and_suspected_covid - reported patients in adult inpatient beds with confirmed or suspected COVID-19 (includes observation beds)  
* total_pediatric_patients_hospitalized_confirmed_and_suspected_covid - reported patients in pediatric inpatient beds with confirmed or suspected COVID-19 (includes observation beds)  
* staffed_icu_adult_patients_confirmed_and_suspected_covid - reported patients in an adult ICU bed who have suspected or confirmed COVID-19  
* previous_day_admission_[pediatric|adult]_covid_[suspected|confirmed] - previous day admissions to a hospital with confirmed or suspected CoVID-19  
  
Data availability vary over time, and there are also coverage variables included for each of the key statistics.  Data are loaded, and a plot of data availability by state and metric is created:  
```{r, fig.height=9, fig.width=9}

# Read and glimpse downloaded hospital file
hospRaw <- readr::read_csv("./RInputFiles/Coronavirus/CDC_h_downloaded_210429.csv")
glimpse(hospRaw)

# Plot of states with data availability by metric and month
hospRaw %>% 
    mutate(tb=customTimeBucket(date)) %>% 
    group_by(state, tb) %>% 
    summarize_if(is.numeric, sum, na.rm=TRUE) %>% 
    group_by(tb) %>% 
    summarize_all(.funs=function(x) sum(x>0)) %>% 
    pivot_longer(-tb) %>% 
    ggplot(aes(x=tb, y=name)) + 
    geom_tile(aes(fill=value)) + 
    geom_text(aes(label=value)) + 
    scale_fill_continuous(low="white", high="green")

# Check state coverage
hospRaw %>%
    count(state) %>%
    pull(state) %>%
    setdiff(state.abb)

```
  
There are 53 states included (the 50 US states plus DC, PR, VI).  Many of the salient metrics became available in July 2020, though inpatient_beds_used_covid appears to be available throughout.  Three key fields are explored in more detail:  
```{r, fig.height=9, fig.width=9}

# Hospital summary by date
hospSum <- hospRaw %>%
    select(date, 
           inp=inpatient_beds_used_covid, 
           hosp_adult=total_adult_patients_hospitalized_confirmed_and_suspected_covid, 
           hosp_ped=total_pediatric_patients_hospitalized_confirmed_and_suspected_covid
           ) %>%
    group_by(date) %>%
    summarize_all(sum, na.rm=TRUE)

# Summary of 'hosp' field from CTP
hospCTP <- ctp_hier6_210401$dfFiltered %>%
    group_by(date) %>%
    summarize(hosp_ctp=sum(hosp, na.rm=TRUE), .groups="drop")

# Plotted by date, compared with field 'hosp' from CTP
hospSum %>%
    full_join(hospCTP, by="date") %>%
    pivot_longer(-date) %>%
    filter(!is.na(value)) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, color=name)) + 
    labs(title="COVID Hospitalized by Data Source", x="", y="Hospitalized") + 
    scale_color_discrete("Metric")

```
  
COVID Tracking Project ran a data cleaning algorithm, resulting in some differences.  The field "inp" appears to track the CTP metric reasonably well, and the alignment by state for April 2020 through February 2021 is explored:  
```{r, fig.height=9, fig.width=9}

hospCompare <- hospRaw %>%
    select(state, 
           date, 
           inp=inpatient_beds_used_covid
           ) %>%
    full_join(select(ctp_hier6_210401$dfFiltered, state, date, ctp=hosp), by=c("state", "date")) %>%
    filter(state %in% c(state.abb, "DC"), date >= as.Date("2020-04-01"), date <= as.Date("2021-02-28"))

# Get correlations by state (only for dates where both supply data)
hospCompare %>%
    is.na() %>%
    colSums()

hospCorr <- hospCompare %>%
    filter(!is.na(ctp)) %>%
    group_by(state) %>%
    summarize(rmse=sqrt(mean((inp-ctp)**2)), 
              corr=cor(inp, ctp), 
              inp=mean(inp), 
              ctp=mean(ctp),
              rsq=1-rmse/ctp, 
              .groups="drop"
              )

# Plot states by correlation, rsq, and RMSE
hospCorr %>%
    select(state, rmse, corr, .pseudorsq=rsq) %>%
    pivot_longer(-state) %>%
    ggplot(aes(x=fct_reorder(state, value, .fun=min), y=value)) + 
    geom_col(fill="lightblue") + 
    coord_flip() +
    facet_wrap(~name, scales="free_x")

# Plot by individual states, with 'most different' first
hospDiff <- hospCorr %>%
    arrange(rsq) %>%
    pull(state)

hospCompare %>%
    mutate(state=factor(state, levels=hospDiff)) %>%
    pivot_longer(-c(state, date)) %>%
    filter(!is.na(value)) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, color=name)) + 
    facet_wrap(~state, scales="free_y")

```

While there are some meaningful disconnects in the data, particularly in the earlier time periods, there is generally good correlation of the timing and magnitude of peaks and troughs.  The new hospitalized data appear reasonable for usage with the CDC daily data for purposes of highlighting macro trends in disease impact by state.  For potential future exploration, the CTP data could be used through 2020 with the new data picked up starting in 2021.

Hospital data are manually added to the existing dfPerCapita file:  
```{r}

# Create the hospital data file
hospData <- hospRaw %>%
    select(state, date, inp=inpatient_beds_used_covid) %>%
    filter(state %in% c(state.abb, "DC"), !is.na(inp)) %>%
    arrange(date, state)

# Check that there are no duplicates
if (hospData %>% select(state, date) %>% anyDuplicated() != 0) stop("\nDuplicates in the hospital data\n")

# Create hpm (per capita) and hpm7 (per capita rolling 7) variables
hospDataPC <- hospData %>%
    helperMakePerCapita(mapVars=c("inp"="hpm"), popData=getStateData())
hospDataPC %>% summarize(across(where(is.numeric), sum, na.rm=TRUE))

# Create a full per-capita file, taking only state-date combinations that are in dfPerCapita
dfPerCapitaTemp <- cdcDaily_rules7_210421$dfPerCapita %>%
    left_join(hospDataPC, by=c("state", "date"))
dfPerCapitaTemp %>% summarize(across(where(is.numeric), sum, na.rm=TRUE))

```

The file can then be run through the main function, with the per capita file and existing clusters passed:  
```{r eval=FALSE}

# Test full process using hospital data
updDaily_rules7_210421 <- readRunCOVIDTrackingProject(thruLabel="Apr 14, 2021", 
                                                      isCDCDaily=TRUE,
                                                      dfPerCapita=dfPerCapitaTemp, 
                                                      useClusters=cdcDaily_rules7_210421$useClusters
                                                      )

```

The main function requires updates so that it can optionally use the hospital (or any other) data:  
```{r}

# Create a subtitle mapping file (variable to descriptive name)
subTMapper <- c("new_cases"="new cases", 
                "new_deaths"="new deaths", 
                "tot_cases"="cumulative cases", 
                "tot_deaths"="cumulative deaths", 
                "inp"="total hospitalized (not new or cumulative)", 
                "cases"="new cases", 
                "deaths"="new deaths", 
                "hosp"="total in hospital (not new)", 
                "tests"="new tests"
                )


# Helper function to make consolidated cumulative data
helperConsCum <- function(df, 
                          byVars=c("state", "cluster", "date"),
                          nameVar=c("name"),
                          numVar=c("vpm7"), 
                          makePctPos=FALSE
                          ) {
    df %>%
        ungroup() %>%
        select_at(vars(all_of(c(byVars, nameVar, numVar)))) %>%
        arrange_at(vars(all_of(c(byVars, nameVar)))) %>%
        pivot_wider(-all_of(numVar), names_from=nameVar, values_from=numVar) %>%
        mutate(pctpos=if(isTRUE(makePctPos)) cases/tests else 0) %>%
        pivot_longer(-c(all_of(byVars)), values_to=numVar) %>%
        filter(!is.na(get(numVar)))
}


# Function to download/load, process, segment, and analyze data from COVID Tracking Project
readRunCOVIDTrackingProject <- function(thruLabel, 
                                        downloadTo=NULL, 
                                        readFrom=downloadTo, 
                                        compareFile=NULL,
                                        dateChangePlot=FALSE,
                                        dateMetricPrint=TRUE,
                                        writeLog=NULL,
                                        ovrwriteLog=TRUE,
                                        dfPerCapita=NULL,
                                        useClusters=NULL,
                                        hierarchical=TRUE,
                                        returnList=!isTRUE(hierarchical), 
                                        kCut=6,
                                        reAssignState=vector("list", 0),
                                        makeCumulativePlots=TRUE,
                                        skipAssessmentPlots=FALSE,
                                        varMainS7=NULL,
                                        mapperS7=subTMapper,
                                        typeKeepS8=NULL,
                                        isCDCDaily=FALSE,
                                        ...
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # thruLabel: the label for when the data are through (e.g., "Aug 30, 2020")
    # downloadTo: download the most recent COVID Tracking Project data to this location
    #             NULL means do not download any data
    # readFrom: location for reading in the COVID Tracking Project data (defaults to donwloadTo)
    # compareFile: name of the file to use for comparisons when reading in raw data (NULL means no comparison)
    # dateChangePlot: boolean, should changes in dates be captured as a plot rather than as a list?
    # dateMetricPrint: boolean, should the changes by date and metric be printed to the main log?
    # writeLog: name of a separate log file for capturing detailed data on changes between files
    #           NULL means no detailed data captured
    # ovrwriteLog: boolean, should the log file be overwritten and started again from scratch?
    # dfPerCapita: file can be passed directly, which bypasses the loading and processing steps
    # useClusters: file containing clusters by state (NULL means make the clusters from the data)
    # hierarchical: boolean, should hierarchical clusters be produced (if FALSE, will be k-means)?
    # returnList: boolean, should a list be returned or just the cluster object?
    #             refers to what is returned by clusterStates(); the main function always returns a list
    # kCut: number of segments when cutting the hierarchical tree
    # reAssignState: mapping file for assigning a state to another state's cluster
    #                format list("stateToChange"="stateClusterToAssign")
    # makeCumulativePlots: whether to make plots of cumulative metrics
    # skipAssessmentPlots: boolean to skip the plots for assessClusters()
    #                      especially useful if just exploring dendrograms or silhouette widths
    # varMainS7: main variables to use for step 7 (consolidated metrics): NULL means assume CTP defaults
    # mapperS7: mapping file for step 7 (variables to descriptive names)
    # typeKeepS8: variables to be plotted in step 8 (NULL means assume CTP defaults)
    # isCDCDaily: boolean, are the data from CDC (default FALSE means use CTP)
    # ...: arguments to be passed to clusterStates(), will be used only if useClusters is NULL
    
    
    # STEP 1: Get state data
    stateData <- getStateData()
    
    # STEPS 2-4 are run only if dfPerCapita does not exist
    if (is.null(dfPerCapita)) {
        # Call the helper function and pass the key arguments
        helpList <- helperReadRunCTP_steps02_04(downloadTo=downloadTo, 
                                                isCDCDaily=isCDCDaily, 
                                                readFrom=readFrom, 
                                                compareFile=compareFile, 
                                                dateChangePlot=dateChangePlot, 
                                                dateMetricPrint=dateMetricPrint, 
                                                writeLog=writeLog, 
                                                ovrwriteLog=ovrwriteLog, 
                                                stateData=stateData
                                                )
        # Extract the key data files from the helper list for future processing
        dfRaw <- helpList$dfRaw
        dfFiltered <- helpList$dfFiltered
        dfPerCapita <- helpList$dfPerCapita
    } else {
        # There is no raw or filtered data since a properly formatted dfPerCapita was passed
        dfRaw <- NULL
        dfFiltered <- NULL
    }
    
    # STEP 5: Create the clusters (if they have not been passed)
    if (is.null(useClusters)) {
        # Run the clustering process
        clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
        # If hierarchical clusters, cut the tree, otherwise use the output object directly
        if (isTRUE(hierarchical)) {
            useClusters <- cutree(clData, k=kCut)
        } else {
            useClusters <- if(is.na(hierarchical)) clData$objCluster else clData$objCluster$cluster
        }
        # If requested, manually assign clusters to the cluster for another state
        for (xNum in seq_len(length(reAssignState))) {
            useClusters[names(reAssignState)[xNum]] <- useClusters[reAssignState[[xNum]]]
        }
        
    }
    
    # STEP 5a: Stop the process and return what is available if skipAssessmentPlots is TRUE
    if (skipAssessmentPlots) {
        return(list(stateData=stateData, 
                    dfRaw=dfRaw, 
                    dfFiltered=dfFiltered, 
                    dfPerCapita=dfPerCapita, 
                    useClusters=useClusters, 
                    plotData=NULL, 
                    consolidatedPlotData=NULL, 
                    clCum=NULL
                    )
               )
    }
    
    # STEP 6: Create the cluster assessments using cases and deaths
    p4Def <- eval(formals(assessClusters)$p4Vars)  # eval(formals(fun)$arg) returns the default arg in fun
    plotData <- assessClusters(useClusters, 
                               dfState=stateData, 
                               dfBurden=dfPerCapita,
                               thruLabel=thruLabel,
                               plotsTogether=TRUE, 
                               p4Vars=if(isCDCDaily) c(cases="tot_cases", deaths="tot_deaths") else p4Def,
                               p4Fun=if(isCDCDaily) max else eval(formals(assessClusters)$p4Fun), 
                               isCDC=isCDCDaily
                               )
    
    
    # STEP 7: Plot the consolidated metrics
    # Create CTP defaults if passed as NULL
    if (is.null(varMainS7)) 
        varMainS7 <- c("state", "cluster", "date", "pop", "cases", "deaths", "hosp", "tests")
    # Create description for subtitle
    subT <- varMainS7[varMainS7 %in% names(subTMapper)] %>% 
        paste(., subTMapper[.] %>% unname, sep=": ") %>% 
        paste0(collapse=", ")
    # Create consolidated metrics plot
    consolidatedPlotData <- plotConsolidatedMetrics(plotData, varMain=varMainS7, subT=subT, nrowPlot2=2)
    
    
    # STEP 8: Create cumulative metrics if requested
    if (makeCumulativePlots) {
        consPos <- helperConsCum(consolidatedPlotData)
        if (is.null(typeKeepS8)) typeKeepS8 <- eval(formals(makeCumulative)$typeKeep)
        clCum <- makeCumulative(consPos, typeKeep=typeKeepS8)
        # Cumulative plot of each variable (for CDC daily, tot_cases and tot_deaths are already cumulative)
        a <- if(isTRUE(isCDCDaily)) c("tot_cases", "tot_deaths") else ""
        plotCumulativeData(clCum, keyMetricp2="", flagsp2="", makep1=TRUE, makep2=FALSE, namesOtherKeyVar=a)
        # Cumulative plots of relevant key variables
        a <- if(isTRUE(isCDCDaily)) c("new_deaths", "new_cases") else c("deaths", "cases", "tests")
        for (vrbl in a) 
            plotCumulativeData(clCum, keyMetricp2=vrbl, flagsp2=findFlagStates(clCum, keyMetricVal=vrbl))
    } else {
        clCum <- NULL
    }
    
    
    # STEP 9: Return a list of the key data
    list(stateData=stateData, 
         dfRaw=dfRaw, 
         dfFiltered=dfFiltered, 
         dfPerCapita=dfPerCapita, 
         useClusters=useClusters, 
         plotData=plotData, 
         consolidatedPlotData=consolidatedPlotData, 
         clCum=clCum
    )
    
}

```

The routine is then run:  
```{r, fig.height=9, fig.width=9}

# Test full process using hospital data
updDaily_rules7_210421 <- readRunCOVIDTrackingProject(thruLabel="Apr 14, 2021", 
                                                      dfPerCapita=dfPerCapitaTemp, 
                                                      useClusters=cdcDaily_rules7_210421$useClusters, 
                                                      varMainS7=c("state", "cluster", "date", "pop", 
                                                                  "new_cases", "new_deaths", 
                                                                  "tot_cases", "tot_deaths", "inp"
                                                                  ), 
                                                      typeKeepS8=c("new_cases", "new_deaths", 
                                                                   "tot_cases", "tot_deaths"
                                                                   ), 
                                                      isCDCDaily=TRUE
                                                      )

```

Next steps are to update the function so that it can download and integrate data from multiple sources (for CDC daily which keeps cases/deaths, hospitalizations, and other metrics in separate file).
