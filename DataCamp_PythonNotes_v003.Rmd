---
title: "Data Camp Python Notes"
author: "davegoblue"
date: "September 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(engine.path=list(python="C:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python36-32\\python.exe"))
```

## Background and Overview  

DataCamp offer interactive courses related to Python Programming.  Since R Markdown documents can run simple Python code chunks (though the data is not accessible to future chunks, a large difference from R Markdown for R), this document attempts to summarize notes from the modules when possible.

Topic areas summarized include:  
  
* Python Programming (Introduction, Intermediate, Toolbox I/II, Network Analysis I/II)  
* Python Import and Clean Data (Import I/II, Clean)  
* Python Data Manipulation (pandas Foundations, Manipulating DF with pandas, Merging DF with pandas, Databases in pandas, Data Types)  
* Python Visualization (Introduction to Python Data Visualization, Interactive Visualization with Bokeh)  
* Python Statistics (Statistical Thinking in Python Parts I/II)  
* Python Machine Learning ()  
  
The complete version as of July 31, 2017 has been archived as DataCamp_PythonNotes_v001.  Archive files for DataCamp_Python_ImportClean_v002 and DataCamp_Python_Programming_v002 have also been created to contain summaries of those areas.  Further, DataCamp_PythonNotes_v002 was created for capturing summaries of additional topics.

The version of DataCamp_PythonNotes_v002 as of September 1, 2017 has been archived.  Archive files for DataCamp_Python_DataManipulation_v003 and DataCamp_Python_Visualization_v003 have also been created to contain summaries of these areas.  Further, DataCamp_PythonNotes_v003 was created for capturing summaries of additional topics.

This document includes:  
  
* Python Statistics (Statistical Thinking in Python Parts I/II)  
* Python Machine Learning ()  
  
  
## Python Statistics  
###_Statistical Thinking in Python (Part I)_#

Chapter 1 - Graphical exploratory data analysis  
  
Introduction to exploratory data analysis - organizing, plotting, and summarizing data (ala Tukey):  
  
* Example of swing voter dataset (by county) of 2008 election  
* Conversion of tabular data (e.g., pandas DataFrame) to visual data  
  
Plotting a histogram - using matplotlib.pyplot.hist (plt.hist):  
  
* plt.hist(<myData>) will create the basic hsitogram  
	* Can accessorize with plt.xlabel("myXLabel") and plt.ylabel("myYLabel")  
    * Can send the bins with a bins=<myBins> call ; can be a list of actual bin cut-points, or a single integer requesting that number of evenly spaced bins  
* Common practice in Python is to assign _ = plt.hist(<myData>) to avoid the screen being printed with the 3 histogram data outputs  
* Can customize with the Seaborn styling (package developed by Waskom)  
    * import seaborn as sns  
    * sns.set()  # makes Seaborn style the default for graphing  
  
Plotting all data (bee swarm plots):  
  
* Binning bias can lead to the same data being interpreted differently based on different binning in different histograms  
* The swarm plot has the individual points plotted, segmented by categorical variables if appropriate, in a fashion like "jitter" where each point is visible  
* Seaborn can manage swarm plots easilt, provided that data are in a well-organized DataFrame (rows as observations, columns as features)  
	* sns.swarmplot(x="xVar", y="yVar", data=myFrame)  
    * plt.show()  
  
Plotting all data (ECDF = empirical CDF) - especially when swarm plot would be too messy (too much data):  
  
* The ECDF has the y-axis as the cumulative percentile (total amount of data that is smaller than the designated x value)  
* Suppose that a DataFrame df_swing already exists, with a column called "dem_share"  
	* x = np.sort(df_swing["dem_share"])  
    * y = np.arange(1, len(x) + 1) / len(x)  
    * _ = plt.plot(x, y, marker=".", linestyle="none")  # will make dot markers and with no lines  
    * plt.margins(0.02)  # keeps the data from running over the side of the plot  
* Generally, ECDF is a good starting point for analysis - keeps all the data, provides a simple summary  
  
Onward toward the whole story - starting with graphical EDA as per Tukey:  
  
* Chapter 2 - Build on graphical EDA with quantitative EDA  
* Chapter 3/4 - Probabilistic distributions to draw meaningful conclusions (Hacker statistics)  
* Part II - continuing to build towards "the full story"  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



# Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd

rawIris = pd.read_csv(myPath + "iris.csv")
versicolor_petal_length = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Length"]



# Set default Seaborn style
sns.set()

# Plot histogram of versicolor petal lengths
plt.hist(versicolor_petal_length)

# Show histogram
# plt.show()
plt.savefig("_dummyPy149.png", bbox_inches="tight")
plt.clf()


# Plot histogram of versicolor petal lengths
_ = plt.hist(versicolor_petal_length)

# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
# plt.show()
plt.savefig("_dummyPy150.png", bbox_inches="tight")
plt.clf()


# Import numpy
import numpy as np

# Compute number of data points: n_data
n_data = len(versicolor_petal_length)

# Number of bins is the square root of number of data points: n_bins
n_bins = n_data ** 0.5

# Convert number of bins to integer: n_bins
n_bins = int(n_bins)

# Plot the histogram
_ = plt.hist(versicolor_petal_length, bins=n_bins)

# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
# plt.show()
plt.savefig("_dummyPy151.png", bbox_inches="tight")
plt.clf()



df = rawIris[["Species", "Petal.Length"]]
df.columns = ["species", "petal length (cm)"]

# Create bee swarm plot with Seaborn's default settings
_ = sns.swarmplot(x="species", y="petal length (cm)", data=df)

# Label the axes
_ = plt.xlabel('Species')
_ = plt.ylabel("Petal Length (cm)")

# Show the plot
# plt.show()
plt.savefig("_dummyPy152.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y

# Compute ECDF for versicolor data: x_vers, y_vers
x_vers, y_vers = ecdf(versicolor_petal_length)

# Generate plot
_ = plt.plot(x_vers, y_vers, marker=".", linestyle="none")

# Make the margins nice
plt.margins(0.02)

# Label the axes
_ = plt.xlabel("Versicolor Petal Length (cm)")
_ = plt.ylabel("ECDF")

# Display the plot
# plt.show()
plt.savefig("_dummyPy153.png", bbox_inches="tight")
plt.clf()


setosa_petal_length = rawIris.loc[rawIris["Species"] == "setosa", "Petal.Length"]
virginica_petal_length = rawIris.loc[rawIris["Species"] == "virginica", "Petal.Length"]

# Compute ECDFs
x_set, y_set = ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg, y_virg = ecdf(virginica_petal_length)

# Plot all ECDFs on the same plot
_ = plt.plot(x_set, y_set, marker=".", linestyle="none")
_ = plt.plot(x_vers, y_vers, marker=".", linestyle="none")
_ = plt.plot(x_virg, y_virg, marker=".", linestyle="none")

# Make nice margins
plt.margins(0.02)

# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
# plt.show()
plt.savefig("_dummyPy154.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Default Histogram**:  
![](_dummyPy149.png) 
  
**Example #2: Labeled Histogram**:  
![](_dummyPy150.png) 

**Example #3: Custom-Binned Histogram**:  
![](_dummyPy151.png) 

**Example #4: Swarm Plot**:  
![](_dummyPy152.png) 

**Example #5: ECDF (Single Species)**:  
![](_dummyPy153.png) 
  
**Example #6: ECDF (Multiple Species)**:  
![](_dummyPy154.png) 
  
***
  
Chapter 2 - Quantitative exploratory data analysis  
  
Introduction to summary statistics: sample mean and median:  
  
* Means or medians can be added as horizontal lines on the bee swarm plot  
	* np.mean(myData)  
    * np.median(myData)  
  
Percentiles, outliers, and box plots:  
  
* Can grab multiple percentiles using np.percentile(myData, [myPercentileList])  # note that percentiles should be passed, so 25 means 0.25 or 25th percentile  
* Box plots can help to display much of the percentile data simultaneously - center is median, box edges are 0.25/0.75, end of whsikers is 1.5 * IQR (unless data is narrower), with outliers graphed separately  
* _ = sns.boxplot(x="xVar", y="yVar", data=myData)  
  
Variance and standard deviation:  
  
* Variance - average of the squared distance from the mean  
	* np.var(myData)  
* Standard Deviation - square root of the variance  
	* np.sd(myData)  
  
Covariance and Pearson correlation coefficient:  
  
* Initial graphical EDA could come from graphing the data as points, using plt.plot(xVar, yVar, marker=".", linestyle="none")  
* Covariance - how to variables run together (mean of product of differences from x-mean, y-mean)  
* Correlation - adds the advantages of dimensionless and scaled from -1 to 1  
	* Pearson correlation coefficient (rho) - Covariances / [Std(x) * Std(y) ]  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


rawIris = pd.read_csv(myPath + "iris.csv")
versicolor_petal_length = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Length"]
versicolor_petal_width = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Width"]


# Compute the mean: mean_length_vers
mean_length_vers = np.mean(versicolor_petal_length)

# Print the result with some nice formatting
print('I. versicolor:', mean_length_vers, 'cm')


# Specify array of percentiles: percentiles
percentiles = np.array([2.5, 25, 50, 75, 97.5])

# Compute percentiles: ptiles_vers
ptiles_vers = np.percentile(versicolor_petal_length, percentiles)

# Print the result
print(ptiles_vers)


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y

# Compute ECDF for versicolor data: x_vers, y_vers
x_vers, y_vers = ecdf(versicolor_petal_length)


# Plot the ECDF
_ = plt.plot(x_vers, y_vers, '.')
plt.margins(0.02)
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Overlay percentiles as red diamonds.
_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',
         linestyle="none")

# Show the plot
# plt.show()
plt.savefig("_dummyPy155.png", bbox_inches="tight")
plt.clf()


df = rawIris[["Species", "Petal.Length"]]
df.columns = ["species", "petal length (cm)"]


# Create box plot with Seaborn's default settings
_ = sns.boxplot(x="species", y="petal length (cm)", data=df)

# Label the axes
_ = plt.xlabel("Species")
_ = plt.ylabel("Petal Length (cm)")

# Show the plot
# plt.show()
plt.savefig("_dummyPy156.png", bbox_inches="tight")
plt.clf()


# Array of differences to mean: differences
differences = versicolor_petal_length - np.mean(versicolor_petal_length)

# Square the differences: diff_sq
diff_sq = differences ** 2

# Compute the mean square difference: variance_explicit
variance_explicit = np.mean(diff_sq)

# Compute the variance using NumPy: variance_np
variance_np = np.var(versicolor_petal_length)

# Print the results
print(variance_explicit, variance_np)


# Compute the variance: variance
variance = np.var(versicolor_petal_length)

# Print the square root of the variance
print(variance ** 0.5)

# Print the standard deviation
print(np.std(versicolor_petal_length))


# Make a scatter plot
_ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker=".", linestyle="none")

# Set margins
plt.margins(0.02)

# Label the axes
_ = plt.xlabel("Petal Length (cm)")
_ = plt.ylabel("Petal Width (cm)")

# Show the result
# plt.show()
plt.savefig("_dummyPy157.png", bbox_inches="tight")
plt.clf()


# Compute the covariance matrix: covariance_matrix
covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)

# Print covariance matrix
print(covariance_matrix)

# Extract covariance of length and width of petals: petal_cov
petal_cov = covariance_matrix[0, 1]

# Print the length/width covariance
print(petal_cov)


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]

# Compute Pearson correlation coefficient for I. versicolor: r
r = pearson_r(versicolor_petal_length, versicolor_petal_width)

# Print the result
print(r)

```
  
  
**Example #1: ECDF with Key Percentiles**:  
![](_dummyPy155.png) 

**Example #2: Box Plot**:  
![](_dummyPy156.png)

**Example #3: Scatter Plot**:  
![](_dummyPy157.png)

  
***
  
Chapter 3 - Thinking Probabilistically (Discrete Variables)  
  
Probabilistic logic and statistical inference - drawing conclusions about the population based on my sample:  
  
* Multiple re-samples will generate multiple means  
  
Random number generators and hacker statistics - tool for thinking probabilistically:  
  
* Use simulated repeated measurements to compute probabilities  
* Initial simulations were run on games of chance (particularly coin flips) by Pascal et al  
* The NumPy module has many useful random number generators  
	* np.random.random(size=n)  # draws "n" random numbers between 0 and 1  
    * np.random.seed()  # allows for providing a seed for re-producibility  
* Hacker statistics - define simulation, run many times, summarize key statistic of interest  
  
Probability distributions - Binomial:  
  
* Probability Mass Function (PMF) - set of probabilities of discrete outcomes  
	* Rolling a dice is an example - can only get discrete values (integers 1-6)  
* Binomial Distribution - the number of successes, r, in n Bernoullie trials with probability p of success will be binomially distributed  
	* np.random.binomial(nTrials, pSuccess, size=n)  # will simulate the sum of successes in the nTrials each with pSuccess ; will create a 1D array of length n by repeating the simulation n times  
  
Poisson processes and distributions - specified solely by a rate, independent of any previous events:  
  
* Natural births in a hospital, timing of hits to a website, etc.  
* Poisson distribution - the number r of arrivals of a Poisson process in a given time period with an average rate lambda of arrivals is Poisson distributed  
* The Poisson distribution is a limiting case of the Binomial distribution when there is a low succes probability but a large amount of time -- i.e., rare events  
	* np.random.poisson(param, size=)  # where param is the expected number of events and size is the number of simulations desired  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



# Seed the random number generator
np.random.seed(42)

# Initialize random numbers: random_numbers
random_numbers = np.empty(100000)

# Generate random numbers by looping over range(100000)
for i in range(100000):
    random_numbers[i] = np.random.random()

# Plot a histogram
_ = plt.hist(random_numbers)

# Show the plot
# plt.show()
plt.savefig("_dummyPy158.png", bbox_inches="tight")
plt.clf()


def perform_bernoulli_trials(n, p):
    """Perform n Bernoulli trials with success probability p
    and return number of successes."""
    # Initialize number of successes: n_success
    n_success = 0
    
    # Perform trials
    for i in range(n):
        # Choose random number between zero and one: random_number
        random_number = np.random.random()
        
        # If less than p, it's a success so add one to n_success
        if random_number < p:
            n_success +=1
    
    return n_success


# Seed random number generator
np.random.seed(42)

# Initialize the number of defaults: n_defaults
n_defaults = np.empty(1000)

# Compute the number of defaults
for i in range(1000):
    n_defaults[i] = perform_bernoulli_trials(100, 0.05)

# Plot the histogram with default number of bins; label your axes
_ = plt.hist(n_defaults, normed=True)
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('probability')

# Show the plot
# plt.show()
plt.savefig("_dummyPy159.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Compute ECDF: x, y
x, y = ecdf(n_defaults)  # same function as written in previous chapters

# Plot the ECDF with labeled axes
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.xlabel("Number of Defaults in Simulation")
_ = plt.ylabel("ECDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy160.png", bbox_inches="tight")
plt.clf()


# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money
n_lose_money = np.sum(n_defaults >= 10)

# Compute and print probability of losing money
print('Probability of losing money =', n_lose_money / len(n_defaults))


# Take 10,000 samples out of the binomial distribution: n_defaults
n_defaults = np.random.binomial(100, 0.05, size=10000)

# Compute CDF: x, y
x, y = ecdf(n_defaults)

# Plot the CDF with axis labels
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.xlabel("Defaults in 100 loans")
_ = plt.ylabel("CDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy161.png", bbox_inches="tight")
plt.clf()


# Compute bin edges: bins
bins = np.arange(min(n_defaults), max(n_defaults) + 2) - 0.5

# Generate histogram
_ = plt.hist(n_defaults, bins=bins, normed=True)

# Set margins
plt.margins(0.02)

# Label axes
_ = plt.xlabel("Number of Defaults in 100 Loans")
_ = plt.ylabel("Probability")

# Show the plot
# plt.show()
plt.savefig("_dummyPy162.png", bbox_inches="tight")
plt.clf()


# Draw 10,000 samples out of Poisson distribution: samples_poisson
samples_poisson = np.random.poisson(10, size=10000)

# Print the mean and standard deviation
print('Poisson:     ', np.mean(samples_poisson),
                       np.std(samples_poisson))

# Specify values of n and p to consider for Binomial: n, p
n = [20, 100, 1000]
p = [0.5, 0.1, 0.01]

# Draw 10,000 samples for each n,p pair: samples_binomial
for i in range(3):
    samples_binomial = np.random.binomial(n[i], p[i], size=10000)

    # Print results
    print('n =', n[i], 'Binom:', np.mean(samples_binomial),
                                 np.std(samples_binomial))


# Draw 10,000 samples out of Poisson distribution: n_nohitters
n_nohitters = np.random.poisson(251/115, size=10000)

# Compute number of samples that are seven or greater: n_large
n_large = np.sum(n_nohitters >= 7)

# Compute probability of getting seven or more: p_large
p_large = n_large / 10000

# Print the result
print('Probability of seven or more no-hitters:', p_large)


```
  
  
**Example #1: Histogram of Random Numbers**:  
![](_dummyPy158.png) 

**Example #2: Histogram of 1000 Bernoulli Trials (each with p=0.05, n=100)**:  
![](_dummyPy159.png) 

**Example #3: ECDF of 1000 Bernoulli Trials**:  
![](_dummyPy160.png) 

**Example #4: Histogram/ECDF of Binomial Trials (n=100, p=0.05, size=10000)**:  
![](_dummyPy161.png) 

**Example #5: Normed Histogram with Customized Binning for Binomial Trials (n=100, p=0.05, size=10000)**:  
![](_dummyPy162.png) 
  
  
***
  
Chapter 4 - Thinking probabilistically - continuous variables  
  
Probability density functions (PDF) - continuous analog to the PMF:  
  
* Mathematical description of the relative likelihood of observing a value of a continuous variable  
* Areas under the PDF describe the associated probabilities  
* The CDF can be calculated from the PDF, and can be easier to interpret  
  
Introduction to the Normal Distribution - continuous variable with a single peak:  
  
* Mean - center of the peak  
* Standard Deviation - degree of spread of the data around the peak  
* To avoid binning bias, better to compare theoretical/actual CDF rather than histogram and theoretical curve overlaid  
* To simulate normal data, call np.random.normal(mean, std, size=)  
	
Normal distribution - properties and warnings:  
  
* Commonly called the Gaussian dsitribution after its inventor (was previously featured on the German Deutschemark prior to adoption of the Euro)  
* Important caveats include  
	1.  Sometimes, things that you think "should" be normally distributed in fact are not  
    2.  Normal distributions have very light tails (outliers are extremely unlikely), which often does not match to the real-world  
  
Exponential distribution - related to the Poisson distribution:  
  
* The expected number of occurences being Poisson distributed means that the expected time between events will be exponentially distributed  
* The waiting time between arrivals of a Poisson distribution is exponentially distributed - defined by a single parameter, "mean waiting time"  
	* np.random.exponential(mean, size=)  # will simulate "size" times from an exponential with mean "mean"  
* Simulating a story can be very powerful - existence of computers makes the "pen and paper" modelling less necessary in many cases  
  
Final thoughts - course recap:  
  
* Construct instructive plots  
* Compute informative summary statistics  
* Use hacker statistics  
* Think probabilistically  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10
samples_std1 = np.random.normal(20, 1, size=100000)
samples_std3 = np.random.normal(20, 3, size=100000)
samples_std10 = np.random.normal(20, 10, size=100000)

# Make histograms
_ = plt.hist(samples_std1, normed=True, histtype="step", bins=100)
_ = plt.hist(samples_std3, normed=True, histtype="step", bins=100)
_ = plt.hist(samples_std10, normed=True, histtype="step", bins=100)

# Make a legend, set limits and show plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))
plt.ylim(-0.01, 0.42)
# plt.show()
plt.savefig("_dummyPy163.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Generate CDFs
x_std1, y_std1 = ecdf(samples_std1)  # function written earlier in previous chapter
x_std3, y_std3 = ecdf(samples_std3)
x_std10, y_std10 = ecdf(samples_std10)

# Plot CDFs
_ = plt.plot(x_std1, y_std1, marker=".", linestyle="none")
_ = plt.plot(x_std3, y_std3, marker=".", linestyle="none")
_ = plt.plot(x_std10, y_std10, marker=".", linestyle="none")

# Make 2% margin
plt.margins(0.02)

# Make a legend and show the plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')
# plt.show()
plt.savefig("_dummyPy164.png", bbox_inches="tight")
plt.clf()


rawBelmont = pd.read_csv(myPath + "belmont_stakes_1926_2016.csv")

from datetime import datetime

listBelmont = [x.split(".") for x in rawBelmont["Time"]]
timeBelmont = [int(x[0].split(":")[0]) * 60 + int(x[0].split(":")[1]) + int(x[1])/100 for x in listBelmont]
belmont_no_outliers = timeBelmont


# Compute mean and standard deviation: mu, sigma
mu = np.mean(belmont_no_outliers)
sigma = np.std(belmont_no_outliers)

# Sample out of a normal distribution with this mu and sigma: samples
samples = np.random.normal(mu, sigma, size=10000)

# Get the CDF of the samples and of the data
x_theor, y_theor = ecdf(samples)
x, y = ecdf(belmont_no_outliers)

# Plot the CDFs and show the plot
_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x, y, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('Belmont winning time (sec.)')
_ = plt.ylabel('CDF')
# plt.show()
plt.savefig("_dummyPy165.png", bbox_inches="tight")
plt.clf()


# Take a million samples out of the Normal distribution: samples
samples = np.random.normal(mu, sigma, size=1000000)

# Compute the fraction that are faster than 144 seconds: prob
prob = np.mean(samples <= 144)

# Print the result
print('Probability of besting Secretariat:', prob)


def successive_poisson(tau1, tau2, size=1):
    # Draw samples out of first exponential distribution: t1
    t1 = np.random.exponential(tau1, size)
    
    # Draw samples out of second exponential distribution: t2
    t2 = np.random.exponential(tau2, size)
    
    return t1 + t2


# Draw samples of waiting times: waiting_times
waiting_times = successive_poisson(764, 715, size=100000)

# Make the histogram
_ = plt.hist(waiting_times, bins=100, normed=True, histtype="step")

# Label axes
_ = plt.xlabel("Total Waiting Time (Days) for Cycle and No-Hitter")
_ = plt.ylabel("PDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy166.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Histogram of Normal Distribution with Different STD**:  
![](_dummyPy163.png) 

**Example #2: ECDF of Normal Distribution with Different STD**:  
![](_dummyPy164.png) 

**Example #3: ECDF of Belmont Stakes Winning Times**:  
![](_dummyPy165.png) 

**Example #4: Sum of Poisson Outcomes**:  
![](_dummyPy166.png) 


###_Statistical Thinking in Python (Part I)_#
  
Chapter 1 - Parameter estimation by optimization  
  
Optimal parameters - parameters that bring the model in closest agreement with the data:  
  
* Parameters derived from the sample are relevant mainly to that sample; other samples might have other optimal parameters  
* Two key packages for statistics in Python - scipy.stats and statsmodels  
* For this course, though, the focus will be on hacker statistics - simulated data using numpy  
  
Linear regression by least squares - fitting the best slope and intercept to the line:  
  
* Residuals are the vertical distance between the data point and the regression line  
* The "least squares" algorithm defines the best fit to be the line that minimizes the sum-squared residuals  
* Regressions can be run in Python using np.polyfit()  # least squares with polynomials  
	* slope, intercept = np.polyfit(xData, yData, nDegree)  # nDegree is the desired degree of the polynomial, which is to say 1 for linear regression  
  
Importance of EDA (Anscombe's quartet) - 4 fictitious datasets with the same x-bar, y-bar, correlation, and RSS:  
  
* Graphical EDA is vital before diving in to the regression (or other) analysis  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]

# Seed random number generator
np.random.seed(42)

# Compute mean no-hitter time: tau
tau = np.mean(nohitter_times)

# Draw out of an exponential distribution with parameter tau: inter_nohitter_time
inter_nohitter_time = np.random.exponential(tau, 100000)

# Plot the PDF and label axes
_ = plt.hist(inter_nohitter_time,
             bins=50, normed=True, histtype="step")
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy167.png", bbox_inches="tight")
plt.clf()



def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Uses function ecdf() from previous course
# Create an ECDF from real data: x, y
x, y = ecdf(nohitter_times)

# Create a CDF from theoretical samples: x_theor, y_theor
x_theor, y_theor = ecdf(inter_nohitter_time)

# Overlay the plots
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker=".", linestyle="none")

# Margins and axis labels
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy168.png", bbox_inches="tight")
plt.clf()



# Plot the theoretical CDFs
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker='.', linestyle='none')
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Take samples with half tau: samples_half
samples_half = np.random.exponential(tau/2, size=10000)

# Take samples with double tau: samples_double
samples_double = np.random.exponential(tau*2, size=10000)

# Generate CDFs from these samples
x_half, y_half = ecdf(samples_half)
x_double, y_double = ecdf(samples_double)

# Plot these CDFs as lines
_ = plt.plot(x_half, y_half)
_ = plt.plot(x_double, y_double)

# Show the plot
# plt.show()
plt.savefig("_dummyPy169.png", bbox_inches="tight")
plt.clf()



import pandas as pd
rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')

# Set the margins and label axes
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Show the plot
# plt.show()
plt.savefig("_dummyPy170.png", bbox_inches="tight")
plt.clf()



def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


# Show the Pearson correlation coefficient
print(pearson_r(illiteracy, fertility))


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Perform a linear regression using np.polyfit(): a, b
a, b = np.polyfit(illiteracy, fertility, 1)

# Print the results to the screen
print('slope =', a, 'children per woman / percent illiterate')
print('intercept =', b, 'children per woman')

# Make theoretical line to plot
x = np.array([0, 100])
y = a * x + b

# Add regression line to your plot
_ = plt.plot(x, y)

# Draw the plot
# plt.show()
plt.savefig("_dummyPy171.png", bbox_inches="tight")
plt.clf()


# Specify slopes to consider: a_vals
a_vals = np.linspace(0, 0.1, 200)

# Initialize sum of square of residuals: rss
rss = np.empty_like(a_vals)

# Compute sum of square of residuals for each value of a_vals
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy - b)**2)

# Plot the RSS
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')

#plt.show()
plt.savefig("_dummyPy172.png", bbox_inches="tight")
plt.clf()


rawAnscombe = pd.read_csv(myPath + "anscombe.csv", header=None)

anscombe_x = [[float(x) for x in rawAnscombe.iloc[2:, 0]], [float(x) for x in rawAnscombe.iloc[2:, 2]], [float(x) for x in rawAnscombe.iloc[2:, 4]], [float(x) for x in rawAnscombe.iloc[2:, 6]]]

anscombe_y = [[float(x) for x in rawAnscombe.iloc[2:, 1]], [float(x) for x in rawAnscombe.iloc[2:, 3]], [float(x) for x in rawAnscombe.iloc[2:, 5]], [float(x) for x in rawAnscombe.iloc[2:, 7]]]


x=anscombe_x[0]
y=anscombe_y[0]


# Perform linear regression: a, b
a, b = np.polyfit(x, y, 1)

# Print the slope and intercept
print(a, b)

# Generate theoretical x and y data: x_theor, y_theor
x_theor = np.array([3, 15])
y_theor = a * x_theor + b

# Plot the Anscombe data and theoretical line
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.plot(x_theor, y_theor)

# Label the axes
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
# plt.show()
plt.savefig("_dummyPy173.png", bbox_inches="tight")
plt.clf()



# Iterate through x,y pairs
for x, y in zip(anscombe_x, anscombe_y):
    # Compute the slope and intercept: a, b
    a, b = np.polyfit(x, y, 1)
    
    # Print the result
    print('slope:', a, 'intercept:', b)



```
  
  
**Example #1: Exponential Distribution**:  
![](_dummyPy167.png) 
  
**Example #2: Theoretical vs Actual ECDF (Exponential for Time Between No-Hitters)**:  
![](_dummyPy168.png)

**Example #3: ECDF for Exponential Distribution with Half-Rate and Double-Rate**:  
![](_dummyPy169.png)

**Example #4: Scatter-Plot for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy170.png)

**Example #5: Best Linear Fit for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy171.png)

**Example #6: Residual Sum-Squares vs Best-Fit Parameters**:  
![](_dummyPy172.png)

**Example #7: Regression Fit for Anscombe Data #1**:  
![](_dummyPy173.png)
  
***
  
Chapter 2 - Bootstrap confidence intervals  
  
Generating bootstrap replicates - how might the sample statistics change if we acquired a new sample:  
  
* Hacker approach - resample the existing data (WITH replacement), using the same n, then recalculate the sample statistics  
* Can then take multiple re-samples and aggregate the sample statistics for a statistical inference  
* "Bootstrap replicate" is the value of the summary statistic calculated from the "bootstrap sample"  
* Boostrap samples can easily be created in Python  
	* np.random.choice(myData, size=len(myData))  # can specify any size, but len(myData) will be a bootstrap; sampling is WITH replacement  
  
Bootstrap confidence intervals:  
  
* Can generate a function bootstrap_replicate_1d(data, func) which takes a bootstrap sample of data and then applies func to it  
* Can then use a for loop to run through the function, storing the results in a numpy array (or appended to an empty list, or etc.)  
* Can generate the confidence interval using np.percentile(all_replicates, [2.5, 97.5])  # this is for the 95% confidence interval  
  
Pairs bootstrap - bootstrap resample on pairs of data (e.g., keep the x and y connected to each other):  
  
* Bootstrap approach is non-parametric, since it is not forced to fit any particular model  
* Least-squares approach is parametrics, since it is forced to fit a pre-defined specification of the model  
* Can use bootstrap techniques to get confidence intervals on the parametric (e.g., least-squares) models  
	* Use a pairs bootstrap that keeps each of the x/y anchored to each other  
    * Resample WITH replacement on all the x/y pairs, then run the model with this bootstrap resample  
* The approach within Python is a slight modification of the 1D approach  
    * inds = np.arange(len(myXYData))  
    * bs_inds = np.random.choice(inds, len(inds))  
    * Use bs_inds as the .iloc (or index) filters of the dataset  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



rawWeather = pd.read_csv(myPath + "sheffield_weather_station.csv", skiprows=8, delim_whitespace=True)
rainfall = rawWeather["rain"]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y



for a in range(50):
    # Generate bootstrap sample: bs_sample
    bs_sample = np.random.choice(rainfall, size=len(rainfall))
    
    # Compute and plot ECDF from bootstrap sample
    x, y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker='.', linestyle='none',
                 color='gray', alpha=0.1)

# Compute and plot ECDF from original data
x, y = ecdf(rainfall)
_ = plt.plot(x, y, marker='.')

# Make margins and label axes
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy174.png", bbox_inches="tight")
plt.clf()



def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


# Take 10,000 bootstrap replicates of the mean: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.mean, 10000)

# Compute and print SEM
sem = np.std(rainfall) / np.sqrt(len(rainfall))
print(sem)

# Compute and print standard deviation of bootstrap replicates
bs_std = np.std(bs_replicates)
print(bs_std)

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('mean annual rainfall (mm)')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy175.png", bbox_inches="tight")
plt.clf()


# Generate 10,000 bootstrap replicates of the variance: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.var, 10000)

# Put the variance in units of square centimeters
bs_replicates = bs_replicates / 100

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('variance of annual rainfall (sq. cm)')
_ = plt.ylabel('PDF')

# Show the plot
#plt.show()
plt.savefig("_dummyPy176.png", bbox_inches="tight")
plt.clf()



rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]


# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates
bs_replicates = draw_bs_reps(nohitter_times, np.mean, 10000)

# Compute the 95% confidence interval: conf_int
conf_int = np.percentile(bs_replicates, [2.5, 97.5])

# Print the confidence interval
print('95% confidence interval =', conf_int, 'games')

# Plot the histogram of the replicates
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel(r'$\tau$ (games)')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy177.png", bbox_inches="tight")
plt.clf()



def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates: bs_slope_reps, bs_intercept_reps
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    
    return bs_slope_reps, bs_intercept_reps



rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy

# Generate replicates of slope and intercept using pairs bootstrap
bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, 1000)

# Compute and print 95% CI for slope
print(np.percentile(bs_slope_reps, [2.5, 97.5]))

# Plot the histogram
_ = plt.hist(bs_slope_reps, bins=50, normed=True)
_ = plt.xlabel('slope')
_ = plt.ylabel('PDF')

# plt.show()
plt.savefig("_dummyPy178.png", bbox_inches="tight")
plt.clf()


# Generate array of x-values for bootstrap lines: x
x = np.array([0, 100])

# Plot the bootstrap lines
for i in range(100):
    _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i],
                 linewidth=0.5, alpha=0.2, color='red')

# Plot the data
_ = plt.plot(illiteracy, fertility, marker=".", linestyle="none")

# Label axes, set the margins, and show the plot
_ = plt.xlabel('illiteracy')
_ = plt.ylabel('fertility')
plt.margins(0.02)

# plt.show()
plt.savefig("_dummyPy179.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Bootstrap ECDF for Sheffield Annual Rainfall**:  
![](_dummyPy174.png)

**Example #2:  Bootstrap Mean for Sheffield Annual Rainfall**:  
![](_dummyPy175.png)

**Example #3:  Bootstrap Variance for Sheffield Annual Rainfall**:  
![](_dummyPy176.png)

**Example #4: Bootstrap for No-Hitter Times**:  
![](_dummyPy177.png)

**Example #5: Bootstrap Pairs for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy178.png)

**Example #6: Bootstrap Pairs for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy179.png)

  
***
  
Chapter 3 - Hypothesis Testing  
  
Formulating and simulating a hypothesis:  
  
* Hypothesis testing - how reasonable are the observed data assuming a particular hypothesis is true  
* Hacker approach - scrambling the data - example of PA and OH 2008 voting data (% D)  
	* Combine all county level results from PA (67 counties) and OH, ignoring which state they are from  
    * Permute (randomly scramble) the all-county dataset, and declare the first 67 counties as the PA data  
* Running a permutation process in Python using NumPy  
	* dem_share_both = np.concatenate((dem_PA, dem_OH))  # takes a tuple of the desired inputs  
    * dem_share_perm = np.random.permutation(dem_share_both)  # permutes the full dataset  
    * perm_PA = dem_share_perm[:len(dem_share_PA)]  
    * perm_OH = dem_share_perm[len(dem_share_PA):]  
  
Test statistics and p-values - testing the permutations as per the above, assuming the null hypothesis of identical distributions in OH/PA:  
  
* Test statistic - single number that can be computed from observed data, as well as from data simulated under the null hypothesis  
	* Serves as a basis for comparison between predictions and actual observations  
    * Can choose "difference in means" as the test statistic in this case  
* Can compare the observed test statistic to the permutation replicates, and assess likelihood of results "as extreme" as the actual test statistic  
	* The p-value is the probability of getting a result at least as extreme as the observed test statistic, under the assumption that they null hypothesis is valid  
* Null Hypothesis Significance Testing (NHST) - consider the value of the p-value as well as the magnitude of the differences  
	* Statistical significance (p-value) and practical significance are two different things  
  
Bootstrap hypothesis tests:  
  
* Basic analysis pipeline - clearly state null hypothesis; define test statistic; generate many permuted datasets assuming null hypothesis; compute test statistic for each permutation  
* Additional analysis type - have ALL of the Michelson speed of light data, but only the mean calculated by Newcomb - could Newcomb mean have come from Michelson experiment?  
	* Null hypothesis becomes "True mean speed of light in Michelson experiment is equal to Newcomb's reported mean"  
    * Shift the means of the Michelson experimental data (every point) to match the Newcomb reported mean: michelson - np.mean(michelson) + newcomb  
    * Calculate p-values based on permutations of shifted Michelson data  
* One sample test - compare one set of data to a single number  
* Two sample test - compare two sets of data against each other  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2


# Create some fake rain data - July and November
rain_july = np.random.normal(3.5, 1.25, 100)
rain_november = np.random.normal(2.25, 0.75, 100)

for i in range(50):
    # Generate permutation samples
    perm_sample_1, perm_sample_2 = permutation_sample(rain_july, rain_november)
    
    # Compute ECDFs
    x_1, y_1 = ecdf(perm_sample_1)
    x_2, y_2 = ecdf(perm_sample_2)
    
    # Plot ECDFs of permutation sample
    _ = plt.plot(x_1, y_1, marker='.', linestyle='none',
                 color='red', alpha=0.02)
    _ = plt.plot(x_2, y_2, marker='.', linestyle='none',
                 color='blue', alpha=0.02)

# Create and plot ECDFs from original data
x_1, y_1 = ecdf(rain_july)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Display the plot
# plt.show()
plt.savefig("_dummyPy180.png", bbox_inches="tight")
plt.clf()



rawFrog = pd.read_csv(myPath + "frog_tongue.csv", header=14)


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


# Make bee swarm plot
_ = sns.swarmplot(x="ID", y='impact force (mN)', data=rawFrog)

# Label axes
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')

# Show the plot
# plt.show()
plt.savefig("_dummyPy181.png", bbox_inches="tight")
plt.clf()


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


# Define force_a to be frog I and force_b to be frog II
force_a = rawFrog.loc[rawFrog["ID"] == "I", 'impact force (mN)']
force_b = rawFrog.loc[rawFrog["ID"] == "II", 'impact force (mN)']

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a, force_b)

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(force_a, force_b,
                                 diff_of_means, size=10000)

# Compute p-value: p
p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)

# Print the result
print('p-value =', p)


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b - np.mean(force_b) + 0.55

# Function available in previous chapters - copy down
# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)

# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000

# Print the p-value
print('p = ', p)


# Per the notes from DataCamp, the permutation test is a more exact test of the null hypothesis while the (below) bootstrap test is more versatile
#  "However, the permutation test exactly simulates the null hypothesis that the data come from the same distribution, whereas the bootstrap test approximately simulates it. As we will see, though, the bootstrap hypothesis test, while approximate, is more versatile."

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a, force_b)

# Concatenate forces: forces_concat
forces_concat = np.concatenate((force_a, force_b))

# Initialize bootstrap replicates: bs_replicates
bs_replicates = np.empty(10000)

for i in range(10000):
    # Generate bootstrap sample
    bs_sample = np.random.choice(forces_concat, size=len(forces_concat))
    
    # Compute replicate
    bs_replicates[i] = diff_of_means(bs_sample[:len(force_a)],
                                     bs_sample[len(force_a):])

# Compute and print p-value: p
p = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)
print('p-value =', p)


# Compute mean of all forces: mean_force
mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

# Compute and print p-value: p
p = np.sum(bs_replicates > empirical_diff_means) / len(bs_replicates)
print('p-value =', p)

```
  
  
**Example #1: ECDF for July vs November Rainfall**:  
![](_dummyPy180.png)
  
**Example #2: Swarm Plot for Impact Force by Frog**:  
![](_dummyPy181.png)
  
***

Chapter 4 - Hypothesis test examples  
  
A/B Testing - test/control approach for "splash" pages of the website:  
  
* Null hypothesis is that the redesign (page B) has no impact on the click-through rate relative to the control design (page A)  
* Calculate a test statistic as np.mean(B) - np.mean(A)  # average click-through rate is the test statistic  
* Combine and permute the data, and calculate test statistic for permutation replicates  
* Calculate likelihood of seeing data as large or larger than the test statistic  
  
Test of correlation - returning to the Obama vote share vs. county size analysis from prior:  
  
* Null hypothesis is of no correlation between Obama vote share and county size (total votes)  
* Use the Pearson correlation coefficient as the test statistic  
* Leave one of the columns constant and permute the other; calculate the permutation replicate based on this new dataset  
* Repeat many times, and calculate the likelihood of achieving a correlation as extreme as was observed  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2



# Construct arrays of data: dems, reps
dems = np.array([True] * 153 + [False] * 91)
reps = np.array([True] * 136 + [False] * 35)

def frac_yay_dems(dems, reps):
    """Compute fraction of Democrat yay votes."""
    frac = np.sum(dems) / len(dems)
    return frac

# draw_perm_reps is available in previous chapters
# Acquire permutation samples: perm_replicates
perm_replicates = draw_perm_reps(dems, reps, frac_yay_dems, 10000)

# Compute and print p-value: p
p = np.sum(perm_replicates <= 153/244) / len(perm_replicates)
print('p-value =', p)


# No Hitter Dataset is split to include pre-1920 (dead ball era) and 1920-current (live ball era)
rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]

from datetime import datetime
deadEra = rawNoHitter["date"] < datetime.strptime("1920-01-01", "%Y-%m-%d")
deadEra = deadEra[1:]

nht_dead = nohitter_times[deadEra]
nht_live = nohitter_times[deadEra == False]

# Compute the observed difference in mean inter-no-hitter times: nht_diff_obs
nht_diff_obs = diff_of_means(nht_dead, nht_live)

# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates
perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, 10000)

# Compute and print the p-value: p
p = np.sum(perm_replicates <= nht_diff_obs) / len(perm_replicates)
print('p-val =',p)


# Function pearson_r is available from the previous course
rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy


# Compute observed correlation: r_obs
r_obs = pearson_r(illiteracy, fertility)

# Initialize permutation replicates: perm_replicates
perm_replicates = np.empty(10000)

# Draw replicates
for i in range(10000):
    # Permute illiteracy measurments: illiteracy_permuted
    illiteracy_permuted = np.random.permutation(illiteracy)
    
    # Compute Pearson correlation
    perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)

# Compute p-value: p
p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)
print('p-val =', p)


# Function ecdf is available from previous course
rawBee = pd.read_csv(myPath + "bee_sperm.csv", header=3, index_col=None)
control = rawBee.loc[rawBee["Treatment"] == "Control", "Sperm Volume per 500 ul"]
treated = rawBee.loc[rawBee["Treatment"] == "Pesticide", "Sperm Volume per 500 ul"]

# Compute x,y values for ECDFs
x_control, y_control = ecdf(control)
x_treated, y_treated = ecdf(treated)

# Plot the ECDFs
plt.plot(x_control, y_control, marker='.', linestyle='none')
plt.plot(x_treated, y_treated, marker='.', linestyle='none')

# Set the margins
plt.margins(0.02)

# Add a legend
plt.legend(('control', 'treated'), loc='lower right')

# Label axes and show plot
plt.xlabel('millions of alive sperm per mL')
plt.ylabel('ECDF')
# plt.show()
plt.savefig("_dummyPy182.png", bbox_inches="tight")
plt.clf()


# draw_bs_reps() is available back in Chapter 2
# Compute the difference in mean sperm count: diff_means
diff_means = np.mean(control) - np.mean(treated)

# Compute mean of pooled data: mean_count
mean_count = np.mean(np.concatenate((control, treated)))

# Generate shifted data sets
control_shifted = control - np.mean(control) + mean_count
treated_shifted = treated - np.mean(treated) + mean_count

# Generate bootstrap replicates
bs_reps_control = draw_bs_reps(control_shifted,
                       np.mean, size=10000)
bs_reps_treated = draw_bs_reps(treated_shifted,
                       np.mean, size=10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_reps_control - bs_reps_treated

# Compute and print p-value: p
p = np.sum(bs_replicates >= diff_means) / len(bs_replicates)
print('p-value =', p)

```
  
  
**Example #1: A/B Testing for Bees**:  
![](_dummyPy182.png)
  
***
  
Chapter 5 - Case Study  
  
Finch beaks and need for statistics:  
  
* Toolbox includes: Graphical and quantitative EDA ; Parameter estimation ; Confidence intervals ; Hypothesis testing  
* Finches (inhabitants of the Galapagos) formed a key backbone of Darwin's research for the theory of evolution  
	* Benefits include isolation, and a number of very small islands where the entire population can be monitored  
    * Researchers spend time every year in Daphne Major, one of the islands of the Galapagos  
* Finches of Daphne Major include Geospiza fortis and Geospiza scandens  
	* Peter and Rosemary Grant: "40 Years of Evolution: Darwin's Finches on Daphne Major Island" (2014)  
    * Data available for free, public use - see Dryad Digital Repository  
    * Data include Beak Length and Beak Depth  
* Initial analysis for the case study will be G. scandens beak depth changes over time (1975 vs. 2012)  
	* Hypothesis test: did the beaks get deeper from 1975 to 2012?  
  
Variation of beak shapes:  
  
* One hypothesis is that a drought in winter 1976/1977 led to a shortage of small seed (large seed were still pentiful), advataging finches with deeper beaks  
* Question is whether the beak length and beak depth are changing at the same rate, which would mean that the beak shape (aspect ratio) stayed the same  
  
Heritability - hypothesis of mating between G. scandens and G. fortis, producing hybrid birds:  	
  
* The hybrid birds then mate with pure G. scandens birds - process called "introgressive hybridization"  
* Question for testing is "how strongly are traits inherited by offspring from parents?"  
	* Assess for both G. scandens and G. fortis  
  
Final thoughts and course recap:  
  
* Perform EDA - ECDF, summary statistics  
* Estimate parameters - optimization, linear regression, confidence intervals (bootstrap method)  
* Formulate and test hypotheses  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2


def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates: bs_slope_reps, bs_intercept_reps
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    
    return bs_slope_reps, bs_intercept_reps



rawFinch1975 = pd.read_csv(myPath + "finch_beaks_1975.csv", header=0)
rawFinch1975.columns = ["band", "species", "beak_length", "beak_depth"]
rawFinch1975["year"] = 1975
rawFinch2012 = pd.read_csv(myPath + "finch_beaks_2012.csv", header=0)
rawFinch2012.columns = ["band", "species", "beak_length", "beak_depth"]
rawFinch2012["year"] = 2012

df = rawFinch1975.append(rawFinch2012)
bd_1975 = df.loc[(df["year"] == 1975) & (df["species"] == "scandens"), "beak_depth"].reset_index(drop=True)
bd_2012 = df.loc[(df["year"] == 2012) & (df["species"] == "scandens"), "beak_depth"].reset_index(drop=True)
bl_1975 = df.loc[(df["year"] == 1975) & (df["species"] == "scandens"), "beak_length"].reset_index(drop=True)
bl_2012 = df.loc[(df["year"] == 2012) & (df["species"] == "scandens"), "beak_length"].reset_index(drop=True)


# Create bee swarm plot
_ = sns.swarmplot(x="year", y="beak_depth", data=df.loc[df["species"] == "scandens", :])

# Label the axes
_ = plt.xlabel('year')
_ = plt.ylabel('beak depth (mm)')

# Show the plot
# plt.show()
plt.savefig("_dummyPy183.png", bbox_inches="tight")
plt.clf()


# function ecdf() is available from previous chapters
# Compute ECDFs
x_1975, y_1975 = ecdf(bd_1975)
x_2012, y_2012 = ecdf(bd_2012)

# Plot the ECDFs
_ = plt.plot(x_1975, y_1975, marker='.', linestyle='none')
_ = plt.plot(x_2012, y_2012, marker='.', linestyle='none')

# Set margins
plt.margins(0.02)

# Add axis labels and legend
_ = plt.xlabel('beak depth (mm)')
_ = plt.ylabel('ECDF')
_ = plt.legend(('1975', '2012'), loc='lower right')

# Show the plot
# plt.show()
plt.savefig("_dummyPy184.png", bbox_inches="tight")
plt.clf()


# draw_bs_reps() is available from previous chapters
# Compute the difference of the sample means: mean_diff
mean_diff = np.mean(bd_2012) - np.mean(bd_1975)

# Get bootstrap replicates of means
bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, 10000)

# Compute samples of difference of means: bs_diff_replicates
bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

# Compute 95% confidence interval: conf_int
conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5])

# Print the results
print('difference of means =', mean_diff, 'mm')
print('95% confidence interval =', conf_int, 'mm')


# Compute mean of combined data set: combined_mean
combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))

# Shift the samples
bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean
bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean

# Get bootstrap replicates of shifted data sets
bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, 10000)

# Compute replicates of difference of means: bs_diff_replicates
bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

# Compute the p-value
p = np.sum(bs_diff_replicates >= mean_diff) / len(bs_diff_replicates)

# Print p-value
print('p =', p)


# Make scatter plot of 1975 data
_ = plt.plot(bl_1975, bd_1975, marker='.',
             linestyle='none', color="blue", alpha=0.5)

# Make scatter plot of 2012 data
_ = plt.plot(bl_2012, bd_2012, marker='.',
             linestyle='none', color="red", alpha=0.5)

# Label axes and make legend
_ = plt.xlabel('beak length (mm)')
_ = plt.ylabel('beak depth (mm)')
_ = plt.legend(('1975', '2012'), loc='upper left')

# Show the plot
# plt.show()
plt.savefig("_dummyPy185.png", bbox_inches="tight")
plt.clf()


# draw_bs_pairs_linreg() is available in previous chapters
# Compute the linear regressions
slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, 1)
slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, 1)

# Perform pairs bootstrap for the linear regressions
bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, 1000)
bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, 1000)

# Compute confidence intervals of slopes
slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5])
slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5])
intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5])
intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5])


# Print the results
print('1975: slope =', slope_1975,
      'conf int =', slope_conf_int_1975)
print('1975: intercept =', intercept_1975,
      'conf int =', intercept_conf_int_1975)
print('2012: slope =', slope_2012,
      'conf int =', slope_conf_int_2012)
print('2012: intercept =', intercept_2012,
      'conf int =', intercept_conf_int_2012)


# Make scatter plot of 1975 data
_ = plt.plot(bl_1975, bd_1975, marker='.',
             linestyle='none', color='blue', alpha=0.5)

# Make scatter plot of 2012 data
_ = plt.plot(bl_2012, bd_2012, marker='.',
             linestyle='none', color='red', alpha=0.5)

# Label axes and make legend
_ = plt.xlabel('beak length (mm)')
_ = plt.ylabel('beak depth (mm)')
_ = plt.legend(('1975', '2012'), loc='upper left')

# Generate x-values for bootstrap lines: x
x = np.array([10, 17])

# Plot the bootstrap lines
for i in range(100):
    plt.plot(x, bs_intercept_reps_1975[i] + bs_slope_reps_1975[i] * x,
             linewidth=0.5, alpha=0.2, color="blue")
    plt.plot(x, bs_intercept_reps_2012[i] + bs_slope_reps_2012[i] * x,
             linewidth=0.5, alpha=0.2, color="red")

# Draw the plot again
# plt.show()
plt.savefig("_dummyPy186.png", bbox_inches="tight")
plt.clf()


# Compute length-to-depth ratios
ratio_1975 = bl_1975 / bd_1975
ratio_2012 = bl_2012 / bd_2012

# Compute means
mean_ratio_1975 = np.mean(ratio_1975)
mean_ratio_2012 = np.mean(ratio_2012)

# Generate bootstrap replicates of the means
bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, 10000)

# Compute the 99% confidence intervals
conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5])
conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5])

# Print the results
print('1975: mean ratio =', mean_ratio_1975,
      'conf int =', conf_int_1975)
print('2012: mean ratio =', mean_ratio_2012,
      'conf int =', conf_int_2012)


rawScandensBD = pd.read_csv(myPath + "scandens_beak_depth_heredity.csv", header=0)
rawFortisBD = pd.read_csv(myPath + "fortis_beak_depth_heredity.csv", header=0)

bd_parent_fortis = rawFortisBD.iloc[:, 1:].apply(np.mean, axis=1)
bd_offspring_fortis = rawFortisBD["Mid-offspr"]
bd_parent_scandens = rawScandensBD["mid_parent"]
bd_offspring_scandens = rawScandensBD["mid_offspring"]


# Make scatter plots
_ = plt.plot(bd_parent_fortis, bd_offspring_fortis,
             marker=".", linestyle="none", color="blue", alpha=0.5)
_ = plt.plot(bd_parent_scandens, bd_offspring_scandens,
             marker=".", linestyle="none", color="red", alpha=0.5)

# Set margins
plt.margins(0.02)

# Label axes
_ = plt.xlabel('parental beak depth (mm)')
_ = plt.ylabel('offspring beak depth (mm)')

# Add legend
_ = plt.legend(('G. fortis', 'G. scandens'), loc='lower right')

# Show plot
# plt.show()
plt.savefig("_dummyPy187.png", bbox_inches="tight")
plt.clf()


def draw_bs_pairs(x, y, func, size=1):
    """Perform pairs bootstrap for a specified function (func)."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_replicates[i] = func(bs_x, bs_y)
    
    return bs_replicates


# Note that pearson_r() exists from the previous course
# Compute the Pearson correlation coefficients
r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens)
r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis)

# Acquire 1000 bootstrap replicates of Pearson r
bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, 1000)
bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, 1000)

# Compute 95% confidence intervals
conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5])
conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5])

# Print results
print('G. scandens:', r_scandens, conf_int_scandens)
print('G. fortis:', r_fortis, conf_int_fortis)


def heritability(parents, offspring):
    """Compute the heritability from parent and offspring samples."""
    covariance_matrix = np.cov(parents, offspring)
    return covariance_matrix[0, 1] / covariance_matrix[0, 0]

# Compute the heritability
heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens)
heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis)

# Acquire 1000 bootstrap replicates of heritability
replicates_scandens = draw_bs_pairs(
        bd_parent_scandens, bd_offspring_scandens, heritability, size=1000)
replicates_fortis = draw_bs_pairs(
        bd_parent_fortis, bd_offspring_fortis, heritability, size=1000)

# Compute 95% confidence intervals
conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5])
conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5])

# Print results
print('G. scandens:', heritability_scandens, conf_int_scandens)
print('G. fortis:', heritability_fortis, conf_int_fortis)


# Initialize array of replicates: perm_replicates
perm_replicates = np.empty(10000)

# Draw replicates
for i in range(10000):
    # Permute parent beak depths
    bd_parent_permuted = np.random.permutation(bd_parent_scandens)
    perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens)

# Compute p-value: p
p = np.sum(perm_replicates >= heritability_scandens) / len(perm_replicates)

# Print the p-value
print('p-val =', p)

```
  

**Example #1: G. Scandens Beak Depth (1975 vs. 2012) Swarm Plot**:  
![](_dummyPy183.png)

**Example #2: G. Scandens Beak Depth (1975 vs. 2012) ECDF**:  
![](_dummyPy184.png)

**Example #3: G. Scandens Beak Depth vs. Length (1975 vs. 2012) Scatter Plot**:  
![](_dummyPy185.png)

**Example #4: G. Scandens Beak Depth vs. Length (1975 vs. 2012) Bootstrap Replicates**:  
![](_dummyPy186.png)

**Example #5: G. Scandens and G. Fortis Beak Depth Heritability**:  
![](_dummyPy187.png)

  

## Python Machine Learning  
###_Supervised Learning with scikit-learn_#

Chapter 1 - Classification  
  
Supervised learning - labelled data, where there is a correct answer:  
  
* Machine learning - the art and science of giving computers the ability to learn and make decisions from data (without being explicitly programmed)  
* Reinforcement learning - software agents interact with an environment, learning to optimize behavior for a system of rewards and punishments (like behavioral psychology)  
* Supervised Learning - labelled data, with both a target variable and 1+ predictor/feature variables  
	* Classification problems are categorical  
    * Regression problems are continuous  
* Note that features, predictor variables, and independent variables all mean the same thing  
* Similarly, target variables may also be called response variables or dependent variables  
* This course will use scikit-learn/sklearn - integrated well with the SciPy stack  
	* Other libraries like TensorFlow and keras are also good  
  
Exploratory data analysis - beginning with standard "iris" dataset with "species" as the target variable:  
  
* Can grab the dataset files from sklearn using "from sklearn import datasets"  
	* Typically will also need pd, np, plt, and the 'ggplot' style as set using plt.style.use('ggplot')  
    * iris = datasets.load_iris()  # will load the iris data as a "Bunch" file - set of key-value pairs - "data", "target_names", "DESCR", "feature_names", "target"  
    * The iris.data (150x4) and iris.target are both NumPy arrays, which iris.target_names is needed to map the numbers in iris.target to actual flower names  
* For EDA, can be helpful to use pd.scatter_matrix(pd.DataFrame(iris.data, columns=iris.feature_name), color=iris.target)  
  
Classification challenge - take training (already labelled) data to predict test (not yet labelled) data:  
  
* kNN (k-Nearest-Neighbors) is the process of classifying based on the closest "k" data points -- creates decision boundaries  
	* from sklearn.neighbors import KNeighborsClassifier  
    * knn = KNeighborsClassifiers(n_neighbors=6)  # this optimizer require that there is no missing data and that the features are all continuous rather than categorical  
* All machine learning models are implemented as Python classes - implement algorithms for both learning and predicting  
	* Training (fitting) is run with the .fit() method - for example, knn.fit(iris["data"], iris["target"])  
    * Predicting an unlabelled data point is run with .predict() - for example, prediction=knn.predict(X_new)  # must be a NumPy array  
  
Measuring model performance:  
  
* Accuracy is a common metric - # Correct / # Predicted - typically assessed on the "test" data  
	* Train/fit on the training data, but assess the accuracy on the test data  
    * from sklearn.model_selection import train_test_split  
    * X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)  # test_size is proportion to test, while random_state sets the seed for reproducibility  
* Can assess scores of the model using knn.score(X_test, y_test)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



df = pd.read_csv(myPath + "house-votes-84.csv", header=None, na_values="?")
temp = [x for x in df.columns]
temp[0] = "party"
df.columns = temp

df = df.dropna()
for ctr in range(1, df.shape[1]):
    df.iloc[:, ctr] = [1 if x == "y" else 0 for x in df.iloc[:, ctr]]


# Import KNeighborsClassifier from sklearn.neighbors
from sklearn.neighbors import KNeighborsClassifier

# Create arrays for the features and the response variable
y = df['party'].values
X = df.drop('party', axis=1).values

# Create a k-NN classifier with 6 neighbors
knn = KNeighborsClassifier(n_neighbors=6)

# Fit the classifier to the data
knn.fit(X, y)

# Predict the labels for the training data X
y_pred = knn.predict(X)

# Predict and print the label for the new data point X_new
X_new = np.array([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0]).reshape(1, -1)
new_prediction = knn.predict(X_new)
print("Prediction: {}".format(new_prediction))


# Import necessary modules
from sklearn import datasets
import matplotlib.pyplot as plt

# Load the digits dataset: digits
digits = datasets.load_digits()

# Print the keys and DESCR of the dataset
print(digits.keys())
print(digits["DESCR"])

# Print the shape of the images and data keys
print(digits.images.shape)
print(digits.data.shape)

# Display digit 1010
plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')
# plt.show()
plt.savefig("_dummyPy188.png", bbox_inches="tight")
plt.clf()

# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split

# Create feature and target arrays
X = digits.data
y = digits.target

# Split into training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)

# Create a k-NN classifier with 7 neighbors: knn
knn = KNeighborsClassifier(n_neighbors=7)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Print the accuracy
print(knn.score(X_test, y_test))


# Setup arrays to store train and test accuracies
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

# Loop over different values of k
for i, k in enumerate(neighbors):
    # Setup a k-NN Classifier with k neighbors: knn
    knn = KNeighborsClassifier(n_neighbors=k)
    
    # Fit the classifier to the training data
    knn.fit(X_train, y_train)
    
    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(X_train, y_train)
    
    #Compute accuracy on the testing set
    test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
# plt.show()
plt.savefig("_dummyPy189.png", bbox_inches="tight")
plt.clf()

```
  
**Example #1: Digital Image**:  
![](_dummyPy188.png)
  
**Example #2: Train/Test Accuracy vs. # Neighbors**:  
![](_dummyPy189.png)
  
***
  
Chapter 2 - Regression  
  
Introduction to regression - target variable is a continuous variable (e.g., Boston housing dataset):  
  
* Suppose that there is a pd.DataFrame called "boston" which has target variable "MEDV" and a number of predictors  
	* X = boston.drop("MEDV", axis=1).values  
    * y = boston["MEDV"].values  
    * May need to call .reshape(-1, 1) to convert them for future analysis (???) - seems to convert from .shape (X,) to .shape(X, 1) which is preferred by scikit  
* In general, to run a linear regression using sklearn  
	* from sklearn import linear_model  
    * reg = linear_model.LinearRegression()  
    * reg.fit(X, y)  
  
Basics of linear regression - simple linear regression has the format y = a*x + b:  
  
* The error (loss, cost) function for linear regression is typically the sum-square of the residuals -- OLS (Ordinary Least Squares)  
* The scikit API works the same for simple regression with one feature and regression with multiple features  
* The default .score() for regression in scikit is R-squared  
  
Cross-validation - accounting for model performance being dependent on the way the data is split (not generalizable):  
  
* One CV approach splits the data in to k-folds, using each of the folds as its own hold-put sample  
* Implementing cross-validation in scikit-learn includes  
	* from sklearn.model_selection import cross_val_score  
    * reg = LinearRegression()  
    * cv_results = cross_val_score(reg, X, y, cv=5)  # for 5-fold  
  
Regularized regression - avoiding over-fitting due to large coefficients:  
  
* Regularization involves placing a penalty on large coefficients  
* Ridge regression loss function is standard OLS loss function + alpha * sum-squared(coefficients)  # alpha is sometimes called lambad; when 0, this is the same as OLS  
	* from sklearn.linear_model import Ridge  
    * ridge = Ridge(alpha=0.1, normalize=True) # assuming lambda=0.1, run on standardized scale is desired  
    * ridge.fit(X_train, y_train)  
    * ridge_pred = ridge.predict(X_test)  
    * ridge.score(X_test, y_test)  
* Lasso regression loss function is standard OLS loss function + alpha * sum-absolute-value(coefficients)  
	* Run the same as ridge regression, except that Lasso is used for the import and Lasso() for the initial object setup  
    * Lasso regression is especially useful for feature selection, since it tends to select 0 as the coefficient for less important variables  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# Read the CSV file into a DataFrame: df
df = pd.read_csv(myPath + "gapminder_tidy.csv")
df = df.loc[df["Year"] == 2010, :]


# Create arrays for features and target variable
y = df["life"].values
X = df["fertility"].values

# Print the dimensions of X and y before reshaping
print("Dimensions of y before reshaping: {}".format(y.shape))
print("Dimensions of X before reshaping: {}".format(X.shape))

# Reshape X and y
y = y.reshape(-1, 1)
X = X.reshape(-1, 1)

# Print the dimensions of X and y after reshaping
print("Dimensions of y after reshaping: {}".format(y.shape))
print("Dimensions of X after reshaping: {}".format(X.shape))


# Import LinearRegression
from sklearn.linear_model import LinearRegression

# Create the regressor: reg
reg = LinearRegression()
X_fertility = X.copy()

# Create the prediction space
prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)

# Fit the model to the data
reg.fit(X_fertility, y)

# Compute predictions over the prediction space: y_pred
y_pred = reg.predict(prediction_space)

# Print R^2 
print(reg.score(X_fertility, y))

# Plot regression line
plt.plot(prediction_space, y_pred, color='black', linewidth=3)
# plt.show()
plt.savefig("_dummyPy190.png", bbox_inches="tight")
plt.clf()


# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)

# Create the regressor: reg_all
reg_all = LinearRegression()

# Fit the regressor to the training data
reg_all.fit(X_train, y_train)

# Predict on the test data: y_pred
y_pred = reg_all.predict(X_test)

# Compute and print R^2 and RMSE
print("R^2: {}".format(reg_all.score(X_test, y_test)))
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error: {}".format(rmse))


# Import the necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Create a linear regression object: reg
reg = LinearRegression()

# Compute 5-fold cross-validation scores: cv_scores
cv_scores = cross_val_score(reg, X, y, cv=5)

# Print the 5-fold cross-validation scores
print(cv_scores)

print("Average 5-Fold CV Score: {}".format(np.mean(cv_scores)))


# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Create a linear regression object: reg
reg = LinearRegression()

# Perform 3-fold CV
cvscores_3 = cross_val_score(reg, X, y, cv=3)
print(np.mean(cvscores_3))

# Perform 10-fold CV
cvscores_10 = cross_val_score(reg, X, y, cv=10)
print(np.mean(cvscores_10))


# Use mtcars
# df = pd.read_csv(myPath + "mtcars.csv", index_col=0)
# df = df[["mpg", "disp", "hp", "drat", "wt", "qsec"]]

# use winequality-red
# df = pd.read_csv(myPath + "winequality-red.csv", header=0, sep=";")

# use gapminder
df = pd.read_csv(myPath + "gm_2008_region.csv")
df_columns = ['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP', 'BMI_female', 'child_mortality']

X = df[df_columns].values
y = df["life"].values.reshape(-1, 1)

# Import lasso
from sklearn.linear_model import Lasso

# Instantiate a lasso regressor: lasso
lasso = Lasso(alpha=0.4, normalize=True)

# Fit the regressor to the data
lasso.fit(X, y)

# Compute and print the coefficients
lasso_coef = lasso.coef_
print(lasso_coef)

# Plot the coefficients
plt.plot(range(len(df_columns)), lasso_coef)
plt.xticks(range(len(df_columns)), df_columns, rotation=60)
plt.margins(0.02)
# plt.show()
plt.savefig("_dummyPy191.png", bbox_inches="tight")
plt.clf()


def display_plot(cv_scores, cv_scores_std):
    fig = plt.figure()
    ax = fig.add_subplot(1,1,1)
    ax.plot(alpha_space, cv_scores)
    
    std_error = cv_scores_std / np.sqrt(10)
    
    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)
    ax.set_ylabel('CV Score +/- Std Error')
    ax.set_xlabel('Alpha')
    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')
    ax.set_xlim([alpha_space[0], alpha_space[-1]])
    ax.set_xscale('log')
    # plt.show()
    plt.savefig("_dummyPy192.png", bbox_inches="tight")
    plt.clf()


# Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# Setup the array of alphas and lists to store scores
alpha_space = np.logspace(-4, 0, 50)
ridge_scores = []
ridge_scores_std = []

# Create a ridge regressor: ridge
ridge = Ridge(normalize=True)

# Compute scores over range of alphas
for alpha in alpha_space:
    
    # Specify the alpha value to use: ridge.alpha
    ridge.alpha = alpha
    
    # Perform 10-fold CV: ridge_cv_scores
    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)
    
    # Append the mean of ridge_cv_scores to ridge_scores
    ridge_scores.append(np.mean(ridge_cv_scores))
    
    # Append the std of ridge_cv_scores to ridge_scores_std
    ridge_scores_std.append(np.std(ridge_cv_scores))

# Display the plot
display_plot(ridge_scores, ridge_scores_std)

```
  
**Example #1: Linear Regression Line**:  
![](_dummyPy190.png)

**Example #2: Lasso Coefficients**:  
![](_dummyPy191.png)

**Example #3: Ridge Regression with Varying Alpha**:  
![](_dummyPy192.png)
  
  
***
  
Chapter 3 - Model Fine-Tuning  
  
How good is the model?  Accuracy is not always a useful metric, especially in a "class imbalanced" dataset (e.g., 99% of the items are of type A, so 99% accuracy is meaningless):  
  
* Confusion matrix for classification - rows are Actual, columns are Predicted  
	* Precision (aka Positive Predictve Value or PPV): TP / (TP + FP) # of the items that are predicted as Positive, how many are actually positive?  
    * Recall (aka Sensitivity): TP / (TP + FN) # of the items that are actually Positive, how many are predicted Positive  
    * F1 Score (harmonic mean of Precision and Recall): 2 * Precision * Recall / (Precision + Recall)  
* High Precision means that few actual negatives are misclassified as positive  
* High Recall means that most actual positives are accurately classified as positive  
* Automated routine for running confusion matrices in scikit-learn  
	* from sklearn.metrics import classification_report, confusion_matrix  
    * knn = KNeighborsClassifier(n_neighbors=k) ; knn.fit(X_train, y_train) ; y_pred = knn.predict(X_test)  
    * confusion_matrix(y_test, y_pred)  # will give the 2x2  
    * classification_report(y_test, y_pred)  # will give precision-f1-recall-support for each of the possible states of y, plus an aggregate Total  
  
Logistic regression and ROC curves - binary classification that outputs target probability between 0 and 1:  
  
* The logit can be automated in scikit-learn very similarly to the other methodologies  
	* from sklearn.linear_model import LogisticRegression  
    * logreg = LogisticRegression()  
    * logreg.fit(X_train, y_train)  
    * y_pred = logreg.predict(X_test)  
* The ROC (Receiver Operator Characteristic) curve is the plot of True Positive Rate vs. False Positive Rate for all classification thresholds between 0 and 1  
	* from sklearn.metrics import roc_curve  
    * y_pred_prob = logreg.predict_proba(X_test)[:, 1]  # gives back the probabilities rather than the classifications - second column (index 1) is the probability of predicted labels being 1  
    * fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)  
  
Area under ROC curve (AUC) - larger area means better model:  
  
* Running the AUC for a single run of the model  
	* from sklearn.metrics import roc_auc_score  
    * y_pred_prob = logreg.predict_proba(X_test)[:, 1]  
    * roc_auc_score(y_test, y_pred_prob)  
* Alternately, running cross-validation on the AUC  
	* cv_scores = cross_val_score(logreg, X, y, cv=5, scoring="roc_auc")  
  
Hyperparameter tuning - selecting among the possible control parameters for the model:  
  
* Choosing the correct hyper-parameters is key to designing a good model; search the potential solution space, and choose the best  
	* Essential to use cross-validation in this process, as there is otherwise real risk of over-fitting  
* The grid-search cross-validation process is implemented in scikit  
	* from sklearn.model_selection import GridSearchCV  
    * param_grid = { "n_neighbors":np.arange(1, 50) }  # set up as a dictionary, can have as many items as desired  
    * knn = KNeighborsClassifier()  
    * knn_cv = GridSearchCV(knn, param_grid, cv=5)  
    * knn_cv.fit(X, y)  # this performs the actual optimizations for each cell in the grid  
    * knn_cv.best_params_  ; knn_cv.best_score_  # get back the best parameter and the best score  
  
Hold-out set (final evaluation) - how well can the model perform on never before seen data?:  
  
* This requires holding some data back from the CV process  
* Basically, the initial split is Train vs. Hold-Out, with only Train used in the CV, model-fitting, and the like  
* The final report of model performance is based on running the final model selected on the Hold-Out sample  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# PIMA diabetes data
myDF = pd.read_csv(myPath + "diabetes.csv")
y = myDF["diabetes"].values
X = myDF.drop("diabetes", axis=1).values
X_columns = myDF.drop("diabetes", axis=1).columns


# Import necessary modules
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Create training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)

# Instantiate a k-NN classifier: knn
knn = KNeighborsClassifier(n_neighbors=6)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Import the necessary modules
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)

# Create the classifier: logreg
logreg = LogisticRegression()

# Fit the classifier to the training data
logreg.fit(X_train, y_train)

# Predict the labels of the test set: y_pred
y_pred = logreg.predict(X_test)

# Compute and print the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Import necessary modules
from sklearn.metrics import roc_curve

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
# plt.show()
plt.savefig("_dummyPy193.png", bbox_inches="tight")
plt.clf()


# Import necessary modules
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Compute and print AUC score
print("AUC: {}".format(roc_auc_score(y_test, y_pred_prob)))

# Compute cross-validated AUC scores: cv_auc
cv_auc = cross_val_score(logreg, X, y, cv=5, scoring="roc_auc")

# Print list of AUC scores
print("AUC scores computed using 5-fold cross-validation: {}".format(cv_auc))


# Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: C.
# C controls the inverse of the regularization strength, and this is what you will tune in this exercise.
# A large C can lead to an overfit model, while a small C can lead to an underfit model.

# Import necessary modules
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Setup the hyperparameter grid
c_space = np.logspace(-5, 8, 15)
param_grid = {'C': c_space}

# Instantiate a logistic regression classifier: logreg
logreg = LogisticRegression()

# Instantiate the GridSearchCV object: logreg_cv
logreg_cv = GridSearchCV(logreg, param_grid, cv=5)

# Fit it to the data
logreg_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Logistic Regression Parameters: {}".format(logreg_cv.best_params_)) 
print("Best score is {}".format(logreg_cv.best_score_))


# GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. 
# A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out.
# Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.

# Import necessary modules
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Setup the parameters and distributions to sample from: param_dist
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}

# Instantiate a Decision Tree classifier: tree
tree = DecisionTreeClassifier()

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fit it to the data
tree_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))


# Import necessary modules
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Create the hyperparameter grid
c_space = np.logspace(-5, 8, 15)
param_grid = {"C": c_space, "penalty": ['l1', 'l2']}

# Instantiate the logistic regression classifier: logreg
logreg = LogisticRegression()

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Instantiate the GridSearchCV object: logreg_cv
logreg_cv = GridSearchCV(logreg, param_grid, cv=5)

# Fit it to the training data
logreg_cv.fit(X_train, y_train)

# Print the optimal parameters and best score
print("Tuned Logistic Regression Parameter: {}".format(logreg_cv.best_params_))
print("Tuned Logistic Regression Accuracy: {}".format(logreg_cv.best_score_))


# Import necessary modules
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Create the hyperparameter grid
l1_space = np.linspace(0.01, 0.99, 30)
param_grid = {"l1_ratio": l1_space}

# Instantiate the ElasticNet regressor: elastic_net
elastic_net = ElasticNet()

# Setup the GridSearchCV object: gm_cv
gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)

# Fit it to the training data
gm_cv.fit(X_train, y_train)

# Predict on the test set and compute metrics
y_pred = gm_cv.predict(X_test)
r2 = gm_cv.score(X_test, y_test)
mse = mean_squared_error(y_test, y_pred)
print("Tuned ElasticNet l1 ratio: {}".format(gm_cv.best_params_))
print("Tuned ElasticNet R squared: {}".format(r2))
print("Tuned ElasticNet MSE: {}".format(mse))

```
  

**Example #1: ROC Curve for PIMA Diabetes Prediction**:  
![](_dummyPy193.png)
  
  
***
  
Chapter 4 - Pre-processing and Pipelines  
  
Preprocessing data - preparing the data for use in the analysis pipeline:  
  
* Categorical features are by default NOT accepted by the scikit-learn API, although they are OK once split in to dummy variables  
	* scikit-learn: OneHotEncoder()  
    * pandas: get_dummies()  # creates dummy variables on the entire DataFrame, including a column for all levels (one of which should be dropped prior to regression)  
  
Handling missing data - including "stealth" encoding such as values of zero for fields which should be positive:  
  
* df["myCol"].replace(0, np.nan, inplace=True)  # will update df such that any 0 in myCol will be changed to np.nan  
* Can drop all rows with missing data by using df.dropna(), though this may result in an unacceptable amount of data loss (data remaining)  
* Can instead impute the missing values (using the mean/median for the column of the non-missing values)  
	* from sklearn.preprocessing import Imputer  
    * imp = Imputer(missing_values="NaN", strategy="mean", axis=0)  # axis=0 means imputing along columns, while axis=1 would mean rows  
    * imp.fit(X)  # this will fit the Imputer to the DataFrame X  
    * X = imp.transform(X)  # Imputer is known as a transformer  
* Alternately, can use a pipeline project to do the imputing and the model running all at the same time  
	* from sklearn.pipeline import Pipeline  
    * imp = Imputer(missing_values="NaN", strategy="mean", axis=0)  # axis=0 means imputing along columns, while axis=1 would mean rows  
    * logreg = LogisticRegression()  
    * steps = [ ("imputation", imp), ("logistic_regression", logreg) ]  # The tuples are the name and then the command  
    * pipeline = Pipeline(steps)  
    * pipeline.fit(X_train, y_train)  
    * y_pred = pipeline.predict(X_test) ; pipeline.score(X_test, y_test)  
* Note that all steps in a Pipeline() must be transformers  
  
Centering and scaling - particularly important for models that use some form of distance to inform them:  
  
* Can normalize (center and scale) the data using Python  
	* from sklearn.preprocessing import scale  
    * X_scaled = scale(X)  
* Alternately, can use the Pipeline() object, such as  
	* from sklearn.preprocessing import StandardScaler  
    * pipeline = Pipeline( ("scaler", StandardScaler()) , ("knn", KNeighborsClassifier()) )  
* Can further use CV and Grid Search with the Pipeline object  
	* parameters = { knn_n_neighbors=np.arange(1, 50) }  # The double underscore after knn means "use knn from pipeline" and "range n_neighbors over the assigned range"  
    * cv = GridSearchCV(pipeline, param_grid=parameters)  
    * cv.fit(X_train, y_train)  
    * y_pred = cv.predict(X_test)  
    * cv.best_params_  ; cv.score(X_test, y_test)  ; classification_report( y_test, y_pred )  
  
Final thoughts:  
  
* Machine learning techniques for classification and regression  
* Real-world data  
* Under-fitting and over-fitting  
* Test-train split  
* Cross-validation  
* Grid search  
* Regularization (lasso, ridge)  
* Pre-processing  
* Check out scikit-learn documentation or "Introducing Machine Learning for Python" by Andreas Muller  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import ElasticNet


# Read 'gapminder.csv' into a DataFrame: df
df = pd.read_csv(myPath + "gm_2008_region.csv")

# Create a boxplot of life expectancy per region
df.boxplot("life", "Region", rot=60)

# Show the plot
# plt.show()
plt.savefig("_dummyPy194.png", bbox_inches="tight")
plt.clf()


# Create dummy variables: df_region
df_region = pd.get_dummies(df)

# Print the columns of df_region
print(df_region.columns)

# Create dummy variables with drop_first=True: df_region
df_region = pd.get_dummies(df, drop_first=True)

# Print the new columns of df_region
print(df_region.columns)


# Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

X = df_region.drop("life", axis=1).values
y = df_region["life"].values

# Instantiate a ridge regressor: ridge
ridge = Ridge(alpha=0.5, normalize=True)

# Perform 5-fold cross-validation: ridge_cv
ridge_cv = cross_val_score(ridge, X, y, cv=5)

# Print the cross-validated scores
print(ridge_cv)


df = pd.read_csv(myPath + "house-votes-84.csv", header=None)
df.columns = ['party', 'infants', 'water', 'budget', 'physician', 'salvador', 'religious', 'satellite', 'aid', 'missile', 'immigration', 'synfuels', 'education', 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa']

# Convert '?' to NaN
df[df == "?"] = np.nan

# Print the number of NaNs
print(df.isnull().sum())

# Print shape of original DataFrame
print("Shape of Original DataFrame: {}".format(df.shape))

# Print shape of new DataFrame
print("Shape of DataFrame After Dropping All Rows with Missing Values: {}".format(df.dropna().shape))



df = pd.read_csv(myPath + "house-votes-84.csv", header=None)
df.columns = ['party', 'infants', 'water', 'budget', 'physician', 'salvador', 'religious', 'satellite', 'aid', 'missile', 'immigration', 'synfuels', 'education', 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa']

df[df == "?"] = np.nan
df[df == "y"] = 1
df[df == "n"] = 0

X = df.drop("party", axis=1).values
y = df["party"].values


# Import the Imputer module
from sklearn.preprocessing import Imputer
from sklearn.svm import SVC

# Setup the Imputation transformer: imp
imp = Imputer(missing_values="NaN", strategy="most_frequent", axis=0)

# Instantiate the SVC classifier: clf
clf = SVC()

# Setup the pipeline with the required steps: steps
steps = [('imputation', imp),
        ('SVM', clf)]


# Import necessary modules
from sklearn.pipeline import Pipeline

# Setup the pipeline steps: steps
steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),
        ('SVM', SVC())]

# Create the pipeline: pipeline
pipeline = Pipeline(steps)

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit the pipeline to the train set
pipeline.fit(X_train, y_train)

# Predict the labels of the test set
y_pred = pipeline.predict(X_test)

# Compute metrics
print(classification_report(y_test, y_pred))




# Import scale
from sklearn.preprocessing import scale

# X is "quality" from the white wine quality dataset
rawWhite = pd.read_csv(myPath + "white-wine.csv")
y = rawWhite["quality"].values
X = rawWhite.drop("quality", axis=1).values

# Scale the features: X_scaled
X_scaled = scale(X)

# Print the mean and standard deviation of the unscaled features
print("Mean of Unscaled Features: {}".format(np.mean(X))) 
print("Standard Deviation of Unscaled Features: {}".format(np.std(X)))

# Print the mean and standard deviation of the scaled features
print("Mean of Scaled Features: {}".format(np.mean(X_scaled))) 
print("Standard Deviation of Scaled Features: {}".format(np.std(X_scaled)))


# Import the necessary modules
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Setup the pipeline steps: steps
steps = [('scaler', StandardScaler()),
        ('knn', KNeighborsClassifier())]
        
# Create the pipeline: pipeline
pipeline = Pipeline(steps)

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit the pipeline to the training set: knn_scaled
knn_scaled = pipeline.fit(X_train, y_train)

# Instantiate and fit a k-NN classifier to the unscaled data
knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)

# Compute and print metrics
print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))
print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))


# Setup the pipeline
steps = [("scaler", StandardScaler()),
         ("SVM", SVC())]

pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {'SVM__C':[1, 10, 100],
              'SVM__gamma':[0.1, 0.01]}

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)

# Instantiate the GridSearchCV object: cv
cv = GridSearchCV(pipeline, param_grid=parameters, cv=3)

# Fit to the training set
cv.fit(X_train, y_train)

# Predict the labels of the test set: y_pred
y_pred = cv.predict(X_test)

# Compute and print metrics
print("Accuracy: {}".format(cv.score(X_test, y_test)))
print(classification_report(y_test, y_pred))
print("Tuned Model Parameters: {}".format(cv.best_params_))


# Setup the pipeline steps: steps
steps = [("imputation", Imputer(missing_values="NaN", strategy="mean", axis=0)),
         ("scaler", StandardScaler()),
         ("elasticnet", ElasticNet())]

# Create the pipeline: pipeline 
pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {"elasticnet__l1_ratio":np.linspace(0.01 ,1 , 30)}

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Create the GridSearchCV object: gm_cv
gm_cv = GridSearchCV(pipeline, param_grid=parameters, cv=3)

# Fit to the training set
gm_cv.fit(X_train, y_train)

# Compute and print the metrics
r2 = gm_cv.score(X_test, y_test)
print("Tuned ElasticNet Alpha: {}".format(gm_cv.best_params_))
print("Tuned ElasticNet R squared: {}".format(r2))


```
  
  
**Example #1: Box Plot for Life Expectancy by Region (Gapminder)**:  
![](_dummyPy194.png)

	
###_Unsupervised Learning in Python_#
  
Chapter 1 - Clustering for dataset exploration  
  
Unsupervised learning - machine learning data to discover patterns in data:  
  
* Common techniques include clustering and dimension reduction  
* Unsupervised learning contrasts with supervised learning in that there are no labels on the data  
* An example dataset used will be "iris", though without the Species field  
* k-means clustering is implemented within scikit-learn  
	* from sklearn.cluster import KMeans  
    * model = KMeans(n_clusters=3)  
    * model.fit(myData)  
    * labels = model.predict(samples)  
* The k-means model will remember the centroids (cluster means) and assign new data points to the closest cluster  
	* newLabels = model.predict(newData)  
  
Evaluate a clustering - in a manner that does not necessarily assume that there are any pre-defined labels:  
  
* If the labels exist, can run a cross-tab of the segmentes created against the known labels  
	* df = pd.DataFrame( {"labels":myCluster , "species":actualSpecies} )  
    * ct = pd.crosstab(df["labels"], df["species"])  
    * print(ct)  
* If the labels do not exist, can still assess whether clusters tend to be similar within and different without  
	* Inertia: meaurement of clustering quality for how spread out the clusters are (lower is better); distance from each sample to cluster centroid  
    * model.inertia_ # Always calculated when the model.fit() is run, and can be accessed later  
    * Choosing an "elbow" point in the inertia plot is frequently a good strategy, balancing low Inertia with not too many clusters  
  
Transforming features for better clustering:  
  
* Example of the wine database where features have very different variances  
* Within k-means, the variance of a feature will be directly proportional to its influence  
* The StandardScaler() function as per above course transforms every feature to have mean 0 and variance 1  
	* from sklearn.preprocessing import StandardScaler  
    * scaler = StandardScaler()  
    * scaler.fit(samples)  
    * samples_scaled = scaler.transform(samples)  # note that scaler.transform() could instead be applied to brand new data  
* As per the previous course, pipelines can be used to combine a transformation followed by a model run  
	* from sklearn.pipeline import make_pipeline  
    * pipeline = make_pipeline(StandardScaler(), KMeans(n_clusters=3))  
    * pipeline.fit(samples)  # will run the standard scaling and then also build the 3 clusters  
    * labels = pipeline.predict(samples)  # get the projected labels for the samples  
* Other options for pre-processing include MaxAbsScaler and Normalizer  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans


xPoints = [65, -1529, 1709, 1167, -1801, -1635, 1219, -261, -1619, -1843, 1359, 422, -1685, 906, -1564, -25, 830, 1224, -153, 863, -1433, 38, -1585, 635, 244, -2196, 1032, -288, 205, -1210, 1334, 1194, 991, -1835, 84, -1489, 387, -14, -2052, 103, -441, 1103, -1592, -1473, -1355, 41, 476, 1043, -79, 870, 1402, 801, -165, -337, 806, -1435, 548, 468, -1513, 913, -1972, 521, 627, 941, 569, -1876, 906, -1374, -1805, 343, 703, -1684, 1314, 261, 919, -1217, -177, 648, 412, 1015, 679, -1051, 613, -1502, -1727, -1609, -1091, -10, 327, 1220, -1333, 879, -1148, -580, -11, 173, 1327, -1934, -1577, 476, -1652, -126, -1896, 574, -166, 801, 1184, 1088, -1611, -1575, 72, -1400, 1096, -2543, -1345, -1354, 183, -1313, 99, 1172, 86, -210, 264, -255, -260, -1917, 930, 447, -1638, -1957, 927, 1256, -1728, 349, -1516, 187, 896, -1549, 1215, 203, 845, 532, -53, 1388, 1006, 566, 868, -1162, 277, 165, 382, 1145, 814, -1860, 965, -1498, -133, 125, 1060, -1491, 1161, 542, 892, -1499, 316, 139, -1549, 1238, 1255, 255, 451, 1062, 674, 227, -1458, 232, 1596, 804, 1154, 596, 28, 1134, 215, 1410, 1122, 252, -1285, 1528, -239, -257, 593, 79, -1272, 669, 347, -2112, -1629, -1538, -119, -1544, 300, 9, -1334, 475, 593, 413, 558, 498, 356, -1861, -1619, 807, 1627, -1569, 1025, 242, 1231, -1965, 427, -1583, -1571, -981, -1486, 987, 1286, 858, 190, 265, -1618, 240, 1152, -1219, 334, 171, -1198, -122, 1512, 1103, 1309, 199, 814, -1642, 12, 337, 473, 347, -3, -1643, 1446, -1930, -2372, 458, 489, -1023, -1327, -1509, 27, 1016, 477, -1277, 498, 1060, 1002, 1311, 1134, 1276, -633, 126, -1345, -531, 219, -1789, 1231, 1416, -1902, -224, 595, 1219, -1994, -1278, 623, 140, -161, -1747, -1166, 411, 1147, -1655, -1153, 608, -84, 191, 296, -1368, -80, 356, 24, -1490, -1408, -1982]

yPoints = [-768, -429, 698, 1012, -318, -28, 746, -624, -479, -166, 949, -734, 106, 1091, -846, -1186, 1145, 1350, -717, 1068, -238, -746, 84, -584, -531, 494, -556, -399, -801, -349, 772, 1047, 892, -48, -715, -192, -820, -977, -21, -821, -657, 1024, -173, -22, 229, -1237, -1136, 823, -856, 1080, 1075, 1283, -1235, -591, -737, -563, -951, -749, -839, 1511, -411, -886, -1185, 1353, 1070, 147, 910, 284, -967, -799, 1000, -305, 986, -260, 823, -202, -1026, -668, -287, 1134, -914, -10, 1782, -523, -461, -182, -78, -809, -807, 1195, -300, 1115, -304, -1199, -784, -633, 677, 305, -277, 1214, -680, -1043, -700, 748, -831, 1222, 1434, 646, 228, -96, -696, 163, 1168, -230, 251, -191, -1158, -376, 1225, 1308, -797, -1134, -947, -1062, -747, 164, 494, -728, -589, -101, 882, 608, 84, -304, -509, -554, 835, -171, 1233, -1037, 1036, 1473, -1361, 1117, 1036, -1096, 1052, -558, -838, -803, -426, 896, 675, -95, 902, -332, -807, -1133, 1288, -202, 1288, 752, 992, -324, -1344, -1220, -107, 1376, 720, -1285, -559, 1034, -549, -1027, -49, -708, 1113, 544, 1043, -1191, -438, 563, -1044, 1028, 580, -825, -73, 1118, -705, -818, -452, -292, 116, 1007, -1208, -668, -327, -15, -603, -303, -777, -538, -115, 784, 1232, -1352, 788, -789, -810, -590, -160, -733, 627, -84, 893, -608, 1002, -158, -703, -194, 14, 61, -574, 691, 1281, 955, -821, -42, 8, -526, 439, -258, -861, -756, -737, -456, 888, 923, 590, 1148, 845, -422, -212, -746, 727, 1230, -972, 52, 1142, -400, -726, -1060, -1189, -175, -939, -94, -798, 1264, -989, -547, -623, 868, 1382, 628, 1183, 966, -977, -931, -237, -1251, -902, -121, -74, 1215, -163, -754, 924, 1081, -375, -524, -1014, -428, -1163, -60, 90, -808, 778, -401, 8, 689, -976, -423, -1133, -250, -567, -1150, -635, -248, 256, -545]

xNewPoints = [400, 803, -1395, -341, 1547, 245, 1207, 1251, 1810, -1669, -71, 682, 1090, -1676, -1844, 1242, -1861, -1460, 498, 983, -1831, 1306, 353, 1139, 293, -1145, 1187, -2232, -1283, 494, 63, 935, 1580, 1063, -1397, -109, 1173, 920, 582, 952, -1372, 21, 327, -1715, 512, 1130, -1430, -1827, 1418, 1268, 804, 834, -1247, -1294, -227, 218, -1225, -1312, -1335, -301, 202, 1013, 818, 1261, 464, 360, -2150, 1050, 106, -1739, -112, 516, 326, 1090, 364, -1907, -1956, 318, 736, -1441, 321, -1373, 68, 959, -1626, 1067, -1796, -98, -105, 879, 980, -83, 848, 1329, 323, -1526, -1485, -1804, -2019, 287, 822, -35, -1669, -1782, 725, 388, 1490, 0, -1345, -803, -249, -1410, -1751, -1279, 335, 1168, 715, 1300, -1216, -1707, 1207, 186, 803, -1734, 1312, 404, -1932, 656, -1405, -1743, 543, -1973, 1539, -1483, 386, 1170, -80, -68, 713, -1850, 63, 1529, 162, -323, -1517, 888, 483, -55, -1579, 55, -2060, 1118, 487, -1655, -1443, -170, -1824, 1030, 1644, -1476, 1009, -1452, -1740, 1220, 916, -1, 893, 633, -1479, 54, -1519, 289, -9, 1134, 536, 1147, 573, 212, 526, -2018, -235, 159, 228, -1536, 1139, 1011, -142, -2231, 125, -1724, -1604, 737, 672, 1200, 75, 1287, 1065, 1058, -1126, -1101, 235, 1138, 531, 433, -4, -1763, 1364, 566, 1357, 1071, 1001, 1330, -1796, -1272, 847, -1509, -1249, -1277, -1819, 1199, -1255, -1630, 1106, 670, 369, -1263, -366, 244, -1276, 37, -2191, -293, -1657, 736, 1137, -1441, 592, 651, 594, -1879, 215, 499, -1829, 792, 954, -1353, 1055, -1943, -1396, 1289, -1595, -1419, -23, -1260, 960, 551, 370, 825, -1870, -1562, -1263, -63, 885, -143, -1839, 274, -1457, -1590, 925, 197, 288, 724, -1626, 584, 1021, 865, 398, 862, -1359, -1191, 1293, 1256, 93, -1735, 223, 401, -1422, 392, 1251, 17, -1877, -1580, -1595, -1018, 1248, -1255, -1418, 525, 555, 28]

yNewPoints = [-1265, 1282, 55, -1076, 1402, -483, 888, 1155, 965, -308, -937, 1102, 1438, -504, 45, 1020, 6, 153, 898, 1043, -163, 1076, -751, 1545, -1261, -37, 600, 230, -393, -883, -911, 866, 1037, 1027, -505, -908, 947, 1457, -900, 899, -31, -707, -555, 215, -760, 722, -342, 117, 1114, 1419, 1639, 1099, -223, 81, -413, -468, 255, -428, 652, -1251, -905, 1123, 860, 1466, -797, 844, -369, 874, -749, 252, -652, -641, -880, 1109, -694, 187, 139, -405, 1764, -57, -534, 46, -1105, 652, -562, 729, -988, -68, 1170, 1253, 1154, -924, 1093, 1137, -749, -249, -268, 187, -449, -655, 1384, -801, -138, 293, -623, -761, 795, -740, -475, -309, -1006, -70, -300, 173, 624, 1189, 926, 916, 110, -59, 1054, -958, 386, -149, 1118, -510, 220, 961, 117, -74, 1472, -227, 1360, -490, -1261, 1185, -321, -852, 1278, -503, -1412, 966, -1373, -706, 187, 762, 881, -711, -629, -847, 58, 1302, -990, -169, 190, -826, -307, 1264, 1277, 26, 1142, -255, -83, 1286, 732, -726, 841, 1009, -699, -1064, -489, -773, -1133, 971, -847, 689, 790, -607, -815, 67, -1082, -600, -1160, 84, 631, 1043, -481, 84, -1017, -694, 445, 926, 1133, 726, -983, 1180, 1007, 1129, 166, -364, -139, 1010, -881, -764, -1305, -497, 583, 1513, 670, 611, 893, 879, -35, 400, 1170, -712, -557, -599, 737, 1569, -233, 161, 739, 870, -694, -699, -1353, -659, -485, -699, -491, -589, -298, 578, 1301, 31, 1225, 492, 825, -521, -1282, -670, -339, 1337, 980, 103, 807, -118, -310, 1017, -540, -381, -1101, -693, -811, -1017, -1068, 877, 287, -189, -774, -759, 885, -1160, -326, -1045, -291, 166, 741, -780, -832, -799, -180, 1131, 459, 957, -1242, 1109, 24, -38, 1103, -779, -553, -97, -943, -1109, 181, -878, 693, -720, -263, -550, -153, 38, 660, -29, -358, 870, 1187, -699]

points = list(zip([a/1000 for a in xPoints], [b/1000 for b in yPoints]))
new_points = list(zip([a/1000 for a in xNewPoints], [b/1000 for b in yNewPoints]))

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(points)

# Determine the cluster labels of new_points: labels
labels = model.predict(new_points)

# Print cluster labels of new_points
print(labels)


# Assign the columns of new_points: xs and ys
xs = [x for x, y in new_points]
ys = [y for x, y in new_points]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker="D", s=50)
# plt.show()
plt.savefig("_dummyPy195.png", bbox_inches="tight")
plt.clf()


rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values

ks = range(1, 6)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(samples)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
# plt.show()
plt.savefig("_dummyPy196.png", bbox_inches="tight")
plt.clf()


# Create a KMeans model with 3 clusters: model
model = KMeans(n_clusters=3)

# Use fit_predict to fit model and obtain cluster labels: labels
labels = model.fit_predict(samples)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df["labels"], df["varieties"])

# Display ct
print(ct)



# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)


rawFish = pd.read_csv(myPath + "fish.csv", header=None)
samples = rawFish.values[:, 1:]
species = rawFish.values[:, 0]


# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame( {"labels":labels, "species":species} )

# Create crosstab: ct
ct = pd.crosstab(df["labels"], df["species"])

# Display ct
print(ct)


# Note that Normalizer() is different to StandardScaler(), which you used in the previous exercise.
# While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, 
# Normalizer() rescales each sample - here, each company's stock price - independently of the other.


rawStock = pd.read_csv(myPath + "company-stock-movements-2010-2015-incl.csv")
movements = rawStock.values[:, 1:]
companies = rawStock.values[:, 0]

# Import Normalizer
from sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)



# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({'labels': labels, 'companies': companies})

# Display df sorted by cluster label
print(df.sort_values("labels"))

```
  
**Example #1: k-means Clustering**:  
![](_dummyPy195.png)  

**Example #2: Elbow Plot**:  
![](_dummyPy196.png)  
  
***
  
Chapter 2 - Visualization with Hierarchical Clustering and t-SNE  
  
Visualizing hierarchies - groups within groups:  
  
* Eurovision 2016 scoring examples - average scores by country given to various songs  
* The dendrogram (tree diagram) groups the countries together in to larger and larger groups  
* General process for agglomerative hierarchical clustering  
	* Start with everything as its own cluster  
    * Merge the two closest clusters (several methods for defining "closest")  
    * Repeat until there is one gian cluster  
* Hierarchical clustering can be run within SciPy  
	* from scipy.cluster.hierarchy import linkage, dendrogram  
    * mergings = linkage(samples, method="complete")  
    * dendrogram(mergings, lables=country_names, leaf_rotation=90, leaf_font_size=6)  
  
Cluster labels in hierarchical clustering - extracting information from intermediate levels of clustering:  
  
* Intermediate clustering is defined by selecting a height on the dendrogram  
	* The y-axis on a dendrogram is the distance between merging clusters  
    * As such, the selected height on the dendrogram corresponds to a distance between clusters (can stop merging when things as this far apart)  
* Distance between clusters can be measured in many ways - called the linkage method  
	* Complete - distance is defined as the MAXIMUM distance between points in the clusters  
    * Single - distance is defined as the MINIMUM distance between points in the clusters  
* Intermediate cluster labels can be extracted using the fcluster method, which will return a NumPy array of cluster labels  
	* from scipy.cluster.hierarchy import fcluster  
    * intClusters = fcluster(mergings, 15, criterion="distance")  # maximum distance allowed is 15  
    * Can add labels by making a data frame with the values and then sorting by intClusters  
  
t-SNE for 2-dimensional maps (create a 2D map of a dataset) - "t-distributed stochastic neighbor embedding":  
  
* Maps samples to 2D (or occasionally 3D) while preserving nearness of the samples  
* Example using the 4D iris data, provided in an unlabelled manner  
	* The setosa samples end up far apart, but the virginical and versicolor samples end pretty close to each other (consistent with the "elbow" in the inertia plot being at 2 or 3)  
* Running t-SNE in Python  
	* from sklearn.manifold import TSNE  
    * model = TSNE(learning_rate=100)  # may need to try different learning rates (typically 50-200 are good choices), though a bad choice is obvious in badly bunched data  
    * transformed = model.fit_transform(samples)  # fit_transform simultaneously fits and transformes the data (cannot be extended to new data)  
    * xs = transformed[:, 0]  ; ys = transformed[:, 1]  # note that the axes have absolutely no real-world meaning whatsoever  
    * plt.scatter(xs, ys, c=species)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Perform the necessary imports
from sklearn.preprocessing import normalize
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.cluster.hierarchy import fcluster
from sklearn.manifold import TSNE



rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values


# Calculate the linkage: mergings
mergings = linkage(samples, method="complete")

# Plot the dendrogram, using varieties as labels
dendrogram(mergings,
           labels=varieties,
           leaf_rotation=90,
           leaf_font_size=6,
)
# plt.show()
plt.savefig("_dummyPy197.png", bbox_inches="tight")
plt.clf()



rawStock = pd.read_csv(myPath + "company-stock-movements-2010-2015-incl.csv")
movements = rawStock.values[:, 1:]
companies = rawStock.values[:, 0]

# Normalize the movements: normalized_movements
normalized_movements = normalize(movements)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements, method="complete")

# Plot the dendrogram
dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)
# plt.show()
plt.savefig("_dummyPy198.png", bbox_inches="tight")
plt.clf()


rawEuro = pd.read_csv(myPath + "eurovision-2016.csv")

pvtEuro = rawEuro[["From country", "To country", "Televote Rank"]].pivot_table(index="From country", columns="To country", values="Televote Rank", aggfunc=np.mean)

samples = pvtEuro.fillna(0).values
country_names = pvtEuro.index


# Calculate the linkage: mergings
mergings = linkage(samples, method="single")

# Plot the dendrogram
dendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)
# plt.show()
plt.savefig("_dummyPy199.png", bbox_inches="tight")
plt.clf()



rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values


# Calculate the linkage: mergings
mergings = linkage(samples, method="complete")


# Use fcluster to extract labels: labels
labels = fcluster(mergings, 6, criterion="distance")

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df["labels"], df["varieties"])

# Display ct
print(ct)



# Create a TSNE instance: model
model = TSNE(learning_rate=200)

# Apply fit_transform to samples: tsne_features
tsne_features = model.fit_transform(samples)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1st feature: ys
ys = tsne_features[:,1]

# Scatter plot, coloring by variety_numbers
plt.scatter(xs, ys, c=varieties)
# plt.show()
plt.savefig("_dummyPy200.png", bbox_inches="tight")
plt.clf()


# Create a TSNE instance: model
model = TSNE(learning_rate=50)

# Apply fit_transform to normalized_movements: tsne_features
tsne_features = model.fit_transform(normalized_movements)

# Select the 0th feature: xs
xs = tsne_features[:, 0]

# Select the 1th feature: ys
ys = tsne_features[:,1]

# Scatter plot
plt.scatter(xs, ys, alpha=0.5)

# Annotate the points
for x, y, company in zip(xs, ys, companies):
    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
# plt.show()
plt.savefig("_dummyPy201.png", bbox_inches="tight")
plt.clf()


```
  

**Example #1: Dendrogram (Grain Varieties)**:  
![](_dummyPy197.png)  

**Example #2: Dendrogram - Complete Linkage (Normalized Stock Price Movements)**:  
![](_dummyPy198.png)  

**Example #3: Dendrogram - Single Linkage (Eurovision Ratings)**:  
![](_dummyPy199.png)  

**Example #4: t-SNE (Grain Varieties)**:  
![](_dummyPy200.png)  

**Example #5: t-SNE (Normalized Stock Movements)**:  
![](_dummyPy201.png)  


***
  
Chapter 3 - Decorrelating Data and Dimension Reduction  
  
Visualizing PCA transformations - using dimension reduction for more efficient storage and computation:  
  
* Goal is to remove the "noisy" features which both take up space and cause problems with prediction tasks  
* PCA (Principal Component Analysis) is a fundamenal dimension reduction technique  
	* First step is "decorrelation", which does not change the information contained in the data  
    * Second step is "dimension reduction", which removes some information  
* In the initial form of decorrelation, PCA makes all samples mean 0 on every dimension, and rotates the axes so the axes are best aligned with the data  
* PCA is implemented in scikit-learn with both a fit component and a transform component - much like KMeans or StandardScaler  
	* from sklearn.decomposition import PCA  
    * model = PCA()  
    * model.fit(samples)  
    * transformed = model.transform(samples)  
* The output of the PCA implementation will have one row per observation and one column per PCA feature  
	* The columns of the new PCA-transformed matrix will have zero correlation - the data have been de-correlated  
* The "principal components" are the directions of maximum variance in the data; these are the axes chosen  
	* Available in the .components_ attribute  
  
Intrinsic dimension - "number of features required to approximate the dataset":  
  
* Example of lat/lon for a flight travelling northeast - while both lat/lon are provided, the plane is really tracking a 1-dimensional path  
* Suppose that you are looking at 3 features of the versicolor data - the 3D plot will look a lot like a plane, so the intrinsic dimension is 2D  
* With PCA, the intrinsic dimension can be calculated by identifying the number of PCA features with significant variance  
	* pca = PCA() ; pca.fit(samples)  
    * features = range(pca.n_components_)  
    * plt.bar(features, pca.explained_variance_)  
  
Dimension reduction with PCA - representing the same data with fewer features (vital to machine-learning pipelines):  
  
* General idea is to retain the high-variance features (post-PCA) and to discard the others as noisy  
	* Can specify with PCA(n_components=2)  
    * While the assumption that low variance features are unimportant does not always hold, it is very frequently a good assumption  
* Sparse arrays (often represented using a csr_matrix) are an example where discarding low-variance features could be a mistake  
	* The PCA implementation in scikit-learn does NOT support csr_matrix  
    * Instead, need to use Truncated SVD() which interacts with the user and/or Pipeline() in a very similar fashion  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Perform the necessary imports
from scipy.stats import pearsonr
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans


rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values


# Assign the 0th column of grains: width
width = samples[:, 0]

# Assign the 1st column of grains: length
length = samples[:, 1]

# Scatter plot width vs length
plt.scatter(width, length)
plt.axis('equal')
# plt.show()
plt.savefig("_dummyPy202.png", bbox_inches="tight")
plt.clf()


# Calculate the Pearson correlation
correlation, pvalue = pearsonr(width, length)

# Display the correlation
print(correlation)



# Create PCA instance: model
model = PCA()

# Apply the fit_transform method of model to grains: pca_features
pca_features = model.fit_transform(samples)

# Assign 0th column of pca_features: xs
xs = pca_features[:,0]

# Assign 1st column of pca_features: ys
ys = pca_features[:,1]

# Scatter plot xs vs ys
plt.scatter(xs, ys)
plt.axis('equal')
# plt.show()
plt.savefig("_dummyPy203.png", bbox_inches="tight")
plt.clf()


# Calculate the Pearson correlation of xs and ys
correlation, pvalue = pearsonr(xs, ys)

# Display the correlation
print(correlation)


# Make a scatter plot of the untransformed points
plt.scatter(samples[:,0], samples[:,1])

# Create a PCA instance: model
model = PCA()

# Fit model to points
model.fit(samples)

# Get the mean of the grain samples: mean
mean = model.mean_

# Get the first principal component: first_pc
first_pc = model.components_[0, :]

# Plot first_pc as an arrow, starting at mean
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)

# Keep axes on same scale
plt.axis('equal')
# plt.show()
plt.savefig("_dummyPy204.png", bbox_inches="tight")
plt.clf()


# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
# plt.show()
plt.savefig("_dummyPy205.png", bbox_inches="tight")
plt.clf()


rawFish = pd.read_csv(myPath + "fish.csv")
samples = rawFish.values[:, 1:].astype(float)

scaled_samples = StandardScaler().fit_transform(samples)

# Create a PCA model with 2 components: pca
pca = PCA(n_components=2)

# Fit the PCA instance to the scaled samples
pca.fit(scaled_samples)

# Transform the scaled samples: pca_features
pca_features = pca.transform(scaled_samples)

# Print the shape of pca_features
print(pca_features.shape)


documents = ['cats say meow', 'dogs say woof', 'dogs chase cats']


# Create a TfidfVectorizer: tfidf
tfidf = TfidfVectorizer() 

# Apply fit_transform to document: csr_mat
csr_mat = tfidf.fit_transform(documents)

# Print result of toarray() method
print(csr_mat.toarray())

# Get the words: words
words = tfidf.get_feature_names()

# Print words
print(words)


rawWiki = pd.read_csv(myPath + "wikipedia-vectors.csv", index_col=0, encoding="utf-8")
rawTitles = rawWiki.columns
articles = rawWiki.values.transpose()

# Throws an error otherwise on Ibrahimovic, which cannot recode from utf-8 to cp1252 in script
titles = [x.encode("cp1252", errors="replace").decode("cp1252") for x in rawTitles]


# Create a TruncatedSVD instance: svd
svd = TruncatedSVD(n_components=50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(svd, kmeans)


# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)


# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({'label': labels, 'article': titles})

# Display df sorted by cluster label
import sys
print(sys.stdout.encoding)
print(df.sort_values("label"))

```
  
  
**Example #1: Width vs. Length (Grains)**:  
![](_dummyPy202.png)  
  
**Example #2: First Two Principal Components (Grains)**:  
![](_dummyPy203.png)

**Example #3: Length vs. Width and Principal Components as Arrows (Grains)**:  
![](_dummyPy204.png)

**Example #4: Incremental Variance Explained by Component (Grains)**:  
![](_dummyPy205.png)
  
***
  
Chapter 4 - Discovering intepretable features  
  
Non-negative matrix factorization (NMF):  
  
* NMF is another form of dimension reduction technique, but it is an "interpretable" model - easy to explain as a result  
	* Caveat that all elements of the martix must be non-negative (though 0 is fine)  
    * NMF expresses objects as weighted sums of components  
* NMF is available in scikit-learn, using the familiar fit() / transform() methodology  
	* Must always specify the number of components, such as NMF(n_components=2)  
    * Can work with NumPy arrays or with csr_matrix objects  
    * Can be applied to something created through TFIDF - "tf" is work frequency while "idf" is down-weighting for frequent words  
* Running NMF in scikit-learn example  
	* from sklearn.decomposition import NMF  
    * model = NMF(n_components=2)  
    * model.fit(samples)  
    * nmf_features = model.transform(samples)  
    * print(model.components_)  # will have the same dimension / shape as samples  
    * print(nmf.features) # will have 2 columns, each reflecting one of the two requested by the n_components=2 argument  
* Sample reconstruction gets very close to the original values - multiple components_ by features and sum  
	* Can be run using matrix maths, though not in this course  
  
NMF learns interpretable parts - example of scientific articles with word frequencies (20,000 articles x 800 words):  
  
* Suppose that you fit an NMF with 10 components to this 20,000 x 800 raw data  
	* Can then look at the sorted list of top words by components, and draw themes from them  
    * More or less, the components applied to a word-frequency matrix will be the "topics" (groups of similar words)  
    * For images, the NMF will separate images in to cells of commonly occurring pixels  
* Grayscale images can be encoded solely by the brightness of a pixel - so, a non-negative array  
	* Can represent with 0 as black, 1 as white, and fractions in between as various shades of gray  
    * As a pre-processing step, it is often necessary to "flatten" images - make a single 1D array (vector)  
    * Each row is then an image, while each column is a pixel position of an image  
    * To "unflatten" an image, us .reshape((desiredRows, desiredColumns)), where the tuple is passed to the .reshape()  
    * Can then run plt.imshow(myReshape, cmap="grey", interpolation="nearest")  
  
Building recommender systems using NMF:  
  
* Example of recommending articles similar to the article currently being read  
	* General strategy is to apply NMF to the word-frequency array, and find articles with similar topics  
* Suppose that you have a word-frequency array called "articles"  
	* from sklearn.decomposition import NMF  
    * nmf = NMF(n_components=6)  
    * nmf_features = nmf.fit_transform(articles)  
* Strategy for comparing outputs of NMF for similar topics - "cosine similarity"  
	* Key is the general proportions, since otherwise a direct article may score higher than a wordy (weak) article which is diluted by filler  
    * The "cosine similarity" measures the angle between two lines, with higher values meaning greater similarity  
* Calculating cosine similarities on the existing features dataset  
	* from sklearn.preprocessing import normalize  
    * norm_features = normalize(nmf_features)  
    * current_article = norm_features[23, :]  # assuming current article is in index 23 of norm_features  
    * similarities = norm_features.dot(current_article)  
* Alternately, can run this using pandas DataFrame  
	* df = pd.DataFrame(norm_features, index=titles)  
    * current_article = df.loc["Dog bites man"]  # assuming that is the title of the current article  
    * similarities = df.dot(current_article)  
    * similarities.nlargest()  # will print the 5 largest similarities by default; will include self as 1.00000  
  
Final thoughts:  
  
* scikit-learn and SciPy for unsupervised learning  
* Clustering and dimension reduction techniques  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# Import modules
from sklearn.decomposition import NMF
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.preprocessing import Normalizer, MaxAbsScaler
from sklearn.pipeline import make_pipeline



rawWiki = pd.read_csv(myPath + "wikipedia-vectors.csv", index_col=0, encoding="utf-8")
rawTitles = rawWiki.columns
articles = rawWiki.values.transpose()
titles = [x.encode("cp1252", errors="replace").decode("cp1252") for x in rawTitles]


# Create an NMF instance: model
model = NMF(n_components=6)

# Fit the model to articles
model.fit(articles)

# Transform the articles: nmf_features
nmf_features = model.transform(articles)

# Print the NMF features
print(nmf_features)


# Create a pandas DataFrame: df
df = pd.DataFrame(nmf_features, index=titles)

# Print the row for 'Anne Hathaway'
print(df.loc["Anne Hathaway", :])

# Print the row for 'Denzel Washington'
print(df.loc["Denzel Washington", :])


words = []

for word in open (myPath + "wikipedia-vocabulary-utf8.txt", "r"):
    words.append(word.strip())


# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=words)

# Print the shape of the DataFrame
print(components_df.shape)

# Select row 3: component
component = components_df.iloc[3, :]

# Print result of nlargest
print(component.nlargest())


rawDigits = pd.read_csv(myPath + "lcd-digits.csv", header=None)
samples = rawDigits.values


# Select the 0th row: digit
digit = samples[0, :]

# Print digit
print(digit)

# Reshape digit to a 13x8 array: bitmap
bitmap = digit.reshape(13, 8)

# Print bitmap
print(bitmap)

# Use plt.imshow to display bitmap
plt.imshow(bitmap, cmap='gray', interpolation='nearest')
plt.colorbar()
# plt.show()
plt.savefig("_dummyPy206.png", bbox_inches="tight")
plt.clf()


def show_as_image(sample, useSub):
    bitmap = sample.reshape((13, 8))
    # plt.figure()
    plt.subplot(4, 2, useSub)
    plt.imshow(bitmap, cmap='gray', interpolation='nearest')
    


# Create an NMF model: model
model = NMF(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
curSub = 1
for component in model.components_:
    show_as_image(component, useSub=curSub)
    curSub += 1

# plt.colorbar()
plt.tight_layout()
plt.savefig("_dummyPy207.png", bbox_inches="tight")
plt.clf()


# Assign the 0th row of features: digit_features
digit_features = features[0, :]

# Print digit_features
print(digit_features)


# Create a PCA instance: model
model = PCA(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
curSub = 1
for component in model.components_:
    show_as_image(component, useSub=curSub)
    curSub += 1


# plt.colorbar()
plt.tight_layout()
plt.savefig("_dummyPy208.png", bbox_inches="tight")
plt.clf()

# Normalize the NMF features: norm_features
norm_features = normalize(nmf_features)

# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=titles)

# Select the row corresponding to 'Cristiano Ronaldo': article
article = df.loc['Cristiano Ronaldo']

# Compute the dot products: similarities
similarities = df.dot(article)

# Display those with the largest cosine similarity
print(similarities.nlargest())



# Create a MaxAbsScaler: scaler
scaler = MaxAbsScaler()

# Create an NMF model: nmf
nmf = NMF(n_components=20)

# Create a Normalizer: normalizer
normalizer = Normalizer()

# Create a pipeline: pipeline
pipeline = make_pipeline(scaler, nmf, normalizer)



artist_names = [x for x in pd.read_csv(myPath + "artists.csv", header=None).iloc[:, 0]]
rawSparse = pd.read_csv(myPath + "scrobbler-small-sample.csv")

artists = np.zeros(shape=(111, 500))
for ctr in range(rawSparse.shape[0]):
    artists[rawSparse["artist_offset"][ctr], rawSparse["user_offset"][ctr]] = rawSparse["playcount"][ctr]



# Apply fit_transform to artists: norm_features
norm_features = pipeline.fit_transform(artists)


# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=artist_names)

# Select row of 'Bruce Springsteen': artist
artist = df.loc['Bruce Springsteen']

# Compute cosine similarities: similarities
similarities = df.dot(artist)

# Display those with highest cosine similarity
print(similarities.nlargest())


```
  
**Example #1: Example LCD Digit**:  
![](_dummyPy206.png)  
  
**Example #2: NMF Components for LCD Digits**:  
![](_dummyPy207.png)  

**Example #3: PCA Components for LCD Digits**:  
![](_dummyPy208.png)  
  
###_Deep Learning in Python_#

Chapter 1 - Basics of Deep Learning and Neural Networks  
  
Introduction to deep learning - contrast to the "sum of parts" approach as with linear regression:  
  
* Distinction between model with no interactions (straight-sum) and model with interactions (slopes and intercepts may change with covariates)  
* Neural networks are a model that account for interactions very well  
* Deep learning uses especially powerful neural networks to decipher areas like text, images, audio, video, source code, and the like  
* First two chapters of this course focus on conceptual knowledge about deep learning  
	* Third and fourth chapters are more focused on examples and using the models  
* General approach to neural networks includes the layers  
	* Input layer - features  
    * Output layer - prediction (continuous or categorical)  
    * Hidden layer - all layers between the inputs and the outputs (neither observed from the world nor the predicted output)  
    * More nodes in the hidden layers mean more capturing of interactions  
  
Forward propagation - bank transactions example where # transactions ~ # children, # existing accounts:  
  
* Forward propagation takes a customer and passes them as an input layer to the neural network, taking the prediction from the output layer  
	* Basically, each hidden node is some weighted combination of the nodes (which may be the input layer) in the previous layer  
    * Always a multiply-add process, implementable as the dot-product from linear algebra  
* Example process in numpy, assuming the input layer is available as an array  
	* input_data = np.array([2, 3])  
    * weights = { "node0": np.array([1, 1]), "node1": np.array([-1, 1]), "output": np.array([2, -1]) }  
    * node_0_value = (input_data * weights["node_0"]).sum()  
    * node_1_value = (input_data * weights["node_1"]).sum()  
    * hidden_layer_values = np.array([node_0_value, node_1_value])  
    * output = (hidden_layer_values * weight["output"]).sum()  
  
Activation functions - the core of the neural network hidden layers, allowing it to capture non-linearity:  
  
* An activation function is applied to the node inputs to produce the node outputs  
* For a long time, tanh() was a popular activation function, so if the weights summed to 5, then the node would produce tanh(5)  
* Current standard is ReLU or Rectified Linear Activation  
	* Two linear components, but it is surprisingly successful at making predictions  
    * Basically ReLU is 0 for x < 0 and x for x >= 0  
* With an activation function, a node can be considered to have three components  
	* The raw inputs coming to the node  
    * The weighted sum of the nodes (input to the activation function of the node)  
    * The activation function output (what will be passed to the nodes in the next layer)  
  
Deeper networks - many hidden layers are much of what makes modern neural networks so powerful:  
  
* State of the art used to be 10-15 hidden layers, but it is now common to have as much as thousands of hidden layers  
* Over time, deep networks internally build "representations" of the patterns in the data  
	* This (at least partially) replaces the need for feature engineering, since the layers manage that  
    * Subsequent layers build increasingly sophisticated representations of raw data  
* The advantage of deep learning is that the modeler does not need to pre-specify the interactions; they are discovered in the training process  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt




input_data = np.array([3, 5])
weights = { "node_0": np.array([2, 4]), "node_1": np.array([4, -5]), "output": np.array([2, 7]) }

# Calculate node 0 value: node_0_value
node_0_value = (input_data * weights["node_0"]).sum()

# Calculate node 1 value: node_1_value
node_1_value = (input_data * weights["node_1"]).sum()

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_value, node_1_value])

# Calculate output: output
output = (hidden_layer_outputs * weights["output"]).sum()

# Print output
print(output)


def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    output = max(0, input)
    
    # Return the value just calculated
    return(output)

# Calculate node 0 value: node_0_output
node_0_input = (input_data * weights['node_0']).sum()
node_0_output = relu(node_0_input)

# Calculate node 1 value: node_1_output
node_1_input = (input_data * weights['node_1']).sum()
node_1_output = relu(node_1_input)

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_output, node_1_output])

# Calculate model output (do not apply relu)
model_output = (hidden_layer_outputs * weights['output']).sum()

# Print model output
print(model_output)


# Define predict_with_network()
def predict_with_network(input_data_row, weights):
    # Calculate node 0 value
    node_0_input = (input_data_row * weights['node_0']).sum()
    node_0_output = relu(node_0_input)
    
    # Calculate node 1 value
    node_1_input = (input_data_row * weights['node_1']).sum()
    node_1_output = relu(node_1_input)
    
    # Put node values into array: hidden_layer_outputs
    hidden_layer_outputs = np.array([node_0_output, node_1_output])
    
    # Calculate model output
    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()
    model_output = relu(input_to_final_layer)
    
    # Return model output
    return(model_output)

# Create empty list to store prediction results
results = []
for input_data_row in input_data:
    # Append prediction to results
    results.append(predict_with_network(input_data_row, weights))

# Print results
print(results)


def predict_with_network(input_data):
    # Calculate node 0 in the first hidden layer
    node_0_0_input = (input_data * weights["node_0_0"]).sum()
    node_0_0_output = relu(node_0_0_input)
    
    # Calculate node 1 in the first hidden layer
    node_0_1_input = (input_data * weights["node_0_1"]).sum()
    node_0_1_output = relu(node_0_1_input)
    
    # Put node values into array: hidden_0_outputs
    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])
    
    # Calculate node 0 in the second hidden layer
    node_1_0_input = (hidden_0_outputs * weights["node_1_0"]).sum()
    node_1_0_output = relu(node_1_0_input)
    
    # Calculate node 1 in the second hidden layer
    node_1_1_input = (hidden_0_outputs * weights["node_1_1"]).sum()
    node_1_1_output = relu(node_1_1_input)
    
    # Put node values into array: hidden_1_outputs
    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])
    
    # Calculate model output: model_output
    model_output = (hidden_1_outputs * weights["output"]).sum()
    
    # Return model_output
    return(model_output)

weights = { 'node_0_0': np.array([2, 4]), 
            'node_1_1': np.array([2, 2]), 
            'node_0_1': np.array([ 4, -5]), 
            'node_1_0': np.array([-1,  1]), 
            'output': np.array([2, 7])
           }

output = predict_with_network(input_data)
print(output)

```
  
  
***
  
Chapter 2 - Optimizing with Backward Propagation

Need for optimization - goal is to assess how close prediction are to actual values (supervised), and optimize to minimize deltas:  
  
* Loss functions aggregate prediction errors for evey observation in to a single, aggregate number  
* Goal is to find weights that lead to the lowest value for the loss function  
* Gradient descent is a common method for searching the parameter space for this minima  
	* Start at a random point  
    * Find the slope  
    * Take a step downhill  
    * Repeat until all slope is flat/uphill  
  
Gradient descent - continuous "downhill" process to find a (local or global) minima:  
  
* Slope is always multiplied by a small number called the "learning rate" - avoid stepping too far and missing the target - rate is typically on the order of 0.001  
* Calculating the slope for a given weight is a three step process  
	* Slope of the loss function w.r.t. value at the node we feed in to  
    * Value of the node that feeds in to our weight  
    * Slope of the activation function w.r.t. value we feed into  
* Consider a very simple example which just has input node 3, weight 2, activation function identity, output 6 (3*2) and target value 10  
	* Slope of loss function w.r.t value at node we feed in to: 2 * (Predicted - Actual) = 2 * Error = -8  
    * Value of the node that feeds in to our weight: 3  
    * Slope of the activation function w.r.t. value we feed into: 1  
    * So, the overall slope is -8 * 3 * 1 = -24  
    * This slope would then be multiplied by the learning rate, with the weight (2) REDUCED by learning rate * -24 for the next run  
* With multiple points, the array of slopes is called the gradient; thus, gradient descent  
  
Back-propagation - sending the error backwards through the hidden layers so that other gradients can be calculated (weights modified):  
  
* Comes from the chain rule in calculus; important to understand but als tricky; well-implemented by various neural net libraries  
* Process includes back-propagation to estimate the slopes with each weight, updates by the learning rate, then forward-propagation to find the new errors  
	* With new weights and errors, there will be new slopes; repeat as needed  
    * Need to keep track of the slopes of the loss function w.r.t. node values  
  
Back-propagation in practice:  
  
* A node in the hidden layer "takes on" the value of the gradient for that node that was just calculated in the previous step for use in modifying weights coming to that node in the hidden layer  
* Back propagation recap  
	* Start with a random set of weights  
    * Use forward proagation to make a prediction  
    * Use back proagation to calculate the slope of the loss function w.r.t. each weight  
    * Multiply that slope by the learning rate, and subtract from the current weights  
* Stochastic gradient descent - calculating slopes on "batches" rather than on the full data all at once  
	* Common to calculate the slopes on only a subset ("batch") of the data at any particular time  
    * Use a different batch of data to calculate the next update  
    * Start over once all of the data has been used  
    * Each full path through the training data is called the epoch  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    output = max(0, input)
    
    # Return the value just calculated
    return(output)


# Modify predict_with_network() from previous chapters
def predict_with_network(input_data, weights):
    # Calculate node 0 in the first hidden layer
    node_0_input = (input_data * weights["node_0"]).sum()
    node_0_output = relu(node_0_input)
    
    # Calculate node 1 in the first hidden layer
    node_1_input = (input_data * weights["node_1"]).sum()
    node_1_output = relu(node_1_input)
    
    # Put node values into array: hidden_0_outputs
    hidden_0_outputs = np.array([node_0_output, node_1_output])

    # Calculate model output: model_output
    model_output = (hidden_0_outputs * weights["output"]).sum()
    
    # Return model_output
    return(model_output)



# The data point you will make a prediction for
input_data = np.array([0, 3])

# Sample weights
weights_0 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 1]
            }

# The actual target value, used to calculate the error
target_actual = 3

# Make prediction using original weights
model_output_0 = predict_with_network(input_data, weights_0)

# Calculate error: error_0
error_0 = model_output_0 - target_actual

# Create weights that cause the network to make perfect prediction (3): weights_1
weights_1 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 0]
            }

# Make prediction using new weights: model_output_1
model_output_1 = predict_with_network(input_data, weights_1)

# Calculate error: error_1
error_1 = model_output_1 - target_actual

# Print error_0 and error_1
print(error_0)
print(error_1)


from sklearn.metrics import mean_squared_error

# Create model_output_0 
model_output_0 = []
# Create model_output_0
model_output_1 = []


input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]

# Loop over input_data
for row in input_data:
    # Append prediction to model_output_0
    model_output_0.append(predict_with_network(row, weights_0))
    
    # Append prediction to model_output_1
    model_output_1.append(predict_with_network(row, weights_1))


target_actuals = [1, 3, 5, 7]

# Calculate the mean squared error for model_output_0: mse_0
mse_0 = mean_squared_error(target_actuals, model_output_0)

# Calculate the mean squared error for model_output_1: mse_1
mse_1 = mean_squared_error(target_actuals, model_output_1)

# Print mse_0 and mse_1
print("Mean squared error with weights_0: %f" %mse_0)
print("Mean squared error with weights_1: %f" %mse_1)


weights = np.array([0, 2, 1])
input_data = np.array([1, 2, 3])
target = 0

# Calculate the predictions: preds
preds = (weights * input_data).sum()

# Calculate the error: error
error = preds - target

# Calculate the slope: slope
slope = 2 * input_data * error

# Print the slope
print(slope)


# Set the learning rate: learning_rate
learning_rate = 0.01

# Calculate the predictions: preds
preds = (weights * input_data).sum()

# Calculate the error: error
error = preds - target

# Calculate the slope: slope
slope = 2 * input_data * error

# Update the weights: weights_updated
weights_updated = weights - learning_rate * slope

# Get updated predictions: preds_updated
preds_updated = (weights_updated * input_data).sum()

# Calculate updated error: error_updated
error_updated = preds_updated - target

# Print the original error
print(error)

# Print the updated error
print(error_updated)


# Need functions get_slope() and get_mse()
def get_slope(myData, myTarget, myWeight):
    preds = (myWeight * myData).sum()
    error = preds - myTarget
    slope = 2 * myData * error
    return(slope)

def get_mse(myData, myTarget, myWeight):
    preds = (myWeight * myData).sum()
    mse = (preds - myTarget) ** 2
    return(mse)

weights = np.array([0, 2, 1])
input_data = np.array([1, 2, 3])
target = 0

n_updates = 20
mse_hist = []

# Iterate over the number of updates
for i in range(n_updates):
    # Calculate the slope: slope
    slope = get_slope(input_data, target, weights)
    
    # Update the weights: weights
    weights = weights - 0.01 * slope
    
    # Calculate mse with new weights: mse
    mse = get_mse(input_data, target, weights)
    
    # Append the mse to mse_hist
    mse_hist.append(mse)

# Plot the mse history
plt.plot(mse_hist)
plt.xlabel('Iterations')
plt.ylabel('Mean Squared Error')
# plt.show()
plt.savefig("_dummyPy209.png", bbox_inches="tight")
plt.clf()


```
  
**Example #1: Gradient Descent MSE by Iteration**:  
![](_dummyPy209.png)  
  
***
  
Chapter 3 - Building Deep-Learning Models  
  
Creating a keras model - basic four-step process:  
  
* Four main steps in building a deep-learning model  
	* Specify Architecture - number of layers, number of nodes per layer, activation function, etc.  
    * Compile - specify the loss function and some details about optimization  
    * Fit - back-propagation and creation of model weights  
    * Predict - use the model to make predictions  
* Specify Architecture using keras (suppose that a Numpy array "predictors" already exists)  
	* from keras.layers import Dense ; from keras.models import Sequential  
    * n_cols = predictiors.shape[1]  
    * model = Sequential()  # one of two ways to build a model (generally the easier of the two) - Sequential means nodes are only connected to the next layer  
    * model.add(Dense(100, activation="relu", input_shape = (n_cols, )))  # Dense means all nodes in the previous layer connect to all nodes in the next layer (input_shape must be specified for the first layer)  
    * model.add(Dense(100, activation="relu"))  
    * model.add(Dense(1))  # requests a single output node  
  
Compiling and fitting a model - set up for optimization:  
  
* Specify the optimizer - "Adam" is typically an excellent first choice  
	* The optimizer controls the learning rate, which can play a big role in both convergence time and result quality  
    * There are many options that are extremely mathematically complex; best choice is often an optimizer that is flexible and forgiving  
    * "Adam" is usually a good choice - adjusts the learning rate as it performs gradient descent  
* Specify the loss functions - while MSE is common for regression, it is less not the only option for deep learning  
* Code for compiling a model looks like  
	* model.compile(optimizer="adam", loss="mean_squared_error")  
* Fitting the model involves running the back-propagation and gradient descent to update the weights  
	* Frequently a good practice to pre-process the data so that each feature is roughly the same magnitude, such as normalizing (technically not required, just helpful)  
    * model.fit(predictors, target)  
  
Classification models - additional specifications:  
  
* The loss function is frequently updated to "categorical_crossentropy" (somewhat similar to log-loss; lower scores are better)  
	* Can add the call metrics=["accuracy"] to get a print-out of the accuracy after each stage, which is more interpretable  
* The output layer needs to be updated to include a node for every layer, and the activation function is "softmax" (which ensures that all probabilities add to 1)  
* Generally, keras works best when the output categorical variables are aplit in to one column per category (so if there is A, B or C, then have three 1/0 columns reflecting which of these it is)  
	* This is true even for make/miss data which might be encoded as 0/1 already - goal is that the target/output columns always sum to 1  
* General process, assuming a Numpy array predictors and a DataFrame "data" with column "shot_result" already exist, and that n_cols for predictors has been calculated  
	* from keras.utils import to_categorical  
    * target = to_categorical(data.shot_result)   # FYI, converting a DataFrame, minus a column, to NumPy is  newNP = oldPD.drop("badCol", axis=1).as_matrix()  
    * model = Sequential()  
    * model.add(Dense(100, activation="relu", input_shape=(n_cols, )))  
    * model.add(Dense(100, activation="relu"))  
    * model.add(Dense(100, activation="relu"))  
    * model.add(Dense(2, activation="softmax"))  
    * model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])  
    * model.fit(predictors, target)  
  
Using models - process includes Save, Reload, and Predict:  
  
* The general code for the process includes  
	* from keras.models import load_model  
    * model.save("model_file.h5")  
    * my_model = load_model("model_file.h5")  
    * predictions = my_model.predict(predictData)  
    * probability_true = predictions[:, 1]  
* Can verify model architecure using my_model.summary()  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# Import necessary modules
# Not currently available?
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.utils import to_categorical



rawWage = pd.read_csv(myPath + "hourly_wages.csv")
target = rawWage["wage_per_hour"].values
predictors = rawWage.drop("wage_per_hour", axis=1).values

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]

# Set up the model: model
model = Sequential()

# Add the first layer
model.add(Dense(50, activation="relu", input_shape=(n_cols, )))

# Add the second layer
model.add(Dense(32, activation="relu"))

# Add the output layer
model.add(Dense(1))


# Compile the model
model.compile(optimizer="adam", loss="mean_squared_error")

# Verify that model contains information from compiling
print("Loss function: " + model.loss)


# Fit the model
model.fit(predictors, target)


# Using the titanic data
rawTitanic = pd.read_csv(myPath + "titanic_all_numeric.csv")

# Convert the target to categorical: target
target = to_categorical(rawTitanic["survived"])
predictors = rawTitanic.drop("survived", axis=1).values
n_cols = predictors.shape[1]

# Set up the model
model = Sequential()

# Add the first layer
model.add(Dense(32, activation="relu", input_shape=(n_cols, )))

# Add the output layer
model.add(Dense(2, activation="softmax"))

# Compile the model
model.compile(optimizer="sgd", loss="categorical_crossentropy", metrics=['accuracy'])

# Fit the model
model.fit(predictors, target)


pred_data = np.array([[2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1], [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1], [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1], [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0], [3, 27.0, 0, 0, 6.975, 1, False, 0, 0, 1], [3, 31.0, 0, 0, 7.775, 1, False, 0, 0, 1], [1, 39.0, 0, 0, 0.0, 1, False, 0, 0, 1], [3, 18.0, 0, 0, 7.775, 0, False, 0, 0, 1], [2, 39.0, 0, 0, 13.0, 1, False, 0, 0, 1], [1, 33.0, 1, 0, 53.1, 0, False, 0, 0, 1], [3, 26.0, 0, 0, 7.8875, 1, False, 0, 0, 1], [3, 39.0, 0, 0, 24.15, 1, False, 0, 0, 1], [2, 25.0, 0, 1, 26.0, 0, False, 0, 0, 1], [3, 33.0, 0, 0, 7.8958, 1, False, 0, 0, 1], [3, 22.0, 0, 0, 10.5167, 0, False, 0, 0, 1],  [2, 28.0, 0, 0, 10.5, 1, False, 0, 0, 1], [3, 25.0, 0, 0, 7.05, 1, False, 0, 0, 1], [3, 39.0, 0, 5, 29.125, 0, False, 0, 1, 0], [2, 27.0, 0, 0, 13.0, 1, False, 0, 0, 1], [1, 19.0, 0, 0, 30.0, 0, False, 0, 0, 1], [3, 29.69911764705882, 1, 2, 23.45, 0, True, 0, 0, 1], [1, 26.0, 0, 0, 30.0, 1, False, 1, 0, 0], [3, 32.0, 0, 0, 7.75, 1, False, 0, 1, 0]])



# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:, 1]

# print predicted_prob_true
print(predicted_prob_true)

```
  
  
  
***
  
Chapter 4 - Fine-tuning keras Models  
  
Understanding model optimization - a difficult process since there are 1000s of weights, all interacting with each other:  
  
* If the learning rate is too small, the next epoch may be almost no better than the current epoch  
* If the learning rate is too large, the nex epoch may even be much worse  
* One common approach for testing learning rates is to use stochastic gradident descent ("sgd") with multiple values for lr  
	* my_optimizer = SGD(lr=myRate)  
    * model.compile(optimizer = my_optimizer, loss="categorical_crossentropy")  
    * model.fit(predictors, target)  
* There is also a "dying neuron" problem with the "relu" activation function - if a node only gets negative inputs, it may continue only getting negative inputs  
	* The use of tanh() instead produces the "vanishing gradient" problem, where data outside the big slope areas multiplies together to "almost zero" in all of back-propagation  
  
Model validation - extension of the hold-out process specifically for deep learning:  
  
* Typically use a validation split for deep learning rather than cross-validation  
	* Since the datasets tend to be very large, k-fold cross-validation is very computationally expensive  
    * But, since the data are very large, a single validation score based on the large data tends to be more reliable  
    * Can be implemented in keras as model.fit(predictors, target, validation_split=0.3)  
* Basically, the goal is to continue training the model until the validation accuracy is no longer improving (Early Stopping)  
	* from keras.callbacks import EarlyStopping  
    * early_stopping_monitor = EarlyStopping(patience=2)  # typically see 2 or 3 - how many models with no improvement will generate an immediate stop?  
    * model.fit(predictors, target, validation_split=0.3, epochs=20, callbacks=[early_stopping_monitor])  
  
Thinking about model capacity - what architecures will work best for a specific problem and dataset:  
  
* Model capacity is closely related to the terms over-fitting and under-fitting  
* As layers or nodes in a layer are increased, model capacity gets higher and there is a greater likelihood of the variance problem  
* A general workflow for managing the capacity of a model includes  
	* Start with a small network  
    * Get the validation score  
    * Keep increasing capacity until the validation score is no longer improving  
  
Stepping up to images - handwritten images dataset from MNIST:  
  
* Each image is 28x28 (can be flattened to 784 values) of greyscale data  
* Goal of modelling is to predict the digit based on the flattened 784 values  
  
Final thoughts - like riding a bicycle, the hardest part is getting to where you can practice on your own:  
  
* Start with standard prediction problems on tables of numbers  
* Images (with convolutional neural networks) are common next steps  
* Kaggle is a great source for datasets and discussion forums  
* Wikipedia has a page on datasets for machine learning  
* keras.io is also an excellent source of information  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# Import modules
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.optimizers import SGD
from keras.utils import to_categorical



# Using the titanic data
rawTitanic = pd.read_csv(myPath + "titanic_all_numeric.csv")

# Convert the target to categorical: target
target = to_categorical(rawTitanic["survived"])
predictors = rawTitanic.drop("survived", axis=1).values
n_cols = predictors.shape[1]


def get_new_model():
    model = Sequential()
    model.add(Dense(32, activation="relu", input_shape=(n_cols, )))
    model.add(Dense(2, activation="softmax"))
    return(model)


# Create list of learning rates: lr_to_test
lr_to_test = [0.000001, 0.01, 1]

# Loop over learning rates
for lr in lr_to_test:
    print('\n\nTesting model with learning rate: %f\n'%lr )
    model = get_new_model()  # Build new model from scratch
    my_optimizer = SGD(lr=lr)  # Create SGD optimizer with specified learning rate: my_optimizer
    model.compile(optimizer=my_optimizer, loss="categorical_crossentropy") # Compile the model
    model.fit(predictors, target)  # Fit the model


# Specify the model
model = Sequential()
model.add(Dense(100, activation='relu', input_shape = (n_cols, )))
model.add(Dense(100, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=['accuracy'])

# Fit the model
hist = model.fit(predictors, target, validation_split=0.3)


# Import EarlyStopping
from keras.callbacks import EarlyStopping

# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience=2)

# Fit the model
model.fit(predictors, target, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])


# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience=2)

# Create the new model: model_1
model_1 = Sequential()
model_1.add(Dense(10, activation="relu", input_shape=(n_cols,)))
model_1.add(Dense(10, activation="relu"))
model_1.add(Dense(2, activation="softmax"))


# Create the new model: model_2
model_2 = Sequential()
model_2.add(Dense(100, activation="relu", input_shape=(n_cols,)))
model_2.add(Dense(100, activation="relu"))
model_2.add(Dense(2, activation="softmax"))


# Compile model_1 and model_2 using the same parameters
model_1.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model_2.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])


# Fit model_1
model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)

# Fit model_2
model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)


# Create the plot
plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')
plt.xlabel('Epochs')
plt.ylabel('Validation score')
plt.savefig("_dummyPy210.png", bbox_inches="tight")
# plt.show()
plt.clf()


# The input shape to use in the first hidden layer
input_shape = (n_cols,)


# Create the baseline model: model_1
model_1 = Sequential()
model_1.add(Dense(50, activation="relu", input_shape=input_shape))
model_1.add(Dense(2, activation="softmax"))


# Create the new model: model_2
model_2 = Sequential()
model_2.add(Dense(50, activation="relu", input_shape=input_shape))
model_2.add(Dense(50, activation="relu"))
model_2.add(Dense(50, activation="relu"))
model_2.add(Dense(2, activation="softmax"))


# Compile model_1 and model_2
model_1.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model_2.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])


# Fit model 1
model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)

# Fit model 2
model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)

# Create the plot
plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')
plt.xlabel('Epochs')
plt.ylabel('Validation score')
plt.savefig("_dummyPy211.png", bbox_inches="tight")
# plt.show()
plt.clf()


# See post from Dan re using AWS for https://www.datacamp.com/community/tutorials/deep-learning-jupyter-aws#gs.peq1yyU



# Using the MNIST data (2500 samples)
rawMNIST = pd.read_csv(myPath + "mnist.csv", header=None)
y = to_categorical(rawMNIST.iloc[1:, 0])  # Eliminate first row
X = rawMNIST.iloc[1:, 1:].values / 255  # Eliminate first row, normalize to (0, 1) scale
n_cols = X.shape[1]


# Create the model: model
model = Sequential()
model.add(Dense(50, activation="relu", input_shape=(n_cols, )))
model.add(Dense(50, activation="relu"))
model.add(Dense(10, activation="softmax"))

# Compile the model
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Fit the model
model.fit(X, y, validation_split=0.3)

```
  
  
**Example #1: Loss vs. Epochs for Varying Model Complexity (10-10-2 vs. 100-100-2)**:  
![](_dummyPy210.png)  

**Example #2: Loss vs. Epochs for Varying Model Complexity (50-2 vs. 50-50-50-2)**:  
![](_dummyPy211.png)  
  
  
###_Machine Learning: School Budgets_#

Chapter 1 - Exploring Raw Data  
  
Introducing the challenge - from DrivenData, an organization that seeks to leverage Data Science for "social impact":  
  
* The DrivenData challenge in this course incorporates NLP, feature engineering, and hashing tricks for efficiency  
* School budgets are complex and non-standardized, and there is benefit to benchmarking one school's spend against another  
	* The objective is to build a machine-learning algorithm that can automate this process  
    * For example, convert "Algebra books for 8th grade students" to "Textbooks" / "Math" / "Middle School"  
    * Supervised learning problem - have samples of data where the conversions have already occurred  
* Classification problem with 100+ target variables  
	* May be related to Pre-K status (Yes, No, Unknown) or Student Type (Alternative, At-Risk, etc.)  
* Humans disagree on many of the classifications, so the goal is for the algorithm to output percentage likelihoods and not hard/fast classifications  
	* Also known as a "human in the loop" machine learning algorithm - percentages help humans optimize where to focus their efforts  
  
Exploring the data - one goal is to have a "target" column for every possible value of every categorical output:  
  
* Load and preview a small sample of the data - exploration  
	* The data for this exercise will import a CSV file  
    * Exploration through .head(), .info(), .describe() and the like  
  
Looking at data types - encoding labels as categories:  
  
* Two benefits to encoding labels as categories  
	* Machine learning algorithms work on numbers and not strings, so a numeric representation is needed anyway  
    * Strings tend to be slow relative to numbers, since string length is indeterminate  
* Pandas has a special dtype "category" which encodes categorical data numerically  
	* sample_df["label"] = sample_df["label"].astype("category")  # will convert from string to category  
* Can access the dummy variables associated with a categorical column  
	* dummies = pd.get_dummies(sample[["label"]], prefix_sep="_")  
    * dummies.head()  # will have created columns label_a, label_b, etc. (assuming the original object data was strings "a", "b", etc.)  
    * The "label" in label_a is due to the original column name being "labe", while the "a" is due to the original value being "a" and the "_" is as per prefix_sep="_"  
* Lambda functions allow for creating very quick and small functions without resorting to def:  
	* square = lambda x: x*x  
    * categorize_label = lambda x: x.astype("category")  
    * sample_df["label"] = sample_df[["label"]].apply(categorize_label, axis=0)  # will convert from string to category  
  
Measuring success - one of the most importan decisions in algorithm design:  
  
* Accuracy can often be misleading, especially when categories are highly imbalanced  
* An improvement can be to use a log-loss function, where larger numbers are worse and respect more (or more confident) errors  
* Log-loss considers the actual value (1 or 0) and predicted probability, p, that the number is actually 1  
	* Penalty for a given prediction is -log(p) if the actual number is 1, and -log(1-p) if the actual number is 0; perfect predictions get 0 errors and perfectly wrong predictions get infinte errors  
    * The aggregate log loss is the average of the log-loss for each of the individual data points  
    * Better to be less confident (p ~ 0.50) than to be confident and wrong (p ~ 0.90 for something that is actually 0)  
* The log-loss can be implemented in numpy, taking advantage of the .clip() function  
	* eps = 1e-14  
    * predicted = np.clip(predicted, eps, 1-eps)  # ensures that all predictions are in the range of 0-1, and forced to be at least eps away from perfect certainty  
    * loss = -1 * bp.mean( actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted) )  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe"), eval=FALSE}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


colNames = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type',
            'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status',
            'Object_Description', 'Text_2', 'SubFund_Description',
            'Job_Title_Description', 'Text_3', 'Text_4', 'Sub_Object_Description',
            'Location_Description', 'FTE', 'Function_Description',
            'Facility_or_Department', 'Position_Extra', 'Total',
            'Program_Description', 'Fund_Description', 'Text_1']




# Need to get this CSV!
df = pd.read_csv("TrainingData.csv", index_col=0)


# Print the summary statistics
print(df.describe())


# Create the histogram
plt.hist(df["FTE"].dropna())

# Add title and labels
plt.title('Distribution of %full-time \n employee works')
plt.xlabel('% of full-time')
plt.ylabel('num employees')

# Display the histogram
plt.show()


LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']

# Define the lambda function: categorize_label
categorize_label = lambda x: x.astype("category")

# Convert df[LABELS] to a categorical type
df[LABELS] = df[LABELS].apply(categorize_label, axis=0)

# Print the converted dtypes
print(df[LABELS].dtypes)


# Calculate number of unique values for each label: num_unique_labels
num_unique_labels = df[LABELS].apply(pd.Series.nunique)

# Plot number of unique values for each label
num_unique_labels.plot(kind="bar")

# Label the axes
plt.xlabel('Labels')
plt.ylabel('Number of unique values')

# Display the plot
plt.show()


actual_labels = np.array([ 1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.])
correct_confident = np.array([ 0.95,  0.95,  0.95,  0.95,  0.95,  0.05,  0.05,  0.05,  0.05,  0.05])
correct_not_confident = np.array([ 0.65,  0.65,  0.65,  0.65,  0.65,  0.35,  0.35,  0.35,  0.35,  0.35])
wrong_not_confident = np.array([ 0.65,  0.65,  0.65,  0.65,  0.65,  0.35,  0.35,  0.35,  0.35,  0.35])
wrong_confident = np.array([ 0.05,  0.05,  0.05,  0.05,  0.05,  0.95,  0.95,  0.95,  0.95,  0.95])

# need to write function log_loss()
def log_loss(pred, actual, eps=1e-14):
    predicted = np.clip(pred, eps, 1-eps)  # ensures that all predictions are in the range of 0-1, and forced to be at least eps away from perfect certainty 
    loss = -1 * bp.mean( actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted) )
    return loss

# Compute and print log loss for 1st case
correct_confident = compute_log_loss(correct_confident, actual_labels)
print("Log loss, correct and confident: {}".format(correct_confident)) 

# Compute log loss for 2nd case
correct_not_confident = compute_log_loss(correct_not_confident, actual_labels)
print("Log loss, correct and not confident: {}".format(correct_not_confident)) 

# Compute and print log loss for 3rd case
wrong_not_confident = compute_log_loss(wrong_not_confident, actual_labels)
print("Log loss, wrong and not confident: {}".format(wrong_not_confident)) 

# Compute and print log loss for 4th case
wrong_confident = compute_log_loss(wrong_confident, actual_labels)
print("Log loss, wrong and confident: {}".format(wrong_confident)) 

# Compute and print log loss for actual labels
actual_labels = compute_log_loss(actual_labels, actual_labels)
print("Log loss, actual labels: {}".format(actual_labels)) 

```
  
  
***
  
Chapter 2 - Create a Simple First Model  
  
Time to build a model - start with a simple model, understand the challenge, then expand as appropriate:  
  
* Begin by training a model that uses only the numeric data - multi-class logistic regression (treat each label separately and use to predict)  
	* Format predictions to CSV and simulate submitting them to the competition  
* Thoughts on splitting the full dataset in a test-train fashion  
	* Risky if done randomly, as some labels are rare and may be too sparse (or even non-existent) in the training data  
    * One potential solution is StratifiedShuffleSplit, though this works only for a single, categorical, target variable  
    * This course created a utility function, multilabel_test_train_split() to ensure proper stratification given the many target types in this data  
* Example coding process could include  
	* data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)  # fill the NA columns with a fake numeric value  
    * labels_to_use = pd.get_dummies(df[LABELS])  
    * X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size=0.2, seed=123)  
    * from sklearn.linear_model import LogisticRegression  
    * from sklearn.multiclass import OneVsRestClassifier  
    * clf = OneVsRestClassifier(LogisticRegression())  
    * clf.fit(X_train, y_train)  
  
Making predictions - such as a hold-out sample for a competition:  
  
* Load the holdout data, pre-process it in the same manner as used earlier, then run the model on it  
	* holdout = pd.read_csv("myCSVHoldout")  
    * predictions = clf.predict_proba(holdout)  # want the probabilities, not just the 'most likely' value as per .predict()  
    * There may be specific requirements as to how the CSV is formatted and uploaded  
* Converting the matrix provided by predict_proba() in to a DataFrame that can be saved using .to_csv()  
	* prediction_df = pd.DataFrame(predictions, columns=pd.get_dummies(df[LABELS], prefix_sep="__").columns, index=holdout.index)  
    * prediction_df.to_csv("predictions.csv")  
    * score = score_submission(pred_path="predictions.csv")  
  
Very brief introduction to NLP (Natural Language Processing) - convert text to features:  
  
* First step is tokenization - splitting a long string in to segments (frequently, a list of strings)  
	* Decision of where to tokenize (e.g., white-space, non-alphanumeric, etc.)  
* "Bag of words" representation - counting the number of times a particular token appears  
	* This approach discards information contained in word order  
    * A more sophisticated approach might create n-grams, where each sequence of n tokens become their own separate token  
    * The 2-gram is also called the bigram and the 3-gram is also called the trigram  
  
Representing text numerically - extending on the "bag of words" approach:  
  
* Scikit-learn offers good tools for bag-of-words - CountVectorizer()  
	1.  Tokenizes all the strings  
    2.  Notes all the words that appear (vocabulary)  
    3.  Counts the occurrence of each token in the vocabulary for each row  
* Usage example for the CountVectorizer() in scikit  
	* from sklearn.feature_extraction.text import CountVectorizer  
    * TOKENS_BASIC = "\\S+(?=\\s+)"  # can be any regular expression; defining the process for defining and extracting by () the tokens  
    * df["Program_Description"].fillna("", inplace=True)  
    * vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)  
    * vec_basic.fit(df["Program_Description"])  
    * len(vec_basic.get_feature_names())  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe"), eval=FALSE}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# see https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/data/multilabel.py for the multilabel_test_train_split()

NUMERIC_COLUMNS = ['FTE', 'Total']
LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']

# Create the new DataFrame: numeric_data_only
numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)

# Get labels and convert to dummy variables: label_dummies
label_dummies = pd.get_dummies(df[LABELS])

# Create training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,
                                                               label_dummies,
                                                               size=0.2, 
                                                               seed=123)

# Print the info
print("X_train info:")
print(X_train.info())
print("\nX_test info:")  
print(X_test.info())
print("\ny_train info:")  
print(y_train.info())
print("\ny_test info:")  
print(y_test.info()) 


# Import classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Create the DataFrame: numeric_data_only
numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)

# Get labels and convert to dummy variables: label_dummies
label_dummies = pd.get_dummies(df[LABELS])

# Create training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,
                                                               label_dummies,
                                                               size=0.2, 
                                                               seed=123)

# Instantiate the classifier: clf
clf = OneVsRestClassifier(LogisticRegression())

# Fit the classifier to the training data
clf.fit(X_train, y_train)

# Print the accuracy
print("Accuracy: {}".format(clf.score(X_test, y_test)))


# Import classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Instantiate the classifier: clf
clf = OneVsRestClassifier(LogisticRegression())

# Fit the classifier to the training data
clf.fit(X_train, y_train)

# Print the accuracy
print("Accuracy: {}".format(clf.score(X_test, y_test)))


# Load the holdout data: holdout
holdout = pd.read_csv("HoldoutData.csv", index_col=0)

# Generate predictions: predictions
predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))


# Generate predictions: predictions
predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))

# Format predictions in DataFrame: prediction_df
prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,
                             index=holdout.index,
                             data=predictions)


# Save prediction_df to csv
prediction_df.to_csv("predictions.csv")

# Submit the predictions for scoring: score
score = score_submission(pred_path="predictions.csv")

# Print score
print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))


# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Fill missing values in df.Position_Extra
df["Position_Extra"].fillna("", inplace=True)

# Instantiate the CountVectorizer: vec_alphanumeric
vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Fit to the data
vec_alphanumeric.fit(df["Position_Extra"])

# Print the number of tokens and first 15 tokens
msg = "There are {} tokens in Position_Extra if we split on non-alpha numeric"
print(msg.format(len(vec_alphanumeric.get_feature_names())))
print(vec_alphanumeric.get_feature_names()[:15])


# Define combine_text_columns()
def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):
    """ converts all text in each row of data_frame to single vector """
    
    # Drop non-text columns that are in the df
    to_drop = set(to_drop) & set(data_frame.columns.tolist())
    text_data = data_frame.drop(to_drop, axis=1)
    
    # Replace nans with blanks
    text_data.fillna("", inplace=True)
    
    # Join all text items in a row that have a space in between
    return text_data.apply(lambda x: " ".join(x), axis=1)


# Import the CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create the basic token pattern
TOKENS_BASIC = '\\S+(?=\\s+)'

# Create the alphanumeric token pattern
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Instantiate basic CountVectorizer: vec_basic
vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)

# Instantiate alphanumeric CountVectorizer: vec_alphanumeric
vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Create the text vector
text_vector = combine_text_columns(df)

# Fit and transform vec_basic
vec_basic.fit_transform(text_vector)

# Print number of tokens of vec_basic
print("There are {} tokens in the dataset".format(len(vec_basic.get_feature_names())))

# Fit and transform vec_alphanumeric
vec_alphanumeric.fit_transform(text_vector)

# Print number of tokens of vec_alphanumeric
print("There are {} alpha-numeric tokens in the dataset".format(len(vec_alphanumeric.get_feature_names())))


```
  
  
  
***
  
Chapter 3 - Improving Models  
  
Pipelines, features, and text processing:  
  
* Pipeline workflow is a repeatable way to go from raw data to a tarined model  
	* Sequential list of steps, with the output of one step being the input to the next  
    * Each step is a tuple, with a string (name for the step), followed by the step)  
    * Pipelines significantly increase flexibility and reproducibility  
    * from sklearn.pipeline import Pipeline  
    * pl = Pipeline([ ("clf", OneVsRestClassifier(LogisticRegression())) ])  # obviously not needed for a single step; merely illustrative  
* Example workflow  
	* from sklearn.model_selection import train_test_split  
    * X_train, X_test, y_train, y_test = train_test_split(sample_df[["numeric"]], pd.get_dummies(sample_df["label"]), random_state=2)  
    * pl.fit(X_train, y_train)  
    * accuracy = pl.score(X_test, y_test)  
* Can continue to expand the Pipeline, for example, adding an Imputer so that the NaN problem can be addressed/avoided  
	* from sklearn.preprocessing import Imputer  
    * pl = Pipeline([ ("imp", Imputer()) , ("clf", OneVsRestClassifier(LogisticRegression())) ])  # needed now, since there are two steps - Impute, then Classify  
  
Text features and feature unions - working with the text features in the provided dataset:  
  
* Processing multiple data types - problem that CountVectorizer() needs text while Imputer() needs values  
* These problems can be addressed using the Python tools FunctionTransformer() and FeatureUnion()  
* Function Transformer - turns a Python function into an object that a scikit-learn pipeline can understand  
	* Take entire DataFrame, and return all numeric columns  
    * Take entire DataFrame, and return all text columns  
    * Can then pre-process in a separate pipeline for numeric and text  
* Example coding process to implement the FunctionTransformer  
	* from sklearn.preprocessing import FunctionTransformer  
    * get_text_data = FunctionTransformer(lambda x: x[["text"]], validate=False)  # validate=False means it is OK to not check for NaN, dtype, and the like  
    * get_text_data = FunctionTransformer(lambda x: x[["numeric", "with_missing]], validate=False)  
* Continued coding process to bring in FeatureUnion capabilities  
	* from sklearn.pipeline import FeatureUnion  
    * numeric_pipeline = Pipeline( [("selector", get_numeric_data) , ("imputer", Imputer()) ])  
    * text_pipeline = Pipeline( [("selector", get_text_data) , ("vectorizer", CountVectorizer()) ])  
    * pl = Pipeline( [ ("union" , FeatureUnion( [("numeric", numeric_pipeline) , ("text", text_pipeline)] )) , ("clf", OneVsRestClassifier(LogisticRegression())) ])  
  
Choosing a classification model - returning to the sample school budget data for this course:  
  
* Returning to the process from previous chapters - load libraries and data, run pd.get_dummies(), run multilabel_test_train_split()  
* The Pipeline() defined above just needs a few subtle tweaks to be usable with the school budget dataset  
* Can then adapt the Pipeline() in search of model improvements - update the final step for different models than LogisticRegression()  
	* Random Forest, Nave Bayes, k-NN, etc.  
    * from sklearn.ensemble import RandomForestClassifier  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe"), eval=FALSE}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# Import Modules
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import Imputer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import FeatureUnion
from sklearn.ensemble import RandomForestClassifier





# Split and select numeric data only, no nans 
X_train, X_test, y_train, y_test = train_test_split(sample_df[["numeric"]],
                                                    pd.get_dummies(sample_df['label']), 
                                                    random_state=22)

# Instantiate Pipeline object: pl
pl = Pipeline([
        ("clf", OneVsRestClassifier(LogisticRegression()))
    ])

# Fit the pipeline to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on sample data - numeric, no nans: ", accuracy)


# Create training and test sets using only numeric data
X_train, X_test, y_train, y_test = train_test_split(sample_df[["numeric", "with_missing"]],
                                                    pd.get_dummies(sample_df['label']), 
                                                    random_state=456)

# Insantiate Pipeline object: pl
pl = Pipeline([
        ("imp", Imputer()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

# Fit the pipeline to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on sample data - all numeric, incl nans: ", accuracy)


# Split out only the text data
X_train, X_test, y_train, y_test = train_test_split(sample_df["text"],
                                                    pd.get_dummies(sample_df['label']), 
                                                    random_state=456)

# Instantiate Pipeline object: pl
pl = Pipeline([
        ("vec", CountVectorizer()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

# Fit to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on sample data - just text data: ", accuracy)


# Obtain the text data: get_text_data
get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)

# Obtain the numeric data: get_numeric_data
get_numeric_data = FunctionTransformer(lambda x: x[["numeric", "with_missing"]], validate=False)

# Fit and transform the text data: just_text_data
just_text_data = get_text_data.fit_transform(sample_df)

# Fit and transform the numeric data: just_numeric_data
just_numeric_data = get_numeric_data.fit_transform(sample_df)

# Print head to check results
print('Text Data')
print(just_text_data.head())
print('\nNumeric Data')
print(just_numeric_data.head())


# Split using ALL data in sample_df
X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],
                                                    pd.get_dummies(sample_df['label']), 
                                                    random_state=22)

# Create a FeatureUnion with nested pipeline: process_and_join_features
process_and_join_features = FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ("selector", get_text_data),
                    ("vectorizer", CountVectorizer())
                ]))
             ]
        )

# Instantiate nested pipeline: pl
pl = Pipeline([
        ('union', process_and_join_features),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])


# Fit pl to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on sample data - all data: ", accuracy)


# have combine_text_columns() from previous chapter
def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):
    """ converts all text in each row of data_frame to single vector """
    
    # Drop non-text columns that are in the df
    to_drop = set(to_drop) & set(data_frame.columns.tolist())
    text_data = data_frame.drop(to_drop, axis=1)
    
    # Replace nans with blanks
    text_data.fillna("", inplace=True)
    
    # Join all text items in a row that have a space in between
    return text_data.apply(lambda x: " ".join(x), axis=1)


# Get the dummy encoding of the labels
dummy_labels = pd.get_dummies(df[LABELS])

# Get the columns that are features in the original df
NON_LABELS = [c for c in df.columns if c not in LABELS]

# Split into training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],
                                                               dummy_labels,
                                                               0.2, 
                                                               seed=123)

# Preprocess the text data: get_text_data
get_text_data = FunctionTransformer(combine_text_columns, validate=False)

# Preprocess the numeric data: get_numeric_data
get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)


# Data may be available at https://www.drivendata.org/
# Complete the pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ("selector", get_numeric_data),
                    ("imputer", Imputer())
                ])),
                ('text_features', Pipeline([
                    ("selector", get_text_data),
                    ("vectorizer", CountVectorizer())
                ]))
             ]
        )),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])

# Fit to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on budget dataset: ", accuracy)


# Edit model step in pipeline
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer())
                ]))
             ]
        )),
        ("clf", RandomForestClassifier())
    ])

# Fit to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on budget dataset: ", accuracy)


# Add model step to pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer())
                ]))
             ]
        )),
        ("clf", RandomForestClassifier(n_estimators=15))
    ])

# Fit to the training data
pl.fit(X_train, y_train)

# Compute and print accuracy
accuracy = pl.score(X_test, y_test)
print("\nAccuracy on budget dataset: ", accuracy)

```
  
  
  
***
  
Chapter 4 - Learning from Experts  
  
Learning from experts (pre-processing) - text processing, statistical methods, computational efficiency:  
  
* NLP tips for text data  
	* Tokenize on punctuation (hyphens, underscores, etc.)  
    * Include both unigrams and bi-grams as tokens -- vec = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1, 2))  
  
Learning from experts (stats trick) - interaction terms:  
  
* Idea is to identify that "English teacher for 2nd grade" and "2nd grade - budget for English teacher" are probably the same thing  
* Interaction terms allow for accounting for when certain tokens appear together - for example, both "2nd grade" and "English teacher"  
	* Basically, the model moves fro y ~ x1, x2 to y ~ x1, x2, x1*x2  
* Can implement the interaction terms directly from scikit-learn  
	* from sklearn.preprocessing import PolynomialFeatures  
    * interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)   # degree=2 will manage two columns; interaction_only=True means do not multipley by self  
    * The "bias term" is an offset for the model -- include a non-zero y even when there is a zero x  
* The process is much more computationally efficient with a SparseMatrix, which is not supported by PolynomialFeatures  
	* SparseInteractions() is a replacement option for this expercise  
    * SparseInteractions(degree=2).fit_transform(x).toarray()  # just pass it the degree  
  
Learning from experts (computational trick and winning model) - hashing trick:  
  
* Adding new features may cause enromus increases in array size  
* Hashing is a way of increasing memory efficiency  
	* Hashing takes an input and returns an associated hash value  
    * The hash function can be defined to have a maximum size; for example, 250 hash values total, which explicitly limits feature size  
    * Published papers show that even though 2+ tokens are mapped to the same hash, there is very little real-world loss of predictive power  
* The hashing trick is a form of dimension reduction - particularly for large quantities of text  
	* from sklearn.feature_extraction.text import HashingVectorizer  
    * vec = HashingVectorizer(norm=None, non_negative=True, toekn_pattern=TOKENS-ALPHANUMERIC, ngram_range=(1, 2))  
* Class of model used by winner - logistic regression  
	* Carefully create features, and implement clever tricks  
* See https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb for Jupyter notebook  
  
Wrap up and next steps:  
  
* Volume of data is growing - opportunity to make improvements  
* Flexibility of the pipeline process in scikit-learn  
* Quickly test ways of improving submissions  
* Can continue to improve the model  
	* Stemming, stop-words  
    * Random Forest, Nave Bayes, k-NN  
    * Numeric Processing, Imputation  
    * Grid-search optimization  
    * Additional scikit-learn techniques  
* DrivenData has the full dataset (www.drivendata.org)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe"), eval=FALSE}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# combine_text_columns() is available in previous chapters
def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):
    """ converts all text in each row of data_frame to single vector """
    
    # Drop non-text columns that are in the df
    to_drop = set(to_drop) & set(data_frame.columns.tolist())
    text_data = data_frame.drop(to_drop, axis=1)
    
    # Replace nans with blanks
    text_data.fillna("", inplace=True)
    
    # Join all text items in a row that have a space in between
    return text_data.apply(lambda x: " ".join(x), axis=1)



# Import the CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import Imputer
from sklearn.feature_selection import chi2, SelectKBest
from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler
from sklearn.pipeline import FeatureUnion
from sklearn.feature_extraction.text import HashingVectorizer



# Create the text vector
text_vector = combine_text_columns(X_train)

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Instantiate the CountVectorizer: text_features
text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Fit text_features to the text vector
text_features.fit(text_vector)

# Print the first 10 tokens
print(text_features.get_feature_names()[:10])



# Select 300 best features
chi_k = 300

# Perform preprocessing
get_text_data = FunctionTransformer(combine_text_columns, validate=False)
get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Instantiate pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,
                                                   ngram_range=(1, 2))),
                    ('dim_red', SelectKBest(chi2, chi_k))
                ]))
             ]
        )),
        ('scale', MaxAbsScaler()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])


# Code for SpareInteractions available at https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py
from itertools import combinations
import numpy as np
from scipy import sparse
from sklearn.base import BaseEstimator, TransformerMixin


class SparseInteractions(BaseEstimator, TransformerMixin):
    def __init__(self, degree=2, feature_name_separator="_"):
        self.degree = degree
        self.feature_name_separator = feature_name_separator
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        if not sparse.isspmatrix_csc(X):
            X = sparse.csc_matrix(X)
        if hasattr(X, "columns"):
            self.orig_col_names = X.columns
        else:
            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])
        spi = self._create_sparse_interactions(X)
        return spi
    def get_feature_names(self):
        return self.feature_names
    def _create_sparse_interactions(self, X):
        out_mat = []
        self.feature_names = self.orig_col_names.tolist()
        for sub_degree in range(2, self.degree + 1):
            for col_ixs in combinations(range(X.shape[1]), sub_degree):
                # add name for new column
                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])
                self.feature_names.append(name)
                # get column multiplications value
                out = X[:, col_ixs[0]]
                for j in col_ixs[1:]:
                    out = out.multiply(X[:, j])
                out_mat.append(out)

        return sparse.hstack([X] + out_mat)



# Instantiate pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,
                                                   ngram_range=(1, 2))),  
                    ('dim_red', SelectKBest(chi2, chi_k))
                ]))
             ]
        )),
        ("int", SparseInteractions(degree=2)),
        ('scale', MaxAbsScaler()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])



# Get text data: text_data
text_data = combine_text_columns(X_train)

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)' 

# Instantiate the HashingVectorizer: hashing_vec
hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Fit and transform the Hashing Vectorizer
hashed_text = hashing_vec.fit_transform(text_data)

# Create DataFrame and print the head
hashed_df = pd.DataFrame(hashed_text.data)
print(hashed_df.head())


# Instantiate the winning model pipeline: pl
pl = Pipeline([
        ('union', FeatureUnion(
            transformer_list = [
                ('numeric_features', Pipeline([
                    ('selector', get_numeric_data),
                    ('imputer', Imputer())
                ])),
                ('text_features', Pipeline([
                    ('selector', get_text_data),
                    ("vectorizer", HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,
                                                     non_negative=True, norm=None, binary=False,
                                                     ngram_range=(1, 2))),
                    ('dim_red', SelectKBest(chi2, chi_k))
                ]))
             ]
        )),
        ('int', SparseInteractions(degree=2)),
        ('scale', MaxAbsScaler()),
        ('clf', OneVsRestClassifier(LogisticRegression()))
    ])


```
  


###_NLP Fundamentals in Python_#

Chapter 1 - Regular expressions and word tokenization  
  
Introduction to regular expressions - strings with a specialy syntax for string matching:  
  
* Find links in a web page, parse e-mail addresses, etc.  
* The Python re library works well for regular expressions - pattern first, text second  
	* import(re)  
    * re.match("abc", "abcdef")  
    * \w+ (match a word)  
    * \d (match a digit)  
    * \s (match a space)  
    * . (wildcard)  
    * + or * (greedy)  
    * [a-z] (group of characters)  
* Capital letters negate regex, so \S would be "not a space" and \W would be "not a character"  
	
Introduction to tokenization - turning a string or document in to tokens (smaller pieces):  
  
* Break out words, separate punctuation, find hashtags, etc.  
* The nltk library is commonly used; for example, from nltk.tokenize import word_tokenize; word_tokenize("Hi there!")  
	* Words will be seperated as well individual punctautaions  
    * Helpful for parts of speech, repeated words, common words, etc.  
* Additional features of the nltk library include  
	* sent_tokenize (tokenize document by sentences)  
    * regexp_tokenize (tokenize based on a provided regular expression)  
    * TweetTokenizer (special class for hashtags, exclamation points, mentions, and the like  
* Distinctions in re.search() and re.match()  
	* re.match() tries to match from the beginning, while re.search will match from anywhere  
    * The gist is that re.match() is better if you want to find things at the start of the word, while re.search is better if you want to find them anywhere  
  
Advanced tokenization with NLTK and regex:  
  
* The "or" method is represented by the pipe operator, defined using ()  
* For example, to find FULL digits or words, use ( '(\d+|\w+)' )  
* Regex groups and ranges include  
	* [A-Za-z] upper and lower case English  
    * [0-9] number from 0-9  
    * [A-Za-z\-\.] upper/lower case English plus minus plus period  
    * (a-z) a, minus, and z  
    * (\s+|,) spaces or a single comma  
    * [a-z0-9 ]+ will keep matching lower-case English, digits 0-9, and spaces, and will end when it hits anything else  
  
Charting word length with NLTK - using matplotlib:  
  
* After tokenizing the words, use a = [len(x) for x in myTokens]  
* Then, plt.plot(a, kind="hist")  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



my_string = "Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?"

# Import the regex module
import re

# Write a pattern to match sentence endings: sentence_endings
sentence_endings = r"[.?!]"

# Split my_string on sentence endings and print the result
print(re.split(sentence_endings, my_string))

# Find all capitalized words in my_string and print the result
capitalized_words = r"[A-Z]\w+"
print(re.findall(capitalized_words, my_string))

# Split my_string on spaces and print the result
spaces = r"\s+"
print(re.split(spaces, my_string))

# Find all digits in my_string and print the result
digits = r"\d+"
print(re.findall(digits, my_string))

myFile = open(myPath + "mp_holygrail_scene1.txt", "r")
scene_one = myFile.read().replace("\\n", "\n")
myFile.close()


# Import necessary modules
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))

# Print the unique tokens result
print(unique_tokens)


# Search for the first occurrence of "coconuts" in scene_one: match
match = re.search("coconuts", scene_one)

# Print the start and end indexes of match
print(match.start(), match.end())

# Write a regular expression to search for anything in square brackets: pattern1
pattern1 = r"\[.*\]"

# Use re.search to find the first text in square brackets
print(re.search(pattern1, scene_one))

# Find the script notation at the beginning of the fourth sentence and print it
pattern2 = r"[\w\s]+:"
print(re.match(pattern2, sentences[3]))


tweets = ['This is the best #nlp exercise ive found online! #python',
 '#NLP is super fun! <3 #learning',
 'Thanks @datacamp :) #nlp #python']


# Import the necessary modules
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import TweetTokenizer

# Define a regex pattern to find hashtags: pattern1
pattern1 = r"#\w+"

# Use the pattern on the first tweet in the tweets list
regexp_tokenize(tweets[0], pattern1)

# Write a pattern that matches both mentions and hashtags
pattern2 = r"([@|#]\w+)"

# Use the pattern on the last tweet in the tweets list
regexp_tokenize(tweets[-1], pattern2)

# Use the TweetTokenizer to tokenize all tweets into one list
tknzr = TweetTokenizer()
all_tokens = [tknzr.tokenize(t) for t in tweets]
print(all_tokens)


# Unicode ranges for emoji are: ('\U0001F300'-'\U0001F5FF'), ('\U0001F600-\U0001F64F'), ('\U0001F680-\U0001F6FF'), and ('\u2600'-\u26FF-\u2700-\u27BF')

# Tokenize and print all words in german_text
# all_words = word_tokenize(german_text)
# print(all_words)

# Tokenize and print only capital words
capital_words = r"[A-Z]\w+"
# print(regexp_tokenize(german_text, capital_words))

# Tokenize and print only emoji
emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
# print(regexp_tokenize(german_text, emoji))


myFile = open(myPath + "mp_holygrail_longer.txt", "r")
holy_grail = myFile.read().replace("\\n", "\n")
myFile.close()


# Split the script into lines: lines
lines = holy_grail.split('\n')

# Replace all script lines for speaker
pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
lines = [re.sub(pattern, '', l) for l in lines]

# Tokenize each line: tokenized_lines
tokenized_lines = [regexp_tokenize(s, "\w+") for s in lines]

# Make a frequency list of lengths: line_num_words
line_num_words = [len(t_line) for t_line in tokenized_lines]

# Plot a histogram of the line lengths
plt.hist(line_num_words)

# Show the plot
plt.savefig("_dummyPy212.png", bbox_inches="tight")
# plt.show()
plt.clf()

```
  

**Example #1: Histogram of Word Counts - Monty Python Holy Grail**:  
![](_dummyPy212.png)
  
***
  
Chapter 2 - Simple Topic Identification  
  
Word counts with bag-of-words (basic method for finding topics in text):  
  
* First, create tokens; then, second, count them up  
* Often, add pre-processing to make everything lowercase  
* Use the "from collections add Counter"  
	* counter = Counter(word_tokenize(myString))  
    * Structure of the counter is much like a dictionary  
    * counter.most_common(int)  # breaks the ties; for example, if there are 5 words each with count 9, and int=3, you will get only three (3) of the words  
  
Simple text preprocessing - helps make for better input data:  
  
* Tokenization to create bag of words  
	* [w for w in word_tokenize(myText)]  
* Conversion to lower-case  
	* [w for w in word_tokenize(myText.lower())]  
* Removing punctuation and the like  
	* [w for w in word_tokenize(myText.lower()) if w.isalpha()]  
* Removing stop words ("and", "the", etc.)  
	* [t for t in [w for w in word_tokenize(myText.lower()) if w.isalpha()] if t not in stopwords.words("english")]  
* Stemming (lemmatization) - shortening words to their roots  
* Try several techniques and see which outputs are best for the question at hand  
  
Introduction to gensim - popular open-source NLP library based on academic models:  
  
* A "word vector" is a multidimensional array in areas like Male-Female, Verb Tense, Country-Capital, and the like  
* LDA (latent Dirichlet allocation) helps to find relationships (similarities) in the data  
* Corpus (plural corpora) are a set of text used for NLP  
	* from gensim.corpora.dictionary import Dictionary  
    * tokenized_docs = [word_tokenize(doc.lower()) for doc in myDocuments]  
    * dictionary = Dictionary(tokenized_docs)  
    * dictionary.token2id  # dictionary of tokens and frequencies  
    * corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]  # will be a list of lists, with each inner list being a tuple of index to frequency  
  
Tf-Idf with gensim (term-frequency - inverse document frequency]:  
  
* Helps to determine the most important words in each document, as well as similarities in non-stopwords across documents  
	* The words should be down-weighted in importance; for example, "sky" may be frequently used in astronomy, but has little inherent meaning  
    * Basically, ensures that the most common words across ALL of the documents do not show up as the final key words; what do these 2 documents have in common more so than the overall set of documents?  
* The overall tf-idf formula  
	* w(I, j) = tf(I, j) * log (N / df(i))  
    * w(I, j) is the tf-idf for term I in document j  
    * tf(I, j) is the total occurences of token I in document j  
    * N is the number of documents  
    * df(i) is the number of documents that contain token i  
* Can build the tf-idf using gensim  
	* from gensim.models.tfidfmodel import TfidfModel  
    * tfidf = TfidfModel(corpus)  
    * Can then access using tfidf[corpus[myInt]] to get document myInt  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



myFile = open(myPath + "nlp_ch2_article.txt", "r")
article = myFile.read().replace("\\n", "\n")
myFile.close()


# Import modules
from collections import Counter, defaultdict
import re
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import TweetTokenizer
import itertools


# Tokenize the article: tokens
tokens = word_tokenize(article)

# Convert the tokens into lowercase: lower_tokens
lower_tokens = [t.lower() for t in tokens]

# Create a Counter with the lowercase tokens: bow_simple
bow_simple = Counter(lower_tokens)

# Print the 10 most common tokens
print(bow_simple.most_common(10))


english_stops = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', '']

# Import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]

# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

# Create the bag-of-words: bow
bow = Counter(lemmatized)

# Print the 10 most common tokens
print(bow.most_common(10))


# Note that 'articles' is already tokenized here . . . 
# Create from the articles stored in myPath + "WikiFiles"
import os

loadFiles = os.listdir(myPath + "WikiFiles/")
articles = []
for eachFile in loadFiles:
    myFile = open(myPath + "WikiFiles/" + eachFile, "r", encoding='utf-8')
    thisArticle = myFile.read()
    allWords = [t.lower() for t in word_tokenize(thisArticle) if t.isalpha()]
    noStops = [t for t in allWords if t not in english_stops]
    wordnet_lemmatizer = WordNetLemmatizer()
    thisLemma = [wordnet_lemmatizer.lemmatize(t) for t in noStops]
    articles.append(thisLemma)
    myFile.close()

# Import Dictionary
from gensim.corpora.dictionary import Dictionary

# Create a Dictionary from the articles: dictionary
dictionary = Dictionary(articles)

# Select the id for "computer": computer_id
computer_id = dictionary.token2id.get("computer")

# Use computer_id with the dictionary to print the word
print(dictionary.get(computer_id))

# Create a MmCorpus: corpus
corpus = [dictionary.doc2bow(article) for article in articles]

# Print the first 10 word ids with their frequency counts from the fifth document
print(corpus[4][:10])


# Save the fifth document: doc
doc = corpus[4]

# Sort the doc for frequency: bow_doc
bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)

# Print the top 5 words of the document alongside the count
for word_id, word_count in bow_doc[:5]:
    print(dictionary.get(word_id), word_count)
    
# Create the defaultdict: total_word_count
total_word_count = defaultdict(int)
for word_id, word_count in itertools.chain.from_iterable(corpus):
    total_word_count[word_id] += word_count

# Create a sorted list from the defaultdict: sorted_word_count
sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) 

# Print the top 5 words across all documents alongside the count
for word_id, word_count in sorted_word_count[:5]:
    print(dictionary.get(word_id), word_count)


# Import TfidfModel
from gensim.models.tfidfmodel import TfidfModel

# Create a new TfidfModel using the corpus: tfidf
tfidf = TfidfModel(corpus)

# Calculate the tfidf weights of doc: tfidf_weights
tfidf_weights = tfidf[doc]

# Print the first five weights
print(tfidf_weights[0:5])

# Sort the weights from highest to lowest: sorted_tfidf_weights
sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)

# Print the top 5 weighted words
for term_id, weight in sorted_tfidf_weights[:5]:
    print(dictionary.get(term_id), weight)

```
  
  
  
***
  
Chapter 3 - Named Entity Recognition (NER)  
  
Named Entity Recognition (NER) is an NLP task for finding important named entities (people, places, organizations, etc.):  
  
* Helps to answer Who?  What?  When?  Where?  
* The nltk library provides access to the Stanford CoreNLP library  
	* Requires installing some java and updating some system variables  
    * Can instead be used as a stand-alone or as an API server  
* Can instead just use the (simpler) built-in library from nltk  
	* import nltk  
    * token_sent = nltk.word_tokenize(sentence)  
    * tagged_sent = nltk.pos_tag(token_sent)  
    * print(nltk.ne_chunk(tagged_sent))  
  
Introduction to SpaCy (NLP library like gensim, but with different implementations):  
  
* Focus on creating NLP pipelines to generate models and corpora  
* Open-source, with extra libraries and tools (such as Displacy)  
* Need to start by downloading spacy, as well as a number of the trained vectors  
	* import spacy  
    * nlp = spacy.load("en")  
    * nlp.entity can then be used to find entities in the text  
    * doc = nlp(myText)  
    * doc.ents # will show the entities in the document  
    * doc.ents[0], doc.ents[0].label_  # tuple showing the entity and its label (more or less, its type)  
* Why use Spacy for NER?  
	* Different entity types often identified  
    * Fast-growing library  
  
Multilingual NER with polyglot (additional library for NLP):  
  
* The polyglot library has vectors for 130+ languages  
	* Due to this, the library is also useful for trans-literation (literal, word for word, translations)  
* Spanish NER with polyglot  
	* Need to have the program and the appropriate named vectors already installed  
    * from polyglot.text import Text  
    * ptext = Text(myText)  # polyglot will do automatic language detection  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



import nltk
from collections import Counter, defaultdict
# from polyglot.text import Text


myFile = open(myPath + "uber_apple.txt", "r", encoding="utf-8")
article = myFile.read().replace("\\n", "\n")
myFile.close()


# Tokenize the article into sentences: sentences
sentences = nltk.sent_tokenize(article)

# Tokenize each sentence into words: token_sentences
token_sentences = [nltk.word_tokenize(sent) for sent in sentences]

# Tag each tokenized sentence into parts of speech: pos_sentences
pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] 

# Create the named entity chunks: chunked_sentences
chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)

# Test for labelled chunks
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, 'label'):
            print(chunk)
            

# Read in the articles list
myFile = open(myPath + "NLP_article_samples.txt", "r", encoding="utf-8")
rawArticles = myFile.read().replace("\\n", "\n")
myFile.close()

# Tokenize
artSentences = nltk.sent_tokenize(rawArticles)
artTokens = [nltk.word_tokenize(sent) for sent in artSentences]
artPOS = [nltk.pos_tag(sent) for sent in artTokens] 
chunked_sentences = nltk.ne_chunk_sents(artPOS, binary=True)


# Create the defaultdict: ner_categories
ner_categories = defaultdict(int)

# Test for labelled chunks
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, 'label'):
            print(chunk)
            ner_categories[chunk.label()] += 1
            
# Create a list from the dictionary keys for the chart labels: labels
labels = list(ner_categories.keys())

# Create a list of the values: values
values = [ner_categories.get(l) for l in labels]

# Create the pie chart
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)

# Display the chart
# plt.show()
plt.savefig("_dummyPy213.png", bbox_inches="tight")
plt.clf()


# Import spacy (cannot install)
# import spacy

# Instantiate the English model: nlp
# nlp = spacy.load("en", tagger=False, parser=False, matcher=False)

# Create a new document: doc
# doc = nlp(article)

# Print all of the found entities and their labels
# for ent in doc.ents:
#     print(ent.label_, ent.text)


myFile = open(myPath + "NLP_french.txt", "r", encoding="utf-8")
article = myFile.read().replace("\\n", "\n")
myFile.close()


# Create a new text object using Polyglot's Text class: txt
# txt = Text(article)

# Print each of the entities found
# for ent in txt.entities:
#     print(ent)
    
# Print the type of each entity
# print(type(ent))


# Create the list of tuples: entities
# entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]

# Print the entities
# print(entities)


# Initialize the count variable: count
# count = 0

# Iterate over all the entities
# for ent in txt.entities:
    # Check whether the entity contains 'Mrquez' or 'Gabo'
#     if "Mrquez" in ent or "Gabo" in ent:
        # Increment count
#         count += 1

# Print count
# print(count)

# Calculate the percentage of entities that refer to "Gabo": percentage
# percentage = count * 1.0 / len(txt.entities)
# print(percentage)


# Create a set of spaCy entities keeping only their text: spacy_ents
# spacy_ents = {e.text for e in doc.ents} 

# Create a set of the intersection between the spacy and polyglot entities: ensemble_ents
# ensemble_ents = spacy_ents.intersection(poly_ents)

# Print the common entities
# print(ensemble_ents)

# Calculate the number of entities not included in the new ensemble set of entities: num_left_out
# num_left_out = len(spacy_ents.union(poly_ents)) - len(ensemble_ents)
# print(num_left_out)

```
  
  
**Example #1: Pie Chart for Named Entity Labels**:  
![](_dummyPy213.png)
  
***
  
Chapter 4 - Building a "Fake News" Classifier  
  
Supervised Learning with NLP - form of machine learning with labelled training data:  
  
* Goal is to use language to make the classifications  
* Will use scikit-learn, combined with bag-of-words and tf-idf  
* Example might be to classify the type of movie based on a description of the plot  
* Overall supervised learning process  
	1.  Collect and pre-process data  
    2.  Determine labels  
    3.  Split in to test and training data  
    4.  Extract features  
    5.  Train the model, then test on the non-training data  
  
Word Count Vectors with scikit-learn:  
  
* Typically, define y as the target variable and X as the matrix of potential classifiers  
* Can then use test_train_split from sklearn.model_selection  
	* X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<myProportion>, random_state=<myIntSeed>)  
* Can then create a CountVectorizer object imported from sklearn.feature_extraction  
	* count_vectorizer = CountVectorizer(stop_words = "english")  
* Can then use the .fit_transform() methodology from sklearn  
	* count_train = count_vectorizer.fit_transform(X_train.values)  
    * count_test = count_vectorizer.fit_transform(X_test.values)  # runs the exact same methodology on the test data  
    * If the test data has words not in the training data, then this process will throw an error (need to either kill of the words or expand the training data)  
  
Training and Testing Classification Models with scikit-learn:  
  
* Nave Bayes models are commonly used for NLP classification problems  
	* Given some particular data, how likely is a particular outcome?  
    * How does new information change the likelihood of the particular outcome?  
    * Not always the best tool, but has the benefit of being simple and effective  
* Can import Nave Bayes using from sklearn.naive_bayes import MultinomialNB  
	* Works well with count vectorizers, since it expects integer inputs  
    * Does not work as well with floats (tfidf), whene SVM or linear models tend to be stronger  
* General fitting process includes  
	* from sklearn import metrics  
    * nb_classifier = MutinomialNB()  
    * nb_classifier.fit(count_train, y_train)  
    * pred = nb_classifier.predict(count_test)  
    * metrics.accuracy_score(y_test, pred)  # Get the accuracy score  
    * metrics.confusion_matrix(y_test, pred, labels=[0, 1])  
  
Simple NLP, Complex Problems - extending to the complexity of real-world problems:  
  
* Translation can be a big problem - for example, many German words become "economics" in English  
* Sentiment analysis can be tricky due to snark and sarcasm  
	* Different communities may also use the same words very differently  
* Language biases can cause gender biases and the like (the translation in one direction and then back can have the genders wrong or even reversed)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# Note that df is 6335 x 4 (my download is much smaller)
df = pd.read_csv(myPath + "sampleFakeNews.csv")


# Import the necessary modules
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split


# Print the head of df
print(df.head())

# Create a series to store the labels: y
y = df["label"]

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(df["text"], y, test_size=0.33, random_state=53)

# Initialize a CountVectorizer object: count_vectorizer
count_vectorizer = CountVectorizer(stop_words="english")

# Transform the training data using only the 'text' column values: count_train 
count_train = count_vectorizer.fit_transform(X_train)

# Transform the test data using only the 'text' column values: count_test 
count_test = count_vectorizer.transform(X_test)

# Print the first 10 features of the count_vectorizer
print(count_vectorizer.get_feature_names()[:10])


# Import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize a TfidfVectorizer object: tfidf_vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words="english", max_df=0.7)

# Transform the training data: tfidf_train 
tfidf_train = tfidf_vectorizer.fit_transform(X_train)

# Transform the test data: tfidf_test 
tfidf_test = tfidf_vectorizer.transform(X_test)

# Print the first 10 features
print(tfidf_vectorizer.get_feature_names()[:10])

# Print the first 5 vectors of the tfidf training data
print(tfidf_train.A[:5])


# Create the CountVectorizer DataFrame: count_df
count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())

# Create the TfidfVectorizer DataFrame: tfidf_df
tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())

# Print the head of count_df
print(count_df.head())

# Print the head of tfidf_df
print(tfidf_df.head())

# Calculate the difference in columns: difference
difference = set(count_df.columns) - set(tfidf_df.columns)
print(difference)

# Check whether the DataFrames are equal
print(count_df.equals(tfidf_df))


# Import the necessary modules
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB

# Instantiate a Multinomial Naive Bayes classifier: nb_classifier
nb_classifier = MultinomialNB()

# Fit the classifier to the training data
nb_classifier.fit(count_train, y_train)

# Create the predicted tags: pred
pred = nb_classifier.predict(count_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print(score)

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=["FAKE", "REAL"])
print(cm)


# Create a Multinomial Naive Bayes classifier: nb_classifier
nb_classifier = MultinomialNB()

# Fit the classifier to the training data
nb_classifier.fit(tfidf_train, y_train)

# Create the predicted tags: pred
pred = nb_classifier.predict(tfidf_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print(score)

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=["FAKE", "REAL"])
print(cm)


# Create the list of alphas: alphas
alphas = np.arange(0, 1, 0.1)

# Define train_and_predict()
def train_and_predict(alpha):
    # Instantiate the classifier: nb_classifier
    nb_classifier = MultinomialNB(alpha=alpha)
    # Fit to the training data
    nb_classifier.fit(tfidf_train, y_train)
    # Predict the labels: pred
    pred = nb_classifier.predict(tfidf_test)
    # Compute accuracy: score
    score = metrics.accuracy_score(y_test, pred)
    return score

# Iterate over the alphas and print the corresponding score
for alpha in alphas:
    print('Alpha: ', alpha)
    print('Score: ', train_and_predict(alpha))
    print()


# Get the class labels: class_labels
class_labels = nb_classifier.classes_

# Extract the features: feature_names
feature_names = tfidf_vectorizer.get_feature_names()

# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights
feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))

# Print the first class label and the top 20 feat_with_weights entries
print(class_labels[0], feat_with_weights[:20])

# Print the second class label and the bottom 20 feat_with_weights entries
print(class_labels[1], feat_with_weights[-20:])

```
  
  
  
###_Case Studies in Statistical Thinking_#

Chapter 1 - Fish Sleep and Bacteria Growth  
  
Activity of zebrafish (David Prober) and melatonin:  
  
* Each fish in its own well, recorded by a camera (more movements means more wakeful)  
* Interesting since some fish have a mutation related to melatonin, while others ("wild") do not  
* Objective is to quantify the impact of the mutation on wakefulness  
	* Active bout: Period of time where fish is consistently active  
    * Active bout length: Number of consecutive minutes with activity  
* Exponential distribution - waiting time between arrivals of a Poisson process (timing of next event is independent of past history) is exponentially distributed  
* Can gain access to the modules using import dc_stat_think as dcst and then (for example) dcst.ecdf()  
  
Bootstrap confidence intervals:  
  
* Exponential distribution has a single parameter, calculated from the mean of the data  
* Bootstrap samples (with replacement) allow for better understanding of the confidence in the mean used to describe the exponential distribution  
* Bootstrap replicates are sample statistics calculated from a bootstrap sample  
	* Should generally find that, if repeated over time, p% of the observations would be found in the p% confidence interval  
  
Permutation and bootstrap hypothesis tests:  
  
* Genotype definitions include heterozygotes (1 mutated, 1 wild) as well as mutant (2 mutated) and wild (0 mutated)  
* Hypothesis test - assessment of how reasonable the observed data are, given that a specific (null) hypothesis is true  
	* Typically assesssed using one or more test statistics (each a single number that can be assessed for likelihood of "as extreme as")  
* Can assess using dcst.draw_perm_reps() functionality  
  
Linear regressions and pairs bootstraps - Michael Elowitz bacteria growth experiment:  
  
* Can use plt.semilogy() to see the y-axis on a log-curve; useful for data with a constant growth rate  
	* slope, intercept = np.polyfit(x, y, 1)  # the 1 being for a linear regression  
* Can use pairs bootstrap for finding the CI for regression data  
	* Resample each of the (x, y) pairs with replacement, then recalculate the slopes and intercepts  
    * dcst.draw_bs_pairs_linreg(x_data, y_data, size)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Import the dc_stat_think module as dcst
# import dc_stat_think as dcst
# Copy relevant functions from previous
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) / n
    return x, y

def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))

def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    bs_replicates = np.empty(size)
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    return bs_replicates

def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    data = np.concatenate((data1, data2))
    permuted_data = np.random.permutation(data)
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    return perm_sample_1, perm_sample_2

def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    perm_replicates = np.empty(size)
    for i in range(size):
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    return perm_replicates

def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    diff = np.mean(data_1) - np.mean(data_2)
    return diff

def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    inds = np.arange(len(x))
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    return bs_slope_reps, bs_intercept_reps




rawFish = pd.read_csv(myPath + "gandhi_et_al_bouts.csv", header=4)
bout_lengths_wt = rawFish.loc[rawFish["genotype"] == "wt", "bout_length"]
bout_lengths_mut = rawFish.loc[rawFish["genotype"] == "mut", "bout_length"]
bout_lengths_het = rawFish.loc[rawFish["genotype"] == "het", "bout_length"]


# Generate x and y values for plotting ECDFs
x_wt, y_wt = ecdf(bout_lengths_wt)
x_mut, y_mut = ecdf(bout_lengths_mut)

# Plot the ECDFs
_ = plt.plot(x_wt, y_wt, marker='.', linestyle='none')
_ = plt.plot(x_mut, y_mut, marker='.', linestyle='none')

# Make a legend, label axes, and show plot
_ = plt.legend(('wt', 'mut'))
_ = plt.xlabel('active bout length (min)')
_ = plt.ylabel('ECDF')
# plt.show()
plt.savefig("_dummyPy214.png", bbox_inches="tight")
plt.clf()



# Compute mean active bout length
mean_wt = np.mean(bout_lengths_wt)
mean_mut = np.mean(bout_lengths_mut)

# Draw bootstrap replicates
bs_reps_wt = draw_bs_reps(bout_lengths_wt, np.mean, size=10000)
bs_reps_mut = draw_bs_reps(bout_lengths_mut, np.mean, size=10000)

# Compute 95% confidence intervals
conf_int_wt = np.percentile(bs_reps_wt, [2.5, 97.5])
conf_int_mut = np.percentile(bs_reps_mut, [2.5, 97.5])

# Print the results
print("wt:  mean = {0:.3f} min., conf. int. = [{1:.1f}, {2:.1f}] min. \nmut: mean = {3:.3f} min., conf. int. = [{4:.1f}, {5:.1f}] min.".format(mean_wt, *conf_int_wt, mean_mut, *conf_int_mut))


# Compute the difference of means: diff_means_exp
diff_means_exp = np.mean(bout_lengths_het) - np.mean(bout_lengths_wt)

# Draw permutation replicates: perm_reps
perm_reps = draw_perm_reps(bout_lengths_het, bout_lengths_wt, diff_of_means, size=10000)

# Compute the p-value: p-val
p_val = sum(perm_reps >= diff_means_exp) / len(perm_reps)

# Print the result
print('p =', p_val)


# Concatenate arrays: bout_lengths_concat
bout_lengths_concat = np.concatenate((bout_lengths_wt, bout_lengths_het))

# Compute mean of all bout_lengths: mean_bout_length
mean_bout_length = np.mean(bout_lengths_concat)

# Generate shifted arrays
wt_shifted = bout_lengths_wt - np.mean(bout_lengths_wt) + mean_bout_length
het_shifted = bout_lengths_het - np.mean(bout_lengths_het) + mean_bout_length

# Compute 10,000 bootstrap replicates from shifted arrays
bs_reps_wt = draw_bs_reps(wt_shifted, np.mean, 10000)
bs_reps_het = draw_bs_reps(het_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_reps = bs_reps_het - bs_reps_wt

# Compute and print p-value: p
p = sum(bs_reps >= diff_means_exp) / len(bs_reps)
print('p-value =', p)


rawBacteria = pd.read_csv(myPath + "park_bacterial_growth.csv", header=2)
bac_area = rawBacteria["bacterial area (sq. microns)"]
t = rawBacteria["time (hr)"]

# Compute logarithm of the bacterial area: log_bac_area
log_bac_area = np.log(bac_area)

# Compute the slope and intercept: growth_rate, log_a0
growth_rate, log_a0 = np.polyfit(t, log_bac_area, 1)

# Draw 10,000 pairs bootstrap replicates: growth_rate_bs_reps, log_a0_bs_reps
growth_rate_bs_reps, log_a0_bs_reps = draw_bs_pairs_linreg(t, log_bac_area, size=10000)
    
# Compute confidence intervals: growth_rate_conf_int
growth_rate_conf_int = np.percentile(growth_rate_bs_reps, [2.5, 97.5])

# Print the result to the screen
print("Growth rate: {0:.4f} sq. m/hour\n95% conf int: [{1:.4f}, {2:.4f}] sq. m/hour".format(growth_rate, *growth_rate_conf_int))


# Plot data points in a semilog-y plot with axis labeles
_ = plt.semilogy(t, bac_area, marker='.', linestyle='none')

# Generate x-values for the bootstrap lines: t_bs
t_bs = np.array([0, 14])

# Plot the first 100 bootstrap lines
for i in range(100):
    y = np.exp(growth_rate_bs_reps[i] * t_bs + log_a0_bs_reps[i])
    _ = plt.semilogy(t_bs, y, linewidth=0.5, alpha=0.05, color="red")
    
# Label axes and show plot
_ = plt.xlabel('time (hr)')
_ = plt.ylabel('area (sq. m)')
# plt.show()
plt.savefig("_dummyPy215.png", bbox_inches="tight")
plt.clf()


```
  

**Example #1: Active Sleep Bout Length ECDF for Zebrafish (wild-type vs. mutatnt-type)**:  
![](_dummyPy214.png)

**Example #2: Bacteria Growth Rate with Regression Line**:  
![](_dummyPy215.png)
  
***
  
Chapter 2 - 2015 FINA Results  
  
Intro to swimming data and women's 200m:  
  
* Pools have ten lanes, zero indexed (0-9), with races typically held in lanes 1-8  
* Pools are 50m long and feature four styles: breast, butterfly, back, and free  
* Multiple rounds of events, with the fastest swimmers advancing to the next round  
* Data are freely available from omegatiming.com  
  
Do swimmers go faster in the finals?:  
  
* Most personal bests are achieved in the finals rather than in the semi-finals or the heats  
* Do individual female swimmers swim faster in the finals compared to the semifinals?  
	* To account for varying speeds by stroke, use (semi - final) / semi as the metric (fractional improvement)  
  
How does swimming performance decline over long events?:  
  
* The split is the amount of time taken to swim a single lap (50m) of the pool  
* The first split is typically much faster than the others due to the starting system  
* The last split is typically somewhat faster than the others due to no more need to conserve energy  
* Will evaluate "heats only" for 800m freestyle, while excluding the first 2 splits and the last 2 splits  
* The null hypothesis is that split times have no correlation, so the test-statistic will be the Pearson correlation  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Import the dc_stat_think module as dcst
# import dc_stat_think as dcst
# Copy relevant functions from previous
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) / n
    return x, y

def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))

def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    bs_replicates = np.empty(size)
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    return bs_replicates

def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    data = np.concatenate((data1, data2))
    permuted_data = np.random.permutation(data)
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    return perm_sample_1, perm_sample_2

def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    perm_replicates = np.empty(size)
    for i in range(size):
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    return perm_replicates

def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    diff = np.mean(data_1) - np.mean(data_2)
    return diff

def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    inds = np.arange(len(x))
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    return bs_slope_reps, bs_intercept_reps

def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    corr_mat = np.corrcoef(x, y)
    return corr_mat[0,1]


rawSwim = pd.read_csv(myPath + "2015_FINA.csv", header=4)


subUse = (rawSwim["gender"] == "M") & (rawSwim["distance"] == 200) & (rawSwim["stroke"] == "FREE") & (rawSwim["round"] == "PRE") & (rawSwim["splitdistance"] == 200)

mens_200_free_heats = rawSwim.loc[subUse, "cumswimtime"]

# Generate x and y values for ECDF: x, y
x, y = ecdf(mens_200_free_heats)

# Plot the ECDF as dots
plt.plot(x, y, marker=".", linestyle="none")

# Label axes and show plot
plt.xlabel("time (s)")
plt.ylabel("ECDF")
# plt.show()
plt.savefig("_dummyPy216.png", bbox_inches="tight")
plt.clf()


# Compute mean and median swim times
mean_time = np.mean(mens_200_free_heats)
median_time = np.median(mens_200_free_heats)

# Draw 10,000 bootstrap replicates of the mean and median
bs_reps_mean = draw_bs_reps(mens_200_free_heats, np.mean, 10000)
bs_reps_median = draw_bs_reps(mens_200_free_heats, np.median, 10000)

# Compute the 95% confidence intervals
conf_int_mean = np.percentile(bs_reps_mean, [2.5, 97.5])
conf_int_median = np.percentile(bs_reps_median, [2.5, 97.5])

# Print the result to the screen
print("mean time: {0:.2f} sec.\n95% conf int of mean: [{1:.2f}, {2:.2f}] sec.\n\nmedian time: {3:.2f} sec.\n95% conf int of median: [{4:.2f}, {5:.2f}] sec.".format(mean_time, *conf_int_mean, median_time, *conf_int_median))


# Data are organized such that the same swimmer is at the same position in final_times and semi_times
isDone = rawSwim["swimtime"] == rawSwim["cumswimtime"]
subFinal = (rawSwim["distance"] <= 200) & (rawSwim["round"] == "FIN") & (rawSwim["stroke"] != "MEDLEY") & (isDone)
subSemi = (rawSwim["distance"] <= 200) & (rawSwim["round"] == "SEM") & (rawSwim["stroke"] != "MEDLEY") & (isDone)

finalData = rawSwim.loc[subFinal, ["athleteid", "stroke", "distance", "swimtime"]]
fullSemiData = rawSwim.loc[subSemi, ["athleteid", "stroke", "distance", "swimtime"]]
fullSemiData.columns = ["athleteid", "stroke", "distance", "semitime"]
fullData = finalData.merge(fullSemiData)

final_times = fullData["swimtime"]
semi_times = fullData["semitime"]

# Compute fractional difference in time between finals and semis
f = (semi_times - final_times) / semi_times

# Generate x and y values for the ECDF: x, y
x, y = ecdf(f)

# Make a plot of the ECDF
plt.plot(x, y, marker=".", linestyle="none")

# Label axes and show plot
_ = plt.xlabel('f')
_ = plt.ylabel('ECDF')
plt.savefig("_dummyPy217.png", bbox_inches="tight")
# plt.show()
plt.clf()


# Mean fractional time difference: f_mean
f_mean = np.mean(f)

# Get bootstrap reps of mean: bs_reps
bs_reps = draw_bs_reps(f, np.mean, size=10000)

# Compute confidence intervals: conf_int
conf_int = np.percentile(bs_reps, [2.5, 97.5])

# Report
print("mean frac. diff.: {0:.5f}\n95% conf int of mean frac. diff.: [{1:.5f}, {2:.5f}]".format(f_mean, *conf_int))


def swap_random(a, b):
    """Randomly swap entries in two arrays."""
    # Indices to swap
    swap_inds = np.random.random(size=len(a)) < 0.5
    
    # Make copies of arrays a and b for output
    a_out = np.copy(a)
    b_out = np.copy(b)
    
    # Swap values
    a_out[swap_inds] = b[swap_inds]
    b_out[swap_inds] = a[swap_inds]
    
    return a_out, b_out

# Set up array of permutation replicates
perm_reps = np.empty(1000)

for i in range(1000):
    # Generate a permutation sample
    semi_perm, final_perm = swap_random(semi_times, final_times)
    
    # Compute f from the permutation sample
    f = (semi_perm - final_perm) / semi_perm
    
    # Compute and store permutation replicate
    perm_reps[i] = np.mean(f)

# Compute and print p-value
print('p =', sum(perm_reps >= f_mean) / 1000)


# The arrays are organized such that splits[i,j] is the split time for swimmer i for split_number[j]
# Create data using female 800m from heats (athlete 100935 has only 1 split; delete)
subSplits = (rawSwim["distance"] == 800) & (rawSwim["round"] == "PRE") & (rawSwim["stroke"] != "MEDLEY") & (rawSwim["gender"] == "F") & (rawSwim["athleteid"] != 100935)
splitData = rawSwim.loc[subSplits, ["athleteid", "split", "splitswimtime"]]

# Eliminate the first two splits and the last two splits and sort
splitUse = splitData.loc[[x not in [1, 2, 15, 16] for x in splitData["split"]], :].sort_values(["athleteid", "split"])

# Create the list splits with each swimmers splits being a list
splits = []
swimmerList = splitUse["athleteid"].unique()
for swimmer in swimmerList:
    splits.append(list(splitUse.loc[splitUse["athleteid"] == swimmer, "splitswimtime"]))

# Plot the splits for each swimmer
split_number = np.array(sorted(list(splitUse["split"].unique())))
for swimmer in splits:
    _ = plt.plot(split_number, swimmer, lw=1, color='lightgray')
# plt.show()
plt.clf()

# Compute the mean split times
mean_splits = np.mean(splits, axis=0)

# Plot the mean split times
plt.plot(split_number, mean_splits, marker=".", linewidth=3, markersize=12)

# Label axes and show plot
_ = plt.xlabel('split number')
_ = plt.ylabel('split time (s)')
# plt.show()
plt.savefig("_dummyPy218.png", bbox_inches="tight")
plt.clf()


# Perform regression
slowdown, split_3 = np.polyfit(split_number, mean_splits, 1)

# Compute pairs bootstrap
bs_reps, _ = draw_bs_pairs_linreg(split_number, mean_splits, size=10000)

# Compute confidence interval
conf_int = np.percentile(bs_reps, [2.5, 97.5])

# Plot the data with regressions line
_ = plt.plot(split_number, mean_splits, marker='.', linestyle='none')
_ = plt.plot(split_number, slowdown * split_number + split_3, '-')

# Label axes and show plot
_ = plt.xlabel('split number')
_ = plt.ylabel('split time (s)')
# plt.show()
plt.savefig("_dummyPy219.png", bbox_inches="tight")
plt.clf()


# Print the slowdown per split
print("mean slowdown: {0:.3f} sec./split\n95% conf int of mean slowdown: [{1:.3f}, {2:.3f}]sec./split".format(slowdown, *conf_int))


# Observed correlation
rho = pearson_r(split_number, mean_splits)

# Initialize permutation reps
perm_reps_rho = np.empty(10000)

# Make permutation reps
for i in range(10000):
    # Scramble the split number array
    scrambled_split_number = np.random.permutation(split_number)
    # Compute the Person correlation coefficient
    perm_reps_rho[i] = pearson_r(scrambled_split_number, mean_splits)
    
# Compute and print p-value
p_val = sum(perm_reps_rho >= rho) / len(perm_reps_rho)
print('p =', p_val)

```
  
**Example #1: ECDF for Men's 200m Free Heats**:  
![](_dummyPy216.png)

**Example #2: ECDF for Delta Time (Semi vs Final) - Individual 200m Swims**:
![](_dummyPy217.png)

**Example #3: Split Times for Women's 800m Freestyle Prelims**:  
![](_dummyPy218.png)

**Example #4: Split Times and Regression Line for Women's 800m Freestyle Prelims**:  
![](_dummyPy219.png)

  
  
***
  
Chapter 3 - Current Controversy  
  
Introduction to the current controversy (2013 championships):  
  
* Anecdotal evidence of a swirling current in the 2013 pool  
* Currents tend to be strongest (fastest) on the outside, rather than the insides  
* Lane assignments go from the center out (faster qualifier is in lane 4, with second fastest in lane 5)  
* Investigate probabilities of winning medals for lanes 1-3 and lanes 6-8  
* Investigate improvement in speeds (times) for swimmers moving from low to high number lanes  
  
Zigzag effect - looking at the longest race (1500m):  
  
* Do swimmers in high lanes have faster splits with the current rather than against it?  
  
Recap of swimming analysis:  
  
* Exploratory Data Analysis / Sharpen the Question  
* Optimal Parameter Calculation with Confidence Interval  
* Hypothesis test  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Import the dc_stat_think module as dcst
# import dc_stat_think as dcst
# Copy relevant functions from previous
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) / n
    return x, y

def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))

def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    bs_replicates = np.empty(size)
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    return bs_replicates

def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    data = np.concatenate((data1, data2))
    permuted_data = np.random.permutation(data)
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    return perm_sample_1, perm_sample_2

def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    perm_replicates = np.empty(size)
    for i in range(size):
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    return perm_replicates

def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    diff = np.mean(data_1) - np.mean(data_2)
    return diff

def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    inds = np.arange(len(x))
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    return bs_slope_reps, bs_intercept_reps

def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    corr_mat = np.corrcoef(x, y)
    return corr_mat[0,1]



rawSwim = pd.read_csv(myPath + "2013_FINA.csv", header=4)

# Pull swimmers who had a 50m semi and 50m final for the same stroke in a different (high vs. low) lane type
subFull50 = (rawSwim["distance"] == 50) & ((rawSwim["round"] == "FIN") | (rawSwim["round"] == "SEM"))
swimFull50 = rawSwim.loc[subFull50, ["athleteid", "eventid", "round", "distance", "stroke", "lane", "swimtime", "gender"]]

finLow = (swimFull50["round"] == "FIN") & ([x in [1, 2, 3] for x in swimFull50["lane"]])
finHigh = (swimFull50["round"] == "FIN") & ([x in [6, 7, 8] for x in swimFull50["lane"]])
semLow = (swimFull50["round"] == "SEM") & ([x in [1, 2, 3] for x in swimFull50["lane"]])
semHigh = (swimFull50["round"] == "SEM") & ([x in [6, 7, 8] for x in swimFull50["lane"]])

# High finals with low semifinals or low finals with high semifinals
highLow = swimFull50.loc[finHigh, ["athleteid", "stroke"]].merge(swimFull50.loc[semLow, ["athleteid", "stroke"]])
lowHigh = swimFull50.loc[finLow, ["athleteid", "stroke"]].merge(swimFull50.loc[semHigh, ["athleteid", "stroke"]])

# Pull the relevant data
allMixes = highLow.append(lowHigh).sort_values(["athleteid", "stroke"])
allMixSwims = allMixes.merge(swimFull50)

# Make the relevant arrays (already sorted by swimmer/event, so can just separate)
swimtime_high_lanes = np.array(allMixSwims.loc[[x in [6, 7, 8] for x in allMixSwims["lane"]], "swimtime"])
swimtime_low_lanes = np.array(allMixSwims.loc[[x in [1, 2, 3] for x in allMixSwims["lane"]], "swimtime"])


# The swim times are stored in the Numpy arrays swimtime_high_lanes and swimtime_low_lanes. Entry i in the respective arrays are for the same swimmer in the same event.
# Compute the fractional improvement of being in high lane: f
f = (swimtime_low_lanes - swimtime_high_lanes) / swimtime_low_lanes

# Make x and y values for ECDF: x, y
x, y = ecdf(f)

# Plot the ECDFs as dots
plt.plot(x, y, marker=".", linestyle="none")

# Label the axes and show the plot
plt.xlabel("f")
plt.ylabel("ECDF")
# plt.show()
plt.savefig("_dummyPy220.png", bbox_inches="tight")
plt.clf()


# Compute the mean difference: f_mean
f_mean = np.mean(f)

# Draw 10,000 bootstrap replicates: bs_reps
bs_reps = draw_bs_reps(f, np.mean, size=10000)

# Compute 95% confidence interval: conf_int
conf_int = np.percentile(bs_reps, [2.5, 97.5])

# Print the result
print("mean frac. diff.: {0:.5f}\n95% conf int of mean frac. diff.: [{1:.5f}, {2:.5f}]".format(f_mean, *conf_int))


# Shift f: f_shift
f_shift = f - f_mean

# Draw 100,000 bootstrap replicates of the mean: bs_reps
bs_reps = draw_bs_reps(f_shift, np.mean, size=100000)

# Compute and report the p-value
p_val = sum(bs_reps >= f_mean) / 100000
print('p =', p_val)



# Create the high and low lanes for 2015 data
rawSwim15 = pd.read_csv(myPath + "2015_FINA.csv", header=4)

# Pull swimmers who had a 50m semi and 50m final for the same stroke in a different (high vs. low) lane type
subFull50_15 = (rawSwim15["distance"] == 50) & ((rawSwim15["round"] == "FIN") | (rawSwim15["round"] == "SEM"))
swimFull50_15 = rawSwim15.loc[subFull50_15, ["athleteid", "eventid", "round", "distance", "stroke", "lane", "swimtime", "gender"]]

finLow_15 = (swimFull50_15["round"] == "FIN") & ([x in [1, 2, 3] for x in swimFull50_15["lane"]])
finHigh_15 = (swimFull50_15["round"] == "FIN") & ([x in [6, 7, 8] for x in swimFull50_15["lane"]])
semLow_15 = (swimFull50_15["round"] == "SEM") & ([x in [1, 2, 3] for x in swimFull50_15["lane"]])
semHigh_15 = (swimFull50_15["round"] == "SEM") & ([x in [6, 7, 8] for x in swimFull50_15["lane"]])

# High finals with low semifinals or low finals with high semifinals
highLow_15 = swimFull50_15.loc[finHigh_15, ["athleteid", "stroke"]].merge(swimFull50_15.loc[semLow_15, ["athleteid", "stroke"]])
lowHigh_15 = swimFull50_15.loc[finLow_15, ["athleteid", "stroke"]].merge(swimFull50_15.loc[semHigh_15, ["athleteid", "stroke"]])

# Pull the relevant data
allMixes_15 = highLow_15.append(lowHigh_15).sort_values(["athleteid", "stroke"])
allMixSwims_15 = allMixes_15.merge(swimFull50_15)

# Make the relevant arrays (already sorted by swimmer/event, so can just separate)
swimtime_high_lanes_15 = np.array(allMixSwims_15.loc[[x in [6, 7, 8] for x in allMixSwims_15["lane"]], "swimtime"])
swimtime_low_lanes_15 = np.array(allMixSwims_15.loc[[x in [1, 2, 3] for x in allMixSwims_15["lane"]], "swimtime"])



# Compute f and its mean
f = (swimtime_low_lanes_15 - swimtime_high_lanes_15) / swimtime_low_lanes_15
f_mean = np.mean(f)

# Draw 10,000 bootstrap replicates
bs_reps = draw_bs_reps(f, np.mean, size=10000)

# Compute 95% confidence interval
conf_int = np.percentile(bs_reps, [2.5, 97.5])

# Shift f
f_shift = f - f_mean

# Draw 100,000 bootstrap replicates of the mean
bs_reps = draw_bs_reps(f_shift, np.mean, size=100000)

# Compute the p-value
p_val = sum(bs_reps >= f_mean) / 100000

# Print the results
print("mean frac. diff.: {0:.5f}\n95% conf int of mean frac. diff.: [{1:.5f}, {2:.5f}]\np-value: {3:.5f}".format(f_mean, *conf_int,p_val))



# I have already calculated the mean fractional differences for the 2013 and 2015 Worlds for you, and they are stored in f_13 and f_15. The corresponding lane numbers are in the array lanes
# Plot the the fractional difference for 2013 and 2015
# Use only races longer than 200m, discarding splits 1-2 and the final 2 splits
rawSwim15 = pd.read_csv(myPath + "2015_FINA.csv", header=4)

subFullLong_15 = (rawSwim15["distance"] > 200) & ([x not in [50, 100] for x in rawSwim15["splitdistance"]]) & ([x >= 100 for x in (rawSwim15["distance"] - rawSwim15["splitdistance"])]) & ((rawSwim15["round"] == "FIN") | (rawSwim15["round"] == "SEM"))

keySwim15 = rawSwim15.loc[subFullLong_15, ["eventid", "round", "split", "lane", "splitswimtime"]]
keySwim15["oddEven"] = keySwim15["split"] % 2
tempKey15 = keySwim15.pivot_table(index=["eventid", "round", "lane"], columns="oddEven", values="splitswimtime", aggfunc=np.mean)
tempKey15["frac"] = 2 * (tempKey15[1] - tempKey15[0]) / (tempKey15[1] + tempKey15[0])
f_15 = np.array(tempKey15[["frac"]].groupby("lane").mean())
f_15 = np.array([float(x) for x in f_15])


rawSwim13 = pd.read_csv(myPath + "2013_FINA.csv", header=4)

subFullLong_13 = (rawSwim13["distance"] > 200) & ([x not in [50, 100] for x in rawSwim13["splitdistance"]]) & ([x >= 100 for x in (rawSwim13["distance"] - rawSwim13["splitdistance"])]) & ((rawSwim13["round"] == "FIN") | (rawSwim13["round"] == "SEM"))

keySwim13 = rawSwim13.loc[subFullLong_13, ["eventid", "round", "split", "lane", "splitswimtime"]]
keySwim13["oddEven"] = keySwim13["split"] % 2
tempKey13 = keySwim13.pivot_table(index=["eventid", "round", "lane"], columns="oddEven", values="splitswimtime", aggfunc=np.mean)
tempKey13["frac"] = 2 * (tempKey13[1] - tempKey13[0]) / (tempKey13[1] + tempKey13[0])
f_13 = np.array(tempKey13[["frac"]].groupby("lane").mean())
f_13 = np.array([float(x) for x in f_13])

lanes = np.arange(1, 9)
plt.plot(lanes, f_13, marker=".", markersize=12, linestyle="none")
plt.plot(lanes, f_15, marker=".", markersize=12, linestyle="none")

# Add a legend
_ = plt.legend((2013, 2015))

# Label axes and show plot
plt.xlabel("lane")
plt.ylabel('frac. diff. (odd - even)')
# plt.show()
plt.savefig("_dummyPy221.png", bbox_inches="tight")
plt.clf()


# Compute the slope and intercept of the frac diff/lane curve
slope, intercept = np.polyfit(lanes, f_13, 1)

# Compute bootstrap replicates
bs_reps_slope, bs_reps_int = draw_bs_pairs_linreg(lanes, f_13, size=10000)

# Compute 95% confidence interval of slope
conf_int = np.percentile(bs_reps_slope, [2.5, 97.5])

# Print slope and confidence interval
print("slope: {0:.5f} per lane\n95% conf int: [{1:.5f}, {2:.5f}] per lane".format(slope, *conf_int))

# x-values for plotting regression lines
x = np.array([1, 8])

# Plot 100 bootstrap replicate lines
for i in range(100):
    _ = plt.plot(x, bs_reps_slope[i] * x + bs_reps_int[i], color='red', alpha=0.2, linewidth=0.5)
    
# Update the plot
plt.draw()
# plt.show()
plt.savefig("_dummyPy222.png", bbox_inches="tight")
plt.clf()

# Compute observed correlation: rho
rho = pearson_r(lanes, f_13)

# Initialize permutation reps: perm_reps_rho
perm_reps_rho = np.empty(10000)

# Make permutation reps
for i in range(10000):
    scrambled_lanes = np.random.permutation(lanes)
    perm_reps_rho[i] = pearson_r(scrambled_lanes, f_13)
    
# Compute and print p-value
p_val = sum(perm_reps_rho >= rho) / 10000
print('p =', p_val)

```
  
**Example #1: ECDF for Proportional Change in Time for High vs. Low Lanes (50m)**:  
![](_dummyPy220.png)

**Example #2: Mean Zig-Zag Effect by Lane (2013 vs. 2015) - 400+m Swims**:  
![](_dummyPy221.png)

**Example #3: Bootstrap Replicate Regressions for Zig-Zag Effect vs. Lane Number**:  
![](_dummyPy222.png)
  
***
  
Chapter 4 - Statistical Seismology and the Parkfield Experiment  
  
Introduction to statistical seismology and the Parkfield experiment:  
  
* Parkfield region is part of the San Andreas fault, and is of particular interest to seismologists  
* A "location parameter" defines the shift of an exponential distribution along the x-axis (called the "completeness threshhold" in seismology, and denoted as m-t)  
	* "Completeness threshhold" is the minimum magnitude at which all earthquakes can be reliably measured  
* The Gutenberg-Richter law says that the magnitudes of earthquakes in a given area are exponentially distributed  
* The b-value is np.log(10) times the mean of the earthquake magnitude in a particular area  
  
Timing of major earthquakes and the Parkfield sequence:  
  
* Two common textbook models for earthquake timing - exponential/Poisson, and Gaussian  
* In general, if looking at all eartquakes in a stable zone, the ECDF appears exponential (no correlations)  
* However, if looking at all of the 8+ magnitude earthquakes, there is evidence that they may occur about every 200 years (might be Gaussian)  
	* Can use the formal=True argument of the dcst.ecdf() function to plot the lines between the points if they are too sparse to form a pleasing visual curve)  
    * Suggestion to use formals for any dataset with less than 20 data points, though it is always OK to use formals  
  
How are Parkfield earthquake times distributed?:  
  
* Running the hypothesis tests for normality based on Nankai megathrust earthquakes  
* The K-S (Kolmogorov-Smirnov) statistic can be useful for assessing how closely data resemble a Gaussian distribution  
	* The K-S is the maximum distance between a theoretical normal ECDF and the observed ECDF for a set of data  
    * The K-S statistic will always be at a corner of the actual ECDF, either a convex or a concave corner  
* Key part of the test is to run K-S on hacker stats pulled using the same number of observed data points, but from an np.random.normal() distribution  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Import the dc_stat_think module as dcst
# import dc_stat_think as dcst
# Copy relevant functions from previous
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) / n
    return x, y

def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))

def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    bs_replicates = np.empty(size)
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    return bs_replicates

def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    data = np.concatenate((data1, data2))
    permuted_data = np.random.permutation(data)
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    return perm_sample_1, perm_sample_2

def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    perm_replicates = np.empty(size)
    for i in range(size):
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    return perm_replicates

def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    diff = np.mean(data_1) - np.mean(data_2)
    return diff

def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    inds = np.arange(len(x))
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    return bs_slope_reps, bs_intercept_reps

def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    corr_mat = np.corrcoef(x, y)
    return corr_mat[0,1]

def _ecdf_formal(x, data):
    output = np.empty_like(x)
    for i, x_val in enumerate(x):
        j = 0
        while j < len(data) and x_val >= data[j]:
            j += 1
        output[i] = j
    return output / len(data)

def _convert_data(data, inf_ok=False, min_len=1):
    # If it's scalar, convert to array
    if np.isscalar(data):
        data = np.array([data], dtype=np.float)
    # Convert data to NumPy array
    data = np.array(data, dtype=np.float)
    # Make sure it is 1D
    if len(data.shape) != 1:
        raise RuntimeError('Input must be a 1D array or Pandas series.')
    # Remove NaNs
    data = data[~np.isnan(data)]
    # Check for infinite entries
    if not inf_ok and np.isinf(data).any():
        raise RuntimeError('All entries must be finite.')
    # Check to minimal length
    if len(data) < min_len:
        raise RuntimeError('Array must have at least {0:d} non-NaN entries.'.format(min_len))
    return data

def ecdf_formal(x, data):
    if np.isscalar(x):
        return_scalar = True
    else:
        return_scalar = False
    if np.isnan(x).any():
        raise RuntimeError('Input cannot have NaNs.')
    x = _convert_data(x, inf_ok=True)
    data = _convert_data(data, inf_ok=True)
    out = _ecdf_formal(x, np.sort(data))
    if return_scalar:
        return out[0]
    return out

def _ecdf_dots(data):
    return np.sort(data), np.arange(1, len(data)+1) / len(data)

def _ecdf_formal_for_plotting(data, buff=0.1, min_x=None, max_x=None):
    x, y = _ecdf_dots(data)
    # Set defaults for min and max tails
    if min_x is None:
        min_x = x[0] - (x[-1] - x[0])*buff
    if max_x is None:
        max_x = x[-1] + (x[-1] - x[0])*buff
    # Set up output arrays
    x_formal = np.empty(2*(len(x) + 1))
    y_formal = np.empty(2*(len(x) + 1))
    # y-values for steps
    y_formal[:2] = 0
    y_formal[2::2] = y
    y_formal[3::2] = y
    # x- values for steps
    x_formal[0] = min_x
    x_formal[1] = x[0]
    x_formal[2::2] = x
    x_formal[3:-1:2] = x[1:]
    x_formal[-1] = max_x
    return x_formal, y_formal

def fancyEcdf(data, formal=False, buff=0.1, min_x=None, max_x=None):
    if formal and buff is None and (min_x is None or max_x is None):
        raise RunetimeError('If `buff` is None, `min_x` and `max_x` must be specified.')
    data = _convert_data(data)
    if formal:
        return _ecdf_formal_for_plotting(data, buff=buff, min_x=min_x, max_x=max_x)
    else:
        return _ecdf_dots(data)



# Read in the Parkfield data and calculate the 1950-2017 magnitude data
rawParkfield = pd.read_csv(myPath + "parkfield_earthquakes_1950-2017.csv", header=2)
rawParkfield["mag"].describe()
rawParkfield["magType"].value_counts()
mags = rawParkfield["mag"]

# Use what was provided in the course as intra-eartquake gaps for 'major' Parkfield earthquakes
time_gap = np.array([ 24.06570842 , 20.07665982 , 21.01848049 , 12.24640657 , 32.05475702 , 38.2532512 ])


# When you do it this time, though, take a shortcut in generating the ECDF. You may recall that putting an asterisk before an argument in a function splits what follows into separate arguments.
# Since dcst.ecdf() returns two values, we can pass them as the x, y positional arguments to plt.plot() as plt.plot(*dcst.ecdf(data_you_want_to_plot)).
# Make the plot
plt.plot(*ecdf(mags), marker=".", linestyle="none")

# Label axes and show plot
plt.xlabel("magnitude")
plt.ylabel("ECDF")
# plt.show()
plt.savefig("_dummyPy223.png", bbox_inches="tight")
plt.clf()


def b_value(mags, mt, perc=[2.5, 97.5], n_reps=None):
    """Compute the b-value and optionally its confidence interval."""
    # Extract magnitudes above completeness threshold: m
    m = mags[mags >= mt]
    # Compute b-value: b
    b = (np.mean(m) - mt) * np.log(10)
    # Draw bootstrap replicates
    if n_reps is None:
        return b
    else:
        m_bs_reps = draw_bs_reps(m, np.mean, size=n_reps)
        # Compute b-value from replicates: b_bs_reps
        b_bs_reps = (m_bs_reps - mt) * np.log(10)
        # Compute confidence interval: conf_int
        conf_int = np.percentile(b_bs_reps, perc)
        return b, conf_int

mt = 3

# Compute b-value and 95% confidence interval
b, conf_int = b_value(mags, mt, perc=[2.5, 97.5], n_reps=10000)

# Generate samples to for theoretical ECDF
m_theor = np.random.exponential(b/np.log(10), size=100000) + mt

# Plot the theoretical CDF
_ = plt.plot(*ecdf(m_theor))

# Plot the ECDF (slicing mags >= mt)
_ = plt.plot(*ecdf(mags[mags >= mt]), marker='.', linestyle='none')

# Pretty up and show the plot
_ = plt.xlabel('magnitude')
_ = plt.ylabel('ECDF')
_ = plt.xlim(2.8, 6.2)
# plt.show()
plt.savefig("_dummyPy224.png", bbox_inches="tight")
plt.clf()

# Report the results
print("b-value: {0:.2f}\n95% conf int: [{1:.2f}, {2:.2f}]".format(b, *conf_int))


# Compute the mean time gap: mean_time_gap
mean_time_gap = np.mean(time_gap)

# Standard deviation of the time gap: std_time_gap
std_time_gap = np.std(time_gap)

# Generate theoretical Exponential distribution of timings: time_gap_exp
time_gap_exp = np.random.exponential(mean_time_gap, size=10000)

# Generate theoretical Normal distribution of timings: time_gap_norm
time_gap_norm = np.random.normal(mean_time_gap, std_time_gap, size=10000)

# Plot theoretical CDFs
_ = plt.plot(*ecdf(time_gap_exp))
_ = plt.plot(*ecdf(time_gap_norm))

# Plot Parkfield ECDF
_ = plt.plot(*fancyEcdf(time_gap, formal=True, min_x=-10, max_x=50))

# Add legend
_ = plt.legend(('Exp.', 'Norm.'), loc='upper left')

# Label axes, set limits and show plot
_ = plt.xlabel('time gap (years)')
_ = plt.ylabel('ECDF')
_ = plt.xlim(-10, 50)
# plt.show()
plt.savefig("_dummyPy225.png", bbox_inches="tight")
plt.clf()


today = 2017.8
last_quake = 2004.74

# Draw samples from the Exponential distribution: exp_samples
exp_samples = np.random.exponential(mean_time_gap, size=100000)

# Draw samples from the Normal distribution: norm_samples
norm_samples = np.random.normal(mean_time_gap, std_time_gap, size=100000)

# No earthquake as of today, so only keep samples that are long enough
exp_samples = exp_samples[exp_samples > today - last_quake]
norm_samples = norm_samples[norm_samples > today - last_quake]

# Compute the confidence intervals with medians
conf_int_exp = np.percentile(exp_samples, [2.5, 50, 97.5]) + last_quake
conf_int_norm = np.percentile(norm_samples, [2.5, 50, 97.5]) + last_quake

# Print the results
print('Exponential:', conf_int_exp)
print('     Normal:', conf_int_norm)


def ks_stat(data1, data2):
    # Compute ECDF from data: x, y
    x, y = ecdf(data1)
    # Compute corresponding values of the target CDF
    cdf = ecdf_formal(x, data2)
    # Compute distances between concave corners and CDF
    D_top = y - cdf
    # Compute distance between convex corners and CDF
    D_bottom = cdf - y + 1/len(data1)
    return np.max((D_top, D_bottom))


def draw_ks_reps(n, f, args=(), size=10000, n_reps=10000):
    # Generate samples from target distribution
    x_f = f(*args, size=size)
    # Initialize K-S replicates
    reps = np.empty(n_reps)
    # Draw replicates
    for i in range(n_reps):
        # Draw samples for comparison
        x_samp = f(*args, size=n)
        # Compute K-S statistic
        reps[i] = ks_stat(x_samp, x_f)
    return reps


# Draw target distribution: x_f
x_f = np.random.exponential(mean_time_gap, size=10000)

# Compute K-S stat: d
d = ks_stat(time_gap, x_f)

# Draw K-S replicates: reps
reps = draw_ks_reps(len(time_gap), f=np.random.exponential, args=(mean_time_gap,), size=1000, n_reps=1000)

# Compute and print p-value
p_val = sum(reps >= d) / 1000
print('p =', p_val)

```
  
  
**Example #1: ECDF of Parkfield Earthquake Magnitudes**:  
![](_dummyPy223.png)

**Example #2: ECDF of Parkfield Earthquake Magnitudes vs. Theoretical (Above Completeness Threshold mt=3)**:  
![](_dummyPy224.png)

**Example #3: Normal vs. Exponential for Time Gaps of Major Parkfield Earthquakes**:  
![](_dummyPy225.png)
  
  
***
  
Chapter 5 - Earthquakes and Oil Mining in Oklahoma  
  
Variations in earthquake frequency and seismicity:  
  
* Geological events can change seismicity, in addition to traditional tectonic plate movements  
	* Mt St Helens is an example - eruptions lead to more frequent earthquakes due to movement of magma  
* Hydraulic fracturing (fracking) may impact seismicity by way of wastewater injections  
  
Earthquake magnitudes in Oklahoma:  
  
* Analogous to the Mt St Helens example, where the ECDF look very different (many more large earthquakes) around the time of the 2004 eruption  
* Analysis is to look at the frequency of earthquakes before/after the beginning of wastewater injections  
  
Closing comments:  
  
* Swimmers do not typically swim faster in the finals  
* There was a current in the 2013 FINA swimming pool  
* Parkfield earthquakes follow a Poisson process  
* OK has more earthquakes but a similar frequency distribution after the start of fracking  
* Melatonin helps zebrafish to sleep  
* Statistical inference pipeline (EDA - Parameter estimation - Hypothesis testing)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Import the dc_stat_think module as dcst
# import dc_stat_think as dcst
# Copy relevant functions from previous
def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) / n
    return x, y

def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))

def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    bs_replicates = np.empty(size)
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    return bs_replicates

def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    data = np.concatenate((data1, data2))
    permuted_data = np.random.permutation(data)
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    return perm_sample_1, perm_sample_2

def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    perm_replicates = np.empty(size)
    for i in range(size):
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    return perm_replicates

def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    diff = np.mean(data_1) - np.mean(data_2)
    return diff

def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    inds = np.arange(len(x))
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    return bs_slope_reps, bs_intercept_reps

def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    corr_mat = np.corrcoef(x, y)
    return corr_mat[0,1]

def _ecdf_formal(x, data):
    output = np.empty_like(x)
    for i, x_val in enumerate(x):
        j = 0
        while j < len(data) and x_val >= data[j]:
            j += 1
        output[i] = j
    return output / len(data)

def _convert_data(data, inf_ok=False, min_len=1):
    # If it's scalar, convert to array
    if np.isscalar(data):
        data = np.array([data], dtype=np.float)
    # Convert data to NumPy array
    data = np.array(data, dtype=np.float)
    # Make sure it is 1D
    if len(data.shape) != 1:
        raise RuntimeError('Input must be a 1D array or Pandas series.')
    # Remove NaNs
    data = data[~np.isnan(data)]
    # Check for infinite entries
    if not inf_ok and np.isinf(data).any():
        raise RuntimeError('All entries must be finite.')
    # Check to minimal length
    if len(data) < min_len:
        raise RuntimeError('Array must have at least {0:d} non-NaN entries.'.format(min_len))
    return data

def ecdf_formal(x, data):
    if np.isscalar(x):
        return_scalar = True
    else:
        return_scalar = False
    if np.isnan(x).any():
        raise RuntimeError('Input cannot have NaNs.')
    x = _convert_data(x, inf_ok=True)
    data = _convert_data(data, inf_ok=True)
    out = _ecdf_formal(x, np.sort(data))
    if return_scalar:
        return out[0]
    return out

def _ecdf_dots(data):
    return np.sort(data), np.arange(1, len(data)+1) / len(data)

def _ecdf_formal_for_plotting(data, buff=0.1, min_x=None, max_x=None):
    x, y = _ecdf_dots(data)
    # Set defaults for min and max tails
    if min_x is None:
        min_x = x[0] - (x[-1] - x[0])*buff
    if max_x is None:
        max_x = x[-1] + (x[-1] - x[0])*buff
    # Set up output arrays
    x_formal = np.empty(2*(len(x) + 1))
    y_formal = np.empty(2*(len(x) + 1))
    # y-values for steps
    y_formal[:2] = 0
    y_formal[2::2] = y
    y_formal[3::2] = y
    # x- values for steps
    x_formal[0] = min_x
    x_formal[1] = x[0]
    x_formal[2::2] = x
    x_formal[3:-1:2] = x[1:]
    x_formal[-1] = max_x
    return x_formal, y_formal

def fancyEcdf(data, formal=False, buff=0.1, min_x=None, max_x=None):
    if formal and buff is None and (min_x is None or max_x is None):
        raise RunetimeError('If `buff` is None, `min_x` and `max_x` must be specified.')
    data = _convert_data(data)
    if formal:
        return _ecdf_formal_for_plotting(data, buff=buff, min_x=min_x, max_x=max_x)
    else:
        return _ecdf_dots(data)

def b_value(mags, mt, perc=[2.5, 97.5], n_reps=None):
    """Compute the b-value and optionally its confidence interval."""
    # Extract magnitudes above completeness threshold: m
    m = mags[mags >= mt]
    # Compute b-value: b
    b = (np.mean(m) - mt) * np.log(10)
    # Draw bootstrap replicates
    if n_reps is None:
        return b
    else:
        m_bs_reps = draw_bs_reps(m, np.mean, size=n_reps)
        # Compute b-value from replicates: b_bs_reps
        b_bs_reps = (m_bs_reps - mt) * np.log(10)
        # Compute confidence interval: conf_int
        conf_int = np.percentile(b_bs_reps, perc)
        return b, conf_int

def ks_stat(data1, data2):
    # Compute ECDF from data: x, y
    x, y = ecdf(data1)
    # Compute corresponding values of the target CDF
    cdf = ecdf_formal(x, data2)
    # Compute distances between concave corners and CDF
    D_top = y - cdf
    # Compute distance between convex corners and CDF
    D_bottom = cdf - y + 1/len(data1)
    return np.max((D_top, D_bottom))


def draw_ks_reps(n, f, args=(), size=10000, n_reps=10000):
    # Generate samples from target distribution
    x_f = f(*args, size=size)
    # Initialize K-S replicates
    reps = np.empty(n_reps)
    # Draw replicates
    for i in range(n_reps):
        # Draw samples for comparison
        x_samp = f(*args, size=n)
        # Compute K-S statistic
        reps[i] = ks_stat(x_samp, x_f)
    return reps


rawQuakesOK = pd.read_csv(myPath + "oklahoma_earthquakes_1950-2017.csv", header=2)
mags = rawQuakesOK["mag"]

from datetime import datetime
time = [datetime.strptime(x, "%Y-%m-%d %H:%M:%S.%f") for x in rawQuakesOK["time"]]

# Plot time vs. magnitude
_ = plt.plot(time, mags, marker=".", linestyle="none", alpha=0.1)

# Label axes and show the plot
_ = plt.xlabel('time (year)')
_ = plt.ylabel('magnitude')
# plt.show()
plt.clf()


quakesDT = rawQuakesOK.copy()
quakesDT["convTime"] = time
preData = quakesDT.loc[quakesDT["convTime"] < datetime(2010, 1, 1), :].sort_values("convTime")
postData = quakesDT.loc[quakesDT["convTime"] >= datetime(2010, 1, 1), :].sort_values("convTime")

preTimes = preData.loc[preData["mag"] >= 3, "convTime"]
postTimes = postData.loc[postData["mag"] >= 3, "convTime"]

import datetime as dtTemp
dt_pre = [preTimes.iloc[a+1, ] - preTimes.iloc[a, ] for a in range(preTimes.shape[0] - 1)]
dt_post = [postTimes.iloc[a+1, ] - postTimes.iloc[a, ] for a in range(postTimes.shape[0] - 1)]
dt_pre = [x  / dtTemp.timedelta(days=1)for x in dt_pre if x > dtTemp.timedelta(days=14)]
dt_post = [x / dtTemp.timedelta(days=1) for x in dt_post if x > dtTemp.timedelta(days=14)]


# The variablesdt_preanddt_postrespectively contain the time gap between all earthquakes of magnitude at least 3 from pre-2010 and post-2010 in units of days.
# Compute mean interearthquake time
mean_dt_pre = np.mean(dt_pre)
mean_dt_post = np.mean(dt_post)

# Draw 10,000 bootstrap replicates of the mean
bs_reps_pre = draw_bs_reps(dt_pre, np.mean, size=10000)
bs_reps_post = draw_bs_reps(dt_post, np.mean, size=10000)

# Compute the confidence interval
conf_int_pre = np.percentile(bs_reps_pre, [2.5, 97.5])
conf_int_post = np.percentile(bs_reps_post, [2.5, 97.5])

# Print the results
print("1980 through 2009\nmean time gap: {0:.2f} days\n95% conf int: [{1:.2f}, {2:.2f}] days".format(mean_dt_pre, *conf_int_pre))

print("2010 through mid-2017\nmean time gap: {0:.2f} days\n95% conf int: [{1:.2f}, {2:.2f}] days".format(mean_dt_post, *conf_int_post))


# Compute the observed test statistic
mean_dt_diff = mean_dt_pre - mean_dt_post

# Shift the post-2010 data to have the same mean as the pre-2010 data
dt_post_shift = dt_post - mean_dt_post + mean_dt_pre

# Compute 10,000 bootstrap replicates from arrays
bs_reps_pre = draw_bs_reps(dt_pre, np.mean, size=10000)
bs_reps_post = draw_bs_reps(dt_post_shift, np.mean, size=10000)

# Get replicates of difference of means
bs_reps = bs_reps_pre - bs_reps_post

# Compute and print the p-value
p_val = np.sum(bs_reps >= mean_dt_diff) / 10000
print('p =', p_val)

```
  
  
















