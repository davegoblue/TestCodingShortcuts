---
title: "Data Camp Python Notes"
author: "davegoblue"
date: "September 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(engine.path=list(python="C:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python36-32\\python.exe"))
```

## Background and Overview  

DataCamp offer interactive courses related to Python Programming.  Since R Markdown documents can run simple Python code chunks (though the data is not accessible to future chunks, a large difference from R Markdown for R), this document attempts to summarize notes from the modules when possible.

Topic areas summarized include:  
  
* Python Programming (Introduction, Intermediate, Toolbox I/II, Network Analysis I/II)  
* Python Import and Clean Data (Import I/II, Clean)  
* Python Data Manipulation (pandas Foundations, Manipulating DF with pandas, Merging DF with pandas, Databases in pandas, Data Types)  
* Python Visualization (Introduction to Python Data Visualization, Interactive Visualization with Bokeh)  
* Python Statistics (Statistical Thinking in Python Parts I/II)  
* Python Machine Learning ()  
  
The complete version as of July 31, 2017 has been archived as DataCamp_PythonNotes_v001.  Archive files for DataCamp_Python_ImportClean_v002 and DataCamp_Python_Programming_v002 have also been created to contain summaries of those areas.  Further, DataCamp_PythonNotes_v002 was created for capturing summaries of additional topics.

The version of DataCamp_PythonNotes_v002 as of September 1, 2017 has been archived.  Archive files for DataCamp_Python_DataManipulation_v003 and DataCamp_Python_Visualization_v003 have also been created to contain summaries of these areas.  Further, DataCamp_PythonNotes_v003 was created for capturing summaries of additional topics.

This document includes:  
  
* Python Statistics (Statistical Thinking in Python Parts I/II)  
* Python Machine Learning ()  
  
  
## Python Statistics  
###_Statistical Thinking in Python (Part I)_#

Chapter 1 - Graphical exploratory data analysis  
  
Introduction to exploratory data analysis - organizing, plotting, and summarizing data (ala Tukey):  
  
* Example of swing voter dataset (by county) of 2008 election  
* Conversion of tabular data (e.g., pandas DataFrame) to visual data  
  
Plotting a histogram - using matplotlib.pyplot.hist (plt.hist):  
  
* plt.hist(<myData>) will create the basic hsitogram  
	* Can accessorize with plt.xlabel("myXLabel") and plt.ylabel("myYLabel")  
    * Can send the bins with a bins=<myBins> call ; can be a list of actual bin cut-points, or a single integer requesting that number of evenly spaced bins  
* Common practice in Python is to assign _ = plt.hist(<myData>) to avoid the screen being printed with the 3 histogram data outputs  
* Can customize with the Seaborn styling (package developed by Waskom)  
    * import seaborn as sns  
    * sns.set()  # makes Seaborn style the default for graphing  
  
Plotting all data (bee swarm plots):  
  
* Binning bias can lead to the same data being interpreted differently based on different binning in different histograms  
* The swarm plot has the individual points plotted, segmented by categorical variables if appropriate, in a fashion like "jitter" where each point is visible  
* Seaborn can manage swarm plots easilt, provided that data are in a well-organized DataFrame (rows as observations, columns as features)  
	* sns.swarmplot(x="xVar", y="yVar", data=myFrame)  
    * plt.show()  
  
Plotting all data (ECDF = empirical CDF) - especially when swarm plot would be too messy (too much data):  
  
* The ECDF has the y-axis as the cumulative percentile (total amount of data that is smaller than the designated x value)  
* Suppose that a DataFrame df_swing already exists, with a column called "dem_share"  
	* x = np.sort(df_swing["dem_share"])  
    * y = np.arange(1, len(x) + 1) / len(x)  
    * _ = plt.plot(x, y, marker=".", linestyle="none")  # will make dot markers and with no lines  
    * plt.margins(0.02)  # keeps the data from running over the side of the plot  
* Generally, ECDF is a good starting point for analysis - keeps all the data, provides a simple summary  
  
Onward toward the whole story - starting with graphical EDA as per Tukey:  
  
* Chapter 2 - Build on graphical EDA with quantitative EDA  
* Chapter 3/4 - Probabilistic distributions to draw meaningful conclusions (Hacker statistics)  
* Part II - continuing to build towards "the full story"  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



# Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd

rawIris = pd.read_csv(myPath + "iris.csv")
versicolor_petal_length = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Length"]



# Set default Seaborn style
sns.set()

# Plot histogram of versicolor petal lengths
plt.hist(versicolor_petal_length)

# Show histogram
# plt.show()
plt.savefig("_dummyPy149.png", bbox_inches="tight")
plt.clf()


# Plot histogram of versicolor petal lengths
_ = plt.hist(versicolor_petal_length)

# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
# plt.show()
plt.savefig("_dummyPy150.png", bbox_inches="tight")
plt.clf()


# Import numpy
import numpy as np

# Compute number of data points: n_data
n_data = len(versicolor_petal_length)

# Number of bins is the square root of number of data points: n_bins
n_bins = n_data ** 0.5

# Convert number of bins to integer: n_bins
n_bins = int(n_bins)

# Plot the histogram
_ = plt.hist(versicolor_petal_length, bins=n_bins)

# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
# plt.show()
plt.savefig("_dummyPy151.png", bbox_inches="tight")
plt.clf()



df = rawIris[["Species", "Petal.Length"]]
df.columns = ["species", "petal length (cm)"]

# Create bee swarm plot with Seaborn's default settings
_ = sns.swarmplot(x="species", y="petal length (cm)", data=df)

# Label the axes
_ = plt.xlabel('Species')
_ = plt.ylabel("Petal Length (cm)")

# Show the plot
# plt.show()
plt.savefig("_dummyPy152.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y

# Compute ECDF for versicolor data: x_vers, y_vers
x_vers, y_vers = ecdf(versicolor_petal_length)

# Generate plot
_ = plt.plot(x_vers, y_vers, marker=".", linestyle="none")

# Make the margins nice
plt.margins(0.02)

# Label the axes
_ = plt.xlabel("Versicolor Petal Length (cm)")
_ = plt.ylabel("ECDF")

# Display the plot
# plt.show()
plt.savefig("_dummyPy153.png", bbox_inches="tight")
plt.clf()


setosa_petal_length = rawIris.loc[rawIris["Species"] == "setosa", "Petal.Length"]
virginica_petal_length = rawIris.loc[rawIris["Species"] == "virginica", "Petal.Length"]

# Compute ECDFs
x_set, y_set = ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg, y_virg = ecdf(virginica_petal_length)

# Plot all ECDFs on the same plot
_ = plt.plot(x_set, y_set, marker=".", linestyle="none")
_ = plt.plot(x_vers, y_vers, marker=".", linestyle="none")
_ = plt.plot(x_virg, y_virg, marker=".", linestyle="none")

# Make nice margins
plt.margins(0.02)

# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
# plt.show()
plt.savefig("_dummyPy154.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Default Histogram**:  
![](_dummyPy149.png) 
  
**Example #2: Labeled Histogram**:  
![](_dummyPy150.png) 

**Example #3: Custom-Binned Histogram**:  
![](_dummyPy151.png) 

**Example #4: Swarm Plot**:  
![](_dummyPy152.png) 

**Example #5: ECDF (Single Species)**:  
![](_dummyPy153.png) 
  
**Example #6: ECDF (Multiple Species)**:  
![](_dummyPy154.png) 
  
***
  
Chapter 2 - Quantitative exploratory data analysis  
  
Introduction to summary statistics: sample mean and median:  
  
* Means or medians can be added as horizontal lines on the bee swarm plot  
	* np.mean(myData)  
    * np.median(myData)  
  
Percentiles, outliers, and box plots:  
  
* Can grab multiple percentiles using np.percentile(myData, [myPercentileList])  # note that percentiles should be passed, so 25 means 0.25 or 25th percentile  
* Box plots can help to display much of the percentile data simultaneously - center is median, box edges are 0.25/0.75, end of whsikers is 1.5 * IQR (unless data is narrower), with outliers graphed separately  
* _ = sns.boxplot(x="xVar", y="yVar", data=myData)  
  
Variance and standard deviation:  
  
* Variance - average of the squared distance from the mean  
	* np.var(myData)  
* Standard Deviation - square root of the variance  
	* np.sd(myData)  
  
Covariance and Pearson correlation coefficient:  
  
* Initial graphical EDA could come from graphing the data as points, using plt.plot(xVar, yVar, marker=".", linestyle="none")  
* Covariance - how to variables run together (mean of product of differences from x-mean, y-mean)  
* Correlation - adds the advantages of dimensionless and scaled from -1 to 1  
	* Pearson correlation coefficient (rho) - Covariances / [Std(x) * Std(y) ]  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


rawIris = pd.read_csv(myPath + "iris.csv")
versicolor_petal_length = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Length"]
versicolor_petal_width = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Width"]


# Compute the mean: mean_length_vers
mean_length_vers = np.mean(versicolor_petal_length)

# Print the result with some nice formatting
print('I. versicolor:', mean_length_vers, 'cm')


# Specify array of percentiles: percentiles
percentiles = np.array([2.5, 25, 50, 75, 97.5])

# Compute percentiles: ptiles_vers
ptiles_vers = np.percentile(versicolor_petal_length, percentiles)

# Print the result
print(ptiles_vers)


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y

# Compute ECDF for versicolor data: x_vers, y_vers
x_vers, y_vers = ecdf(versicolor_petal_length)


# Plot the ECDF
_ = plt.plot(x_vers, y_vers, '.')
plt.margins(0.02)
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Overlay percentiles as red diamonds.
_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',
         linestyle="none")

# Show the plot
# plt.show()
plt.savefig("_dummyPy155.png", bbox_inches="tight")
plt.clf()


df = rawIris[["Species", "Petal.Length"]]
df.columns = ["species", "petal length (cm)"]


# Create box plot with Seaborn's default settings
_ = sns.boxplot(x="species", y="petal length (cm)", data=df)

# Label the axes
_ = plt.xlabel("Species")
_ = plt.ylabel("Petal Length (cm)")

# Show the plot
# plt.show()
plt.savefig("_dummyPy156.png", bbox_inches="tight")
plt.clf()


# Array of differences to mean: differences
differences = versicolor_petal_length - np.mean(versicolor_petal_length)

# Square the differences: diff_sq
diff_sq = differences ** 2

# Compute the mean square difference: variance_explicit
variance_explicit = np.mean(diff_sq)

# Compute the variance using NumPy: variance_np
variance_np = np.var(versicolor_petal_length)

# Print the results
print(variance_explicit, variance_np)


# Compute the variance: variance
variance = np.var(versicolor_petal_length)

# Print the square root of the variance
print(variance ** 0.5)

# Print the standard deviation
print(np.std(versicolor_petal_length))


# Make a scatter plot
_ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker=".", linestyle="none")

# Set margins
plt.margins(0.02)

# Label the axes
_ = plt.xlabel("Petal Length (cm)")
_ = plt.ylabel("Petal Width (cm)")

# Show the result
# plt.show()
plt.savefig("_dummyPy157.png", bbox_inches="tight")
plt.clf()


# Compute the covariance matrix: covariance_matrix
covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)

# Print covariance matrix
print(covariance_matrix)

# Extract covariance of length and width of petals: petal_cov
petal_cov = covariance_matrix[0, 1]

# Print the length/width covariance
print(petal_cov)


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]

# Compute Pearson correlation coefficient for I. versicolor: r
r = pearson_r(versicolor_petal_length, versicolor_petal_width)

# Print the result
print(r)

```
  
  
**Example #1: ECDF with Key Percentiles**:  
![](_dummyPy155.png) 

**Example #2: Box Plot**:  
![](_dummyPy156.png)

**Example #3: Scatter Plot**:  
![](_dummyPy157.png)

  
***
  
Chapter 3 - Thinking Probabilistically (Discrete Variables)  
  
Probabilistic logic and statistical inference - drawing conclusions about the population based on my sample:  
  
* Multiple re-samples will generate multiple means  
  
Random number generators and hacker statistics - tool for thinking probabilistically:  
  
* Use simulated repeated measurements to compute probabilities  
* Initial simulations were run on games of chance (particularly coin flips) by Pascal et al  
* The NumPy module has many useful random number generators  
	* np.random.random(size=n)  # draws "n" random numbers between 0 and 1  
    * np.random.seed()  # allows for providing a seed for re-producibility  
* Hacker statistics - define simulation, run many times, summarize key statistic of interest  
  
Probability distributions - Binomial:  
  
* Probability Mass Function (PMF) - set of probabilities of discrete outcomes  
	* Rolling a dice is an example - can only get discrete values (integers 1-6)  
* Binomial Distribution - the number of successes, r, in n Bernoullie trials with probability p of success will be binomially distributed  
	* np.random.binomial(nTrials, pSuccess, size=n)  # will simulate the sum of successes in the nTrials each with pSuccess ; will create a 1D array of length n by repeating the simulation n times  
  
Poisson processes and distributions - specified solely by a rate, independent of any previous events:  
  
* Natural births in a hospital, timing of hits to a website, etc.  
* Poisson distribution - the number r of arrivals of a Poisson process in a given time period with an average rate lambda of arrivals is Poisson distributed  
* The Poisson distribution is a limiting case of the Binomial distribution when there is a low succes probability but a large amount of time -- i.e., rare events  
	* np.random.poisson(param, size=)  # where param is the expected number of events and size is the number of simulations desired  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



# Seed the random number generator
np.random.seed(42)

# Initialize random numbers: random_numbers
random_numbers = np.empty(100000)

# Generate random numbers by looping over range(100000)
for i in range(100000):
    random_numbers[i] = np.random.random()

# Plot a histogram
_ = plt.hist(random_numbers)

# Show the plot
# plt.show()
plt.savefig("_dummyPy158.png", bbox_inches="tight")
plt.clf()


def perform_bernoulli_trials(n, p):
    """Perform n Bernoulli trials with success probability p
    and return number of successes."""
    # Initialize number of successes: n_success
    n_success = 0
    
    # Perform trials
    for i in range(n):
        # Choose random number between zero and one: random_number
        random_number = np.random.random()
        
        # If less than p, it's a success so add one to n_success
        if random_number < p:
            n_success +=1
    
    return n_success


# Seed random number generator
np.random.seed(42)

# Initialize the number of defaults: n_defaults
n_defaults = np.empty(1000)

# Compute the number of defaults
for i in range(1000):
    n_defaults[i] = perform_bernoulli_trials(100, 0.05)

# Plot the histogram with default number of bins; label your axes
_ = plt.hist(n_defaults, normed=True)
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('probability')

# Show the plot
# plt.show()
plt.savefig("_dummyPy159.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Compute ECDF: x, y
x, y = ecdf(n_defaults)  # same function as written in previous chapters

# Plot the ECDF with labeled axes
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.xlabel("Number of Defaults in Simulation")
_ = plt.ylabel("ECDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy160.png", bbox_inches="tight")
plt.clf()


# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money
n_lose_money = np.sum(n_defaults >= 10)

# Compute and print probability of losing money
print('Probability of losing money =', n_lose_money / len(n_defaults))


# Take 10,000 samples out of the binomial distribution: n_defaults
n_defaults = np.random.binomial(100, 0.05, size=10000)

# Compute CDF: x, y
x, y = ecdf(n_defaults)

# Plot the CDF with axis labels
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.xlabel("Defaults in 100 loans")
_ = plt.ylabel("CDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy161.png", bbox_inches="tight")
plt.clf()


# Compute bin edges: bins
bins = np.arange(min(n_defaults), max(n_defaults) + 2) - 0.5

# Generate histogram
_ = plt.hist(n_defaults, bins=bins, normed=True)

# Set margins
plt.margins(0.02)

# Label axes
_ = plt.xlabel("Number of Defaults in 100 Loans")
_ = plt.ylabel("Probability")

# Show the plot
# plt.show()
plt.savefig("_dummyPy162.png", bbox_inches="tight")
plt.clf()


# Draw 10,000 samples out of Poisson distribution: samples_poisson
samples_poisson = np.random.poisson(10, size=10000)

# Print the mean and standard deviation
print('Poisson:     ', np.mean(samples_poisson),
                       np.std(samples_poisson))

# Specify values of n and p to consider for Binomial: n, p
n = [20, 100, 1000]
p = [0.5, 0.1, 0.01]

# Draw 10,000 samples for each n,p pair: samples_binomial
for i in range(3):
    samples_binomial = np.random.binomial(n[i], p[i], size=10000)

    # Print results
    print('n =', n[i], 'Binom:', np.mean(samples_binomial),
                                 np.std(samples_binomial))


# Draw 10,000 samples out of Poisson distribution: n_nohitters
n_nohitters = np.random.poisson(251/115, size=10000)

# Compute number of samples that are seven or greater: n_large
n_large = np.sum(n_nohitters >= 7)

# Compute probability of getting seven or more: p_large
p_large = n_large / 10000

# Print the result
print('Probability of seven or more no-hitters:', p_large)


```
  
  
**Example #1: Histogram of Random Numbers**:  
![](_dummyPy158.png) 

**Example #2: Histogram of 1000 Bernoulli Trials (each with p=0.05, n=100)**:  
![](_dummyPy159.png) 

**Example #3: ECDF of 1000 Bernoulli Trials**:  
![](_dummyPy160.png) 

**Example #4: Histogram/ECDF of Binomial Trials (n=100, p=0.05, size=10000)**:  
![](_dummyPy161.png) 

**Example #5: Normed Histogram with Customized Binning for Binomial Trials (n=100, p=0.05, size=10000)**:  
![](_dummyPy162.png) 
  
  
***
  
Chapter 4 - Thinking probabilistically - continuous variables  
  
Probability density functions (PDF) - continuous analog to the PMF:  
  
* Mathematical description of the relative likelihood of observing a value of a continuous variable  
* Areas under the PDF describe the associated probabilities  
* The CDF can be calculated from the PDF, and can be easier to interpret  
  
Introduction to the Normal Distribution - continuous variable with a single peak:  
  
* Mean - center of the peak  
* Standard Deviation - degree of spread of the data around the peak  
* To avoid binning bias, better to compare theoretical/actual CDF rather than histogram and theoretical curve overlaid  
* To simulate normal data, call np.random.normal(mean, std, size=)  
	
Normal distribution - properties and warnings:  
  
* Commonly called the Gaussian dsitribution after its inventor (was previously featured on the German Deutschemark prior to adoption of the Euro)  
* Important caveats include  
	1.  Sometimes, things that you think "should" be normally distributed in fact are not  
    2.  Normal distributions have very light tails (outliers are extremely unlikely), which often does not match to the real-world  
  
Exponential distribution - related to the Poisson distribution:  
  
* The expected number of occurences being Poisson distributed means that the expected time between events will be exponentially distributed  
* The waiting time between arrivals of a Poisson distribution is exponentially distributed - defined by a single parameter, "mean waiting time"  
	* np.random.exponential(mean, size=)  # will simulate "size" times from an exponential with mean "mean"  
* Simulating a story can be very powerful - existence of computers makes the "pen and paper" modelling less necessary in many cases  
  
Final thoughts - course recap:  
  
* Construct instructive plots  
* Compute informative summary statistics  
* Use hacker statistics  
* Think probabilistically  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10
samples_std1 = np.random.normal(20, 1, size=100000)
samples_std3 = np.random.normal(20, 3, size=100000)
samples_std10 = np.random.normal(20, 10, size=100000)

# Make histograms
_ = plt.hist(samples_std1, normed=True, histtype="step", bins=100)
_ = plt.hist(samples_std3, normed=True, histtype="step", bins=100)
_ = plt.hist(samples_std10, normed=True, histtype="step", bins=100)

# Make a legend, set limits and show plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))
plt.ylim(-0.01, 0.42)
# plt.show()
plt.savefig("_dummyPy163.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Generate CDFs
x_std1, y_std1 = ecdf(samples_std1)  # function written earlier in previous chapter
x_std3, y_std3 = ecdf(samples_std3)
x_std10, y_std10 = ecdf(samples_std10)

# Plot CDFs
_ = plt.plot(x_std1, y_std1, marker=".", linestyle="none")
_ = plt.plot(x_std3, y_std3, marker=".", linestyle="none")
_ = plt.plot(x_std10, y_std10, marker=".", linestyle="none")

# Make 2% margin
plt.margins(0.02)

# Make a legend and show the plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')
# plt.show()
plt.savefig("_dummyPy164.png", bbox_inches="tight")
plt.clf()


rawBelmont = pd.read_csv(myPath + "belmont_stakes_1926_2016.csv")

from datetime import datetime

listBelmont = [x.split(".") for x in rawBelmont["Time"]]
timeBelmont = [int(x[0].split(":")[0]) * 60 + int(x[0].split(":")[1]) + int(x[1])/100 for x in listBelmont]
belmont_no_outliers = timeBelmont


# Compute mean and standard deviation: mu, sigma
mu = np.mean(belmont_no_outliers)
sigma = np.std(belmont_no_outliers)

# Sample out of a normal distribution with this mu and sigma: samples
samples = np.random.normal(mu, sigma, size=10000)

# Get the CDF of the samples and of the data
x_theor, y_theor = ecdf(samples)
x, y = ecdf(belmont_no_outliers)

# Plot the CDFs and show the plot
_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x, y, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('Belmont winning time (sec.)')
_ = plt.ylabel('CDF')
# plt.show()
plt.savefig("_dummyPy165.png", bbox_inches="tight")
plt.clf()


# Take a million samples out of the Normal distribution: samples
samples = np.random.normal(mu, sigma, size=1000000)

# Compute the fraction that are faster than 144 seconds: prob
prob = np.mean(samples <= 144)

# Print the result
print('Probability of besting Secretariat:', prob)


def successive_poisson(tau1, tau2, size=1):
    # Draw samples out of first exponential distribution: t1
    t1 = np.random.exponential(tau1, size)
    
    # Draw samples out of second exponential distribution: t2
    t2 = np.random.exponential(tau2, size)
    
    return t1 + t2


# Draw samples of waiting times: waiting_times
waiting_times = successive_poisson(764, 715, size=100000)

# Make the histogram
_ = plt.hist(waiting_times, bins=100, normed=True, histtype="step")

# Label axes
_ = plt.xlabel("Total Waiting Time (Days) for Cycle and No-Hitter")
_ = plt.ylabel("PDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy166.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Histogram of Normal Distribution with Different STD**:  
![](_dummyPy163.png) 

**Example #2: ECDF of Normal Distribution with Different STD**:  
![](_dummyPy164.png) 

**Example #3: ECDF of Belmont Stakes Winning Times**:  
![](_dummyPy165.png) 

**Example #4: Sum of Poisson Outcomes**:  
![](_dummyPy166.png) 


###_Statistical Thinking in Python (Part I)_#
  
Chapter 1 - Parameter estimation by optimization  
  
Optimal parameters - parameters that bring the model in closest agreement with the data:  
  
* Parameters derived from the sample are relevant mainly to that sample; other samples might have other optimal parameters  
* Two key packages for statistics in Python - scipy.stats and statsmodels  
* For this course, though, the focus will be on hacker statistics - simulated data using numpy  
  
Linear regression by least squares - fitting the best slope and intercept to the line:  
  
* Residuals are the vertical distance between the data point and the regression line  
* The "least squares" algorithm defines the best fit to be the line that minimizes the sum-squared residuals  
* Regressions can be run in Python using np.polyfit()  # least squares with polynomials  
	* slope, intercept = np.polyfit(xData, yData, nDegree)  # nDegree is the desired degree of the polynomial, which is to say 1 for linear regression  
  
Importance of EDA (Anscombe's quartet) - 4 fictitious datasets with the same x-bar, y-bar, correlation, and RSS:  
  
* Graphical EDA is vital before diving in to the regression (or other) analysis  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]

# Seed random number generator
np.random.seed(42)

# Compute mean no-hitter time: tau
tau = np.mean(nohitter_times)

# Draw out of an exponential distribution with parameter tau: inter_nohitter_time
inter_nohitter_time = np.random.exponential(tau, 100000)

# Plot the PDF and label axes
_ = plt.hist(inter_nohitter_time,
             bins=50, normed=True, histtype="step")
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy167.png", bbox_inches="tight")
plt.clf()



def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Uses function ecdf() from previous course
# Create an ECDF from real data: x, y
x, y = ecdf(nohitter_times)

# Create a CDF from theoretical samples: x_theor, y_theor
x_theor, y_theor = ecdf(inter_nohitter_time)

# Overlay the plots
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker=".", linestyle="none")

# Margins and axis labels
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy168.png", bbox_inches="tight")
plt.clf()



# Plot the theoretical CDFs
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker='.', linestyle='none')
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Take samples with half tau: samples_half
samples_half = np.random.exponential(tau/2, size=10000)

# Take samples with double tau: samples_double
samples_double = np.random.exponential(tau*2, size=10000)

# Generate CDFs from these samples
x_half, y_half = ecdf(samples_half)
x_double, y_double = ecdf(samples_double)

# Plot these CDFs as lines
_ = plt.plot(x_half, y_half)
_ = plt.plot(x_double, y_double)

# Show the plot
# plt.show()
plt.savefig("_dummyPy169.png", bbox_inches="tight")
plt.clf()



import pandas as pd
rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')

# Set the margins and label axes
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Show the plot
# plt.show()
plt.savefig("_dummyPy170.png", bbox_inches="tight")
plt.clf()



def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


# Show the Pearson correlation coefficient
print(pearson_r(illiteracy, fertility))


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Perform a linear regression using np.polyfit(): a, b
a, b = np.polyfit(illiteracy, fertility, 1)

# Print the results to the screen
print('slope =', a, 'children per woman / percent illiterate')
print('intercept =', b, 'children per woman')

# Make theoretical line to plot
x = np.array([0, 100])
y = a * x + b

# Add regression line to your plot
_ = plt.plot(x, y)

# Draw the plot
# plt.show()
plt.savefig("_dummyPy171.png", bbox_inches="tight")
plt.clf()


# Specify slopes to consider: a_vals
a_vals = np.linspace(0, 0.1, 200)

# Initialize sum of square of residuals: rss
rss = np.empty_like(a_vals)

# Compute sum of square of residuals for each value of a_vals
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy - b)**2)

# Plot the RSS
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')

#plt.show()
plt.savefig("_dummyPy172.png", bbox_inches="tight")
plt.clf()


rawAnscombe = pd.read_csv(myPath + "anscombe.csv", header=None)

anscombe_x = [[float(x) for x in rawAnscombe.iloc[2:, 0]], [float(x) for x in rawAnscombe.iloc[2:, 2]], [float(x) for x in rawAnscombe.iloc[2:, 4]], [float(x) for x in rawAnscombe.iloc[2:, 6]]]

anscombe_y = [[float(x) for x in rawAnscombe.iloc[2:, 1]], [float(x) for x in rawAnscombe.iloc[2:, 3]], [float(x) for x in rawAnscombe.iloc[2:, 5]], [float(x) for x in rawAnscombe.iloc[2:, 7]]]


x=anscombe_x[0]
y=anscombe_y[0]


# Perform linear regression: a, b
a, b = np.polyfit(x, y, 1)

# Print the slope and intercept
print(a, b)

# Generate theoretical x and y data: x_theor, y_theor
x_theor = np.array([3, 15])
y_theor = a * x_theor + b

# Plot the Anscombe data and theoretical line
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.plot(x_theor, y_theor)

# Label the axes
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
# plt.show()
plt.savefig("_dummyPy173.png", bbox_inches="tight")
plt.clf()



# Iterate through x,y pairs
for x, y in zip(anscombe_x, anscombe_y):
    # Compute the slope and intercept: a, b
    a, b = np.polyfit(x, y, 1)
    
    # Print the result
    print('slope:', a, 'intercept:', b)



```
  
  
**Example #1: Exponential Distribution**:  
![](_dummyPy167.png) 
  
**Example #2: Theoretical vs Actual ECDF (Exponential for Time Between No-Hitters)**:  
![](_dummyPy168.png)

**Example #3: ECDF for Exponential Distribution with Half-Rate and Double-Rate**:  
![](_dummyPy169.png)

**Example #4: Scatter-Plot for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy170.png)

**Example #5: Best Linear Fit for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy171.png)

**Example #6: Residual Sum-Squares vs Best-Fit Parameters**:  
![](_dummyPy172.png)

**Example #7: Regression Fit for Anscombe Data #1**:  
![](_dummyPy173.png)
  
***
  
Chapter 2 - Bootstrap confidence intervals  
  
Generating bootstrap replicates - how might the sample statistics change if we acquired a new sample:  
  
* Hacker approach - resample the existing data (WITH replacement), using the same n, then recalculate the sample statistics  
* Can then take multiple re-samples and aggregate the sample statistics for a statistical inference  
* "Bootstrap replicate" is the value of the summary statistic calculated from the "bootstrap sample"  
* Boostrap samples can easily be created in Python  
	* np.random.choice(myData, size=len(myData))  # can specify any size, but len(myData) will be a bootstrap; sampling is WITH replacement  
  
Bootstrap confidence intervals:  
  
* Can generate a function bootstrap_replicate_1d(data, func) which takes a bootstrap sample of data and then applies func to it  
* Can then use a for loop to run through the function, storing the results in a numpy array (or appended to an empty list, or etc.)  
* Can generate the confidence interval using np.percentile(all_replicates, [2.5, 97.5])  # this is for the 95% confidence interval  
  
Pairs bootstrap - bootstrap resample on pairs of data (e.g., keep the x and y connected to each other):  
  
* Bootstrap approach is non-parametric, since it is not forced to fit any particular model  
* Least-squares approach is parametrics, since it is forced to fit a pre-defined specification of the model  
* Can use bootstrap techniques to get confidence intervals on the parametric (e.g., least-squares) models  
	* Use a pairs bootstrap that keeps each of the x/y anchored to each other  
    * Resample WITH replacement on all the x/y pairs, then run the model with this bootstrap resample  
* The approach within Python is a slight modification of the 1D approach  
    * inds = np.arange(len(myXYData))  
    * bs_inds = np.random.choice(inds, len(inds))  
    * Use bs_inds as the .iloc (or index) filters of the dataset  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



rawWeather = pd.read_csv(myPath + "sheffield_weather_station.csv", skiprows=8, delim_whitespace=True)
rainfall = rawWeather["rain"]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y



for a in range(50):
    # Generate bootstrap sample: bs_sample
    bs_sample = np.random.choice(rainfall, size=len(rainfall))
    
    # Compute and plot ECDF from bootstrap sample
    x, y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker='.', linestyle='none',
                 color='gray', alpha=0.1)

# Compute and plot ECDF from original data
x, y = ecdf(rainfall)
_ = plt.plot(x, y, marker='.')

# Make margins and label axes
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy174.png", bbox_inches="tight")
plt.clf()



def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


# Take 10,000 bootstrap replicates of the mean: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.mean, 10000)

# Compute and print SEM
sem = np.std(rainfall) / np.sqrt(len(rainfall))
print(sem)

# Compute and print standard deviation of bootstrap replicates
bs_std = np.std(bs_replicates)
print(bs_std)

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('mean annual rainfall (mm)')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy175.png", bbox_inches="tight")
plt.clf()


# Generate 10,000 bootstrap replicates of the variance: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.var, 10000)

# Put the variance in units of square centimeters
bs_replicates = bs_replicates / 100

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('variance of annual rainfall (sq. cm)')
_ = plt.ylabel('PDF')

# Show the plot
#plt.show()
plt.savefig("_dummyPy176.png", bbox_inches="tight")
plt.clf()



rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]


# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates
bs_replicates = draw_bs_reps(nohitter_times, np.mean, 10000)

# Compute the 95% confidence interval: conf_int
conf_int = np.percentile(bs_replicates, [2.5, 97.5])

# Print the confidence interval
print('95% confidence interval =', conf_int, 'games')

# Plot the histogram of the replicates
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel(r'$\tau$ (games)')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy177.png", bbox_inches="tight")
plt.clf()



def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates: bs_slope_reps, bs_intercept_reps
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    
    return bs_slope_reps, bs_intercept_reps



rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy

# Generate replicates of slope and intercept using pairs bootstrap
bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, 1000)

# Compute and print 95% CI for slope
print(np.percentile(bs_slope_reps, [2.5, 97.5]))

# Plot the histogram
_ = plt.hist(bs_slope_reps, bins=50, normed=True)
_ = plt.xlabel('slope')
_ = plt.ylabel('PDF')

# plt.show()
plt.savefig("_dummyPy178.png", bbox_inches="tight")
plt.clf()


# Generate array of x-values for bootstrap lines: x
x = np.array([0, 100])

# Plot the bootstrap lines
for i in range(100):
    _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i],
                 linewidth=0.5, alpha=0.2, color='red')

# Plot the data
_ = plt.plot(illiteracy, fertility, marker=".", linestyle="none")

# Label axes, set the margins, and show the plot
_ = plt.xlabel('illiteracy')
_ = plt.ylabel('fertility')
plt.margins(0.02)

# plt.show()
plt.savefig("_dummyPy179.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Bootstrap ECDF for Sheffield Annual Rainfall**:  
![](_dummyPy174.png)

**Example #2:  Bootstrap Mean for Sheffield Annual Rainfall**:  
![](_dummyPy175.png)

**Example #3:  Bootstrap Variance for Sheffield Annual Rainfall**:  
![](_dummyPy176.png)

**Example #4: Bootstrap for No-Hitter Times**:  
![](_dummyPy177.png)

**Example #5: Bootstrap Pairs for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy178.png)

**Example #6: Bootstrap Pairs for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy179.png)

  
***
  
Chapter 3 - Hypothesis Testing  
  
Formulating and simulating a hypothesis:  
  
* Hypothesis testing - how reasonable are the observed data assuming a particular hypothesis is true  
* Hacker approach - scrambling the data - example of PA and OH 2008 voting data (% D)  
	* Combine all county level results from PA (67 counties) and OH, ignoring which state they are from  
    * Permute (randomly scramble) the all-county dataset, and declare the first 67 counties as the PA data  
* Running a permutation process in Python using NumPy  
	* dem_share_both = np.concatenate((dem_PA, dem_OH))  # takes a tuple of the desired inputs  
    * dem_share_perm = np.random.permutation(dem_share_both)  # permutes the full dataset  
    * perm_PA = dem_share_perm[:len(dem_share_PA)]  
    * perm_OH = dem_share_perm[len(dem_share_PA):]  
  
Test statistics and p-values - testing the permutations as per the above, assuming the null hypothesis of identical distributions in OH/PA:  
  
* Test statistic - single number that can be computed from observed data, as well as from data simulated under the null hypothesis  
	* Serves as a basis for comparison between predictions and actual observations  
    * Can choose "difference in means" as the test statistic in this case  
* Can compare the observed test statistic to the permutation replicates, and assess likelihood of results "as extreme" as the actual test statistic  
	* The p-value is the probability of getting a result at least as extreme as the observed test statistic, under the assumption that they null hypothesis is valid  
* Null Hypothesis Significance Testing (NHST) - consider the value of the p-value as well as the magnitude of the differences  
	* Statistical significance (p-value) and practical significance are two different things  
  
Bootstrap hypothesis tests:  
  
* Basic analysis pipeline - clearly state null hypothesis; define test statistic; generate many permuted datasets assuming null hypothesis; compute test statistic for each permutation  
* Additional analysis type - have ALL of the Michelson speed of light data, but only the mean calculated by Newcomb - could Newcomb mean have come from Michelson experiment?  
	* Null hypothesis becomes "True mean speed of light in Michelson experiment is equal to Newcomb's reported mean"  
    * Shift the means of the Michelson experimental data (every point) to match the Newcomb reported mean: michelson - np.mean(michelson) + newcomb  
    * Calculate p-values based on permutations of shifted Michelson data  
* One sample test - compare one set of data to a single number  
* Two sample test - compare two sets of data against each other  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2


# Create some fake rain data - July and November
rain_july = np.random.normal(3.5, 1.25, 100)
rain_november = np.random.normal(2.25, 0.75, 100)

for i in range(50):
    # Generate permutation samples
    perm_sample_1, perm_sample_2 = permutation_sample(rain_july, rain_november)
    
    # Compute ECDFs
    x_1, y_1 = ecdf(perm_sample_1)
    x_2, y_2 = ecdf(perm_sample_2)
    
    # Plot ECDFs of permutation sample
    _ = plt.plot(x_1, y_1, marker='.', linestyle='none',
                 color='red', alpha=0.02)
    _ = plt.plot(x_2, y_2, marker='.', linestyle='none',
                 color='blue', alpha=0.02)

# Create and plot ECDFs from original data
x_1, y_1 = ecdf(rain_july)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Display the plot
# plt.show()
plt.savefig("_dummyPy180.png", bbox_inches="tight")
plt.clf()



rawFrog = pd.read_csv(myPath + "frog_tongue.csv", header=14)


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


# Make bee swarm plot
_ = sns.swarmplot(x="ID", y='impact force (mN)', data=rawFrog)

# Label axes
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')

# Show the plot
# plt.show()
plt.savefig("_dummyPy181.png", bbox_inches="tight")
plt.clf()


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


# Define force_a to be frog I and force_b to be frog II
force_a = rawFrog.loc[rawFrog["ID"] == "I", 'impact force (mN)']
force_b = rawFrog.loc[rawFrog["ID"] == "II", 'impact force (mN)']

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a, force_b)

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(force_a, force_b,
                                 diff_of_means, size=10000)

# Compute p-value: p
p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)

# Print the result
print('p-value =', p)


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b - np.mean(force_b) + 0.55

# Function available in previous chapters - copy down
# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)

# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000

# Print the p-value
print('p = ', p)


# Per the notes from DataCamp, the permutation test is a more exact test of the null hypothesis while the (below) bootstrap test is more versatile
#  "However, the permutation test exactly simulates the null hypothesis that the data come from the same distribution, whereas the bootstrap test approximately simulates it. As we will see, though, the bootstrap hypothesis test, while approximate, is more versatile."

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a, force_b)

# Concatenate forces: forces_concat
forces_concat = np.concatenate((force_a, force_b))

# Initialize bootstrap replicates: bs_replicates
bs_replicates = np.empty(10000)

for i in range(10000):
    # Generate bootstrap sample
    bs_sample = np.random.choice(forces_concat, size=len(forces_concat))
    
    # Compute replicate
    bs_replicates[i] = diff_of_means(bs_sample[:len(force_a)],
                                     bs_sample[len(force_a):])

# Compute and print p-value: p
p = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)
print('p-value =', p)


# Compute mean of all forces: mean_force
mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

# Compute and print p-value: p
p = np.sum(bs_replicates > empirical_diff_means) / len(bs_replicates)
print('p-value =', p)

```
  
  
**Example #1: ECDF for July vs November Rainfall**:  
![](_dummyPy180.png)
  
**Example #2: Swarm Plot for Impact Force by Frog**:  
![](_dummyPy181.png)
  
***

Chapter 4 - Hypothesis test examples  
  
A/B Testing - test/control approach for "splash" pages of the website:  
  
* Null hypothesis is that the redesign (page B) has no impact on the click-through rate relative to the control design (page A)  
* Calculate a test statistic as np.mean(B) - np.mean(A)  # average click-through rate is the test statistic  
* Combine and permute the data, and calculate test statistic for permutation replicates  
* Calculate likelihood of seeing data as large or larger than the test statistic  
  
Test of correlation - returning to the Obama vote share vs. county size analysis from prior:  
  
* Null hypothesis is of no correlation between Obama vote share and county size (total votes)  
* Use the Pearson correlation coefficient as the test statistic  
* Leave one of the columns constant and permute the other; calculate the permutation replicate based on this new dataset  
* Repeat many times, and calculate the likelihood of achieving a correlation as extreme as was observed  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2



# Construct arrays of data: dems, reps
dems = np.array([True] * 153 + [False] * 91)
reps = np.array([True] * 136 + [False] * 35)

def frac_yay_dems(dems, reps):
    """Compute fraction of Democrat yay votes."""
    frac = np.sum(dems) / len(dems)
    return frac

# draw_perm_reps is available in previous chapters
# Acquire permutation samples: perm_replicates
perm_replicates = draw_perm_reps(dems, reps, frac_yay_dems, 10000)

# Compute and print p-value: p
p = np.sum(perm_replicates <= 153/244) / len(perm_replicates)
print('p-value =', p)


# No Hitter Dataset is split to include pre-1920 (dead ball era) and 1920-current (live ball era)
rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]

from datetime import datetime
deadEra = rawNoHitter["date"] < datetime.strptime("1920-01-01", "%Y-%m-%d")
deadEra = deadEra[1:]

nht_dead = nohitter_times[deadEra]
nht_live = nohitter_times[deadEra == False]

# Compute the observed difference in mean inter-no-hitter times: nht_diff_obs
nht_diff_obs = diff_of_means(nht_dead, nht_live)

# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates
perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, 10000)

# Compute and print the p-value: p
p = np.sum(perm_replicates <= nht_diff_obs) / len(perm_replicates)
print('p-val =',p)


# Function pearson_r is available from the previous course
rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy


# Compute observed correlation: r_obs
r_obs = pearson_r(illiteracy, fertility)

# Initialize permutation replicates: perm_replicates
perm_replicates = np.empty(10000)

# Draw replicates
for i in range(10000):
    # Permute illiteracy measurments: illiteracy_permuted
    illiteracy_permuted = np.random.permutation(illiteracy)
    
    # Compute Pearson correlation
    perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)

# Compute p-value: p
p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)
print('p-val =', p)


# Function ecdf is available from previous course
rawBee = pd.read_csv(myPath + "bee_sperm.csv", header=3, index_col=None)
control = rawBee.loc[rawBee["Treatment"] == "Control", "Sperm Volume per 500 ul"]
treated = rawBee.loc[rawBee["Treatment"] == "Pesticide", "Sperm Volume per 500 ul"]

# Compute x,y values for ECDFs
x_control, y_control = ecdf(control)
x_treated, y_treated = ecdf(treated)

# Plot the ECDFs
plt.plot(x_control, y_control, marker='.', linestyle='none')
plt.plot(x_treated, y_treated, marker='.', linestyle='none')

# Set the margins
plt.margins(0.02)

# Add a legend
plt.legend(('control', 'treated'), loc='lower right')

# Label axes and show plot
plt.xlabel('millions of alive sperm per mL')
plt.ylabel('ECDF')
# plt.show()
plt.savefig("_dummyPy182.png", bbox_inches="tight")
plt.clf()


# draw_bs_reps() is available back in Chapter 2
# Compute the difference in mean sperm count: diff_means
diff_means = np.mean(control) - np.mean(treated)

# Compute mean of pooled data: mean_count
mean_count = np.mean(np.concatenate((control, treated)))

# Generate shifted data sets
control_shifted = control - np.mean(control) + mean_count
treated_shifted = treated - np.mean(treated) + mean_count

# Generate bootstrap replicates
bs_reps_control = draw_bs_reps(control_shifted,
                       np.mean, size=10000)
bs_reps_treated = draw_bs_reps(treated_shifted,
                       np.mean, size=10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_reps_control - bs_reps_treated

# Compute and print p-value: p
p = np.sum(bs_replicates >= diff_means) / len(bs_replicates)
print('p-value =', p)

```
  
  
**Example #1: A/B Testing for Bees**:  
![](_dummyPy182.png)
  
***
  
Chapter 5 - Case Study  
  
Finch beaks and need for statistics:  
  
* Toolbox includes: Graphical and quantitative EDA ; Parameter estimation ; Confidence intervals ; Hypothesis testing  
* Finches (inhabitants of the Galapagos) formed a key backbone of Darwin's research for the theory of evolution  
	* Benefits include isolation, and a number of very small islands where the entire population can be monitored  
    * Researchers spend time every year in Daphne Major, one of the islands of the Galapagos  
* Finches of Daphne Major include Geospiza fortis and Geospiza scandens  
	* Peter and Rosemary Grant: "40 Years of Evolution: Darwin's Finches on Daphne Major Island" (2014)  
    * Data available for free, public use - see Dryad Digital Repository  
    * Data include Beak Length and Beak Depth  
* Initial analysis for the case study will be G. scandens beak depth changes over time (1975 vs. 2012)  
	* Hypothesis test: did the beaks get deeper from 1975 to 2012?  
  
Variation of beak shapes:  
  
* One hypothesis is that a drought in winter 1976/1977 led to a shortage of small seed (large seed were still pentiful), advataging finches with deeper beaks  
* Question is whether the beak length and beak depth are changing at the same rate, which would mean that the beak shape (aspect ratio) stayed the same  
  
Heritability - hypothesis of mating between G. scandens and G. fortis, producing hybrid birds:  	
  
* The hybrid birds then mate with pure G. scandens birds - process called "introgressive hybridization"  
* Question for testing is "how strongly are traits inherited by offspring from parents?"  
	* Assess for both G. scandens and G. fortis  
  
Final thoughts and course recap:  
  
* Perform EDA - ECDF, summary statistics  
* Estimate parameters - optimization, linear regression, confidence intervals (bootstrap method)  
* Formulate and test hypotheses  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2


def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates: bs_slope_reps, bs_intercept_reps
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    
    return bs_slope_reps, bs_intercept_reps



rawFinch1975 = pd.read_csv(myPath + "finch_beaks_1975.csv", header=0)
rawFinch1975.columns = ["band", "species", "beak_length", "beak_depth"]
rawFinch1975["year"] = 1975
rawFinch2012 = pd.read_csv(myPath + "finch_beaks_2012.csv", header=0)
rawFinch2012.columns = ["band", "species", "beak_length", "beak_depth"]
rawFinch2012["year"] = 2012

df = rawFinch1975.append(rawFinch2012)
bd_1975 = df.loc[(df["year"] == 1975) & (df["species"] == "scandens"), "beak_depth"].reset_index(drop=True)
bd_2012 = df.loc[(df["year"] == 2012) & (df["species"] == "scandens"), "beak_depth"].reset_index(drop=True)
bl_1975 = df.loc[(df["year"] == 1975) & (df["species"] == "scandens"), "beak_length"].reset_index(drop=True)
bl_2012 = df.loc[(df["year"] == 2012) & (df["species"] == "scandens"), "beak_length"].reset_index(drop=True)


# Create bee swarm plot
_ = sns.swarmplot(x="year", y="beak_depth", data=df.loc[df["species"] == "scandens", :])

# Label the axes
_ = plt.xlabel('year')
_ = plt.ylabel('beak depth (mm)')

# Show the plot
# plt.show()
plt.savefig("_dummyPy183.png", bbox_inches="tight")
plt.clf()


# function ecdf() is available from previous chapters
# Compute ECDFs
x_1975, y_1975 = ecdf(bd_1975)
x_2012, y_2012 = ecdf(bd_2012)

# Plot the ECDFs
_ = plt.plot(x_1975, y_1975, marker='.', linestyle='none')
_ = plt.plot(x_2012, y_2012, marker='.', linestyle='none')

# Set margins
plt.margins(0.02)

# Add axis labels and legend
_ = plt.xlabel('beak depth (mm)')
_ = plt.ylabel('ECDF')
_ = plt.legend(('1975', '2012'), loc='lower right')

# Show the plot
# plt.show()
plt.savefig("_dummyPy184.png", bbox_inches="tight")
plt.clf()


# draw_bs_reps() is available from previous chapters
# Compute the difference of the sample means: mean_diff
mean_diff = np.mean(bd_2012) - np.mean(bd_1975)

# Get bootstrap replicates of means
bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, 10000)

# Compute samples of difference of means: bs_diff_replicates
bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

# Compute 95% confidence interval: conf_int
conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5])

# Print the results
print('difference of means =', mean_diff, 'mm')
print('95% confidence interval =', conf_int, 'mm')


# Compute mean of combined data set: combined_mean
combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))

# Shift the samples
bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean
bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean

# Get bootstrap replicates of shifted data sets
bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, 10000)

# Compute replicates of difference of means: bs_diff_replicates
bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

# Compute the p-value
p = np.sum(bs_diff_replicates >= mean_diff) / len(bs_diff_replicates)

# Print p-value
print('p =', p)


# Make scatter plot of 1975 data
_ = plt.plot(bl_1975, bd_1975, marker='.',
             linestyle='none', color="blue", alpha=0.5)

# Make scatter plot of 2012 data
_ = plt.plot(bl_2012, bd_2012, marker='.',
             linestyle='none', color="red", alpha=0.5)

# Label axes and make legend
_ = plt.xlabel('beak length (mm)')
_ = plt.ylabel('beak depth (mm)')
_ = plt.legend(('1975', '2012'), loc='upper left')

# Show the plot
# plt.show()
plt.savefig("_dummyPy185.png", bbox_inches="tight")
plt.clf()


# draw_bs_pairs_linreg() is available in previous chapters
# Compute the linear regressions
slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, 1)
slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, 1)

# Perform pairs bootstrap for the linear regressions
bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, 1000)
bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, 1000)

# Compute confidence intervals of slopes
slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5])
slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5])
intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5])
intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5])


# Print the results
print('1975: slope =', slope_1975,
      'conf int =', slope_conf_int_1975)
print('1975: intercept =', intercept_1975,
      'conf int =', intercept_conf_int_1975)
print('2012: slope =', slope_2012,
      'conf int =', slope_conf_int_2012)
print('2012: intercept =', intercept_2012,
      'conf int =', intercept_conf_int_2012)


# Make scatter plot of 1975 data
_ = plt.plot(bl_1975, bd_1975, marker='.',
             linestyle='none', color='blue', alpha=0.5)

# Make scatter plot of 2012 data
_ = plt.plot(bl_2012, bd_2012, marker='.',
             linestyle='none', color='red', alpha=0.5)

# Label axes and make legend
_ = plt.xlabel('beak length (mm)')
_ = plt.ylabel('beak depth (mm)')
_ = plt.legend(('1975', '2012'), loc='upper left')

# Generate x-values for bootstrap lines: x
x = np.array([10, 17])

# Plot the bootstrap lines
for i in range(100):
    plt.plot(x, bs_intercept_reps_1975[i] + bs_slope_reps_1975[i] * x,
             linewidth=0.5, alpha=0.2, color="blue")
    plt.plot(x, bs_intercept_reps_2012[i] + bs_slope_reps_2012[i] * x,
             linewidth=0.5, alpha=0.2, color="red")

# Draw the plot again
# plt.show()
plt.savefig("_dummyPy186.png", bbox_inches="tight")
plt.clf()


# Compute length-to-depth ratios
ratio_1975 = bl_1975 / bd_1975
ratio_2012 = bl_2012 / bd_2012

# Compute means
mean_ratio_1975 = np.mean(ratio_1975)
mean_ratio_2012 = np.mean(ratio_2012)

# Generate bootstrap replicates of the means
bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, 10000)

# Compute the 99% confidence intervals
conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5])
conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5])

# Print the results
print('1975: mean ratio =', mean_ratio_1975,
      'conf int =', conf_int_1975)
print('2012: mean ratio =', mean_ratio_2012,
      'conf int =', conf_int_2012)


rawScandensBD = pd.read_csv(myPath + "scandens_beak_depth_heredity.csv", header=0)
rawFortisBD = pd.read_csv(myPath + "fortis_beak_depth_heredity.csv", header=0)

bd_parent_fortis = rawFortisBD.iloc[:, 1:].apply(np.mean, axis=1)
bd_offspring_fortis = rawFortisBD["Mid-offspr"]
bd_parent_scandens = rawScandensBD["mid_parent"]
bd_offspring_scandens = rawScandensBD["mid_offspring"]


# Make scatter plots
_ = plt.plot(bd_parent_fortis, bd_offspring_fortis,
             marker=".", linestyle="none", color="blue", alpha=0.5)
_ = plt.plot(bd_parent_scandens, bd_offspring_scandens,
             marker=".", linestyle="none", color="red", alpha=0.5)

# Set margins
plt.margins(0.02)

# Label axes
_ = plt.xlabel('parental beak depth (mm)')
_ = plt.ylabel('offspring beak depth (mm)')

# Add legend
_ = plt.legend(('G. fortis', 'G. scandens'), loc='lower right')

# Show plot
# plt.show()
plt.savefig("_dummyPy187.png", bbox_inches="tight")
plt.clf()


def draw_bs_pairs(x, y, func, size=1):
    """Perform pairs bootstrap for a specified function (func)."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_replicates[i] = func(bs_x, bs_y)
    
    return bs_replicates


# Note that pearson_r() exists from the previous course
# Compute the Pearson correlation coefficients
r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens)
r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis)

# Acquire 1000 bootstrap replicates of Pearson r
bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, 1000)
bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, 1000)

# Compute 95% confidence intervals
conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5])
conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5])

# Print results
print('G. scandens:', r_scandens, conf_int_scandens)
print('G. fortis:', r_fortis, conf_int_fortis)


def heritability(parents, offspring):
    """Compute the heritability from parent and offspring samples."""
    covariance_matrix = np.cov(parents, offspring)
    return covariance_matrix[0, 1] / covariance_matrix[0, 0]

# Compute the heritability
heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens)
heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis)

# Acquire 1000 bootstrap replicates of heritability
replicates_scandens = draw_bs_pairs(
        bd_parent_scandens, bd_offspring_scandens, heritability, size=1000)
replicates_fortis = draw_bs_pairs(
        bd_parent_fortis, bd_offspring_fortis, heritability, size=1000)

# Compute 95% confidence intervals
conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5])
conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5])

# Print results
print('G. scandens:', heritability_scandens, conf_int_scandens)
print('G. fortis:', heritability_fortis, conf_int_fortis)


# Initialize array of replicates: perm_replicates
perm_replicates = np.empty(10000)

# Draw replicates
for i in range(10000):
    # Permute parent beak depths
    bd_parent_permuted = np.random.permutation(bd_parent_scandens)
    perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens)

# Compute p-value: p
p = np.sum(perm_replicates >= heritability_scandens) / len(perm_replicates)

# Print the p-value
print('p-val =', p)

```
  

**Example #1: G. Scandens Beak Depth (1975 vs. 2012) Swarm Plot**:  
![](_dummyPy183.png)

**Example #2: G. Scandens Beak Depth (1975 vs. 2012) ECDF**:  
![](_dummyPy184.png)

**Example #3: G. Scandens Beak Depth vs. Length (1975 vs. 2012) Scatter Plot**:  
![](_dummyPy185.png)

**Example #4: G. Scandens Beak Depth vs. Length (1975 vs. 2012) Bootstrap Replicates**:  
![](_dummyPy186.png)

**Example #5: G. Scandens and G. Fortis Beak Depth Heritability**:  
![](_dummyPy187.png)

  

## Python Machine Learning  
###_Supervised Learning with scikit-learn_#

Chapter 1 - Classification  
  
Supervised learning - labelled data, where there is a correct answer:  
  
* Machine learning - the art and science of giving computers the ability to learn and make decisions from data (without being explicitly programmed)  
* Reinforcement learning - software agents interact with an environment, learning to optimize behavior for a system of rewards and punishments (like behavioral psychology)  
* Supervised Learning - labelled data, with both a target variable and 1+ predictor/feature variables  
	* Classification problems are categorical  
    * Regression problems are continuous  
* Note that features, predictor variables, and independent variables all mean the same thing  
* Similarly, target variables may also be called response variables or dependent variables  
* This course will use scikit-learn/sklearn - integrated well with the SciPy stack  
	* Other libraries like TensorFlow and keras are also good  
  
Exploratory data analysis - beginning with standard "iris" dataset with "species" as the target variable:  
  
* Can grab the dataset files from sklearn using "from sklearn import datasets"  
	* Typically will also need pd, np, plt, and the 'ggplot' style as set using plt.style.use('ggplot')  
    * iris = datasets.load_iris()  # will load the iris data as a "Bunch" file - set of key-value pairs - "data", "target_names", "DESCR", "feature_names", "target"  
    * The iris.data (150x4) and iris.target are both NumPy arrays, which iris.target_names is needed to map the numbers in iris.target to actual flower names  
* For EDA, can be helpful to use pd.scatter_matrix(pd.DataFrame(iris.data, columns=iris.feature_name), color=iris.target)  
  
Classification challenge - take training (already labelled) data to predict test (not yet labelled) data:  
  
* kNN (k-Nearest-Neighbors) is the process of classifying based on the closest "k" data points -- creates decision boundaries  
	* from sklearn.neighbors import KNeighborsClassifier  
    * knn = KNeighborsClassifiers(n_neighbors=6)  # this optimizer require that there is no missing data and that the features are all continuous rather than categorical  
* All machine learning models are implemented as Python classes - implement algorithms for both learning and predicting  
	* Training (fitting) is run with the .fit() method - for example, knn.fit(iris["data"], iris["target"])  
    * Predicting an unlabelled data point is run with .predict() - for example, prediction=knn.predict(X_new)  # must be a NumPy array  
  
Measuring model performance:  
  
* Accuracy is a common metric - # Correct / # Predicted - typically assessed on the "test" data  
	* Train/fit on the training data, but assess the accuracy on the test data  
    * from sklearn.model_selection import train_test_split  
    * X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)  # test_size is proportion to test, while random_state sets the seed for reproducibility  
* Can assess scores of the model using knn.score(X_test, y_test)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



df = pd.read_csv(myPath + "house-votes-84.csv", header=None, na_values="?")
temp = [x for x in df.columns]
temp[0] = "party"
df.columns = temp

df = df.dropna()
for ctr in range(1, df.shape[1]):
    df.iloc[:, ctr] = [1 if x == "y" else 0 for x in df.iloc[:, ctr]]


# Import KNeighborsClassifier from sklearn.neighbors
from sklearn.neighbors import KNeighborsClassifier

# Create arrays for the features and the response variable
y = df['party'].values
X = df.drop('party', axis=1).values

# Create a k-NN classifier with 6 neighbors
knn = KNeighborsClassifier(n_neighbors=6)

# Fit the classifier to the data
knn.fit(X, y)

# Predict the labels for the training data X
y_pred = knn.predict(X)

# Predict and print the label for the new data point X_new
X_new = np.array([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0]).reshape(1, -1)
new_prediction = knn.predict(X_new)
print("Prediction: {}".format(new_prediction))


# Import necessary modules
from sklearn import datasets
import matplotlib.pyplot as plt

# Load the digits dataset: digits
digits = datasets.load_digits()

# Print the keys and DESCR of the dataset
print(digits.keys())
print(digits["DESCR"])

# Print the shape of the images and data keys
print(digits.images.shape)
print(digits.data.shape)

# Display digit 1010
plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')
# plt.show()
plt.savefig("_dummyPy188.png", bbox_inches="tight")
plt.clf()

# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split

# Create feature and target arrays
X = digits.data
y = digits.target

# Split into training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)

# Create a k-NN classifier with 7 neighbors: knn
knn = KNeighborsClassifier(n_neighbors=7)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Print the accuracy
print(knn.score(X_test, y_test))


# Setup arrays to store train and test accuracies
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

# Loop over different values of k
for i, k in enumerate(neighbors):
    # Setup a k-NN Classifier with k neighbors: knn
    knn = KNeighborsClassifier(n_neighbors=k)
    
    # Fit the classifier to the training data
    knn.fit(X_train, y_train)
    
    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(X_train, y_train)
    
    #Compute accuracy on the testing set
    test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
# plt.show()
plt.savefig("_dummyPy189.png", bbox_inches="tight")
plt.clf()

```
  
**Example #1: Digital Image**:  
![](_dummyPy188.png)
  
**Example #2: Train/Test Accuracy vs. # Neighbors**:  
![](_dummyPy189.png)
  
***
  
Chapter 2 - Regression  
  
Introduction to regression - target variable is a continuous variable (e.g., Boston housing dataset):  
  
* Suppose that there is a pd.DataFrame called "boston" which has target variable "MEDV" and a number of predictors  
	* X = boston.drop("MEDV", axis=1).values  
    * y = boston["MEDV"].values  
    * May need to call .reshape(-1, 1) to convert them for future analysis (???) - seems to convert from .shape (X,) to .shape(X, 1) which is preferred by scikit  
* In general, to run a linear regression using sklearn  
	* from sklearn import linear_model  
    * reg = linear_model.LinearRegression()  
    * reg.fit(X, y)  
  
Basics of linear regression - simple linear regression has the format y = a*x + b:  
  
* The error (loss, cost) function for linear regression is typically the sum-square of the residuals -- OLS (Ordinary Least Squares)  
* The scikit API works the same for simple regression with one feature and regression with multiple features  
* The default .score() for regression in scikit is R-squared  
  
Cross-validation - accounting for model performance being dependent on the way the data is split (not generalizable):  
  
* One CV approach splits the data in to k-folds, using each of the folds as its own hold-put sample  
* Implementing cross-validation in scikit-learn includes  
	* from sklearn.model_selection import cross_val_score  
    * reg = LinearRegression()  
    * cv_results = cross_val_score(reg, X, y, cv=5)  # for 5-fold  
  
Regularized regression - avoiding over-fitting due to large coefficients:  
  
* Regularization involves placing a penalty on large coefficients  
* Ridge regression loss function is standard OLS loss function + alpha * sum-squared(coefficients)  # alpha is sometimes called lambad; when 0, this is the same as OLS  
	* from sklearn.linear_model import Ridge  
    * ridge = Ridge(alpha=0.1, normalize=True) # assuming lambda=0.1, run on standardized scale is desired  
    * ridge.fit(X_train, y_train)  
    * ridge_pred = ridge.predict(X_test)  
    * ridge.score(X_test, y_test)  
* Lasso regression loss function is standard OLS loss function + alpha * sum-absolute-value(coefficients)  
	* Run the same as ridge regression, except that Lasso is used for the import and Lasso() for the initial object setup  
    * Lasso regression is especially useful for feature selection, since it tends to select 0 as the coefficient for less important variables  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# Read the CSV file into a DataFrame: df
df = pd.read_csv(myPath + "gapminder_tidy.csv")
df = df.loc[df["Year"] == 2010, :]


# Create arrays for features and target variable
y = df["life"].values
X = df["fertility"].values

# Print the dimensions of X and y before reshaping
print("Dimensions of y before reshaping: {}".format(y.shape))
print("Dimensions of X before reshaping: {}".format(X.shape))

# Reshape X and y
y = y.reshape(-1, 1)
X = X.reshape(-1, 1)

# Print the dimensions of X and y after reshaping
print("Dimensions of y after reshaping: {}".format(y.shape))
print("Dimensions of X after reshaping: {}".format(X.shape))


# Import LinearRegression
from sklearn.linear_model import LinearRegression

# Create the regressor: reg
reg = LinearRegression()
X_fertility = X.copy()

# Create the prediction space
prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)

# Fit the model to the data
reg.fit(X_fertility, y)

# Compute predictions over the prediction space: y_pred
y_pred = reg.predict(prediction_space)

# Print R^2 
print(reg.score(X_fertility, y))

# Plot regression line
plt.plot(prediction_space, y_pred, color='black', linewidth=3)
# plt.show()
plt.savefig("_dummyPy190.png", bbox_inches="tight")
plt.clf()


# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)

# Create the regressor: reg_all
reg_all = LinearRegression()

# Fit the regressor to the training data
reg_all.fit(X_train, y_train)

# Predict on the test data: y_pred
y_pred = reg_all.predict(X_test)

# Compute and print R^2 and RMSE
print("R^2: {}".format(reg_all.score(X_test, y_test)))
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error: {}".format(rmse))


# Import the necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Create a linear regression object: reg
reg = LinearRegression()

# Compute 5-fold cross-validation scores: cv_scores
cv_scores = cross_val_score(reg, X, y, cv=5)

# Print the 5-fold cross-validation scores
print(cv_scores)

print("Average 5-Fold CV Score: {}".format(np.mean(cv_scores)))


# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Create a linear regression object: reg
reg = LinearRegression()

# Perform 3-fold CV
cvscores_3 = cross_val_score(reg, X, y, cv=3)
print(np.mean(cvscores_3))

# Perform 10-fold CV
cvscores_10 = cross_val_score(reg, X, y, cv=10)
print(np.mean(cvscores_10))


# Use mtcars
# df = pd.read_csv(myPath + "mtcars.csv", index_col=0)
# df = df[["mpg", "disp", "hp", "drat", "wt", "qsec"]]

# use winequality-red
# df = pd.read_csv(myPath + "winequality-red.csv", header=0, sep=";")

# use gapminder
df = pd.read_csv(myPath + "gm_2008_region.csv")
df_columns = ['population', 'fertility', 'HIV', 'CO2', 'BMI_male', 'GDP', 'BMI_female', 'child_mortality']

X = df[df_columns].values
y = df["life"].values.reshape(-1, 1)

# Import lasso
from sklearn.linear_model import Lasso

# Instantiate a lasso regressor: lasso
lasso = Lasso(alpha=0.4, normalize=True)

# Fit the regressor to the data
lasso.fit(X, y)

# Compute and print the coefficients
lasso_coef = lasso.coef_
print(lasso_coef)

# Plot the coefficients
plt.plot(range(len(df_columns)), lasso_coef)
plt.xticks(range(len(df_columns)), df_columns, rotation=60)
plt.margins(0.02)
# plt.show()
plt.savefig("_dummyPy191.png", bbox_inches="tight")
plt.clf()


def display_plot(cv_scores, cv_scores_std):
    fig = plt.figure()
    ax = fig.add_subplot(1,1,1)
    ax.plot(alpha_space, cv_scores)
    
    std_error = cv_scores_std / np.sqrt(10)
    
    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)
    ax.set_ylabel('CV Score +/- Std Error')
    ax.set_xlabel('Alpha')
    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')
    ax.set_xlim([alpha_space[0], alpha_space[-1]])
    ax.set_xscale('log')
    # plt.show()
    plt.savefig("_dummyPy192.png", bbox_inches="tight")
    plt.clf()


# Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# Setup the array of alphas and lists to store scores
alpha_space = np.logspace(-4, 0, 50)
ridge_scores = []
ridge_scores_std = []

# Create a ridge regressor: ridge
ridge = Ridge(normalize=True)

# Compute scores over range of alphas
for alpha in alpha_space:
    
    # Specify the alpha value to use: ridge.alpha
    ridge.alpha = alpha
    
    # Perform 10-fold CV: ridge_cv_scores
    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)
    
    # Append the mean of ridge_cv_scores to ridge_scores
    ridge_scores.append(np.mean(ridge_cv_scores))
    
    # Append the std of ridge_cv_scores to ridge_scores_std
    ridge_scores_std.append(np.std(ridge_cv_scores))

# Display the plot
display_plot(ridge_scores, ridge_scores_std)

```
  
**Example #1: Linear Regression Line**:  
![](_dummyPy190.png)

**Example #2: Lasso Coefficients**:  
![](_dummyPy191.png)

**Example #3: Ridge Regression with Varying Alpha**:  
![](_dummyPy192.png)
  
  
***
  
Chapter 3 - Model Fine-Tuning  
  
How good is the model?  Accuracy is not always a useful metric, especially in a "class imbalanced" dataset (e.g., 99% of the items are of type A, so 99% accuracy is meaningless):  
  
* Confusion matrix for classification - rows are Actual, columns are Predicted  
	* Precision (aka Positive Predictve Value or PPV): TP / (TP + FP) # of the items that are predicted as Positive, how many are actually positive?  
    * Recall (aka Sensitivity): TP / (TP + FN) # of the items that are actually Positive, how many are predicted Positive  
    * F1 Score (harmonic mean of Precision and Recall): 2 * Precision * Recall / (Precision + Recall)  
* High Precision means that few actual negatives are misclassified as positive  
* High Recall means that most actual positives are accurately classified as positive  
* Automated routine for running confusion matrices in scikit-learn  
	* from sklearn.metrics import classification_report, confusion_matrix  
    * knn = KNeighborsClassifier(n_neighbors=k) ; knn.fit(X_train, y_train) ; y_pred = knn.predict(X_test)  
    * confusion_matrix(y_test, y_pred)  # will give the 2x2  
    * classification_report(y_test, y_pred)  # will give precision-f1-recall-support for each of the possible states of y, plus an aggregate Total  
  
Logistic regression and ROC curves - binary classification that outputs target probability between 0 and 1:  
  
* The logit can be automated in scikit-learn very similarly to the other methodologies  
	* from sklearn.linear_model import LogisticRegression  
    * logreg = LogisticRegression()  
    * logreg.fit(X_train, y_train)  
    * y_pred = logreg.predict(X_test)  
* The ROC (Receiver Operator Characteristic) curve is the plot of True Positive Rate vs. False Positive Rate for all classification thresholds between 0 and 1  
	* from sklearn.metrics import roc_curve  
    * y_pred_prob = logreg.predict_proba(X_test)[:, 1]  # gives back the probabilities rather than the classifications - second column (index 1) is the probability of predicted labels being 1  
    * fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)  
  
Area under ROC curve (AUC) - larger area means better model:  
  
* Running the AUC for a single run of the model  
	* from sklearn.metrics import roc_auc_score  
    * y_pred_prob = logreg.predict_proba(X_test)[:, 1]  
    * roc_auc_score(y_test, y_pred_prob)  
* Alternately, running cross-validation on the AUC  
	* cv_scores = cross_val_score(logreg, X, y, cv=5, scoring="roc_auc")  
  
Hyperparameter tuning - selecting among the possible control parameters for the model:  
  
* Choosing the correct hyper-parameters is key to designing a good model; search the potential solution space, and choose the best  
	* Essential to use cross-validation in this process, as there is otherwise real risk of over-fitting  
* The grid-search cross-validation process is implemented in scikit  
	* from sklearn.model_selection import GridSearchCV  
    * param_grid = { "n_neighbors":np.arange(1, 50) }  # set up as a dictionary, can have as many items as desired  
    * knn = KNeighborsClassifier()  
    * knn_cv = GridSearchCV(knn, param_grid, cv=5)  
    * knn_cv.fit(X, y)  # this performs the actual optimizations for each cell in the grid  
    * knn_cv.best_params_  ; knn_cv.best_score_  # get back the best parameter and the best score  
  
Hold-out set (final evaluation) - how well can the model perform on never before seen data?:  
  
* This requires holding some data back from the CV process  
* Basically, the initial split is Train vs. Hold-Out, with only Train used in the CV, model-fitting, and the like  
* The final report of model performance is based on running the final model selected on the Hold-Out sample  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# PIMA diabetes data
myDF = pd.read_csv(myPath + "diabetes.csv")
y = myDF["diabetes"].values
X = myDF.drop("diabetes", axis=1).values
X_columns = myDF.drop("diabetes", axis=1).columns


# Import necessary modules
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Create training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)

# Instantiate a k-NN classifier: knn
knn = KNeighborsClassifier(n_neighbors=6)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Import the necessary modules
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)

# Create the classifier: logreg
logreg = LogisticRegression()

# Fit the classifier to the training data
logreg.fit(X_train, y_train)

# Predict the labels of the test set: y_pred
y_pred = logreg.predict(X_test)

# Compute and print the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


# Import necessary modules
from sklearn.metrics import roc_curve

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
# plt.show()
plt.savefig("_dummyPy193.png", bbox_inches="tight")
plt.clf()


# Import necessary modules
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Compute and print AUC score
print("AUC: {}".format(roc_auc_score(y_test, y_pred_prob)))

# Compute cross-validated AUC scores: cv_auc
cv_auc = cross_val_score(logreg, X, y, cv=5, scoring="roc_auc")

# Print list of AUC scores
print("AUC scores computed using 5-fold cross-validation: {}".format(cv_auc))


# Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: C.
# C controls the inverse of the regularization strength, and this is what you will tune in this exercise.
# A large C can lead to an overfit model, while a small C can lead to an underfit model.

# Import necessary modules
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Setup the hyperparameter grid
c_space = np.logspace(-5, 8, 15)
param_grid = {'C': c_space}

# Instantiate a logistic regression classifier: logreg
logreg = LogisticRegression()

# Instantiate the GridSearchCV object: logreg_cv
logreg_cv = GridSearchCV(logreg, param_grid, cv=5)

# Fit it to the data
logreg_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Logistic Regression Parameters: {}".format(logreg_cv.best_params_)) 
print("Best score is {}".format(logreg_cv.best_score_))


# GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. 
# A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out.
# Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions.

# Import necessary modules
from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV

# Setup the parameters and distributions to sample from: param_dist
param_dist = {"max_depth": [3, None],
              "max_features": randint(1, 9),
              "min_samples_leaf": randint(1, 9),
              "criterion": ["gini", "entropy"]}

# Instantiate a Decision Tree classifier: tree
tree = DecisionTreeClassifier()

# Instantiate the RandomizedSearchCV object: tree_cv
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

# Fit it to the data
tree_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))


# Import necessary modules
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Create the hyperparameter grid
c_space = np.logspace(-5, 8, 15)
param_grid = {"C": c_space, "penalty": ['l1', 'l2']}

# Instantiate the logistic regression classifier: logreg
logreg = LogisticRegression()

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Instantiate the GridSearchCV object: logreg_cv
logreg_cv = GridSearchCV(logreg, param_grid, cv=5)

# Fit it to the training data
logreg_cv.fit(X_train, y_train)

# Print the optimal parameters and best score
print("Tuned Logistic Regression Parameter: {}".format(logreg_cv.best_params_))
print("Tuned Logistic Regression Accuracy: {}".format(logreg_cv.best_score_))


# Import necessary modules
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Create the hyperparameter grid
l1_space = np.linspace(0.01, 0.99, 30)
param_grid = {"l1_ratio": l1_space}

# Instantiate the ElasticNet regressor: elastic_net
elastic_net = ElasticNet()

# Setup the GridSearchCV object: gm_cv
gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)

# Fit it to the training data
gm_cv.fit(X_train, y_train)

# Predict on the test set and compute metrics
y_pred = gm_cv.predict(X_test)
r2 = gm_cv.score(X_test, y_test)
mse = mean_squared_error(y_test, y_pred)
print("Tuned ElasticNet l1 ratio: {}".format(gm_cv.best_params_))
print("Tuned ElasticNet R squared: {}".format(r2))
print("Tuned ElasticNet MSE: {}".format(mse))

```
  

**Example #1: ROC Curve for PIMA Diabetes Prediction**:  
![](_dummyPy193.png)
  
  
***
  
Chapter 4 - Pre-processing and Pipelines  
  
Preprocessing data - preparing the data for use in the analysis pipeline:  
  
* Categorical features are by default NOT accepted by the scikit-learn API, although they are OK once split in to dummy variables  
	* scikit-learn: OneHotEncoder()  
    * pandas: get_dummies()  # creates dummy variables on the entire DataFrame, including a column for all levels (one of which should be dropped prior to regression)  
  
Handling missing data - including "stealth" encoding such as values of zero for fields which should be positive:  
  
* df["myCol"].replace(0, np.nan, inplace=True)  # will update df such that any 0 in myCol will be changed to np.nan  
* Can drop all rows with missing data by using df.dropna(), though this may result in an unacceptable amount of data loss (data remaining)  
* Can instead impute the missing values (using the mean/median for the column of the non-missing values)  
	* from sklearn.preprocessing import Imputer  
    * imp = Imputer(missing_values="NaN", strategy="mean", axis=0)  # axis=0 means imputing along columns, while axis=1 would mean rows  
    * imp.fit(X)  # this will fit the Imputer to the DataFrame X  
    * X = imp.transform(X)  # Imputer is known as a transformer  
* Alternately, can use a pipeline project to do the imputing and the model running all at the same time  
	* from sklearn.pipeline import Pipeline  
    * imp = Imputer(missing_values="NaN", strategy="mean", axis=0)  # axis=0 means imputing along columns, while axis=1 would mean rows  
    * logreg = LogisticRegression()  
    * steps = [ ("imputation", imp), ("logistic_regression", logreg) ]  # The tuples are the name and then the command  
    * pipeline = Pipeline(steps)  
    * pipeline.fit(X_train, y_train)  
    * y_pred = pipeline.predict(X_test) ; pipeline.score(X_test, y_test)  
* Note that all steps in a Pipeline() must be transformers  
  
Centering and scaling - particularly important for models that use some form of distance to inform them:  
  
* Can normalize (center and scale) the data using Python  
	* from sklearn.preprocessing import scale  
    * X_scaled = scale(X)  
* Alternately, can use the Pipeline() object, such as  
	* from sklearn.preprocessing import StandardScaler  
    * pipeline = Pipeline( ("scaler", StandardScaler()) , ("knn", KNeighborsClassifier()) )  
* Can further use CV and Grid Search with the Pipeline object  
	* parameters = { knn_n_neighbors=np.arange(1, 50) }  # The double underscore after knn means "use knn from pipeline" and "range n_neighbors over the assigned range"  
    * cv = GridSearchCV(pipeline, param_grid=parameters)  
    * cv.fit(X_train, y_train)  
    * y_pred = cv.predict(X_test)  
    * cv.best_params_  ; cv.score(X_test, y_test)  ; classification_report( y_test, y_pred )  
  
Final thoughts:  
  
* Machine learning techniques for classification and regression  
* Real-world data  
* Under-fitting and over-fitting  
* Test-train split  
* Cross-validation  
* Grid search  
* Regularization (lasso, ridge)  
* Pre-processing  
* Check out scikit-learn documentation or "Introducing Machine Learning for Python" by Andreas Muller  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import ElasticNet


# Read 'gapminder.csv' into a DataFrame: df
df = pd.read_csv(myPath + "gm_2008_region.csv")

# Create a boxplot of life expectancy per region
df.boxplot("life", "Region", rot=60)

# Show the plot
# plt.show()
plt.savefig("_dummyPy194.png", bbox_inches="tight")
plt.clf()


# Create dummy variables: df_region
df_region = pd.get_dummies(df)

# Print the columns of df_region
print(df_region.columns)

# Create dummy variables with drop_first=True: df_region
df_region = pd.get_dummies(df, drop_first=True)

# Print the new columns of df_region
print(df_region.columns)


# Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

X = df_region.drop("life", axis=1).values
y = df_region["life"].values

# Instantiate a ridge regressor: ridge
ridge = Ridge(alpha=0.5, normalize=True)

# Perform 5-fold cross-validation: ridge_cv
ridge_cv = cross_val_score(ridge, X, y, cv=5)

# Print the cross-validated scores
print(ridge_cv)


df = pd.read_csv(myPath + "house-votes-84.csv", header=None)
df.columns = ['party', 'infants', 'water', 'budget', 'physician', 'salvador', 'religious', 'satellite', 'aid', 'missile', 'immigration', 'synfuels', 'education', 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa']

# Convert '?' to NaN
df[df == "?"] = np.nan

# Print the number of NaNs
print(df.isnull().sum())

# Print shape of original DataFrame
print("Shape of Original DataFrame: {}".format(df.shape))

# Print shape of new DataFrame
print("Shape of DataFrame After Dropping All Rows with Missing Values: {}".format(df.dropna().shape))



df = pd.read_csv(myPath + "house-votes-84.csv", header=None)
df.columns = ['party', 'infants', 'water', 'budget', 'physician', 'salvador', 'religious', 'satellite', 'aid', 'missile', 'immigration', 'synfuels', 'education', 'superfund', 'crime', 'duty_free_exports', 'eaa_rsa']

df[df == "?"] = np.nan
df[df == "y"] = 1
df[df == "n"] = 0

X = df.drop("party", axis=1).values
y = df["party"].values


# Import the Imputer module
from sklearn.preprocessing import Imputer
from sklearn.svm import SVC

# Setup the Imputation transformer: imp
imp = Imputer(missing_values="NaN", strategy="most_frequent", axis=0)

# Instantiate the SVC classifier: clf
clf = SVC()

# Setup the pipeline with the required steps: steps
steps = [('imputation', imp),
        ('SVM', clf)]


# Import necessary modules
from sklearn.pipeline import Pipeline

# Setup the pipeline steps: steps
steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),
        ('SVM', SVC())]

# Create the pipeline: pipeline
pipeline = Pipeline(steps)

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit the pipeline to the train set
pipeline.fit(X_train, y_train)

# Predict the labels of the test set
y_pred = pipeline.predict(X_test)

# Compute metrics
print(classification_report(y_test, y_pred))




# Import scale
from sklearn.preprocessing import scale

# X is "quality" from the white wine quality dataset
rawWhite = pd.read_csv(myPath + "white-wine.csv")
y = rawWhite["quality"].values
X = rawWhite.drop("quality", axis=1).values

# Scale the features: X_scaled
X_scaled = scale(X)

# Print the mean and standard deviation of the unscaled features
print("Mean of Unscaled Features: {}".format(np.mean(X))) 
print("Standard Deviation of Unscaled Features: {}".format(np.std(X)))

# Print the mean and standard deviation of the scaled features
print("Mean of Scaled Features: {}".format(np.mean(X_scaled))) 
print("Standard Deviation of Scaled Features: {}".format(np.std(X_scaled)))


# Import the necessary modules
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Setup the pipeline steps: steps
steps = [('scaler', StandardScaler()),
        ('knn', KNeighborsClassifier())]
        
# Create the pipeline: pipeline
pipeline = Pipeline(steps)

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit the pipeline to the training set: knn_scaled
knn_scaled = pipeline.fit(X_train, y_train)

# Instantiate and fit a k-NN classifier to the unscaled data
knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)

# Compute and print metrics
print('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))
print('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))


# Setup the pipeline
steps = [("scaler", StandardScaler()),
         ("SVM", SVC())]

pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {'SVM__C':[1, 10, 100],
              'SVM__gamma':[0.1, 0.01]}

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)

# Instantiate the GridSearchCV object: cv
cv = GridSearchCV(pipeline, param_grid=parameters, cv=3)

# Fit to the training set
cv.fit(X_train, y_train)

# Predict the labels of the test set: y_pred
y_pred = cv.predict(X_test)

# Compute and print metrics
print("Accuracy: {}".format(cv.score(X_test, y_test)))
print(classification_report(y_test, y_pred))
print("Tuned Model Parameters: {}".format(cv.best_params_))


# Setup the pipeline steps: steps
steps = [("imputation", Imputer(missing_values="NaN", strategy="mean", axis=0)),
         ("scaler", StandardScaler()),
         ("elasticnet", ElasticNet())]

# Create the pipeline: pipeline 
pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {"elasticnet__l1_ratio":np.linspace(0.01 ,1 , 30)}

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Create the GridSearchCV object: gm_cv
gm_cv = GridSearchCV(pipeline, param_grid=parameters, cv=3)

# Fit to the training set
gm_cv.fit(X_train, y_train)

# Compute and print the metrics
r2 = gm_cv.score(X_test, y_test)
print("Tuned ElasticNet Alpha: {}".format(gm_cv.best_params_))
print("Tuned ElasticNet R squared: {}".format(r2))


```
  
  
**Example #1: Box Plot for Life Expectancy by Region (Gapminder)**:  
![](_dummyPy194.png)

	
###_Unsupervised Learning in Python_#
  
Chapter 1 - Clustering for dataset exploration  
  
Unsupervised learning - machine learning data to discover patterns in data:  
  
* Common techniques include clustering and dimension reduction  
* Unsupervised learning contrasts with supervised learning in that there are no labels on the data  
* An example dataset used will be "iris", though without the Species field  
* k-means clustering is implemented within scikit-learn  
	* from sklearn.cluster import KMeans  
    * model = KMeans(n_clusters=3)  
    * model.fit(myData)  
    * labels = model.predict(samples)  
* The k-means model will remember the centroids (cluster means) and assign new data points to the closest cluster  
	* newLabels = model.predict(newData)  
  
Evaluate a clustering - in a manner that does not necessarily assume that there are any pre-defined labels:  
  
* If the labels exist, can run a cross-tab of the segmentes created against the known labels  
	* df = pd.DataFrame( {"labels":myCluster , "species":actualSpecies} )  
    * ct = pd.crosstab(df["labels"], df["species"])  
    * print(ct)  
* If the labels do not exist, can still assess whether clusters tend to be similar within and different without  
	* Inertia: meaurement of clustering quality for how spread out the clusters are (lower is better); distance from each sample to cluster centroid  
    * model.inertia_ # Always calculated when the model.fit() is run, and can be accessed later  
    * Choosing an "elbow" point in the inertia plot is frequently a good strategy, balancing low Inertia with not too many clusters  
  
Transforming features for better clustering:  
  
* Example of the wine database where features have very different variances  
* Within k-means, the variance of a feature will be directly proportional to its influence  
* The StandardScaler() function as per above course transforms every feature to have mean 0 and variance 1  
	* from sklearn.preprocessing import StandardScaler  
    * scaler = StandardScaler()  
    * scaler.fit(samples)  
    * samples_scaled = scaler.transform(samples)  # note that scaler.transform() could instead be applied to brand new data  
* As per the previous course, pipelines can be used to combine a transformation followed by a model run  
	* from sklearn.pipeline import make_pipeline  
    * pipeline = make_pipeline(StandardScaler(), KMeans(n_clusters=3))  
    * pipeline.fit(samples)  # will run the standard scaling and then also build the 3 clusters  
    * labels = pipeline.predict(samples)  # get the projected labels for the samples  
* Other options for pre-processing include MaxAbsScaler and Normalizer  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans


xPoints = [65, -1529, 1709, 1167, -1801, -1635, 1219, -261, -1619, -1843, 1359, 422, -1685, 906, -1564, -25, 830, 1224, -153, 863, -1433, 38, -1585, 635, 244, -2196, 1032, -288, 205, -1210, 1334, 1194, 991, -1835, 84, -1489, 387, -14, -2052, 103, -441, 1103, -1592, -1473, -1355, 41, 476, 1043, -79, 870, 1402, 801, -165, -337, 806, -1435, 548, 468, -1513, 913, -1972, 521, 627, 941, 569, -1876, 906, -1374, -1805, 343, 703, -1684, 1314, 261, 919, -1217, -177, 648, 412, 1015, 679, -1051, 613, -1502, -1727, -1609, -1091, -10, 327, 1220, -1333, 879, -1148, -580, -11, 173, 1327, -1934, -1577, 476, -1652, -126, -1896, 574, -166, 801, 1184, 1088, -1611, -1575, 72, -1400, 1096, -2543, -1345, -1354, 183, -1313, 99, 1172, 86, -210, 264, -255, -260, -1917, 930, 447, -1638, -1957, 927, 1256, -1728, 349, -1516, 187, 896, -1549, 1215, 203, 845, 532, -53, 1388, 1006, 566, 868, -1162, 277, 165, 382, 1145, 814, -1860, 965, -1498, -133, 125, 1060, -1491, 1161, 542, 892, -1499, 316, 139, -1549, 1238, 1255, 255, 451, 1062, 674, 227, -1458, 232, 1596, 804, 1154, 596, 28, 1134, 215, 1410, 1122, 252, -1285, 1528, -239, -257, 593, 79, -1272, 669, 347, -2112, -1629, -1538, -119, -1544, 300, 9, -1334, 475, 593, 413, 558, 498, 356, -1861, -1619, 807, 1627, -1569, 1025, 242, 1231, -1965, 427, -1583, -1571, -981, -1486, 987, 1286, 858, 190, 265, -1618, 240, 1152, -1219, 334, 171, -1198, -122, 1512, 1103, 1309, 199, 814, -1642, 12, 337, 473, 347, -3, -1643, 1446, -1930, -2372, 458, 489, -1023, -1327, -1509, 27, 1016, 477, -1277, 498, 1060, 1002, 1311, 1134, 1276, -633, 126, -1345, -531, 219, -1789, 1231, 1416, -1902, -224, 595, 1219, -1994, -1278, 623, 140, -161, -1747, -1166, 411, 1147, -1655, -1153, 608, -84, 191, 296, -1368, -80, 356, 24, -1490, -1408, -1982]

yPoints = [-768, -429, 698, 1012, -318, -28, 746, -624, -479, -166, 949, -734, 106, 1091, -846, -1186, 1145, 1350, -717, 1068, -238, -746, 84, -584, -531, 494, -556, -399, -801, -349, 772, 1047, 892, -48, -715, -192, -820, -977, -21, -821, -657, 1024, -173, -22, 229, -1237, -1136, 823, -856, 1080, 1075, 1283, -1235, -591, -737, -563, -951, -749, -839, 1511, -411, -886, -1185, 1353, 1070, 147, 910, 284, -967, -799, 1000, -305, 986, -260, 823, -202, -1026, -668, -287, 1134, -914, -10, 1782, -523, -461, -182, -78, -809, -807, 1195, -300, 1115, -304, -1199, -784, -633, 677, 305, -277, 1214, -680, -1043, -700, 748, -831, 1222, 1434, 646, 228, -96, -696, 163, 1168, -230, 251, -191, -1158, -376, 1225, 1308, -797, -1134, -947, -1062, -747, 164, 494, -728, -589, -101, 882, 608, 84, -304, -509, -554, 835, -171, 1233, -1037, 1036, 1473, -1361, 1117, 1036, -1096, 1052, -558, -838, -803, -426, 896, 675, -95, 902, -332, -807, -1133, 1288, -202, 1288, 752, 992, -324, -1344, -1220, -107, 1376, 720, -1285, -559, 1034, -549, -1027, -49, -708, 1113, 544, 1043, -1191, -438, 563, -1044, 1028, 580, -825, -73, 1118, -705, -818, -452, -292, 116, 1007, -1208, -668, -327, -15, -603, -303, -777, -538, -115, 784, 1232, -1352, 788, -789, -810, -590, -160, -733, 627, -84, 893, -608, 1002, -158, -703, -194, 14, 61, -574, 691, 1281, 955, -821, -42, 8, -526, 439, -258, -861, -756, -737, -456, 888, 923, 590, 1148, 845, -422, -212, -746, 727, 1230, -972, 52, 1142, -400, -726, -1060, -1189, -175, -939, -94, -798, 1264, -989, -547, -623, 868, 1382, 628, 1183, 966, -977, -931, -237, -1251, -902, -121, -74, 1215, -163, -754, 924, 1081, -375, -524, -1014, -428, -1163, -60, 90, -808, 778, -401, 8, 689, -976, -423, -1133, -250, -567, -1150, -635, -248, 256, -545]

xNewPoints = [400, 803, -1395, -341, 1547, 245, 1207, 1251, 1810, -1669, -71, 682, 1090, -1676, -1844, 1242, -1861, -1460, 498, 983, -1831, 1306, 353, 1139, 293, -1145, 1187, -2232, -1283, 494, 63, 935, 1580, 1063, -1397, -109, 1173, 920, 582, 952, -1372, 21, 327, -1715, 512, 1130, -1430, -1827, 1418, 1268, 804, 834, -1247, -1294, -227, 218, -1225, -1312, -1335, -301, 202, 1013, 818, 1261, 464, 360, -2150, 1050, 106, -1739, -112, 516, 326, 1090, 364, -1907, -1956, 318, 736, -1441, 321, -1373, 68, 959, -1626, 1067, -1796, -98, -105, 879, 980, -83, 848, 1329, 323, -1526, -1485, -1804, -2019, 287, 822, -35, -1669, -1782, 725, 388, 1490, 0, -1345, -803, -249, -1410, -1751, -1279, 335, 1168, 715, 1300, -1216, -1707, 1207, 186, 803, -1734, 1312, 404, -1932, 656, -1405, -1743, 543, -1973, 1539, -1483, 386, 1170, -80, -68, 713, -1850, 63, 1529, 162, -323, -1517, 888, 483, -55, -1579, 55, -2060, 1118, 487, -1655, -1443, -170, -1824, 1030, 1644, -1476, 1009, -1452, -1740, 1220, 916, -1, 893, 633, -1479, 54, -1519, 289, -9, 1134, 536, 1147, 573, 212, 526, -2018, -235, 159, 228, -1536, 1139, 1011, -142, -2231, 125, -1724, -1604, 737, 672, 1200, 75, 1287, 1065, 1058, -1126, -1101, 235, 1138, 531, 433, -4, -1763, 1364, 566, 1357, 1071, 1001, 1330, -1796, -1272, 847, -1509, -1249, -1277, -1819, 1199, -1255, -1630, 1106, 670, 369, -1263, -366, 244, -1276, 37, -2191, -293, -1657, 736, 1137, -1441, 592, 651, 594, -1879, 215, 499, -1829, 792, 954, -1353, 1055, -1943, -1396, 1289, -1595, -1419, -23, -1260, 960, 551, 370, 825, -1870, -1562, -1263, -63, 885, -143, -1839, 274, -1457, -1590, 925, 197, 288, 724, -1626, 584, 1021, 865, 398, 862, -1359, -1191, 1293, 1256, 93, -1735, 223, 401, -1422, 392, 1251, 17, -1877, -1580, -1595, -1018, 1248, -1255, -1418, 525, 555, 28]

yNewPoints = [-1265, 1282, 55, -1076, 1402, -483, 888, 1155, 965, -308, -937, 1102, 1438, -504, 45, 1020, 6, 153, 898, 1043, -163, 1076, -751, 1545, -1261, -37, 600, 230, -393, -883, -911, 866, 1037, 1027, -505, -908, 947, 1457, -900, 899, -31, -707, -555, 215, -760, 722, -342, 117, 1114, 1419, 1639, 1099, -223, 81, -413, -468, 255, -428, 652, -1251, -905, 1123, 860, 1466, -797, 844, -369, 874, -749, 252, -652, -641, -880, 1109, -694, 187, 139, -405, 1764, -57, -534, 46, -1105, 652, -562, 729, -988, -68, 1170, 1253, 1154, -924, 1093, 1137, -749, -249, -268, 187, -449, -655, 1384, -801, -138, 293, -623, -761, 795, -740, -475, -309, -1006, -70, -300, 173, 624, 1189, 926, 916, 110, -59, 1054, -958, 386, -149, 1118, -510, 220, 961, 117, -74, 1472, -227, 1360, -490, -1261, 1185, -321, -852, 1278, -503, -1412, 966, -1373, -706, 187, 762, 881, -711, -629, -847, 58, 1302, -990, -169, 190, -826, -307, 1264, 1277, 26, 1142, -255, -83, 1286, 732, -726, 841, 1009, -699, -1064, -489, -773, -1133, 971, -847, 689, 790, -607, -815, 67, -1082, -600, -1160, 84, 631, 1043, -481, 84, -1017, -694, 445, 926, 1133, 726, -983, 1180, 1007, 1129, 166, -364, -139, 1010, -881, -764, -1305, -497, 583, 1513, 670, 611, 893, 879, -35, 400, 1170, -712, -557, -599, 737, 1569, -233, 161, 739, 870, -694, -699, -1353, -659, -485, -699, -491, -589, -298, 578, 1301, 31, 1225, 492, 825, -521, -1282, -670, -339, 1337, 980, 103, 807, -118, -310, 1017, -540, -381, -1101, -693, -811, -1017, -1068, 877, 287, -189, -774, -759, 885, -1160, -326, -1045, -291, 166, 741, -780, -832, -799, -180, 1131, 459, 957, -1242, 1109, 24, -38, 1103, -779, -553, -97, -943, -1109, 181, -878, 693, -720, -263, -550, -153, 38, 660, -29, -358, 870, 1187, -699]

points = list(zip([a/1000 for a in xPoints], [b/1000 for b in yPoints]))
new_points = list(zip([a/1000 for a in xNewPoints], [b/1000 for b in yNewPoints]))

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(points)

# Determine the cluster labels of new_points: labels
labels = model.predict(new_points)

# Print cluster labels of new_points
print(labels)


# Assign the columns of new_points: xs and ys
xs = [x for x, y in new_points]
ys = [y for x, y in new_points]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker="D", s=50)
# plt.show()
plt.savefig("_dummyPy195.png", bbox_inches="tight")
plt.clf()


rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values

ks = range(1, 6)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(samples)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
# plt.show()
plt.savefig("_dummyPy196.png", bbox_inches="tight")
plt.clf()


# Create a KMeans model with 3 clusters: model
model = KMeans(n_clusters=3)

# Use fit_predict to fit model and obtain cluster labels: labels
labels = model.fit_predict(samples)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df["labels"], df["varieties"])

# Display ct
print(ct)



# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)


rawFish = pd.read_csv(myPath + "fish.csv", header=None)
samples = rawFish.values[:, 1:]
species = rawFish.values[:, 0]


# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame( {"labels":labels, "species":species} )

# Create crosstab: ct
ct = pd.crosstab(df["labels"], df["species"])

# Display ct
print(ct)


# Note that Normalizer() is different to StandardScaler(), which you used in the previous exercise.
# While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, 
# Normalizer() rescales each sample - here, each company's stock price - independently of the other.


rawStock = pd.read_csv(myPath + "company-stock-movements-2010-2015-incl.csv")
movements = rawStock.values[:, 1:]
companies = rawStock.values[:, 0]

# Import Normalizer
from sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)



# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({'labels': labels, 'companies': companies})

# Display df sorted by cluster label
print(df.sort_values("labels"))

```
  
**Example #1: k-means Clustering**:  
![](_dummyPy195.png)  

**Example #2: Elbow Plot**:  
![](_dummyPy196.png)  
  
***
  
Chapter 2 - Visualization with Hierarchical Clustering and t-SNE  
  
Visualizing hierarchies - groups within groups:  
  
* Eurovision 2016 scoring examples - average scores by country given to various songs  
* The dendrogram (tree diagram) groups the countries together in to larger and larger groups  
* General process for agglomerative hierarchical clustering  
	* Start with everything as its own cluster  
    * Merge the two closest clusters (several methods for defining "closest")  
    * Repeat until there is one gian cluster  
* Hierarchical clustering can be run within SciPy  
	* from scipy.cluster.hierarchy import linkage, dendrogram  
    * mergings = linkage(samples, method="complete")  
    * dendrogram(mergings, lables=country_names, leaf_rotation=90, leaf_font_size=6)  
  
Cluster labels in hierarchical clustering - extracting information from intermediate levels of clustering:  
  
* Intermediate clustering is defined by selecting a height on the dendrogram  
	* The y-axis on a dendrogram is the distance between merging clusters  
    * As such, the selected height on the dendrogram corresponds to a distance between clusters (can stop merging when things as this far apart)  
* Distance between clusters can be measured in many ways - called the linkage method  
	* Complete - distance is defined as the MAXIMUM distance between points in the clusters  
    * Single - distance is defined as the MINIMUM distance between points in the clusters  
* Intermediate cluster labels can be extracted using the fcluster method, which will return a NumPy array of cluster labels  
	* from scipy.cluster.hierarchy import fcluster  
    * intClusters = fcluster(mergings, 15, criterion="distance")  # maximum distance allowed is 15  
    * Can add labels by making a data frame with the values and then sorting by intClusters  
  
t-SNE for 2-dimensional maps (create a 2D map of a dataset) - "t-distributed stochastic neighbor embedding":  
  
* Maps samples to 2D (or occasionally 3D) while preserving nearness of the samples  
* Example using the 4D iris data, provided in an unlabelled manner  
	* The setosa samples end up far apart, but the virginical and versicolor samples end pretty close to each other (consistent with the "elbow" in the inertia plot being at 2 or 3)  
* Running t-SNE in Python  
	* from sklearn.manifold import TSNE  
    * model = TSNE(learning_rate=100)  # may need to try different learning rates (typically 50-200 are good choices), though a bad choice is obvious in badly bunched data  
    * transformed = model.fit_transform(samples)  # fit_transform simultaneously fits and transformes the data (cannot be extended to new data)  
    * xs = transformed[:, 0]  ; ys = transformed[:, 1]  # note that the axes have absolutely no real-world meaning whatsoever  
    * plt.scatter(xs, ys, c=species)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Perform the necessary imports
from sklearn.preprocessing import normalize
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.cluster.hierarchy import fcluster
from sklearn.manifold import TSNE



rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values


# Calculate the linkage: mergings
mergings = linkage(samples, method="complete")

# Plot the dendrogram, using varieties as labels
dendrogram(mergings,
           labels=varieties,
           leaf_rotation=90,
           leaf_font_size=6,
)
# plt.show()
plt.savefig("_dummyPy197.png", bbox_inches="tight")
plt.clf()



rawStock = pd.read_csv(myPath + "company-stock-movements-2010-2015-incl.csv")
movements = rawStock.values[:, 1:]
companies = rawStock.values[:, 0]

# Normalize the movements: normalized_movements
normalized_movements = normalize(movements)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements, method="complete")

# Plot the dendrogram
dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)
# plt.show()
plt.savefig("_dummyPy198.png", bbox_inches="tight")
plt.clf()


rawEuro = pd.read_csv(myPath + "eurovision-2016.csv")

pvtEuro = rawEuro[["From country", "To country", "Televote Rank"]].pivot_table(index="From country", columns="To country", values="Televote Rank", aggfunc=np.mean)

samples = pvtEuro.fillna(0).values
country_names = pvtEuro.index


# Calculate the linkage: mergings
mergings = linkage(samples, method="single")

# Plot the dendrogram
dendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)
# plt.show()
plt.savefig("_dummyPy199.png", bbox_inches="tight")
plt.clf()



rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values


# Calculate the linkage: mergings
mergings = linkage(samples, method="complete")


# Use fcluster to extract labels: labels
labels = fcluster(mergings, 6, criterion="distance")

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df["labels"], df["varieties"])

# Display ct
print(ct)



# Create a TSNE instance: model
model = TSNE(learning_rate=200)

# Apply fit_transform to samples: tsne_features
tsne_features = model.fit_transform(samples)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1st feature: ys
ys = tsne_features[:,1]

# Scatter plot, coloring by variety_numbers
plt.scatter(xs, ys, c=varieties)
# plt.show()
plt.savefig("_dummyPy200.png", bbox_inches="tight")
plt.clf()


# Create a TSNE instance: model
model = TSNE(learning_rate=50)

# Apply fit_transform to normalized_movements: tsne_features
tsne_features = model.fit_transform(normalized_movements)

# Select the 0th feature: xs
xs = tsne_features[:, 0]

# Select the 1th feature: ys
ys = tsne_features[:,1]

# Scatter plot
plt.scatter(xs, ys, alpha=0.5)

# Annotate the points
for x, y, company in zip(xs, ys, companies):
    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
# plt.show()
plt.savefig("_dummyPy201.png", bbox_inches="tight")
plt.clf()


```
  

**Example #1: Dendrogram (Grain Varieties)**:  
![](_dummyPy197.png)  

**Example #2: Dendrogram - Complete Linkage (Normalized Stock Price Movements)**:  
![](_dummyPy198.png)  

**Example #3: Dendrogram - Single Linkage (Eurovision Ratings)**:  
![](_dummyPy199.png)  

**Example #4: t-SNE (Grain Varieties)**:  
![](_dummyPy200.png)  

**Example #5: t-SNE (Normalized Stock Movements)**:  
![](_dummyPy201.png)  


***
  
Chapter 3 - Decorrelating Data and Dimension Reduction  
  
Visualizing PCA transformations - using dimension reduction for more efficient storage and computation:  
  
* Goal is to remove the "noisy" features which both take up space and cause problems with prediction tasks  
* PCA (Principal Component Analysis) is a fundamenal dimension reduction technique  
	* First step is "decorrelation", which does not change the information contained in the data  
    * Second step is "dimension reduction", which removes some information  
* In the initial form of decorrelation, PCA makes all samples mean 0 on every dimension, and rotates the axes so the axes are best aligned with the data  
* PCA is implemented in scikit-learn with both a fit component and a transform component - much like KMeans or StandardScaler  
	* from sklearn.decomposition import PCA  
    * model = PCA()  
    * model.fit(samples)  
    * transformed = model.transform(samples)  
* The output of the PCA implementation will have one row per observation and one column per PCA feature  
	* The columns of the new PCA-transformed matrix will have zero correlation - the data have been de-correlated  
* The "principal components" are the directions of maximum variance in the data; these are the axes chosen  
	* Available in the .components_ attribute  
  
Intrinsic dimension - "number of features required to approximate the dataset":  
  
* Example of lat/lon for a flight travelling northeast - while both lat/lon are provided, the plane is really tracking a 1-dimensional path  
* Suppose that you are looking at 3 features of the versicolor data - the 3D plot will look a lot like a plane, so the intrinsic dimension is 2D  
* With PCA, the intrinsic dimension can be calculated by identifying the number of PCA features with significant variance  
	* pca = PCA() ; pca.fit(samples)  
    * features = range(pca.n_components_)  
    * plt.bar(features, pca.explained_variance_)  
  
Dimension reduction with PCA - representing the same data with fewer features (vital to machine-learning pipelines):  
  
* General idea is to retain the high-variance features (post-PCA) and to discard the others as noisy  
	* Can specify with PCA(n_components=2)  
    * While the assumption that low variance features are unimportant does not always hold, it is very frequently a good assumption  
* Sparse arrays (often represented using a csr_matrix) are an example where discarding low-variance features could be a mistake  
	* The PCA implementation in scikit-learn does NOT support csr_matrix  
    * Instead, need to use Truncated SVD() which interacts with the user and/or Pipeline() in a very similar fashion  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Perform the necessary imports
from scipy.stats import pearsonr
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans


rawGrain = pd.read_csv(myPath + "grains.csv", header=None)
samples = rawGrain.iloc[:, 0:7].values
varieties = rawGrain.iloc[:, 7].values


# Assign the 0th column of grains: width
width = samples[:, 0]

# Assign the 1st column of grains: length
length = samples[:, 1]

# Scatter plot width vs length
plt.scatter(width, length)
plt.axis('equal')
# plt.show()
plt.savefig("_dummyPy202.png", bbox_inches="tight")
plt.clf()


# Calculate the Pearson correlation
correlation, pvalue = pearsonr(width, length)

# Display the correlation
print(correlation)



# Create PCA instance: model
model = PCA()

# Apply the fit_transform method of model to grains: pca_features
pca_features = model.fit_transform(samples)

# Assign 0th column of pca_features: xs
xs = pca_features[:,0]

# Assign 1st column of pca_features: ys
ys = pca_features[:,1]

# Scatter plot xs vs ys
plt.scatter(xs, ys)
plt.axis('equal')
# plt.show()
plt.savefig("_dummyPy203.png", bbox_inches="tight")
plt.clf()


# Calculate the Pearson correlation of xs and ys
correlation, pvalue = pearsonr(xs, ys)

# Display the correlation
print(correlation)


# Make a scatter plot of the untransformed points
plt.scatter(samples[:,0], samples[:,1])

# Create a PCA instance: model
model = PCA()

# Fit model to points
model.fit(samples)

# Get the mean of the grain samples: mean
mean = model.mean_

# Get the first principal component: first_pc
first_pc = model.components_[0, :]

# Plot first_pc as an arrow, starting at mean
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)

# Keep axes on same scale
plt.axis('equal')
# plt.show()
plt.savefig("_dummyPy204.png", bbox_inches="tight")
plt.clf()


# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
# plt.show()
plt.savefig("_dummyPy205.png", bbox_inches="tight")
plt.clf()


rawFish = pd.read_csv(myPath + "fish.csv")
samples = rawFish.values[:, 1:].astype(float)

scaled_samples = StandardScaler().fit_transform(samples)

# Create a PCA model with 2 components: pca
pca = PCA(n_components=2)

# Fit the PCA instance to the scaled samples
pca.fit(scaled_samples)

# Transform the scaled samples: pca_features
pca_features = pca.transform(scaled_samples)

# Print the shape of pca_features
print(pca_features.shape)


documents = ['cats say meow', 'dogs say woof', 'dogs chase cats']


# Create a TfidfVectorizer: tfidf
tfidf = TfidfVectorizer() 

# Apply fit_transform to document: csr_mat
csr_mat = tfidf.fit_transform(documents)

# Print result of toarray() method
print(csr_mat.toarray())

# Get the words: words
words = tfidf.get_feature_names()

# Print words
print(words)


rawWiki = pd.read_csv(myPath + "wikipedia-vectors.csv", index_col=0, encoding="utf-8")
rawTitles = rawWiki.columns
articles = rawWiki.values.transpose()

# Throws an error otherwise on Ibrahimovic, which cannot recode from utf-8 to cp1252 in script
titles = [x.encode("cp1252", errors="replace").decode("cp1252") for x in rawTitles]


# Create a TruncatedSVD instance: svd
svd = TruncatedSVD(n_components=50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(svd, kmeans)


# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)


# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({'label': labels, 'article': titles})

# Display df sorted by cluster label
import sys
print(sys.stdout.encoding)
print(df.sort_values("label"))

```
  
  
**Example #1: Width vs. Length (Grains)**:  
![](_dummyPy202.png)  
  
**Example #2: First Two Principal Components (Grains)**:  
![](_dummyPy203.png)

**Example #3: Length vs. Width and Principal Components as Arrows (Grains)**:  
![](_dummyPy204.png)

**Example #4: Incremental Variance Explained by Component (Grains)**:  
![](_dummyPy205.png)
  
***
  
Chapter 4 - Discovering intepretable features  
  
Non-negative matrix factorization (NMF):  
  
* NMF is another form of dimension reduction technique, but it is an "interpretable" model - easy to explain as a result  
	* Caveat that all elements of the martix must be non-negative (though 0 is fine)  
    * NMF expresses objects as weighted sums of components  
* NMF is available in scikit-learn, using the familiar fit() / transform() methodology  
	* Must always specify the number of components, such as NMF(n_components=2)  
    * Can work with NumPy arrays or with csr_matrix objects  
    * Can be applied to something created through TFIDF - "tf" is work frequency while "idf" is down-weighting for frequent words  
* Running NMF in scikit-learn example  
	* from sklearn.decomposition import NMF  
    * model = NMF(n_components=2)  
    * model.fit(samples)  
    * nmf_features = model.transform(samples)  
    * print(model.components_)  # will have the same dimension / shape as samples  
    * print(nmf.features) # will have 2 columns, each reflecting one of the two requested by the n_components=2 argument  
* Sample reconstruction gets very close to the original values - multiple components_ by features and sum  
	* Can be run using matrix maths, though not in this course  
  
NMF learns interpretable parts - example of scientific articles with word frequencies (20,000 articles x 800 words):  
  
* Suppose that you fit an NMF with 10 components to this 20,000 x 800 raw data  
	* Can then look at the sorted list of top words by components, and draw themes from them  
    * More or less, the components applied to a word-frequency matrix will be the "topics" (groups of similar words)  
    * For images, the NMF will separate images in to cells of commonly occurring pixels  
* Grayscale images can be encoded solely by the brightness of a pixel - so, a non-negative array  
	* Can represent with 0 as black, 1 as white, and fractions in between as various shades of gray  
    * As a pre-processing step, it is often necessary to "flatten" images - make a single 1D array (vector)  
    * Each row is then an image, while each column is a pixel position of an image  
    * To "unflatten" an image, us .reshape((desiredRows, desiredColumns)), where the tuple is passed to the .reshape()  
    * Can then run plt.imshow(myReshape, cmap="grey", interpolation="nearest")  
  
Building recommender systems using NMF:  
  
* Example of recommending articles similar to the article currently being read  
	* General strategy is to apply NMF to the word-frequency array, and find articles with similar topics  
* Suppose that you have a word-frequency array called "articles"  
	* from sklearn.decomposition import NMF  
    * nmf = NMF(n_components=6)  
    * nmf_features = nmf.fit_transform(articles)  
* Strategy for comparing outputs of NMF for similar topics - "cosine similarity"  
	* Key is the general proportions, since otherwise a direct article may score higher than a wordy (weak) article which is diluted by filler  
    * The "cosine similarity" measures the angle between two lines, with higher values meaning greater similarity  
* Calculating cosine similarities on the existing features dataset  
	* from sklearn.preprocessing import normalize  
    * norm_features = normalize(nmf_features)  
    * current_article = norm_features[23, :]  # assuming current article is in index 23 of norm_features  
    * similarities = norm_features.dot(current_article)  
* Alternately, can run this using pandas DataFrame  
	* df = pd.DataFrame(norm_features, index=titles)  
    * current_article = df.loc["Dog bites man"]  # assuming that is the title of the current article  
    * similarities = df.dot(current_article)  
    * similarities.nlargest()  # will print the 5 largest similarities by default; will include self as 1.00000  
  
Final thoughts:  
  
* scikit-learn and SciPy for unsupervised learning  
* Clustering and dimension reduction techniques  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# Import modules
from sklearn.decomposition import NMF
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from sklearn.preprocessing import Normalizer, MaxAbsScaler
from sklearn.pipeline import make_pipeline



rawWiki = pd.read_csv(myPath + "wikipedia-vectors.csv", index_col=0, encoding="utf-8")
rawTitles = rawWiki.columns
articles = rawWiki.values.transpose()
titles = [x.encode("cp1252", errors="replace").decode("cp1252") for x in rawTitles]


# Create an NMF instance: model
model = NMF(n_components=6)

# Fit the model to articles
model.fit(articles)

# Transform the articles: nmf_features
nmf_features = model.transform(articles)

# Print the NMF features
print(nmf_features)


# Create a pandas DataFrame: df
df = pd.DataFrame(nmf_features, index=titles)

# Print the row for 'Anne Hathaway'
print(df.loc["Anne Hathaway", :])

# Print the row for 'Denzel Washington'
print(df.loc["Denzel Washington", :])


words = []

for word in open (myPath + "wikipedia-vocabulary-utf8.txt", "r"):
    words.append(word.strip())


# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=words)

# Print the shape of the DataFrame
print(components_df.shape)

# Select row 3: component
component = components_df.iloc[3, :]

# Print result of nlargest
print(component.nlargest())


rawDigits = pd.read_csv(myPath + "lcd-digits.csv", header=None)
samples = rawDigits.values


# Select the 0th row: digit
digit = samples[0, :]

# Print digit
print(digit)

# Reshape digit to a 13x8 array: bitmap
bitmap = digit.reshape(13, 8)

# Print bitmap
print(bitmap)

# Use plt.imshow to display bitmap
plt.imshow(bitmap, cmap='gray', interpolation='nearest')
plt.colorbar()
# plt.show()
plt.savefig("_dummyPy206.png", bbox_inches="tight")
plt.clf()


def show_as_image(sample, useSub):
    bitmap = sample.reshape((13, 8))
    # plt.figure()
    plt.subplot(4, 2, useSub)
    plt.imshow(bitmap, cmap='gray', interpolation='nearest')
    


# Create an NMF model: model
model = NMF(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
curSub = 1
for component in model.components_:
    show_as_image(component, useSub=curSub)
    curSub += 1

# plt.colorbar()
plt.tight_layout()
plt.savefig("_dummyPy207.png", bbox_inches="tight")
plt.clf()


# Assign the 0th row of features: digit_features
digit_features = features[0, :]

# Print digit_features
print(digit_features)


# Create a PCA instance: model
model = PCA(n_components=7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
curSub = 1
for component in model.components_:
    show_as_image(component, useSub=curSub)
    curSub += 1


# plt.colorbar()
plt.tight_layout()
plt.savefig("_dummyPy208.png", bbox_inches="tight")
plt.clf()

# Normalize the NMF features: norm_features
norm_features = normalize(nmf_features)

# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=titles)

# Select the row corresponding to 'Cristiano Ronaldo': article
article = df.loc['Cristiano Ronaldo']

# Compute the dot products: similarities
similarities = df.dot(article)

# Display those with the largest cosine similarity
print(similarities.nlargest())



# Create a MaxAbsScaler: scaler
scaler = MaxAbsScaler()

# Create an NMF model: nmf
nmf = NMF(n_components=20)

# Create a Normalizer: normalizer
normalizer = Normalizer()

# Create a pipeline: pipeline
pipeline = make_pipeline(scaler, nmf, normalizer)



artist_names = [x for x in pd.read_csv(myPath + "artists.csv", header=None).iloc[:, 0]]
rawSparse = pd.read_csv(myPath + "scrobbler-small-sample.csv")

artists = np.zeros(shape=(111, 500))
for ctr in range(rawSparse.shape[0]):
    artists[rawSparse["artist_offset"][ctr], rawSparse["user_offset"][ctr]] = rawSparse["playcount"][ctr]



# Apply fit_transform to artists: norm_features
norm_features = pipeline.fit_transform(artists)


# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=artist_names)

# Select row of 'Bruce Springsteen': artist
artist = df.loc['Bruce Springsteen']

# Compute cosine similarities: similarities
similarities = df.dot(artist)

# Display those with highest cosine similarity
print(similarities.nlargest())


```
  
**Example #1: Example LCD Digit**:  
![](_dummyPy206.png)  
  
**Example #2: NMF Components for LCD Digits**:  
![](_dummyPy207.png)  

**Example #3: PCA Components for LCD Digits**:  
![](_dummyPy208.png)  
  
###_Deep Learning in Python_#

Chapter 1 - Basics of Deep Learning and Neural Networks  
  
Introduction to deep learning - contrast to the "sum of parts" approach as with linear regression:  
  
* Distinction between model with no interactions (straight-sum) and model with interactions (slopes and intercepts may change with covariates)  
* Neural networks are a model that account for interactions very well  
* Deep learning uses especially powerful neural networks to decipher areas like text, images, audio, video, source code, and the like  
* First two chapters of this course focus on conceptual knowledge about deep learning  
	* Third and fourth chapters are more focused on examples and using the models  
* General approach to neural networks includes the layers  
	* Input layer - features  
    * Output layer - prediction (continuous or categorical)  
    * Hidden layer - all layers between the inputs and the outputs (neither observed from the world nor the predicted output)  
    * More nodes in the hidden layers mean more capturing of interactions  
  
Forward propagation - bank transactions example where # transactions ~ # children, # existing accounts:  
  
* Forward propagation takes a customer and passes them as an input layer to the neural network, taking the prediction from the output layer  
	* Basically, each hidden node is some weighted combination of the nodes (which may be the input layer) in the previous layer  
    * Always a multiply-add process, implementable as the dot-product from linear algebra  
* Example process in numpy, assuming the input layer is available as an array  
	* input_data = np.array([2, 3])  
    * weights = { "node0": np.array([1, 1]), "node1": np.array([-1, 1]), "output": np.array([2, -1]) }  
    * node_0_value = (input_data * weights["node_0"]).sum()  
    * node_1_value = (input_data * weights["node_1"]).sum()  
    * hidden_layer_values = np.array([node_0_value, node_1_value])  
    * output = (hidden_layer_values * weight["output"]).sum()  
  
Activation functions - the core of the neural network hidden layers, allowing it to capture non-linearity:  
  
* An activation function is applied to the node inputs to produce the node outputs  
* For a long time, tanh() was a popular activation function, so if the weights summed to 5, then the node would produce tanh(5)  
* Current standard is ReLU or Rectified Linear Activation  
	* Two linear components, but it is surprisingly successful at making predictions  
    * Basically ReLU is 0 for x < 0 and x for x >= 0  
* With an activation function, a node can be considered to have three components  
	* The raw inputs coming to the node  
    * The weighted sum of the nodes (input to the activation function of the node)  
    * The activation function output (what will be passed to the nodes in the next layer)  
  
Deeper networks - many hidden layers are much of what makes modern neural networks so powerful:  
  
* State of the art used to be 10-15 hidden layers, but it is now common to have as much as thousands of hidden layers  
* Over time, deep networks internally build "representations" of the patterns in the data  
	* This (at least partially) replaces the need for feature engineering, since the layers manage that  
    * Subsequent layers build increasingly sophisticated representations of raw data  
* The advantage of deep learning is that the modeler does not need to pre-specify the interactions; they are discovered in the training process  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt




input_data = np.array([3, 5])
weights = { "node_0": np.array([2, 4]), "node_1": np.array([4, -5]), "output": np.array([2, 7]) }

# Calculate node 0 value: node_0_value
node_0_value = (input_data * weights["node_0"]).sum()

# Calculate node 1 value: node_1_value
node_1_value = (input_data * weights["node_1"]).sum()

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_value, node_1_value])

# Calculate output: output
output = (hidden_layer_outputs * weights["output"]).sum()

# Print output
print(output)


def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    output = max(0, input)
    
    # Return the value just calculated
    return(output)

# Calculate node 0 value: node_0_output
node_0_input = (input_data * weights['node_0']).sum()
node_0_output = relu(node_0_input)

# Calculate node 1 value: node_1_output
node_1_input = (input_data * weights['node_1']).sum()
node_1_output = relu(node_1_input)

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_output, node_1_output])

# Calculate model output (do not apply relu)
model_output = (hidden_layer_outputs * weights['output']).sum()

# Print model output
print(model_output)


# Define predict_with_network()
def predict_with_network(input_data_row, weights):
    # Calculate node 0 value
    node_0_input = (input_data_row * weights['node_0']).sum()
    node_0_output = relu(node_0_input)
    
    # Calculate node 1 value
    node_1_input = (input_data_row * weights['node_1']).sum()
    node_1_output = relu(node_1_input)
    
    # Put node values into array: hidden_layer_outputs
    hidden_layer_outputs = np.array([node_0_output, node_1_output])
    
    # Calculate model output
    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()
    model_output = relu(input_to_final_layer)
    
    # Return model output
    return(model_output)

# Create empty list to store prediction results
results = []
for input_data_row in input_data:
    # Append prediction to results
    results.append(predict_with_network(input_data_row, weights))

# Print results
print(results)


def predict_with_network(input_data):
    # Calculate node 0 in the first hidden layer
    node_0_0_input = (input_data * weights["node_0_0"]).sum()
    node_0_0_output = relu(node_0_0_input)
    
    # Calculate node 1 in the first hidden layer
    node_0_1_input = (input_data * weights["node_0_1"]).sum()
    node_0_1_output = relu(node_0_1_input)
    
    # Put node values into array: hidden_0_outputs
    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])
    
    # Calculate node 0 in the second hidden layer
    node_1_0_input = (hidden_0_outputs * weights["node_1_0"]).sum()
    node_1_0_output = relu(node_1_0_input)
    
    # Calculate node 1 in the second hidden layer
    node_1_1_input = (hidden_0_outputs * weights["node_1_1"]).sum()
    node_1_1_output = relu(node_1_1_input)
    
    # Put node values into array: hidden_1_outputs
    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])
    
    # Calculate model output: model_output
    model_output = (hidden_1_outputs * weights["output"]).sum()
    
    # Return model_output
    return(model_output)

weights = { 'node_0_0': np.array([2, 4]), 
            'node_1_1': np.array([2, 2]), 
            'node_0_1': np.array([ 4, -5]), 
            'node_1_0': np.array([-1,  1]), 
            'output': np.array([2, 7])
           }

output = predict_with_network(input_data)
print(output)

```
  
  
***
  
Chapter 2 - Optimizing with Backward Propagation

Need for optimization - goal is to assess how close prediction are to actual values (supervised), and optimize to minimize deltas:  
  
* Loss functions aggregate prediction errors for evey observation in to a single, aggregate number  
* Goal is to find weights that lead to the lowest value for the loss function  
* Gradient descent is a common method for searching the parameter space for this minima  
	* Start at a random point  
    * Find the slope  
    * Take a step downhill  
    * Repeat until all slope is flat/uphill  
  
Gradient descent - continuous "downhill" process to find a (local or global) minima:  
  
* Slope is always multiplied by a small number called the "learning rate" - avoid stepping too far and missing the target - rate is typically on the order of 0.001  
* Calculating the slope for a given weight is a three step process  
	* Slope of the loss function w.r.t. value at the node we feed in to  
    * Value of the node that feeds in to our weight  
    * Slope of the activation function w.r.t. value we feed into  
* Consider a very simple example which just has input node 3, weight 2, activation function identity, output 6 (3*2) and target value 10  
	* Slope of loss function w.r.t value at node we feed in to: 2 * (Predicted - Actual) = 2 * Error = -8  
    * Value of the node that feeds in to our weight: 3  
    * Slope of the activation function w.r.t. value we feed into: 1  
    * So, the overall slope is -8 * 3 * 1 = -24  
    * This slope would then be multiplied by the learning rate, with the weight (2) REDUCED by learning rate * -24 for the next run  
* With multiple points, the array of slopes is called the gradient; thus, gradient descent  
  
Back-propagation - sending the error backwards through the hidden layers so that other gradients can be calculated (weights modified):  
  
* Comes from the chain rule in calculus; important to understand but als tricky; well-implemented by various neural net libraries  
* Process includes back-propagation to estimate the slopes with each weight, updates by the learning rate, then forward-propagation to find the new errors  
	* With new weights and errors, there will be new slopes; repeat as needed  
    * Need to keep track of the slopes of the loss function w.r.t. node values  
  
Back-propagation in practice:  
  
* A node in the hidden layer "takes on" the value of the gradient for that node that was just calculated in the previous step for use in modifying weights coming to that node in the hidden layer  
* Back propagation recap  
	* Start with a random set of weights  
    * Use forward proagation to make a prediction  
    * Use back proagation to calculate the slope of the loss function w.r.t. each weight  
    * Multiply that slope by the learning rate, and subtract from the current weights  
* Stochastic gradient descent - calculating slopes on "batches" rather than on the full data all at once  
	* Common to calculate the slopes on only a subset ("batch") of the data at any particular time  
    * Use a different batch of data to calculate the next update  
    * Start over once all of the data has been used  
    * Each full path through the training data is called the epoch  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    output = max(0, input)
    
    # Return the value just calculated
    return(output)


# Modify predict_with_network() from previous chapters
def predict_with_network(input_data, weights):
    # Calculate node 0 in the first hidden layer
    node_0_input = (input_data * weights["node_0"]).sum()
    node_0_output = relu(node_0_input)
    
    # Calculate node 1 in the first hidden layer
    node_1_input = (input_data * weights["node_1"]).sum()
    node_1_output = relu(node_1_input)
    
    # Put node values into array: hidden_0_outputs
    hidden_0_outputs = np.array([node_0_output, node_1_output])

    # Calculate model output: model_output
    model_output = (hidden_0_outputs * weights["output"]).sum()
    
    # Return model_output
    return(model_output)



# The data point you will make a prediction for
input_data = np.array([0, 3])

# Sample weights
weights_0 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 1]
            }

# The actual target value, used to calculate the error
target_actual = 3

# Make prediction using original weights
model_output_0 = predict_with_network(input_data, weights_0)

# Calculate error: error_0
error_0 = model_output_0 - target_actual

# Create weights that cause the network to make perfect prediction (3): weights_1
weights_1 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 0]
            }

# Make prediction using new weights: model_output_1
model_output_1 = predict_with_network(input_data, weights_1)

# Calculate error: error_1
error_1 = model_output_1 - target_actual

# Print error_0 and error_1
print(error_0)
print(error_1)


from sklearn.metrics import mean_squared_error

# Create model_output_0 
model_output_0 = []
# Create model_output_0
model_output_1 = []


input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]

# Loop over input_data
for row in input_data:
    # Append prediction to model_output_0
    model_output_0.append(predict_with_network(row, weights_0))
    
    # Append prediction to model_output_1
    model_output_1.append(predict_with_network(row, weights_1))


target_actuals = [1, 3, 5, 7]

# Calculate the mean squared error for model_output_0: mse_0
mse_0 = mean_squared_error(target_actuals, model_output_0)

# Calculate the mean squared error for model_output_1: mse_1
mse_1 = mean_squared_error(target_actuals, model_output_1)

# Print mse_0 and mse_1
print("Mean squared error with weights_0: %f" %mse_0)
print("Mean squared error with weights_1: %f" %mse_1)


weights = np.array([0, 2, 1])
input_data = np.array([1, 2, 3])
target = 0

# Calculate the predictions: preds
preds = (weights * input_data).sum()

# Calculate the error: error
error = preds - target

# Calculate the slope: slope
slope = 2 * input_data * error

# Print the slope
print(slope)


# Set the learning rate: learning_rate
learning_rate = 0.01

# Calculate the predictions: preds
preds = (weights * input_data).sum()

# Calculate the error: error
error = preds - target

# Calculate the slope: slope
slope = 2 * input_data * error

# Update the weights: weights_updated
weights_updated = weights - learning_rate * slope

# Get updated predictions: preds_updated
preds_updated = (weights_updated * input_data).sum()

# Calculate updated error: error_updated
error_updated = preds_updated - target

# Print the original error
print(error)

# Print the updated error
print(error_updated)


# Need functions get_slope() and get_mse()
def get_slope(myData, myTarget, myWeight):
    preds = (myWeight * myData).sum()
    error = preds - myTarget
    slope = 2 * myData * error
    return(slope)

def get_mse(myData, myTarget, myWeight):
    preds = (myWeight * myData).sum()
    mse = (preds - myTarget) ** 2
    return(mse)

weights = np.array([0, 2, 1])
input_data = np.array([1, 2, 3])
target = 0

n_updates = 20
mse_hist = []

# Iterate over the number of updates
for i in range(n_updates):
    # Calculate the slope: slope
    slope = get_slope(input_data, target, weights)
    
    # Update the weights: weights
    weights = weights - 0.01 * slope
    
    # Calculate mse with new weights: mse
    mse = get_mse(input_data, target, weights)
    
    # Append the mse to mse_hist
    mse_hist.append(mse)

# Plot the mse history
plt.plot(mse_hist)
plt.xlabel('Iterations')
plt.ylabel('Mean Squared Error')
# plt.show()
plt.savefig("_dummyPy209.png", bbox_inches="tight")
plt.clf()


```
  
**Example #1: Gradient Descent MSE by Iteration**:  
![](_dummyPy209.png)  
  
***
  
Chapter 3 - Building Deep-Learning Models  
  
Creating a keras model - basic four-step process:  
  
* Four main steps in building a deep-learning model  
	* Specify Architecture - number of layers, number of nodes per layer, activation function, etc.  
    * Compile - specify the loss function and some details about optimization  
    * Fit - back-propagation and creation of model weights  
    * Predict - use the model to make predictions  
* Specify Architecture using keras (suppose that a Numpy array "predictors" already exists)  
	* from keras.layers import Dense ; from keras.models import Sequential  
    * n_cols = predictiors.shape[1]  
    * model = Sequential()  # one of two ways to build a model (generally the easier of the two) - Sequential means nodes are only connected to the next layer  
    * model.add(Dense(100, activation="relu", input_shape = (n_cols, )))  # Dense means all nodes in the previous layer connect to all nodes in the next layer (input_shape must be specified for the first layer)  
    * model.add(Dense(100, activation="relu"))  
    * model.add(Dense(1))  # requests a single output node  
  
Compiling and fitting a model - set up for optimization:  
  
* Specify the optimizer - "Adam" is typically an excellent first choice  
	* The optimizer controls the learning rate, which can play a big role in both convergence time and result quality  
    * There are many options that are extremely mathematically complex; best choice is often an optimizer that is flexible and forgiving  
    * "Adam" is usually a good choice - adjusts the learning rate as it performs gradient descent  
* Specify the loss functions - while MSE is common for regression, it is less not the only option for deep learning  
* Code for compiling a model looks like  
	* model.compile(optimizer="adam", loss="mean_squared_error")  
* Fitting the model involves running the back-propagation and gradient descent to update the weights  
	* Frequently a good practice to pre-process the data so that each feature is roughly the same magnitude, such as normalizing (technically not required, just helpful)  
    * model.fit(predictors, target)  
  
Classification models - additional specifications:  
  
* The loss function is frequently updated to "categorical_crossentropy" (somewhat similar to log-loss; lower scores are better)  
	* Can add the call metrics=["accuracy"] to get a print-out of the accuracy after each stage, which is more interpretable  
* The output layer needs to be updated to include a node for every layer, and the activation function is "softmax" (which ensures that all probabilities add to 1)  
* Generally, keras works best when the output categorical variables are aplit in to one column per category (so if there is A, B or C, then have three 1/0 columns reflecting which of these it is)  
	* This is true even for make/miss data which might be encoded as 0/1 already - goal is that the target/output columns always sum to 1  
* General process, assuming a Numpy array predictors and a DataFrame "data" with column "shot_result" already exist, and that n_cols for predictors has been calculated  
	* from keras.utils import to_categorical  
    * target = to_categorical(data.shot_result)   # FYI, converting a DataFrame, minus a column, to NumPy is  newNP = oldPD.drop("badCol", axis=1).as_matrix()  
    * model = Sequential()  
    * model.add(Dense(100, activation="relu", input_shape=(n_cols, )))  
    * model.add(Dense(100, activation="relu"))  
    * model.add(Dense(100, activation="relu"))  
    * model.add(Dense(2, activation="softmax"))  
    * model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])  
    * model.fit(predictors, target)  
  
Using models - process includes Save, Reload, and Predict:  
  
* The general code for the process includes  
	* from keras.models import load_model  
    * model.save("model_file.h5")  
    * my_model = load_model("model_file.h5")  
    * predictions = my_model.predict(predictData)  
    * probability_true = predictions[:, 1]  
* Can verify model architecure using my_model.summary()  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# Import necessary modules
# Not currently available?
# import keras
# from keras.layers import Dense
# from keras.models import Sequential
# from keras.utils import to_categorical



rawWage = pd.read_csv(myPath + "hourly_wages.csv")
target = rawWage["wage_per_hour"].values
predictors = rawWage.drop("wage_per_hour", axis=1).values

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]

# Set up the model: model
# model = Sequential()

# Add the first layer
# model.add(Dense(50, activation="relu", input_shape=(n_cols, )))

# Add the second layer
# model.add(Dense(32, activation="relu"))

# Add the output layer
# model.add(Dense(1))


# Compile the model
# model.compile(optimizer="adam", loss="mean_squared_error")

# Verify that model contains information from compiling
# print("Loss function: " + model.loss)


# Fit the model
# model.fit(predictors, target)


# Using the titanic data
rawTitanic = pd.read_csv(myPath + "titanic_all_numeric.csv")

# Convert the target to categorical: target
target = rawTitanic["survived"].astype("category")
predictors = rawTitanic.drop("survived", axis=1).values
n_cols = predictors.shape[1]

# Set up the model
# model = Sequential()

# Add the first layer
# model.add(Dense(32, activation="relu", input_shape=(n_cols, )))

# Add the output layer
# model.add(Dense(2, activation="softmax"))

# Compile the model
# model.compile(optimizer="sgd", loss="categorical_crossentropy", metrics=['accuracy'])

# Fit the model
# model.fit(predictors, target)


# Calculate predictions: predictions
# predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
# predicted_prob_true = predictions[:, 1]

# print predicted_prob_true
# print(predicted_prob_true)

```
  
  
  
***
  
Chapter 4 - Fine-tuning keras Models  
  
Understanding model optimization - a difficult process since there are 1000s of weights, all interacting with each other:  
  
* If the learning rate is too small, the next epoch may be almost no better than the current epoch  
* If the learning rate is too large, the nex epoch may even be much worse  
* One common approach for testing learning rates is to use stochastic gradident descent ("sgd") with multiple values for lr  
	* my_optimizer = SGD(lr=myRate)  
    * model.compile(optimizer = my_optimizer, loss="categorical_crossentropy")  
    * model.fit(predictors, target)  
* There is also a "dying neuron" problem with the "relu" activation function - if a node only gets negative inputs, it may continue only getting negative inputs  
	* The use of tanh() instead produces the "vanishing gradient" problem, where data outside the big slope areas multiplies together to "almost zero" in all of back-propagation  
  
Model validation - extension of the hold-out process specifically for deep learning:  
  
* Typically use a validation split for deep learning rather than cross-validation  
	* Since the datasets tend to be very large, k-fold cross-validation is very computationally expensive  
    * But, since the data are very large, a single validation score based on the large data tends to be more reliable  
    * Can be implemented in keras as model.fit(predictors, target, validation_split=0.3)  
* Basically, the goal is to continue training the model until the validation accuracy is no longer improving (Early Stopping)  
	* from keras.callbacks import EarlyStopping  
    * early_stopping_monitor = EarlyStopping(patience=2)  # typically see 2 or 3 - how many models with no improvement will generate an immediate stop?  
    * model.fit(predictors, target, validation_split=0.3, epochs=20, callbacks=[early_stopping_monitor])  
  
Thinking about model capacity - what architecures will work best for a specific problem and dataset:  
  
* Model capacity is closely related to the terms over-fitting and under-fitting  
* As layers or nodes in a layer are increased, model capacity gets higher and there is a greater likelihood of the variance problem  
* A general workflow for managing the capacity of a model includes  
	* Start with a small network  
    * Get the validation score  
    * Keep increasing capacity until the validation score is no longer improving  
  
Stepping up to images - handwritten images dataset from MNIST:  
  
* Each image is 28x28 (can be flattened to 784 values) of greyscale data  
* Goal of modelling is to predict the digit based on the flattened 784 values  
  
Final thoughts - like riding a bicycle, the hardest part is getting to where you can practice on your own:  
  
* Start with standard prediction problems on tables of numbers  
* Images (with convolutional neural networks) are common next steps  
* Kaggle is a great source for datasets and discussion forums  
* Wikipedia has a page on datasets for machine learning  
* keras.io is also an excellent source of information  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# need to find or create function get_new_model()
# Import the SGD optimizer
# from keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [0.000001, 0.01, 1]

# Loop over learning rates
for lr in lr_to_test:
    print('\n\nTesting model with learning rate: %f\n'%lr )
    
    # Build new model to test, unaffected by previous models
    # model = get_new_model()
    
    # Create SGD optimizer with specified learning rate: my_optimizer
    # my_optimizer = SGD(lr=lr)
    
    # Compile the model
    # model.compile(optimizer=my_optimizer, loss="categorical_crossentropy")
    
    # Fit the model
    # model.fit(predictors, target)


# May need to locate this data . . . 
# Save the number of columns in predictors: n_cols
rawMNIST = pd.read_csv(myPath + "mnist.csv", header=None)
target = rawMNIST.iloc[:, 0]
predictors = rawMNIST.iloc[:, 1:]

n_cols = predictors.shape[1]
input_shape = (n_cols,)

# Specify the model
# model = Sequential()
# model.add(Dense(100, activation='relu', input_shape = input_shape))
# model.add(Dense(100, activation='relu'))
# model.add(Dense(2, activation='softmax'))

# Compile the model
# model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=['accuracy'])

# Fit the model
# hist = model.fit(predictors, target, validation_split=0.3)


# Import EarlyStopping
# from keras.callbacks import EarlyStopping

# Define early_stopping_monitor
# early_stopping_monitor = EarlyStopping(patience=2)

# Fit the model
# model.fit(predictors, target, epochs=30, validation_split=0.3, callbacks=[early_stopping_monitor])


# Define early_stopping_monitor
# early_stopping_monitor = EarlyStopping(patience=2)

# Create the new model: model_2
# model_2 = Sequential()

# Add the first and second layers
# model_2.add(Dense(100, activation="relu", input_shape=input_shape))
# model_2.add(Dense(100, activation="relu"))

# Add the output layer
# model_2.add(Dense(2, activation="softmax"))

# Compile model_2
# model_2.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Fit model_1
# model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)

# Fit model_2
# model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)

# Create the plot
# plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')
# plt.xlabel('Epochs')
# plt.ylabel('Validation score')
# plt.show()


# The input shape to use in the first hidden layer
input_shape = (n_cols,)

# Create the new model: model_2
# model_2 = Sequential()

# Add the first, second, and third hidden layers
# model_2.add(Dense(50, activation="relu", input_shape=input_shape))
# model_2.add(Dense(50, activation="relu"))
# model_2.add(Dense(50, activation="relu"))

# Add the output layer
# model_2.add(Dense(2, activation="softmax"))

# Compile model_2
# model_2.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Fit model 1
# model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)

# Fit model 2
# model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)

# Create the plot
# plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')
# plt.xlabel('Epochs')
# plt.ylabel('Validation score')
# plt.show()


# See post from Dan re using AWS for https://www.datacamp.com/community/tutorials/deep-learning-jupyter-aws#gs.peq1yyU
# Using the MNIST data (2500 samples)
# Create the model: model
# model = Sequential()

# Add the first hidden layer
# model.add(Dense(50, activation="relu", input_shape=(784, )))

# Add the second hidden layer
# model.add(Dense(50, activation="relu"))

# Add the output layer
# model.add(Dense(10, activation="softmax"))

# Compile the model
# model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Fit the model
# model.fit(X, y, validation_split=0.3)

```
  
  
  
###_Machine Learning: School Budgets_#

Chapter 1 - Exploring Raw Data  
  
Introducing the challenge - from DrivenData, an organization that seeks to leverage Data Science for "social impact":  
  
* The DrivenData challenge in this course incorporates NLP, feature engineering, and hashing tricks for efficiency  
* School budgets are complex and non-standardized, and there is benefit to benchmarking one school's spend against another  
	* The objective is to build a machine-learning algorithm that can automate this process  
    * For example, convert "Algebra books for 8th grade students" to "Textbooks" / "Math" / "Middle School"  
    * Supervised learning problem - have samples of data where the conversions have already occurred  
* Classification problem with 100+ target variables  
	* May be related to Pre-K status (Yes, No, Unknown) or Student Type (Alternative, At-Risk, etc.)  
* Humans disagree on many of the classifications, so the goal is for the algorithm to output percentage likelihoods and not hard/fast classifications  
	* Also known as a "human in the loop" machine learning algorithm - percentages help humans optimize where to focus their efforts  
  
Exploring the data - one goal is to have a "target" column for every possible value of every categorical output:  
  
* Load and preview a small sample of the data - exploration  
	* The data for this exercise will import a CSV file  
    * Exploration through .head(), .info(), .describe() and the like  
  
Looking at data types - encoding labels as categories:  
  
* Two benefits to encoding labels as categories  
	* Machine learning algorithms work on numbers and not strings, so a numeric representation is needed anyway  
    * Strings tend to be slow relative to numbers, since string length is indeterminate  
* Pandas has a special dtype "category" which encodes categorical data numerically  
	* sample_df["label"] = sample_df["label"].astype("category")  # will convert from string to category  
* Can access the dummy variables associated with a categorical column  
	* dummies = pd.get_dummies(sample[["label"]], prefix_sep="_")  
    * dummies.head()  # will have created columns label_a, label_b, etc. (assuming the original object data was strings "a", "b", etc.)  
    * The "label" in label_a is due to the original column name being "labe", while the "a" is due to the original value being "a" and the "_" is as per prefix_sep="_"  
* Lambda functions allow for creating very quick and small functions without resorting to def:  
	* square = lambda x: x*x  
    * categorize_label = lambda x: x.astype("category")  
    * sample_df["label"] = sample_df[["label"]].apply(categorize_label, axis=0)  # will convert from string to category  
  
Measuring success - one of the most importan decisions in algorithm design:  
  
* Accuracy can often be misleading, especially when categories are highly imbalanced  
* An improvement can be to use a log-loss function, where larger numbers are worse and respect more (or more confident) errors  
* Log-loss considers the actual value (1 or 0) and predicted probability, p, that the number is actually 1  
	* Penalty for a given prediction is -log(p) if the actual number is 1, and -log(1-p) if the actual number is 0; perfect predictions get 0 errors and perfectly wrong predictions get infinte errors  
    * The aggregate log loss is the average of the log-loss for each of the individual data points  
    * Better to be less confident (p ~ 0.50) than to be confident and wrong (p ~ 0.90 for something that is actually 0)  
* The log-loss can be implemented in numpy, taking advantage of the .clip() function  
	* eps = 1e-14  
    * predicted = np.clip(predicted, eps, 1-eps)  # ensures that all predictions are in the range of 0-1, and forced to be at least eps away from perfect certainty  
    * loss = -1 * bp.mean( actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted) )  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe"), eval=FALSE}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


colNames = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type',
            'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status',
            'Object_Description', 'Text_2', 'SubFund_Description',
            'Job_Title_Description', 'Text_3', 'Text_4', 'Sub_Object_Description',
            'Location_Description', 'FTE', 'Function_Description',
            'Facility_or_Department', 'Position_Extra', 'Total',
            'Program_Description', 'Fund_Description', 'Text_1']




# Need to get this CSV!
df = pd.read_csv("TrainingData.csv", index_col=0)


# Print the summary statistics
print(df.describe())


# Create the histogram
plt.hist(df["FTE"].dropna())

# Add title and labels
plt.title('Distribution of %full-time \n employee works')
plt.xlabel('% of full-time')
plt.ylabel('num employees')

# Display the histogram
plt.show()


LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']

# Define the lambda function: categorize_label
categorize_label = lambda x: x.astype("category")

# Convert df[LABELS] to a categorical type
df[LABELS] = df[LABELS].apply(categorize_label, axis=0)

# Print the converted dtypes
print(df[LABELS].dtypes)


# Calculate number of unique values for each label: num_unique_labels
num_unique_labels = df[LABELS].apply(pd.Series.nunique)

# Plot number of unique values for each label
num_unique_labels.plot(kind="bar")

# Label the axes
plt.xlabel('Labels')
plt.ylabel('Number of unique values')

# Display the plot
plt.show()


actual_labels = np.array([ 1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.])
correct_confident = np.array([ 0.95,  0.95,  0.95,  0.95,  0.95,  0.05,  0.05,  0.05,  0.05,  0.05])
correct_not_confident = np.array([ 0.65,  0.65,  0.65,  0.65,  0.65,  0.35,  0.35,  0.35,  0.35,  0.35])
wrong_not_confident = np.array([ 0.65,  0.65,  0.65,  0.65,  0.65,  0.35,  0.35,  0.35,  0.35,  0.35])
wrong_confident = np.array([ 0.05,  0.05,  0.05,  0.05,  0.05,  0.95,  0.95,  0.95,  0.95,  0.95])

# need to write function log_loss()
def log_loss(pred, actual, eps=1e-14):
    predicted = np.clip(pred, eps, 1-eps)  # ensures that all predictions are in the range of 0-1, and forced to be at least eps away from perfect certainty 
    loss = -1 * bp.mean( actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted) )
    return loss

# Compute and print log loss for 1st case
correct_confident = compute_log_loss(correct_confident, actual_labels)
print("Log loss, correct and confident: {}".format(correct_confident)) 

# Compute log loss for 2nd case
correct_not_confident = compute_log_loss(correct_not_confident, actual_labels)
print("Log loss, correct and not confident: {}".format(correct_not_confident)) 

# Compute and print log loss for 3rd case
wrong_not_confident = compute_log_loss(wrong_not_confident, actual_labels)
print("Log loss, wrong and not confident: {}".format(wrong_not_confident)) 

# Compute and print log loss for 4th case
wrong_confident = compute_log_loss(wrong_confident, actual_labels)
print("Log loss, wrong and confident: {}".format(wrong_confident)) 

# Compute and print log loss for actual labels
actual_labels = compute_log_loss(actual_labels, actual_labels)
print("Log loss, actual labels: {}".format(actual_labels)) 

```
  
  
***
  
Chapter 2 - Create a Simple First Model  
  
Time to build a model - start with a simple model, understand the challenge, then expand as appropriate:  
  
* Begin by training a model that uses only the numeric data - multi-class logistic regression (treat each label separately and use to predict)  
	* Format predictions to CSV and simulate submitting them to the competition  
* Thoughts on splitting the full dataset in a test-train fashion  
	* Risky if done randomly, as some labels are rare and may be too sparse (or even non-existent) in the training data  
    * One potential solution is StratifiedShuffleSplit, though this works only for a single, categorical, target variable  
    * This course created a utility function, multilabel_test_train_split() to ensure proper stratification given the many target types in this data  
* Example coding process could include  
	* data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)  # fill the NA columns with a fake numeric value  
    * labels_to_use = pd.get_dummies(df[LABELS])  
    * X_train, X_test, y_train, y_test = multilabel_train_test_split(data_to_train, labels_to_use, size=0.2, seed=123)  
    * from sklearn.linear_model import LogisticRegression  
    * from sklearn.multiclass import OneVsRestClassifier  
    * clf = OneVsRestClassifier(LogisticRegression())  
    * clf.fit(X_train, y_train)  
  
Making predictions - such as a hold-out sample for a competition:  
  
* Load the holdout data, pre-process it in the same manner as used earlier, then run the model on it  
	* holdout = pd.read_csv("myCSVHoldout")  
    * predictions = clf.predict_proba(holdout)  # want the probabilities, not just the 'most likely' value as per .predict()  
    * There may be specific requirements as to how the CSV is formatted and uploaded  
* Converting the matrix provided by predict_proba() in to a DataFrame that can be saved using .to_csv()  
	* prediction_df = pd.DataFrame(predictions, columns=pd.get_dummies(df[LABELS], prefix_sep="__").columns, index=holdout.index)  
    * prediction_df.to_csv("predictions.csv")  
    * score = score_submission(pred_path="predictions.csv")  
  
Very brief introduction to NLP (Natural Language Processing) - convert text to features:  
  
* First step is tokenization - splitting a long string in to segments (frequently, a list of strings)  
	* Decision of where to tokenize (e.g., white-space, non-alphanumeric, etc.)  
* "Bag of words" representation - counting the number of times a particular token appears  
	* This approach discards information contained in word order  
    * A more sophisticated approach might create n-grams, where each sequence of n tokens become their own separate token  
    * The 2-gram is also called the bigram and the 3-gram is also called the trigram  
  
Representing text numerically - extending on the "bag of words" approach:  
  
* Scikit-learn offers good tools for bag-of-words - CountVectorizer()  
	1.  Tokenizes all the strings  
    2.  Notes all the words that appear (vocabulary)  
    3.  Counts the occurrence of each token in the vocabulary for each row  
* Usage example for the CountVectorizer() in scikit  
	* from sklearn.feature_extraction.text import CountVectorizer  
    * TOKENS_BASIC = "\\S+(?=\\s+)"  # can be any regular expression; defining the process for defining and extracting by () the tokens  
    * df["Program_Description"].fillna("", inplace=True)  
    * vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)  
    * vec_basic.fit(df["Program_Description"])  
    * len(vec_basic.get_feature_names())  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe"), eval=FALSE}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# see https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/data/multilabel.py for the multilabel_test_train_split()

NUMERIC_COLUMNS = ['FTE', 'Total']
LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']

# Create the new DataFrame: numeric_data_only
numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)

# Get labels and convert to dummy variables: label_dummies
label_dummies = pd.get_dummies(df[LABELS])

# Create training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,
                                                               label_dummies,
                                                               size=0.2, 
                                                               seed=123)

# Print the info
print("X_train info:")
print(X_train.info())
print("\nX_test info:")  
print(X_test.info())
print("\ny_train info:")  
print(y_train.info())
print("\ny_test info:")  
print(y_test.info()) 


# Import classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Create the DataFrame: numeric_data_only
numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)

# Get labels and convert to dummy variables: label_dummies
label_dummies = pd.get_dummies(df[LABELS])

# Create training and test sets
X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,
                                                               label_dummies,
                                                               size=0.2, 
                                                               seed=123)

# Instantiate the classifier: clf
clf = OneVsRestClassifier(LogisticRegression())

# Fit the classifier to the training data
clf.fit(X_train, y_train)

# Print the accuracy
print("Accuracy: {}".format(clf.score(X_test, y_test)))


# Import classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Instantiate the classifier: clf
clf = OneVsRestClassifier(LogisticRegression())

# Fit the classifier to the training data
clf.fit(X_train, y_train)

# Print the accuracy
print("Accuracy: {}".format(clf.score(X_test, y_test)))


# Load the holdout data: holdout
holdout = pd.read_csv("HoldoutData.csv", index_col=0)

# Generate predictions: predictions
predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))


# Generate predictions: predictions
predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))

# Format predictions in DataFrame: prediction_df
prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,
                             index=holdout.index,
                             data=predictions)


# Save prediction_df to csv
prediction_df.to_csv("predictions.csv")

# Submit the predictions for scoring: score
score = score_submission(pred_path="predictions.csv")

# Print score
print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))


# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create the token pattern: TOKENS_ALPHANUMERIC
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Fill missing values in df.Position_Extra
df["Position_Extra"].fillna("", inplace=True)

# Instantiate the CountVectorizer: vec_alphanumeric
vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Fit to the data
vec_alphanumeric.fit(df["Position_Extra"])

# Print the number of tokens and first 15 tokens
msg = "There are {} tokens in Position_Extra if we split on non-alpha numeric"
print(msg.format(len(vec_alphanumeric.get_feature_names())))
print(vec_alphanumeric.get_feature_names()[:15])


# Define combine_text_columns()
def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):
    """ converts all text in each row of data_frame to single vector """
    
    # Drop non-text columns that are in the df
    to_drop = set(to_drop) & set(data_frame.columns.tolist())
    text_data = data_frame.drop(to_drop, axis=1)
    
    # Replace nans with blanks
    text_data.fillna("", inplace=True)
    
    # Join all text items in a row that have a space in between
    return text_data.apply(lambda x: " ".join(x), axis=1)


# Import the CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Create the basic token pattern
TOKENS_BASIC = '\\S+(?=\\s+)'

# Create the alphanumeric token pattern
TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\s+)'

# Instantiate basic CountVectorizer: vec_basic
vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)

# Instantiate alphanumeric CountVectorizer: vec_alphanumeric
vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)

# Create the text vector
text_vector = combine_text_columns(df)

# Fit and transform vec_basic
vec_basic.fit_transform(text_vector)

# Print number of tokens of vec_basic
print("There are {} tokens in the dataset".format(len(vec_basic.get_feature_names())))

# Fit and transform vec_alphanumeric
vec_alphanumeric.fit_transform(text_vector)

# Print number of tokens of vec_alphanumeric
print("There are {} alpha-numeric tokens in the dataset".format(len(vec_alphanumeric.get_feature_names())))


```
  
  
  
***
  
Chapter 3 - Improving Models  
  


	






	
	
	

	
	





