---
title: "Data Camp Python Notes"
author: "davegoblue"
date: "September 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(engine.path=list(python="C:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python36-32\\python.exe"))
```

## Background and Overview  

DataCamp offer interactive courses related to Python Programming.  Since R Markdown documents can run simple Python code chunks (though the data is not accessible to future chunks, a large difference from R Markdown for R), this document attempts to summarize notes from the modules when possible.

Topic areas summarized include:  
  
* Python Programming (Introduction, Intermediate, Toolbox I/II, Network Analysis I/II)  
* Python Import and Clean Data (Import I/II, Clean)  
* Python Data Manipulation (pandas Foundations, Manipulating DF with pandas, Merging DF with pandas, Databases in pandas, Data Types)  
* Python Visualization (Introduction to Python Data Visualization, Interactive Visualization with Bokeh)  
* Python Statistics (Statistical Thinking in Python Parts I/II)  
* Python Machine Learning ()  
  
The complete version as of July 31, 2017 has been archived as DataCamp_PythonNotes_v001.  Archive files for DataCamp_Python_ImportClean_v002 and DataCamp_Python_Programming_v002 have also been created to contain summaries of those areas.  Further, DataCamp_PythonNotes_v002 was created for capturing summaries of additional topics.

The version of DataCamp_PythonNotes_v002 as of September 1, 2017 has been archived.  Archive files for DataCamp_Python_DataManipulation_v003 and DataCamp_Python_Visualization_v003 have also been created to contain summaries of these areas.  Further, DataCamp_PythonNotes_v003 was created for capturing summaries of additional topics.

This document includes:  
  
* Python Statistics (Statistical Thinking in Python Parts I/II)  
* Python Machine Learning ()  
  
  
## Python Statistics  
###_Statistical Thinking in Python (Part I)_#

Chapter 1 - Graphical exploratory data analysis  
  
Introduction to exploratory data analysis - organizing, plotting, and summarizing data (ala Tukey):  
  
* Example of swing voter dataset (by county) of 2008 election  
* Conversion of tabular data (e.g., pandas DataFrame) to visual data  
  
Plotting a histogram - using matplotlib.pyplot.hist (plt.hist):  
  
* plt.hist(<myData>) will create the basic hsitogram  
	* Can accessorize with plt.xlabel("myXLabel") and plt.ylabel("myYLabel")  
    * Can send the bins with a bins=<myBins> call ; can be a list of actual bin cut-points, or a single integer requesting that number of evenly spaced bins  
* Common practice in Python is to assign _ = plt.hist(<myData>) to avoid the screen being printed with the 3 histogram data outputs  
* Can customize with the Seaborn styling (package developed by Waskom)  
    * import seaborn as sns  
    * sns.set()  # makes Seaborn style the default for graphing  
  
Plotting all data (bee swarm plots):  
  
* Binning bias can lead to the same data being interpreted differently based on different binning in different histograms  
* The swarm plot has the individual points plotted, segmented by categorical variables if appropriate, in a fashion like "jitter" where each point is visible  
* Seaborn can manage swarm plots easilt, provided that data are in a well-organized DataFrame (rows as observations, columns as features)  
	* sns.swarmplot(x="xVar", y="yVar", data=myFrame)  
    * plt.show()  
  
Plotting all data (ECDF = empirical CDF) - especially when swarm plot would be too messy (too much data):  
  
* The ECDF has the y-axis as the cumulative percentile (total amount of data that is smaller than the designated x value)  
* Suppose that a DataFrame df_swing already exists, with a column called "dem_share"  
	* x = np.sort(df_swing["dem_share"])  
    * y = np.arange(1, len(x) + 1) / len(x)  
    * _ = plt.plot(x, y, marker=".", linestyle="none")  # will make dot markers and with no lines  
    * plt.margins(0.02)  # keeps the data from running over the side of the plot  
* Generally, ECDF is a good starting point for analysis - keeps all the data, provides a simple summary  
  
Onward toward the whole story - starting with graphical EDA as per Tukey:  
  
* Chapter 2 - Build on graphical EDA with quantitative EDA  
* Chapter 3/4 - Probabilistic distributions to draw meaningful conclusions (Hacker statistics)  
* Part II - continuing to build towards "the full story"  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



# Import plotting modules
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd

rawIris = pd.read_csv(myPath + "iris.csv")
versicolor_petal_length = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Length"]



# Set default Seaborn style
sns.set()

# Plot histogram of versicolor petal lengths
plt.hist(versicolor_petal_length)

# Show histogram
# plt.show()
plt.savefig("_dummyPy149.png", bbox_inches="tight")
plt.clf()


# Plot histogram of versicolor petal lengths
_ = plt.hist(versicolor_petal_length)

# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
# plt.show()
plt.savefig("_dummyPy150.png", bbox_inches="tight")
plt.clf()


# Import numpy
import numpy as np

# Compute number of data points: n_data
n_data = len(versicolor_petal_length)

# Number of bins is the square root of number of data points: n_bins
n_bins = n_data ** 0.5

# Convert number of bins to integer: n_bins
n_bins = int(n_bins)

# Plot the histogram
_ = plt.hist(versicolor_petal_length, bins=n_bins)

# Label axes
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('count')

# Show histogram
# plt.show()
plt.savefig("_dummyPy151.png", bbox_inches="tight")
plt.clf()



df = rawIris[["Species", "Petal.Length"]]
df.columns = ["species", "petal length (cm)"]

# Create bee swarm plot with Seaborn's default settings
_ = sns.swarmplot(x="species", y="petal length (cm)", data=df)

# Label the axes
_ = plt.xlabel('Species')
_ = plt.ylabel("Petal Length (cm)")

# Show the plot
# plt.show()
plt.savefig("_dummyPy152.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y

# Compute ECDF for versicolor data: x_vers, y_vers
x_vers, y_vers = ecdf(versicolor_petal_length)

# Generate plot
_ = plt.plot(x_vers, y_vers, marker=".", linestyle="none")

# Make the margins nice
plt.margins(0.02)

# Label the axes
_ = plt.xlabel("Versicolor Petal Length (cm)")
_ = plt.ylabel("ECDF")

# Display the plot
# plt.show()
plt.savefig("_dummyPy153.png", bbox_inches="tight")
plt.clf()


setosa_petal_length = rawIris.loc[rawIris["Species"] == "setosa", "Petal.Length"]
virginica_petal_length = rawIris.loc[rawIris["Species"] == "virginica", "Petal.Length"]

# Compute ECDFs
x_set, y_set = ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg, y_virg = ecdf(virginica_petal_length)

# Plot all ECDFs on the same plot
_ = plt.plot(x_set, y_set, marker=".", linestyle="none")
_ = plt.plot(x_vers, y_vers, marker=".", linestyle="none")
_ = plt.plot(x_virg, y_virg, marker=".", linestyle="none")

# Make nice margins
plt.margins(0.02)

# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
# plt.show()
plt.savefig("_dummyPy154.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Default Histogram**:  
![](_dummyPy149.png) 
  
**Example #2: Labeled Histogram**:  
![](_dummyPy150.png) 

**Example #3: Custom-Binned Histogram**:  
![](_dummyPy151.png) 

**Example #4: Swarm Plot**:  
![](_dummyPy152.png) 

**Example #5: ECDF (Single Species)**:  
![](_dummyPy153.png) 
  
**Example #6: ECDF (Multiple Species)**:  
![](_dummyPy154.png) 
  
***
  
Chapter 2 - Quantitative exploratory data analysis  
  
Introduction to summary statistics: sample mean and median:  
  
* Means or medians can be added as horizontal lines on the bee swarm plot  
	* np.mean(myData)  
    * np.median(myData)  
  
Percentiles, outliers, and box plots:  
  
* Can grab multiple percentiles using np.percentile(myData, [myPercentileList])  # note that percentiles should be passed, so 25 means 0.25 or 25th percentile  
* Box plots can help to display much of the percentile data simultaneously - center is median, box edges are 0.25/0.75, end of whsikers is 1.5 * IQR (unless data is narrower), with outliers graphed separately  
* _ = sns.boxplot(x="xVar", y="yVar", data=myData)  
  
Variance and standard deviation:  
  
* Variance - average of the squared distance from the mean  
	* np.var(myData)  
* Standard Deviation - square root of the variance  
	* np.sd(myData)  
  
Covariance and Pearson correlation coefficient:  
  
* Initial graphical EDA could come from graphing the data as points, using plt.plot(xVar, yVar, marker=".", linestyle="none")  
* Covariance - how to variables run together (mean of product of differences from x-mean, y-mean)  
* Correlation - adds the advantages of dimensionless and scaled from -1 to 1  
	* Pearson correlation coefficient (rho) - Covariances / [Std(x) * Std(y) ]  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


rawIris = pd.read_csv(myPath + "iris.csv")
versicolor_petal_length = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Length"]
versicolor_petal_width = rawIris.loc[rawIris["Species"] == "versicolor", "Petal.Width"]


# Compute the mean: mean_length_vers
mean_length_vers = np.mean(versicolor_petal_length)

# Print the result with some nice formatting
print('I. versicolor:', mean_length_vers, 'cm')


# Specify array of percentiles: percentiles
percentiles = np.array([2.5, 25, 50, 75, 97.5])

# Compute percentiles: ptiles_vers
ptiles_vers = np.percentile(versicolor_petal_length, percentiles)

# Print the result
print(ptiles_vers)


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y

# Compute ECDF for versicolor data: x_vers, y_vers
x_vers, y_vers = ecdf(versicolor_petal_length)


# Plot the ECDF
_ = plt.plot(x_vers, y_vers, '.')
plt.margins(0.02)
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Overlay percentiles as red diamonds.
_ = plt.plot(ptiles_vers, percentiles/100, marker='D', color='red',
         linestyle="none")

# Show the plot
# plt.show()
plt.savefig("_dummyPy155.png", bbox_inches="tight")
plt.clf()


df = rawIris[["Species", "Petal.Length"]]
df.columns = ["species", "petal length (cm)"]


# Create box plot with Seaborn's default settings
_ = sns.boxplot(x="species", y="petal length (cm)", data=df)

# Label the axes
_ = plt.xlabel("Species")
_ = plt.ylabel("Petal Length (cm)")

# Show the plot
# plt.show()
plt.savefig("_dummyPy156.png", bbox_inches="tight")
plt.clf()


# Array of differences to mean: differences
differences = versicolor_petal_length - np.mean(versicolor_petal_length)

# Square the differences: diff_sq
diff_sq = differences ** 2

# Compute the mean square difference: variance_explicit
variance_explicit = np.mean(diff_sq)

# Compute the variance using NumPy: variance_np
variance_np = np.var(versicolor_petal_length)

# Print the results
print(variance_explicit, variance_np)


# Compute the variance: variance
variance = np.var(versicolor_petal_length)

# Print the square root of the variance
print(variance ** 0.5)

# Print the standard deviation
print(np.std(versicolor_petal_length))


# Make a scatter plot
_ = plt.plot(versicolor_petal_length, versicolor_petal_width, marker=".", linestyle="none")

# Set margins
plt.margins(0.02)

# Label the axes
_ = plt.xlabel("Petal Length (cm)")
_ = plt.ylabel("Petal Width (cm)")

# Show the result
# plt.show()
plt.savefig("_dummyPy157.png", bbox_inches="tight")
plt.clf()


# Compute the covariance matrix: covariance_matrix
covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)

# Print covariance matrix
print(covariance_matrix)

# Extract covariance of length and width of petals: petal_cov
petal_cov = covariance_matrix[0, 1]

# Print the length/width covariance
print(petal_cov)


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]

# Compute Pearson correlation coefficient for I. versicolor: r
r = pearson_r(versicolor_petal_length, versicolor_petal_width)

# Print the result
print(r)

```
  
  
**Example #1: ECDF with Key Percentiles**:  
![](_dummyPy155.png) 

**Example #2: Box Plot**:  
![](_dummyPy156.png)

**Example #3: Scatter Plot**:  
![](_dummyPy157.png)

  
***
  
Chapter 3 - Thinking Probabilistically (Discrete Variables)  
  
Probabilistic logic and statistical inference - drawing conclusions about the population based on my sample:  
  
* Multiple re-samples will generate multiple means  
  
Random number generators and hacker statistics - tool for thinking probabilistically:  
  
* Use simulated repeated measurements to compute probabilities  
* Initial simulations were run on games of chance (particularly coin flips) by Pascal et al  
* The NumPy module has many useful random number generators  
	* np.random.random(size=n)  # draws "n" random numbers between 0 and 1  
    * np.random.seed()  # allows for providing a seed for re-producibility  
* Hacker statistics - define simulation, run many times, summarize key statistic of interest  
  
Probability distributions - Binomial:  
  
* Probability Mass Function (PMF) - set of probabilities of discrete outcomes  
	* Rolling a dice is an example - can only get discrete values (integers 1-6)  
* Binomial Distribution - the number of successes, r, in n Bernoullie trials with probability p of success will be binomially distributed  
	* np.random.binomial(nTrials, pSuccess, size=n)  # will simulate the sum of successes in the nTrials each with pSuccess ; will create a 1D array of length n by repeating the simulation n times  
  
Poisson processes and distributions - specified solely by a rate, independent of any previous events:  
  
* Natural births in a hospital, timing of hits to a website, etc.  
* Poisson distribution - the number r of arrivals of a Poisson process in a given time period with an average rate lambda of arrivals is Poisson distributed  
* The Poisson distribution is a limiting case of the Binomial distribution when there is a low succes probability but a large amount of time -- i.e., rare events  
	* np.random.poisson(param, size=)  # where param is the expected number of events and size is the number of simulations desired  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



# Seed the random number generator
np.random.seed(42)

# Initialize random numbers: random_numbers
random_numbers = np.empty(100000)

# Generate random numbers by looping over range(100000)
for i in range(100000):
    random_numbers[i] = np.random.random()

# Plot a histogram
_ = plt.hist(random_numbers)

# Show the plot
# plt.show()
plt.savefig("_dummyPy158.png", bbox_inches="tight")
plt.clf()


def perform_bernoulli_trials(n, p):
    """Perform n Bernoulli trials with success probability p
    and return number of successes."""
    # Initialize number of successes: n_success
    n_success = 0
    
    # Perform trials
    for i in range(n):
        # Choose random number between zero and one: random_number
        random_number = np.random.random()
        
        # If less than p, it's a success so add one to n_success
        if random_number < p:
            n_success +=1
    
    return n_success


# Seed random number generator
np.random.seed(42)

# Initialize the number of defaults: n_defaults
n_defaults = np.empty(1000)

# Compute the number of defaults
for i in range(1000):
    n_defaults[i] = perform_bernoulli_trials(100, 0.05)

# Plot the histogram with default number of bins; label your axes
_ = plt.hist(n_defaults, normed=True)
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('probability')

# Show the plot
# plt.show()
plt.savefig("_dummyPy159.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Compute ECDF: x, y
x, y = ecdf(n_defaults)  # same function as written in previous chapters

# Plot the ECDF with labeled axes
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.xlabel("Number of Defaults in Simulation")
_ = plt.ylabel("ECDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy160.png", bbox_inches="tight")
plt.clf()


# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money
n_lose_money = np.sum(n_defaults >= 10)

# Compute and print probability of losing money
print('Probability of losing money =', n_lose_money / len(n_defaults))


# Take 10,000 samples out of the binomial distribution: n_defaults
n_defaults = np.random.binomial(100, 0.05, size=10000)

# Compute CDF: x, y
x, y = ecdf(n_defaults)

# Plot the CDF with axis labels
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.xlabel("Defaults in 100 loans")
_ = plt.ylabel("CDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy161.png", bbox_inches="tight")
plt.clf()


# Compute bin edges: bins
bins = np.arange(min(n_defaults), max(n_defaults) + 2) - 0.5

# Generate histogram
_ = plt.hist(n_defaults, bins=bins, normed=True)

# Set margins
plt.margins(0.02)

# Label axes
_ = plt.xlabel("Number of Defaults in 100 Loans")
_ = plt.ylabel("Probability")

# Show the plot
# plt.show()
plt.savefig("_dummyPy162.png", bbox_inches="tight")
plt.clf()


# Draw 10,000 samples out of Poisson distribution: samples_poisson
samples_poisson = np.random.poisson(10, size=10000)

# Print the mean and standard deviation
print('Poisson:     ', np.mean(samples_poisson),
                       np.std(samples_poisson))

# Specify values of n and p to consider for Binomial: n, p
n = [20, 100, 1000]
p = [0.5, 0.1, 0.01]

# Draw 10,000 samples for each n,p pair: samples_binomial
for i in range(3):
    samples_binomial = np.random.binomial(n[i], p[i], size=10000)

    # Print results
    print('n =', n[i], 'Binom:', np.mean(samples_binomial),
                                 np.std(samples_binomial))


# Draw 10,000 samples out of Poisson distribution: n_nohitters
n_nohitters = np.random.poisson(251/115, size=10000)

# Compute number of samples that are seven or greater: n_large
n_large = np.sum(n_nohitters >= 7)

# Compute probability of getting seven or more: p_large
p_large = n_large / 10000

# Print the result
print('Probability of seven or more no-hitters:', p_large)


```
  
  
**Example #1: Histogram of Random Numbers**:  
![](_dummyPy158.png) 

**Example #2: Histogram of 1000 Bernoulli Trials (each with p=0.05, n=100)**:  
![](_dummyPy159.png) 

**Example #3: ECDF of 1000 Bernoulli Trials**:  
![](_dummyPy160.png) 

**Example #4: Histogram/ECDF of Binomial Trials (n=100, p=0.05, size=10000)**:  
![](_dummyPy161.png) 

**Example #5: Normed Histogram with Customized Binning for Binomial Trials (n=100, p=0.05, size=10000)**:  
![](_dummyPy162.png) 
  
  
***
  
Chapter 4 - Thinking probabilistically - continuous variables  
  
Probability density functions (PDF) - continuous analog to the PMF:  
  
* Mathematical description of the relative likelihood of observing a value of a continuous variable  
* Areas under the PDF describe the associated probabilities  
* The CDF can be calculated from the PDF, and can be easier to interpret  
  
Introduction to the Normal Distribution - continuous variable with a single peak:  
  
* Mean - center of the peak  
* Standard Deviation - degree of spread of the data around the peak  
* To avoid binning bias, better to compare theoretical/actual CDF rather than histogram and theoretical curve overlaid  
* To simulate normal data, call np.random.normal(mean, std, size=)  
	
Normal distribution - properties and warnings:  
  
* Commonly called the Gaussian dsitribution after its inventor (was previously featured on the German Deutschemark prior to adoption of the Euro)  
* Important caveats include  
	1.  Sometimes, things that you think "should" be normally distributed in fact are not  
    2.  Normal distributions have very light tails (outliers are extremely unlikely), which often does not match to the real-world  
  
Exponential distribution - related to the Poisson distribution:  
  
* The expected number of occurences being Poisson distributed means that the expected time between events will be exponentially distributed  
* The waiting time between arrivals of a Poisson distribution is exponentially distributed - defined by a single parameter, "mean waiting time"  
	* np.random.exponential(mean, size=)  # will simulate "size" times from an exponential with mean "mean"  
* Simulating a story can be very powerful - existence of computers makes the "pen and paper" modelling less necessary in many cases  
  
Final thoughts - course recap:  
  
* Construct instructive plots  
* Compute informative summary statistics  
* Use hacker statistics  
* Think probabilistically  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



# Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10
samples_std1 = np.random.normal(20, 1, size=100000)
samples_std3 = np.random.normal(20, 3, size=100000)
samples_std10 = np.random.normal(20, 10, size=100000)

# Make histograms
_ = plt.hist(samples_std1, normed=True, histtype="step", bins=100)
_ = plt.hist(samples_std3, normed=True, histtype="step", bins=100)
_ = plt.hist(samples_std10, normed=True, histtype="step", bins=100)

# Make a legend, set limits and show plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))
plt.ylim(-0.01, 0.42)
# plt.show()
plt.savefig("_dummyPy163.png", bbox_inches="tight")
plt.clf()


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Generate CDFs
x_std1, y_std1 = ecdf(samples_std1)  # function written earlier in previous chapter
x_std3, y_std3 = ecdf(samples_std3)
x_std10, y_std10 = ecdf(samples_std10)

# Plot CDFs
_ = plt.plot(x_std1, y_std1, marker=".", linestyle="none")
_ = plt.plot(x_std3, y_std3, marker=".", linestyle="none")
_ = plt.plot(x_std10, y_std10, marker=".", linestyle="none")

# Make 2% margin
plt.margins(0.02)

# Make a legend and show the plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')
# plt.show()
plt.savefig("_dummyPy164.png", bbox_inches="tight")
plt.clf()


rawBelmont = pd.read_csv(myPath + "belmont_stakes_1926_2016.csv")

from datetime import datetime

listBelmont = [x.split(".") for x in rawBelmont["Time"]]
timeBelmont = [int(x[0].split(":")[0]) * 60 + int(x[0].split(":")[1]) + int(x[1])/100 for x in listBelmont]
belmont_no_outliers = timeBelmont


# Compute mean and standard deviation: mu, sigma
mu = np.mean(belmont_no_outliers)
sigma = np.std(belmont_no_outliers)

# Sample out of a normal distribution with this mu and sigma: samples
samples = np.random.normal(mu, sigma, size=10000)

# Get the CDF of the samples and of the data
x_theor, y_theor = ecdf(samples)
x, y = ecdf(belmont_no_outliers)

# Plot the CDFs and show the plot
_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x, y, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('Belmont winning time (sec.)')
_ = plt.ylabel('CDF')
# plt.show()
plt.savefig("_dummyPy165.png", bbox_inches="tight")
plt.clf()


# Take a million samples out of the Normal distribution: samples
samples = np.random.normal(mu, sigma, size=1000000)

# Compute the fraction that are faster than 144 seconds: prob
prob = np.mean(samples <= 144)

# Print the result
print('Probability of besting Secretariat:', prob)


def successive_poisson(tau1, tau2, size=1):
    # Draw samples out of first exponential distribution: t1
    t1 = np.random.exponential(tau1, size)
    
    # Draw samples out of second exponential distribution: t2
    t2 = np.random.exponential(tau2, size)
    
    return t1 + t2


# Draw samples of waiting times: waiting_times
waiting_times = successive_poisson(764, 715, size=100000)

# Make the histogram
_ = plt.hist(waiting_times, bins=100, normed=True, histtype="step")

# Label axes
_ = plt.xlabel("Total Waiting Time (Days) for Cycle and No-Hitter")
_ = plt.ylabel("PDF")

# Show the plot
# plt.show()
plt.savefig("_dummyPy166.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Histogram of Normal Distribution with Different STD**:  
![](_dummyPy163.png) 

**Example #2: ECDF of Normal Distribution with Different STD**:  
![](_dummyPy164.png) 

**Example #3: ECDF of Belmont Stakes Winning Times**:  
![](_dummyPy165.png) 

**Example #4: Sum of Poisson Outcomes**:  
![](_dummyPy166.png) 


###_Statistical Thinking in Python (Part I)_#
  
Chapter 1 - Parameter estimation by optimization  
  
Optimal parameters - parameters that bring the model in closest agreement with the data:  
  
* Parameters derived from the sample are relevant mainly to that sample; other samples might have other optimal parameters  
* Two key packages for statistics in Python - scipy.stats and statsmodels  
* For this course, though, the focus will be on hacker statistics - simulated data using numpy  
  
Linear regression by least squares - fitting the best slope and intercept to the line:  
  
* Residuals are the vertical distance between the data point and the regression line  
* The "least squares" algorithm defines the best fit to be the line that minimizes the sum-squared residuals  
* Regressions can be run in Python using np.polyfit()  # least squares with polynomials  
	* slope, intercept = np.polyfit(xData, yData, nDegree)  # nDegree is the desired degree of the polynomial, which is to say 1 for linear regression  
  
Importance of EDA (Anscombe's quartet) - 4 fictitious datasets with the same x-bar, y-bar, correlation, and RSS:  
  
* Graphical EDA is vital before diving in to the regression (or other) analysis  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]

# Seed random number generator
np.random.seed(42)

# Compute mean no-hitter time: tau
tau = np.mean(nohitter_times)

# Draw out of an exponential distribution with parameter tau: inter_nohitter_time
inter_nohitter_time = np.random.exponential(tau, 100000)

# Plot the PDF and label axes
_ = plt.hist(inter_nohitter_time,
             bins=50, normed=True, histtype="step")
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy167.png", bbox_inches="tight")
plt.clf()



def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


# Uses function ecdf() from previous course
# Create an ECDF from real data: x, y
x, y = ecdf(nohitter_times)

# Create a CDF from theoretical samples: x_theor, y_theor
x_theor, y_theor = ecdf(inter_nohitter_time)

# Overlay the plots
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker=".", linestyle="none")

# Margins and axis labels
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy168.png", bbox_inches="tight")
plt.clf()



# Plot the theoretical CDFs
plt.plot(x_theor, y_theor)
plt.plot(x, y, marker='.', linestyle='none')
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

# Take samples with half tau: samples_half
samples_half = np.random.exponential(tau/2, size=10000)

# Take samples with double tau: samples_double
samples_double = np.random.exponential(tau*2, size=10000)

# Generate CDFs from these samples
x_half, y_half = ecdf(samples_half)
x_double, y_double = ecdf(samples_double)

# Plot these CDFs as lines
_ = plt.plot(x_half, y_half)
_ = plt.plot(x_double, y_double)

# Show the plot
# plt.show()
plt.savefig("_dummyPy169.png", bbox_inches="tight")
plt.clf()



import pandas as pd
rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')

# Set the margins and label axes
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Show the plot
# plt.show()
plt.savefig("_dummyPy170.png", bbox_inches="tight")
plt.clf()



def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


# Show the Pearson correlation coefficient
print(pearson_r(illiteracy, fertility))


# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

# Perform a linear regression using np.polyfit(): a, b
a, b = np.polyfit(illiteracy, fertility, 1)

# Print the results to the screen
print('slope =', a, 'children per woman / percent illiterate')
print('intercept =', b, 'children per woman')

# Make theoretical line to plot
x = np.array([0, 100])
y = a * x + b

# Add regression line to your plot
_ = plt.plot(x, y)

# Draw the plot
# plt.show()
plt.savefig("_dummyPy171.png", bbox_inches="tight")
plt.clf()


# Specify slopes to consider: a_vals
a_vals = np.linspace(0, 0.1, 200)

# Initialize sum of square of residuals: rss
rss = np.empty_like(a_vals)

# Compute sum of square of residuals for each value of a_vals
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy - b)**2)

# Plot the RSS
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')

#plt.show()
plt.savefig("_dummyPy172.png", bbox_inches="tight")
plt.clf()


rawAnscombe = pd.read_csv(myPath + "anscombe.csv", header=None)

anscombe_x = [[float(x) for x in rawAnscombe.iloc[2:, 0]], [float(x) for x in rawAnscombe.iloc[2:, 2]], [float(x) for x in rawAnscombe.iloc[2:, 4]], [float(x) for x in rawAnscombe.iloc[2:, 6]]]

anscombe_y = [[float(x) for x in rawAnscombe.iloc[2:, 1]], [float(x) for x in rawAnscombe.iloc[2:, 3]], [float(x) for x in rawAnscombe.iloc[2:, 5]], [float(x) for x in rawAnscombe.iloc[2:, 7]]]


x=anscombe_x[0]
y=anscombe_y[0]


# Perform linear regression: a, b
a, b = np.polyfit(x, y, 1)

# Print the slope and intercept
print(a, b)

# Generate theoretical x and y data: x_theor, y_theor
x_theor = np.array([3, 15])
y_theor = a * x_theor + b

# Plot the Anscombe data and theoretical line
_ = plt.plot(x, y, marker=".", linestyle="none")
_ = plt.plot(x_theor, y_theor)

# Label the axes
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
# plt.show()
plt.savefig("_dummyPy173.png", bbox_inches="tight")
plt.clf()



# Iterate through x,y pairs
for x, y in zip(anscombe_x, anscombe_y):
    # Compute the slope and intercept: a, b
    a, b = np.polyfit(x, y, 1)
    
    # Print the result
    print('slope:', a, 'intercept:', b)



```
  
  
**Example #1: Exponential Distribution**:  
![](_dummyPy167.png) 
  
**Example #2: Theoretical vs Actual ECDF (Exponential for Time Between No-Hitters)**:  
![](_dummyPy168.png)

**Example #3: ECDF for Exponential Distribution with Half-Rate and Double-Rate**:  
![](_dummyPy169.png)

**Example #4: Scatter-Plot for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy170.png)

**Example #5: Best Linear Fit for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy171.png)

**Example #6: Residual Sum-Squares vs Best-Fit Parameters**:  
![](_dummyPy172.png)

**Example #7: Regression Fit for Anscombe Data #1**:  
![](_dummyPy173.png)
  
***
  
Chapter 2 - Bootstrap confidence intervals  
  
Generating bootstrap replicates - how might the sample statistics change if we acquired a new sample:  
  
* Hacker approach - resample the existing data (WITH replacement), using the same n, then recalculate the sample statistics  
* Can then take multiple re-samples and aggregate the sample statistics for a statistical inference  
* "Bootstrap replicate" is the value of the summary statistic calculated from the "bootstrap sample"  
* Boostrap samples can easily be created in Python  
	* np.random.choice(myData, size=len(myData))  # can specify any size, but len(myData) will be a bootstrap; sampling is WITH replacement  
  
Bootstrap confidence intervals:  
  
* Can generate a function bootstrap_replicate_1d(data, func) which takes a bootstrap sample of data and then applies func to it  
* Can then use a for loop to run through the function, storing the results in a numpy array (or appended to an empty list, or etc.)  
* Can generate the confidence interval using np.percentile(all_replicates, [2.5, 97.5])  # this is for the 95% confidence interval  
  
Pairs bootstrap - bootstrap resample on pairs of data (e.g., keep the x and y connected to each other):  
  
* Bootstrap approach is non-parametric, since it is not forced to fit any particular model  
* Least-squares approach is parametrics, since it is forced to fit a pre-defined specification of the model  
* Can use bootstrap techniques to get confidence intervals on the parametric (e.g., least-squares) models  
	* Use a pairs bootstrap that keeps each of the x/y anchored to each other  
    * Resample WITH replacement on all the x/y pairs, then run the model with this bootstrap resample  
* The approach within Python is a slight modification of the 1D approach  
    * inds = np.arange(len(myXYData))  
    * bs_inds = np.random.choice(inds, len(inds))  
    * Use bs_inds as the .iloc (or index) filters of the dataset  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



rawWeather = pd.read_csv(myPath + "sheffield_weather_station.csv", skiprows=8, delim_whitespace=True)
rainfall = rawWeather["rain"]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y



for a in range(50):
    # Generate bootstrap sample: bs_sample
    bs_sample = np.random.choice(rainfall, size=len(rainfall))
    
    # Compute and plot ECDF from bootstrap sample
    x, y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker='.', linestyle='none',
                 color='gray', alpha=0.1)

# Compute and plot ECDF from original data
x, y = ecdf(rainfall)
_ = plt.plot(x, y, marker='.')

# Make margins and label axes
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy174.png", bbox_inches="tight")
plt.clf()



def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


# Take 10,000 bootstrap replicates of the mean: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.mean, 10000)

# Compute and print SEM
sem = np.std(rainfall) / np.sqrt(len(rainfall))
print(sem)

# Compute and print standard deviation of bootstrap replicates
bs_std = np.std(bs_replicates)
print(bs_std)

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('mean annual rainfall (mm)')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy175.png", bbox_inches="tight")
plt.clf()


# Generate 10,000 bootstrap replicates of the variance: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.var, 10000)

# Put the variance in units of square centimeters
bs_replicates = bs_replicates / 100

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel('variance of annual rainfall (sq. cm)')
_ = plt.ylabel('PDF')

# Show the plot
#plt.show()
plt.savefig("_dummyPy176.png", bbox_inches="tight")
plt.clf()



rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]


# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates
bs_replicates = draw_bs_reps(nohitter_times, np.mean, 10000)

# Compute the 95% confidence interval: conf_int
conf_int = np.percentile(bs_replicates, [2.5, 97.5])

# Print the confidence interval
print('95% confidence interval =', conf_int, 'games')

# Plot the histogram of the replicates
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel(r'$\tau$ (games)')
_ = plt.ylabel('PDF')

# Show the plot
# plt.show()
plt.savefig("_dummyPy177.png", bbox_inches="tight")
plt.clf()



def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates: bs_slope_reps, bs_intercept_reps
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    
    return bs_slope_reps, bs_intercept_reps



rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy

# Generate replicates of slope and intercept using pairs bootstrap
bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, 1000)

# Compute and print 95% CI for slope
print(np.percentile(bs_slope_reps, [2.5, 97.5]))

# Plot the histogram
_ = plt.hist(bs_slope_reps, bins=50, normed=True)
_ = plt.xlabel('slope')
_ = plt.ylabel('PDF')

# plt.show()
plt.savefig("_dummyPy178.png", bbox_inches="tight")
plt.clf()


# Generate array of x-values for bootstrap lines: x
x = np.array([0, 100])

# Plot the bootstrap lines
for i in range(100):
    _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i],
                 linewidth=0.5, alpha=0.2, color='red')

# Plot the data
_ = plt.plot(illiteracy, fertility, marker=".", linestyle="none")

# Label axes, set the margins, and show the plot
_ = plt.xlabel('illiteracy')
_ = plt.ylabel('fertility')
plt.margins(0.02)

# plt.show()
plt.savefig("_dummyPy179.png", bbox_inches="tight")
plt.clf()


```
  
  
**Example #1: Bootstrap ECDF for Sheffield Annual Rainfall**:  
![](_dummyPy174.png)

**Example #2:  Bootstrap Mean for Sheffield Annual Rainfall**:  
![](_dummyPy175.png)

**Example #3:  Bootstrap Variance for Sheffield Annual Rainfall**:  
![](_dummyPy176.png)

**Example #4: Bootstrap for No-Hitter Times**:  
![](_dummyPy177.png)

**Example #5: Bootstrap Pairs for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy178.png)

**Example #6: Bootstrap Pairs for Fertility vs Illiteracy (Gapminder)**:  
![](_dummyPy179.png)

  
***
  
Chapter 3 - Hypothesis Testing  
  
Formulating and simulating a hypothesis:  
  
* Hypothesis testing - how reasonable are the observed data assuming a particular hypothesis is true  
* Hacker approach - scrambling the data - example of PA and OH 2008 voting data (% D)  
	* Combine all county level results from PA (67 counties) and OH, ignoring which state they are from  
    * Permute (randomly scramble) the all-county dataset, and declare the first 67 counties as the PA data  
* Running a permutation process in Python using NumPy  
	* dem_share_both = np.concatenate((dem_PA, dem_OH))  # takes a tuple of the desired inputs  
    * dem_share_perm = np.random.permutation(dem_share_both)  # permutes the full dataset  
    * perm_PA = dem_share_perm[:len(dem_share_PA)]  
    * perm_OH = dem_share_perm[len(dem_share_PA):]  
  
Test statistics and p-values - testing the permutations as per the above, assuming the null hypothesis of identical distributions in OH/PA:  
  
* Test statistic - single number that can be computed from observed data, as well as from data simulated under the null hypothesis  
	* Serves as a basis for comparison between predictions and actual observations  
    * Can choose "difference in means" as the test statistic in this case  
* Can compare the observed test statistic to the permutation replicates, and assess likelihood of results "as extreme" as the actual test statistic  
	* The p-value is the probability of getting a result at least as extreme as the observed test statistic, under the assumption that they null hypothesis is valid  
* Null Hypothesis Significance Testing (NHST) - consider the value of the p-value as well as the magnitude of the differences  
	* Statistical significance (p-value) and practical significance are two different things  
  
Bootstrap hypothesis tests:  
  
* Basic analysis pipeline - clearly state null hypothesis; define test statistic; generate many permuted datasets assuming null hypothesis; compute test statistic for each permutation  
* Additional analysis type - have ALL of the Michelson speed of light data, but only the mean calculated by Newcomb - could Newcomb mean have come from Michelson experiment?  
	* Null hypothesis becomes "True mean speed of light in Michelson experiment is equal to Newcomb's reported mean"  
    * Shift the means of the Michelson experimental data (every point) to match the Newcomb reported mean: michelson - np.mean(michelson) + newcomb  
    * Calculate p-values based on permutations of shifted Michelson data  
* One sample test - compare one set of data to a single number  
* Two sample test - compare two sets of data against each other  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2


# Create some fake rain data - July and November
rain_july = np.random.normal(3.5, 1.25, 100)
rain_november = np.random.normal(2.25, 0.75, 100)

for i in range(50):
    # Generate permutation samples
    perm_sample_1, perm_sample_2 = permutation_sample(rain_july, rain_november)
    
    # Compute ECDFs
    x_1, y_1 = ecdf(perm_sample_1)
    x_2, y_2 = ecdf(perm_sample_2)
    
    # Plot ECDFs of permutation sample
    _ = plt.plot(x_1, y_1, marker='.', linestyle='none',
                 color='red', alpha=0.02)
    _ = plt.plot(x_2, y_2, marker='.', linestyle='none',
                 color='blue', alpha=0.02)

# Create and plot ECDFs from original data
x_1, y_1 = ecdf(rain_july)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Display the plot
# plt.show()
plt.savefig("_dummyPy180.png", bbox_inches="tight")
plt.clf()



rawFrog = pd.read_csv(myPath + "frog_tongue.csv", header=14)


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


# Make bee swarm plot
_ = sns.swarmplot(x="ID", y='impact force (mN)', data=rawFrog)

# Label axes
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')

# Show the plot
# plt.show()
plt.savefig("_dummyPy181.png", bbox_inches="tight")
plt.clf()


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


# Define force_a to be frog I and force_b to be frog II
force_a = rawFrog.loc[rawFrog["ID"] == "I", 'impact force (mN)']
force_b = rawFrog.loc[rawFrog["ID"] == "II", 'impact force (mN)']

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a, force_b)

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(force_a, force_b,
                                 diff_of_means, size=10000)

# Compute p-value: p
p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)

# Print the result
print('p-value =', p)


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b - np.mean(force_b) + 0.55

# Function available in previous chapters - copy down
# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)

# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000

# Print the p-value
print('p = ', p)


# Per the notes from DataCamp, the permutation test is a more exact test of the null hypothesis while the (below) bootstrap test is more versatile
#  "However, the permutation test exactly simulates the null hypothesis that the data come from the same distribution, whereas the bootstrap test approximately simulates it. As we will see, though, the bootstrap hypothesis test, while approximate, is more versatile."

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a, force_b)

# Concatenate forces: forces_concat
forces_concat = np.concatenate((force_a, force_b))

# Initialize bootstrap replicates: bs_replicates
bs_replicates = np.empty(10000)

for i in range(10000):
    # Generate bootstrap sample
    bs_sample = np.random.choice(forces_concat, size=len(forces_concat))
    
    # Compute replicate
    bs_replicates[i] = diff_of_means(bs_sample[:len(force_a)],
                                     bs_sample[len(force_a):])

# Compute and print p-value: p
p = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)
print('p-value =', p)


# Compute mean of all forces: mean_force
mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

# Compute and print p-value: p
p = np.sum(bs_replicates > empirical_diff_means) / len(bs_replicates)
print('p-value =', p)

```
  
  
**Example #1: ECDF for July vs November Rainfall**:  
![](_dummyPy180.png)
  
**Example #2: Swarm Plot for Impact Force by Frog**:  
![](_dummyPy181.png)
  
***

Chapter 4 - Hypothesis test examples  
  
A/B Testing - test/control approach for "splash" pages of the website:  
  
* Null hypothesis is that the redesign (page B) has no impact on the click-through rate relative to the control design (page A)  
* Calculate a test statistic as np.mean(B) - np.mean(A)  # average click-through rate is the test statistic  
* Combine and permute the data, and calculate test statistic for permutation replicates  
* Calculate likelihood of seeing data as large or larger than the test statistic  
  
Test of correlation - returning to the Obama vote share vs. county size analysis from prior:  
  
* Null hypothesis is of no correlation between Obama vote share and county size (total votes)  
* Use the Pearson correlation coefficient as the test statistic  
* Leave one of the columns constant and permute the other; calculate the permutation replicate based on this new dataset  
* Repeat many times, and calculate the likelihood of achieving a correlation as extreme as was observed  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2



# Construct arrays of data: dems, reps
dems = np.array([True] * 153 + [False] * 91)
reps = np.array([True] * 136 + [False] * 35)

def frac_yay_dems(dems, reps):
    """Compute fraction of Democrat yay votes."""
    frac = np.sum(dems) / len(dems)
    return frac

# draw_perm_reps is available in previous chapters
# Acquire permutation samples: perm_replicates
perm_replicates = draw_perm_reps(dems, reps, frac_yay_dems, 10000)

# Compute and print p-value: p
p = np.sum(perm_replicates <= 153/244) / len(perm_replicates)
print('p-value =', p)


# No Hitter Dataset is split to include pre-1920 (dead ball era) and 1920-current (live ball era)
rawNoHitter = pd.read_csv(myPath + "mlb_nohitters.csv", parse_dates=["date"])
nohitter_times = rawNoHitter["game_number"].diff()[1:]

from datetime import datetime
deadEra = rawNoHitter["date"] < datetime.strptime("1920-01-01", "%Y-%m-%d")
deadEra = deadEra[1:]

nht_dead = nohitter_times[deadEra]
nht_live = nohitter_times[deadEra == False]

# Compute the observed difference in mean inter-no-hitter times: nht_diff_obs
nht_diff_obs = diff_of_means(nht_dead, nht_live)

# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates
perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, 10000)

# Compute and print the p-value: p
p = np.sum(perm_replicates <= nht_diff_obs) / len(perm_replicates)
print('p-val =',p)


# Function pearson_r is available from the previous course
rawGap = pd.read_csv(myPath + "literacy_birth_rate.csv", index_col=None)
fertility = rawGap["fertility"]
female_literacy = rawGap["female literacy"]
illiteracy = 100 - female_literacy


# Compute observed correlation: r_obs
r_obs = pearson_r(illiteracy, fertility)

# Initialize permutation replicates: perm_replicates
perm_replicates = np.empty(10000)

# Draw replicates
for i in range(10000):
    # Permute illiteracy measurments: illiteracy_permuted
    illiteracy_permuted = np.random.permutation(illiteracy)
    
    # Compute Pearson correlation
    perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)

# Compute p-value: p
p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)
print('p-val =', p)


# Function ecdf is available from previous course
rawBee = pd.read_csv(myPath + "bee_sperm.csv", header=3, index_col=None)
control = rawBee.loc[rawBee["Treatment"] == "Control", "Sperm Volume per 500 ul"]
treated = rawBee.loc[rawBee["Treatment"] == "Pesticide", "Sperm Volume per 500 ul"]

# Compute x,y values for ECDFs
x_control, y_control = ecdf(control)
x_treated, y_treated = ecdf(treated)

# Plot the ECDFs
plt.plot(x_control, y_control, marker='.', linestyle='none')
plt.plot(x_treated, y_treated, marker='.', linestyle='none')

# Set the margins
plt.margins(0.02)

# Add a legend
plt.legend(('control', 'treated'), loc='lower right')

# Label axes and show plot
plt.xlabel('millions of alive sperm per mL')
plt.ylabel('ECDF')
# plt.show()
plt.savefig("_dummyPy182.png", bbox_inches="tight")
plt.clf()


# draw_bs_reps() is available back in Chapter 2
# Compute the difference in mean sperm count: diff_means
diff_means = np.mean(control) - np.mean(treated)

# Compute mean of pooled data: mean_count
mean_count = np.mean(np.concatenate((control, treated)))

# Generate shifted data sets
control_shifted = control - np.mean(control) + mean_count
treated_shifted = treated - np.mean(treated) + mean_count

# Generate bootstrap replicates
bs_reps_control = draw_bs_reps(control_shifted,
                       np.mean, size=10000)
bs_reps_treated = draw_bs_reps(treated_shifted,
                       np.mean, size=10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_reps_control - bs_reps_treated

# Compute and print p-value: p
p = np.sum(bs_replicates >= diff_means) / len(bs_replicates)
print('p-value =', p)

```
  
  
**Example #1: A/B Testing for Bees**:  
![](_dummyPy182.png)
  
***
  
Chapter 5 - Case Study  
  
Finch beaks and need for statistics:  
  
* Toolbox includes: Graphical and quantitative EDA ; Parameter estimation ; Confidence intervals ; Hypothesis testing  
* Finches (inhabitants of the Galapagos) formed a key backbone of Darwin's research for the theory of evolution  
	* Benefits include isolation, and a number of very small islands where the entire population can be monitored  
    * Researchers spend time every year in Daphne Major, one of the islands of the Galapagos  
* Finches of Daphne Major include Geospiza fortis and Geospiza scandens  
	* Peter and Rosemary Grant: "40 Years of Evolution: Darwin's Finches on Daphne Major Island" (2014)  
    * Data available for free, public use - see Dryad Digital Repository  
    * Data include Beak Length and Beak Depth  
* Initial analysis for the case study will be G. scandens beak depth changes over time (1975 vs. 2012)  
	* Hypothesis test: did the beaks get deeper from 1975 to 2012?  
  
Variation of beak shapes:  
  
* One hypothesis is that a drought in winter 1976/1977 led to a shortage of small seed (large seed were still pentiful), advataging finches with deeper beaks  
* Question is whether the beak length and beak depth are changing at the same rate, which would mean that the beak shape (aspect ratio) stayed the same  
  
Heritability - hypothesis of mating between G. scandens and G. fortis, producing hybrid birds:  	
  
* The hybrid birds then mate with pure G. scandens birds - process called "introgressive hybridization"  
* Question for testing is "how strongly are traits inherited by offspring from parents?"  
	* Assess for both G. scandens and G. fortis  
  
Final thoughts and course recap:  
  
* Perform EDA - ECDF, summary statistics  
* Estimate parameters - optimization, linear regression, confidence intervals (bootstrap method)  
* Formulate and test hypotheses  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""
    
    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)
    
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
        
        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)
    
    return perm_replicates


def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""
    
    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1) - np.mean(data_2)
    
    return diff


def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)
    
    # Return entry [0,1]
    return corr_mat[0,1]


def ecdf(data):
    """Compute ECDF for a one-dimensional array of measurements."""
    
    # Number of data points: n
    n = len(data)
    
    # x-data for the ECDF: x
    x = np.sort(data)
    
    # y-data for the ECDF: y
    y = np.arange(1, n+1) / n
    
    return x, y


def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))


def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    
    # Initialize array of replicates: bs_replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    
    return bs_replicates


def permutation_sample(data1, data2):
    """Generate a permutation sample from two data sets."""
    
    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))
    
    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)
    
    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    
    return perm_sample_1, perm_sample_2


def draw_bs_pairs_linreg(x, y, size=1):
    """Perform pairs bootstrap for linear regression."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates: bs_slope_reps, bs_intercept_reps
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, size=len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    
    return bs_slope_reps, bs_intercept_reps



rawFinch1975 = pd.read_csv(myPath + "finch_beaks_1975.csv", header=0)
rawFinch1975.columns = ["band", "species", "beak_length", "beak_depth"]
rawFinch1975["year"] = 1975
rawFinch2012 = pd.read_csv(myPath + "finch_beaks_2012.csv", header=0)
rawFinch2012.columns = ["band", "species", "beak_length", "beak_depth"]
rawFinch2012["year"] = 2012

df = rawFinch1975.append(rawFinch2012)
bd_1975 = df.loc[(df["year"] == 1975) & (df["species"] == "scandens"), "beak_depth"].reset_index(drop=True)
bd_2012 = df.loc[(df["year"] == 2012) & (df["species"] == "scandens"), "beak_depth"].reset_index(drop=True)
bl_1975 = df.loc[(df["year"] == 1975) & (df["species"] == "scandens"), "beak_length"].reset_index(drop=True)
bl_2012 = df.loc[(df["year"] == 2012) & (df["species"] == "scandens"), "beak_length"].reset_index(drop=True)


# Create bee swarm plot
_ = sns.swarmplot(x="year", y="beak_depth", data=df.loc[df["species"] == "scandens", :])

# Label the axes
_ = plt.xlabel('year')
_ = plt.ylabel('beak depth (mm)')

# Show the plot
# plt.show()
plt.savefig("_dummyPy183.png", bbox_inches="tight")
plt.clf()


# function ecdf() is available from previous chapters
# Compute ECDFs
x_1975, y_1975 = ecdf(bd_1975)
x_2012, y_2012 = ecdf(bd_2012)

# Plot the ECDFs
_ = plt.plot(x_1975, y_1975, marker='.', linestyle='none')
_ = plt.plot(x_2012, y_2012, marker='.', linestyle='none')

# Set margins
plt.margins(0.02)

# Add axis labels and legend
_ = plt.xlabel('beak depth (mm)')
_ = plt.ylabel('ECDF')
_ = plt.legend(('1975', '2012'), loc='lower right')

# Show the plot
# plt.show()
plt.savefig("_dummyPy184.png", bbox_inches="tight")
plt.clf()


# draw_bs_reps() is available from previous chapters
# Compute the difference of the sample means: mean_diff
mean_diff = np.mean(bd_2012) - np.mean(bd_1975)

# Get bootstrap replicates of means
bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, 10000)

# Compute samples of difference of means: bs_diff_replicates
bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

# Compute 95% confidence interval: conf_int
conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5])

# Print the results
print('difference of means =', mean_diff, 'mm')
print('95% confidence interval =', conf_int, 'mm')


# Compute mean of combined data set: combined_mean
combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))

# Shift the samples
bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean
bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean

# Get bootstrap replicates of shifted data sets
bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, 10000)

# Compute replicates of difference of means: bs_diff_replicates
bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

# Compute the p-value
p = np.sum(bs_diff_replicates >= mean_diff) / len(bs_diff_replicates)

# Print p-value
print('p =', p)


# Make scatter plot of 1975 data
_ = plt.plot(bl_1975, bd_1975, marker='.',
             linestyle='none', color="blue", alpha=0.5)

# Make scatter plot of 2012 data
_ = plt.plot(bl_2012, bd_2012, marker='.',
             linestyle='none', color="red", alpha=0.5)

# Label axes and make legend
_ = plt.xlabel('beak length (mm)')
_ = plt.ylabel('beak depth (mm)')
_ = plt.legend(('1975', '2012'), loc='upper left')

# Show the plot
# plt.show()
plt.savefig("_dummyPy185.png", bbox_inches="tight")
plt.clf()


# draw_bs_pairs_linreg() is available in previous chapters
# Compute the linear regressions
slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, 1)
slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, 1)

# Perform pairs bootstrap for the linear regressions
bs_slope_reps_1975, bs_intercept_reps_1975 = draw_bs_pairs_linreg(bl_1975, bd_1975, 1000)
bs_slope_reps_2012, bs_intercept_reps_2012 = draw_bs_pairs_linreg(bl_2012, bd_2012, 1000)

# Compute confidence intervals of slopes
slope_conf_int_1975 = np.percentile(bs_slope_reps_1975, [2.5, 97.5])
slope_conf_int_2012 = np.percentile(bs_slope_reps_2012, [2.5, 97.5])
intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975, [2.5, 97.5])
intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012, [2.5, 97.5])


# Print the results
print('1975: slope =', slope_1975,
      'conf int =', slope_conf_int_1975)
print('1975: intercept =', intercept_1975,
      'conf int =', intercept_conf_int_1975)
print('2012: slope =', slope_2012,
      'conf int =', slope_conf_int_2012)
print('2012: intercept =', intercept_2012,
      'conf int =', intercept_conf_int_2012)


# Make scatter plot of 1975 data
_ = plt.plot(bl_1975, bd_1975, marker='.',
             linestyle='none', color='blue', alpha=0.5)

# Make scatter plot of 2012 data
_ = plt.plot(bl_2012, bd_2012, marker='.',
             linestyle='none', color='red', alpha=0.5)

# Label axes and make legend
_ = plt.xlabel('beak length (mm)')
_ = plt.ylabel('beak depth (mm)')
_ = plt.legend(('1975', '2012'), loc='upper left')

# Generate x-values for bootstrap lines: x
x = np.array([10, 17])

# Plot the bootstrap lines
for i in range(100):
    plt.plot(x, bs_intercept_reps_1975[i] + bs_slope_reps_1975[i] * x,
             linewidth=0.5, alpha=0.2, color="blue")
    plt.plot(x, bs_intercept_reps_2012[i] + bs_slope_reps_2012[i] * x,
             linewidth=0.5, alpha=0.2, color="red")

# Draw the plot again
# plt.show()
plt.savefig("_dummyPy186.png", bbox_inches="tight")
plt.clf()


# Compute length-to-depth ratios
ratio_1975 = bl_1975 / bd_1975
ratio_2012 = bl_2012 / bd_2012

# Compute means
mean_ratio_1975 = np.mean(ratio_1975)
mean_ratio_2012 = np.mean(ratio_2012)

# Generate bootstrap replicates of the means
bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean, 10000)
bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean, 10000)

# Compute the 99% confidence intervals
conf_int_1975 = np.percentile(bs_replicates_1975, [0.5, 99.5])
conf_int_2012 = np.percentile(bs_replicates_2012, [0.5, 99.5])

# Print the results
print('1975: mean ratio =', mean_ratio_1975,
      'conf int =', conf_int_1975)
print('2012: mean ratio =', mean_ratio_2012,
      'conf int =', conf_int_2012)


rawScandensBD = pd.read_csv(myPath + "scandens_beak_depth_heredity.csv", header=0)
rawFortisBD = pd.read_csv(myPath + "fortis_beak_depth_heredity.csv", header=0)

bd_parent_fortis = rawFortisBD.iloc[:, 1:].apply(np.mean, axis=1)
bd_offspring_fortis = rawFortisBD["Mid-offspr"]
bd_parent_scandens = rawScandensBD["mid_parent"]
bd_offspring_scandens = rawScandensBD["mid_offspring"]


# Make scatter plots
_ = plt.plot(bd_parent_fortis, bd_offspring_fortis,
             marker=".", linestyle="none", color="blue", alpha=0.5)
_ = plt.plot(bd_parent_scandens, bd_offspring_scandens,
             marker=".", linestyle="none", color="red", alpha=0.5)

# Set margins
plt.margins(0.02)

# Label axes
_ = plt.xlabel('parental beak depth (mm)')
_ = plt.ylabel('offspring beak depth (mm)')

# Add legend
_ = plt.legend(('G. fortis', 'G. scandens'), loc='lower right')

# Show plot
# plt.show()
plt.savefig("_dummyPy187.png", bbox_inches="tight")
plt.clf()


def draw_bs_pairs(x, y, func, size=1):
    """Perform pairs bootstrap for a specified function (func)."""
    
    # Set up array of indices to sample from: inds
    inds = np.arange(len(x))
    
    # Initialize replicates
    bs_replicates = np.empty(size)
    
    # Generate replicates
    for i in range(size):
        bs_inds = np.random.choice(inds, len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_replicates[i] = func(bs_x, bs_y)
    
    return bs_replicates


# Note that pearson_r() exists from the previous course
# Compute the Pearson correlation coefficients
r_scandens = pearson_r(bd_parent_scandens, bd_offspring_scandens)
r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis)

# Acquire 1000 bootstrap replicates of Pearson r
bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, 1000)
bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, 1000)

# Compute 95% confidence intervals
conf_int_scandens = np.percentile(bs_replicates_scandens, [2.5, 97.5])
conf_int_fortis = np.percentile(bs_replicates_fortis, [2.5, 97.5])

# Print results
print('G. scandens:', r_scandens, conf_int_scandens)
print('G. fortis:', r_fortis, conf_int_fortis)


def heritability(parents, offspring):
    """Compute the heritability from parent and offspring samples."""
    covariance_matrix = np.cov(parents, offspring)
    return covariance_matrix[0, 1] / covariance_matrix[0, 0]

# Compute the heritability
heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens)
heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis)

# Acquire 1000 bootstrap replicates of heritability
replicates_scandens = draw_bs_pairs(
        bd_parent_scandens, bd_offspring_scandens, heritability, size=1000)
replicates_fortis = draw_bs_pairs(
        bd_parent_fortis, bd_offspring_fortis, heritability, size=1000)

# Compute 95% confidence intervals
conf_int_scandens = np.percentile(replicates_scandens, [2.5, 97.5])
conf_int_fortis = np.percentile(replicates_fortis, [2.5, 97.5])

# Print results
print('G. scandens:', heritability_scandens, conf_int_scandens)
print('G. fortis:', heritability_fortis, conf_int_fortis)


# Initialize array of replicates: perm_replicates
perm_replicates = np.empty(10000)

# Draw replicates
for i in range(10000):
    # Permute parent beak depths
    bd_parent_permuted = np.random.permutation(bd_parent_scandens)
    perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens)

# Compute p-value: p
p = np.sum(perm_replicates >= heritability_scandens) / len(perm_replicates)

# Print the p-value
print('p-val =', p)

```
  

**Example #1: G. Scandens Beak Depth (1975 vs. 2012) Swarm Plot**:  
![](_dummyPy183.png)

**Example #2: G. Scandens Beak Depth (1975 vs. 2012) ECDF**:  
![](_dummyPy184.png)

**Example #3: G. Scandens Beak Depth vs. Length (1975 vs. 2012) Scatter Plot**:  
![](_dummyPy185.png)

**Example #4: G. Scandens Beak Depth vs. Length (1975 vs. 2012) Bootstrap Replicates**:  
![](_dummyPy186.png)

**Example #5: G. Scandens and G. Fortis Beak Depth Heritability**:  
![](_dummyPy187.png)

  

## Python Machine Learning  
###_Supervised Learning with scikit-learn_#

Chapter 1 - Classification  
  
Supervised learning - labelled data, where there is a correct answer:  
  
* Machine learning - the art and science of giving computers the ability to learn and make decisions from data (without being explicitly programmed)  
* Reinforcement learning - software agents interact with an environment, learning to optimize behavior for a system of rewards and punishments (like behavioral psychology)  
* Supervised Learning - labelled data, with both a target variable and 1+ predictor/feature variables  
	* Classification problems are categorical  
    * Regression problems are continuous  
* Note that features, predictor variables, and independent variables all mean the same thing  
* Similarly, target variables may also be called response variables or dependent variables  
* This course will use scikit-learn/sklearn - integrated well with the SciPy stack  
	* Other libraries like TensorFlow and keras are also good  
  
Exploratory data analysis - beginning with standard "iris" dataset with "species" as the target variable:  
  
* Can grab the dataset files from sklearn using "from sklearn import datasets"  
	* Typically will also need pd, np, plt, and the 'ggplot' style as set using plt.style.use('ggplot')  
    * iris = datasets.load_iris()  # will load the iris data as a "Bunch" file - set of key-value pairs - "data", "target_names", "DESCR", "feature_names", "target"  
    * The iris.data (150x4) and iris.target are both NumPy arrays, which iris.target_names is needed to map the numbers in iris.target to actual flower names  
* For EDA, can be helpful to use pd.scatter_matrix(pd.DataFrame(iris.data, columns=iris.feature_name), color=iris.target)  
  
Classification challenge - take training (already labelled) data to predict test (not yet labelled) data:  
  
* kNN (k-Nearest-Neighbors) is the process of classifying based on the closest "k" data points -- creates decision boundaries  
	* from sklearn.neighbors import KNeighborsClassifier  
    * knn = KNeighborsClassifiers(n_neighbors=6)  # this optimizer require that there is no missing data and that the features are all continuous rather than categorical  
* All machine learning models are implemented as Python classes - implement algorithms for both learning and predicting  
	* Training (fitting) is run with the .fit() method - for example, knn.fit(iris["data"], iris["target"])  
    * Predicting an unlabelled data point is run with .predict() - for example, prediction=knn.predict(X_new)  # must be a NumPy array  
  
Measuring model performance:  
  
* Accuracy is a common metric - # Correct / # Predicted - typically assessed on the "test" data  
	* Train/fit on the training data, but assess the accuracy on the test data  
    * from sklearn.model_selection import train_test_split  
    * X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)  # test_size is proportion to test, while random_state sets the seed for reproducibility  
* Can assess scores of the model using knn.score(X_test, y_test)  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt



df = pd.read_csv(myPath + "house-votes-84.csv", header=None, na_values="?")
temp = [x for x in df.columns]
temp[0] = "party"
df.columns = temp

df = df.dropna()
for ctr in range(1, df.shape[1]):
    df.iloc[:, ctr] = [1 if x == "y" else 0 for x in df.iloc[:, ctr]]


# Import KNeighborsClassifier from sklearn.neighbors
from sklearn.neighbors import KNeighborsClassifier

# Create arrays for the features and the response variable
y = df['party'].values
X = df.drop('party', axis=1).values

# Create a k-NN classifier with 6 neighbors
knn = KNeighborsClassifier(n_neighbors=6)

# Fit the classifier to the data
knn.fit(X, y)

# Predict the labels for the training data X
y_pred = knn.predict(X)

# Predict and print the label for the new data point X_new
X_new = np.array([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0]).reshape(1, -1)
new_prediction = knn.predict(X_new)
print("Prediction: {}".format(new_prediction))


# Import necessary modules
from sklearn import datasets
import matplotlib.pyplot as plt

# Load the digits dataset: digits
digits = datasets.load_digits()

# Print the keys and DESCR of the dataset
print(digits.keys())
print(digits["DESCR"])

# Print the shape of the images and data keys
print(digits.images.shape)
print(digits.data.shape)

# Display digit 1010
# plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')
# plt.show()


# Import necessary modules
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.model_selection import train_test_split

# Create feature and target arrays
X = digits.data
y = digits.target

# Split into training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)

# Create a k-NN classifier with 7 neighbors: knn
knn = KNeighborsClassifier(n_neighbors=7)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Print the accuracy
print(knn.score(X_test, y_test))


# Setup arrays to store train and test accuracies
neighbors = np.arange(1, 9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

# Loop over different values of k
for i, k in enumerate(neighbors):
    # Setup a k-NN Classifier with k neighbors: knn
    knn = KNeighborsClassifier(n_neighbors=k)
    
    # Fit the classifier to the training data
    knn.fit(X_train, y_train)
    
    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(X_train, y_train)
    
    #Compute accuracy on the testing set
    test_accuracy[i] = knn.score(X_test, y_test)

# Generate plot
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
# plt.show()

```
  
  
***
  
Chapter 2 - Regression  
  
Introduction to regression - target variable is a continuous variable (e.g., Boston housing dataset):  
  
* Suppose that there is a pd.DataFrame called "boston" which has target variable "MEDV" and a number of predictors  
	* X = boston.drop("MEDV", axis=1).values  
    * y = boston["MEDV"].values  
    * May need to call .reshape(-1, 1) to convert them for future analysis (???) - seems to convert from .shape (X,) to .shape(X, 1) which is preferred by scikit  
* In general, to run a linear regression using sklearn  
	* from sklearn import linear_model  
    * reg = linear_model.LinearRegression()  
    * reg.fit(X, y)  
  
Basics of linear regression - simple linear regression has the format y = a*x + b:  
  
* The error (loss, cost) function for linear regression is typically the sum-square of the residuals -- OLS (Ordinary Least Squares)  
* The scikit API works the same for simple regression with one feature and regression with multiple features  
* The default .score() for regression in scikit is R-squared  
  
Cross-validation - accounting for model performance being dependent on the way the data is split (not generalizable):  
  
* One CV approach splits the data in to k-folds, using each of the folds as its own hold-put sample  
* Implementing cross-validation in scikit-learn includes  
	* from sklearn.model_selection import cross_val_score  
    * reg = LinearRegression()  
    * cv_results = cross_val_score(reg, X, y, cv=5)  # for 5-fold  
  
Regularized regression - avoiding over-fitting due to large coefficients:  
  
* Regularization involves placing a penalty on large coefficients  
* Ridge regression loss function is standard OLS loss function + alpha * sum-squared(coefficients)  # alpha is sometimes called lambad; when 0, this is the same as OLS  
	* from sklearn.linear_model import Ridge  
    * ridge = Ridge(alpha=0.1, normalize=True) # assuming lambda=0.1, run on standardized scale is desired  
    * ridge.fit(X_train, y_train)  
    * ridge_pred = ridge.predict(X_test)  
    * ridge.score(X_test, y_test)  
* Lasso regression loss function is standard OLS loss function + alpha * sum-absolute-value(coefficients)  
	* Run the same as ridge regression, except that Lasso is used for the import and Lasso() for the initial object setup  
    * Lasso regression is especially useful for feature selection, since it tends to select 0 as the coefficient for less important variables  
  
Example code includes:  
```{r engine='python', engine.path=list(python="C:\\Users\\Dave\\Anaconda3\\python.exe")}

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# Read the CSV file into a DataFrame: df
df = pd.read_csv(myPath + "gapminder_tidy.csv")
df = df.loc[df["Year"] == 2010, :]


# Create arrays for features and target variable
y = df["life"].values
X = df["fertility"].values

# Print the dimensions of X and y before reshaping
print("Dimensions of y before reshaping: {}".format(y.shape))
print("Dimensions of X before reshaping: {}".format(X.shape))

# Reshape X and y
y = y.reshape(-1, 1)
X = X.reshape(-1, 1)

# Print the dimensions of X and y after reshaping
print("Dimensions of y after reshaping: {}".format(y.shape))
print("Dimensions of X after reshaping: {}".format(X.shape))


# Import LinearRegression
from sklearn.linear_model import LinearRegression

# Create the regressor: reg
reg = LinearRegression()
X_fertility = X.copy()

# Create the prediction space
prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)

# Fit the model to the data
reg.fit(X_fertility, y)

# Compute predictions over the prediction space: y_pred
y_pred = reg.predict(prediction_space)

# Print R^2 
print(reg.score(X_fertility, y))

# Plot regression line
plt.plot(prediction_space, y_pred, color='black', linewidth=3)
# plt.show()
plt.clf()


# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)

# Create the regressor: reg_all
reg_all = LinearRegression()

# Fit the regressor to the training data
reg_all.fit(X_train, y_train)

# Predict on the test data: y_pred
y_pred = reg_all.predict(X_test)

# Compute and print R^2 and RMSE
print("R^2: {}".format(reg_all.score(X_test, y_test)))
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Root Mean Squared Error: {}".format(rmse))


# Import the necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Create a linear regression object: reg
reg = LinearRegression()

# Compute 5-fold cross-validation scores: cv_scores
cv_scores = cross_val_score(reg, X, y, cv=5)

# Print the 5-fold cross-validation scores
print(cv_scores)

print("Average 5-Fold CV Score: {}".format(np.mean(cv_scores)))


# Import necessary modules
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

# Create a linear regression object: reg
reg = LinearRegression()

# Perform 3-fold CV
cvscores_3 = cross_val_score(reg, X, y, cv=3)
print(np.mean(cvscores_3))

# Perform 10-fold CV
cvscores_10 = cross_val_score(reg, X, y, cv=10)
print(np.mean(cvscores_10))


# Use mtcars
df = pd.read_csv(myPath + "mtcars.csv", index_col=0)
df = df[["mpg", "disp", "hp", "drat", "wt", "qsec"]]

X = df.drop("mpg", axis=1).values
y = df["mpg"].values.reshape(-1, 1)
df_columns = df.drop("mpg", axis=1).columns

# Import lasso
from sklearn.linear_model import Lasso

# Instantiate a lasso regressor: lasso
lasso = Lasso(alpha=0.4, normalize=True)

# Fit the regressor to the data
lasso.fit(X, y)

# Compute and print the coefficients
lasso_coef = lasso.coef_
print(lasso_coef)

# Plot the coefficients
plt.plot(range(len(df_columns)), lasso_coef)
plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)
plt.margins(0.02)
# plt.show()
plt.clf()


def display_plot(cv_scores, cv_scores_std):
    fig = plt.figure()
    ax = fig.add_subplot(1,1,1)
    ax.plot(alpha_space, cv_scores)
    
    std_error = cv_scores_std / np.sqrt(10)
    
    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)
    ax.set_ylabel('CV Score +/- Std Error')
    ax.set_xlabel('Alpha')
    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')
    ax.set_xlim([alpha_space[0], alpha_space[-1]])
    ax.set_xscale('log')
    # plt.show()
    plt.clf()


# Import necessary modules
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# Setup the array of alphas and lists to store scores
alpha_space = np.logspace(-4, 0, 50)
ridge_scores = []
ridge_scores_std = []

# Create a ridge regressor: ridge
ridge = Ridge(normalize=True)

# Compute scores over range of alphas
for alpha in alpha_space:
    
    # Specify the alpha value to use: ridge.alpha
    ridge.alpha = alpha
    
    # Perform 10-fold CV: ridge_cv_scores
    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)
    
    # Append the mean of ridge_cv_scores to ridge_scores
    ridge_scores.append(np.mean(ridge_cv_scores))
    
    # Append the std of ridge_cv_scores to ridge_scores_std
    ridge_scores_std.append(np.std(ridge_cv_scores))

# Display the plot
display_plot(ridge_scores, ridge_scores_std)

```
  
  
***
  
Chapter 3 - Model Fine-Tuning  
  


	






	
	
	



