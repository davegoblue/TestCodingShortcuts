---
title: "Additional Coding Examples"
author: "davegoblue"
date: "August 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background  
This document explores video poker hands, using <http://wizardofodds.com/games/video-poker/methodology/> as a template, and applying DataCamp_Insights_v001 materials where applicable.  This document builds on the more salient components of the analysis in AdditionalCoding_v001.Rmd.  
  
## Simulations  
### Potential Hand Types  
The card deck will be considered to be 1-52, where 1-13, 14-26, 27-39, and 40-52 shall each be the suits.  Further, the lowest number of each suit shall be considered the Ace, and the highest shall be considered the King.  There are 52c5 (2,598,960) hands applicable to a typical poker game with a 52-card deck.  All of the possible combinations are created and assessed, with caching due to moderate run times:  
```{r, cache=TRUE}
jbHands <- matrix(data=0L, nrow=choose(52, 5), ncol=5)

startTime <- proc.time()
intCtr <- 1
for (i in 1:48) {
    for (j in (i+1):49) {
        for (k in (j+1):50) {
            for (m in (k+1):51) {
                for (n in (m+1):52) {
                               jbHands[intCtr, ] <- c(i, j, k, m, n)
                               intCtr <- intCtr + 1
                }
            }

        }
    }
}
proc.time() - startTime


## Find the flushes
jbFlush <- apply(jbHands, 1, FUN = function(x) { diff(range( (x-1) %/% 13 )) == 0 })

## Find the number of uniques
jbUniques <- apply(jbHands, 1, FUN = function(x) { length(unique(x %% 13)) })

## Find the range, specifically is it exactly 5
foo <- function(x) {
    min( diff(range( (x-1) %% 13 )) , diff(range( (x-2) %% 13 )) )
}
jbRange <- apply(jbHands, 1, FUN = foo)

# Is the maximum number of cards equal to 3 (only for jbUniques %in% c(2, 3))
foo <- function(x) {
    max(table(x %% 13))
}
jbMax <- apply(jbHands[jbUniques %in% c(2, 3), ], 1, FUN=foo)

# Get the totals by card rank
cRank <- matrix(data=0L, nrow=choose(52, 5), ncol=13)
for (intCtr in 1:13) {
    cRank[, intCtr] <- as.integer( rowSums( (jbHands %% 13) == (intCtr %% 13) ) )
}

proc.time() - startTime
```
  
Hand types are then declared, with this portion not cached since 1) it will need further refinement, and 2) it runs quickly (with the exception of the table function, later updated with dplyr/tidyr for speed):  
```{r}
jbType <- rep(-1L, nrow(jbHands))  # Default value is -1

# Manage the pairs - only convert if pair is 1 (Ace) or 11-13 (J, Q, K)
hiPair <- cRank[, 1] == 2 | cRank[, 11] == 2 | cRank[, 12] == 2 | cRank[, 13] == 2
jbType[jbUniques == 4 & hiPair] <- 0

# Manage the straights and flushes, including SF and RF
jbType[jbFlush == FALSE & jbUniques == 5 & jbRange == 4] <- 3  # Straights as 3
jbType[jbFlush == TRUE & jbRange > 4] <- 5  # Flushes as 5

# Declare the straight flush and royal flush
rfVector <- c(1L, 10L, 11L, 12L, 0L)
foo <- function(x) {
    if (sum( x %% 13 == rfVector ) ==5 ) { return(799) }
    else { return(49) }
}
jbType[jbFlush == TRUE & jbRange == 4] <- apply(jbHands[jbFlush == TRUE & jbRange == 4, ], 1, FUN=foo)

# Look at the subset that composes Two Pair, Trips, Full House, and Quads
jbTemp <- jbUniques[jbUniques %in% c(2, 3)]
jbTempValue <- rep(0L, length(jbTemp))

jbTempValue[jbTemp == 3 & jbMax != 3] <- 1  # Two Pair
jbTempValue[jbTemp == 3 & jbMax == 3] <- 2  # Trips
jbTempValue[jbTemp == 2 & jbMax == 3] <- 8  # Full House
jbTempValue[jbTemp == 2 & jbMax != 3] <- 24  # Quads

jbType[jbUniques %in% c(2, 3)] <- jbTempValue


library(dplyr)
library(tidyr)
library(ggplot2)

# Use dplyr and tidyr since they are MUCH faster than table() for the cross-tabs
data.frame(jbType=jbType) %>% group_by(jbType) %>% summarize(count=n())
data.frame(jbType=jbType, jbUniques=jbUniques) %>% 
    group_by(jbType, jbUniques) %>% 
    summarize(count=n()) %>%
    spread(jbUniques, count, fill=0)
data.frame(jbType=jbType, jbRange=jbRange) %>% 
    group_by(jbType, jbRange) %>% 
    summarize(count=n()) %>%
    spread(jbType, count, fill=0)

```
  
Potential refinements will include allowing rank and kickers to impact value of quads (e.g., Bonus or Double Double Bonus).  
  
Sample plots are created using ggplot2 (cached, since they take a while to plot off the large frame):  
```{r, cache=TRUE}
# Practice with ggplot2
library(ggplot2)

graphFrame <- data.frame(jbFlush=as.integer(jbFlush), jbRange=jbRange, 
                         jbType=jbType, jbUniques=jbUniques
                         )

# Bar chart for jbRange x jbUniques
ggplot(graphFrame, aes(x=as.factor(jbRange))) + geom_bar(aes(fill=as.factor(jbUniques)))

# Stacked bar chart for jbRange x jbUniques
ggplot(graphFrame, aes(x=as.factor(jbRange))) + 
    geom_bar(aes(fill=as.factor(jbUniques)), position="fill")

# Bar chart for jbRange x jbUniques, faceted by jbType
ggplot(graphFrame, aes(x=as.factor(jbRange))) + 
    geom_bar(aes(fill=as.factor(jbUniques))) + 
    facet_wrap(~ jbType, ncol=5)

# Stacked bar chart for jbRange x jbUniques, faceted by jbType
ggplot(graphFrame, aes(x=as.factor(jbRange))) + 
    geom_bar(aes(fill=as.factor(jbUniques)), position="fill") + 
    facet_wrap(~ jbType, ncol=5)

```
  
As per the tables, the graphs line up with expectations.  
  
Next, the program takes a stab at implementing the algorithm described by the Wizard of Odds at <http://wizardofodds.com/games/video-poker/methodology/> for vastly reducing run times for a program of this type.  The Wizard recommends an 11-step process, including:  
  
1.  Initialize array 0 (2,598,960) for the result of a single dealt hand with 0 draws, array 1 (270,725 x 16) for a 1-card draw, array 2 (22,100 x 16) for a 2-card draw, array 3 (1,326 x 16) for a 3-card draw, array 4 (52 x 16) for a 4-card draw, and array 5 (1 x 16) for a 5-card draw.  The 16 is for the maximum number of outcomes (e.g., -1 or +799) for the hand, and could be expanded if a game had more than 16 paying types.  
  
2.  Loop through every possible hand (these are the jbHands and jbType in this algorithm), and populate array0, array1, array2, array3, array4, and array5 based on possible permutations of cards held on the draw  
  
First, the 16-hand condition is tested, and the relevant arrays are set up:
```{r}

nrowArray0 <- data.frame(jbType=jbType) %>% 
    group_by(jbType) %>% 
    summarize(ct=n()) %>%
    arrange(jbType)
nrowArray0

if (nrow(nrowArray0) > 16) { 
    stop(paste0("There are too many hand types, cap is 16, this will have: ", nrow(nrowArray0)))
}
print("OK")

# Initialize array1-array5
array1 <- matrix(data=0L, nrow=choose(52, 4), ncol=16)
array2 <- matrix(data=0L, nrow=choose(52, 3), ncol=16)
array3 <- matrix(data=0L, nrow=choose(52, 2), ncol=16)
array4 <- matrix(data=0L, nrow=choose(52, 1), ncol=16)
array5 <- matrix(data=0L, nrow=choose(52, 0), ncol=16)

# Create array0 which will have an index number corresponding to the associated jbType
array0 <- match(jbType, nrowArray0[[1]])

data.frame(array0=array0) %>% 
    group_by(array0) %>%
    summarize(ct=n()) %>%
    arrange(array0)

```
  
One area for exploration is to associate indices to each combination of cards, start with an nCm array as per <http://wizardofvegas.com/forum/questions-and-answers/math/13687-my-methodology-for-video-poker-analysis-article-question/>.  
```{r}
mtxCombin <- matrix(data=0L, nrow=52, ncol=5)
for (intCtr in 1:52) {
    for (intCtr2 in 1:5) {
        # Note that choose() is a guarded function where choose(n, k) returns 0 for k > n
        mtxCombin[intCtr, intCtr2] <- choose(intCtr, intCtr2)
    }
}
```
  
The array can then be used to convert any 2 cards to an index, as follows:  
```{r}
idxCard2 <- function(c1, c2) {
    # Need to convert the C++ algorithm which is 0:51 to R which wants 1:52
    1 + mtxCombin[52, 2] - mtxCombin[53-c1, 2] +
        mtxCombin[52-c1, 1] - mtxCombin[53-c2, 1]
}

# Test the index creation process
mtxA <- matrix(data=0L, nrow=choose(52, 2), ncol=3)
curRow <- 1
for (intCtr in 1:51) {
    for (intCtr2 in (intCtr+1):52) {
        mtxA[curRow, 1:3] <- c(intCtr, intCtr2, idxCard2(c1=intCtr, c2=intCtr2))
        curRow <- curRow + 1
    }
}

# Make sure it worked OK!
head(mtxA)
tail(mtxA)
length(mtxA[, 3]) == length(unique(mtxA[, 3]))
range(mtxA[, 3]) == c(1, choose(52, 2))
```
  
The array can also be used to convert any 3 cards to an index, as follows:  
```{r}
idxCard3 <- function(c1, c2, c3) {
    # Need to convert the C++ algorithm which is 0:51 to R which wants 1:52
    1 + mtxCombin[52, 3] - mtxCombin[53-c1, 3] +
        mtxCombin[52-c1, 2] - mtxCombin[53-c2, 2] +
        mtxCombin[52-c2, 1] - mtxCombin[53-c3, 1]
}

# Test the index creation process
mtxA <- matrix(data=0L, nrow=choose(52, 3), ncol=4)
curRow <- 1
for (intCtr in 1:50) {
    for (intCtr2 in (intCtr+1):51) {
        for (intCtr3 in (intCtr2+1):52) {
            mtxA[curRow, 1:4] <- c(intCtr, intCtr2, intCtr3, 
                                   idxCard3(c1=intCtr, c2=intCtr2, c3=intCtr3)
                                   )
            curRow <- curRow + 1
        }
    }
}

# Make sure it worked OK!
head(mtxA)
tail(mtxA)
length(mtxA[, 4]) == length(unique(mtxA[, 4]))
range(mtxA[, 4]) == c(1, choose(52, 3))
```
  
And, the same algorithm can be used for 4 cards, an area which should save substantial time:  
```{r}
idxCard4 <- function(c1, c2, c3, c4) {
    # Need to convert the C++ algorithm which is 0:51 to R which wants 1:52
    1 + mtxCombin[52, 4] - mtxCombin[53-c1, 4] +
        mtxCombin[52-c1, 3] - mtxCombin[53-c2, 3] +
        mtxCombin[52-c2, 2] - mtxCombin[53-c3, 2] + 
        mtxCombin[52-c3, 1] - mtxCombin[53-c4, 1]
}

# Test the index creation process
mtxA <- matrix(data=0L, nrow=choose(52, 4), ncol=5)
curRow <- 1
for (intCtr in 1:49) {
    for (intCtr2 in (intCtr+1):50) {
        for (intCtr3 in (intCtr2+1):51) {
            for (intCtr4 in (intCtr3+1):52) {
                mtxA[curRow, 1:5] <- c(intCtr, intCtr2, intCtr3, intCtr4, 
                                       idxCard4(c1=intCtr, c2=intCtr2, c3=intCtr3, c4=intCtr4)
                                       )
                curRow <- curRow + 1
            }
        }
    }
}

# Make sure it worked OK!
head(mtxA)
tail(mtxA)
length(mtxA[, 5]) == length(unique(mtxA[, 5]))
range(mtxA[, 5]) == c(1, choose(52, 4))
```
  
These functions should simplify and speed-up the conversion of any given 5-card hand to a score if it is instead held as a 0-4 card hand.  

The algorithms are then tested against jbHands (the 2,598,960 x 5 data frame representing all possible hands) and its associated array0 (the 2,598,960 x 1 integer vector representing the hand value as a 1-10).  
```{r}

# The array for keeping everything (jbHands as 2,598,960 x 5 for the cards, 
# and array0 as 2,598,960 x 1 for the outcomes) is already populated
startTime <- proc.time()

dim(jbHands)
length(array0)
data.frame(array0=array0) %>% 
    group_by(array0) %>% 
    summarize(ct=n()) %>% 
    arrange(-array0)

proc.time() - startTime


# Fill the array (array5 as 1 x 16) for keeping nothing
startTime <- proc.time()

a <- data.frame(val=array0) %>% group_by(val) %>% 
    summarize(ct=n()) %>% arrange(val)

for ( intCtr in seq_along(unique(a$val)) ) {
    array5[1, intCtr] <- a$ct[a$val == intCtr]
}

proc.time() - startTime


# Test keeping a single card, and fill the array (array4 as 52 x 16) for keeping 1
startTime <- proc.time()

idx <- integer(0)
val <- integer(0)
ct <- integer(0)
for (c1 in 1:5) {
    temp <- data.frame(card1=jbHands[, c1], val=array0) %>% 
        group_by(card1, val) %>%
        summarize(ct=n()) %>%
        arrange(card1, val)
    
    temp$idx <- temp$card1
    
    idx <- c(idx, temp$idx)
    val <- c(val, temp$val)
    ct <- c(ct, temp$ct)
    
}

a <- data.frame(idx=idx, val=val, ct=ct) %>%
    group_by(idx, val) %>% summarize(ct=sum(ct))

for ( intCtr in seq_along(unique(a$val)) ) {
    array4[a$idx[a$val == intCtr], intCtr] <- a$ct[a$val == intCtr]
}

proc.time() - startTime


# Test keeping two cards, and fill the array (array3 as 1,326 x 16) for keeping two cards
startTime <- proc.time()

idx <- integer(0)
val <- integer(0)
ct <- integer(0)
for (c1 in 1:4) {
    for (c2 in (c1+1):5) {
        temp <- data.frame(card1=jbHands[, c1], card2=jbHands[, c2], val=array0) %>% 
            group_by(card1, card2, val) %>%
            summarize(ct=n()) %>%
            arrange(card1, card2, val)
    
        temp$idx <- idxCard2(c1=temp$card1, c2=temp$card2)
    
        idx <- c(idx, temp$idx)
        val <- c(val, temp$val)
        ct <- c(ct, temp$ct)
    
    }
}

a <- data.frame(idx=idx, val=val, ct=ct) %>%
    group_by(idx, val) %>% summarize(ct=sum(ct))

for ( intCtr in seq_along(unique(a$val)) ) {
    array3[a$idx[a$val == intCtr], intCtr] <- a$ct[a$val == intCtr]
}

proc.time() - startTime


# Test keeping three cards, and fill the array (array2 as 22,100 x 16) for keeping three cards
startTime <- proc.time()

idx <- integer(0)
val <- integer(0)
ct <- integer(0)
for (c1 in 1:3) {
    for (c2 in (c1+1):4) {
        for (c3 in (c2+1):5) {
            temp <- data.frame(card1=jbHands[, c1], card2=jbHands[, c2], 
                               card3=jbHands[, c3], val=array0
                               ) %>% 
                group_by(card1, card2, card3, val) %>%
                summarize(ct=n()) %>%
                arrange(card1, card2, card3, val)
    
            temp$idx <- idxCard3(c1=temp$card1, c2=temp$card2, c3=temp$card3)
    
            idx <- c(idx, temp$idx)
            val <- c(val, temp$val)
            ct <- c(ct, temp$ct)
    
        }
    }
}

a <- data.frame(idx=idx, val=val, ct=ct) %>%
    group_by(idx, val) %>% summarize(ct=sum(ct))

for ( intCtr in seq_along(unique(a$val)) ) {
    array2[a$idx[a$val == intCtr], intCtr] <- a$ct[a$val == intCtr]
}

proc.time() - startTime


# Test keeping four cards, and fill the array (array1 as 270,725 x 16) for keeping four cards
startTime <- proc.time()

idx <- integer(0)
val <- integer(0)
ct <- integer(0)
for (c1 in 1:2) {
    for (c2 in (c1+1):3) {
        for (c3 in (c2+1):4) {
            for (c4 in (c3+1):5) {
                temp <- data.frame(card1=jbHands[, c1], card2=jbHands[, c2], 
                                   card3=jbHands[, c3], card4=jbHands[, c4], 
                                   val=array0
                                   ) %>% 
                    group_by(card1, card2, card3, card4, val) %>%
                    summarize(ct=n()) %>%
                    arrange(card1, card2, card3, card4, val)
    
                temp$idx <- idxCard4(c1=temp$card1, c2=temp$card2, 
                                     c3=temp$card3, c4=temp$card4
                                     )
    
                idx <- c(idx, temp$idx)
                val <- c(val, temp$val)
                ct <- c(ct, temp$ct)
            
            }
        }
    }
}

a <- data.frame(idx=idx, val=val, ct=ct) %>%
    group_by(idx, val) %>% summarize(ct=sum(ct))

for ( intCtr in seq_along(unique(a$val)) ) {
    array1[a$idx[a$val == intCtr], intCtr] <- a$ct[a$val == intCtr]
}

proc.time() - startTime

```
  
While this is still running an order of magnitude slower than the Wizard of Odds methodology, it is encouraging that the "order of magnitude" is 30 seconds vs. 3 seconds.  There may be room for further streamlining, but the code appears to be functional and with a reasonable run-time for next steps.
  
The dimensions and counts of the various arrays are then confirmed:  
```{r}
# The actual arrays (array1-array5)
sapply(list(array5=array5, array4=array4, array3=array3, array2=array2, array1=array1), 
       FUN=function(x) {c(sum(x)/choose(52, 5), sum(rowSums(x) > 0), sum(colSums(x) > 0)) } 
       )

# The original array
length(array0)/choose(52, 5)
nrow(jbHands)
length(unique(array0))
```
  
The arrays are all of the proper dimensions and counts.  Next, a process is built to look at a subset of hands and to evaluate each of the potential options assuming cards CAN be replaced (the next step will be to reverse this assumption):  
```{r}
set.seed(1609010715)

# Take a smaller sample of the hands
# keyHands <- sort(sample(nrow(jbHands), 400000, replace=FALSE))
keyHands <- 1:nrow(jbHands)
jbSmall <- jbHands[keyHands, ]


startTime <- proc.time()

# Convert each of the arrays to a value
mapArray <- matrix(data=c(-1, 0, 1, 2, 3, 5, 8, 24, 49, 799, 0, 0, 0, 0, 0, 0), ncol=1)
dim(mapArray)

expVal5 <- (array5 %*% mapArray) / choose(52, 5)
expVal4 <- (array4 %*% mapArray) / choose(51, 4)
expVal3 <- (array3 %*% mapArray) / choose(50, 3)
expVal2 <- (array2 %*% mapArray) / choose(49, 2)
expVal1 <- (array1 %*% mapArray) / choose(48, 1)
expVal0 <- matrix(data=mapArray[array0, 1], ncol=1)

# Evaluate all the draw-0 options
draw0 <- matrix(data=expVal0[keyHands, ], ncol=1)

# Evaluate all the draw-5 options
draw5 <- matrix(data=expVal5, nrow=nrow(jbSmall), ncol=1)

# Evaluate all the draw-4 options
keyData <- integer(0)
for (intCtr in 1:5) {
    keyData <- c(keyData, expVal4[jbSmall[, intCtr], 1, drop=TRUE])
}
draw4 <- matrix(data=keyData, ncol=5, byrow=FALSE)

# Evaluate all the draw-3 options
keyData <- integer(0)
for (intCtr in 1:4) {
    for (intCtr2 in (intCtr+1):5) {
        keyData <- c(keyData, 
                     expVal3[idxCard2( c1=jbSmall[ , intCtr], c2=jbSmall[ , intCtr2] ), 
                             1, drop=TRUE]
                     )
    }
}
draw3 <- matrix(data=keyData, ncol=10, byrow=FALSE)

# Evaluate all the draw-2 options
keyData <- integer(0)
for (intCtr in 1:3) {
    for (intCtr2 in (intCtr+1):4) {
        for (intCtr3 in (intCtr2+1):5) {
            keyData <- c(keyData, 
                         expVal2[idxCard3( c1=jbSmall[, intCtr], c2=jbSmall[, intCtr2] ,
                                           c3=jbSmall[, intCtr3]
                                          ), 1, drop=TRUE]
                         )
        }
    }
}
draw2 <- matrix(data=keyData, ncol=10, byrow=FALSE)

# Evaluate all the draw-1 options
keyData <- integer(0)
for (intCtr in 1:2) {
    for (intCtr2 in (intCtr+1):3) {
        for (intCtr3 in (intCtr2+1):4) {
            for (intCtr4 in (intCtr3+1):5) {
                keyData <- c(keyData, 
                             expVal1[idxCard4( c1=jbSmall[, intCtr], c2=jbSmall[, intCtr2] ,
                                               c3=jbSmall[, intCtr3], c4=jbSmall[, intCtr4]
                                              ), 1, drop=TRUE]
                            )
            }
        }
    }
}
draw1 <- matrix(data=keyData, ncol=5, byrow=FALSE)

# Integrate the drawing summaries
cmbDraw <- cbind(draw0, draw1, draw2, draw3, draw4, draw5)

# Assess the best option by row
cmbBestIdx <- apply(cmbDraw, 1, FUN=which.max)
hist(cmbBestIdx, breaks=0:33, col=c( rep("blue", 1), rep("lightblue", 5), rep("orange", 10),
                                     rep("lightgreen", 10), rep("red", 5), rep("black", 1) ) )


# Assess the best value by row
cmbBestVal <- apply(cmbDraw, 1, FUN=max)
summary(cmbBestVal)


proc.time() - startTime

```
  
While still an order of magnitude slower than the reference code, this runs through all the options in 30 seconds.  
  
Next, the draw-one situation is assessed assuming that you do NOT sample with replacement, which is to say that you will never get back the card you discarded on the redraw:  
```{r}

# Evaluate all the draw-1 options
keyData <- integer(0)
for (intCtr in 1:2) {
    for (intCtr2 in (intCtr+1):3) {
        for (intCtr3 in (intCtr2+1):4) {
            for (intCtr4 in (intCtr3+1):5) {
                newEV <- expVal1[idxCard4( c1=jbSmall[, intCtr], c2=jbSmall[, intCtr2] ,
                                               c3=jbSmall[, intCtr3], c4=jbSmall[, intCtr4]
                                           ), 1, drop=TRUE]
                newEV <- (48 * newEV - expVal0[, 1, drop=TRUE]) / 47
                str(newEV)
                keyData <- c(keyData, newEV)
            }
        }
    }
}
newDraw1 <- matrix(data=keyData, ncol=5, byrow=FALSE)


# Assess the best option by row
newBestIdx <- apply(cbind(cmbDraw[, 1, drop=FALSE], newDraw1, cmbDraw[, 7:32]), 1, FUN=which.max)
hist(newBestIdx, breaks=0:33, col=c( rep("blue", 1), rep("lightblue", 5), rep("orange", 10),
                                     rep("lightgreen", 10), rep("red", 5), rep("black", 1) ) )

# Assess the best value by row
newBestVal <- apply(cbind(cmbDraw[, 1, drop=FALSE], newDraw1, cmbDraw[, 7:32]), 1, FUN=max)
summary(newBestVal)

# Summarize the changes in index
sum(newBestIdx != cmbBestIdx)
data.frame(oldIdx=cmbBestIdx, newIdx=newBestIdx) %>% 
    group_by(oldIdx, newIdx) %>% summarize(ct=n()) %>% 
    ggplot(aes(x=oldIdx, y=newIdx)) + geom_point(aes(size=ct), col="blue")

# Summarize the changes in value
sum(newBestVal != cmbBestVal)
sum(newBestVal > cmbBestVal)
sum(newBestVal < cmbBestVal)

hist((newBestVal - cmbBestVal)[newBestVal != cmbBestVal], col="lightblue")
temp <- data.frame(newIdx=newBestIdx, newVal=newBestVal, oldVal=cmbBestVal) %>%
        mutate(deltaVal = ((newVal - oldVal) != 0), changeVal=(newVal - oldVal)) %>% 
        group_by(newIdx) %>% 
        summarize(nHands=n(), nChg=sum(deltaVal), totChg=sum(changeVal), 
                  maxChg=max(changeVal), minChg=min(changeVal), meanChg=mean(changeVal)
                  )
print(as.data.frame(temp))
```
  
The results are as expected, including:  
  
* Some hands that previously had an optimal play other than draw 1 now fall under draw 1  
* All hands that now optimize as draw1 have a positively changed EV, averaging in the 1%-2% range  
  
Next, the draw-two situation is assessed assuming that you do NOT sample with replacement, which is to say that you will never get back either of the cards you discarded on the redraw.  Essentially, this requires deleting the EV of each of the 4-card combinations, then adding back the EV of the 5-card combination (which is double-deleted, once in each of the 4-card deletions):  
```{r}
keyData <- integer(0)
for (intCtr in 1:3) {
    for (intCtr2 in (intCtr+1):4) {
        for (intCtr3 in (intCtr2+1):5) {
            
            basEV <- expVal2[idxCard3( c1=jbSmall[, intCtr], c2=jbSmall[, intCtr2] , 
                                       c3=jbSmall[, intCtr3]
                                       ), 1, drop=TRUE]
            
            keyA <- (1:5)[-c(intCtr, intCtr2, intCtr3)][1]
            keyB <- (1:5)[-c(intCtr, intCtr2, intCtr3)][2]
            
            sortA <- sort(c(intCtr, intCtr2, intCtr3, keyA))
            sortB <- sort(c(intCtr, intCtr2, intCtr3, keyB))
            
            expA <- expVal1[idxCard4( c1=jbSmall[, sortA[1]], c2=jbSmall[, sortA[2]], 
                                      c3=jbSmall[, sortA[3]], c4=jbSmall[, sortA[4]]
                                      ), 1, drop=TRUE]
            
            expB <- expVal1[idxCard4( c1=jbSmall[, sortB[1]], c2=jbSmall[, sortB[2]], 
                                      c3=jbSmall[, sortB[3]], c4=jbSmall[, sortB[4]]
                                      ), 1, drop=TRUE]
            
            str(basEV)
            str(expA)
            str(expB)
            
            newEV <- (1176 * basEV - 48 * expA - 48 * expB + expVal0[, 1, drop=TRUE]) / 1081
            str(newEV)
            
            keyData <- c(keyData, newEV)
        }
    }
}
newDraw2 <- matrix(data=keyData, ncol=10, byrow=FALSE)

# Assess the best option by row (adding in only the changed 2-card draws, not the 1-card from above)
newBestIdx <- apply(cbind(cmbDraw[, 1:6, drop=FALSE], newDraw2, cmbDraw[, 17:32]), 1, FUN=which.max)
hist(newBestIdx, breaks=0:33, col=c( rep("blue", 1), rep("lightblue", 5), rep("orange", 10),
                                     rep("lightgreen", 10), rep("red", 5), rep("black", 1) ) )

# Assess the best value by row (adding in only the changed 2-card draws)
newBestVal <- apply(cbind(cmbDraw[, 1:6, drop=FALSE], newDraw2, cmbDraw[, 17:32]), 1, FUN=max)
summary(newBestVal)

# Summarize the changes in index
sum(newBestIdx != cmbBestIdx)
data.frame(oldIdx=cmbBestIdx, newIdx=newBestIdx) %>% 
    group_by(oldIdx, newIdx) %>% summarize(ct=n()) %>% 
    ggplot(aes(x=oldIdx, y=newIdx)) + geom_point(aes(size=ct), col="blue")

# Summarize the changes in value
sum(newBestVal != cmbBestVal)
sum(newBestVal > cmbBestVal)
sum(newBestVal < cmbBestVal)

hist((newBestVal - cmbBestVal)[newBestVal != cmbBestVal], col="lightblue")
temp <- data.frame(newIdx=newBestIdx, newVal=newBestVal, oldVal=cmbBestVal) %>%
        mutate(deltaVal = ((newVal - oldVal) != 0), changeVal=(newVal - oldVal)) %>% 
        group_by(newIdx) %>% 
        summarize(nHands=n(), nChg=sum(deltaVal), totChg=sum(changeVal), 
                  maxChg=max(changeVal), minChg=min(changeVal), meanChg=mean(changeVal)
                  )
print(as.data.frame(temp))

```

The results are as expected, including:  
  
* Some hands that previously had an optimal play other than draw 2 now fall under draw 2  
* All hands that now optimize as draw-2 have a positively changed EV, averaging a gain of ~3%  
  