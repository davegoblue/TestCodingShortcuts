---
title: "Integrate Download and EDA Functions"
author: "davegoblue"
date: "6/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background  
[METAR](https://en.wikipedia.org/wiki/METAR) are hourly weather data collected at airports, including  visibility, wind, temperature, dew point, precipitation, clouds, barometric pressure, and other features that may impact safe aviation.

Iowa State University has a great database of archived METAR data, stored in a manner that makes for easy, automated downloads in CSV format.

The files 'WeatherDownload_202005_v002.Rmd' and 'WeatherEDA_202005_v002.Rmd' contain functions for downloading weather files from the Iowa State server, running EDA, applying a constant format, and saving for further analysis.

The intent of this script is to take the key download and EDA functions, place them in a source file, and then run them as needed to output additional data.
  
#### _Sourcing Files_  
Prior to sourcing functions, here are a few formatting helpers that are included here:  
```{r}

# The functions sourced from the scripts use tidyverse and lubridate frequently
library(tidyverse)
library(lubridate)


# Create a regex search string for METAR (the hourly time such as 53Z is pre-pended)
genMET <- ".*?(VRB|\\d{3})(\\d{2})(G\\d{2})?KT(.*?)(\\d{1,2}SM).*?\\s(M?\\d{2})/(M?\\d{2}).*?(A\\d{4}).*?RMK.*?(SLP\\d{3}).*?(T\\d{8})"

# Create the variable names that the above regex parses in to
labsMET <- c("METAR", "WindDir", "WindSpeed", "WindGust", "Dummy", "Visibility", 
             "TempC", "DewC", "Altimeter", "SLP", "FahrC"
             )

# Expected columns for the downloaded METAR files
metType <- readr::cols(station=readr::col_character(), 
                       valid=readr::col_datetime(), 
                       tmpf=readr::col_double(),
                       dwpf=readr::col_double(),
                       relh=readr::col_double(),
                       drct=readr::col_double(),
                       sknt=readr::col_double(),
                       p01i=readr::col_character(),  # needs to handle 'T' for trace
                       alti=readr::col_double(),
                       mslp=readr::col_double(),
                       vsby=readr::col_double(),
                       gust=readr::col_double(),
                       skyc1=readr::col_character(),
                       skyc2=readr::col_character(), 
                       skyc3=readr::col_character(), 
                       skyc4=readr::col_character(),
                       skyl1=readr::col_double(),
                       skyl2=readr::col_double(),
                       skyl3=readr::col_double(),
                       skyl4=readr::col_double(),
                       wxcodes=readr::col_character(),
                       ice_accretion_1hr=readr::col_character(), # needs to handle 'T' for trace
                       ice_accretion_3hr=readr::col_character(), # needs to handle 'T' for trace
                       ice_accretion_6hr=readr::col_character(), # needs to handle 'T' for trace
                       peak_wind_gust=readr::col_double(),
                       peak_wind_drct=readr::col_double(),
                       peak_wind_time=readr::col_datetime(),
                       feel=readr::col_double(),
                       metar=readr::col_character()
                       )

# The main path for the files
filePath <- "./RInputFiles/ProcessedMETAR/"

# Descriptive names for key variables
varMapper <- c(WindDir="Wind Direction (degrees)", 
               predomDir="General Prevailing Wind Direction",
               WindSpeed="Wind Speed (kts)",
               WindSpeed5="Wind Speed (kts), rounded to nearest 5 knots", 
               Visibility="Visibility (SM)", 
               TempC="Temperature (C)", 
               DewC="Dew Point (C)", 
               Altimeter="Altimeter (inches Hg)",
               Altimeter10="Altimeter (inches Hg), rounded to nearest 0.1 inHg", 
               modSLP="Sea-Level Pressure (hPa)", 
               TempF="Temperature (F)",
               DewF="Dew Point (F)", 
               TempF5="Temperature (F), rounded to nearest 5 degrees",
               DewF5="Dew Point (F), rounded to nearest 5 degrees", 
               cType1="First Cloud Layer Type", 
               cLevel1="First Cloud Layer Height (ft)",
               month="Month", 
               year="Year",
               wType="Greatest Sky Obscuration", 
               day="Day of Month"
               )

# File name to city name mapper
cityNameMapper <- c(katl_2016="Atlanta, GA (2016)",
                    kbos_2016="Boston, MA (2016)", 
                    kdca_2016="Washington, DC (2016)", 
                    kden_2016="Denver, CO (2016)", 
                    kdfw_2016="Dallas, TX (2016)", 
                    kdtw_2016="Detroit, MI (2016)", 
                    kewr_2016="Newark, NJ (2016)",
                    kgrb_2016="Green Bay, WI (2016)",
                    kgrr_2016="Grand Rapids, MI (2016)",
                    kiah_2016="Houston, TX (2016)",
                    kind_2016="Indianapolis, IN (2016)",
                    klas_2015="Las Vegas, NV (2015)",
                    klas_2016="Las Vegas, NV (2016)", 
                    klas_2017="Las Vegas, NV (2017)", 
                    klas_2018="Las Vegas, NV (2018)",
                    klas_2019="Las Vegas, NV (2019)",
                    klax_2016="Los Angeles, CA (2016)", 
                    klnk_2016="Lincoln, NE (2016)",
                    kmia_2016="Miami, FL (2016)", 
                    kmke_2016="Milwaukee, WI (2016)",
                    kmsn_2016="Madison, WI (2016)",
                    kmsp_2016="Minneapolis, MN (2016)",
                    kmsy_2015="New Orleans, LA (2015)",
                    kmsy_2016="New Orleans, LA (2016)", 
                    kmsy_2017="New Orleans, LA (2017)", 
                    kord_2014="Chicago, IL (2014)",
                    kord_2015="Chicago, IL (2015)",
                    kord_2016="Chicago, IL (2016)", 
                    kord_2017="Chicago, IL (2017)", 
                    kord_2018="Chicago, IL (2018)",
                    kphl_2016="Philadelphia, PA (2016)", 
                    kphx_2016="Phoenix, AZ (2016)", 
                    ksan_2015="San Diego, CA (2015)",
                    ksan_2016="San Diego, CA (2016)",
                    ksan_2017="San Diego, CA (2017)",
                    ksat_2016="San Antonio, TX (2016)", 
                    ksea_2016="Seattle, WA (2016)", 
                    ksfo_2016="San Francisco, CA (2016)", 
                    ksjc_2016="San Jose, CA (2016)",
                    kstl_2016="Saint Louis, MO (2016)", 
                    ktpa_2016="Tampa Bay, FL (2016)", 
                    ktvc_2016="Traverse City, MI (2016)"
                    )

# Map of variable to chart name
mapChartVar <- c(p1Inches="1-hr Precip", 
                 p36Inches="3-hr or 6-hr Precip", 
                 p24Inches="24-hr Precip"
                 )

```
  
The relevant functions are stored in 'WeatherDownloadFunctions_v001.R' and 'WeatherEDAFunctions_v001.R'.  They are now sourced here:  
```{r}

# Functions for downloading and saving a weather file
source("./WeatherDownloadFunctions_v001.R")
source("./WeatherEDAFunctions_v001.R")

```
  
Remaining cities from top 20 MSA are downloaded for 2016:  
```{r}

# STEP 0: Define a named list of the stations and years for downloading
runFiles <- list(bos=c(2016), 
                 sfo=c(2016),
                 sea=c(2016),
                 tpa=c(2016),
                 den=c(2016),
                 stl=c(2016)
                 )

```
  
```{r cache=TRUE}

cat("\nRunning downloads by station for:\n")
sapply(runFiles, FUN=function(x) length(x))

# STEP 1: Download the raw data files from Iowa State
for (station in names(runFiles)) {
    for (year in runFiles[[station]]) {
        getASOSStationTime(stationID=str_to_upper(station), analysisYears=year, ovrWrite=TRUE)
    }
}

```
  
```{r cache=TRUE}

cat("\nRunning main EDA process on key files\n")

# STEP 2: Process and format the raw data files, and save as .rds
for (station in names(runFiles)) {
    for (year in runFiles[[station]]) {
    
        coreString <- paste0("metar_k", station, "_", as.character(year))
    
        cat("\nProcessing Airport:", station, "for year:", year)
        integrateProcessingMETAR(paste0("./RInputFiles/", coreString, ".txt"), 
                                 startDay=paste0(as.character(year-1), "-12-31"), 
                                 endDay=paste0(as.character(year+1), "-01-01"), 
                                 genMET=genMET, 
                                 labsMET=labsMET, 
                                 saveLoc=paste0("./RInputFiles/ProcessedMETAR/", coreString, ".rds"), 
                                 ovrWrite=TRUE,
                                 colTypes=metType, 
                                 logFile=paste0("./RInputFiles/ProcessedMETAR/", coreString, ".log"),
                                 useNAforIncomplete=TRUE,
                                 showSLPGraph=FALSE
                                 )
        cat("\nFinished Processing Airport:", station, "in year:", year,"\n")
        
    }
}

```
  
Only the components of the EDA that are essential to checking the data and creating an output file for analysis are included.

Base EDA is run, with log files produced (pulls in a number of files previously downloaded, as well as the files recently downloaded):  
```{r cache=TRUE}

# Update the runFiles list for the stations to be run here
runFiles <- list(ord=c(2014, 2018), 
                 las=c(2018, 2019),
                 atl=c(2016),
                 bos=c(2016),
                 dca=c(2016),
                 den=c(2016),
                 dfw=c(2016),
                 lax=c(2016),
                 mia=c(2016),
                 phl=c(2016),
                 phx=c(2016),
                 sat=c(2016),
                 sea=c(2016),
                 sfo=c(2016),
                 sjc=c(2016),
                 stl=c(2016),
                 tpa=c(2016)
                 )

# STEP 3a: Create the list of files to be run, process EDA, and output log files
fileNames <- character(0)
for (station in names(runFiles)) {
    for (year in runFiles[[station]]) {
        fileNames <- c(fileNames, paste0("k", station, "_", as.character(year)))
    }
}

cat("\nEDA process will be run for all of:\n\n", paste0(fileNames, collapse="\n"), "\n", sep="")

for (fName in fileNames) {
    assign(fName, logAndPDFCombinedEDA(fName))
}

```
  
```{r cache=TRUE}

cat("\nRunning the rain data\n")

# NEED TO FIX for Chicago 2014 rainfall - time recorded as 013 should be 0013
# Should fix function fnPrecip1-fnPrecip6 to handle this better
if (exists("kord_2014")) {
    kord_2014 <- kord_2014 %>%
        mutate(origMETAR=str_replace(origMETAR, pattern="RAB013 ", replacement="RAB0013 "))
}

# Run for rain, with logs and pdf sent to files
rain_List <- wrapPrecipTimes(fileNames, 
                             pType="(?<!FZ)RA", 
                             pExt="_RA", 
                             pTypeName="rain", 
                             writeLogFile="rain_extra0625_IntervalTimes.log",
                             writeLogPDF="rain_extra0625_IntervalTimes.pdf",
                             writeLogPath=filePath,
                             appendWriteFile=FALSE
                             )

```
  
```{r cache=TRUE}

cat("\nRunning the snow data\n")

# Run for snow, with logs and pdf sent to files
snow_List <- wrapPrecipTimes(fileNames, 
                             pType="(?<!BL)SN", 
                             pExt="_SN", 
                             pTypeName="snow", 
                             writeLogFile="snow_extra0625_IntervalTimes.log",
                             writeLogPDF="snow_extra0625_IntervalTimes.pdf",
                             writeLogPath=filePath,
                             appendWriteFile=FALSE
                             )

```
  
```{r cache=TRUE}

cat("\nRunning the thunder data\n")

# NEED TO FIX for Saint Louis 2016 thunder - time recorded as TSE57 should be TSE0757
# Should fix function fnPrecip1-fnPrecip6 to handle this better
if (exists("kstl_2016")) {
    kstl_2016 <- kstl_2016 %>%
        mutate(origMETAR=str_replace(origMETAR, pattern="TSE57 ", replacement="TSE0757 "))
}

# Run for thunder, with logs and pdf sent to files
thunder_List <- wrapPrecipTimes(fileNames, 
                                pType="(?<!VC)TS", 
                                pExt="_TS", 
                                pTypeName="thunder", 
                                writeLogFile="thunder_extra0625_IntervalTimes.log",
                                writeLogPDF="thunder_extra0625_IntervalTimes.pdf",
                                writeLogPath=filePath,
                                appendWriteFile=FALSE
                                )

```
  
Daily high-low temperatures and precipitation summaries are extracted, an integrated file is produced, and a predominant wind direction is added:  
```{r cache=TRUE}

# Function created to extract daily high-low temperatures
extractDailyHighLow <- function(x, f=getDailyHighLow, mapper=cityNameMapper, makePDF=NULL) {
    
    # FUNCTION ARGUMENTS:
    # x: a list of file names as a character string
    # f: the function to be called (getDailyHighLow was written for this purpose)
    # mapper: named vector for mapping file name to locale name
    # makePDF: the desired name of the PDF output (NULL means no output)

    # Extract high-low temperatures
    all_hilo <- map_dfr(.x=x, .f=f, .id="source") %>%
        mutate(source=x[as.integer(source)], locale=mapper[source]) %>%
        select(source, locale, everything())

    # Create the plot if makePDF is provided
    if (!is.null(makePDF)) {
        plotDailyHiLo(all_hilo, pdfName=makePDF)
    }
    
    # Return the high-low temperatures object
    all_hilo

}

# Get the high-low data for every file in fileNames
all_hilo <- extractDailyHighLow(fileNames)


extractDailyPrecipitation <- function(x, 
                                      f=getPrecipAmount, 
                                      cityMapper=cityNameMapper, 
                                      chartMapper=mapChartVar,
                                      makePDF=NULL
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # x: character vector of file names for getting daily precipitation
    # f: function for getting daily precipitation (getPrecipAmount was written for this)
    # cityMapper: mapping function for file name to locale name
    # chartMapper: mapping function for chart category to readable name
    # makePDF: file name for PDF output (NULL means do not create)
    
    # Extracted precipitation amounts
    all_pin <- map_dfr(.x=x, .f=f, .id="source") %>%
        mutate(source=x[as.integer(source)], locale=cityMapper[source]) %>%
        select(source, locale, everything())

    # Generate PDF for precipitation plots if requested
    if (!is.null(makePDF)) {
        plotPrecipNAHours(all_pin, mapper=chartMapper, pdfName=makePDF)
    }

    # Plot the time periods provided in chartNameMapper
    for (plotVar in names(chartMapper)) {
        # Counts of !is.na by hour
        p1 <- all_pin %>%
            mutate(hour=factor(lubridate::hour(lubridate::round_date(dtime, unit="hours"))), 
                   notNA=!is.na(get(plotVar))
                   ) %>%
            group_by(source, hour) %>%
            summarize(pct=mean(notNA)) %>%
            ggplot(aes(x=hour, y=pct, group=source)) + 
            geom_line(alpha=0.25) + 
            labs(y="Percent of Days with Observations at Hour", 
                 x="METAR Hour (Z)", 
                 title=paste0("Capture for ", chartMapper[plotVar]), 
                 caption="Each line is a single location/year"
                 ) +
            scale_x_discrete(breaks=seq(0, 23, by=3))
        print(p1)
    }

    # Sum precipitation by year for p1, p36 (using only 0Z-6Z-12Z-18Z), p24
    mod_pin <- all_pin %>%
        mutate(p1Inches=ifelse(is.na(p1Inches), 0, p1Inches),
               p36ok=lubridate::hour(lubridate::round_date(dtime, unit="hours")) %in% c(0, 6, 12, 18),
               p36Inches=ifelse(is.na(p36Inches) | !p36ok, 0, p36Inches),
               p24Inches=ifelse(is.na(p24Inches), 0, p24Inches)
               )

    # Create plot for annual precipitation sums by metric summed (1-hour, 6-hour, 24-hour)
    p2 <- mod_pin %>%
        group_by(locale) %>%
        summarize_if(is.numeric, sum) %>%
        rename(`1-Hour`=p1Inches, `6-Hour`=p36Inches, `24-Hour`=p24Inches) %>%
        pivot_longer(-locale, names_to="Unit", values_to="Inches") %>%
        ggplot(aes(x=Unit, y=Inches)) + 
        geom_col(fill="lightblue") + 
        geom_text(aes(label=round(Inches, 1), y=Inches+10)) +
        labs(title="Annual precipitation when summing by various units", x="") +
        facet_wrap(~locale)
    print(p2)
    
    # Return the precipitation object
    all_pin

}

# Get the daily precipitation data for every file in fileNames
all_pin <- extractDailyPrecipitation(fileNames)


# Combine the relevant data files
allData <- combineProcessedFiles(fileNames)

# Show counts by sourceName
allData %>%
    count(source, sourceName)


# Modify windDir so that it is just N, NE, E, SE, S, SW, W, NW, 000, Variable
modData <- allData %>%
    mutate(tempDir=ifelse(is.na(WindDir) | WindDir %in% c("000", "VRB"), -1, as.numeric(WindDir)),
           predomDir=factor(case_when(is.na(WindDir) ~ "Error", 
                                      WindDir=="000" ~ "000", 
                                      WindDir=="VRB" ~ "VRB", 
                                      tempDir >= 337.5 ~ "N", 
                                      tempDir <= 22.5 ~ "N",
                                      tempDir <= 67.5 ~ "NE", 
                                      tempDir <= 112.5 ~ "E", 
                                      tempDir <= 157.5 ~ "SE", 
                                      tempDir <= 202.5 ~ "S", 
                                      tempDir <= 247.5 ~ "SW", 
                                      tempDir <= 292.5 ~ "W", 
                                      tempDir <= 337.5 ~ "NW",
                                      TRUE ~ "Error"
                                      ), 
                            levels=c("Error", "000", "VRB", "NE", "E", "SE", "S", "SW", "W", "NW", "N")
                            )
           )

# Show a glimpse of the processed data
glimpse(modData)

```
  
Functions are then written to create cloud data (level, type, height by source and date-time) and relevant summaries (clouds of up to 12,000 feet, bucketed, with values for minimum height and minimum ceiling):  
```{r cache=TRUE}

createCloudData <- function(df, 
                            fctLayers=c("VV", "OVC", "BKN", "SCT", "FEW"),
                            maxHeight=12000
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble of integrated weather station data
    # fctLayers: the factor levels for the cloud layers
    # maxHeight: a common maximum cloud height since different stations have different max 
    #            (NULL means do not create chart)
    
    # Select the source, sourceName, dtime and cLevel variables; pivot cLevel down
    cLevels <- df %>%
        select(source, sourceName, dtime, starts_with("cLevel")) %>%
        pivot_longer(-c(source, sourceName, dtime), names_to="level", values_to="height") %>%
        mutate(level=as.integer(str_replace(level, pattern="cLevel", replacement="")))

    # Select the source, sourceName, dtime and cType variables; pivot cType down
    cTypes <- df %>%
        select(source, sourceName, dtime, starts_with("cType")) %>%
        pivot_longer(-c(source, sourceName, dtime), names_to="level", values_to="type") %>%
        mutate(level=as.integer(str_replace(level, pattern="cType", replacement="")))

    # Join the data
    cData <- cLevels %>%
        inner_join(cTypes, by=c("source", "sourceName", "dtime", "level"))
    
    # Confirm that no information was lost during the inner join
    if (nrow(cData) < max(nrow(cLevels), nrow(cTypes))) {
        cat("\ncLevels:", nrow(cLevels), "cTypes:", nrow(cTypes), "cData:", nrow(cData))
        stop("\nLost rows during the inner join - diagnose and fix\n")
    }

    # Plot cloud heights, using only non-NA
    p1 <- cData %>%
        filter(!is.na(height)) %>%
        ggplot(aes(x=fct_reorder(sourceName, height, .fun=max, na.rm=TRUE), y=height)) + 
        geom_violin(fill="lightblue") + 
        coord_flip() + 
        labs(x="", y="Cloud Height (feet)", title="Density of cloud heights by locale")
    print(p1)

    # Plot cloud obscuration by locale
    p2 <- cData %>%
        filter(type!="") %>%
        mutate(type=factor(type, levels=fctLayers)) %>%
        ggplot(aes(x=fct_reorder(sourceName, height, .fun=max, na.rm=TRUE), fill=type)) + 
        geom_bar(position="stack") + 
        coord_flip() + 
        labs(x="", y="Cloud Layer Obscuration", title="Cloud obscuration by locale") + 
        scale_fill_discrete("", rev(fctLayers)) + 
        theme(legend.position="bottom")
    print(p2)

    # Plot cloud heights and obscurations, up to a specified maximum (provided that it is not null)
    if (!is.null(maxHeight)) {
        # Cloud height violin plot
        p3 <- cData %>%
            filter(!is.na(height)) %>%
            filter(height <= maxHeight) %>%
            ggplot(aes(x=sourceName, y=height)) + 
            geom_violin(fill="lightblue") + 
            coord_flip() + 
            labs(x="", 
                 y="Cloud Height (feet)", 
                 title=paste0("Density of cloud heights by locale (up to ", maxHeight, " feet)")
                 )
        print(p3)
        
        # Cloud obscuration plot
        p4 <- cData %>%
            filter(type!="") %>%
            filter(height <= maxHeight) %>%
            mutate(type=factor(type, levels=fctLayers)) %>%
            ggplot(aes(x=fct_reorder(sourceName, sourceName, .fun=length), fill=type)) + 
            geom_bar(position="stack") + 
            coord_flip() + 
            labs(x="", 
                 y="Cloud Layer Obscuration", 
                 title=paste0("Cloud obscuration by locale (up to ", maxHeight, " feet)")
                 ) + 
            scale_fill_discrete("", rev(fctLayers)) + 
            theme(legend.position="bottom")
        print(p4)
    }
    
    # Return the cData object
    cData
    
}

# Create the cloud data object
cData <- createCloudData(modData)

# Create the modified cloud data object (add CLR level at -100 if no clouds)
modCData <- cloudsLevel0(cData, maxHeight=12000, byVars=c("source", "sourceName", "dtime")) %>%
    mutate(type=factor(type, levels=c("VV", "OVC", "BKN", "SCT", "FEW", "CLR")))
modCData


# Helper function to convert cloud heights to levels (leave in main program; like a function argument)
helperHeightFactor <- function(x) {
    
    factor(case_when(x == -100 ~ "None", 
                     x <= 1000 ~ "0-1000", 
                     x <= 3000 ~ "1000-3000", 
                     x <= 6000 ~ "3000-6000",
                     x <= 12000 ~ "6000-12000"
                     ), 
           levels=c("None", "6000-12000", "3000-6000", "1000-3000", "0-1000")
           )
    
}


# Function to modify the clouds data and create relevant plots
plotAndModifyClouds <- function(df, helper=helperHeightFactor) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble holding the clouds data
    # helper: helper function for converting cloud heights to categorical factors
    
    # Checks for consistency
    df %>%
        count(ceilingType, obscType) %>%
        print()
    df %>%
        count(cloudType, ceilingType) %>%
        print()
    df %>%
        count(cloudType, obscType) %>%
        print()

    # Cloud obscuration by source
    plotMaxObsc(df, 
                xVar="sourceName", 
                fillVar="obscType", 
                title="Maximum Cloud Obscuration", 
                orderByVariable="obscType",
                orderByValue="CLR"
                )

    # Add month as both an integer and factor, and hour as an integer
    df <- df %>%
        mutate(month=lubridate::month(dtime), 
               hour=lubridate::hour(dtime), 
               monthfct=factor(month.abb[month], levels=month.abb[1:12])
               )
    
    # Cloud obscuration by month
    plotMaxObsc(df, 
                xVar="monthfct", 
                fillVar="obscType", 
                title="Maximum Cloud Obscuration", 
                posnBar="fill"
                )

    # Create the factored heights for minimum and ceiling
    df <- df %>%
        mutate(ceilFactor=helper(ceilingHeight), 
               minCFactor=helper(cloudHeight)
               )

    # Create plots for the minimum cloud height and ceiling height
    plotMaxObsc(df, 
                xVar="sourceName", 
                fillVar="minCFactor", 
                title="Minimum Cloud Height", 
                orderByVariable="minCFactor",
                orderByValue="None"
                )
    
    plotMaxObsc(df, 
                xVar="sourceName", 
                fillVar="ceilFactor", 
                title="Ceiling Height", 
                orderByVariable="ceilFactor",
                orderByValue="None"
                )

    # Return the modified clouds object with month and hour
    df
            
}

# Get the key clouds data (heights and ceilings)
cloudSummary <- hgtCeilObsc(modCData, byVars=c("source", "sourceName", "dtime")) %>%
    plotAndModifyClouds()

```
  
A final file containing processed METAR, daily high/low and precipitation, hourly booleans by precipitation type, and cloud height buckets (minimum, minimum ceiling) is created and saved:  
```{r}

# Function to add the boolean by type hourly precipitation data to finalData
addCurrentPrecip <- function(df, f=makeTibbleFromPrecip) {

    # FUNCTION ARGUMENTS:
    # df: data frame or tibble containing final, integrated data
    # f: function for mapping precipitation lists
    # CAVEAT: will use thunder_list, rain_list, and snow_list from global scope
    
    thunderData <- map_dfr(.x=list(thunder_List), 
                           .f=f, 
                           varName="isThunder"
                           )

    rainData <- map_dfr(.x=list(rain_List), 
                        .f=f, 
                        varName="isRain"
                        )

    snowData <- map_dfr(.x=list(snow_List), 
                        .f=f,
                        varName="isSnow"
                        )

    # Inner join the precipitation data
    precipData <- rainData %>%
        inner_join(snowData, by=c("source", "dtime")) %>%
        inner_join(thunderData, by=c("source", "dtime"))

    # Confirm that no rows have been lost
    if (nrow(precipData) < max(nrow(rainData), nrow(snowData), nrow(thunderData))) {
        stop("\nPrecipitation data have mislaignment issues")
    }


    # Merge in to finalData
    if (nrow(df) != nrow(precipData)) {
        stop("\nPrecipitation data are not aligned with final data")
    }

    df <- df %>%
        inner_join(precipData, by=c("source", "dtime"))

    # Confirm that no rows have been lost
    if (nrow(df) < nrow(precipData)) {
        stop("\nPrecipitation data did not merge correctly with final data")
    }
    
    # Return the frame with precipitation boolean added
    df
    
}


# Function to add the temperature and precipitation summary data
addTempPrecipSummaries <- function(df, precipFile, tempFile) {
    
    # FUNCTION ARGUMENTS:
    # df: the main data frame or tibble
    # precipFile: the data frame or tibble containing the precipitation data
    # tempFile: the data frame or tibble containing the temperature data

    # Merge together the precipitation and temperature data
    precipTempData <- precipFile %>%
        select(source, dtime, p1Inches, p36Inches, p24Inches) %>%
        inner_join(select(tempFile, source, dtime, tempFHi, tempFLo), 
                   by=c("source", "dtime")
                   )

    # Confirm that no rows have been lost
    if (nrow(precipTempData) < max(nrow(precipFile), nrow(tempFile))) {
        stop("\nPrecipitation and temperature summaries have mislaignment issues")
    }

    # Merge in to df
    if (nrow(df) != nrow(precipTempData)) {
        stop("\nPrecipitation and temperature summaries are not aligned with final data")
    }

    df <- df %>%
        inner_join(precipTempData, by=c("source", "dtime"))

    # Confirm that no rows have been lost
    if (nrow(df) < nrow(precipTempData)) {
        stop("\nPrecipitation and temperature summaries did not merge correctly with final data")
    }
    
    # Return the updated data frame
    df
    
}


# Function to add cloud summary
addCloudSummary <- function(df, cloudFile) {
    
    # FUNCTION ARGUMENTS:
    # df: the main data frame or tibble
    # cloudFile: the data frame or tibble containing the cloud summary data

    # Check for consistency in number of rows
    if (nrow(df) != nrow(cloudFile)) {
        stop("\nCloud summaries are not aligned with final data")
    }

    # Merge cloudFile in to df
    df <- df %>%
        inner_join(select(cloudFile, source, dtime, cloudHeight, cloudType, ceilingHeight, ceilingType), 
                   by=c("source", "dtime")
                   ) %>%
        rename(minHeight=cloudHeight, minType=cloudType)

    # Confirm that no rows have been lost
    if (nrow(df) < nrow(cloudFile)) {
        stop("\nCloud summaries did not merge correctly with final data")
    }
    
    # Return the updated data frame
    df
    
}


# Function to check for duplicates and enforce a 'same-year' restriction
enforceYearCheck <- function(df) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble to be checked
    
    stationTime <- df %>%
        select(source, dtime) %>%
        mutate(station=str_sub(source, 1, 4)) %>%
        select(station, dtime)

    finalDups <- stationTime %>%
        duplicated()

    # Duplicates are expected if the same station occurs in succeeding years
    # This is driven by downloading/processing 1 day on either side of the year requested
    if (sum(finalDups) > 0) {
        cat("\nDuplicated records occur during:\n")
        stationTime %>% 
            filter(finalDups) %>% 
            count(station, date=lubridate::date(dtime)) %>%
            print()
    } else {
        cat("\nAll data are unique by station and dtime")
    }

    # Filter data to require that year in file name matches year in dtime
    filteredData <- df %>%
        filter(as.integer(str_sub(source, 6, -1))==year)
    
    # Report on summary statistics
    cat("\nEnforcing year restriction reduces data rows from:", 
        nrow(df), 
        "to:", 
        nrow(filteredData), 
        "\n"
        )
    
    # Return the filtered data file
    filteredData

}


# Select the appropriate variables from modData
# Add boolean precipitation by type and hour
# Add summaries for high-low temperature and precipitation amount
# Add summaries for clouds
# Require that the year in the file source match the year of dtime
finalData <- modData %>%
    select(source, 
           locale=sourceName, 
           dtime, 
           origMETAR, 
           year, 
           monthint, 
           month, 
           day, 
           WindDir,
           WindSpeed,
           WindGust,
           predomDir,
           Visibility,
           Altimeter,
           TempF,
           DewF,
           modSLP,
           starts_with("cType"),
           starts_with("cLevel")
           ) %>%
    addCurrentPrecip() %>%
    addTempPrecipSummaries(precipFile=all_pin, tempFile=all_hilo) %>%
    addCloudSummary(cloudFile=cloudSummary) %>%
    enforceYearCheck()


# Report summary of the final file, and save as RDS
summary(finalData)
saveRDS(finalData, "./RInputFiles/ProcessedMETAR/metar_postEDA_extra_20200626.rds")

```
  
The precipitation data is then integrated and saved:  
```{r}

# Integrate the rain, snow, and thunder lists
allPrecipList <- list(rain=rain_List, 
                      snow=snow_List, 
                      thunder=thunder_List
                      )

# Provide a structure summary, and save as RDS
str(allPrecipList, max.level = 2)
saveRDS(allPrecipList, "./RInputFiles/ProcessedMETAR/metar_precipLists_extra_20200626.rds")

```
  
The clouds data are then integrated and saved:
```{r}

checkAndFilterClouds <- function(cloudFile, mainFile) {
    
    # FUNCTION ARGUMENTS:
    # cloudFile: the data frame or tibble containing the clouds data
    # mainFile: the data frame or tibble containing the final integrated data
    
    # Get the unique list 
    modCDataTimes <- cloudFile %>%
        count(source, dtime)
    
    # In final, integrated data but not in clouds data
    mainFile %>%
        select(source, dtime) %>%
        anti_join(modCDataTimes, by=c("source", "dtime")) %>%
        print()

    # In modCDataTimes but not in main file
    modCDataTimes %>%
        select(source, dtime) %>%
        anti_join(mainFile, by=c("source", "dtime")) %>%
        count(source, date=lubridate::date(dtime)) %>%
        pivot_wider(source, names_from="date", values_from="n") %>%
        mutate_if(is.numeric, ~ifelse(is.na(.), 0, .)) %>%
        as.data.frame() %>%
        print()

    # Filter modCData so that year aligns as with fileteredData (return by default)
    modCData %>%
        filter(as.integer(str_sub(source, 6, 9))==lubridate::year(dtime))
    
}


# Run the function to get the filtered clouds data, then save as RDS
modCSaveData <- checkAndFilterClouds(cloudFile=modCData, mainFile=finalData)
saveRDS(modCSaveData, "./RInputFiles/ProcessedMETAR/metar_modifiedClouds_extra_20200626.rds")

```

Functions and analyses not run:  
```{r eval=FALSE}

# Fix for visibility > 10
cat("\nResetting Visibility > 10 to 10\n")
filteredData %>% 
    filter(Visibility > 10)

summary(filteredData$Visibility)
filteredData <- filteredData %>%
    mutate(Visibility=ifelse(is.na(Visibility), NA, pmin(Visibility, 10)))
summary(filteredData$Visibility)


# Create rounded TempF and DewF in allData
allData <- allData %>%
    mutate(TempF5=5*round(round(TempF)/5), 
           DewF5=5*round(round(DewF)/5), 
           WindSpeed5=5*round(WindSpeed/5),
           Altimeter10=round(Altimeter, 1)
           )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleCountsByMetric.pdf"))

# Counts by Metric for allData
plotcountsByMetric(allData, 
                   mets=c("month", "year",
                          "WindDir", "WindSpeed5", 
                          "Visibility", "Altimeter10",
                          "TempF5", "DewF5", 
                          "wType"
                          ), 
                   title="Comparisons Across Locales (red dots are the median)", 
                   facetOn="sourceName",
                   showCentral=TRUE, 
                   multiPageWrap=TRUE, 
                   maxPerPage=12, 
                   balancePages=TRUE
                   )

# Redirect to standard plotting
dev.off()


# Example for allData - using mixes of WindSpeed, Altimeter, TempF, DewF, TempC, DewC
numCorList <- list(c("TempC", "TempF"), 
                   c("DewC", "DewF"), 
                   c("TempF", "DewF"), 
                   c("Altimeter", "WindSpeed"), 
                   c("Altimeter", "TempF")
                   )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleCorrelations.pdf"))

# Run the list through plotNumCor()
for (x in numCorList) {
    plotNumCor(allData, 
               var1=x[1], 
               var2=x[2], 
               alpha=0.2,
               maxSize=3,
               subT="Red dashed line is the overall slope", 
               diagnose=TRUE, 
               facetOn="sourceName", 
               showCentral=TRUE
               )
}

# Redirect to standard plotting
dev.off()



# Key factor variables include month, wType, predomDir
# Key numeric variables include WindSpeed, Altimeter, TempF, DewF, Visibility
fctNumList <- list(c("month", "WindSpeed"), 
                   c("month", "Altimeter"), 
                   c("month", "TempF"), 
                   c("month", "DewF"), 
                   c("month", "Visibility"),
                   c("wType", "WindSpeed"),
                   c("wType", "Altimeter"),
                   c("wType", "Visibility"),
                   c("predomDir", "WindSpeed"),
                   c("predomDir", "Altimeter"),
                   c("predomDir", "TempF"),
                   c("predomDir", "DewF")
                   )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleFactorVsNumeric.pdf"))

for (x in fctNumList) {
    plotFactorNumeric(modData, 
                      fctVar=x[1], 
                      numVar=x[2], 
                      subT="Red dots are the overall average", 
                      showXLabel=FALSE,
                      diagnose=TRUE, 
                      facetOn="sourceName",
                      showCentral=TRUE, 
                      multiPageWrap=TRUE,
                      maxPerPage=12,
                      balancePages=TRUE
                      )
}

# Redirect to standard plotting
dev.off()


# Key factor variables include month, wType, predomDir
# Key numeric variables include WindSpeed, Altimeter, TempF, DewF, Visibility
fctNumListIQR <- list(c("month", "WindSpeed"), 
                      c("month", "Altimeter"), 
                      c("wType", "WindSpeed"),
                      c("predomDir", "Altimeter"),
                      c("predomDir", "TempF"),
                      c("predomDir", "DewF")
                      )

# Save to an external PDF
pdf(paste0(filePath, "CrossLocaleIQRFactorVsNumeric.pdf"))

for (x in fctNumListIQR) {
    plotMedianIQR(modData, 
                  fctVar=x[1], 
                  numVar=x[2], 
                  subT="Red dots are the overall mid-quantile", 
                  showXLabel=FALSE,
                  diagnose=TRUE, 
                  facetOn="sourceName",
                  showCentral=TRUE, 
                  multiPageWrap=TRUE,
                  maxPerPage=12,
                  balancePages=TRUE
                  )
}

# Redirect to standard plotting
dev.off()


plotMedianIQR(modData, 
              fctVar="wType", 
              numVar="WindSpeed", 
              subT="Red dots are the overall mid-quantile", 
              showXLabel=FALSE,
              diagnose=TRUE, 
              facetOn="sourceName",
              showCentral=TRUE, 
              ylimits=c(0, 15), 
              multiPageWrap=TRUE,
              maxPerPage=12,
              balancePages=TRUE
              )


# Counts by Metric for predomDir using mod2016Data
plotcountsByMetric(modData, 
                   mets=c("predomDir"), 
                   title="Comparisons Across Locales (red dots are the median)", 
                   facetOn="sourceName",
                   showCentral=TRUE, 
                   multiPageWrap=TRUE, 
                   maxPerPage=12,
                   balancePages=TRUE
                   )

```
  
```{r eval=FALSE}

cityCloudList <- list(c("klas_2016", "ksan_2016", "kiah_2016", "kmsy_2016"), 
                      c("kgrb_2016", "kgrr_2016", "kdtw_2016", "ktvc_2016"), 
                      c("klnk_2016", "kmsp_2016", "kmsn_2016", "kind_2016"), 
                      c("kmke_2016", "kord_2016", "kewr_2016")
                      )

for (x in cityCloudList) {
    cloudUse <- cloudSummary %>%
        filter(source %in% x)
    plotMaxObsc(cloudUse, 
                xVar="monthfct", 
                fillVar="obscType", 
                title="Maximum Cloud Obscuration", 
                facetOn="sourceName", 
                posnBar="fill"
                )
}

```
  
```{r eval=FALSE}

# Find the distances for obscType by locale vs. locale
obscDist <- findCloudDist(cloudSummary, 
                          byVar=c("source", "sourceName"), 
                          fctVar="obscType", 
                          pivotVar="monthfct"
                          )
obscDist


plotCloudDist(obscDist, subT="Based on % of obscuration type by month")

# Run for minCFactor
findCloudDist(cloudSummary, 
              byVar=c("source", "sourceName"), 
              fctVar="minCFactor", 
              pivotVar="monthfct"
              ) %>%
    plotCloudDist(subT="Based on % in each minimum cloud height bucket by month")

# Run for ceilFactor
findCloudDist(cloudSummary, 
              byVar=c("source", "sourceName"), 
              fctVar="ceilFactor", 
              pivotVar="monthfct"
              ) %>%
    plotCloudDist(subT="Based on % in each ceiling height bucket by month")


set.seed(2006040940)

ceilDistData <- findCloudDist(cloudSummary, 
                              byVar=c("source", "sourceName"), 
                              fctVar="ceilFactor", 
                              pivotVar="monthfct", 
                              returnPivotOnly=TRUE
                              )

tibble::tibble(locale=ceilDistData$sourceName, 
               cluster=kmeans(dist(ceilDistData[3:ncol(ceilDistData)]), centers=5, nstart=1000)$cluster
               ) %>%
    arrange(-cluster) %>%
    as.data.frame()


hclust(dist(ceilDistData[3:ncol(ceilDistData)]), method="complete") %>%
    plot(labels=ceilDistData$sourceName, cex=0.5, main="Hierarchical on Ceiling Height: method=complete")

hclust(dist(ceilDistData[3:ncol(ceilDistData)]), method="single") %>%
    plot(labels=ceilDistData$sourceName, cex=0.5, main="Hierarchical on Ceiling Height: method=single")


heightDistData <- findCloudDist(cloudSummary, 
                                byVar=c("source", "sourceName"), 
                                fctVar="minCFactor", 
                                pivotVar="monthfct", 
                                returnPivotOnly=TRUE
                                )

hclust(dist(heightDistData[3:ncol(heightDistData)]), method="complete") %>%
    plot(labels=heightDistData$sourceName, cex=0.5, 
         main="Hierarchical on Minimum Cloud Height: method=complete"
         )

hclust(dist(heightDistData[3:ncol(heightDistData)]), method="single") %>%
    plot(labels=heightDistData$sourceName, cex=0.5, 
         main="Hierarchical on Minimum Cloud Height: method=single"
         )


set.seed(2006041003)

cl5 <- tibble::tibble(locale=heightDistData$sourceName, 
               cluster=kmeans(dist(heightDistData[3:ncol(heightDistData)]), centers=5, nstart=1000)$cluster
               ) %>%
    arrange(-cluster)

cl6 <- tibble::tibble(locale=heightDistData$sourceName, 
               cluster=kmeans(dist(heightDistData[3:ncol(heightDistData)]), centers=6, nstart=1000)$cluster
               ) %>%
    arrange(-cluster)

cl7 <- tibble::tibble(locale=heightDistData$sourceName, 
               cluster=kmeans(dist(heightDistData[3:ncol(heightDistData)]), centers=7, nstart=1000)$cluster
               ) %>%
    arrange(-cluster)


cl5 %>%
    rename(cl5=cluster) %>%
    inner_join(rename(cl6, cl6=cluster), by="locale") %>%
    inner_join(rename(cl7, cl7=cluster), by="locale") %>%
    as.data.frame()

```
  
