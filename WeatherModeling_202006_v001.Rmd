---
title: "Weather Modeling"
author: "davegoblue"
date: "6/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
The file 'WeatherEDA_202005_v002.Rmd' contains exploratory data analysis for historical weather data as contained in METAR archives hosted by Iowa State University.

Data have been dowloaded, processed, cleaned, and integrated for several stations (airports) and years, with .rds files saved in "./RInputFiles/ProcessedMETAR".

This module will perform initial modeling on the processed weather files.

#### _Data Availability_  
There are three main processed files available for further exploration:  
  
_metar_postEDA_20200617.rds_  
  
* source (chr) - the reporting station and time  
* locale (chr) - the descriptive name for source  
* dtime (dttm) - the date-time for the observation  
* origMETAR (chr) - the original METAR associated with the observation at that source and date-time  
* year (dbl) - the year, extracted from dtime  
* monthint (dbl) - the month, extracted from dtime, as an integer  
* month (fct) - the month, extracted from dtime, as a three-character abbreviation (factor)  
* day (int) - the day of the month, extracted from dtime  
* WindDir (chr) - previaling wind direction in degrees, stored as a character since 'VRB' means variable  
* WindSpeed (int) - the prevailing wind speed in knots  
* WindGust (dbl) - the wind gust speed in knots (NA if there is no recorded wind gust at that hour)  
* predomDir (chr) - the predominant wind direction as NE-E-SE-S-SW-W-NW-N-VRB-000-Error  
* Visibility (dbl) - surface visibility in statute miles  
* Altimeter (dbl) - altimeter in inches of mercury  
* TempF (dbl) - the Fahrenheit temperature  
* DewF (dbl) - the Fahrenheit dew point  
* modSLP (dbl) - Sea-Level Pressure (SLP), adjusted to reflect that SLP is recorded as 0-1000 but reflects data that are 950-1050  
* cTypen (chr) - the cloud type of the nth cloud layer (FEW, BKN, SCT, OVC, or VV)  
* cLeveln (dbl) - the cloud height in feet of the nth cloud layer  
* isRain (lgl) - was rain occurring at the moment the METAR was captured?  
* isSnow (lgl) - was snow occurring at the moment the METAR was captured?  
* isThunder (lgl) - was thunder occurring at the moment the METAR was captured?  
* p1Inches (dbl) - how many inches of rain occurred in the past hour?  
* p36Inches (dbl) - how many inches of rain occurred in the past 3/6 hours (3-hour summaries at 3Z-9Z-15Z-21Z and 6-hour summaries at 6Z-12Z-18Z-24Z and NA at any other Z times)?  
* p24Inches (dbl) - how many inches of rain occurred in the past 24 hours (at 12Z, NA at all other times)  
* tempFHi (dbl) - the high temperature in the past 24 hours, in Fahrenheit (reported once per day)  
* tempFLo (dbl) - the low temperature in the past 24 hours, in Fahrenheit (reported once per day)  
* minHeight (dbl) - the minimum cloud height in feet (-100 means 'no clouds')  
* minType (fct) - amount of obscuration at the minimum cloud height (VV > OVC > BKN > SCT > FEW > CLR)  
* ceilingHeight (dbl) - the minimum cloud ceiling in feet (-100 means 'no ceiling')  
* ceilingType (fct) - amount of obscuration at the minimum ceiling height (VV > OVC > BKN)  
  
_metar_modifiedClouds_20200617.rds_  
  
* source (chr) - the reporting station and time  
* sourceName (chr) - the descriptive name for source  
* dtime (dttm) - the date-time for the observation  
* level (dbl) - cloud level (level 0 is inserted for every source-dtime as a base layer of clear)  
* height (dbl) - level height (height -100 is inserted for every source-dtime as a base layer of clear)  
* type (dbl) - level type (type CLR is inserted for every source-dtime as a base layer of clear)  
  
_metar_precipLists_20200617.rds_  
  
* Contains elements for each of rain/snow/thunder for each of 2015/2016/2017  
* Each element contains a list and a tibble  
* The tibble is precipLength and contains precipitation by month as source-locale-month-hours-events  
* The list is precipList and contains data on each precipitation interval  
  
Glimpses of the three main files are as follows:  
```{r}

# The tidyverse library will be used throughout
library(tidyverse)


# Main weather data
metarData <- readRDS("./RInputFiles/ProcessedMETAR/metar_postEDA_20200617.rds")
glimpse(metarData)

# Extra clouds data
cloudData <- readRDS("./RInputFiles/ProcessedMETAR/metar_modifiedClouds_20200617.rds")
glimpse(cloudData)

# Precipitation summaries
precipData <- readRDS("./RInputFiles/ProcessedMETAR/metar_precipLists_20200617.rds")
names(precipData)
names(precipData$rain2016)
names(precipData$rain2016$precipList)
names(precipData$rain2016$precipList$kdtw_2016_RA)
glimpse(precipData$rain2016$precipLength)
glimpse(precipData$rain2016$precipList$kdtw_2016_RA)

```

#### _Helpers - Functions and Mappings_  
There are a few functions and mappings that can help with the modeling process:  
```{r}

# The process frequently uses tidyverse and lubridate
library(tidyverse)
library(lubridate)


# The main path for the files
filePath <- "./RInputFiles/ProcessedMETAR/"


# Descriptive names for key variables
varMapper <- c(source="Original source file name", 
               locale="Descriptive name",
               dtime="Date-Time (UTC)",
               origMETAR="Original METAR",
               year="Year",
               monthint="Month",
               month="Month", 
               day="Day of Month",
               WindDir="Wind Direction (degrees)", 
               WindSpeed="Wind Speed (kts)",
               WindGust="Wind Gust (kts)",
               predomDir="General Prevailing Wind Direction",
               Visibility="Visibility (SM)", 
               Altimeter="Altimeter (inches Hg)",
               TempF="Temperature (F)",
               DewF="Dew Point (F)", 
               modSLP="Sea-Level Pressure (hPa)", 
               cType1="First Cloud Layer Type", 
               cLevel1="First Cloud Layer Height (ft)",
               isRain="Rain at Observation Time",
               isSnow="Snow at Observation Time",
               isThunder="Thunder at Obsevation Time",
               tempFHi="24-hour High Temperature (F)",
               tempFLo="24-hour Low Temperature (F)",
               minHeight="Minimum Cloud Height (ft)",
               minType="Obscuration Level at Minimum Cloud Height",
               ceilingHeight="Minimum Ceiling Height (ft)",
               ceilingType="Obscuration Level at Minimum Ceiling Height"
               )


# File name to city name mapper
cityNameMapper <- c(kdtw_2016="Detroit, MI (2016)", 
                    kewr_2016="Newark, NJ (2016)",
                    kgrb_2016="Green Bay, WI (2016)",
                    kgrr_2016="Grand Rapids, MI (2016)",
                    kiah_2016="Houston, TX (2016)",
                    kind_2016="Indianapolis, IN (2016)",
                    klas_2015="Las Vegas, NV (2015)",
                    klas_2016="Las Vegas, NV (2016)", 
                    klas_2017="Las Vegas, NV (2017)", 
                    klnk_2016="Lincoln, NE (2016)",
                    kmke_2016="Milwaukee, WI (2016)",
                    kmsn_2016="Madison, WI (2016)",
                    kmsp_2016="Minneapolis, MN (2016)",
                    kmsy_2015="New Orleans, LA (2015)",
                    kmsy_2016="New Orleans, LA (2016)", 
                    kmsy_2017="New Orleans, LA (2017)", 
                    kord_2015="Chicago, IL (2015)",
                    kord_2016="Chicago, IL (2016)", 
                    kord_2017="Chicago, IL (2017)", 
                    ksan_2015="San Diego, CA (2015)",
                    ksan_2016="San Diego, CA (2016)",
                    ksan_2017="San Diego, CA (2017)",
                    ktvc_2016="Traverse City, MI (2016)"
                    )


```
  
#### _Initial Exploration_  
The caret package allows for many types of models to be called on the data.

A function is written to subset the data and create appropriate train-test splits:  
```{r}

# Create test and train data, filtered for locales and months, and keeping only relevant variables
createTestTrain <- function(df, 
                            sources=NULL, 
                            vrbls=NULL,
                            convFactor="locale",
                            months=1:12,
                            testSize=0.3,
                            noNA=TRUE,
                            seed=NULL
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble containing the data
    # sources: the source records to be included (NULL will include all)
    # vrbls: the variables to be included (NULL will include all)
    # convFactor: variable to convert to a factor after processing (NULL for none)
    # months: the months to be included
    # testSize: the fraction of observations to be included as test
    # seed: the seed for reproducibility

    # Set the seed if it has been passed
    if (!is.null(seed)) { set.seed(seed) }
    
    # Set sources to be all sources if NULL
    if (is.null(sources)) { 
        sources <- df %>%
            count(source) %>%
            pull(source)
    }
    
    # Set vrbls to be all variables if NULL
    if (is.null(vrbls)) { vrbls <- names(df) }
    
    # Filter the data for the relevant sources and months, and limit to vars
    dfMod <- df %>%
        filter(source %in% sources, monthint %in% months) %>%
        select_at(vars(all_of(vrbls)))
    
    # Use only complete cases if noNA=TRUE
    if (noNA) {
        dfMod <- dfMod %>%
            filter(complete.cases(dfMod))
    }
    
    # Convert the key variable to a factor if requested
    if (!is.null(convFactor)) {
        dfMod <- dfMod %>%
            mutate_at(vars(all_of(convFactor)), factor)
    }
    
    # Split in to test and train (do not sort so that records by locale do not end up in any given order)
    idx <- sample(1:nrow(dfMod), round((1-testSize) * nrow(dfMod)), replace=FALSE)

    # Return the test and train data
    list(testData=dfMod[-idx, ], 
         trainData=dfMod[idx, ]
         )
    
}

```
  
As an example, data for 2016 are pulled for two of the cities - Las Vegas and New Orleans:  
```{r}

ttLists <- createTestTrain(metarData, 
                           sources=c("klas_2016", "kmsy_2016"), 
                           vrbls=c("locale", "month", "predomDir", "TempF", "DewF", "Altimeter", "modSLP"),
                           seed=2006181356
                           )
ttLists

```
  
A simple random forest can then be applied to the data, with several options for complexity:  
```{r cache=TRUE}

# Create a tuning grid and run the models
trGrid <- expand.grid(min.node.size=c(1, 5, 10, 25, 100), 
                      mtry=c(1, 2, 3, 4, 5, 6), 
                      splitrule=c("gini")
                      )

caretModel <- caret::train(locale ~ ., 
                           data=ttLists$trainData,
                           method="ranger",
                           tuneGrid=trGrid,
                           trControl=caret::trainControl(method="cv", number=5),
                           num.trees=50
                           )
caretModel

```
  
The model is not worried about over-fitting, choosing the smallest minimum node size and the largest mrty.  How well does this model work on the test data?  
```{r}

# Run the best parameters from ranger in randomForest
caretBest <- randomForest::randomForest(locale ~ ., 
                                        data=ttLists$trainData,
                                        ntree=50, 
                                        nodesize=1, 
                                        mtry=6
                                        )
caretBest

```
  
The model estimates an OOB error rate of between 1%-2%, suggesting a strong ability to differentiate Las Vegas from New Orleans.

A function is built to assess performance on the test data:  
```{r}

# Predictions and confusion matrices on test data
evalPredictions <- function(model, 
                            testData, 
                            printAll=TRUE,
                            printCM=printAll, 
                            printConfSummary=printAll, 
                            printConfTable=printAll, 
                            showPlots=TRUE
                            ) {
    
    # FUNCTION ARGUMENTS:
    # model: a trained model
    # testData: the test data to apply the model against
    # printAll: whether to print the text summaries (by default, carries to the next three arguments)
    # printCM: whether to print the confusion matrix
    # printConfSummary: whether to print the voting summary (rough proxy for confidence) of the predictions
    # printConfTable: whether to summarize the accuracy by voting summary
    # showPlots: whether to display plots related to the outputs
    
    # Get the predicted class and probabilities
    testClass <- predict(model, newdata=testData)
    testProbs <- predict(model, newdata=testData, type="prob")
    if (printCM) {
        print(caret::confusionMatrix(testClass, testData$locale))
    }
    
    # Create a tibble containing class prediction, maximum probability, and individual predictions
    tblProbs <- tibble::as_tibble(testProbs) %>%
        mutate(maxProb=apply(., 1, FUN=max), 
               sumProb=apply(., 1, FUN=sum), 
               predClass=testClass, 
               locale=testData$locale, 
               accurate=(predClass==locale)
               )
    
    # Describe the maximum probability by source
    if (printConfSummary) {
        tblProbs %>%
            group_by(locale) %>%
            summarize(meanMax=mean(maxProb), medianMax=median(maxProb), 
                      pct90Plus=mean(maxProb > 0.9), pct50Minus=mean(maxProb < 0.5)
                      ) %>%
        print()
    }
    
    # Create a table of accuracy by source and prediction confidence
    p1Data <- tblProbs %>%
        mutate(predProb=0.5 * round(2*maxProb, 1)) %>%
        group_by(predProb, locale) %>%
        summarize(pctCorrect=mean(accurate), nCorrect=sum(accurate), nObs=n())
    
    p1Print <- p1Data %>%
        group_by(predProb) %>%
        summarize(nCorrect=sum(nCorrect), nObs=sum(nObs)) %>%
        mutate(pctCorrect=nCorrect/nObs)
    if (printConfTable) {
        print(p1Print)
    }
    
    cat("\nMean Error-Squared Between Confidence of Prediction and Accuracy of Precition\n")
    p1Print %>%
        mutate(err2=nObs*(pctCorrect-predProb)**2) %>%
        summarize(meanError2=sum(err2)/sum(nObs)) %>%
        print()
    
    # Plot the maximum probability forecasted by row
    p1 <- p1Data %>%
        ggplot(aes(x=predProb)) +
        geom_col(aes(y=nObs, fill=locale)) + 
        labs(x="Maximum probability predicted", y="# Observations", 
             title="Count of Maximum Probability Predicted by Locale"
             )
    p2 <- p1Data %>%
        ggplot(aes(x=predProb)) +
        geom_line(aes(y=pctCorrect, group=locale, color=locale)) + 
        geom_abline(aes(intercept=0, slope=1), lty=2) +
        ylim(c(0, 1)) + 
        labs(x="Maximum probability predicted", y="Actual Probability Correct", 
             title="Accuracy of Maximum Probability Predicted by Locale"
             )
    
    if (showPlots) {
        print(p1)
        print(p2)
    }
    
    tblProbs
    
}

```
  
And the function is then run for the Las Vegas and New Orleans data:  
```{r}

evalPredictions(caretBest, testData=ttLists$testData)

caret::varImp(caretBest) %>%
    rownames_to_column() %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(x="", y="Importance")

```
  
The model has greater than 98% accuracy in splitting Las Vegas and New Orleans, driven by 1) New Orleans being extremely humid and Las Vegas being a desert, and 2) New Orleans being at sea level and Las Vegas being at high altitude.
  
Using modSLP and Altimeter is arguably cheating since the values and relationships between them are highly driven by a location's height relative to sea-level.  How does the model perform if these are deleted?  
```{r cache=TRUE}

# Create a tuning grid and run the models
trGrid <- expand.grid(min.node.size=c(1, 5, 10, 25, 100), 
                      mtry=c(1, 2, 3, 4), 
                      splitrule=c("gini")
                      )

caretModel <- caret::train(locale ~ DewF + month + predomDir + TempF, 
                           data=ttLists$trainData,
                           method="ranger",
                           tuneGrid=trGrid,
                           trControl=caret::trainControl(method="cv", number=5),
                           num.trees=50
                           )
caretModel

```
  
The model can then be run using the best parameters:  
```{r}

# Run the best parameters from ranger in randomForest
caretBest <- randomForest::randomForest(locale ~ DewF + month + predomDir + TempF, 
                                        data=ttLists$trainData,
                                        ntree=50, 
                                        nodesize=1, 
                                        mtry=4
                                        )
caretBest

```
  
As expected, performance dips to around 95% accuracy:  
```{r}

evalPredictions(caretBest, testData=ttLists$testData)

caret::varImp(caretBest) %>%
    rownames_to_column() %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(x="", y="Importance")

```
  

The very large difference in dew point drives the high classification ability:  
```{r}

ttLists$trainData %>% 
    bind_rows(ttLists$testData) %>%
    ggplot(aes(x=TempF, y=DewF)) + 
    geom_bin2d() + 
    facet_wrap(~locale) + 
    geom_density_2d() + 
    scale_fill_continuous(low="white", high="black")

```

Suppose on the other hand that the model tries to distinguish New Orleans from Houston:  
```{r}

ttLists <- createTestTrain(metarData, 
                           sources=c("kiah_2016", "kmsy_2016"), 
                           vrbls=c("locale", "month", "predomDir", "TempF", "DewF"),
                           seed=2006181440
                           )
ttLists
```
  
```{r cache=TRUE}

# Create a tuning grid and run the models
trGrid <- expand.grid(min.node.size=c(1, 5, 10, 25, 100), 
                      mtry=c(1, 2, 3, 4), 
                      splitrule=c("gini")
                      )

caretModel <- caret::train(locale ~ DewF + month + predomDir + TempF, 
                           data=ttLists$trainData,
                           method="ranger",
                           tuneGrid=trGrid,
                           trControl=caret::trainControl(method="cv", number=5),
                           num.trees=50
                           )
caretModel

```
  
The best parameters from the model can then be run:  
```{r}

# Run the best parameters from ranger in randomForest
caretBest <- randomForest::randomForest(locale ~ DewF + month + predomDir + TempF, 
                                        data=ttLists$trainData,
                                        ntree=50, 
                                        nodesize=10, 
                                        mtry=4
                                        )
caretBest

```
  
Accuracy dips to the 70% range, though this is still surprisingly high given how similar the climates are in New Orleans and Houston:  
```{r}

evalPredictions(caretBest, testData=ttLists$testData)

caret::varImp(caretBest) %>%
    rownames_to_column() %>%
    ggplot(aes(x=fct_reorder(rowname, -Overall), y=Overall)) + 
    geom_col() + 
    labs(x="", y="Importance")

```
  
The mix of temperature and dew-point is the primary driver of the classiciations.  There are many more "low confidence" (close voting) predictions for these two cities:  
```{r}

ttLists$trainData %>% 
    bind_rows(ttLists$testData) %>%
    ggplot(aes(x=TempF, y=DewF)) + 
    geom_bin2d() + 
    facet_wrap(~locale) + 
    geom_density_2d() + 
    scale_fill_continuous(low="white", high="black")

```
  
It is impressive that the model can tease out distinctions in data that are, at a glance, very similar.
  
#### _Temperature and Dew Point_  
Suppose that the only information available about two cities were their temperatures and dew points.  How well would a basic random forest, with mtry=2, classify the cities?

A function is written to take two locales and a random seed, create relevant test-train data, run a random forest model, make predictions, assess the accuracy, and return the relevant objects:  
```{r}

# The createTestTrain function is updated to purely split an input dataframe
createTestTrain <- function(df, 
                            testSize=0.3, 
                            sortTrain=FALSE,
                            noNA=TRUE,
                            seed=NULL
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble for analysis
    # testSize: the proportion of the data to be used as test
    # sortTrain: boolean, whether to sort training indices to maintain the original order of training data
    # noNA: boolean, whether to include only complete cases
    # seed: the random seed to be used (NULL means no seed)
    
    # Filter out NA if requested
    if (noNA) {
        df <- df %>%
            filter(complete.cases(df))
    }
    
    # Get the desired number of train objects
    nTrain <- round((1-testSize) * nrow(df))
    
    # Set the random seed if it has been passed
    if (!is.null(seed)) { set.seed(seed) }
    
    # Get the indices for the training data
    idxTrain <- sample(1:nrow(df), size=nTrain, replace=FALSE)
    
    # Sort if requested
    if (sortTrain) { idxTrain <- sort(idxTrain) }
    
    # Return a list containing the train and test data
    list(trainData=df[idxTrain, ], 
         testData=df[-idxTrain, ]
         )
    
}


# Run a random forest for two locales
rfTwoLocales <- function(df,
                         loc1, 
                         loc2,
                         locVar="source",
                         otherVar="dtime",
                         vrbls=c("TempF", "DewF"),
                         pred="locale",
                         seed=NULL,
                         ntree=100,
                         mtry=NULL
                         ) {

    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble
    # loc1: the first locale
    # loc2: the second locale
    # locVar: the name of the variable where loc1 and loc2 can be found
    # otherVar: other variables to be kept, but not used in modeling
    # vrbls: explanatory variables for modeling
    # pred: predictor variable for modeling
    # seed: the random seed (NULL means no seed)
    # ntree: the number of trees to grow in the random forest
    # mtry: the splitting parameter for the random forest (NULL means use all variables)
    
    # Filter df such that it includes only observations in loc1 or loc2
    # Select only the predictor and variables of interest
    dfModel <- df %>%
        filter(get(locVar) %in% c(loc1, loc2)) %>%
        select_at(vars(all_of(c(pred, otherVar, vrbls))))
    
    # Create the test-train split (will randomize using seed if provided)
    ttLists <- createTestTrain(dfModel, testSize=0.3, seed=seed)

    # Set the seed if requested
    if (!is.null(seed)) { set.seed(seed) }
    
    # Set mtry to be the length of the variables if not provided
    if (is.null(mtry)) { mtry <- length(vrbls) }
    
    # Run the random forest on the training data
    rfModel <- randomForest::randomForest(x=ttLists$trainData[, vrbls], 
                                          y=factor(ttLists$trainData[, pred, drop=TRUE]), 
                                          mtry=mtry, 
                                          ntree=ntree
                                          )
    
    # Create predictions on the test data
    testPred <- predict(rfModel, ttLists$testData)
    
    # Augment the test data with the predictions
    testData <- ttLists$testData %>%
        mutate(predicted=testPred, correct=(testPred==get(pred)))
    
    # Grab the accuracy and accuracy by class
    
    # Return the objects as a list
    list(rfModel=rfModel, 
         testData=testData,
         errorRate=colMeans(rfModel$err.rate)
         )
    
}

```
  
The function is then run for every combination of locales from 2016 in cityNameMapper.  A common random seed is applied to every run of the process:  
```{r cache=TRUE}

# Create the file names to explore
names_2016 <- grep(x=names(cityNameMapper), pattern="_2016", value=TRUE)

# Create a container list to hold the output
list_2016_TempF_DewF <- vector("list", 0.5*length(names_2016)*(length(names_2016)-1))

n <- 1
for (ctr in 1:(length(names_2016)-1)) {
    for (ctr2 in (ctr+1):length(names_2016)) {
        list_2016_TempF_DewF[[n]] <- rfTwoLocales(metarData, 
                                                  loc1=names_2016[ctr], 
                                                  loc2=names_2016[ctr2], 
                                                  vrbls=c("TempF", "DewF"),
                                                  ntree=25
                                                  )
        n <- n + 1
    }
}

```
  
Statistics about the overall accuracy can then be captured:  
```{r}

# Helper function for map_dfr
helperAccuracyLocale <- function(x) {
    
    y <- x$errorRate
    tibble::tibble(locale1=names(y)[2], 
                   locale2=names(y)[3], 
                   accOverall=1-y[1], 
                   accLocale1=1-y[2], 
                   accLocale2=1-y[3]
                   )
    
}

# Create a tibble from the underlying accuracy data
acc_TempF_DewF <- map_dfr(list_2016_TempF_DewF, .f=helperAccuracyLocale)

# Assess the top 10 accuracy and the bottom 10 accuracy
acc_TempF_DewF %>%
    arrange(-accOverall) %>%
    head(10)
acc_TempF_DewF %>%
    arrange(accOverall) %>%
    head(10)

```
  
Not surprisingly, accuracy is highest when comparing cold-weather cities or hot-humid cities to Las Vegas.  Accuracy is lowest when comparing cold-weather cities against each other.  Interestingly, Chicago-Milwaukee and Grand Rapids-Traverse City have again emerged as the two "closest" cities in this analysis.

Of further interest is an overall accuracy by city:  
```{r}

allAccuracy <- select(acc_TempF_DewF, locale=locale1, other=locale2, accOverall, accLocale=accLocale1) %>%
    bind_rows(select(acc_TempF_DewF, locale=locale2, other=locale1, accOverall, accLocale=accLocale2))

# Find the best match by locale
allAccuracy %>%
    group_by(locale) %>%
    top_n(accOverall, n=1) %>%
    arrange(-accOverall)

# Find the worst match by locale
allAccuracy %>%
    group_by(locale) %>%
    top_n(-accOverall, n=1) %>%
    arrange(accOverall)

# Overall accuracy by location plot
allAccuracy %>%
    group_by(locale) %>%
    summarize_if(is.numeric, mean) %>%
    ggplot(aes(x=fct_reorder(locale, accOverall), y=accOverall)) + 
    geom_point(size=2) + 
    geom_text(aes(y=accOverall+0.02, label=paste0(round(100*accOverall), "%"))) +
    coord_flip() + 
    labs(x="", 
         y="Average Accuracy", 
         title="Average Accuracy Predicting Locale",
         subtitle="Predictions made 1:1 to each other locale (average accuracy reported)",
         caption="Temperature and Dew Point as only predictors\n(50% is baseline coinflip accuracy)"
         ) + 
    ylim(c(0.5, 1))

# Overall accuracy heatmap
allAccuracy %>% 
    ggplot(aes(x=locale, y=other)) + 
    geom_tile(aes(fill=accOverall)) + 
    theme(axis.text.x=element_text(angle=90)) + 
    scale_fill_continuous("Accuracy", high="darkblue", low="white") + 
    labs(title="Accuracy Predicting Locale vs. Locale", 
         caption="Temperature and Dew Point as only predictors\n(50% is baseline coinflip accuracy)",
         x="",
         y=""
         )

```
  
One hypothesis is that adding month as a predictor may help for distinguishing cities in different climates.  For example, summer in the warm season in Detroit may look like spring/fall in Houston or New Orleans:  
```{r cache=TRUE}

# Create a container list to hold the output
list_2016_TempF_DewF_month <- vector("list", 0.5*length(names_2016)*(length(names_2016)-1))

n <- 1
for (ctr in 1:(length(names_2016)-1)) {
    for (ctr2 in (ctr+1):length(names_2016)) {
        list_2016_TempF_DewF_month[[n]] <- rfTwoLocales(metarData, 
                                                        loc1=names_2016[ctr], 
                                                        loc2=names_2016[ctr2], 
                                                        vrbls=c("TempF", "DewF", "month"),
                                                        ntree=25
                                                        )
        n <- n + 1
    }
}

```
  
Accuracy can then be assessed:  
```{r}

# Create a tibble from the underlying accuracy data
acc_TempF_DewF_month <- map_dfr(list_2016_TempF_DewF_month, .f=helperAccuracyLocale)

# Assess the top 10 accuracy and the bottom 10 accuracy
acc_TempF_DewF_month %>%
    arrange(-accOverall) %>%
    head(10)
acc_TempF_DewF_month %>%
    arrange(accOverall) %>%
    head(10)


allAccuracy_month <- select(acc_TempF_DewF_month, 
                            locale=locale1, 
                            other=locale2, 
                            accOverall, 
                            accLocale=accLocale1
                            ) %>%
    bind_rows(select(acc_TempF_DewF_month, 
                     locale=locale2, 
                     other=locale1, 
                     accOverall, 
                     accLocale=accLocale2
                     )
              )

# Find the best match by locale
allAccuracy_month %>%
    group_by(locale) %>%
    top_n(accOverall, n=1) %>%
    arrange(-accOverall)

# Find the worst match by locale
allAccuracy_month %>%
    group_by(locale) %>%
    top_n(-accOverall, n=1) %>%
    arrange(accOverall)

# Overall accuracy by location plot
allAccuracy_month %>%
    group_by(locale) %>%
    summarize_if(is.numeric, mean) %>%
    ggplot(aes(x=fct_reorder(locale, accOverall), y=accOverall)) + 
    geom_point(size=2) + 
    geom_text(aes(y=accOverall+0.02, label=paste0(round(100*accOverall), "%"))) +
    coord_flip() + 
    labs(x="", 
         y="Average Accuracy", 
         title="Average Accuracy Predicting Locale",
         subtitle="Predictions made 1:1 to each other locale (average accuracy reported)",
         caption="Temperature and Dew Point as only predictors\n(50% is baseline coinflip accuracy)"
         ) + 
    ylim(c(0.5, 1))

# Overall accuracy heatmap
allAccuracy_month %>% 
    ggplot(aes(x=locale, y=other)) + 
    geom_tile(aes(fill=accOverall)) + 
    theme(axis.text.x=element_text(angle=90)) + 
    scale_fill_continuous("Accuracy", high="darkblue", low="white") + 
    labs(title="Accuracy Predicting Locale vs. Locale", 
         caption="Temperature and Dew Point as only predictors\n(50% is baseline coinflip accuracy)",
         x="",
         y=""
         )

```
  
Accuracies are meaningfully higher.  Of interest, there is a new group of highly differentiated locales, including New Orleans/Houston vs. Traverse City/Minneapolis.  As hypothesized, adding month significantly aids in differentiating wintry cities from hot-humid cities, as summer in a wintry city may otherwise be hard to distinguish from spring/fall in a hot-humid city.

In fact, with the exception of Houston vs. New Orleans, there appears to be very good differentiation for each of Las Vegas, San Diego, Hoston, and New Orleans from all other locales.

Change in accuracy can also be plotted:  
```{r}

# Pull the accuracies for the two different models
x1 <- allAccuracy %>%
    group_by(locale) %>%
    summarize(acc1=mean(accOverall))
x2 <- allAccuracy_month %>%
    group_by(locale) %>%
    summarize(acc2=mean(accOverall))

# Merge accuracy data and plot
x1 %>%
    inner_join(x2, by="locale") %>%
    ggplot(aes(x=fct_reorder(locale, acc2))) + 
    geom_point(aes(y=acc2), size=2) +
    geom_point(aes(y=acc1), size=1) + 
    geom_text(aes(y=acc2+0.02, label=paste0(round(100*acc2), "%"))) +
    geom_text(aes(y=acc1-0.02, label=paste0(round(100*acc1), "%")), size=3) +
    geom_segment(aes(xend=fct_reorder(locale, acc2), y=acc1, yend=acc2-0.005), 
                 arrow=arrow(length=unit(0.3, "cm"), type="closed")
                 ) +
    coord_flip() + 
    labs(x="", 
         y="Average Accuracy", 
         title="Average Accuracy Predicting Locale",
         subtitle="Predictions made 1:1 to each other locale (average accuracy reported)",
         caption="Temperature, Dew Point, month as predictors\n(50% is baseline coinflip accuracy)"
         ) + 
    ylim(c(0.5, 1))

```
  
The increase in accuracy is particularly striking for New Orleans and Houston.

Next, the simple model is run to classify locale across the full dataset.  The rfTwoLocales() function can be called in a slightly modified manner for this purpose:  
```{r}

# Run random forest for multiple locales
rfMultiLocale <- function(tbl, 
                          vrbls,
                          locs=NULL, 
                          locVar="source", 
                          otherVar="dtime",
                          pred="locale", 
                          seed=NULL, 
                          ntree=100, 
                          mtry=NULL
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble
    # vrbls: explanatory variables for modeling
    # locs: the locations to use (NULL means all)
    # locVar: the name of the variable where locs can be found
    # otherVar: other variables to be kept, but not used in modeling
    # pred: predictor variable for modeling
    # seed: the random seed (NULL means no seed)
    # ntree: the number of trees to grow in the random forest
    # mtry: the splitting parameter for the random forest (NULL means use all variables)    
    
    # Create locs if it has not been passed
    if (is.null(locs)) {
        locs <- tbl %>% pull(locVar) %>% unique() %>% sort()
        cat("\nRunning for locations:\n")
        print(locs)
    }
    
    # Pass to rfTwoLocales
    rfOut <- rfTwoLocales(tbl, 
                          loc1=locs, 
                          loc2=c(), 
                          locVar=locVar,
                          otherVar=otherVar, 
                          vrbls=vrbls, 
                          pred=pred, 
                          seed=seed, 
                          ntree=ntree, 
                          mtry=mtry
                          )
    
    # Return the list object
    rfOut
    
}

```
  
The function can then be applied to the 2016 data:  
```{r cache=TRUE}

# Run random forest for all 2016 locales
rf_all_2016_TDm <- rfMultiLocale(metarData, 
                                 vrbls=c("TempF", "DewF", "month"),
                                 locs=names_2016, 
                                 ntree=50, 
                                 seed=2006201341
                                 )

```
  
Summaries can then be created for the accuracy in predicting each locale:  
```{r}

# Create summary of accuracy
all2016Accuracy <- rf_all_2016_TDm$testData %>%
    mutate(locale=factor(locale, levels=levels(predicted))) %>%
    count(locale, predicted, correct) %>%
    group_by(locale) %>%
    mutate(pct=n/sum(n)) %>%
    ungroup()

# Calculate the number of levels
nLevels <- length(levels(factor(all2016Accuracy$locale)))
nullAcc <- 1 / nLevels

# Create plot for overall accuracy
all2016Accuracy %>%
    filter(locale==predicted) %>%
    ggplot(aes(x=fct_reorder(locale, pct))) + 
    geom_point(aes(y=pct), size=2) + 
    geom_text(aes(y=pct+0.04, label=paste0(round(100*pct), "%"))) +
    geom_hline(aes(yintercept=nullAcc), lty=2) +
    coord_flip() + 
    ylim(0, 1) + 
    labs(x="", 
         y="Correctly Predicted", 
         title="Accuracy of Locale Predictions", 
         subtitle="(positive detection rate by locale)", 
         caption=paste0("Temperature, Dew Point, Month as predictors\n(", 
                        round(100*nullAcc), 
                        "% is baseline null accuracy)"
                        )
         )

# Order locales sensibly
locOrder <- all2016Accuracy %>%
    filter(correct) %>%
    arrange(pct) %>%
    pull(locale)

# Create plot for which locales are classified as each other
all2016Accuracy %>%
    mutate(locale=factor(locale, levels=locOrder), 
           predPretty=factor(str_replace(predicted, pattern=", ", replacement="\n"), 
                             levels=str_replace(locOrder, pattern=", ", replacement="\n")
                             )
           ) %>%
    ggplot(aes(y=locale, x=predPretty)) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("% Predicted As", low="white", high="green") + 
    scale_x_discrete(position="top") +
    theme(axis.text.x=element_text(angle=90)) + 
    labs(x="",
         y="Actual Locale", 
         title="Predicted Locale vs. Actual Locale", 
         caption="Temperature, Dew Point, Month as predictors"
         )

```
  
The model is fairly accurate in predicting Las Vegas and San Diego.  The model frequently predicts Houston and New Orleans as one of the group, but often classifying one locale as the other.

Accuracy is much lower for many of the cold wether cities.  While there is improvement relative to the null accuracy, there is significant overlap in the temperature and dew point by month.

Clouds (minimum levels and ceiling heights) can potentially help further differentiate the cold weather cities on the downwind side of the lake, as well as helping to further pull apart Las Vegas (almost always clear) and San Diego (frequent marine layer).

A helper function is built to map cloud heights to buckets, then minimum cloud height and ceiling are added to the model:  
```{r}

# Convert cloud height to buckets and factor
mapCloudHeight <- function(x) {
    
    factor(case_when(x==-100 ~ "None", 
                     x <= 1000 ~ "Surface", 
                     x <= 3000 ~ "Low", 
                     x <= 6000 ~ "Medium", 
                     x <= 12000 ~ "High", 
                     TRUE ~ "Error"
                     ), 
           levels=c("Surface", "Low", "Medium", "High", "None")
           )
    
}

# Apply to metarData and keep variables of interest
modData <- metarData %>%
    mutate(minHeight=mapCloudHeight(minHeight), ceilingHeight=mapCloudHeight(ceilingHeight))

```
  
An updated random forest model is then run:  
```{r cache=TRUE}

# Run random forest for all 2016 locales
rf_all_2016_TDmc <- rfMultiLocale(modData, 
                                  vrbls=c("TempF", "DewF", "month", "minHeight", "ceilingHeight"),
                                  locs=names_2016, 
                                  ntree=50, 
                                  seed=2006201355
                                  )

```
  
The evaluation process is converted to a function:  
```{r}

# Evaluate model predictions
evalPredictions <- function(lst, 
                            plotCaption
                            ) {

    # FUNCTION ARGUMENTS:
    # lst: the list containing outputs of the modeling
    # plotCaption: description of predictors used
    
    # Create summary of accuracy
    all2016Accuracy <- lst$testData %>%
        mutate(locale=factor(locale, levels=levels(predicted))) %>%
        count(locale, predicted, correct) %>%
        group_by(locale) %>%
        mutate(pct=n/sum(n)) %>%
        ungroup()

    # Calculate the number of levels
    nLevels <- length(levels(factor(all2016Accuracy$locale)))
    nullAcc <- 1 / nLevels

    # Create plot for overall accuracy
    p1 <- all2016Accuracy %>%
        filter(locale==predicted) %>%
        ggplot(aes(x=fct_reorder(locale, pct))) + 
        geom_point(aes(y=pct), size=2) + 
        geom_text(aes(y=pct+0.04, label=paste0(round(100*pct), "%"))) +
        geom_hline(aes(yintercept=nullAcc), lty=2) +
        coord_flip() + 
        ylim(0, 1) + 
        labs(x="", 
             y="Correctly Predicted", 
             title="Accuracy of Locale Predictions", 
             subtitle="(positive detection rate by locale)", 
             caption=paste0(plotCaption, 
                            " as predictors\n(", 
                            round(100*nullAcc), 
                            "% is baseline null accuracy)"
                            )
             )
    print(p1)

    # Order locales sensibly
    locOrder <- all2016Accuracy %>%
        filter(correct) %>%
        arrange(pct) %>%
        pull(locale)

    # Create plot for which locales are classified as each other
    p2 <- all2016Accuracy %>%
        mutate(locale=factor(locale, levels=locOrder), 
               predPretty=factor(str_replace(predicted, pattern=", ", replacement="\n"), 
                                 levels=str_replace(locOrder, pattern=", ", replacement="\n")
                                 )
               ) %>%
        ggplot(aes(y=locale, x=predPretty)) + 
        geom_tile(aes(fill=pct)) + 
        geom_text(aes(label=paste0(round(100*pct), "%"))) + 
        scale_fill_continuous("% Predicted As", low="white", high="green") + 
        scale_x_discrete(position="top") +
        theme(axis.text.x=element_text(angle=90)) + 
        labs(x="",
             y="Actual Locale", 
             title="Predicted Locale vs. Actual Locale", 
             caption=paste0(plotCaption, " as predictors")
             )
    print(p2)
    
    # Return the accuracy object
    all2016Accuracy
    
}

evalPredictions(rf_all_2016_TDmc, plotCaption = "Temp, Dew Point, Month, Cloud/Ceiling Height")

```
  
Accuracy increases, especially for San Diego and several of the cold wether cities.
  
The model can also be built out to consider wind speed and wind direction.  No attempt yet is made to control for over-fitting, which becomes especially likely with wind direction classified by every 10 degrees:  
```{r cache=TRUE}

# Run random forest for all 2016 locales
rf_all_2016_TDmcw <- rfMultiLocale(modData, 
                                   vrbls=c("TempF", "DewF", 
                                           "month", 
                                           "minHeight", "ceilingHeight", 
                                           "WindSpeed", "predomDir"
                                           ),
                                   locs=names_2016, 
                                   ntree=50, 
                                   seed=2006201355
                                   )

```
  
The evaluation process can again be run:  
```{r}

evalPredictions(rf_all_2016_TDmcw, 
                plotCaption = "Temp, Dew Point, Month, Cloud/Ceiling Height, Wind Speed/Direction"
                )

```
  
Including wind significantly improves model accuracy for most locales.  Even the cold weather cities are now being predicted with 33%-50% accurcay.
  
Next steps are to explore the prediction accuracy by month, hour, etc., to see if there is any low-hanging fruit for further modifications to the model.

