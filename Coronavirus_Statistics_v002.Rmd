---
title: "Coronavirus US Data"
author: "davegoblue"
date: "10/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```

## Background
This file is for analysis of several coronavirus data sources focused on the US in 2020.  Data include areas such as cases, hospitalizations, and deaths tracked to cornavirus, as well as all-cause deaths.

The previous version of this file includes many exploratory analysis components.  This version is designed with the following portions of the previous code:  
  
1.  Functions  
2.  Run key analyses  
3.  Save and load key files  
  
### _COVID Tracking Project_ 
The first section is for analysis of data from [The COVID Tracking Project](https://covidtracking.com/).  This file contains data on positive tests, hospitalizations, deaths, and the like for coronavirus cases in the US.  Downloaded data are unique by state and date.

#### _Key Functions (COVID Tracking Project, USA Facts, CDC All-Cause Deaths)_  
Functions for working with data from the COVID Tracking Project, USA Facts, and the CDC All-Cause Death data are available in a separate file which is sourced below:  
```{r}

source("./Coronavirus_Statistics_Functions_v002.R")

```

### _Running Key Analyses (COVID Tracking Project)_  
The data from COVID Tracking Project can be loaded and analyzed using the functions above and a variable mapping file:  
```{r cache=TRUE}

# STEP 0: Create a variable mapping file
varMapper <- c("cases"="Cases", 
               "newCases"="Increase in cases, most recent 30 days",
               "casesroll7"="Rolling 7-day mean cases", 
               "deaths"="Deaths", 
               "newDeaths"="Increase in deaths, most recent 30 days",
               "deathsroll7"="Rolling 7-day mean deaths", 
               "cpm"="Cases per million",
               "cpm7"="Cases per day (7-day rolling mean) per million", 
               "newcpm"="Increase in cases, most recent 30 days, per million",
               "dpm"="Deaths per million", 
               "dpm7"="Deaths per day (7-day rolling mean) per million", 
               "newdpm"="Increase in deaths, most recent 30 days, per million", 
               "hpm7"="Currently Hospitalized per million (7-day rolling mean)", 
               "tpm"="Tests per million", 
               "tpm7"="Tests per million per day (7-day rolling mean)"
               )


# Test function for hierarchical clustering with Vermont reassigned to New Hampshire
test_hier5 <- readRunCOVIDTrackingProject(thruLabel="Aug 20, 2020", 
                                          readFrom="./RInputFiles/Coronavirus/CV_downloaded_200820.csv", 
                                          hierarchical=TRUE,
                                          kCut=6, 
                                          reAssignState=list("VT"="NH"), 
                                          minShape=3, 
                                          ratioDeathvsCase = 5, 
                                          ratioTotalvsShape = 0.5, 
                                          minDeath=100, 
                                          minCase=10000
                                          )


# Test function for k-means clustering using the per capita data file previously created
test_km5 <- readRunCOVIDTrackingProject(thruLabel="Aug 20, 2020", 
                                        dfPerCapita=test_hier5$dfPerCapita,
                                        hierarchical=FALSE,
                                        makeCumulativePlots=FALSE,
                                        minShape=3, 
                                        ratioDeathvsCase = 5, 
                                        ratioTotalvsShape = 0.5, 
                                        minDeath=100, 
                                        minCase=10000, 
                                        nCenters=5,
                                        testCenters=1:10, 
                                        iter.max=20,
                                        nstart=10, 
                                        seed=2008261400
                                        )

  
# Run in a session that has access to the old data files
# identical(test_hier5$useClusters, clustVec)  # TRUE
# identical(test_km5$useClusters, testCluster_km5$objCluster$cluster)  # TRUE

```
  
Cluster files produced using the new functions and the existing August 20, 2020 data are identical to 'clustVec' and 'testCluster_km5' produced in the _v001 code.
  
Further, new data is downloaded that is current through September 30, 2020, and the test_hier5 clusters are used for analysis:  
```{r cache=TRUE}

# Test function for hierarchical clustering with Vermont reassigned to New Hampshire
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_201001.csv"
test_hier5_201001 <- readRunCOVIDTrackingProject(thruLabel="Sep 30, 2020", 
                                                 readFrom=locDownload, 
                                                 compareFile=test_hier5$dfRaw,
                                                 useClusters=test_hier5$useClusters
                                                 )

```
  
Segments appear to be on trend with previous analysis, with what was previously called "late pandemic" having peaked and the segments that had smaller deaths per million showing higher percentage increases.
  

### _USA Facts_ 
The second section is for analysis of data from [USA Facts](https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/).  This site contains county-level data for the US focused on coronavirus cases and deaths.  Data are provided with one row per county and one column for each date, and are thus unique by county.  There are differences in state and US totals for coronavirus cases and deaths when reported by COVID Tracking Project and USA Facts, though trends, timing, and relative magnitudes are generally in agreement between the two sources.
  
#### _Key Functions (USA Facts)_  
Functions for reading and analyzing data from USA Facts are sourced above from Coronavirus_Statistics_Functions_v002.R
  
### _Running Key Analyses (USA Facts)_  
The data from USA Facts can be loaded and analyzed using the sourced functions, a variable mapping file, and a main function that calls the other functions and returns a list:  
```{r cache=TRUE}

# STEP 1a: Define the locations for the population, cases, and deaths file
popFile <- "./RInputFiles/Coronavirus/covid_county_population_usafacts.csv"
caseFile_20200903 <- "./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20200903.csv"
deathFile_20200903 <- "./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20200903.csv"
maxDate_20200903 <- "2020-08-31"

# STEP 1b: Read and pivot data from USA Facts; extract population data file as pop_usafacts
rawUSAFacts_20200903 <- readPivotUSAFacts(popFile=popFile, 
                                          caseFile=caseFile_20200903, 
                                          deathFile=deathFile_20200903, 
                                          unassignedDate=maxDate_20200903
                                          )
pop_usafacts <- rawUSAFacts_20200903$pop

# STEP 2: Read case and death data (redundant), combine, and add population totals; no clusters by default
burden_20200903_new <- readUSAFacts(
    caseFile=caseFile_20200903, 
    deathFile=deathFile_20200903, 
    stateClusters=NULL
    )


# STEP 3: Explore the cases and deaths by county (can be repeated for other counties)
casesDeathsByCounty(useDate=maxDate_20200903, 
                    inclStates=c("FL", "GA", "SC", "AL", "MS"), 
                    caseData=rawUSAFacts_20200903$cases, 
                    deathData=rawUSAFacts_20200903$deaths,
                    highCaseAmount=80000, 
                    highDeathAmount=2000
                    )

# STEP 4: Create county-level clusters (k-means, 5 clusters, minimum county population 25k)
clust_20200903_new <- prepClusterCounties(burdenFile=burden_20200903_new, 
                                          maxDate=maxDate_20200903, 
                                          minPop=25000, 
                                          hierarchical=FALSE, 
                                          minShape=3,
                                          ratioDeathvsCase = 5,
                                          ratioTotalvsShape = 0.5,
                                          minDeath=100,
                                          minCase=5000,
                                          nCenters=5,
                                          testCenters=1:25,
                                          iter.max=20,
                                          nstart=10,
                                          seed=2009081450
                                          )


# STEP 5: Extract the clusters from the clustering object
clustVec_county_20200903_new <- clust_20200903_new$objCluster$objCluster$cluster

# STEP 6: Assess the quality of the new clusters
helperACC_county_20200903_new <- helperAssessCountyClusters(clustVec_county_20200903_new, 
                                                            dfPop=clust_20200903_new$countyFiltered, 
                                                            dfBurden=clust_20200903_new$countyFiltered, 
                                                            thruLabel="Sep 3, 2020", 
                                                            plotsTogether=TRUE, 
                                                            orderCluster=TRUE
                                                            )

# STEP 7: Create a plot of cumulative burden by cluster
helperACC_county_20200903_new %>%
    select(cluster, date, pop, cases, deaths) %>%
    group_by(cluster, date) %>%
    summarize_if(is.numeric, sum, na.rm=TRUE) %>%
    arrange(date) %>%
    mutate(cpmcum=cumsum(cases)*1000000/pop, dpmcum=cumsum(deaths)*1000000/pop) %>%
    ungroup() %>%
    select(cluster, date, cases=cpmcum, deaths=dpmcum) %>%
    pivot_longer(-c(cluster, date)) %>%
    ggplot(aes(x=date, y=value, color=cluster)) + 
    geom_line(size=1) + 
    geom_text(data=~filter(., date==max(date)), 
              aes(x=date+lubridate::days(2), label=round(value)), 
              size=3, 
              hjust=0
              ) +
    labs(x="", title="Cumulative burden per million people by segment", y="") +
    facet_wrap(~c("cases"="Cases per million", "deaths"="Deaths per million")[name], scales="free_y") + 
    scale_x_date(date_breaks="1 months", date_labels="%b", expand=expand_scale(c(0, 0.1)))

# STEP 8: Add back clusters not used for analysis (code 999) and associated disease data
clusterStateData_20200903_new <- helperMakeClusterStateData(helperACC_county_20200903_new, 
                                                            dfBurden=clust_20200903_new$countyDailyPerCapita,
                                                            orderCluster=TRUE
                                                            )

# STEP 9: Run an example state-level summary (can expand to other states)
stateCountySummary(states=c("MN", "ND", "SD", "WI"),
                   df=changeOrderLabel(clusterStateData_20200903_new, grpVars="fipsCounty"),
                   keyDate=maxDate_20200903,
                   showQuadrants=TRUE, 
                   showCumulative=TRUE, 
                   facetCumulativeByState = TRUE, 
                   showAllFactorLevels = TRUE
                   )

# STEP 10a: Read in updated raw data
caseFile_20200917 <- "./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20200917.csv"
deathFile_20200917 <- "./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20200917.csv"
maxDate_20200917 <- "2020-09-15"

rawUSAFacts_20200917 <- readPivotUSAFacts(popFile=popFile, 
                                          caseFile=caseFile_20200917, 
                                          deathFile=deathFile_20200917, 
                                          unassignedDate=maxDate_20200917
                                          )


# STEP 10b: Use existing state-level segments with updated raw data
burden_20200917 <- readUSAFacts(
    caseFile=caseFile_20200917, 
    deathFile=deathFile_20200917,
    stateClusters=test_hier5_201001$useClusters
    )

# STEP 11: Compare burden in old data and new data
bind_rows(burden_20200903_new, burden_20200917, .id="source") %>%
    mutate(source=factor(case_when(source==1 ~ "2020-09-03", source==2 ~ "2020-09-17", TRUE ~ "Unknown"), 
                         levels=c("2020-09-17", "2020-09-03", "Unknown")
                         )
           ) %>%
    group_by(source, date) %>%
    summarize(cumDeaths=sum(cumDeaths), cumCases=sum(cumCases)) %>%
    pivot_longer(-c(source, date)) %>%
    ggplot(aes(x=date, y=value/1000, group=source, color=source)) + 
    geom_line() + 
    facet_wrap(~c("cumCases"="Cases", "cumDeaths"="Deaths")[name], scales="free_y") + 
    scale_x_date(date_breaks="1 months", date_labels="%m") + 
    labs(y="Burden (000s)", title="US National Coronavirus Burden by Source")

# STEP 12: Show top-level findings by segment
plotBurdenData(burden_20200917, maxDate=maxDate_20200917, minPop=10000)

# STEP 13: Create county-level clusters (k-means, 5 clusters, minimum county population 25k)
clust_20200917_new <- prepClusterCounties(burdenFile=burden_20200917, 
                                          maxDate=maxDate_20200917, 
                                          minPop=25000, 
                                          hierarchical=FALSE, 
                                          minShape=3,
                                          ratioDeathvsCase = 5,
                                          ratioTotalvsShape = 0.5,
                                          minDeath=100,
                                          minCase=5000,
                                          nCenters=5,
                                          testCenters=1:25,
                                          iter.max=20,
                                          nstart=10,
                                          seed=2009081450
                                          )

# STEP 14: Assess the previous clusters on the updated data
helper_test_20200917 <- helperAssessCountyClusters(vecCluster=clustVec_county_20200903_new, 
                                                   dfPop=clust_20200917_new$countyFiltered, 
                                                   dfBurden=clust_20200917_new$countyFiltered, 
                                                   thruLabel="Sep 15, 2020", 
                                                   plotsTogether=TRUE, 
                                                   showMap=TRUE, 
                                                   clusterPlotsTogether=TRUE, 
                                                   orderCluster=TRUE
                                                   )

# STEP 15: Create normalized data
cNorm_20200917 <- helperMakeNormData(helper_test_20200917)

# STEP 16: Assess lags for early pandemic and late pandemic
lagData_early_20200917 <- helperTestLags(cNorm_20200917, minDate="2020-03-01", maxDate="2020-05-31")
lagData_late_20200917 <- helperTestLags(cNorm_20200917, minDate="2020-06-01", maxDate="2020-09-15")

# STEP 17: Generate a list of key counties
keyCounties_20200917 <- helper_test_20200917 %>%
    mutate(state=str_pad(state, width=5, side="left", pad="0")) %>%
    filter(pop >= 100000) %>%
    group_by(state, cluster) %>%
    summarize(dpm=sum(dpm), pop=mean(pop)) %>%
    group_by(cluster) %>%
    top_n(n=3, wt=dpm) %>%
    ungroup() %>%
    arrange(cluster, -dpm) %>%
    inner_join(select(usmap::countypop, -pop_2015), by=c("state"="fips")) %>%
    mutate(countyName=paste0(cluster, " - ", 
                             stringr::str_replace(county, "County|Parish", "("), 
                             abbr, 
                             ")"
                             )
           ) %>%
    select(-abbr, -county)

# STEP 18: Keep only key counties
cNorm_keyCounties_20200917 <- helper_test_20200917 %>%
    mutate(state=str_pad(state, width=5, side="left", pad="0")) %>%
    inner_join(select(keyCounties_20200917, state, countyName), by=c("state"="state")) %>%
    helperMakeNormData(aggBy=c("countyName", "state", "cluster"))

# STEP 19: Create early and late lags for key counties
lagData_early_keycounties_20200917 <- helperTestLags(cNorm_keyCounties_20200917, 
                                                     minDate="2020-03-01", 
                                                     maxDate="2020-05-31", 
                                                     aggBy=c("countyName", "state", "cluster"), 
                                                     maxRatio=0.25
                                                     )
lagData_late_keycounties_20200917 <- helperTestLags(cNorm_keyCounties_20200917, 
                                           minDate="2020-06-01", 
                                           maxDate="2020-09-15", 
                                           aggBy=c("countyName", "state", "cluster"), 
                                           maxRatio=0.25
                                           )

# STEP 20: Run for only key counties
exploreTopCounties(helper_test_20200917, minDate="2020-03-01", maxDate="2020-05-31", nVar="pop", nKey=12)
exploreTopCounties(helper_test_20200917, minDate="2020-06-15", maxDate="2020-09-15", nVar="pop", nKey=12)

```
  
The process is further converted to functional form as follows:  
  
1.  Capability to either start the process from scratch or to leverage existing clusters at the state or county level  
2.  Add the capability to compare a newly read file to an existing file  
3.  Add the capability to show burden by existing segment after reading in an existing file  
4.  Add the capability to prepare the data (create countyFiltered) without running the clustering proces  
5.  Add the cumulative burden chart to helperAssessCountyClusters  
  
New data are downloaded from 2020-10-03, and the updated code includes:  
```{r cache=TRUE}

# STEP 1a: Define the locations for the population, cases, and deaths file
popFile <- "./RInputFiles/Coronavirus/covid_county_population_usafacts.csv"
caseFile_20201003 <- "./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20201003.csv"
deathFile_20201003 <- "./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20201003.csv"
maxDate_20201003 <- "2020-09-30"

# STEP 1b: Read and pivot data from USA Facts; extract population data file as pop_usafacts
rawUSAFacts_20201003 <- readPivotUSAFacts(popFile=popFile, 
                                          caseFile=caseFile_20201003, 
                                          deathFile=deathFile_20201003, 
                                          unassignedDate=maxDate_20201003
                                          )
pop_usafacts <- rawUSAFacts_20201003$pop

# STEP 2: Read case and death data (redundant), combine, and add population totals
# Add previous clusters by county rather than by state (function readUSAFacts updated)
burden_20201003 <- readUSAFacts(
    caseFile=caseFile_20201003, 
    deathFile=deathFile_20201003, 
    oldFile=burden_20200903_new,
    showBurdenMinPop=10000,
    maxDate=maxDate_20201003,
    stateClusters=NULL, 
    countyClusters=clustVec_county_20200903_new
    )

# STEP 3: Create appropriately filtered data without creating new clusters
clust_20201003 <- prepClusterCounties(burdenFile=burden_20201003, 
                                      maxDate=maxDate_20201003, 
                                      minPop=25000,
                                      createClusters=FALSE
                                      )

# STEP 4: Assess the existing clusters against the new data, including the cumulative plot
helperACC_county_20201003 <- helperAssessCountyClusters(vecCluster=clustVec_county_20200903_new, 
                                                        dfPop=clust_20201003$countyFiltered, 
                                                        dfBurden=clust_20201003$countyFiltered, 
                                                        showCum=TRUE,
                                                        thruLabel="Sep 30, 2020", 
                                                        plotsTogether=TRUE, 
                                                        orderCluster=TRUE
                                                        )

# STEP 5: Add back clusters not used for analysis (code 999) and associated disease data
clusterStateData_20201003 <- helperMakeClusterStateData(helperACC_county_20201003, 
                                                        dfBurden=clust_20201003$countyDailyPerCapita,
                                                        orderCluster=TRUE
                                                        )

# STEP 6: Run an example state-level summary using several of the largest states
stateCountySummary(states=c("CA", "TX", "FL", "NY", "IL", "PA"),
                   df=changeOrderLabel(clusterStateData_20201003, grpVars="fipsCounty"),
                   keyDate=maxDate_20201003,
                   showQuadrants=TRUE, 
                   showCumulative=TRUE, 
                   facetCumulativeByState = TRUE, 
                   showAllFactorLevels = TRUE
                   )

# STEP 7: Assess lags for largest counties and counties with highest death rates
# STEP 7a: Save data for top-100 counties by death rate (filtered to only counties of 100k+ population)
top100_dpm_20201003 <- exploreTopCounties(helperACC_county_20201003, 
                                          minDate="2020-03-01", 
                                          maxDate="2020-09-30", 
                                          minPop=100000,
                                          nVar="dpm", 
                                          nKey=100, 
                                          plotData=FALSE
                                          )
glimpse(top100_dpm_20201003)

# STEP 7b: Top-3 by population by cluster with plots
exploreTopCounties(helperACC_county_20201003, 
                   minDate="2020-03-01", 
                   maxDate="2020-09-30", 
                   topNBy="cluster",
                   nVar="pop", 
                   nKey=3, 
                   plotData=TRUE
                   )

# STEP 7c: Top-5 by dpm by cluster with plots
exploreTopCounties(helperACC_county_20201003, 
                   minDate="2020-03-01", 
                   maxDate="2020-09-30", 
                   topNBy="cluster",
                   nVar="dpm", 
                   nKey=5, 
                   plotData=TRUE
                   )

```
  
### _CDC All-Cause Deaths_ 
The CDC maintain public data for all-cause deaths by week and jurisdiction, available at [CDC Weekly Deaths by Jurisdiction](https://catalog.data.gov/dataset/weekly-counts-of-deaths-by-jurisdiction-and-age-group).  

These data are known to have a lag between death and reporting, and the CDC back-correct to report deaths at the time the death occurred even if the death is reported in following weeks.  This means totals for recent weeks tend to run low (lag), and the CDC run a projection of the expected total number of deaths given the historical lag times.  Per other analysts on the internet, there is currently significant supra-lag, with lag times much longer than historical averages causing CDC projected deaths for recent weeks to be low.

#### _Key Functions (CDC All-Cause Deaths)_  
Functions for reading and analyzing CDC all-cause deaths data have been sourced above from Coronavirus_Statistics_Functions_v002.R.
  
### _Running Key Analyses (CDC All-Cause Deaths)_  
The CDC data can be loaded and analyzed:  
```{r cache=TRUE}

# Read and process the CDC data
cdc20200923 <- readProcessCDC("Weekly_counts_of_deaths_by_jurisdiction_and_age_group_downloaded_20200923.csv", 
                              weekThru=30
                              )

# Generate plots of the processed CDC data
cdcBasicPlots(cdc20200923, clustVec=test_hier5$useClusters)


# Example cohort analysis for age 65+
list_65plus <- cdcCohortAnalysis(cohortName="65+ years old", 
                                 df=cdc20200923,
                                 critFilter=list("age"=c("65-74 years", 
                                                         "75-84 years", 
                                                         "85 years and older"
                                                         )
                                                 ), 
                                 curYear=2020, 
                                 startYear=2015,
                                 startWeek=9,
                                 plotTitle="All-cause deaths for 65+ years old cohort", 
                                 predActualPlotsOnePage=TRUE
                                 )

# Example cohort analysis for full US
list_allUS <- cdcCohortAnalysis(cohortName="all ages, all states", 
                                df=cdc20200923,
                                curYear=2020, 
                                startYear=2015,
                                startWeek=9,
                                plotTitle="All-cause US total deaths",
                                predActualPlotsOnePage=TRUE
                                )

# Example cohort analysis for handful of state hit early
list_early <- cdcCohortAnalysis(cohortName="all ages, NY/NJ/CT/MA", 
                                df=cdc20200923,
                                critFilter=list("state"=c("NY", "NJ", "CT", "MA")),
                                curYear=2020, 
                                startYear=2015,
                                startWeek=9,
                                plotTitle="All-cause total deaths for NY/NJ/CT/MA",
                                predActualPlotsOnePage=TRUE
                                )

```
  
Next steps are to continue with the CDC aggregates function (analyze by cluster or age or state ot etc.):  
```{r cache=TRUE}

# Testing the aggregation function for cluster
# Need to fix plot chart titles
clusterList_hier5_201001 <- helperKeyStateClusterMetrics(test_hier5_201001)
clusterAgg_20200923 <- cdcAggregateSummary(df=cdc20200923, 
                                           critVar="state", 
                                           critSubsets=clusterList_hier5_201001$stateCluster,
                                           startWeek=9, 
                                           critListNames=paste0("cluster ", 1:5),
                                           factorCritList=FALSE,
                                           popData=clusterList_hier5_201001$pop,
                                           cvDeathData=clusterList_hier5_201001$deaths,
                                           idVarName="cluster"
                                           )


# Testing the aggregation function for state (no plots)
stateAgg_20200923 <- cdcAggregateSummary(df=cdc20200923, 
                                         critVar="state", 
                                         critSubsets=names(clusterList_hier5_201001$clData),
                                         startWeek=9, 
                                         idVarName="state", 
                                         subListNames=names(clusterList_hier5_201001$clData),
                                         showAllPlots=FALSE
                                         )


# Create a mapping of epiweek to month (use 2020 for this)
epiMonth <- tibble::tibble(dt=as.Date("2020-01-01")+0:365, 
                           month=lubridate::month(dt),
                           quarter=lubridate::quarter(dt),
                           ew=lubridate::epiweek(dt)
                           ) %>%
    count(ew, month, quarter) %>%
    arrange(ew, -n) %>%
    group_by(ew) %>%
    summarize(month=factor(month.abb[first(month)], levels=month.abb), quarter=first(quarter))

# Create plots by state
helperKeyStateExcessPlots(df=stateAgg_20200923, 
                          epiMonth=epiMonth,
                          cvDeaths=test_hier5_201001$consolidatedPlotData,
                          startWeek=10,
                          cvDeathDate=as.Date("2020-07-31"),
                          subT="CDC data through July 2020 (Q3 incomplete)"
                          )


# Testing the aggregation function for age (no plots)
ageAgg_20200923 <- cdcAggregateSummary(df=cdc20200923, 
                                       critVar="age", 
                                       critSubsets=levels(cdc20200923$age),
                                       startWeek=9, 
                                       idVarName="age", 
                                       subListNames=levels(cdc20200923$age),
                                       showAllPlots=TRUE
                                       )


# Estimated US population by age (2020)
usPopAge2020 <- survival::uspop2[, , "2020"] %>%
    apply(1, FUN=sum) %>%
    tibble::tibble(ageActual=as.integer(names(.)), pop_2020=.)
usPopAge2020

# Sums by age bucket
usPopBucket2020 <- usPopAge2020 %>%
    mutate(age=factor(case_when(ageActual <= 24 ~ "Under 25 years", 
                                ageActual <= 44 ~ "25-44 years", 
                                ageActual <= 64 ~ "45-64 years", 
                                ageActual <= 74 ~ "65-74 years", 
                                ageActual <= 84 ~ "75-84 years", 
                                TRUE ~ "85 years and older"
                                ), levels=levels(cdc20200923$age)
                      )
           ) %>%
    group_by(age) %>%
    summarize(pop=sum(pop_2020))
usPopBucket2020

# Create plots by age
helperKeyAgeExcessPlots(df=ageAgg_20200923, 
                        epiMonth=epiMonth,
                        cvDeaths=test_hier5_201001$consolidatedPlotData,
                        popData=usPopBucket2020,
                        startWeek=10,
                        cvDeathDate=as.Date("2020-07-31"),
                        subT="CDC data through July 2020 (Q3 incomplete)"
                        )


# Create data at the level of elderly and non-elderly, using 65 as the start if elderly
ageOld <- c("65-74 years", "75-84 years", "85 years and older")
cdcTwoAge <- cdc20200923 %>%
    mutate(ageSplit=factor(ifelse(age %in% ageOld, "65 and over", "Under 65"), 
                           levels=c("Under 65", "65 and over")
                           )
           ) %>%
    group_by(state, year, week, ageSplit) %>%
    summarize(deaths=sum(deaths)) %>%
    ungroup() %>%
    mutate(keyVar=paste(ageSplit, state, sep="_"))
cdcTwoAge

# Testing the aggregation function for state and elderly (no plots)
ageStateAgg_20200923 <- cdcAggregateSummary(df=cdcTwoAge, 
                                            critVar="keyVar", 
                                            critSubsets=unique(cdcTwoAge$keyVar),
                                            startWeek=9, 
                                            idVarName="keyVar", 
                                            subListNames=unique(cdcTwoAge$keyVar),
                                            showAllPlots=FALSE
                                            ) %>%
    tidyr::separate(keyVar, into=c("age", "state"), sep="_")

# Plot of excess deaths ratio by state and age
ageStateAgg_20200923 %>%
    filter(year==2020, week>=10) %>%
    group_by(age, state) %>%
    summarize(excess=sum(delta)) %>%
    ggplot(aes(x=fct_reorder(state, excess, .fun=sum), y=excess/1000, fill=age)) + 
    geom_col(position="fill") + 
    coord_flip() + 
    labs(x="State", 
         y="% of All-cause excess deaths", 
         title="Proportion of all-cause excess deaths by age cohort in 2020", 
         subtitle="CDC data March 2020 through July 2020"
         ) +
    scale_fill_discrete("Age Group") + 
    geom_hline(aes(yintercept=0.25), lty=2)

# Plots by age cohort
for (age in levels(cdcTwoAge$ageSplit)) {
    cdcCohortAnalysis(df=cdcTwoAge, 
                      cohortName=paste0("ages ", age), 
                      critFilter=list("ageSplit"=age),
                      plotTitle=paste0("All-cause total deaths for age ", age), 
                      showSubsetPlots=FALSE,
                      showPredActualPlots=TRUE,
                      predActualPlotsOnePage=TRUE
                      )
}

```
  
Next steps are to further clean up and integrate the functions so that the full process can be run with more recent data using fewer changes to the code.
  
### _Running the Full Process_ 
First, data are downloaded from COVID Tracking Project (run only once) without creating clusters (this chunk is set to eval=FALSE to avoid over-writing previously downloaded data):  
```{r eval=FALSE}

# Test function for hierarchical clustering with Vermont reassigned to New Hampshire
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_201010.csv"
test_hier5_201010 <- readRunCOVIDTrackingProject(thruLabel="Oct 9, 2020", 
                                                 downloadTo=locDownload, 
                                                 compareFile=test_hier5_201001$dfRaw,
                                                 useClusters=test_hier5_201001$useClusters
                                                 )


```
  
The segments are recreated from the downloaded data so that evolution in segment type and membership can be explored:  
```{r cache=TRUE}

# Explore the newly downloaded data and the dendrogram if using for clusters
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_201010.csv"
test_hier_201010_dendonly <- readRunCOVIDTrackingProject(thruLabel="Oct 9, 2020", 
                                                         readFrom=locDownload, 
                                                         compareFile=test_hier5_201001$dfRaw,
                                                         hierarchical=TRUE, 
                                                         minShape=3, 
                                                         ratioDeathvsCase = 5, 
                                                         ratioTotalvsShape = 0.5, 
                                                         minDeath=100, 
                                                         minCase=10000, 
                                                         skipAssessmentPlots=TRUE
                                                         )

```
  
The dendrogram reveals similarities in several of the main clusters - a clear early and high group (NJ, NY, CT, MA); a clear late and high group (GA, FL, SC, TX, AL, NV, AZ, MS); a clear early and medium group (MI, DE, PA, IN, IL, IN, MD); 1-2 groups that have had low death rates so far; and two groups (LA, DC, RI) and (VT, ME, CO, WA) that appear different than in previous analyses.

Cutting to seven segments should allow for further exploration of the two stand-alone groups of distinct states while leaving most other states in their previous segments.  The full process is then run:  
```{r cache=TRUE}

locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_201010.csv"
test_hier7_201010 <- readRunCOVIDTrackingProject(thruLabel="Oct 9, 2020", 
                                                 readFrom=locDownload, 
                                                 compareFile=test_hier5_201001$dfRaw,
                                                 hierarchical=TRUE, 
                                                 kCut=7, 
                                                 minShape=3, 
                                                 ratioDeathvsCase = 5, 
                                                 ratioTotalvsShape = 0.5, 
                                                 minDeath=100, 
                                                 minCase=10000
                                                 )

```
  
The LA, DC, RI segment does not merit being on its own.  It has a small population (6 million) and has broadly the same shape, with somewhat higher deaths, as the much larger MI, DE, PA, IN, IL, MD segment.  The number of segments can be cut back to 6 as this is the first segment that would be consolidated by that.

The VT, ME, CO, WA segment also questionably merit stand-alone treatment.  It is a variation of the segment that had mild disease early, but is rather differentiated on the dendrogram and has a population (15 million) that is at least in the same order of magnitude as the other clusters.

Suppose that 6 segments are created, using the parameters "as is":  
```{r cache=TRUE}

locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_201010.csv"
test_hier6_201010 <- readRunCOVIDTrackingProject(thruLabel="Oct 9, 2020", 
                                                 readFrom=locDownload, 
                                                 compareFile=test_hier5_201001$dfRaw,
                                                 hierarchical=TRUE, 
                                                 kCut=6, 
                                                 minShape=3, 
                                                 ratioDeathvsCase = 5, 
                                                 ratioTotalvsShape = 0.5, 
                                                 minDeath=100, 
                                                 minCase=10000
                                                 )

```
  
The segments appear to follow reasonable patterns:  
  
* High impact, skewed early  
* Medium impact, skewed early  
* Medium impact, skewed late  
* Small impact, skewed early (two segments are broadly in this category)  
* Small impact, sustained  
  
Comparisons of segment membership can also be made:  
```{r}

# Confirm that the names 
if (!all.equal(names(test_hier5$useClusters), names(test_hier6_201010$useClusters))) {
    stop("\nIssue with cluster names not matching\n")
}

# Create a data frame of segment changes
segChanges <- tibble::tibble(state=names(test_hier5$useClusters), 
                             oldCluster=test_hier5$useClusters, 
                             newCluster=test_hier6_201010$useClusters
                             )

# Counts of states by oldCluster and newCluster
segChanges %>%
    count(oldCluster, newCluster)

# Print states that changed clusters
segChanges %>%
    filter(oldCluster != newCluster)

```
  
What was previously one of the two low-impact segments has been split in to:  
  
* CO, ME, VT, WA (new segment from split); and  
* IA, MN, NE, NH, NM, OH, VA, WI (new segment from split); and  
* KS, KY, MO, ND, SD (consolidates to other low-impact segment that has rising cases with stable deaths)  
  
At a glance it seems reasonable, though the "lower impact" segments could theoretically all be consolidated without losing too much differentiation.  That said, the dendrogram built with reasonable input parameters considers the split of the lower-impact segments to be more meaningful than the split of the moderate-early and moderate-late segments.  And, the moderate-early and moderate-late split is valuable for analysis, so the decision is made to remain with the 6 hierarchical clusters as of the October 9, 2020 data.
  
Next, the USA Facts process is updated to be run using a single function, including the capability to download new data (function moved to Coronavirus_Statistics_Functions_v002.R):  
```{r cache=TRUE}

# Run file for existing data to confirm functionality
tmpCty <- readRunUSAFacts(maxDate="2020-09-30", 
                          popLoc="./RInputFiles/Coronavirus/covid_county_population_usafacts.csv", 
                          caseLoc="./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20201003.csv", 
                          deathLoc="./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20201003.csv", 
                          oldFile=burden_20200903_new, 
                          existingCountyClusters=clustVec_county_20200903_new
                          )

# Run subsets of the data
runUSAFactsSubsets(tmpCty, keyStates=c("CA", "TX", "FL", "NY", "IL", "PA"))
runUSAFactsSubsets(tmpCty, 
                   lagMetrics=tibble::tibble(metric=c("pop", "dpm"), 
                                             n=c(3, 5), 
                                             minDate="2020-03-01", 
                                             maxDate="2020-09-30"
                                             )
                   )

# Confirm that outputs are identical
identical(tmpCty$pop, pop_usafacts)
identical(tmpCty$burdenData, burden_20201003)
identical(tmpCty$clusterData, clust_20201003)
identical(tmpCty$clustVec, clustVec_county_20200903_new)
identical(tmpCty$helperACC_county, helperACC_county_20201003)
identical(tmpCty$clusterStateData, clusterStateData_20201003)

```
  
The function as written can re-create previous results using existing data.  Next steps are to enable data downloads and to create new segments using more recent data.

Data as of October 12, 2020 can be downloaded and processed against the existing segments:  
```{r cache=TRUE}

caseLoc_20201012 <- "./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20201012.csv"
deathLoc_20201012 <- "./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20201012.csv"

# Run file for existing data to confirm functionality
cty_20201012 <- readRunUSAFacts(maxDate="2020-10-10", 
                                popLoc="./RInputFiles/Coronavirus/covid_county_population_usafacts.csv", 
                                caseLoc=caseLoc_20201012, 
                                deathLoc=deathLoc_20201012, 
                                dlCaseDeath=TRUE,
                                oldFile=burden_20200903_new, 
                                existingCountyClusters=clustVec_county_20200903_new
                                )

```
  

Finally, the CDC data can be run using a single function:  
```{r}

# Create a mapping of epiweek to month (use 2020 for this)
epiMonth <- tibble::tibble(dt=as.Date("2020-01-01")+0:365, 
                           month=lubridate::month(dt),
                           quarter=lubridate::quarter(dt),
                           ew=lubridate::epiweek(dt)
                           ) %>%
    count(ew, month, quarter) %>%
    arrange(ew, -n) %>%
    group_by(ew) %>%
    summarize(month=factor(month.abb[first(month)], levels=month.abb), quarter=first(quarter))
epiMonth



# Population by age bucket (estimated) for 2020
usPopBucket2020 <- survival::uspop2[, , "2020"] %>%
    apply(1, FUN=sum) %>%
    tibble::tibble(ageActual=as.integer(names(.)), pop_2020=.) %>%
    mutate(age=factor(case_when(ageActual <= 24 ~ "Under 25 years", 
                                ageActual <= 44 ~ "25-44 years", 
                                ageActual <= 64 ~ "45-64 years", 
                                ageActual <= 74 ~ "65-74 years", 
                                ageActual <= 84 ~ "75-84 years", 
                                TRUE ~ "85 years and older"
                                ), levels=levels(cdc20200923$age)
                      )
           ) %>%
    group_by(age) %>%
    summarize(pop=sum(pop_2020))
usPopBucket2020



# Load and process the CDC data
cdcLoc <- "Weekly_counts_of_deaths_by_jurisdiction_and_age_group_downloaded_20200923.csv"
cdcList_20200923 <- readRunCDCAllCause(loc=cdcLoc, 
                                       dir="./RInputFiles/Coronavirus/",
                                       startYear=2015, 
                                       curYear=2020,
                                       weekThru=30, 
                                       startWeek=9, 
                                       lst=test_hier5, 
                                       epiMap=epiMonth, 
                                       cvDeathThru="2020-07-31", 
                                       cdcPlotStartWeek=10, 
                                       agePopData=usPopBucket2020
                                       )

# Download latest CDC data and process
cdcLoc <- "Weekly_counts_of_deaths_by_jurisdiction_and_age_group_downloaded_20201014.csv"
cdcList_20201014 <- readRunCDCAllCause(loc=cdcLoc, 
                                       dir="./RInputFiles/Coronavirus/",
                                       startYear=2015, 
                                       curYear=2020,
                                       weekThru=34, 
                                       startWeek=9, 
                                       lst=test_hier5_201001, 
                                       epiMap=epiMonth, 
                                       cvDeathThru="2020-08-22", 
                                       cdcPlotStartWeek=10, 
                                       agePopData=usPopBucket2020, 
                                       dlData=TRUE, 
                                       ovrWriteError=FALSE
                                       )

```
  
Next, a capability for saving and reading .RDS files is included, and key files from previous analyses are saved:  
```{r cache=TRUE}

# File used for comparison in readRunCOVIDTrackingProject
saveToRDS(test_hier5_201001, ovrWriteError=FALSE)

# Files used for comparison in readRunUSAFacts
saveToRDS(burden_20200903_new, ovrWriteError=FALSE)
saveToRDS(clustVec_county_20200903_new, ovrWriteError=FALSE)

# Files used for CDC all-cause deaths
saveToRDS(epiMonth, ovrWriteError=FALSE)
saveToRDS(usPopBucket2020, ovrWriteError=FALSE)

```

The full process of downloading new data and creating new segments is then:  
  
1.  Download new data from COVID Tracking Project and create 6 clusters (can update parameters if the dendrogram and plots do not look acceptable)  
2.  Download new data from USA Facts and create 5 clusters (can updated parameters if the charts do not look acceptable)  
3.  Download new data from CDC and run analyses against it  
  
Example code includes:  
```{r cache=TRUE}

# Create segments and download data from COVID Tracking Project
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_201015.csv"
test_hier6_201014 <- readRunCOVIDTrackingProject(thruLabel="Oct 14, 2020", 
                                                 downloadTo=if(file.exists(locDownload)) NULL else locDownload,
                                                 readFrom=locDownload, 
                                                 compareFile=readFromRDS("test_hier5_201001")$dfRaw,
                                                 hierarchical=TRUE, 
                                                 kCut=6, 
                                                 minShape=3, 
                                                 ratioDeathvsCase = 5, 
                                                 ratioTotalvsShape = 0.5, 
                                                 minDeath=100, 
                                                 minCase=10000
                                                 )

```
  
``` {r cache=TRUE}

# Create segments and download data from USA Facts
caseLoc <- "./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20201015.csv"
deathLoc <- "./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20201015.csv"

# Run file for existing data to confirm functionality
cty_20201015 <- readRunUSAFacts(maxDate="2020-10-13", 
                                popLoc="./RInputFiles/Coronavirus/covid_county_population_usafacts.csv", 
                                caseLoc=caseLoc, 
                                deathLoc=deathLoc, 
                                dlCaseDeath=!(file.exists(caseLoc) & file.exists(deathLoc)),
                                oldFile=readFromRDS("burden_20200903_new"), 
                                existingStateClusters=test_hier6_201014$useClusters,
                                createClusters=TRUE, 
                                minShape=3,
                                ratioDeathvsCase = 5,
                                ratioTotalvsShape = 0.5,
                                minDeath=100,
                                minCase=5000,
                                nCenters=5,
                                testCenters=1:25,
                                iter.max=20,
                                nstart=10,
                                seed=2010151358
                                )

```
  
```{r cache=TRUE}

# Download latest CDC data and process
cdcLoc <- "Weekly_counts_of_deaths_by_jurisdiction_and_age_group_downloaded_20201015.csv"
cdcList_20201015 <- readRunCDCAllCause(loc=cdcLoc, 
                                       startYear=2015, 
                                       curYear=2020,
                                       weekThru=34, 
                                       startWeek=9, 
                                       lst=test_hier6_201014, 
                                       epiMap=readFromRDS("epiMonth"), 
                                       cvDeathThru="2020-08-22", 
                                       cdcPlotStartWeek=10, 
                                       agePopData=readFromRDS("usPopBucket2020"), 
                                       dlData=TRUE, 
                                       ovrWriteError=FALSE
                                       )

```
  
The process should be able to run in a stand-alone fashion, with any data needed available to be loaded from the saved RDS files.
  
The process can also be run using existing data and segments:  
```{r cache=TRUE}

# Use existing segments with different data (already downloaded)
locDownload <- "./RInputFiles/Coronavirus/CV_downloaded_201015.csv"
test_old_201014 <- readRunCOVIDTrackingProject(thruLabel="Oct 14, 2020", 
                                               readFrom=locDownload, 
                                               compareFile=readFromRDS("test_hier5_201001")$dfRaw,
                                               useClusters=readFromRDS("test_hier5_201001")$useClusters
                                               )

```

A similar approach can be taken with the county data:  
```{r cache=TRUE}

# Previously downloaded data from USA Facts
caseLoc <- "./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20201015.csv"
deathLoc <- "./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20201015.csv"

# Use existing segments with different data (already downloaded)
old_20201015 <- readRunUSAFacts(maxDate="2020-10-13", 
                                popLoc="./RInputFiles/Coronavirus/covid_county_population_usafacts.csv", 
                                caseLoc=caseLoc, 
                                deathLoc=deathLoc, 
                                oldFile=readFromRDS("burden_20200903_new"), 
                                existingStateClusters=readFromRDS("test_hier5_201001")$useClusters,
                                existingCountyClusters=readFromRDS("clustVec_county_20200903_new"),
                                createClusters=FALSE
                                )

```
  
A similar approach can be taken with the CDC all-cause deaths data:  
```{r cache=TRUE}

# Use data that have previously been downloaded
cdcLoc <- "Weekly_counts_of_deaths_by_jurisdiction_and_age_group_downloaded_20201015.csv"
cdcList_old_20201015 <- readRunCDCAllCause(loc=cdcLoc, 
                                           startYear=2015, 
                                           curYear=2020,
                                           weekThru=34, 
                                           startWeek=9, 
                                           lst=readFromRDS("test_hier5_201001"), 
                                           epiMap=readFromRDS("epiMonth"), 
                                           agePopData=readFromRDS("usPopBucket2020"), 
                                           cvDeathThru="2020-08-22", 
                                           cdcPlotStartWeek=10
                                           )

```
  
The evolution of the county-level segments from September 3 to October 15 can also be explored.  Since the process is run using kmeans, segment numbers have no inherent meaning:  
```{r}

# Previous and current county clusters data
ctyChange <- tibble::tibble(fips1=names(readFromRDS("clustVec_county_20200903_new")), 
                            fips2=names(cty_20201015$clustVec),
                            cluster1=readFromRDS("clustVec_county_20200903_new"), 
                            cluster2=cty_20201015$clustVec, 
                            mism=(fips1!=fips2), 
                            fips=stringr::str_pad(fips1, side="left", pad="0", width=5)
                            ) %>%
    left_join(usmap::countypop, by="fips")

# Control totals
ctyChange %>%
    summarize(sumMismatch=sum(mism), sumna=sum(is.na(county)), sumpop=sum(pop_2015))

# Change in segments
ctyChange %>%
    group_by(cluster1, cluster2) %>%
    summarize(n=n(), pop=sum(pop_2015)) %>%
    ggplot(aes(x=factor(cluster2), y=factor(cluster1))) + 
    geom_tile(aes(fill=pop)) + 
    geom_text(aes(label=paste0("n=", n, "\nPop. ", round(pop/1000000, 1), " mln"))) + 
    scale_fill_continuous("Population", high="lightgreen", low="white") + 
    labs(x="New Cluster (Oct 15)", y="Old Cluster (Sep 3)", title="Evolution of County Clusters")

```
  
There has been some meaningful evolution of the county-level clusters, particularly for the small cluster with a large disease burden.  The totals are examined:  
```{r}

smallCluster <- ctyChange %>%
    count(cluster1) %>%
    filter(n==min(n)) %>%
    pull(cluster1)

segChange <- ctyChange %>%
    filter(cluster1==smallCluster) %>%
    arrange(cluster2, -pop_2015) %>%
    select(-fips1, -fips2, -mism)

segChange %>%
    filter(pop_2015 >= 250000) %>%
    as.data.frame()

segChange %>%
    mutate(cluster2=factor(cluster2)) %>%
    filter(pop_2015 >= 100000) %>%
    usmap::plot_usmap(regions="counties", data=., values="cluster2") + 
    scale_fill_discrete("New segment", na.value="white")

cty_20201015$burdenData %>%
    select(countyFIPS, date, cumDeaths, population, countyName=county) %>%
    mutate(countyFIPS=stringr::str_pad(countyFIPS, side="left", width=5, pad="0")) %>%
    inner_join(select(ctyChange, fips, cluster1, cluster2), by=c("countyFIPS"="fips")) %>%
    group_by(cluster1, cluster2, date) %>%
    summarize(cumDeaths=sum(cumDeaths), population=sum(population)) %>%
    ggplot(aes(x=date, y=1000000*cumDeaths/population)) + 
    geom_line(aes(color=factor(cluster2))) + 
    facet_grid(paste0("New ", cluster2)~paste0("Old ", cluster1)) + 
    labs(x="Date", 
         y="Cumulative Deaths per million", 
         title="Deaths per million by segment", 
         subtitle="Old segment (03 SEP) and new segment (15 OCT)"
         )

```
  
The segmenting methodology puts a much higher weight on total burden than on shape of the curve.  So, as some of the (mostly) southern counties have had significant burden increases, the segments have been reorganizing to reflect that.

Next steps are to work with the county clusters using a higher weight on shape of the curve.  It will be meaningful to have distinct early/high and late/high segments similar to what is seen with the states.  The patterns appear potentially similar to the states:  
  
* Early and high  
* Early and moderate
* Late and high  
* Late and moderate  
* Persistently moderate  
* Persistently low  
  
The county clustering process is updated to allow for hierarchical clustering and a more rules-based clustering based on simple heuristics (high, medium, low; early, sustained, late):  
```{r}

# Function to run the USA Facts (US county-level coronavirus data) clustering process
readRunUSAFacts <- function(maxDate, 
                            popLoc, 
                            caseLoc, 
                            deathLoc, 
                            dlPop=FALSE, 
                            dlCaseDeath=FALSE, 
                            ovrWrite=FALSE, 
                            ovrWriteError=TRUE, 
                            oldFile=NULL, 
                            showBurdenMinPop=10000, 
                            minPopCluster=25000,
                            existingStateClusters=NULL, 
                            existingCountyClusters=NULL, 
                            createClusters=FALSE, 
                            hierarchical=FALSE,
                            kCut=6,
                            ...
                            ) {
    
    # FUNCTION ARGUMENTS:
    # maxDate: the maximum data to use for data from the cases and deaths file
    # popLoc: location where the county-level population data are stored
    # caseLoc: location where the county-level cases data are stored
    # deathLoc: location where the county-level deaths data are stored
    # dlPop: boolean, should new population data be downloaded to popLoc
    # dlCaseDeath: boolean, should new case data and death data be downloaded to caseLoc and deathLoc
    # ovrWrite: boolean, if data are downloaded to an existing file, should it be over-written
    # ovrWriteError: boolean, if ovrWrite is FALSE and an attempt to overwrite is made, should it error out?
    # oldFile: old file for comparing metrics against (NULL means no old file for comarisons)
    # showBurdenMinPop: minimum population for showing in burden by cluster plots (NULL means skip plot)
    # minPopCluster: minimum population for including county in running cluster-level metrics
    # existingStateClusters: location of an existing named vector with clusters by state (NULL means none)
    # existingCountyClusters: location of an existing named vector with clusters by county (NULL means none)
    #                         if existingStateClusters is not NULL, then existingCountyClusters is ignored
    # createClusters: boolean, whether to create new clusters (only set up for kmeans)
    # hierarchical: whether to create hierarchical clusters
    #               TRUE means run hierarchical clustering
    #               FALSE means run kmeans clustering
    #               NA means run rules-based clustering
    # kCut; if hierarchical clustering is used, what k (number of clusters in cutree) should be used?
    # ...: other arguments that will be passed to prepClusterCounties
    
    # STEP 0: Download new files (if requested)
    urlCase <- "https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_confirmed_usafacts.csv"
    urlDeath <- "https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_deaths_usafacts.csv"
    urlPop <- "https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_county_population_usafacts.csv"
    
    # Helper function to download a file
    helperDownload <- function(url, loc, ovrWrite=ovrWrite, ovrWriteError=ovrWriteError) {
        # If the file exists, mention it and proceed as per the guard checks
        if (file.exists(loc)) {
            cat("\nFile:", loc, "already exists\n")
            if (!ovrWrite & ovrWriteError) stop("\nExiting due to ovrWrite=FALSE and ovrWriteError=TRUE\n")
            if (!ovrWrite & !ovrWriteError) {
                cat("\nFile is NOT downloaded again\n")
                return(NULL)
            }
        }
        # Download the file and change to read-only
        download.file(url, destfile=loc, method="curl")
        Sys.chmod(loc, mode="0555", use_umask = FALSE)
    }
    
    if (dlPop) helperDownload(urlPop, loc=popLoc)
    if (dlCaseDeath) helperDownload(urlCase, loc=caseLoc)
    if (dlCaseDeath) helperDownload(urlDeath, loc=deathLoc)
    
    # STEP 1: Read in the population file
    pop <- readr::read_csv(popLoc) %>%
        rename(countyName=`County Name`, state=State)
    
    # STEP 2: Read case and death data, combine, and add population totals and existing clusters
    burdenData <- readUSAFacts(
        caseFile=caseLoc, 
        deathFile=deathLoc, 
        countyPopFile=pop,
        oldFile=oldFile,
        showBurdenMinPop=showBurdenMinPop,
        maxDate=maxDate,
        stateClusters=existingStateClusters, 
        countyClusters=existingCountyClusters, 
        glimpseRaw=FALSE
    )
    
    # STEP 3: Create appropriately filtered data, and new clusters if requested
    clusterData <- prepClusterCounties(burdenFile=burdenData, 
                                       maxDate=maxDate, 
                                       minPop=minPopCluster,
                                       createClusters=createClusters, 
                                       hierarchical=hierarchical, 
                                       returnList=TRUE,
                                       ...
    )
    
    # STEP 4: Assess clusters against the new data
    # STEP 4a: Extract the county-level clusters (new clusters if created, existing otherwise)
    if (createClusters) {
        if (is.na(hierarchical)) clustVec <- clusterData$objCluster$objCluster
        else if (hierarchical) clustVec <- cutree(clusterData$objCluster$objCluster, k=kCut)
        else clustVec <- clusterData$objCluster$objCluster$cluster
    }
    else {
        clustVec <- existingCountyClusters
    }
    
    # STEP 4b: Show the cumulative data, order by cluster, and keep the plots together
    helperACC_county <- helperAssessCountyClusters(vecCluster=clustVec, 
                                                   dfPop=clusterData$countyFiltered, 
                                                   dfBurden=clusterData$countyFiltered, 
                                                   showCum=TRUE,
                                                   thruLabel=format(as.Date(maxDate), "%b %d, %Y"), 
                                                   plotsTogether=TRUE, 
                                                   orderCluster=TRUE
    )
    
    # STEP 5: Add back clusters not used for analysis (code 999) and associated disease data
    # May want to change the approach to population data
    clusterStateData <- helperMakeClusterStateData(dfPlot=helperACC_county, 
                                                   dfPop=usmap::countypop,
                                                   dfBurden=clusterData$countyDailyPerCapita,
                                                   orderCluster=TRUE
    )
    
    # STEP 6: Return a list of the key files
    list(pop=pop, 
         burdenData=burdenData, 
         clusterData=clusterData, 
         clustVec=clustVec, 
         helperACC_county=helperACC_county, 
         clusterStateData=clusterStateData,
         maxDate=maxDate
    )
    
}



# STEP 4: Create county-level clusters
prepClusterCounties <- function(burdenFile, 
                                maxDate,
                                minPop, 
                                createClusters=TRUE,
                                hierarchical=FALSE, 
                                returnList=!hierarchical, 
                                ...
                                ) {
    
    # FUNCTION ARGUMENTS:
    # burdenFile: the pivoted file containing the burdens data
    # maxDate: the latest date to be used from the data
    # minPop: the smallest population for a county to be included
    # createClusters: boolean, whether to create county-level clusters (if FALSE, object returned as NULL)
    #                 FALSE would be for prepping and converting data only (those objects are returned)
    # hierarchical: whether to create hierarchical clusters
    #               TRUE means run hierarchical clustering
    #               FALSE means run kmeans clustering
    #               NA means run rules-based clustering
    # returnList: whether the clustering call returns a list (only set up for TRUE as of now)
    # ...: other arguments to be passed to clusterStates
    
    # STEP 1: Select only desired variables from burden file
    countyCumPerCapita <- burdenFile %>%
        select(state=countyFIPS, date, cpm=cumCasesPer, dpm=cumDeathPer, population) %>%
        arrange(state, date)
    
    # STEP 2: Confirm that there are no duplicates and that every county has the same dates
    # This should be 1 provided that there are no duplicates
    countyCumPerCapita %>% 
        count(state, date) %>% 
        pull(n) %>% 
        max()
    
    # This should have no standard deviation if the same number of records exist on every day
    countyCumPerCapita %>%
        mutate(n=1) %>%
        group_by(date) %>%
        summarize(n=sum(n), population=sum(population)) %>%
        summarize_at(vars(all_of(c("n", "population"))), .funs=list(sd=sd, min=min, max=max))
    
    # STEP 3: Convert to daily new totals rather than cumulative data
    countyDailyPerCapita <- countyCumPerCapita %>%
        group_by(state) %>%
        arrange(date) %>%
        mutate_at(vars(all_of(c("cpm", "dpm"))), ~ifelse(row_number()==1, ., .-lag(.))) %>%
        ungroup()
    
    # STEP 4: Add rolling 7 aggregates and total cases/deaths
    countyDailyPerCapita <- countyDailyPerCapita %>%
        arrange(state, date) %>%
        group_by(state) %>%
        helperRollingAgg(origVar="cpm", newName="cpm7", k=7) %>%
        helperRollingAgg(origVar="dpm", newName="dpm7", k=7) %>%
        ungroup() %>%
        mutate(cases=cpm*population/1000000, deaths=dpm*population/1000000)
    
    # STEP 5: Filter the data prior to clustering
    countyFiltered <- countyDailyPerCapita %>%
        filter(population >= minPop, date <= as.Date(maxDate)) %>%
        mutate(state=as.character(state))
    
    # STEP 6: Check the implications of the filtering
    # Check number of counties that will fail the test for 100 deaths per million or 5000 cases per million
    is0 <- function(x) mean(x==0)
    isltn <- function(x, n) mean(x<n)
    islt100 <- function(x) isltn(x, n=100)
    islt5000 <- function(x) isltn(x, n=5000)
    
    countyFiltered %>% 
        group_by(state) %>% 
        summarize_at(c("cpm", "dpm"), sum) %>% 
        ungroup() %>%
        summarize_at(vars(all_of(c("cpm", "dpm"))), 
                     .funs=list(mean_is0=is0, mean_lt100=islt100, mean_lt5000=islt5000)
        ) %>%
        print()
    
    # Run county-level clusters if requested, otherwise store as NULL
    objCluster <- if(createClusters) {
        clusterStates(countyFiltered, hierarchical=hierarchical, returnList=returnList, ...)
    } else {
        NULL
    }
    
    # Return all of the relevant objects
    list(objCluster=objCluster, 
         countyFiltered=countyFiltered, 
         countyDailyPerCapita=countyDailyPerCapita, 
         countyCumPerCapita=countyCumPerCapita
    )
    
}



# Function to create clusters for the state data (requires all data from same year, as currently true)
clusterStates <- function(df, 
                          caseVar="cpm", 
                          deathVar="dpm",
                          shapeFunc=lubridate::month, 
                          minShape=NULL, 
                          maxShape=NULL,
                          minDeath=0,
                          maxDeath=Inf,
                          minCase=0,
                          maxCase=Inf,
                          ratioTotalvsShape=1, 
                          ratioDeathvsCase=1, 
                          hierarchical=TRUE, 
                          hierMethod="complete", 
                          nCenters=3, 
                          iter.max=10,
                          nstart=1,
                          testCenters=NULL,
                          returnList=FALSE, 
                          seed=NULL
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame containing cases and deaths data
    # caseVar: the variable containing the cases per capita data
    # deathVar: the variable containing the deaths per capita data
    # shapeFunc: the function to be used for creating the shape of the curve
    # minShape: the minimum value to be used for shape (to avoid very small amounts of data in Jan/Feb/Mar)
    #           shape is the month, so 4 means start with April data (NULL means keep everything)
    # maxShape: the maximum value to be used for shape (to avoid very small amounts of data in a partial month)
    #           shape is the month, so 9 means end with September data (NULL means keep everything)
    # minDeath: use this value as a floor for the death metric when calculating shape
    # maxDeath: use this value as a maximum when calculating distance using deaths 
    # minCase: use this value as a floor for the case metric when calculating shape
    # maxCase: use this value as a maximum when calculating distance using cases 
    # ratioTotalvsShape: amount of standard deviation to be kept in total variable vs shape variables
    # ratioDeathvsCase: amount of standard deviation to be kept in deaths vs cases 
    #                   (total death data will be scaled to have sd this many times higher than cases)
    #                   (death percentages by time period will be scaled directly by this amount)
    # hierarchical: whether to create hierarchical clusters
    #               TRUE means run hierarchical clustering
    #               FALSE means run kmeans clustering
    #               NA means run rules-based clustering
    # hierMethod: the method for hierarchical clustering (e.g., 'complete' or 'single')
    # nCenters: the number of centers to use for kmeans clustering
    # testCenters: integer vector of centers to test (will create an elbow plot); NULL means do not test
    # iter.max: maximumum number of kmeans iterations (default in kmeans algorithm is 10)
    # nstart: number of random sets chosen for kmeans (default in kmeans algorithm is 1)
    # returnList: boolean, if FALSE just the cluster object is returned
    #                      if TRUE, a list is returned with dfCluster and the cluster object
    # seed: set the seed to this value (NULL means no seed)
    
    # Extract key information (aggregates and by shapeFunc for each state)
    df <- df %>%
        select_at(vars(all_of(c("date", "state", caseVar, deathVar)))) %>%
        purrr::set_names(c("date", "state", "cases", "deaths")) %>%
        mutate(timeBucket=shapeFunc(date)) %>%
        group_by(state, timeBucket) %>%
        summarize(cases=sum(cases), deaths=sum(deaths)) %>%
        ungroup()
    
    # Limit to only relevant time buckets if requested
    if (!is.null(minShape)) {
        df <- df %>%
            filter(timeBucket >= minShape)
    }
    if (!is.null(maxShape)) {
        df <- df %>%
            filter(timeBucket <= maxShape)
    }
    
    # Extract an aggregate by state, scaled so that they have the proper ratio
    dfAgg <- df %>%
        group_by(state) %>%
        summarize(totalCases=sum(cases), totalDeaths=sum(deaths)) %>%
        mutate(totalCases=pmin(totalCases, maxCase), totalDeaths=pmin(totalDeaths, maxDeath)) %>%
        ungroup() %>%
        mutate(totalDeaths=ratioDeathvsCase*totalDeaths*sd(totalCases)/sd(totalDeaths))
    
    # Extract the percentages (shapes) by month, scaled so that they have the proper ratio
    dfShape <- df %>%
        pivot_longer(-c(state, timeBucket)) %>%
        group_by(state, name) %>%
        mutate(tot=pmax(sum(value), ifelse(name=="deaths", minDeath, minCase)), 
               value=ifelse(name=="deaths", ratioDeathvsCase, 1) * value / tot) %>%
        select(-tot) %>%
        pivot_wider(state, names_from=c(name, timeBucket), values_from=value) %>%
        ungroup()
    
    # Function to calculate SD of a subset of columns
    calcSumSD <- function(df) {
        df %>% 
            ungroup() %>% 
            select(-state) %>% 
            summarize_all(.funs=sd) %>% 
            as.vector() %>% 
            sum()
    }
    
    # Down-weight the aggregate data so that there is the proper sum of sd in aggregates and shapes
    aggSD <- calcSumSD(dfAgg)
    shapeSD <- calcSumSD(dfShape)
    dfAgg <- dfAgg %>%
        mutate_if(is.numeric, ~. * ratioTotalvsShape * shapeSD / aggSD)
    
    # Combine so there is one row per state
    dfCluster <- dfAgg %>%
        inner_join(dfShape, by="state")
    
    # convert 'state' to rowname
    keyData <- dfCluster %>% column_to_rownames("state")
    
    # Create hierarchical segments or kmeans segments
    if (is.na(hierarchical)) {
        # Create pseudo-rules-based segments
        if (!is.null(seed)) set.seed(seed)
        # STEP 1: Classify high-medium-low based on deaths and cases
        hml <- kmeans(select(keyData, starts_with("total")), centers=3)
        # STEP 2: Classify early-late based on shape
        esl <- kmeans(select(keyData, -starts_with("total")), centers=2)
        # STEP 3: Create a final segment
        objCluster <- 2*(hml$cluster-1) + esl$cluster
    } else if (hierarchical) {
        # Create hierarchical segments
        objCluster <-  hclust(dist(keyData), method=hierMethod)
        plot(objCluster)
    } else {
        # Create k-means segments
        # Create an elbow plot if testCenters is not NULL
        if (!is.null(testCenters)) {
            helperElbow(keyData, testCenters=testCenters, iter.max=iter.max, nstart=nstart, silhouette=TRUE)
        }
        # Create the kmeans cluster object, setting a seed if requested
        if (!is.null(seed)) set.seed(seed)
        objCluster <- kmeans(keyData, centers=nCenters, iter.max=iter.max, nstart=nstart)
        cat("\nCluster means and counts\n")
        n=objCluster$size %>% cbind(objCluster$centers) %>% round(2) %>% t() %>% print()
    }
    
    # Return the data and object is a list if returnList is TRUE, otherwise return only the clustering object
    if (!isTRUE(returnList)) {
        objCluster
    } else {
        list(objCluster=objCluster, dfCluster=dfCluster)
    }
    
}



# Create segments and download data from USA Facts
caseLoc <- "./RInputFiles/Coronavirus/covid_confirmed_usafacts_downloaded_20201015.csv"
deathLoc <- "./RInputFiles/Coronavirus/covid_deaths_usafacts_downloaded_20201015.csv"

# Create 3x2 clusters for hml and shape
cty_20201015_rule6 <- readRunUSAFacts(maxDate="2020-10-13", 
                                      popLoc="./RInputFiles/Coronavirus/covid_county_population_usafacts.csv", 
                                      caseLoc=caseLoc, 
                                      deathLoc=deathLoc, 
                                      dlCaseDeath=!(file.exists(caseLoc) & file.exists(deathLoc)),
                                      oldFile=readFromRDS("burden_20200903_new"), 
                                      existingStateClusters=test_hier6_201014$useClusters,
                                      createClusters=TRUE, 
                                      hierarchical=NA,
                                      minShape=3,
                                      ratioDeathvsCase = 5,
                                      ratioTotalvsShape = 0.25,
                                      minDeath=100,
                                      minCase=5000
                                      )

```
  
At a glance, the rule-based approach creates reasonable county-level segments that are generally well differentiated as far as burden and shape.  Next steps are to continue to explore the county-level segmenting approach.

The rules-based approach pulls apart differences in both shape and magnitude, focused mainly on coronavirus deaths.  The cases data are less reliable given the significant variation in testing by locale and timing.

Suppose that three archetype deaths per million by month are examined:  
```{r}

# Archetype curves for deaths by month
archeDeaths <- tibble::tibble(mon=factor(rep(month.abb[3:9], times=3), levels=month.abb), 
                              lvl=rep(letters[1:3], each=7), 
                              dpm=c(0, 250, 750, 500, 250, 100, 50, 
                                    0, 25, 200, 200, 125, 50, 50, 
                                    0, 0, 50, 50, 50, 150, 200
                                    )
                              )

# Plot of dpm by archetype
p1 <- archeDeaths %>%
    ggplot(aes(x=mon, y=dpm)) + 
    geom_line(aes(group=lvl, color=lvl)) + 
    labs(x="", y="Deaths per million", title="Simulated archetype deaths per million") + 
    scale_color_discrete("Archetype")
p1
p1 + facet_wrap(~lvl, scales="free_y") + labs(subtitle="Each facet is on its own scale")

```
  
The rules based segmentation will pull these apart.  Archetype a is high and early, archetype b is medium and early, archetype c is medium and late.  Any reasonable distance metric will likely separate a and c, but a further objective is to have b (very) roughly equidistant from both a and c.  The distance from a is driven by total volume and the distance from c is driven by shape.

Suppose that standard euclidean distance is calculated with the burden by month completely unscaled:  
```{r}

archeDeaths %>%
    pivot_wider(lvl, names_from=mon, values_from=dpm) %>%
    column_to_rownames("lvl") %>%
    dist(method="euclidean", upper=TRUE)

```
  
As expected, a and c are the most distant from each other, but b is twice as close to c as to a based on similarity in disease burden.  This may tend to drive very different shapes in to the same segment, forcing the segmentation to be "too similar" to just a ranking by total deaths.

Suppose instead that a log transformation is applied to the deaths by month:  
```{r}

archeDeaths %>%
    mutate(logdpm=log10(pmax(dpm, 0)+1)) %>%
    pivot_wider(lvl, names_from=mon, values_from=logdpm) %>%
    column_to_rownames("lvl") %>%
    dist(method="euclidean", upper=TRUE)

```
  
This approach is encouraging and can be applied to the rules-based segmentation data:  
```{r}

ruleSegData <- cty_20201015_rule6$clusterStateData %>%
    filter(!is.na(date)) %>%
    mutate(mon=factor(month.abb[mon=lubridate::month(date)], levels=month.abb)) %>%
    group_by(fipsCounty, cluster, mon) %>%
    summarize(deaths=sum(deaths), pop=mean(pop)) %>%
    group_by(cluster, mon) %>%
    summarize(deaths=sum(deaths), pop=sum(pop)) %>%
    ungroup() %>%
    mutate(dpm=1000000*deaths/pop) %>%
    filter(mon %in% month.abb[3:9])

p2 <- ruleSegData %>%
    ggplot(aes(x=mon, y=dpm)) + 
    geom_line(aes(group=cluster, color=cluster)) + 
    labs(x="", y="Deaths per million", title="Deather per million by cluster") + 
    scale_color_discrete("Cluster")
p2
p2 + facet_wrap(~cluster, scales="free_y") + labs(subtitle="Each facet is on its own scale")

segDist <- ruleSegData %>%
    mutate(logdpm=log10(1+pmax(0, dpm))) %>%
    pivot_wider(cluster, names_from=mon, values_from="logdpm") %>%
    column_to_rownames("cluster") %>%
    dist(method="euclidean", upper=TRUE, diag=TRUE) %>% as.matrix() %>% as.data.frame() %>%
    rownames_to_column("clusterA") %>% pivot_longer(-clusterA, names_to="clusterB")

segDist %>%
    filter(clusterA != clusterB, clusterA != 999, clusterB != 999) %>%
    ggplot(aes(x=clusterA, y=clusterB)) + 
    geom_tile(aes(fill=value)) + 
    geom_text(aes(label=round(value, 1))) + 
    scale_fill_continuous(high="red", low="white", "Distance") + 
    labs(x="", 
         y="", 
         title="Euclidean distance between cluster averages", 
         subtitle="Calculated using log10 deaths per million by month"
         )

```
  
The distances seem sensible, with the methodology assigning high distance when either magnitude OR shape is different.  Suppose the methodology is applied to all counties that are not "too small" (classified as 999):  
```{r}

testSegData <- cty_20201015_rule6$clusterStateData %>%
    filter(!is.na(date), cluster != 999) %>%
    mutate(mon=factor(month.abb[mon=lubridate::month(date)], levels=month.abb)) %>%
    group_by(fipsCounty, cluster, mon) %>%
    summarize(deaths=sum(deaths), pop=mean(pop)) %>%
    ungroup() %>%
    mutate(dpm=1000000*deaths/pop, logdpm=log10(1+pmax(0, dpm))) %>%
    filter(mon %in% month.abb[3:9])

testDistData <- testSegData %>%
    pivot_wider(fipsCounty, names_from=mon, values_from=logdpm) %>%
    column_to_rownames("fipsCounty") %>%
    dist()

testHierClust <- hclust(testDistData, method="complete")
plot(testHierClust)

```
  
The dendrogram is encouraging, with an early split in to three, each followed by a split in two.  Suppose that 6 segments are created from this:  
```{r}

testClustVec <- cutree(testHierClust, k=6)

testSegOut <- testSegData %>%
    mutate(newCluster=testClustVec[fipsCounty])

testSegOut %>%
    group_by(fipsCounty, cluster, newCluster) %>%
    summarize(deaths=sum(deaths), pop=mean(pop)) %>%
    group_by(newCluster, cluster) %>%
    summarize(n=n(), pop=sum(pop), deaths=sum(deaths)) %>%
    ggplot(aes(x=factor(newCluster), y=cluster)) + 
    geom_tile(aes(fill=pop/1000000)) + 
    geom_text(aes(label=paste0(round(pop/1000000, 1), " million\n(n=", n, ")"))) + 
    scale_fill_continuous("Pop (millions)", high="lightgreen", low="white") + 
    labs(x="New cluster", y="Rules-based cluster", title="Change in cluster by methodology")

testSegOut %>%
    group_by(mon, cluster, newCluster) %>%
    summarize(deaths=sum(deaths), pop=sum(pop), n=n()) %>%
    ungroup() %>%
    mutate(dpm=1000000*deaths/pop, newCluster=factor(newCluster)) %>%
    ggplot(aes(x=mon, y=dpm)) + 
    geom_line(aes(color=newCluster, group=newCluster)) + 
    geom_text(aes(x="Jun", y=750, label=paste0("n=", n))) +
    facet_grid(newCluster ~ cluster) +
    labs(x="Rules-based cluster", y="New cluster", title="Deaths per million by cluster") + 
    scale_color_discrete("New cluster")

```
  
Further exploration is needed to understand the driver behind splitting some observations that, at a glance, look very similar.
  
The issue with the log10 approach is that zero and small start to become better differentiated than small and large (e.g., 0 deaths per million becomes 0, 100 deaths per million becomes 2, and 1000 deaths per million becomes 3).  So, the goal of reducing spurious differentiation among outliers on the high end (2000 is 3.3., 4000 is 3.6) has been achieved, but at the cost of losing the power to appropriately separate high, medium, and low.

An additional approach could take advantage of the timing of deaths in the national curve (using only the counties that meet the population elgibility criteria:  
```{r}

ruleNationData <- cty_20201015_rule6$clusterStateData %>%
    filter(!is.na(date)) %>%
    mutate(mon=factor(month.abb[mon=lubridate::month(date)], levels=month.abb)) %>%
    group_by(cluster, date) %>%
    summarize(deaths=sum(deaths), pop=sum(pop)) %>%
    mutate(dpm=1000000*deaths/pop)

ruleNationData %>%
    filter(cluster!=999) %>%
    ggplot(aes(x=date, y=dpm)) + 
    geom_line() + 
    geom_smooth(color="red", span=0.25, method="loess") +
    labs(x="", 
         y="Deaths per million", 
         title="US coronavirus deaths per million by day", 
         subtitle="Caution that each facet has its own y scale"
         ) + 
    scale_x_date(date_breaks="1 month", date_labels="%b") + 
    facet_wrap(~cluster, scales="free_y")

ruleNationData %>%
    filter(cluster!=999) %>%
    mutate(mon=factor(month.abb[lubridate::month(date)], levels=month.abb)) %>%
    group_by(cluster, mon) %>%
    summarize(deaths=sum(deaths), pop=mean(pop)) %>%
    mutate(dpm=1000000*deaths/pop) %>%
    ggplot(aes(x=mon, y=dpm)) + 
    geom_line(aes(group=cluster, color=cluster)) + 
    labs(x="", 
         y="Deaths per million", 
         title="US coronavirus deaths per million by month", 
         subtitle="Caution that each facet has its own y scale and October is only a partial month"
         ) + 
    facet_wrap(~cluster, scales="free_y")

```
  
The rule-based segments are nicely differentiated visually, both as far as shape and maximum y-axis value.

There is no meaningful spike in coronavirus until April and October is an incomplete month.  Perhaps distance metrics would better be calculated using:  
  
* April and earlier  
* May-June  
* July-August  
* September and later  
  
This avoids some of the spurious zero points that emerge from treating each month in each county as a separate observation:  
```{r}

ruleNationData %>%
    filter(cluster!=999) %>%
    mutate(mon=factor(month.abb[lubridate::month(date)], levels=month.abb), 
           monGroup=case_when(mon %in% c("Jan", "Feb", "Mar", "Apr") ~ "thru Apr", 
                              mon %in% c("May", "Jun") ~ "May-Jun", 
                              mon %in% c("Jul", "Aug") ~ "Jul-Aug", 
                              mon %in% c("Sep", "Oct") ~ "Sep and later",
                              TRUE ~ "Map error"
                              ), 
           monGroup=factor(monGroup, levels=c("thru Apr", "May-Jun", "Jul-Aug", "Sep and later", "Map error"))
           ) %>%
    group_by(cluster, monGroup) %>%
    summarize(deaths=sum(deaths), pop=mean(pop)) %>%
    mutate(dpm=1000000*deaths/pop) %>%
    ggplot(aes(x=monGroup, y=dpm)) + 
    geom_line(aes(group=cluster, color=cluster)) + 
    labs(x="", 
         y="Deaths per million", 
         title="US coronavirus deaths per million by month", 
         subtitle="Caution that each facet has its own y scale and October is only a partial month"
         ) + 
    ylim(c(0, NA)) +
    facet_wrap(~cluster, scales="free_y")

```

There are now three distinct shapes of the curve:  
  
* Peak in Jul-Aug (one at ~800 dpm, one at ~400 dpm, one at ~100 dpm)  
* Peak in May-Jun (one at ~400 dpm, one at ~100 dpm)  
* Peak in thru Apr (one at ~1000 dpm)  
  
Arguably, the rules-based segmenting approach outperforms attempts to calculate a single distance metric between the counties.  The risk is that there may be underlying patterns in the data that are not detected by a pre-determined set of rules.

Suppose that the CI for the individual counties is also plotted on the same chart:  
```{r}

allCountyRules <- cty_20201015_rule6$clusterStateData %>%
    filter(!is.na(date)) %>%
    mutate(mon=factor(month.abb[mon=lubridate::month(date)], levels=month.abb), 
                      monGroup=case_when(mon %in% c("Jan", "Feb", "Mar", "Apr") ~ "thru Apr", 
                              mon %in% c("May", "Jun") ~ "May-Jun", 
                              mon %in% c("Jul", "Aug") ~ "Jul-Aug", 
                              mon %in% c("Sep", "Oct") ~ "Sep and later",
                              TRUE ~ "Map error"
                              ), 
           monGroup=factor(monGroup, levels=c("thru Apr", "May-Jun", "Jul-Aug", "Sep and later", "Map error"))
           ) %>%
    group_by(fipsCounty, countyName, state, cluster, monGroup) %>%
    summarize(deaths=sum(deaths), pop=mean(pop)) %>%
    ungroup() %>%
    rbind(mutate(., state="All", countyName="All", fipsCounty="All")) %>%
    group_by(fipsCounty, countyName, state, cluster, monGroup) %>%
    summarize(deaths=sum(deaths), pop=sum(pop)) %>%
    mutate(dpm=1000000*deaths/pop, moddpm=dpm*ifelse(monGroup=="Sep and later", 61/43, 1)) %>%
    ungroup()

ciData <- allCountyRules %>%
    filter(fipsCounty != "All") %>%
    group_by(cluster, monGroup) %>%
    summarize(ci05=quantile(moddpm, 0.05), 
              ci25=quantile(moddpm, 0.25), 
              ci75=quantile(moddpm, 0.75), 
              ci95=quantile(moddpm, 0.95)
              ) %>%
    ungroup()

allCountyRules %>%
    ggplot(aes(x=monGroup)) + 
    geom_line(data=~filter(., fipsCounty=="All"), aes(y=moddpm, group=cluster, color=cluster), lwd=2) + 
    geom_ribbon(data=ciData, aes(ymin=ci25, ymax=ci75, group=cluster), alpha=0.5) +
    geom_ribbon(data=ciData, aes(ymin=ci05, ymax=ci95, group=cluster), fill="grey", alpha=0.5) +
    labs(x="", 
         y="Deaths per million (thru Oct 13*)", 
         title="US coronavirus deaths per million tracked by county", 
         subtitle="All facets on same y scale\nDark ribbon covers 50% of counties, light ribbon covers 90%", 
         caption="* Data through Oct 13 ('Sep and later' scaled by 61/43 to account for shorter time period)"
         ) + 
    ylim(c(0, NA)) +
    facet_wrap(~cluster)

```
  
Tentatively, the analysis will continue with the rules-based segmentation approach to county clusters since it seems to be driving meaningful differentiation for further analysis.

Suppose that evolution in cases is assessed using the county-level clusters:  
```{r}

allCountyRules <- cty_20201015_rule6$clusterStateData %>%
    filter(!is.na(date)) %>%
    mutate(mon=factor(month.abb[mon=lubridate::month(date)], levels=month.abb), 
                      monGroup=case_when(mon %in% c("Jan", "Feb", "Mar", "Apr") ~ "thru Apr", 
                              mon %in% c("May", "Jun") ~ "May-Jun", 
                              mon %in% c("Jul", "Aug") ~ "Jul-Aug", 
                              mon %in% c("Sep", "Oct") ~ "Sep and later",
                              TRUE ~ "Map error"
                              ), 
           monGroup=factor(monGroup, levels=c("thru Apr", "May-Jun", "Jul-Aug", "Sep and later", "Map error"))
           ) %>%
    group_by(fipsCounty, countyName, state, cluster, monGroup) %>%
    summarize(deaths=sum(deaths), cases=sum(cases), pop=mean(pop)) %>%
    ungroup() %>%
    rbind(mutate(., state="All", countyName="All", fipsCounty="All")) %>%
    group_by(fipsCounty, countyName, state, cluster, monGroup) %>%
    summarize(deaths=sum(deaths), cases=sum(cases), pop=sum(pop)) %>%
    mutate(dpm=1000000*deaths/pop, 
           moddpm=dpm*ifelse(monGroup=="Sep and later", 61/43, 1), 
           cpm=1000000*cases/pop, 
           modcpm=cpm*ifelse(monGroup=="Sep and later", 61/43, 1)
           ) %>%
    ungroup()

ciDataCases <- allCountyRules %>%
    filter(fipsCounty != "All") %>%
    group_by(cluster, monGroup) %>%
    summarize(ci05=quantile(modcpm, 0.05), 
              ci25=quantile(modcpm, 0.25), 
              ci75=quantile(modcpm, 0.75), 
              ci95=quantile(modcpm, 0.95)
              ) %>%
    ungroup()

allCountyRules %>%
    ggplot(aes(x=monGroup)) + 
    geom_line(data=~filter(., fipsCounty=="All"), aes(y=modcpm, group=cluster, color=cluster), lwd=2) + 
    geom_ribbon(data=ciDataCases, aes(ymin=ci25, ymax=ci75, group=cluster), alpha=0.5) +
    geom_ribbon(data=ciDataCases, aes(ymin=ci05, ymax=ci95, group=cluster), fill="grey", alpha=0.5) +
    labs(x="", 
         y="Cases per million (thru Oct 13*)", 
         title="US coronavirus cases per million tracked by county", 
         subtitle="All facets on same y scale\nDark ribbon covers 50% of counties, light ribbon covers 90%", 
         caption="* Data through Oct 13 ('Sep and later' scaled by 61/43 to account for shorter time period)"
         ) + 
    ylim(c(0, NA)) +
    facet_wrap(~cluster)

```
  
The shapes are broadly consistent, with the exception that the segment with early/high deaths is fairly low on total case counts.  Suppose that county-level segments are instead created using a preponderance of cases rather than deaths:  
```{r}

# Create 3x2 clusters for hml and shape
popLoc <- "./RInputFiles/Coronavirus/covid_county_population_usafacts.csv"
cty_20201015_rule6_cases <- readRunUSAFacts(maxDate="2020-10-13", 
                                            popLoc=popLoc, 
                                            caseLoc=caseLoc, 
                                            deathLoc=deathLoc, 
                                            dlCaseDeath=!(file.exists(caseLoc) & file.exists(deathLoc)),
                                            oldFile=readFromRDS("burden_20200903_new"), 
                                            existingStateClusters=test_hier6_201014$useClusters,
                                            createClusters=TRUE, 
                                            hierarchical=NA,
                                            minShape=4,
                                            ratioDeathvsCase = 0.2,
                                            ratioTotalvsShape = 0.25,
                                            minDeath=100,
                                            minCase=5000
                                            )

```
  
Interestingly, there is no longer a segment with high deaths early since it is not well-differemtiated by having (relatively) high cases early.  This is suggestive that deaths are a better metric than cases for tracking the historical impact of cornavirus by county (though given lags, cases are more likely to be a current leading indicator of burden).

Suppose that the movements by segment are examined:  
```{r}

# Function to examine the difference in case clusters and death clusters
examineCasevDeathClusters <- function(caseList, 
                                      deathList, 
                                      popMin=0, 
                                      ctyMin=0
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # caseList: the processed object using cases
    # deathList: the processed object using deaths
    # popMin: in the text overlap grid, include a record if it has at least this much population
    # ctyMin: in the text overlap grid, include a record if it has at least this many counties
    #         records will be included if they meet EITHER the popMin hurdle OR the ctyMin hurdle
    
    # Pull the case cluster data    
    caseCluster <- caseList[["clusterStateData"]] %>%
        count(fipsCounty, cluster) %>%
        rename(caseCluster=cluster) %>%
        select(-n)

    # Integrate the death cluster data
    case_death <- deathList[["clusterStateData"]] %>%
        rename(deathCluster=cluster) %>%
        full_join(caseCluster, by="fipsCounty")
    
    # Plot the differences
    p1 <- case_death %>%
        group_by(fipsCounty, deathCluster, caseCluster) %>%
        summarize(deaths=sum(deaths, na.rm=TRUE), cases=sum(cases, na.rm=TRUE), pop=mean(pop)) %>%
        group_by(deathCluster, caseCluster) %>%
        summarize(n=n(), pop=sum(pop), cases=sum(cases), deaths=sum(deaths)) %>%
        filter(pop>=popMin | n >= ctyMin) %>%
        ggplot(aes(y=deathCluster, x=caseCluster)) + 
        geom_tile(aes(fill=deaths/cases)) + 
        geom_text(aes(label=paste0("n=", n, 
                                   " (Pop: ", round(pop/1000000, 1), 
                                   ")\nCases: ", round(cases/1000, 1), 
                                   "\nDeaths: ", round(deaths/1000, 1), " (", round(100*deaths/cases, 1), "%)"
                                   )
                      ), 
                  size=3.5
                  ) + 
        scale_fill_continuous("CFR", low="white", high="orange") + 
        labs(x="County cluster driven by cases", 
             y="County cluster driven by deaths", 
             title="County cluster comparison when driven by cases vs. deaths", 
             subtitle="n: # counties; Pop: population (millions); Cases: cases (000); Deaths: deaths (000) (CFR)"
             )
    if (ctyMin > 0 & popMin > 0) { 
        p1 <- p1 + 
            labs(caption=paste0("Includes overlaps with at least ", 
                                ctyMin, 
                                " counties OR at least ", 
                                round(popMin/1000, 0), 
                                " thousand population"
                                )
                 )
    }
    print(p1)
    
    # Create a plotting data frame
    plotData <- case_death %>%
        group_by(date, deathCluster, caseCluster) %>%
        summarize(deaths=sum(deaths, na.rm=TRUE), cases=sum(cases, na.rm=TRUE), pop=sum(pop), n=n()) %>%
        ungroup() %>%
        mutate(cpm=1000000*cases/pop, dpm=1000000*deaths/pop) %>%
        group_by(deathCluster, caseCluster) %>%
        mutate(cpm7=zoo::rollmean(cpm, k=7, fill=NA), dpm7=zoo::rollmean(dpm, k=7, fill=NA))

    # Create plot for deaths per million trend by cluster overlap
    p2 <- plotData %>%
        ggplot(aes(x=date, y=dpm7)) + 
        geom_line(aes(group=deathCluster, color=deathCluster)) + 
        geom_text(data=~mutate(filter(., date==lubridate::round_date(max(date))), dpm7=Inf), 
                  aes(label=paste0("n=", n), color=deathCluster), 
                  hjust=1, vjust=1
                  ) +
        facet_grid(deathCluster ~ caseCluster, switch="y") +
        labs(x="County case cluster", 
             y="County death cluster", 
             title="Deaths per million (7-day rolling mean) by county case cluster vs. county death cluster"
             )
    print(p2)
    
    # Create plot for cases per million trend by cluster overlap
    p3 <- plotData %>%
        ggplot(aes(x=date, y=cpm7)) + 
        geom_line(aes(group=caseCluster, color=caseCluster)) + 
        geom_text(data=~mutate(filter(., date==lubridate::round_date(max(date))), cpm7=Inf), 
                  aes(label=paste0("n=", n), color=caseCluster), 
                  hjust=1, vjust=1
                  ) +
        facet_grid(deathCluster ~ caseCluster, switch="y") +
        labs(x="County case cluster", 
             y="County death cluster", 
             title="Cases per million (7-day rolling mean) by county case cluster vs. county death cluster"
             )
    print(p3)
    
    # Return a list of the two key frames
    list(case_death=case_death, plotData=plotData)
    
}

# Run comparisons for cases and deaths
temp_caseDeath_001 <- examineCasevDeathClusters(caseList=cty_20201015_rule6_cases, 
                                                deathList=cty_20201015_rule6
                                                )

```
  
Significant variation in case fatality rate (deaths over confirmed cases) by both geography and time drive significant changes in the segments.  Further, the "high cases" segment consists of multiple shapes, including a group that is early and a group that is late.

Next steps are to explore why the death clusters seem to be better differentiated on shape than the case clusters even when plotting using cases.  Perhaps something is not working as intended when calculating distances based on shape of the curve.

Suppose that the key clustering data is pulled, using the adjustments above such that cases have 5x the weight as deaths:  
```{r}

shapeTestData <- cty_20201015_rule6_cases$clusterData$objCluster$dfCluster
shapeTestData %>%
    summarize_if(is.numeric, .funs=list(sd)) %>%
    mutate(all="all") %>%
    pivot_longer(-all, names_to="metric", values_to="sd") %>%
    ggplot(aes(x=fct_reorder(metric, sd), y=sd)) + 
    geom_col(fill="lightblue") +
    geom_text(aes(y=sd+0.01, label=round(sd, 2)), hjust=0) + 
    coord_flip() + 
    labs(x="", y="Standard deviation", title="Standard deviation by scaled variable, clustering by cases")

```
  
The standard deviations appear appropriately weighted in favor of cases, though perhaps the 5:1 scaling is insufficient to overcome the significant disparities in death timing.

Suppose that a very pure segmentation by cases is attempted, and using data only from April-September:  
```{r}

# Create 3x2 clusters for hml and shape
popLoc <- "./RInputFiles/Coronavirus/covid_county_population_usafacts.csv"
cty_20201015_rule6_caseshvy <- readRunUSAFacts(maxDate="2020-10-13", 
                                               popLoc=popLoc, 
                                               caseLoc=caseLoc, 
                                               deathLoc=deathLoc, 
                                               dlCaseDeath=!(file.exists(caseLoc) & file.exists(deathLoc)),
                                               oldFile=readFromRDS("burden_20200903_new"), 
                                               existingStateClusters=test_hier6_201014$useClusters,
                                               createClusters=TRUE, 
                                               hierarchical=NA,
                                               minShape=4,
                                               maxShape=9,
                                               ratioDeathvsCase = 0.001,
                                               ratioTotalvsShape = 0.25,
                                               minDeath=100,
                                               minCase=5000
                                               )

```
  
These segments appear much better tailored to differences in case volume and shape by county.  A key finding is that even a modest weight on deaths (0.2 of cases) leads the segments to be overly driven by deaths.  This is likely driven by the difference in deaths, especially among early/high, being extremely differentiated from each other over s short period of time (which will produce a very high euclidean distance that needs to be substantially scaled down to not dominate)

The comparisons of death and cases can again be run, with the main text overlap grid using only cases with at least 5 counties OR at least 500,000 population:  
```{r}

# Run comparisons for cases and deaths
temp_caseDeath_002 <- examineCasevDeathClusters(caseList=cty_20201015_rule6_caseshvy, 
                                                deathList=cty_20201015_rule6, 
                                                ctyMin=5, 
                                                popMin=500000
                                                )

```
  
The switching among segments is now largely driven by differences in CFR, as desired.  The plots of deaths by cluster overlap appear driven by death cluster while the plots of cases by cluster overlap appear driven by case cluster.  These segments appear suitable for further analysis.
  
