---
title: "Data Camp Insights"
author: "davegoblue"
date: "July 7, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background and Overview  
DataCamp offers several interactive courses related to R Programming.  While much of it is review, it is always helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  This document summarizes a few key insights from:  
  
* Introduction to R  
* Intermediate R  
* Writing Functions in R  
* Importing Data in to R  
* Cleaning Data in R (tidyr, lubridate, stringr, etc.)  
* Data Manipulation (dplyr, data.table, xts/zoo)  
* Data Visualization (ggplot2, ggvis)  
  
An additional document will be maintained for several of the more statistical areas of the Data Camp offering, as well as for the few courses offered in Python.  
  
## Key Insights and Findings  
###_Introduction to R and Intermediate R_  
There are a few nuggest from within these beginning modules, including:  
  
####_Generic statements_  
* factor(x, ordered=TRUE, levels=c(myLevels)) creates ordinal factors (e.g., a > b > c)  
* subset(a, b) is functionally the same as a[a$b, ] but easier to read  
* & looks at each element while && looks only at the first element (same for | and ||)  
* Inside of a for loop, break kills the loop entirely while next moves back to the top for the next item  
* args(function) shows the arguments (with defaults) for function  
* search() shows the current search path (all auto-load packages and all attached packages)  
* cat("expression") will print the expression or direct it to a file; this is a way to allow \n and \t to take effect in a print statement  
* unique() keeps only the non-duplicated elements of a vector  
* unlist() converts a list back to a vector, somewhat similar to as.vector() on a matrix  
* sort() will sort a vector, but not a data frame  
* rep(a, times=m, each=n) replicates each element of a n times, and then the whole string m times  
* append(x, values, after=length(x)) will insert values in to vector x after point after  
* rev() reverses a vector  
* Inside a grep, "\\1" captures what is inside the ()  
    
####_Apply usages_  
* lapply() operates on a vector/list and always returns a list  
* sapply() is lapply but converted to a vector/array when possible (same as lapply if not possible); if USE.NAMES=FALSE then the vector will be unnamed, though the default is USE.NAMES=TRUE for a named vector  
* vapply(X, FUN, FUN.VALUE, ... , USE.NAMES=TRUE) is safer than sapply in that you specify what type of vector each iteration should produce; e.g., FUN.VALUE=character(1) or FUN.VALUE=numeric(3), with an error if the vector produced by an iteration is not exactly that  
  
####_Dates and times_  
* Sys.Date() grabs the system date as class "Date", with units of days  
* Sys.time() grabs the system time as class "POSIXct", with units of seconds  
* Sys.timezone() shows the system timezone  
* Years are formatted as %Y (4-digit) or %y (2-digit)  
* Months are formatted as %m (2-digit) or %B (full character) or %b (3-character)  
* Days are formatted as %d (2-digit)  
* Weekdays are formatted as %A (full name) or %a (partial name)  
* Times include %H (24-hour hour), %M (minutes), %S (seconds)  
* ?strptime will provide a lot more detail on the formats  
  
Below is some sample code showing examples for the generic statements:  
```{r}
# Factors
xRaw = c("High", "High", "Low", "Low", "Medium", "Very High", "Low")

xFactorNon = factor(xRaw, levels=c("Low", "Medium", "High", "Very High"))
xFactorNon
xFactorNon[xFactorNon == "High"] > xFactorNon[xFactorNon == "Low"][1]

xFactorOrder = factor(xRaw, ordered=TRUE, levels=c("Low", "Medium", "High", "Very High"))
xFactorOrder
xFactorOrder[xFactorOrder == "High"] > xFactorOrder[xFactorOrder == "Low"][1]


# Subsets
data(mtcars)
subset(mtcars, mpg>=25)
identical(subset(mtcars, mpg>=25), mtcars[mtcars$mpg>=25, ])
subset(mtcars, mpg>25, select=c("mpg", "cyl", "disp"))


# & and && (same as | and ||)
compA <- c(2, 3, 4, 1, 2, 3)
compB <- c(1, 2, 3, 4, 5, 6)
(compA > compB) & (compA + compB < 6)
(compA > compB) | (compA + compB < 6)
(compA > compB) && (compA + compB < 6)
(compA > compB) || (compA + compB < 6)


# Loops and cat()
# for (a in b) {
#     do stuff
#     if (exitCond) { break }
#     if (nextCond) { next }
#     do some more stuff
# }
for (myVal in compA*compB) {
    print(paste0("myVal is: ", myVal))
    if ((myVal %% 3) == 0) { cat("Divisible by 3, not happy about that\n\n"); next }
    print("That is not divisible by 3")
    if ((myVal %% 5) == 0) { cat("Exiting due to divisible by 5 but not divisible by 3\n\n"); break }
    cat("Onwards and upwards\n\n")
}


# args() and search()
args(plot.default)
search()


# unique()
compA
unique(compA)


# unlist()
listA <- as.list(compA)
unlist(listA)
identical(compA, unlist(listA))


# sort()
sort(mtcars$mpg)
sort(mtcars$mpg, decreasing=TRUE)

# rep()
rep(1:6, times=2)  # 1:6 followed by 1:6
rep(1:6, each=2)  # 1 1 2 2 3 3 4 4 5 5 6 6
rep(1:6, times=2, each=3)  # 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 6 6 6 repeated twice (each comes first)
rep(1:6, times=6:1)  # 1 1 1 1 1 1 2 2 2 2 2 3 3 3 3 4 4 4 5 5 6


# append()
myWords <- c("The", "cat", "in", "the", "hat")
paste(append(myWords, c("is", "fun", "to", "read")), collapse=" ")
paste(append(myWords, "funny", 4), collapse=" ")

# grep("//1")
sampMsg <- "This is from myname@subdomain.mydomain.com again"
gsub("(^.*\\w*[a-zA-Z0-9]+@)([a-zA-Z0-9]+\\.[a-zA-Z0-9.]+)(.*$)", "\\1", sampMsg)
gsub("(^.*\\w*[a-zA-Z0-9]+@)([a-zA-Z0-9]+\\.[a-zA-Z0-9.]+)(.*$)", "\\2", sampMsg)
gsub("(^.*\\w*[a-zA-Z0-9]+@)([a-zA-Z0-9]+\\.[a-zA-Z0-9.]+)(.*$)", "\\3", sampMsg)

# rev()
compA
rev(compA)

```
  
Below is some sample code showing examples for the apply statements:  
```{r}
# lapply
args(lapply)
lapply(1:5, FUN=sqrt)
lapply(1:5, FUN=function(x, y=2) { c(x=x, y=y, pow=x^y) }, y=3)
lapply(1:5, FUN=function(x, y=2) { if (x <= 3) {c(x=x, y=y, pow=x^y) } else { c(pow=x^y) } }, y=3)

# sapply (defaults to returning a named vector/array if possible; is lapply otherwise)
args(sapply)
args(simplify2array)
sapply(1:5, FUN=sqrt)
sapply(1:5, FUN=function(x, y=2) { c(x=x, y=y, pow=x^y) }, y=3)
sapply(1:5, FUN=function(x, y=2) { if (x <= 3) {c(x=x, y=y, pow=x^y) } else { c(pow=x^y) } }, y=3)

# vapply (tells sapply exactly what should be returned; errors out otherwise)
args(vapply)
vapply(1:5, FUN=sqrt, FUN.VALUE=numeric(1))
vapply(1:5, FUN=function(x, y=2) { c(x=x, y=y, pow=x^y) }, FUN.VALUE=numeric(3), y=3)

```
  
Below is some sample code for handing dates and times in R:  
```{r}
Sys.Date()
Sys.time()
args(strptime)

rightNow <- as.POSIXct(Sys.time())
format(rightNow, "%Y**%M-%d %H hours and %M minutes", usetz=TRUE)

lastChristmasNoon <- as.POSIXct("2015-12-25 12:00:00", format="%Y-%m-%d %X")
rightNow - lastChristmasNoon

nextUMHomeGame <- as.POSIXct("16/SEP/3 12:00:00", format="%y/%b/%d %H:%M:%S", tz="America/Detroit")
nextUMHomeGame - rightNow

# Time zones available in R
OlsonNames()

# From ?strptime (excerpted)
#
# ** General formats **
# %c Date and time. Locale-specific on output, "%a %b %e %H:%M:%S %Y" on input.
# %F Equivalent to %Y-%m-%d (the ISO 8601 date format).
# %T Equivalent to %H:%M:%S.
# %D Date format such as %m/%d/%y: the C99 standard says it should be that exact format
# %x Date. Locale-specific on output, "%y/%m/%d" on input.
# %X Time. Locale-specific on output, "%H:%M:%S" on input.
# 
# ** Key Components **
# %y Year without century (00-99). On input, values 00 to 68 are prefixed by 20 and 69 to 99 by 19
# %Y Year with century
# %m Month as decimal number (01-12).
# %b Abbreviated month name in the current locale on this platform.
# %B Full month name in the current locale.
# %d Day of the month as decimal number (01-31).
# %e Day of the month as decimal number (1-31), with a leading space for a single-digit number.
# %a Abbreviated weekday name in the current locale on this platform.
# %A Full weekday name in the current locale.
# %H Hours as decimal number (00-23)
# %I Hours as decimal number (01-12)
# %M Minute as decimal number (00-59).
# %S Second as integer (00-61), allowing for up to two leap-seconds (but POSIX-compliant implementations will ignore leap seconds).
# 
# ** Additional Options **
# %C Century (00-99): the integer part of the year divided by 100.
# 
# %g The last two digits of the week-based year (see %V). (Accepted but ignored on input.)
# %G The week-based year (see %V) as a decimal number. (Accepted but ignored on input.)
# 
# %h Equivalent to %b.
# 
# %j Day of year as decimal number (001-366).
# 
# %n Newline on output, arbitrary whitespace on input.
# 
# %p AM/PM indicator in the locale. Used in conjunction with %I and not with %H. An empty string in some locales (and the behaviour is undefined if used for input in such a locale).  Some platforms accept %P for output, which uses a lower-case version: others will output P.
# 
# %r The 12-hour clock time (using the locale's AM or PM). Only defined in some locales.
# 
# %R Equivalent to %H:%M.
# 
# %t Tab on output, arbitrary whitespace on input.
# 
# %u Weekday as a decimal number (1-7, Monday is 1).
# 
# %U Week of the year as decimal number (00-53) using Sunday as the first day 1 of the week (and typically with the first Sunday of the year as day 1 of week 1). The US convention.
# 
# %V Week of the year as decimal number (01-53) as defined in ISO 8601. If the week (starting on Monday) containing 1 January has four or more days in the new year, then it is considered week 1. Otherwise, it is the last week of the previous year, and the next week is week 1. (Accepted but ignored on input.)
# 
# %w Weekday as decimal number (0-6, Sunday is 0).
# 
# %W Week of the year as decimal number (00-53) using Monday as the first day of week (and typically with the first Monday of the year as day 1 of week 1). The UK convention.
# 
# For input, only years 0:9999 are accepted.
# 
# %z Signed offset in hours and minutes from UTC, so -0800 is 8 hours behind UTC. Values up to +1400 are accepted as from R 3.1.1: previous versions only accepted up to +1200. (Standard only for output.)
# 
# %Z (Output only.) Time zone abbreviation as a character string (empty if not available). This may not be reliable when a time zone has changed abbreviations over the years.

```
  
Additionally, code from several practice examples is added:  
```{r}
set.seed(1608221310)

me <- 89
other_199 <- round(rnorm(199, mean=75.45, sd=11.03), 0)

mean(other_199)
sd(other_199)

desMeans <- c(72.275, 76.24, 74.5, 77.695)
desSD <- c(12.31, 11.22, 12.5, 12.53)

prevData <- c(rnorm(200, mean=72.275, sd=12.31), 
              rnorm(200, mean=76.24, sd=11.22), 
              rnorm(200, mean=74.5, sd=12.5),
              rnorm(200, mean=77.695, sd=12.53) 
              )
previous_4 <- matrix(data=prevData, ncol=4)

curMeans <- apply(previous_4, 2, FUN=mean)
curSD <- apply(previous_4, 2, FUN=sd)

previous_4 <- t(apply(previous_4, 1, FUN=function(x) { desMeans + (desSD / curSD) * (x - curMeans) } ))

apply(round(previous_4, 0), 2, FUN=mean)
apply(round(previous_4, 0), 2, FUN=sd)
previous_4 <- round(previous_4, 0)


# Merge me and other_199: my_class
my_class <- c(me, other_199)

# cbind() my_class and previous_4: last_5
last_5 <- cbind(my_class, previous_4)

# Name last_5 appropriately
nms <- paste0("year_", 1:5)
colnames(last_5) <- nms


# Build histogram of my_class
hist(my_class)

# Generate summary of last_5
summary(last_5)

# Build boxplot of last_5
boxplot(last_5)


# How many grades in your class are higher than 75?
sum(my_class > 75)

# How many students in your class scored strictly higher than you?
sum(my_class > me)

# What's the proportion of grades below or equal to 64 in the last 5 years?
mean(last_5 <= 64)


# Is your grade greater than 87 and smaller than or equal to 89?
me > 87 & me <= 89

# Which grades in your class are below 60 or above 90?
my_class < 60 | my_class > 90

# What's the proportion of grades in your class that is average?
mean(my_class >= 70 & my_class <= 85)


# How many students in the last 5 years had a grade of 80 or 90?
sum(last_5 %in% c(80, 90))

# Define n_smart
n_smart <- sum(my_class >= 80)

# Code the if-else construct
if (n_smart > 50) {
    print("smart class")
} else {
    print("rather average")
}

# Define prop_less
prop_less <- mean(my_class < me)

# Code the control construct
if (prop_less > 0.9) {
    print("you're among the best 10 percent")
} else if (prop_less > 0.8) {
    print("you're among the best 20 percent")
} else {
    print("need more analysis")
}

# Embedded control structure: fix the error
if (mean(my_class) < 75) {
  if (mean(my_class) > me) {
    print("average year, but still smarter than me")
  } else {
    print("average year, but I'm not that bad")
  }
} else {
  if (mean(my_class) > me) {
    print("smart year, even smarter than me")
  } else {
    print("smart year, but I am smarter")
  }
}

# Create top_grades
top_grades <- my_class[my_class >= 85]

# Create worst_grades
worst_grades <- my_class[my_class < 65]

# Write conditional statement
if (length(top_grades) > length(worst_grades)) { print("top grades prevail") }

```
  
  
###_Writing Functions in R_  
Hadley and Charlotte Wickham led a course on writing functions in R.  Broadly, the course includes advice on when/how to use functions, as well as specific advice about commands available through library(purrr).  
  
Key pieces of advice include:  
  
* Write a function once you have cut and paste some code twice or more  
* Solve a simple problem before writing the function  
* A good function is both correct and understandable  
* Abstract away the for loops when possible (focus on data/actions, solve iteration more easily, have more understandable code), for example using purrr::map() or purr::map_<type>() where type can be dbl, chr, lgl, int, forcing a type-certain output  
* Use purrr::safely() and purrr::possibly() for better error handling  
* Use purr::pmap or purr::walk2 to iterate over 2+ arguments  
* Iterate functions for their side effects (printing, plotting, etc.) using purrr::walk()  
* Use stop() and stopifnot() for error catching of function arguments/output formats  
* Avoid type-inconsistent functions (e.g., sapply)  
* Avoid non-standard functions  
* Never rely on global options (e.g., how the user will have set stringsAsFactors)  
  
John Chambers gave a few useful slogans about functions:  
  
* Everything that exists is an object  
* Everything that happens is a function call  
  
Each function has three components:  
  
* formals(x) are in essence the arguments as in args(), but as a list  
* body(x) is the function code  
* environment(x) is where it was defined
  
Only the LAST evaluated expression is returned.  The use of return() is recommended only for early-returns in a special case (for example, when a break() will be called).  
  
Further, functions can be written anonymously on the command line, such as (function (x) {x + 1}) (1:5).  A function should only depend on arguments passed to it, not variables from a parent enviornment.  Every time the function is called, it receives a clean working environment.  Once it finishes, its variables are no longer available unless they were returned (either by default as the last operation, or by way of return()):  
  
```{r}
# Components of a function
args(rnorm)
formals(rnorm)
body(rnorm)
environment(rnorm)


# What is passed back
funDummy <- function(x) {
    if (x <= 2) {
        print("That is too small")
        return(3)  # This ends the function by convention
    }
    ceiling(x)  # This is the defaulted return() value if nothing happened to prevent the code getting here
}

funDummy(1)
funDummy(5)


# Anonymous functions
(function (x) {x + 1}) (1:5)

```
  
The course includes some insightful discussion of vectors.  As it happens, lists and data frames are just special collections of vectors in R.  Each column of a data frame is a vector, while each element of a list is either 1) an embedded data frame (which is eventually a vector by way of columns), 2) an embedded list (which is eventually a vector by way of recursion), or 3) an actual vector.  
  
The atomic vectors are of types logical, integer, character, and double; complex and raw are rarer types that are also available.  Lists are just recursive vectors, which is to say that lists can contain other lists and can be hetergeneous.  To explore vectors, you have:  
  
* typeof() for the type  
* length() for the length  
  
Note that NULL is the absence of a vector and has length 0.  NA is the absence of an element in the vector and has length 1.  All math operations with NA return NA; for example NA == NA will return NA.  
  
There are some good tips on extracting element from a list:  
  
* [] is to extract a sub-list  
* [[]] and $ more common and extract elements while removing an element of hierachy  
* seq_along(mtcars) will return 1:11 since there are 11 elements.  Helfpully, is applied to a frame with no columns, this returns integer(0) which means the for() loop does not crash  
* mtcars[[11]] will return the 11th element (11th column) of mtcars  
* vector("type", "length") will create a n empty vector of the requested type and length  
* range(x, na.rm=FALSE) gives vector c(xmin, xmax) which can be handy for plotting, scaling, and the like  
  
```{r}
# Data types
data(mtcars)
str(mtcars)
typeof(mtcars)  # n.b. that this is technically a "list"
length(mtcars)


# NULL and NA
length(NULL)
typeof(NULL)
length(NA)
typeof(NA)
NULL == NULL
NULL == NA
NA == NA
is.null(NULL)
is.null(NA)
is.na(NULL)
is.na(NA)


# Extraction
mtcars[["mpg"]][1:5]
mtcars[[2]][1:5]
mtcars$hp[1:5]


# Relevant lengths
seq_along(mtcars)
x <- data.frame()
seq_along(x)
length(seq_along(x))

foo <- function(x) { for (eachCol in seq_along(x)) { print(typeof(x[[eachCol]])) }}
foo(mtcars)
foo(x)  # Note that this does nothing!
data(airquality)
str(airquality)
foo(airquality)


# Range command
mpgRange <- range(mtcars$mpg)
mpgRange
mpgScale <- (mtcars$mpg - mpgRange[1]) / (mpgRange[2] - mpgRange[1])
summary(mpgScale)
```
  
The typical arguments in a function use a consistent, simple naming function:  
  
* x, y, z: vectors  
* df: data frame  
* i, j: numeric indices (generally rows and columns)  
* n: length of number of rows  
* p: number of columns  
  
Data arguments should come before detail arguments, and detail arguments should be given reasonable default values.  See for example rnorm(n, mean=0, sd=1).  The number requested (n) must be specified, but defaults are available for the details (mean and standard deviation).  
  
####_Functional Programming and library(purrr)_  
Functions can be passed as arguments to other functions, which is at the core of functional programming.  For example:  
```{r}
do_math <- function(x, fun) { fun(x) }
do_math(1:10, fun=mean)
do_math(1:10, fun=sd)
```
  
The library(purrr) takes advantage of this, and in a type-consistent manner.  There are functions for:  
  
* map() will create a list as the output  
* map_chr() will create a character vector as the output  
* map_dbl() will create a double vector as the output  
* map_int() will create an integer vector as the output  
* map_lgl() will create a logical (boolean) vector as the output  
  
The general arguments are .x (a list or an atomic vector) and .f which can be either a function, an anonymous function (formula with ~), or an extractor .x[[.f]].  For example:  
```{r}
library(purrr)
library(RColorBrewer)  # Need to have in non-cached chunk for later

data(mtcars)

# Create output as a list
map(.x=mtcars, .f=sum)

# Create same output as a double
map_dbl(.x=mtcars, .f=sum)

# Create same output as integer
# map_int(.x=mtcars, .f=sum) . . . this would bomb since it is not actually an integere
map_int(.x=mtcars, .f=function(x) { as.integer(round(sum(x), 0)) } )

# Same thing but using an anonymous function with ~ and .
map_int(.x=mtcars, .f = ~ as.integer(round(sum(.), 0)) )

# Create a boolean vector
map_lgl(.x=mtcars, .f = ~ ifelse(sum(.) > 200, TRUE, FALSE) )

# Create a character vector
map_chr(.x=mtcars, .f = ~ ifelse(sum(.) > 200, "Large", "Not So Large") )

# Use the extractor [pulls the first row]
map_dbl(.x=mtcars, .f=1)

# Example from help file using chaining
mtcars %>%
  split(.$cyl) %>%
  map(~ lm(mpg ~ wt, data = .x)) %>%
  map(summary) %>%
  map_dbl("r.squared")

# Using sapply
sapply(split(mtcars, mtcars$cyl), FUN=function(.x) { summary(lm(mpg ~ wt, data=.x))$r.squared } )

# Use the extractor from a list
cylSplit <- split(mtcars, mtcars$cyl)
map(cylSplit, "mpg")
map(cylSplit, "cyl")
```
  
The purrr library has several additional interesting functions:  
  
* safely() is a wrapper for any functions that traps the errors and returns a relevant list  
* possibly() is similar to safely() with the exception that a default value for error cases is supplied  
* quietly() is a wrapper to suppress verbosity  
* transpose() reverses the order of lists (making the inner-most lists the outer-most lists), which is an easy way to extract either all the answers or all the error cases  
* map2(.x, .y, .f) allows two inputs to be passed to map()  
* pmap(.l, .f) allows passing a named list with as many inputs as needed to function .f  
* invoke_map(.f, .x, ...) lets you iterate over a list of functions .f  
* walk() is like map() but called solely to get function side effects (plot, save, etc.); it also returns the object that is passed to it, which can be convenient for chaining (piping)  
  
Some example code includes:  
```{r}
library(purrr)  # Called again for clarity; all these key functions belong to purrr

# safely(.f, otherwise = NULL, quiet = TRUE)
safe_log10 <- safely(log10)
map(list(0, 1, 10, "a"), .f=safe_log10)

# possibly(.f, otherwise, quiet = TRUE)
poss_log10 <- possibly(log10, otherwise=NaN)
map_dbl(list(0, 1, 10, "a"), .f=poss_log10)

# transpose() - note that this can become masked by data.table::transpose() so be careful
purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))
purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$result
unlist(purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$result)
purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$error
map_lgl(purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$error, is.null)

# map2(.x, .y, .f)
map2(list(5, 10, 20), list(1, 2, 3), .f=rnorm) # rnorm(5, 1), rnorm(10, 2), and rnorm(20, 3)

# pmap(.l, .f)
pmap(list(n=list(5, 10, 20), mean=list(1, 5, 10), sd=list(0.1, 0.5, 0.1)), rnorm)

# invoke_map(.f, .x, ...)
invoke_map(list(rnorm, runif, rexp), n=5)

# walk() is for the side effects of a function
x <- list(1, "\n\ta\n", 3)
x %>% walk(cat)

# Chaining is available by way of the %>% operator
pretty_titles <- c("N(0, 1)", "Uniform(0, 1)", "Exponential (rate=1)")
set.seed(1607120947)
x <- invoke_map(list(rnorm, runif, rexp), n=5000)
foo <- function(x) { map(x, .f=summary) }
par(mfrow=c(1, 3))
pwalk(list(x=x, main=pretty_titles), .f=hist, xlab="", col="light blue") %>% map(.f=foo)
par(mfrow=c(1, 1))

```
  
####_Writing Robust Functions_  
There are two potentially desirable behaviors with functions:  
  
* Relaxed (default R approach) - make reasonable guesses about what you mean, which is particularly useful for interactive analyses  
* Robust (programming) - strict functions that throw errors rather than guessing in light of uncertainty  
  
As a best practice, R functions that will be used for programming (as opposed to interactive command line work) should be written in a robust manner.  Three standard problems should be avoided/mitigated:  
  
* Type-unstable - may return a vector one time, and a list the next  
* Non-standard evaluation - can use succinct API, but can introduce ambiguity  
* Hidden arguments - dependence on global functions/environments  
  
There are several methods available for throwing errors within an R function:  
  
* stopifnot(expression) will stop and throw an error unless expression is TRUE  
* if (expression) { stop("Error", call.=FALSE) }  
* if (expression) { stop(" 'x' should be a character vector", call.=FALSE) }  
    * call.=FALSE means that the call to the function should not be shown (???) - Hadley recommends this  
  
One example that commonly creates surprises is the [,] operator for extraction.  Adding [ , , drop=FALSE] ensures that you will still have what you passed (e.g., a matrix or data frame) rather than conversion of a chunk of data to a vector.

Another common source of error is sapply() which will return a vector when it can and a list otherwise.  The map() and map_typ() functions in purrr are designed to be type-stable; if the output is not as expected, they will error out.  
  
Non-standard evaluations take advantage of the existence of something else (e.g., a variable in the parent environment that has not been passed).  This can cause confusion and improper results.  
  
* subset(mtcars, disp > 400) takes advantage of disp being an element of mtcars; disp would crash if called outside subset  
* This can cause problems when it is embedded inside a function  
* ggplot and dplyr frequently have these behaviors also  
    * The risk is that you can also put variables from the global environment in to the same call  
  
Pure functions have the key properties that 1) their output depends only on their inputs, and 2) they do not impact the outside world other than by way of their return value.  Specifically, the function should not depend on how the user has configured their global options as shown in options(), nor should it modify those options() settings upon return of control to the parent environment.  
  
A few examples are shown below:  
```{r}
# Throwing errors to stop a function (cannot actually run these!)
# stopifnot(FALSE)
# if (FALSE) { stop("Error: ", call.=FALSE) }
# if (FALSE) { stop("Error: This condition needed to be set as TRUE", call.=FALSE) }

# Behavior of [,] and [,,drop=FALSE]
mtxTest <- matrix(data=1:9, nrow=3, byrow=TRUE)
class(mtxTest)
mtxTest[1, ]
class(mtxTest[1, ])
mtxTest[1, , drop=FALSE]
class(mtxTest[1, , drop=FALSE])

# Behavior of sapply() - may not get what you are expecting
foo <- function(x) { x^2 }
sapply(1:5, FUN=foo)
class(sapply(1:5, FUN=foo))
sapply(c(1, list(1.5, 2, 2.5), 3, 4, 5), FUN=foo)
class(sapply(c(1, list(1.5, 2, 2.5), 3, 4, 5), FUN=foo))
sapply(list(1, c(1.5, 2, 2.5), 3, 4, 5), FUN=foo)
class(sapply(list(1, c(1.5, 2, 2.5), 3, 4, 5), FUN=foo))

```
  
This was a very enjoyable and instructive course.  
  
###_Importing Data in to R_  
This course provides an overview of loading data in to R from five main sources:  
  
* Flat files  
* Excel files  
* Statistical software  
* Databases  
* Web data  
  
####_Reading Flat Files_  
At the most basic level, the utlis library easily handles reading most types of flat files:  
  
* read.table(file, header=FALSE, sep="", stringsAsFactors=default.stringsAsFactors(), <many more>)  
* read.csv(file, header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "", ...)  
* read.delim(file, header = TRUE, sep = "\t", quote = "\"",  dec = ".", fill = TRUE, comment.char = "", ...)  
  
There are also European equivalents in case the decimal needs to be set as "," to read in the file:  
  
* read.csv2(file, header = TRUE, sep = ";", quote = "\"", dec = ",", fill = TRUE, comment.char = "", ...)  
* read.delim2(file, header = TRUE, sep = "\t", quote = "\"", dec = ",", fill = TRUE, comment.char = "", ...)  
  
The file.path() command is a nice way to put together file paths.  It is more or less equivalent to paste(, sep="/"), but with the benefit that sep is machine/operating-system dependent, so it may be easier to use across platforms.  
  
Further, there is the option to use colClasses() to specify the type in each column, with NULL meaning do not import.  Abbreviations can be used for these as well:  
```{r}
# colClasses (relevant abbreviations)
R.utils::colClasses("-?cdfilnrzDP")

# file.path example
file.path("..", "myplot.pdf")

# Key documentation for reading flat files
# 
# read.table(file, header = FALSE, sep = "", quote = "\"'",
#            dec = ".", numerals = c("allow.loss", "warn.loss", "no.loss"),
#            row.names, col.names, as.is = !stringsAsFactors,
#            na.strings = "NA", colClasses = NA, nrows = -1,
#            skip = 0, check.names = TRUE, fill = !blank.lines.skip,
#            strip.white = FALSE, blank.lines.skip = TRUE,
#            comment.char = "#",
#            allowEscapes = FALSE, flush = FALSE,
#            stringsAsFactors = default.stringsAsFactors(),
#            fileEncoding = "", encoding = "unknown", text, skipNul = FALSE)
# 
# read.csv(file, header = TRUE, sep = ",", quote = "\"",
#          dec = ".", fill = TRUE, comment.char = "", ...)
# 
# read.csv2(file, header = TRUE, sep = ";", quote = "\"",
#           dec = ",", fill = TRUE, comment.char = "", ...)
# 
# read.delim(file, header = TRUE, sep = "\t", quote = "\"",
#            dec = ".", fill = TRUE, comment.char = "", ...)
# 
# read.delim2(file, header = TRUE, sep = "\t", quote = "\"",
#             dec = ",", fill = TRUE, comment.char = "", ...)
```
  
There are also two libraries that can be especially helpful for reading in flat files - readr and data.table.  
  
* readr::read_delim() handles many data types  
* readr::read_delim(file, delim=",") will read a CSV
    * assumes col_names=TRUE (eq to header=TRUE)
    * assumes col_types=NULL (imputed from first 100 rows, side effect - no need for stringAsFactors = FALSE)  
* col_types can use short type, where c=character, d=double (numeric), i=integer, l=logical (boolean), _=skip  
    * col_names = FALSE means make your own  
    * col_names = c() means here are the column names you should use  
* skip=<number to skip>  
* n_max=<number to read>  
* read_csv() is for CSV  
* read_tsv is for tab-separated values	
  
* data.table() is designed for speed  
    * data.table::fread() is for fast reading  
    * The fread() automatically handles the column names and also infers the column separators  
    * This is a faster, more convenients, and easier to customize version of read.table()  
  
* Wrappers for the readr() function  
    * fac <- col_factor(levels = c("Beef", "Meat", "Poultry"))  
    * int <- col_integer()  
	* hotdogsFactor <- read_tsv("hotdogs.txt",  
	                            col_names = c("type", "calories", "sodium"), 
	                            col_types = list(fac, int, int)
	                            )
  
####_Reading Excel Files_    
Further, the library(readxl) is handy for loading Excel sheets:  
  
* readxl::excel_sheets() will list the sheets  
    * excel_sheets(path)  
* readxl::read_excel() will read in a specific sheet  
    * read_excel(path, sheet = 1, col_names = TRUE, col_types = NULL, na = "", skip=0)	
        * col_names: Either TRUE to use the first row as column names, FALSE to number columns sequentially     from X1 to Xn, or a character vector giving a name for each column  
        * col_types: Either NULL to guess from the spreadsheet or a character vector containing "blank", "numeric", "date" or "text"  
* lapply(excel_sheets(myXLS), FUN=read_excel, path=myXLS) provide all data from all sheets in a list  
  
####_Reading Statistical Software Files_  
R can also load files from common statistical software such as SAS, STATA, SPSS, and MATLAB/Octave.  The packages haven() by Wickham and foreign() by the R core team are two common examples.  The R.matlab() allows for reading to/from MATLAB/Octave:  
  
The library(haven) contains wrappers to the ReadStat package, a C library by Evan Miller, for reading files from SAS, STATA, and SPSS:  
  
* read_sas(filename)	
* read_stata(filename)  
* read_dta(filename)  
    * as_factor(R_column) will help if the type is "labelled"  
    * as.character(as_factor(R_column)) will turn it back to a character vector  
* read_spss(filename) which is a wrapper to read_por() and read_sav()	
  
The library(foreign) can read/write all types of foreign formats, with some caveats:  
  
* Only SAS libraries (.xport) can be read in; seems quite a drawback to not be able to read SAS files!  
* read.dta(file, convert.factors = TRUE, convert.dates=TRUE, missing.type=FALSE)  
* read.spss(file, use.value.labels = TRUE, to.data.frame = FALSE)  
  
Finally, the R.matlab() library is available for reading/writing MATLAB/Octave files.  Per the help file:   
  
* Methods readMat() and writeMat() for reading and writing MAT files. For user with MATLAB v6 or newer installed (either locally or on a remote host), the package also provides methods for controlling MATLAB (trademark) via R and sending and retrieving data between R and MATLAB.  
  
* In brief, this package provides a one-directional interface from R to MATLAB, with communication taking place via a TCP/IP connection and with data transferred either through another connection or via the file system. On the MATLAB side, the TCP/IP connection is handled by a small Java add-on.  
  
* The methods for reading and writing MAT files are stable. The R to MATLAB interface, that is the Matlab class, is less prioritized and should be considered a beta version.  
  
* readMat(con, maxLength=NULL, fixNames=TRUE, drop=c("singletonLists"), sparseMatrixClass=c("Matrix", "SparseM", "matrix"), verbose=FALSE, ...)  
    * Returns a named list structure containing all variables in the MAT file structure.  
  
* writeMat(con, ..., matVersion="5", onWrite=NULL, verbose=FALSE)  
    * con: Binary connection to which the MAT file structure should be written to. A string is interpreted as filename, which then will be opened (and closed afterwards).  
    * ...: Named variables to be written where the names must be unique.  
  
####_Reading Relational DB Files_  
Relational databases in R (DBMS tend to use SQL for queries), including libraries:  
  
* RMySQL  
* RPostgresSQL  
* ROracle  
* RSQLite  

Conventions are specified in DBI; see library(DBI):  
  
* dbConnect(drv, ...)  
* drv  
  
Create the connection as "con" (or whatever) and then use that elsewhere:  
  
* dbListTables(con)  # What tables are in this  
* dbReadTable(con, tablename)  
  
When finished, dbDisconnect(con) as a courtesy so as to not tie up resources.  
  
SQL queries from inside R - per previous, library(DBI) and then create the connection "con":  
  
* dbGetQuery(con, valid_SQL_code)  
    * Appears that \" is an escape character to use quotes inside quotes  
    * dbGetQuery(con, "SELECT name FROM employees WHERE started_at > \"2012-09-01\"")  
    * dbGetQuery(con, "SELECT * FROM products WHERE contract=1")  
* res <- dbSendQuery(con, validSQLcode) # If instead you want to grab only chunks of records  
    * dbFetch(res)  # Can specify chunking, such as dbFetch(res, n=1) for one line at a time  
    * dbHasCompleted(res) # see whether it is done  
        * while(!dbHasCompleted(res)) { chunk <- dbFetch(res, n=1); print(chunk) }  
    * dbClearResult(res)  
  
For example, using "./SQLforDataCampRMD_v01.db", run a few SQL commands:  
```{r}
# uses libraries DBI for the connection and RSQLite to interface with SQLite Browser on my machine
con <- DBI::dbConnect(RSQLite::SQLite(), "SQLforDataCampRMD_v01.db")

# List the tables, and drop dummy if it already exists
DBI::dbListTables(con)
DBI::dbGetQuery(con, "DROP TABLE IF EXISTS dummy")

# Create blank table
DBI::dbListTables(con)
DBI::dbGetQuery(con, "CREATE TABLE IF NOT EXISTS dummy (id PRIMARY KEY, name CHAR)")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (1, 'Amy')")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (2, 'Bill')")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (2, 'Jen')") # Should do nothing
DBI::dbGetQuery(con, "SELECT * FROM dummy")
DBI::dbListTables(con)

# Can continue passing SQL commands back and forth as needed

# Close the connection
DBI::dbDisconnect(con)

```
  
####_Reading Web Data_  
Many of the R read-in libraries already work well with web data.  For example, read.csv("mywebsite.com", stringAsFactors=FALSE) will read a CSV right off the internet.  Further, there are options for:  
  
* download.file(url, dest_path) # Reproducibility advantages over right-click and save  
* library(httr) by Hadley Wickham, including GET() and content()  
  
The jsonlite library is good for working with JSON:  
  
* fromJSON(url) will create a named R list (often creates a data frame also)  
    * JSON objects are name:value pairs  
    * JSON arrays convert to vectors  
    * JSON can also created nested arrays  
* toJSON()  
  
Prettify adds indentation to a JSON string; minify removes all indentation/whitespace:  
  
* prettify()    prettify(txt, indent = 4)  
* minify()	    minify(txt)  
  
```{r}
jsonLoc <- file.path("../../..", "PythonDirectory", "UMModule04", "roster_data.json")
jsonData <- jsonlite::fromJSON(jsonLoc)
str(jsonData)
head(jsonData)
```
  
###_Cleaning Data in R_  
The general analysis pipeline is Collect -> Clean -> Analyze -> Report.  Cleaning is needed so the raw data can work with more traditional tools (e.g., packages in Python or R).  50% - 80% of time is spent in the Collect/Clean realm, even though this is not the most exciting (and thus taught) part of data analysis.  There are generally three stages of data cleaning:  Explore -> Tidy -> Prepare
  
Exploring the Data:  
  
* class()  
* dim()  
* names()  # column names  
* str()  
* dplyr::glimpse()  # a nicer version of str()  
* summary()  
  
Viewing the Data:  
  
* head(file, n=6)  
* tail(file, n=6)  
* hist(variable)  
* plot(x, y)  
* print()  # not recommended for larger datasets  
  
Tidy data - Wickham 2014, Principles of Tidy Data:  
  
* Each row should be an observation	
* Each column should be a variable (attribute)	
* Column headers should not be variables; e.g., eye color should be a single column, not many columns of 0/1  
* Each intersection should be a value (intersection of the observation and attribute)  
* Only one type of observational unit (e.g., each row is a person) per table  
  
The principles of tidy data can be implemented using library(tidyr):  
  
* tidyr::gather(data, key, value, ..., na.rm = FALSE, convert = FALSE, factor_key = FALSE)  # gather the data in to key-value pairs  
    * key, value	Names of key and value columns to create in output.  
    * ...	Specification of columns to gather. Use bare variable names. Select all variables between x and z with x:z, exclude y with -y. For more options, see the select documentation.  
  
* tidyr::spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE)  # spread the key-value pairs in to columns  
    * key	The bare (unquoted) name of the column whose values will be used as column headings.  
    * value	The bare (unquoted) name of the column whose values will populate the cells.  
    * fill	If set, missing values will be replaced with this value. Note that there are two types of missing in the input: explicit missing values (i.e. NA), and implicit missings, rows that simply aren't present. Both types of missing value will be replaced by fill.  
  
* tidyr::separate(data, col, into, sep = "[^[:alnum:]]+", remove = TRUE, convert = FALSE, extra = "warn", fill = "warn", ...)  
    * col	Bare column name.  
    * into	Names of new variables to create as character vector.  
    * sep	Separator between columns.  If character, is interpreted as a regular expression. The default value is a regular expression that matches any sequence of non-alphanumeric values.  If numeric, interpreted as positions to split at. Positive values start at 1 at the far-left of the string; negative value start at -1 at the far-right of the string. The length of sep should be one less than into.  
  
* tidyr::unite(data, col, ..., sep = "_", remove = TRUE)  
    * col	(Bare) name of column to add  
    * ...	Specification of columns to unite. Use bare variable names. Select all variables between x and z with x:z, exclude y with -y. For more options, see the select documentation.  
    * sep	Separator to use between values.  
  
Common symptoms of messy data include:  
  
* Column headers are values rather than variable names -- use tidyr::gather()  
* Variables stored in both rows and columns -- use tidyr::spread()  
* Multiple variables are stored in a single column -- use tidyr::separate()  
* Singe type of observational unit (e.g., people) stored in 2+ tables  
* Multiple types of observational units (e.g., people and pets) stored in a single table  
  
Example code includes:  
```{r}
# tidyr::gather()
stocks <- data.frame(time = as.Date('2009-01-01') + 0:4, 
                     X = rnorm(5, 0, 1), Y = rnorm(5, 0, 2), Z = rnorm(5, 0, 4)
                     )
stocks
# will create new columns stock (each of X, Y, Z) and price (the values that had been in X, Y, and Z), 
# while not gathering the time variable; final table will be time-stock-price
stockGather <- tidyr::gather(stocks, stock, price, -time)  
stockGather

# tidyr::spread()
tidyr::spread(stockGather, stock, price)
# TRUE (this fully reverses what the gather function has done)
identical(tidyr::spread(stockGather, stock, price), stocks)  


# tidyr::separate()
df <- data.frame(x = c(NA, "a.b", "a.d", "b.c"))
df
# by default, the splits occur on anything that is not alphanumeric, 
# so you get column A as whatever is before the dot and column B as whatever is after the dot
dfSep <- tidyr::separate(df, x, c("A", "B"))
dfSep

# tidyr::unite()
tidyr::unite(dfSep, united, c(A, B), sep="")
is.na(dfSep) # caution . . . 
is.na(tidyr::unite(dfSep, united, c(A, B), sep="")) # caution . . . 
```
  
* The library(lubridate) can be helpful for coercing strings to dates  
    * ymd()  # coerces a character string that is in year-month-day originally  
    * mdy() # coerces a character string that is in month-day-year originally  
    * hms() # coerces a character string that is in hours-minutes-seconds originally  
    * ymd_hms() # coerces a character string that is in year-month-day-hours-minutes-seconds  
  
* The library(stringr) can be helpful for working with strings  
	* stringr::str_trim()  # trails the leading and trailing white space  
	* stringr::str_pad(char, width, side, pad)  # adds characters in place of white space at the start/end  
	* stringr::str_detect(inVector, searchTerm) # is searchTerm in each iterm of vector (boolean, same length as inVector)  
	* stringr::str_replace(inVector, searchTerm, replaceTerm) # searchTerm will be replaced by replaceTerm in each iterm of vector (output same length/type as inVector)  
        * social_df$status <- str_replace(social_df$status, "^$", NA) # Nice way to make the blanks in to NA  
  
* The tolower() and toupper() commands can be very useful also  
  
* Missing and special values  
    * May be randomly missing, but very dangerous to assume!  
    * In R, these are NA  
        * Excel may have #N/A  
        * SPSS and SAS may have .  
        * Sometimes just shows up as a missing string  
    * Inf is for infinite  
    * NaN is for not a number  
  
* Finding these special values  
    * is.na()  
	* any(is.na())  
	* sum(is.na())  
	* summary()  # will tell the number of NA  
	* complete.cases()  # TRUE is the entire row is non-NA; FALSE otherwise  
	* na.omit()  # equivalent to x[complete.cases(x), ]  
  
Example code includes:  
```{r}
# lubridate::ymd()
lubridate::ymd("160720")
lubridate::ymd("2016-7-20")
lubridate::ymd("16jul20")
lubridate::ymd("16/07/20")

# lubridate::hms()
lubridate::hms("07h15:00")
lubridate::hms("17 hours, 15 minutes 00 seconds")
lubridate::hms("07-15-00")

# From ?stringr::str_detect
# 
# str_detect(string, pattern)
#   string	Input vector. Either a character vector, or something coercible to one.
#   pattern	Pattern to look for.  The default interpretation is a regular expression, as described in stringi-search-regex. Control options with regex().  Match a fixed string (i.e. by comparing only bytes), using fixed(x). This is fast, but approximate. Generally, for matching human text, you'll want coll(x) which respects character matching rules for the specified locale.  Match character, word, line and sentence boundaries with boundary(). An empty pattern, "", is equivalent to boundary("character").
# 

fruit <- c("apple", "banana", "pear", "pinapple")

stringr::str_detect(fruit, "a")
stringr::str_detect(fruit, "^a")
stringr::str_detect(fruit, "a$")
stringr::str_detect(fruit, "b")
stringr::str_detect(fruit, "[aeiou]")

# Also vectorised over pattern
stringr::str_detect("aecfg", letters)


# From ?stringr::str_replace
#
# str_replace(string, pattern, replacement)
# str_replace_all(string, pattern, replacement)
#   string	Input vector. Either a character vector, or something coercible to one.
#   pattern, replacement	Supply separate pattern and replacement strings to vectorise over the patterns. References of the form \1, \2 will be replaced with the contents of the respective matched group (created by ()) within the pattern.  For str_replace_all only, you can perform multiple patterns and replacements to each string, by passing a named character to pattern.
#

someNA <- c(letters, "", LETTERS, "")
someNA[someNA==""] <- NA
someNA

fruits <- c("one apple", "two pears", "three bananas")
stringr::str_replace(fruits, "[aeiou]", "-")  # Replace FIRST instance
stringr::str_replace_all(fruits, "[aeiou]", "-")  # Replace ALL instances

stringr::str_replace(fruits, "([aeiou])", "\\1\\1\\1")  # Triple up on the first vowel
stringr::str_replace(fruits, "[aeiou]", c("1", "2", "3"))  # First vowel to 1, 2, 3 in word 1, 2, 3
stringr::str_replace(fruits, c("a", "e", "i"), "-")  # First a -> - in word 1, first e -> - in word 2 . . . 

stringr::str_replace_all(fruits, "([aeiou])", "\\1\\1")  # Double up on all vowels
stringr::str_replace_all(fruits, "[aeiou]", c("1", "2", "3"))  # All vowels to 1, 2, 3, in word 1, 2, 3
stringr::str_replace_all(fruits, c("a", "e", "i"), "-")  # All a -> - in word 1, . . . 

```
  
Further, the outline from the weather gathering data cleaning challenge is noted:  
  
* The weather data tidying challenge  
	* Historical weather data from Boston  
	    * 12 months beginning December 2014  
	    * Columns are values (X1 means day 1, X2 means day 2, etc.), while measure (Max, Min, etc.) should be variables  
		* Variables coded incorrectly  
		* Missing and extreme values  
		* Etc.  
	
* STEP 1: UNDERSTAND STRUCTURE  
	* class(), dim(), names(), str(), dplyr::glimpse(), summary()  
* STEP 2: LOOK AT DATA  
	* head(), tail(), print(), hist(), plot()  
* STEP 3: TIDY DATA  
	* gather(), spread(), separate()  
* STEP 4: CONVERT TYPES  
    * lubridate() and variants, as.character() and variants, stringr() and variants, tidyr::unite()  
* STEP 5: MANAGE MISSING and EXTREME (OUTLIER) VALUES  
    * is.na(), sum(is.na()), any(is.na()), which(is.na())  
    * which(a, arr.ind=TRUE) returns a little matrix of rows and columns - nice!	
  
###_Data Manipulation (dplyr)_  
The library(dplyr) is a grammar of data manipulation.  It is written in C++ so you get the speed of C with the convenience of R.  It is in essence the data frame to data frame portion of plyr (plyr was the original Split-Apply-Combine).  May want to look in to count, transmute, and other verbs added post this summary.  
  
The examples use data(hflights) from library(hflights):  
```{r}
library(dplyr)
library(hflights)
data(hflights)
head(hflights)
summary(hflights)
```
  
The "tbl" is a special type of data frame, which is very helpful for printing:  
  
* tbl_df(myFrame)  # can store or whatever - will be a tbl_df, tbl, and data.frame  
    * Display is modified to fit the window display - will scale with the window  
* glimpse(myFrame) # lets you see al the variables and first few records for each (sort of like str)  
* as.data.frame(tbl_df(myFrame)) # this will be the data frame  
    * identical(as.data.frame(tbl_df(hflights)), hflights)  # FALSE  
    * sum(is.na(as.data.frame(tbl_df(hflights))) != is.na(hflights))  # 0  
    * sum(as.data.frame(tbl_df(hflights)) != hflights, na.rm=TRUE)  # 0  
  
An interesting way to do a lookup table:  
  
* two <- c("AA", "AS")  
* lut <- c("AA" = "American",  "AS" = "Alaska",  "B6" = "JetBlue")  
* two <- lut[two]  
* two  
  
See for example:  
```{r}
lut <- c("AA" = "American", "AS" = "Alaska", "B6" = "JetBlue", "CO" = "Continental", 
         "DL" = "Delta", "OO" = "SkyWest", "UA" = "United", "US" = "US_Airways", 
         "WN" = "Southwest", "EV" = "Atlantic_Southeast", "F9" = "Frontier", 
         "FL" = "AirTran", "MQ" = "American_Eagle", "XE" = "ExpressJet", "YV" = "Mesa"
         )
hflights$Carrier <- lut[hflights$UniqueCarrier]  
glimpse(hflights)  
```
  
There are five main verbs in dplyr:  
  
* select - subset of columns from a dataset  
    * select(df, . . . ) where . . . Are the columns to be kept  
	* starts_with("X"): every name that starts with "X",  
	* ends_with("X"): every name that ends with "X",  
	* contains("X"): every name that contains "X",  
	* matches("X"): every name that matches "X", where "X" can be a regular expression,  
	* num_range("x", 1:5): the variables named x01, x02, x03, x04 and x05,  
	* one_of(x): every name that appears in x, which should be a character vector.  
	* filter - subset of rows from a dataset  
        * filter(df, .)  where ... are 1+ logical tests (so make sure to use == or all.equal() or the like)  
* arrange - reorder rows in a dataset  
    * arrange(df, .) where . are the colunns to reorder by  
* mutate - create new columns in a dataset  
    * mutate(df, .) where each . is a formula for a new variable to be created  
* summarize - create summary statistics for a dataset  
    * summarize(df, .) where each . is a formula like newVar = thisEquation  
        * only aggregate functions (vector as input, single number as output) should be used  
    * dplyr adds several additional aggregate functions such as first, last, nth, n, n_distinct  
		* first(x) - The first element of vector x.  
		* last(x) - The last element of vector x.  
		* nth(x, n) - The nth element of vector x.  
		* n() - The number of rows in the data.frame or group of observations that summarise() describes.  
		* n_distinct(x) - The number of unique values in vector x.  
  
* In general:  
    * select and mutate operate on the variables  
    * filter and arrange operate on the observations  
    * summarize operates on groups of observations  
	* All of these are much cleaner if the data are tidy  
  
* There is also the option to use chaining %>% to process multiple commands		
    * Especially useful for memory storage and readability  
	* The pipe operator (%>%) comes from the magrittr package by Stefan Bache  
	* object %>% function(object will go first)  
	* c(1, 2, 3) %>% sum() # 6  
	* c(1, 2, 3, NA) %>% mean(na.rm=TRUE) # 2  
  
There is also the group_by capability for summaries of sub-groups:  
  
* group_by(df, .) where the . is what to group the data by  
    * The magic is when you run summarize() on data with group_by run on it; results will be by group  
	* If you have group_by(df, a, b) %>% summarize(timeSum = sum(time)) # all observations by a-b  
	* If you have group_by(df, a, b) %>% summarize(timeSum = sum(time)) %>% summarize(timeA = sum(timeSum)) # all observations by a  
	* If you have group_by(df, a, b) %>% summarize(timeSum = sum(time)) %>% summarize(timeA = sum(timeSum)) %>% summarize(timeAll = sum(timeA)) # all observations  
  
The dplyr library can also work with databases.  It only loads the data that you need, and you do not need to know the relevant SQL code -- dplyr writes the SQL code for you.  
  
Basic select and mutate examples include:  
```{r}
data(hflights)

# Make it faster, as well as a prettier printer
hflights <- tbl_df(hflights)
hflights
class(hflights)

# Select examples
select(hflights, ActualElapsedTime, AirTime, ArrDelay, DepDelay)
select(hflights, Origin:Cancelled)
select(hflights, Year:DayOfWeek, ArrDelay:Diverted)
select(hflights, ends_with("Delay"))
select(hflights, UniqueCarrier, ends_with("Num"), starts_with("Cancel"))
select(hflights, ends_with("Time"), ends_with("Delay"))

# Mutate example
m1 <- mutate(hflights, loss = ArrDelay - DepDelay, loss_ratio = loss / DepDelay)
class(m1)
m1
glimpse(m1)

```
  
Additionally, examples for filter and arrange:  
```{r}

# Examples for filter

filter(hflights, Distance >= 3000)  # All flights that traveled 3000 miles or more
filter(hflights, UniqueCarrier %in% c("B6", "WN", "DL"))
filter(hflights, (TaxiIn + TaxiOut) > AirTime)  # Flights where taxiing took longer than flying
filter(hflights, DepTime < 500 | ArrTime > 2200)  # Flights departed before 5am or arrived after 10pm
filter(hflights, DepDelay > 0, ArrDelay < 0)  # Flights that departed late but arrived ahead of schedule
filter(hflights, Cancelled == 1, DepDelay > 0) # Flights that were cancelled after being delayed

c1 <- filter(hflights, Dest == "JFK")  # Flights that had JFK as their destination: c1
c2 <- mutate(c1, Date = paste(Year, Month, DayofMonth, sep="-"))  # Create a Date column: c2
select(c2, Date, DepTime, ArrTime, TailNum)  # Print out a selection of columns of c2
dtc <- filter(hflights, Cancelled == 1, !is.na(DepDelay))  # Definition of dtc


# Examples for arrange

arrange(dtc, DepDelay)  # Arrange dtc by departure delays
arrange(dtc, CancellationCode)  # Arrange dtc so that cancellation reasons are grouped
arrange(dtc, UniqueCarrier, DepDelay)  # Arrange dtc according to carrier and departure delays
arrange(hflights, UniqueCarrier, desc(DepDelay))  # Arrange by carrier and decreasing departure delays
arrange(hflights, DepDelay + ArrDelay)  # Arrange flights by total delay (normal order)

```
  
Additionally, examples for the summarize verb:  
```{r}
# Print out a summary with variables min_dist and max_dist
summarize(hflights, min_dist = min(Distance), max_dist = max(Distance))

# Print out a summary with variable max_div
summarize(filter(hflights, Diverted == 1), max_div = max(Distance))

# Remove rows that have NA ArrDelay: temp1
temp1 <- filter(hflights, !is.na(ArrDelay))

# Generate summary about ArrDelay column of temp1
summarize(temp1, earliest=min(ArrDelay), average=mean(ArrDelay), latest=max(ArrDelay), sd=sd(ArrDelay))

# Keep rows that have no NA TaxiIn and no NA TaxiOut: temp2
temp2 <- filter(hflights, !is.na(TaxiIn), !is.na(TaxiOut))

# Print the maximum taxiing difference of temp2 with summarise()
summarize(temp2, max_taxi_diff = max(abs(TaxiIn - TaxiOut)))

# Generate summarizing statistics for hflights
summarize(hflights, n_obs = n(), n_carrier = n_distinct(UniqueCarrier), n_dest = n_distinct(Dest))

# All American Airline flights
aa <- filter(hflights, UniqueCarrier == "AA")

# Generate summarizing statistics for aa 
summarize(aa, n_flights = n(), n_canc = sum(Cancelled), avg_delay = mean(ArrDelay, na.rm=TRUE))

```
  
Additionally, examples for the pipe/chain as per magrittr:  
```{r}
# Find the average delta in taxi times
hflights %>%
    mutate(diff = (TaxiOut - TaxiIn)) %>%
    filter(!is.na(diff)) %>%
    summarize(avg = mean(diff))

# Find flights that average less than 70 mph assuming 100 wasted minutes per flight
hflights %>%
    mutate(RealTime = ActualElapsedTime + 100, mph = 60 * Distance / RealTime) %>%
    filter(!is.na(mph), mph < 70) %>%
    summarize(n_less = n(), n_dest = n_distinct(Dest), min_dist = min(Distance), max_dist = max(Distance))

# Find flights that average less than 105 mph, or that are diverted/cancelled
hflights %>%
  mutate(RealTime = ActualElapsedTime + 100, mph = Distance / RealTime * 60) %>%
  filter(mph < 105 | Cancelled == 1 | Diverted == 1) %>%
  summarize(n_non = n(), n_dest = n_distinct(Dest), min_dist = min(Distance), max_dist = max(Distance))

# Find overnight flights
filter(hflights, !is.na(DepTime), !is.na(ArrTime), DepTime > ArrTime) %>%
    summarize(num = n())

```
  
There is also the group_by capability, typically for use with summarize:  
```{r}
# Make an ordered per-carrier summary of hflights
group_by(hflights, UniqueCarrier) %>%
    summarize(p_canc = 100 * mean(Cancelled, na.rm=TRUE), avg_delay = mean(ArrDelay, na.rm=TRUE)) %>%
    arrange(avg_delay, p_canc)

# Ordered overview of average arrival delays per carrier
hflights %>%
    filter(!is.na(ArrDelay), ArrDelay > 0) %>%
    group_by(UniqueCarrier) %>%
    summarize(avg = mean(ArrDelay)) %>%
    mutate(rank = rank(avg)) %>%
    arrange(rank)

# How many airplanes only flew to one destination?
hflights %>%
  group_by(TailNum) %>%
  summarise(destPerTail = n_distinct(Dest)) %>%
  filter(destPerTail == 1) %>%
  summarise(nplanes=n())

# Find the most visited destination for each carrier
hflights %>%
  group_by(UniqueCarrier, Dest) %>%
  summarise(n = n()) %>%
  mutate(rank = rank(-n)) %>%
  filter(rank == 1)

# Use summarise to calculate n_carrier
library(data.table)
hflights2 <- as.data.table(hflights)
hflights2 %>%
    summarize(n_carrier = n_distinct(UniqueCarrier))

```
  
And, dplyr can be used with databases, including writing the SQL query that matches to the dplyr request.  The results are cached to avoid constantly pinging the server:  
```{r, cache=TRUE}
# Set up a connection to the mysql database
my_db <- src_mysql(dbname = "dplyr", 
                   host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com", 
                   port = 3306, 
                   user = "student",
                   password = "datacamp")

# Reference a table within that source: nycflights
nycflights <- tbl(my_db, "dplyr")

# glimpse at nycflights
glimpse(nycflights)

# Ordered, grouped summary of nycflights
nycflights %>%
    group_by(carrier) %>%
    summarize(n_flights = n(), avg_delay = mean(arr_delay)) %>%
    arrange(avg_delay)

```
  
###_Data Manipulation (data.table)_  
The data.table library is designed to simplify and speed up work with large datasets.  The language is broadly analogous to SQL, with syntax that includes equivalents for SELECT, WHERE, and GROUP BY.  Some general attributes of a data.table object include:  
  
* Set of columns; every column is the same length but may be of different type  
* Goal #1: Reduce programming time (fewer function calls, less variable name repetition)  
* Goal #2: Reduce compute time (fast aggregation, update by reference  
* Currently in-memory (64-bit and 100 GB is routine; one-quarter-terabyte RAM is available through Amazon EC2 for a few dollars per hours)  
* Ordered joins (useful in finance/time series and also genomics)  
  
NOTE - all data.table are also data.frame, and if a package is not aware of data.table, then it will act as data.frame for that package.  
  
General syntax is:  
  
* myDataTable[condition, data/transforms, order by]  
    * Extracts all rows that meet condition, provides the requested data/transforms, and orders accordingly  
    * Analogous to SQL - WHERE, SELECT, GROUP BY  
    * DT[i, j, by]  
  
Example table creation:  
  
* DT <- data.table(A = 1:6, B=c("a", "b", "c"), C=rnorm(6), D=TRUE)  
    * "We like character vectors in data.table"  
    * Need to use 1L for integer, and NA_integer_ for NA/integer (rather than boolean)  
    * DT[3:5, ] is the same as DT[3:5] -- either will return rows 3-5  
    * Note that .N contains the number of rows  
  
* Select columns in data.table (second argument)  
    * .(B, C) is the same as list(B, C) and will select the columns named "B" and "C"  
	* .(mysum = sum(B)) will sum the entirety of column B for the rows requested and call the column mysum  
	* .(B, C= sum(C)) will recycle sum(C) everywhere and also pull B  
	* DT[,plot(A, C)] will plot A vs C  
	* DT[ , B] will return a VECTOR and not a data.table  
	* DT[ , .(B)] will return a data.table  
  	
* Using a by variable allows for sum/mean/etc. by grouping:  
	* DT[ , .(mysum = sum(B)), by=.(C)] will sum column B BY each C for the rows requested, and call the column mysum  
	* DT[ , .(mysum = sum(B)), by=.(myMod = C%%2)] will sum column B BY each Cmod2 for the rows requested, and call the column mysum  
	* Can skip the .() if you have just a single SELECT or a single GROUP BY  
		* Order depends on what it finds first -- not necessarily sorted, just aggregated BY  
  
Some example code includes:  
```{r}
library(data.table)

DT <- data.table(a = c(1, 2), b=LETTERS[1:4])
str(DT)
DT

# Print the second to last row of DT using .N
DT[.N-1]

# Print the column names of DT
names(DT)

# Print the number or rows and columns of DT
dim(DT)

# Select row 2 twice and row 3, returning a data.table with three rows where row 2 is a duplicate of row 1.
DT[c(2, 2:3)]


DT <- data.table(A = 1:5, B = letters[1:5], C = 6:10)
str(DT)
DT

# Subset rows 1 and 3, and columns B and C
DT[c(1, 3), .(B, C)]

# Assign to ans the correct value
ans <- DT[ , .(B, val=A*C)]
ans

# Fill in the blanks such that ans2 equals target
target <- data.table(B = c("a", "b", "c", "d", "e", "a", "b", "c", "d", "e"), 
                     val = as.integer(c(6:10, 1:5))
                     )
ans2 <- DT[, .(B, val = c(C, A))]
identical(target, ans2)


DT <- as.data.table(iris)
str(DT)

# For each Species, print the mean Sepal.Length
DT[ , mean(Sepal.Length), Species]

# Print mean Sepal.Length, grouping by first letter of Species
DT[ , mean(Sepal.Length), substr(Species, 1, 1)]
str(DT)
identical(DT, as.data.table(iris))


# Group the specimens by Sepal area (to the nearest 10 cm2) and count how many occur in each group.
DT[, .N, by = 10 * round(Sepal.Length * Sepal.Width / 10)]

# Now name the output columns `Area` and `Count`
DT[, .(Count=.N), by = .(Area = 10 * round(Sepal.Length * Sepal.Width / 10))]


# Create the data.table DT
set.seed(1L)
DT <- data.table(A = rep(letters[2:1], each = 4L), 
                 B = rep(1:4, each = 2L), 
                 C = sample(8)
                 )
str(DT)
DT


# Create the new data.table, DT2
DT2 <- DT[, .(C = cumsum(C)), by = .(A, B)]
str(DT2)
DT2


# Select from DT2 the last two values from C while you group by A
DT2[, .(C = tail(C, 2)), by = A]

```
  
The chaining operation in data.table is run as [statement][next statement].  
  
* The .SD means "Subset of Data"  
    * By default, .SD means all of the columns other than the columns specified in by (and only accessible in j)  
	* DT[ , lapply(.SD, median), by = Species]  
	* Recall that .() is just an alias to a list, so it is not needed for lapply (which always returns a list anyway)  
  
* The := operator is for adding by reference  
	* If it already exists, it is updated as per the call  
	* If it does not already exist, it is created  
	* DT[ , c("x", "z") := .(rev(x), 10:6)]  # will reverse x and create z as 10-9-8-7-6]  
	* Anything with := NULL will remove the columns instantly  
	* DT[ , MyCols :=NULL] will look for a column called MyCols  
	* DT[, (MyCols) := NULL] will use whatever MyCols references, allowing for MyCols to be a variable  
	* DT[2:4, z:=sum(y), by=x]  # Will create z as requested for rows 2:4 and create z=NA everywhere else; interesting (and risky perhaps .)  
  
* The set() syntax is another option:  
	* for (i in 1:5) DT[i, z := i+1]  
	* for (i in 1:5) set(DT, i, 3L, i+1])  # take DT, act on column 3 (happens to be z in this example) and makes it i+1  
  
* The setnames() syntax is yet another option  
	* setnames(DT, "old", "new")  
  
* The setcolorder() syntax is yet another option  
	* setcolorder(DT, c(new_order))  
  
* A wrap up of the set() family:  
	* set() is a loopable, low overhead version of :=  
	* You can use setnames() to set or change column names  
	* setcolorder() lets you reorder the columns of a data.table  
  
Example code includes:  
```{r}
set.seed(1L)
DT <- data.table(A = rep(letters[2:1], each = 4L), 
                 B = rep(1:4, each = 2L), 
                 C = sample(8)) 
str(DT)
DT


# Perform operation using chaining
DT[ , .(C = cumsum(C)), by = .(A, B)][ , .(C = tail(C, 2)), by=.(A)]


data(iris)
DT <- as.data.table(iris)
str(DT)


# Perform chained operations on DT
DT[ , .(Sepal.Length = median(Sepal.Length), Sepal.Width = median(Sepal.Width), 
        Petal.Length = median(Petal.Length), Petal.Width = median(Petal.Width)), 
        by=.(Species)][order(-Species)]

# Mean of columns
# DT[ , lapply(.SD, FUN=mean), by=.(x)]

# Median of columns
# DT[ , lapply(.SD, FUN=median), by=.(x)]

# Calculate the sum of the Q columns
# DT[ , lapply(.SD, FUN=sum), , .SDcols=2:4]

# Calculate the sum of columns H1 and H2 
# DT[ , lapply(.SD, FUN=sum), , .SDcols=paste0("H", 1:2)]

# Select all but the first row of groups 1 and 2, returning only the grp column and the Q columns
# foo = function(x) { x[-1] }
# DT[ , lapply(.SD, FUN=foo), by=.(grp), .SDcols=paste0("Q", 1:3)]

# Sum of all columns and the number of rows
# DT[, c(lapply(.SD, FUN=sum), .N), by=.(x), .SDcols=names(DT)]

# Cumulative sum of column x and y while grouping by x and z > 8
# DT[, lapply(.SD, FUN=cumsum), by=.(by1=x, by2=(z>8)), .SDcols=c("x", "y")]

# Chaining
# DT[, lapply(.SD, FUN=cumsum), by=.(by1=x, by2=(z>8)), .SDcols=c("x", "y")][ , lapply(.SD, FUN=max), by=.(by1), .SDcols=c("x", "y")]


# The data.table DT
DT <- data.table(A = letters[c(1, 1, 1, 2, 2)], B = 1:5)
str(DT)
DT


# Add column by reference: Total
DT[ , Total:=sum(B), by=.(A)]
DT

# Add 1 to column B
DT[c(2,4) , B:=B+1L, ]
DT

# Add a new column Total2
DT[2:4, Total2:=sum(B), by=.(A)]
DT

# Remove the Total column
DT[ , Total := NULL, ]
DT

# Select the third column using `[[`
DT[[3]]

# A data.table DT has been created for you
DT <- data.table(A = c(1, 1, 1, 2, 2), B = 1:5)
str(DT)
DT


# Update B, add C and D
DT[ , c("B", "C", "D") := .(B + 1, A + B, 2), ]
DT

# Delete my_cols
my_cols <- c("B", "C")
DT[ , (my_cols) := NULL, ]
DT

# Delete column 2 by number
DT[[2]] <- NULL
DT

# Set the seed
# set.seed(1)

# Check the DT that is made available to you
# DT

# For loop with set
# for(i in 2:4) { set(DT, sample(nrow(DT), 3), i, NA) }

# Change the column names to lowercase
# setnames(DT, letters[1:4])

# Print the resulting DT to the console
# DT

# Define DT
DT <- data.table(a = letters[c(1, 1, 1, 2, 2)], b = 1)
str(DT)
DT


# Add a suffix "_2" to all column names
setnames(DT, paste0(names(DT), "_2"))
DT

# Change column name "a_2" to "A2"
setnames(DT, "a_2", "A2")
DT

# Reverse the order of the columns
setcolorder(DT, 2:1)
DT

```
  
* Section 8 - Indexing (using column names in i)  
	* DT[A == "a"]  # returns only the rows where column A has value "a"  
	* w <- DT[, A == "a"]  # creates a new variable w that is the boolean evaluation of A == "a"  
		* Note that this is a vector and not a list since it is not wrapped in .()  
	* DT[w] will return the same thing as DT[A == "a"]  
	* The data.table() package automatically creates an index the first time you use the variable  
		* DT[A == "a"]  # takes however long it needs  
		* DT[A == "b"]  # now runs very fast since it is indexed  
  
* Section 9 - creating and using a key  
	* setkey(DT, varname)  
	* DT["b"]  # will find where varname that has been set as key is equal to "b"  
	* DT["b", mult="first"]  # will return only the first match  
	* DT["b", mult="last"]  # will return only the last match  
	* If one of the requested keys is not found, a row with NA is returned  
		* DT[c("b", "d")] could return an NA  
		* DT[c("b", "d"), nomatch = 0] will never return an NA; instead it will just skip the rows  
	* If you create setkey(DT, A, B) then it will be indexed by both A and B  
		* DT[.("b", 5)]  # this will pull rows where A == "b" and B == 5  
  
* Section 10 - Rolling joins (typically used for time series)  
	* DT[.("b", 4), roll=TRUE]  # If there is a "b", 4 then it will find it; if not, then it will find the closest previous match  
	* DT[.("b", 4), roll="nearest"]  # If there is a "b", 4 then it will find it; if not, then it will find the nearest match  
	* DT[.("b", 4), roll=+Inf]  # If there is a "b", 4 then it will find it; if not, then it will find the closest previous match  
	* DT[.("b", 4), roll=-Inf]  # If there is a "b", 4 then it will find it; if not, then it will find the closest succeeding match  
	* DT[.("b", 4), roll=2]  # If there is a "b", 4 then it will find it; if not, then it will find the closest previous match provided it was within 2  
	* DT[.("b", 4), roll=-2]  # If there is a "b", 4 then it will find it; if not, then it will find the closest succeeding match provided it was within 2  
	* DT[.("b", 4), roll=TRUE, rollends=FALSE]  # If there is a "b", 4 then it will find it; if not, then it will find the closest previous match, except it will not go beyond the data  
  
Example code includes:  
```{r}
# iris as a data.table
iris <- as.data.table(iris)

# Remove the "Sepal." prefix
names(iris) <- gsub("Sepal\\.", "", names(iris))

# Remove the two columns starting with "Petal"
iris[, c("Petal.Length", "Petal.Width") := NULL, ]

# Cleaned up iris data.table
str(iris)

# Area is greater than 20 square centimeters
iris[ Width * Length > 20 ]

# Add new boolean column
iris[, is_large := Width * Length > 25]

# Now large observations with is_large
iris[is_large == TRUE]
iris[(is_large)] # Also OK


# The 'keyed' data.table DT
DT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)], 
                 B = c(5, 4, 1, 9, 8, 8, 6), 
                 C = 6:12)
setkey(DT, A, B)
str(DT)
DT

# Select the "b" group
DT["b"]

# "b" and "c" groups
DT[c("b", "c")]

# The first row of the "b" and "c" groups
DT[c("b", "c"), mult = "first"]

# First and last row of the "b" and "c" groups
DT[c("b", "c"), .SD[c(1, .N)], by = .EACHI]

# Copy and extend code for instruction 4: add printout
DT[c("b", "c"), { print(.SD); .SD[c(1, .N)] }, by = .EACHI]


# Keyed data.table DT
DT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)], 
                 B = c(5, 4, 1, 9, 8, 8, 6), 
                 C = 6:12, 
                 key = "A,B")
str(DT)
DT

# Get the key of DT
key(DT)

# Row where A == "b" and B == 6
DT[.("b", 6)]

# Return the prevailing row
DT[.("b", 6), roll=TRUE]

# Return the nearest row
DT[.("b", 6), roll="nearest"]

# Keyed data.table DT
DT <- data.table(A = letters[c(2, 1, 2, 3, 1, 2, 3)], 
                 B = c(5, 4, 1, 9, 8, 8, 6), 
                 C = 6:12, 
                 key = "A,B")
str(DT)
DT


# Print the sequence (-2):10 for the "b" group
DT[.("b", (-2):10)]

# Add code: carry the prevailing values forwards
DT[.("b", (-2):10), roll=TRUE]

# Add code: carry the first observation backwards
DT[.("b", (-2):10), roll=TRUE, rollends=TRUE]

```
  
###_Data Manipulation (xts and zoo)_  
Jeff Ryan, the creator of quantmod and organizer of the R/Finance conference, has developed xts and zoo to simplify working with time series data.  The course will cover five areas (chapters):  
  
* Chapter 1: Create, import, and export time series  
* Chapter 2: Subset, extract, and more  
* Chapter 3: Merge and modify time series  
* Chapter 4: Apply and aggregate by time  
* Chapter 5: Advanced and extra features of xts  
  
"xts" stands for extensible time series.  The core of each "xts" is a "zoo" object, consisting of a matrix plus an index.  
  
* Basically, it is data plus an array of times  
	* x <- matrix(data=1:4, ncol=2)  
	* idx <- as.Date(c("2015-01-01", "2015-02-01"))  
	    * The idx needs to be "time based", though the type of time object can be flexible - Date, POSIX times, timeData, chron, . . . 	
	    * The index should be in increasing order of time (earlier at the type)  

* The xts functions allow for joining the index and the data  
	* X <- xts(x, order.by = idx)  # Can add arguments unique=TRUE (force times to be unique) and tzone="<what you want>" to override the system time-zone  
	* If the "idx" that you passed is not sorted ascending (earliest times first), the xts call will sort both the "x" and the "idx" such that the resulting xts object is ascending in time  

There are a few special behaviors of xts:  
  
* The xts object is a matrix with associated times for each object  
* Subsets will preserve the matrix form (even if taking just a single row or a single column -- no drop=TRUE default)  
* Attributes are (generally) preserved even when you subset  
* The "xts" object is a subset of "zoo", meaning that it preserves all the power of the "zoo" capability  
  
The "xts" object can be de-constructed when needed:  
  
* coredata(x, fmt=FALSE) brings back the matrix  
* index(x) brings back the index  
  
Data usually already exists and needs to be "wrangled" in to a proper format for xts/zoo.  The easiest way to convert is using as.xts().  You can coerce truly external data after loading it, and can also save data with Can also save with write.zoo(x, "file").  

Subsetting based on time is a particular strength of xts.  xts supports ISO8601:2004 (the standard, "right way", to unambiguously consider times):  
  
* Moving left-to-right for the most significant to least significant impact  
* YYYY-MM-DDTHH:MM:SS format  
* Specifying only the year (e.g., 2014) is fine, while specifying only the month (e.g., "02") is not  
  
xts allows for four methods of specifying dates or intervals:  
  
1) One and two-sided intervals ("2004" or "2001/2005")  
2) Truncated representation ("201402/03")  
3) Time support ("2014-02-22 08:30:00")  
4) Repeating intervals ("T08:00/T09:00")  
  
Can also use some traditional R-like methods (since xts extends zoo, and zoo extends base R):  
  
* Integer indexing - x[c(1, 2, 3), ]  
* Logical vectors - x[index(x) > "2016-08-20"]  
* Date objects - x[index(as.POSIXct(c("2016-06-25", "2016-06-27")))]  
  
Can set the flag which.i = TRUE to get back the correct records (row numbers).  For example, index <- x["2007-06-26/2007-06-28", which.i = TRUE].  
  
Description of key behaviors when working with an xts object:  
  
* All subsets will preserve the matrix (drop=FALSE)  
* Order is always preserved - cannot intentionally or uninetntionally reorder the data - requesting c(1, 2) or c(2, 1) returns the same thing  
* Binary search and memcpy are leveraged, meaning that it works faster than base R  
* Index and object attributes are always preserved  
  
xts introduces a few relatives of the head() and tail() functionality.  These are the first() and last() functions.  
  
* first(edhec[, "Funds of Funds"], "4 months")  
* last(edhec[, "Funds of Funds"], "1 year")  
* Can uses a negative to mean "except", such as "-4 months"  
* The first() and last() can be nested within one another  

Math operations using xts - xts is a matrix - need to be careful about matrix operations.  Math operations are run only on the intersection of items:  
  
* Only the intersecting observations will be (for example) added together -- others are dropped! 
* Sometimes it may be necessary to drop the xts class -- drop=TRUE, coredata(), as.numeric(), etc.  
* Special handling (described in the next chapter) may be needed when you want the union of dates  
  
Merging time series is common.  Merge (cbind, merge) combines by columns, but joining based on index.  
  
* Syntax is merge (<time series to merge>, join="outer", fill=NA)  # Defaults are "outer" and NA
join can also be "inner", "left", or "right"  
* fill is available to allow missing values to be coerced as needed  
* If you merge(x, as.Date(c("2016-08-14"))) then you will have a new date (2016-08-14) in your database  
  
Merge (rbind( combine by rows, though all rows must already have an index.  Basically, the rbind MUST be used on a time series.  
  
Missing data is common, and xts inherits all of the zoo methods for dealing with missing data.  The locf is the "last observation carry forward" (latest value that is not NA) - called with na.locf:  
  
* na.locf(object, na.rm=TRUE, fromLast = FALSE, maxgap = Inf)  
* Generic function for replacing eachNAwith the most recent non-NAprior to it.  
  
The NA can be managed in several ways:  
  
* na.fill(object, fill, . )  # fill the NA with the fill value  
* na.trim(object, . )  # remove NA that are at the beginning or end  
* na.omit(object, . )  # remove all NA  
* na.approx(object, . )  # interpolate NA based on distance from object  
  
Lag operators and difference operations.  Seasonality is a repeating pattern.  There is often a need to compare seasonality -- for example, compare Mondays.  Stationarity refers to some bound of the series.
  
The lag() function will change the timestamp, so that (for example) today can be merged as last week:  
  
* lag(x, k=1, na.pad=TRUE, . )  # positive k will shift the values FORWARD  
* Note that base R and zoo are the opposite, where lag(k=<negative>) means move forward  
* This is not what the literature recommends, and zoo follows the literature, with k=<positive> shifting time forward  
  
The "one period lag first difference" is calculated as diff(x, lag=1, differences=1, arithmetic=TRUE, log=FALSE, na.pad=TRUE, . ).
  
There are two main approaches for applying functions on discrete periods or intervals:  
  
* period.apply(x, INDEX, FUN, . )  
    * INDEX should be a vector of end-points of a period  
    * The end-point will be the last observation per interval  
        * endpoints(x, on="years") will create an endpoint vector by year  ## can be "days" or "seconds" or the like; always starts with 0  
    * data(PerformanceAnalytics::edhec); edhec_4yr <- edhec["1997/2001"]; ep <- endpoints(edhec_4yr, "years"); period.apply(edhec_4yr, INDEX=ep, FUN=mean)  
    * There are shortcut functions like apply.yearly() which take care of all the indexing and endpoints automatically  
* split(x, f="months")  
    * This will split the data by month  
    * Outcome would be a list by months  
  
Time series aggregation can also be handled by xts:  
  
* Useful to convert a univariate series to range bars (OHLC = Open, High, Low, Close)  
    * Provides a summary of a particular period - start, max, min, end  
    * to.period(x, period="months", k=1, indexAt, name=NULL, OHLC=TRUE, . )  
        * indexAt lets you adjust labelling of outputs (default is end of period), while name lets you define the roots used in the columns  
    * to.period(edhec["1997/2001", 1], "years", name="EDHEC")  
    * to.period(edhec["1997/2001", 1], "years", name="EDHEC", indexAt="firstof")  
    * to.period(edhec["1997/2001", 1], "years", name="EDHEC", OHLC=FALSE)  # will pull the last observation only  
  
Time series data can also be managed in a "rolling" manner - discrete or continuous:  
  
* Discrete rolling windows would be things like "month to date"  
    * split() followed by lapply() using FUN=cumsum, cumprod, cummin, cummax  
    * edhec.yrs <- split(edhec[, 1], f="years")  
    * edhec.yrs <- lapply(edhec.yrs, FUN=cumsum)  
    * edhec.ytd <- do.call(rbind, edhec.yrs)  
* Continuous rolling windows are managed through:  
    * rollapply(data, width, FUN, . , by=1, by.column = TRUE, fill= if (na.pad) NA, na.pad=TRUE, partial=TRUE, align=c("right", "center", "left"))  
  
Internals of xts such as indices and timezones:  
  
* The index is always stored as fractional seconds since midnight 1970-01-01 UTC  
* xts will use tclass (attribute for extraction) - if you passed in a date, you get back a date -- indexClass()  
* xts will use tzone (attribute for time zone) -- indexTZ()  
* xts will use indexFormat (attribute for optional display preferences) -- indexFormat() <- <valid sprintf command>  
* Sys.setenv(TZ = "America/Chicago")  
    * help(OlsonNames)  
  
Final topics:  
  
* Periodicity - identify underlying regularity in the data (what type of data do we have)  
    * May be irregular data, so this is just an estimate -- periodicity()  
* Counting -- number of discrete periods (unique endpoints) -- note that monthly data has the same answer for ndays() and nmonths()  
    * Only makes sense to count periods if the data have HIGHER frequency than what you are trying to count  
* Broken down time can be extracted with .index  
    * index(Z); .indexmday(Z) # month day; .indexyday(Z) # year day; .indexyear(Z) + 1900  
* Can align timing -- align.time(x, n=60) # n is in seconds  
    * make.index.unique(x, eps=1e-06, drop=FALSE, fromLast=FALSE, . ) will help to manage duplicates  
  
Example code includes:  
```{r}
library(xts)
library(zoo)

x <- matrix(data=1:4, ncol=2)
idx <- as.Date(c("2015-01-01", "2015-02-01"))

# Create the xts
X <- xts(x, order.by = idx)

# Decosntruct the xts
coredata(X, fmt=FALSE)
index(X)

# Working with the sunspots data
data(sunspots)
class(sunspots)
sunspots_xts <- as.xts(sunspots)
class(sunspots_xts)
head(sunspots_xts)

# Example from chapter #1
ex_matrix <- xts(matrix(data=c(1, 1, 1, 2, 2, 2), ncol=2), 
                 order.by=as.Date(c("2016-06-01", "2016-06-02", "2016-06-03"))
                 )
core <- coredata(ex_matrix)

# View the structure of ex_matrix
str(ex_matrix)

# Extract the 3rd observation of the 2nd column of ex_matrix
ex_matrix[3, 2]

# Extract the 3rd observation of the 2nd column of core 
core[3, 2]

# Create the object data using 5 random numbers
data <- rnorm(5)

# Create dates as a Date class object starting from 2016-01-01
dates <- seq(as.Date("2016-01-01"), length = 5, by = "days")

# Use xts() to create smith
smith <- xts(x = data, order.by = dates)

# Create bday (1899-05-08) using a POSIXct date class object
bday <- as.POSIXct("1899-05-08")

# Create hayek and add a new attribute called born
hayek <- xts(x = data, order.by = dates, born = bday)

# Extract the core data of hayek
hayek_core <- coredata(hayek)

# View the class of hayek_core
class(hayek_core)

# Extract the index of hayek
hayek_index <- index(hayek)

# View the class of hayek_index
class(hayek_index)

# Create dates
dates <- as.Date("2016-01-01") + 0:4

# Create ts_a
ts_a <- xts(x = 1:5, order.by = dates)

# Create ts_b
ts_b <- xts(x = 1:5, order.by = as.POSIXct(dates))

# Extract the rows of ts_a using the index of ts_b
ts_a[index(ts_b)]

# Extract the rows of ts_b using the index of ts_a
ts_b[index(ts_a)]


data(austres)

# Convert austres to an xts object called au
au <- as.xts(austres)

# Convert your xts object (au) into a matrix am
am <- as.matrix(au)

# Convert the original austres into a matrix am2
am2 <- as.matrix(austres)

# Create dat by reading tmp_file
tmp_file <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1127/datasets/tmp_file.csv"
dat <- read.csv(tmp_file)  

# Convert dat into xts
xts(dat, order.by = as.Date(rownames(dat), "%m/%d/%Y"))

# Read tmp_file using read.zoo
dat_zoo <- read.zoo(tmp_file, index.column = 0, sep = ",", format = "%m/%d/%Y")

# Convert dat_zoo to xts
dat_xts <- as.xts(dat_zoo)

# Convert sunspots to xts using as.xts(). Save this as sunspots_xts
sunspots_xts <- as.xts(sunspots)

# Get the temporary file name
tmp <- tempfile()

# Write the xts object using zoo to tmp 
write.zoo(sunspots_xts, sep = ",", file = tmp)

# Read the tmp file. FUN = as.yearmon converts strings such as Jan 1749 into a proper time class
sun <- read.zoo(tmp, sep = ",", FUN = as.yearmon)

# Convert sun into xts. Save this as sun_xts
sun_xts <- as.xts(sun)



data(edhec, package="PerformanceAnalytics")

head(edhec["2007-01", 1])
head(edhec["2007-01/2007-03", 1])
head(edhec["200701/03", 1])

first(edhec[, "Funds of Funds"], "4 months")
last(edhec[, "Funds of Funds"], "1 year")



```
  
###_Data Visualization (ggplot2 part 1)_  
Data visulaization is the combination of Statistics and Design:  
  
* Aids in communication and perception  
* Exploratory plots are designed for a very small audience (just yourself even) and can be very dense  
* Explanatory plots are designed to synthesize and communicate broadly - very labor intensive to create  
  
The Anscombe plot examples show four different datasets explained by the identical linear model.  This reinforces the importance of plotting the data prior to running analyses and drawing conclusions.  
  
The "Grammar of Graphics" is a plotting framework based on the book by Leland Wilkinson, "Grammar of Graphics" 2(1999).  The gist is that graphics are made of distinct layers of grammatical elements.  Meaningful plots are created through aesthetic mapping.  
  
Essential Grammatical Elements include:  
  
* MANDATORY (this course) - Data (dataset for plotting), Aesthetics (scales for mapping the data), Geometries (visual elements)  
* OPTIONAL (next course) - Facets (plotting small multiples), Statistics, Coordinates (space for plotting), Themes (non-data ink)  
  
The ggplot2 package was one of the first developed and designed by Hadley Wickham.  It implements the "Grammar of Graphics" in R, for example with:  
  
* Data:  iris  
* Aesthetic:  x=Sepal.Length, y=Sepal.Width  
* Geometries:  geom_jitter(alpha = 0.6)  
* Facets:  facet_grid(. ~ Species)  
* Statistics:  stat_smooth(method = "lm", se = F, col="red")  
* Coordinates/Themes:  to be explored in a later course  
  
The Anscombe data is good to have plotted for reference:  
```{r}
library(ggplot2)
data(anscombe)

ansX <- with(anscombe, c(x1, x2, x3, x4))
ansY <- with(anscombe, c(y1, y2, y3, y4))
ansType <- rep(1:4, each=nrow(anscombe))
ansFrame <- data.frame(x=ansX, y=ansY, series=factor(ansType))

# ggplot example for Anscombe data
ggplot(ansFrame, aes(x=x, y=y)) + 
    geom_point() + 
    geom_smooth(method="lm", col="red", se=FALSE, fullrange=TRUE) + 
    facet_wrap(~ series, nrow=2)

```
  
As well, the basic example code from above is useful to explore:  
```{r}
data(iris)

ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + 
    geom_jitter(alpha = 0.6) + 
    facet_grid(. ~ Species) + 
    stat_smooth(method = "lm", se = FALSE, col="red")
```
  
Some additional basic ggplot syntax includes (cached, since plotting each point of the diamonds dataset is taxing for the graphics):  
```{r, cache=TRUE}
# Explore the mtcars data frame with str()
data(mtcars)
str(mtcars)

# Execute the following command
ggplot(mtcars, aes(x = cyl, y = mpg)) +
  geom_point()

# Change the command below so that cyl is treated as factor
ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +
  geom_point()

# A scatter plot has been made for you
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point()

# Replace ___ with the correct vector
ggplot(mtcars, aes(x = wt, y = mpg, col = disp)) +
  geom_point()

# Replace ___ with the correct vector
ggplot(mtcars, aes(x = wt, y = mpg, size = disp)) +
  geom_point()


# Explore the diamonds data frame with str()
data(diamonds)
str(diamonds)

# Add geom_point() with +
ggplot(diamonds, aes(x = carat, y = price)) + geom_point()

# Add geom_point() and geom_smooth() with +
ggplot(diamonds, aes(x = carat, y = price)) + geom_point() + geom_smooth()

# The plot you created in the previous exercise
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point() +
  geom_smooth()

# Copy the above command but show only the smooth line
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_smooth()

# Copy the above command and assign the correct value to col in aes()
ggplot(diamonds, aes(x = carat, y = price, col=clarity)) +
  geom_smooth()

# Keep the color settings from previous command. Plot only the points with argument alpha.
ggplot(diamonds, aes(x = carat, y = price, col=clarity)) +
  geom_point(alpha = 0.4)

# Create the object containing the data and aes layers: dia_plot
dia_plot <- ggplot(diamonds, aes(x = carat, y=price))

# Add a geom layer with + and geom_point()
dia_plot + geom_point()

# Add the same geom layer, but with aes() inside
dia_plot + geom_point(aes(col = clarity))


set.seed(1)
# The dia_plot object has been created for you
dia_plot <- ggplot(diamonds, aes(x = carat, y = price))

# Expand dia_plot by adding geom_point() with alpha set to 0.2
dia_plot <- dia_plot + geom_point(alpha = 0.2)

# Plot dia_plot with additional geom_smooth() with se set to FALSE
dia_plot + geom_smooth(se = FALSE)

# Copy the command from above and add aes() with the correct mapping to geom_smooth()
dia_plot + geom_smooth(se = FALSE, aes(col = clarity))

```
  
Data Layer - How data structure influences plots (ggplot2 vs. base):  
  
* Scatterplot in base:  plot(iris$Sepal.Length, iris$Sepal.width); points(iris$Petal.Length, iris$Petal.Width, col="red")  
* Many limitations to the base approach, including  
	* Plot does not get redrawn, meaning some of the Petal Length/Width end up "off plot"  
	* Plot is drawn solely as an image; it is not an object that we can retain/manipulate  
	* Need to manually add/adjust the legend, a potential source for errors  
	* No unified framework for plotting (different functions for each plot type)  
Differences in ggplot	
	* All plots are always called with ggplot(<data>, aes(<aes>)) + geom_<mygeom>(), plus additional layers as needed  
	* The ggplot can be stored as an object using the assignment operator ( <- )  
* Note that the base package is generally fine for a very simple plot  
  
Can add some additional points (similar to points() in base, but with axes rescaling for you):  
  
* ggplot() + geom_point() + geom_point(aes(<newaes>), col="red")  
	* Plotting space is adjusted  
	* This is due to ggplot2 having created an object (which can thus be manipulated)  
* While the above works, it is really a misuse of the grammar of graphics - "please never do this!"  
	* For example, there is no legend, and the axis labels are incorrect  
* In reality, the X axis should be "length" and the Y axis should be width  
	* Suppose that you make iris.wide, a 300x4 consisting of Species, Part (Petal, Sepal), Length, Width) instead of the original 150x5  
	* So now, color can be based on Part, with X = Length and Y = Width  
* When the data is made tidy, there are many advantages in the plotting  
	* Tidy data for iris is format Species - Part (Sepal or Petal) - Measure (Length or Width) - Value  
  
Example code includes:  
```{r}
data(mtcars)
str(mtcars)

# Plot the correct variables of mtcars
plot(mtcars$wt, mtcars$mpg, col=mtcars$cyl)

# Change cyl inside mtcars to a factor
mtcars$cyl <- as.factor(mtcars$cyl)

# Make the same plot as in the first instruction
plot(mtcars$wt, mtcars$mpg, col=mtcars$cyl)

# Use lm() to calculate a linear model and save it as carModel
carModel <- lm(mpg ~ wt, data = mtcars)

# Call abline() with carModel as first argument and set lty to 2
abline(carModel, lty=2)

# Plot each subset efficiently with lapply
# You don't have to edit this code
lapply(mtcars$cyl, function(x) {
  abline(lm(mpg ~ wt, mtcars, subset = (cyl == x)), col = x)
  })

# This code will draw the legend of the plot
# You don't have to edit this code
legend(x = 5, y = 33, legend = levels(mtcars$cyl), 
       col = 1:3, pch = 1, bty = "n")


# Plot 1: add geom_point() to this command to create a scatter plot
ggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +
  geom_point()  # Fill in using instructions Plot 1

# Plot 2: include the lines of the linear models, per cyl
ggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +
  geom_point() + # Copy from Plot 1
  geom_smooth(method="lm", se=FALSE)   # Fill in using instructions Plot 2

# Plot 3: include a lm for the entire dataset in its whole
ggplot(mtcars, aes(x = wt, y = mpg, col = cyl)) +
  geom_point() + # Copy from Plot 2
  geom_smooth(method="lm", se=FALSE) + # Copy from Plot 2
  geom_smooth(aes(group = 1), method="lm", se=FALSE, linetype = 2)   # Fill in using instructions Plot 3


data(iris)
str(iris)


# Option 1
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point() +
  geom_point(aes(x = Petal.Length, y = Petal.Width), col = "red")

# DS code to match up to lecturer data set formats
data(iris)
longIris <- tidyr::gather(iris, Type, Measure, -Species)
intIris <- tidyr::separate(longIris, Type, c("Part", "Metric"))
intIris$rowNum <- c(1:150, 1:150, 151:300, 151:300)
iris.wide <- tidyr::spread(intIris, Metric, Measure)
iris.tidy <- dplyr::select(dplyr::mutate(intIris, Value=Measure, Measure=Metric), Species, Part, Measure, Value)

# Option 2
ggplot(iris.wide, aes(x = Length, y = Width, col = Part)) +
  geom_point()


# Consider the structure of iris, iris.wide and iris.tidy (in that order)
str(iris)
str(iris.wide)
str(iris.tidy)

# Think about which dataset you would use to get the plot shown right
# Fill in the ___ to produce the plot given to the right
ggplot(iris.tidy, aes(x = Species, y = Value, col = Part)) +
  geom_jitter() +
  facet_grid(. ~ Measure)

# Load the tidyr package
library(tidyr)

# Fill in the ___ to produce to the correct iris.tidy dataset
iris.tidy <- iris %>%
  gather(key, Value, -Species) %>%
  separate(key, c("Part", "Measure"), "\\.") 

# Consider the head of iris, iris.wide and iris.tidy (in that order)
head(iris)
head(iris.wide)
head(iris.tidy)

# Think about which dataset you would use to get the plot shown right
# Fill in the ___ to produce the plot given to the right
ggplot(iris.wide, aes(x = Length, y = Width, col = Part)) +
  geom_jitter() +
  facet_grid(. ~ Species)

# Add column with unique ids (don't need to change)
iris$Flower <- 1:nrow(iris)

# Fill in the ___ to produce to the correct iris.wide dataset
iris.wide <- iris %>%
  gather(key, value, -Species, -Flower) %>%
  separate(key, c("Part", "Measure"), "\\.") %>%
  spread(Measure, value)

```
  
Visible aesthetics are the cornerstone of the ggplot:  
  
* Specific to ggplot, "how things look" is an attribute rather than an aesthetic  
* For ggplot, the aesthetic is just the mapping of the variables to the plots  
* For example, on a scatterplot, the mapping of variables to the x-axis and the y-axis is an aesthetic (as is the mapping of a variable to color)  
	* It is always a data frame COLUMN that is mapped on to an aesthetic  
* Typical aesthetics include (n.b., many of these can function as EITHER aesthetic mappings OR attributes)  
	* x - x-axis position  
	* y - y-axis position  
	* colour - colour of dots, outlines of other shapes  
	* fill - fill colour  
	* size - diameter of points, thickness of lines  
	* alpha - transparency (0 = transparent, 1=opaque)  
	* linetype - line dash pattern  
	* labels - direct labels of an item (text on a plot or axes)  
	* shape - shape  
* Common mistake is to use attribute rather than aesthetic and/or to overwrite the aesthetic with the attribute  
  
Modifying aesthetics:  
  
* position "identity"" is the default for a scatterplot - data will be plotted directly as per the aesthetic mapping  
	* geom_point(position = "identity") is technically fine also, though not needed as it is the default  
	* geom_point(position = "jitter") is the same as geom_jitter()  
	* Can also define the jitter, such as posn.j <- position_jitter(width = 0.1), followed later by geom_point(position = posn.j)  
* scale functions  
	* scale_x  
	* scale_y  
	* scale_color  
	* scale_fill  
	* scale_shape  
	* scale_linetype  
* limit functions (max and min for example)  
* breaks  
* expand (always a c(multiplicative, additive)), for creating gaps between the data and the axes  
* labels (to explicitly define the labels) . . . Which can also just use the labs() function  
  
Best practices for choosing among the aesthetics (though note that "there is a fair bit of creativity involved"):  
  
* Jacques Bertin ("Seminology of Graphics", 1967) and William Cleveland ("Perception of Visual Elements", 1990s)  
* Form should always follow function (what is the point of the graph)  
	* Beautiful plots is not the primary objective  
	* Never misrepresent or obscure (do not create plots just to confuse the issue)  
	* Always some information loss when translating between raw data and visual representations  
* Colors can be a confusing way to differentiate continuous variables (better for discrete variables)  
* Different axes can create misinterpretations also  
* Using shape without color can be very confusing also  
  
Example code includes:  
```{r}
data(mtcars)
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$am <- as.factor(mtcars$am)

# Map cyl to y
ggplot(mtcars, aes(x=mpg, y=cyl)) + geom_point()
  
# Map cyl to x
ggplot(mtcars, aes(y=mpg, x=cyl)) + geom_point()

# Map cyl to col
ggplot(mtcars, aes(y=mpg, x=wt, col=cyl)) + geom_point()

# Change shape and size of the points in the above plot
ggplot(mtcars, aes(y=mpg, x=wt, col=cyl)) + geom_point(shape=1, size=4)

# Map cyl to fill
ggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +
  geom_point()

# Change shape, size and alpha of the points in the above plot
ggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) +
  geom_point(shape=16, size=6, alpha=0.6)

# Map cyl to size
ggplot(mtcars, aes(y=mpg, x=wt, size=cyl)) + geom_point()

# Map cyl to alpha
ggplot(mtcars, aes(y=mpg, x=wt, alpha=cyl)) + geom_point()

# Map cyl to shape 
ggplot(mtcars, aes(y=mpg, x=wt, shape=cyl)) + geom_point()

# Map cyl to labels
ggplot(mtcars, aes(y=mpg, x=wt, label=cyl)) + geom_text()

# Define a hexadecimal color
my_color <- "#123456"

# Set the color aesthetic 
ggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_point()

# Set the color aesthetic and attribute 
ggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_point(col = my_color)

# Set the fill aesthetic and color, size and shape attributes
ggplot(mtcars, aes(x=wt, y=mpg, fill=cyl)) + geom_point(size=10, shape=23, col=my_color)

# Expand to draw points with alpha 0.5
ggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) + geom_point(alpha=0.5)

# Expand to draw points with shape 24 and color yellow
ggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) + geom_point(shape=24, col="yellow")

# Expand to draw text with label x, color red and size 10
ggplot(mtcars, aes(x = wt, y = mpg, fill = cyl)) + geom_text(label="x", col="red", size=10)

# Map mpg onto x, qsec onto y and factor(cyl) onto col
ggplot(mtcars, aes(x=mpg, y=qsec, col=factor(cyl))) + geom_point()

# Add mapping: factor(am) onto shape
ggplot(mtcars, aes(x=mpg, y=qsec, col=factor(cyl), shape=factor(am))) + geom_point()

# Add mapping: (hp/wt) onto size
ggplot(mtcars, aes(x=mpg, y=qsec, col=factor(cyl), shape=factor(am), size=(hp/wt))) + geom_point()

# Basic scatter plot: wt on x-axis and mpg on y-axis; map cyl to col
ggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_point(size=4)

# Hollow circles - an improvement
ggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_point(size=4, shape=1)

# Add transparency - very nice
ggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + geom_point(size=4, alpha=0.6)

```
  
Next, bar plots are examined using the same data:  
```{r}
cyl.am <- ggplot(mtcars, aes(x = factor(cyl), fill = factor(am)))

# Add geom (position = "stack" by default)
cyl.am + geom_bar()

# Fill - show proportion
cyl.am + 
  geom_bar(position = "fill")  

# Dodging - principles of similarity and proximity
cyl.am +
  geom_bar(position = "dodge") 

# Clean up the axes with scale_ functions
val = c("#E41A1C", "#377EB8")
lab = c("Manual", "Automatic")
cyl.am +
  geom_bar(position = "dodge") +
  scale_x_discrete("Cylinders") + 
  scale_y_continuous("Number") +
  scale_fill_manual("Transmission", 
                    values = val,
                    labels = lab) 

# Add a new column called group
mtcars$group <- 0

# Create jittered plot of mtcars: mpg onto x, group onto y
ggplot(mtcars, aes(x = mpg, y=group)) + geom_jitter()

# Change the y aesthetic limits
ggplot(mtcars, aes(x = mpg, y=group)) + geom_jitter() + scale_y_continuous(limits = c(-2, 2))

```

Further, the diamonds data set is explored to show techniques for minimizing over-plotting problems.  Per previous, it is cached due to the lengthy plot times driven by the many data points:  
```{r, cache=TRUE}
data(diamonds)
str(diamonds)

# Scatter plot: carat (x), price (y), clarity (col)
ggplot(diamonds, aes(x=carat, y=price, col=clarity)) + geom_point()

# Adjust for overplotting
ggplot(diamonds, aes(x=carat, y=price, col=clarity)) + geom_point(alpha = 0.5)

# Scatter plot: clarity (x), carat (y), price (col)
ggplot(diamonds, aes(y=carat, x=clarity, col=price)) + geom_point(alpha = 0.5)

# Dot plot with jittering
ggplot(diamonds, aes(y=carat, x=clarity, col=price)) + geom_point(alpha = 0.5, position="jitter")

```
  
The geometries layer includes the most common plot types:  
  
* Scatter - point, jitter, abline  
* Bar - histogram, bar, errorbar  
* Line - line  
  
Scatter plots examples - geom_point(), geom_jitter(), geom_abline():  
  
* geom_point() requires that x and y be aesthetics, and has optional aesthetic/attribute for alpha, colour, fill, shape, and size  
* Can specify the aesthetics inside the geom, meaning that the layered geoms may each have a different aesthetic if desired  
* In general, the geom_point() will INHERIT from the ggplot() call, although:  
    * If anything is explicitly called inside geom_point() that will override what is in the ggplot() statement  
	* Interestingly, can even have geom_point(data=other_data), and that will use other_data but still INHERIT whatever aesthetics for x and y were chosen previously  
* The shape argument is equivalent to the pch argument from base  
	* Note that 21-25 have both fill and color which can be controlled independently  
* Note that geom_vline() and geom_hline() will make vertical and horizontal lines  
* Note that ggplot2 can often handle the statistics for you, and that mean without dispersion can tend to be misleading  
* Always optimize shape, size, and alpha blending  
  
Bar plots examples - histogram, bar, errorbar:  
  
* When using geom_histogram, only x is required to be included as an aesthetic  
	* This is actually binned, with a slightly different algorithm than hist() in base
geom_histogram(binwidth=<mybin>)  # is excluded, will use range()/30  
* The labels are the intervals, while the y-axis is the counts  
	* geom_histogram(aes(y = ..density.. ), binwidth=0.1)  # The ..density.. Asks R to look for the internal data frame created by ggplot() as part of the histogram  
* For coloring, we need to use fill= since this is a shape to be filled (col would apply if these were points in a scatter-gram)  
	* The col would only change the outside of the bars  
* The default is for the bars to be stacked based on value (position = "identity")  
	* position = "dodge" means one bar per category  
	* position = "fill" is a stacked bar chart to 100%  
* The geom_bar is the superset that contains geom_histogram() as one of its components  
	* If just geom_bar() is called, the result will look like geom_histogram(), though you can have a purely categorical variable as the aes(x=) component  
	* The default argument for geom_bar() is stat="bin"  
	* If instead stat="identity" is included, then the actual y value inside aes() will be plotted  
* The geom_errorbar() requires its own aes() call inside it -- covering ymin, ymax, and width  
* The standard publication "dynamite plot" is strongly discouraged -- rationale and alternatives to be explained in a future module  
  
Line plots examples - line:  
  
* The simplest line graph is set up just like a scatterplot, but with calling geom_line() rather than geom_point()  
* Can add col= as part of the aesthetic, which will color the line by that variable  
* The geom_area() call combined with a fill= in the aes() will provide a stacked line chart  
	* If position = "fill" is called inside geom_area(), then it will be a stacked bar, showing proportional trends over time  
* The geom_ribbon() call can put them all on the same scale, provided that ymin=0 is part of aes() inside geom_ribbon(), followed by alpha= so that they are not opaque overlaps  
  
Example code from mtcars includes:  
```{r}
# mtcars point plots
# Plot the cyl on the x-axis and wt on the y-axis
ggplot(mtcars, aes(x=cyl, y=wt)) + geom_point()

# Use geom_jitter() instead of geom_point()
ggplot(mtcars, aes(x=cyl, y=wt)) + geom_jitter()

# Define the position object using position_jitter(): posn.j
posn.j <- position_jitter(width = 0.1)
  
# Use posn.j in geom_point()
ggplot(mtcars, aes(x=cyl, y=wt)) + geom_point(position = posn.j)


# mtcars bar plots
# Make a univariate histogram
ggplot(mtcars, aes(x=mpg)) + geom_histogram()

# Change the bin width to 1
ggplot(mtcars, aes(x=mpg)) + geom_histogram(binwidth = 1)

# Change the y aesthetic to density
ggplot(mtcars, aes(x=mpg)) + geom_histogram(binwidth = 1, aes(y=..density..))

# Custom color code
myBlue <- "#377EB8"

# Change the fill color to myBlue
ggplot(mtcars, aes(x=mpg)) + geom_histogram(binwidth = 1, aes(y=..density..), fill=myBlue)

# Draw a bar plot of cyl, filled according to am
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar()

# Change the position argument to stack
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar(position="stack")

# Change the position argument to fill
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar(position="fill")

# Change the position argument to dodge
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar(position="dodge")

# Draw a bar plot of cyl, filled according to am
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar()

# Change the position argument to "dodge"
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar(position = "dodge")

# Define posn_d with position_dodge()
posn_d <- position_dodge(width=0.2)

# Change the position argument to posn_d
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar(position = posn_d)

# Use posn_d as position and adjust alpha to 0.6
ggplot(mtcars, aes(x=cyl, fill=am)) + geom_bar(position = posn_d, alpha=0.6)

# A basic histogram, add coloring defined by cyl 
ggplot(mtcars, aes(x=mpg, fill=cyl)) +
  geom_histogram(binwidth = 1)

# Change position to identity 
ggplot(mtcars, aes(x=mpg, fill=cyl)) +
  geom_histogram(binwidth = 1, position="identity")

# Change geom to freqpoly (position is identity by default) 
ggplot(mtcars, aes(x=mpg, col=cyl)) +
  geom_freqpoly(binwidth = 1, position="identity")

# Example of how to use a brewed color palette
ggplot(mtcars, aes(x = cyl, fill = am)) +
  geom_bar() +
  scale_fill_brewer(palette = "Set1")

# Basic histogram plot command
ggplot(mtcars, aes(mpg)) + 
  geom_histogram(binwidth = 1)

# Expand the histogram to fill using am
ggplot(mtcars, aes(x=mpg, fill=am)) + 
  geom_histogram(binwidth = 1)

# Change the position argument to "dodge"
ggplot(mtcars, aes(x=mpg, fill=am)) + 
  geom_histogram(binwidth = 1, position="dodge")

# Change the position argument to "fill"
ggplot(mtcars, aes(x=mpg, fill=am)) + 
  geom_histogram(binwidth = 1, position="fill")

# Change the position argument to "identity" and set alpha to 0.4
ggplot(mtcars, aes(x=mpg, fill=am)) + 
  geom_histogram(binwidth = 1, position="identity", alpha = 0.4)

# Change fill to cyl
ggplot(mtcars, aes(x=mpg, fill=cyl)) + 
  geom_histogram(binwidth = 1, position="identity", alpha = 0.4)
```
  
Next, a few examples are run from dataset car::Vocab (cached due to plotting size/time):  
```{r, cache=TRUE}
Vocab <- car::Vocab
str(Vocab)

# Basic scatter plot of vocabulary (y) against education (x). Use geom_point()
ggplot(Vocab, aes(x=education, y=vocabulary)) + geom_point()

# Use geom_jitter() instead of geom_point()
ggplot(Vocab, aes(x=education, y=vocabulary)) + geom_jitter()

# Using the above plotting command, set alpha to a very low 0.2
ggplot(Vocab, aes(x=education, y=vocabulary)) + geom_jitter(alpha = 0.2)

# Using the above plotting command, set the shape to 1
ggplot(Vocab, aes(x=education, y=vocabulary)) + geom_jitter(alpha = 0.2, shape=1)

# Plot education on x and vocabulary on fill
# Use the default brewed color palette
ggplot(Vocab, aes(x = education, fill = vocabulary)) +
  geom_bar(position="fill") +
  scale_fill_brewer()

# Definition of a set of blue colors
blues <- brewer.pal(9, "Blues")

# Make a color range using colorRampPalette() and the set of blues
blue_range <- colorRampPalette(blues)

# Use blue_range to adjust the color of the bars, use scale_fill_manual()
ggplot(Vocab, aes(x = education, fill = factor(vocabulary))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = blue_range(11))

```
  
Lastly, a few additional plots are displayed:  
```{r}
# Print out head of economics
data(economics)
head(economics)

# Plot unemploy as a function of date using a line plot
ggplot(economics, aes(x = date, y = unemploy)) + geom_line()
  
# Adjust plot to represent the fraction of total population that is unemployed
ggplot(economics, aes(x = date, y = unemploy/pop)) + geom_line()

recess <- data.frame(begin=as.Date(c(-31, 1400, 3652, 4199, 7486, 11382), origin="1970-01-01"), end=as.Date(c(304, 1885, 3834, 4687, 7729, 11627), origin="1970-01-01"))

ggplot(economics, aes(x = date, y = unemploy/pop)) +
  geom_line() + 
  geom_rect(data=recess, inherit.aes=FALSE, aes(xmin=begin, xmax=end, ymin=-Inf, ymax=+Inf), fill="red", alpha=0.2)


# Cannot find dataset . . . 
# Check the structure as a starting point
# str(fish.species)

# Use gather to go from fish.species to fish.tidy
# fish.tidy <- gather(fish.species, Species, Capture, -Year)

# Recreate the plot shown on the right
# ggplot(fish.tidy, aes(x = Year, y = Capture, col=Species)) + geom_line()

```
  
The qplot functionality is for making quick and dirty plots:  
  
* qplot(x, y, data=) will make a scatterplot  
* Can add col=, shape=, size= and the like  
* Can add geom="jitter" (or whatever) to be even more explicit  
* Can add alpha=I(0.5) where the I() function means "inhibit", preventing R from treating this as a new data point (somehow . . . )  
  
Basically, the qplot() is nice for just a quick and dirty analysis, though it will have much less flexibility on a go-forward basis.  
  
Example code for qplot includes:  
```{r}
# The old way (shown)
plot(mpg ~ wt, data = mtcars)

# Using ggplot:
ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()

# Using qplot:
qplot(wt, mpg, data=mtcars)

# Categorical:
# cyl
qplot(wt, mpg, data=mtcars, size=cyl)

# gear
qplot(wt, mpg, data=mtcars, size=gear)

# Continuous
# hp
qplot(wt, mpg, data=mtcars, col=hp)

# qsec
qplot(wt, mpg, data=mtcars, col=qsec)

# qplot() with x only
qplot(factor(cyl), data=mtcars)

# qplot() with x and y
qplot(factor(cyl), factor(vs), data=mtcars)

# qplot() with geom set to jitter manually
qplot(factor(cyl), factor(vs), data=mtcars, geom="jitter")

# Make a dot plot with ggplot
ggplot(mtcars, aes(x=cyl, y=wt, fill = factor(am))) + 
    geom_dotplot(stackdir="center", binaxis="y")

# qplot with geom "dotplot", binaxis = "y" and stackdir = "center"
qplot(cyl, wt, fill=factor(am), data=mtcars, geom="dotplot", binaxis="y", stackdir="center")

```
  
Course #1 wrap-up comments:  
  
* Start with "who is the intended audience" so thet form follows function  
* Good plots include Grammatical Elements and Aesthetic Mappings  
	* This course focused on three Grammatical Elements (Data, Aestehtics, and Geometries)  
	* The Aesthetic Mappings can be thought of as scales for encoding elements  
* Proper data formats and transformations make the appropriate information available for plotting  
	* Recall the tidying for iris, iris.wide, and iris.tidy  
	* The proper data depends on the plot that we want to create  
* The aesthetic choice depends on both the variable type (continuous vs. discrete) as well as the ease for the audience to decode the meaning  
* Common plot types include scatter, line, and bar  
* Recall the difference in the aesthetics and the attributes  
* Jitter or alpha can be good techniques for overcoming over-plotting (can use position= as an additional option)  
  
A few warp-up coding exercises for ggplot #1 include:  
```{r}
# Check out the head of ChickWeight
data(ChickWeight)
head(ChickWeight)

# Use ggplot() for the second instruction
ggplot(ChickWeight, aes(x=Time, y=weight)) + geom_line(aes(group=Chick))

# Use ggplot() for the third instruction
ggplot(ChickWeight, aes(x=Time, y=weight, col=Diet)) + geom_line(aes(group=Chick))

# Use ggplot() for the last instruction
ggplot(ChickWeight, aes(x=Time, y=weight, col=Diet)) + geom_line(aes(group=Chick), alpha=0.3) + geom_smooth(lwd=2, se=FALSE)


# Check out the structure of titanic
library(titanic)
library(dplyr)

titanicFull <- titanic::titanic_train
str(titanicFull)

titanic <- titanicFull %>% 
    select(Pclass, Sex, Survived, Age) %>% 
    filter(complete.cases(.))
str(titanic)

# Use ggplot() for the first instruction
ggplot(titanic, aes(x=factor(Pclass), fill=factor(Sex))) + 
    geom_bar(position = "dodge")

# Use ggplot() for the second instruction
ggplot(titanic, aes(x=factor(Pclass), fill=factor(Sex))) + 
    geom_bar(position = "dodge") + 
    facet_grid(. ~ Survived)

# Position jitter (use below)
posn.j <- position_jitter(0.5, 0)

# Use ggplot() for the last instruction
ggplot(titanic, aes(x=factor(Pclass), y=Age, col=factor(Sex))) + 
    geom_jitter(size=3, alpha=0.5, position=posn.j) + 
    facet_grid(. ~ Survived)

```

###_Data Visualization (ggplot2 part 2)_  
The second course expands on the remaining layers of ggplot2:  Statistics, Coordinates, Facets, and Themes.  
  
The statistics layer for ggplot2 has two basic components:  
  
* Called from within a geom  
* Called from outside a geom (independently)  
* The stat functions all start with stat_ meaning that stat_bin() is the binning call for histograms  
	* stat_bin() and geom_bar() are closely related  
	* stat_smooth() and geom_smooth() are closely related -- se is by defautl the 95% CI  
		* The loess function is used for smallish (<1000) bins, with span= available as an option to change the window size  
		* Can instead specifically call method="lm" and the line will be based on OLS, with 95% CI defaulted in addition  
		* Can also add the fullrange= call to request that the predictions be made for out-of-range data  
* The relationships between the stat_ and geom_ help to 1) explain warnings/errors, and 2) provide help pages for best debugging/tuning parameters  
  
The statistics can also be called independently (outside the geom):  
  
* Note that Hmisc::smean.sdl(x, mult=1) will return mean, mean-1SD, mean+1SD as a vector  
    * The default is mult=2, so Hmisc::smean() defaults to providing mean +/- 2SD  
* Within ggplot, this is called as mean_sdl(), with the first argument the vector, and the second argument the mult=  
* Can be called inside stat_summary(fun.data = mean_sdl, fun.args=list(mult=1))  
* Alternately, can be called as stat_summary(fun.y = mean, geom="point") + stat_summary(fun.data = mean_sdl, fun.args=list(mult=1), geom="errorbar", width=0.1)  # will make a point mean with errorbars  
* Note that Hmisc::smean.cl.normal(x) will return mean, then 95% CI  
    * Within ggplot, this is called as mean_cl_normal(x)  
* Can use anything inside stat_summary provided that the output is aligned with what the specified geom will require  
* The most common function calls are stat_summary(), stat_function(), and stat_qq()  
* Now back to the MASS::mammals dataset for brain vs body  
    * Can add stat_function(fun = dnorm, color="red", arg=list(mean = mean(mam.new$body), sd = sd(mam.new$body)) and the normal density will plot on top of the graph  
    * The geom_rug() can be helpful for putting the rug underneath the histogram  
  
Example code from mtcars includes:  
```{r}
# Explore the mtcars data frame with str()
data(mtcars)
str(mtcars)

# A scatter plot with LOESS smooth:
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth()

# A scatter plot with an ordinary Least Squares linear model:
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() + 
  geom_smooth(method = "lm")

# The previous plot, without CI ribbon:
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() + 
  geom_smooth(method = "lm", se=FALSE)

# The previous plot, without points:
ggplot(mtcars, aes(x = wt, y = mpg)) + 
    geom_smooth(method = "lm", se=FALSE)

# Define cyl as a factor variable
ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)

# Complete the following ggplot command as instructed
ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) + 
  stat_smooth(method = "lm", se = FALSE, aes(group=1))

# Plot 1: change the LOESS span
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  # Add span below 
  geom_smooth(se = FALSE, span=0.7)

# Plot 2: Set the overall model to LOESS and use a span of 0.7
ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  # Change method and add span below
  stat_smooth(method = "loess", aes(group = 1), 
              se = FALSE, col = "black", span=0.7)

# Plot 3: Set col to "All", inside the aes layer of stat_smooth()
ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  stat_smooth(method = "loess",
              # Add col inside aes()
              aes(group = 1, col="All"), 
              # Remove the col argument below
              se = FALSE, span = 0.7)

# Plot 4: Add scale_color_manual to change the colors
myColors <- c(brewer.pal(3, "Dark2"), "black")
ggplot(mtcars, aes(x = wt, y = mpg, col = factor(cyl))) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, span = 0.75) +
  stat_smooth(method = "loess", 
              aes(group = 1, col="All"), 
              se = F, span = 0.7) +
  # Add correct arguments to scale_color_manual
  scale_color_manual("Cylinders", values=myColors)



# Display structure of mtcars
str(mtcars)

# Convert cyl and am to factors:
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$am <- as.factor(mtcars$am)

# Define positions:
posn.d <- position_dodge(width = 0.1)
posn.jd <- position_jitterdodge(jitter.width = 0.1, dodge.width = 0.2)
posn.j <- position_jitter(width = 0.2)

# base layers:
wt.cyl.am <- ggplot(mtcars, aes(x=cyl, y=wt, col=am, fill=am, group=am))

# Plot 1: Jittered, dodged scatter plot with transparent points
wt.cyl.am +
  geom_point(position = posn.jd, alpha = 0.6)
  
# Plot 2: Mean and SD - the easy way
wt.cyl.am +
  geom_point(position = posn.jd, alpha = 0.6) + 
  stat_summary(fun.data=mean_sdl, fun.args=list(mult=1), position=posn.d)
  
# Plot 3: Mean and 95% CI - the easy way
wt.cyl.am +
  geom_point(position = posn.jd, alpha = 0.6) + 
  stat_summary(fun.data=mean_cl_normal, position=posn.d)
  
# Plot 4: Mean and SD - with T-tipped error bars - fill in ___
wt.cyl.am +
  stat_summary(geom = "point", fun.y = mean, 
               position = posn.d) +
  stat_summary(geom = "errorbar", fun.data = mean_sdl, 
               position = posn.d, fun.args = list(mult = 1), width = 0.1)

xx <- 1:100

# Function to save range for use in ggplot 
gg_range <- function(x) {
  # Change x below to return the instructed values
  data.frame(ymin = min(x), # Min
             ymax = max(x)
             ) # Max
}

gg_range(xx)
# Required output:
#   ymin ymax
# 1    1  100

# Function to Custom function:
med_IQR <- function(x) {
  # Change x below to return the instructed values
  data.frame(y = median(x), # Median
             ymin = quantile(x, 0.25), # 1st quartile
             ymax = quantile(x, 0.75)
             )  # 3rd quartile
}

med_IQR(xx)
# Required output:
#        y  ymin  ymax
# 25% 50.5 25.75 75.25

wt.cyl.am <- ggplot(mtcars, aes(x = cyl,y = wt, col = am, fill = am, group = am))

# Add three stat_summary calls to wt.cyl.am
wt.cyl.am + 
  stat_summary(geom = "linerange", fun.data = med_IQR, 
               position = posn.d, size = 3) +
  stat_summary(geom = "linerange", fun.data = gg_range, 
               position = posn.d, size = 3, 
               alpha = 0.4) +
  stat_summary(geom = "point", fun.y = median, 
               position = posn.d, size = 3, 
               col = "black", shape = "X")

```
  
Further examples (cached) from car::Vocab include:  
```{r, cache=TRUE}

Vocab <- car::Vocab

# Plot 1: Jittered scatter plot, add a linear model (lm) smooth:
ggplot(Vocab, aes(x = education, y = vocabulary)) +
  geom_jitter(alpha = 0.2) +
  stat_smooth(method="lm", se=FALSE)

# Plot 2: Only lm, colored by year
ggplot(Vocab, aes(x = education, y = vocabulary, col=factor(year))) +
  stat_smooth(method="lm", se=FALSE)

# Plot 3: Set a color brewer palette
ggplot(Vocab, aes(x = education, y = vocabulary, col=factor(year))) +
  stat_smooth(method="lm", se=FALSE) + 
  scale_color_brewer()

# Plot 4: Add the group, specify alpha and size
ggplot(Vocab, aes(x = education, y = vocabulary, col = year, group=factor(year))) +
  stat_smooth(method = "lm", se = FALSE, alpha=0.6, size=2) +
  scale_color_gradientn(colors = brewer.pal(9,"YlOrRd"))

# Use stat_quantile instead of stat_smooth:
ggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) +
  stat_quantile(alpha = 0.6, size = 2) +
  scale_color_gradientn(colors = brewer.pal(9,"YlOrRd"))

# Set quantile to 0.5:
ggplot(Vocab, aes(x = education, y = vocabulary, col = year, group = factor(year))) +
  stat_quantile(alpha = 0.6, size = 2, quantiles=c(0.5)) +
  scale_color_gradientn(colors = brewer.pal(9,"YlOrRd"))

# Plot with linear and loess model
p <- ggplot(Vocab, aes(x = education, y = vocabulary)) +
       stat_smooth(method = "loess", aes(col = "red"), se = F) +
       stat_smooth(method = "lm", aes(col = "blue"), se = F) +
       scale_color_discrete("Model", labels = c("red" = "LOESS", "blue" = "lm"))

# Add stat_sum
p + stat_sum()

# Add stat_sum and set size range
p + stat_sum() + scale_size(range = c(1, 10))

```
  
The coordinates layers control the plot dimensions:  
  
* Typically using coord_, such as coord_cartesian()  
Examples for zooming in on a plot  
	* scale_x_continuous(limits = )  
	* xlim()  
	* coord_cartesian(xlim = )  # has the benefit that there are no more warning messages; the whole plot is created, and then this specific zoom is selected  
* Aspect ratios should often, but not always, be on a 1:1 basis  
	* coord_fixed(xNum)  # the height will now be on a scale of xNum:1 vs. the width  
  
The facets are based on the concept of small multiples as per the Tufte book on "Visulaization of Quantitative Information" (1983):  
  
* The concept is to split one large plot in to several smaller plots that each contain the same coordinate system  
* facet_grid(rows ~ columns), so facet_grid(. ~ Species) will mean make a single row, and use Species as the columns  
	* The facet is just the splitting up of the data based on a factor variable  
  
Example code includes:  
```{r}
data(mtcars); 
mtcars$cyl <- as.factor(mtcars$cyl); 
mtcars$am <- as.factor(mtcars$am)


# Basic ggplot() command, coded for you
p <- ggplot(mtcars, aes(x = wt, y = hp, col = am)) + geom_point() + geom_smooth()

# Add scale_x_continuous
p + scale_x_continuous(limits = c(3,6), expand=c(0,0))

# The proper way to zoom in:
p + coord_cartesian(xlim=c(3, 6))


data(iris)

# Complete basic scatter plot function
base.plot <- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species)) +
               geom_jitter() +
               geom_smooth(method = "lm", se = FALSE)

# Plot base.plot: default aspect ratio
base.plot

# Fix aspect ratio (1:1) of base.plot
base.plot + coord_equal()


# Create stacked bar plot: thin.bar
thin.bar <- ggplot(mtcars, aes(x=1, fill=cyl)) +
              geom_bar()

# Convert thin.bar to pie chart
thin.bar + coord_polar(theta = "y")


# Create stacked bar plot: wide.bar
wide.bar <- ggplot(mtcars, aes(x=1, fill=cyl)) +
              geom_bar(width=1)

# Convert wide.bar to pie chart
wide.bar + coord_polar(theta="y")


# Basic scatter plot:
p <- ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point()

# Separate rows according to transmission type, am
p + facet_grid(am ~ .)

# Separate columns according to cylinders, cyl
p + facet_grid(. ~ cyl)

# Separate by both columns and rows 
p + facet_grid(am ~ cyl)


# Code to create the cyl_am col and myCol vector
mtcars$cyl_am <- paste(mtcars$cyl, mtcars$am, sep = "_")
myCol <- rbind(brewer.pal(9, "Blues")[c(3,6,8)],
               brewer.pal(9, "Reds")[c(3,6,8)])

# Basic scatter plot, add color scale:
ggplot(mtcars, aes(x = wt, y = mpg, col=cyl_am)) +
  geom_point() + scale_color_manual(values = myCol)

# Facet according on rows and columns.
ggplot(mtcars, aes(x = wt, y = mpg, col=cyl_am)) +
  geom_point() + scale_color_manual(values = myCol) + 
  facet_grid(gear ~ vs)

# Add more variables
ggplot(mtcars, aes(x = wt, y = mpg, col=cyl_am, size=disp)) +
  geom_point() + scale_color_manual(values = myCol) + 
  facet_grid(gear ~ vs)



mamsleep <- tidyr::gather(ggplot2::msleep %>% 
                              mutate(total = sleep_total, rem=sleep_rem) %>%
                              select(vore, name, total, rem) %>% 
                              filter(!is.na(total), !is.na(rem)), 
                          sleep, time, -c(vore, name))

mamsleep$sleep <- factor(mamsleep$sleep, levels=c("total", "rem"))
str(mamsleep)


# Basic scatter plot
ggplot(mamsleep, aes(x=time, y=name, col=sleep)) + geom_point()

# Facet rows accoding to vore
ggplot(mamsleep, aes(x=time, y=name, col=sleep)) + geom_point() + facet_grid(vore ~ .)

# Specify scale and space arguments to free up rows
ggplot(mamsleep, aes(x=time, y=name, col=sleep)) + geom_point() + 
    facet_grid(vore ~ ., scale="free_y", space="free_y")

```
  
The themes layer controls all the "non-data ink" on your plot:  
  
* This means the visual elements that are not part of the data; typically text, line, or rectangle  
	* These are called as element_text(), element_line(), or element_rectangle()  
	* The headers for the individual facets are referred to as "strip text"  
* Text is all the titles, axis labels, legend labels, and the like  
* Lines are the axis ticks, axis lines, panel grids, and the like  
* Rectangles include legend background/key, panel background/border, plot/strip background  
* Many of the attributes inherit from others  
	* Everything in text inherits from text  
	* Everything in line inherits from line  
	* Everything in rect inherits from rect  
	* Further, things like axis.text.x will inherit from axis  
* The element_blank() is for removing everything  
	* theme(text = element_blank(), line=element_blank(), rect=element_blank())  # will just plot the points  
  
Often an objective to repeat the theme multiple times in the same presentation:  
  
* Typical approach would be to save the theme layer as an object, then use it as needed  
* There are also some built-in themes available for use  
	* theme_classic()  
	* library(ggthemes); theme_tufte()  
	* original <- theme_update()  # everything in theme_update is modified, with the original values stored in original  
* Can also declare a theme_set() at the top of the code, and everything that follows will use that theme  
  
Example code includes:  
```{r}
# Rough re-engineer of what is pre-defined as z
data(mtcars)
mtcars$cyl <- factor(mtcars$cyl)

myColors <- c(brewer.pal(9, "Blues"))[c(5, 7, 9)]

origZ <- ggplot(mtcars, aes(x=wt, y=mpg, col=cyl)) + 
         geom_point(size=2) + 
         geom_smooth(method="lm", se=FALSE) + 
         facet_wrap(~ cyl, nrow=1) + 
         scale_x_continuous("Weight (lb/1000)") + 
         scale_y_continuous("Miles / (US) gallon") +
         scale_color_manual("Cylinders", values=myColors)
z <- origZ


# Plot 1: change the plot background color to myPink:
myPink <- "#FEE0D2"
z + theme(plot.background = element_rect(fill = myPink))

# Plot 2: adjust the border to be a black line of size 3
z + theme(plot.background = element_rect(fill = myPink, color="black", size=3))

# Plot 3: set panel.background, legend.key, legend.background and strip.background to element_blank()
uniform_panels <- theme(panel.background = element_blank(), 
                        legend.key = element_blank(), 
                        legend.background=element_blank(), 
                        strip.background = element_blank())
z <- z + theme(plot.background = element_rect(fill = myPink, color="black", size=3)) + uniform_panels
z


# Extend z with theme() function and three arguments
z <- z + theme(panel.grid=element_blank(), axis.line=element_line(color="black"),
               axis.ticks=element_line(color="black")
               )
z

# Extend z with theme() function and four arguments
myRed <- "#99000D"
z <- z + theme(strip.text = element_text(size=16, color=myRed), 
               axis.title.y=element_text(color=myRed, hjust=0, face="italic"),
               axis.title.x=element_text(color=myRed, hjust=0, face="italic"), 
               axis.text=element_text(color="black")
               )
z

# Move legend by position
z + theme(legend.position = c(0.85, 0.85))

# Change direction
z + theme(legend.direction = "horizontal")

# Change location by name
z + theme(legend.position = "bottom")

# Remove legend entirely
z + theme(legend.position = "none")


# Increase spacing between facets
library(grid)
z + theme(panel.margin.x = unit(2, "cm"))

# Add code to remove any excess plot margin space
z + theme(panel.margin.x = unit(2, "cm"), plot.margin = unit(c(0,0,0,0), "cm"))



# Make z2 the same as origZ
z2 <- origZ

# Theme layer saved as an object, theme_pink
theme_pink <- theme(panel.background = element_blank(),
                    legend.key = element_blank(),
                    legend.background = element_blank(),
                    strip.background = element_blank(),
                    plot.background = element_rect(fill = myPink, color = "black", size = 3),
                    panel.grid = element_blank(),
                    axis.line = element_line(color = "black"),
                    axis.ticks = element_line(color = "black"),
                    strip.text = element_text(size = 16, color = myRed),
                    axis.title.y = element_text(color = myRed, hjust = 0, face = "italic"),
                    axis.title.x = element_text(color = myRed, hjust = 0, face = "italic"),
                    axis.text = element_text(color = "black"),
                    legend.position = "none")
  
# Apply theme_pink to z2
z2 + theme_pink

# Change code so that old theme is saved as old
old <- theme_update(panel.background = element_blank(),
             legend.key = element_blank(),
             legend.background = element_blank(),
             strip.background = element_blank(),
             plot.background = element_rect(fill = myPink, color = "black", size = 3),
             panel.grid = element_blank(),
             axis.line = element_line(color = "black"),
             axis.ticks = element_line(color = "black"),
             strip.text = element_text(size = 16, color = myRed),
             axis.title.y = element_text(color = myRed, hjust = 0, face = "italic"),
             axis.title.x = element_text(color = myRed, hjust = 0, face = "italic"),
             axis.text = element_text(color = "black"),
             legend.position = "none")

# Display the plot z2
z2

# Restore the old plot
theme_set(old)

# Load ggthemes package
library(ggthemes)

# Apply theme_tufte
z2 + theme_tufte()

# Apply theme_tufte, modified:
z2 + theme_tufte() + 
    theme(legend.position = c(0.9, 0.9), legend.title=element_text(face="italic", size=12), axis.title=element_text(face="bold", size=14))

```
  
Best practices for plotting and visulaization:  
  
* The "dynamite plot" (bar plot with errorbars) is not recommended  
	* Suggests that the data is normally distributed (even though that may not be the case)  
	* Does not provide any information about the n per category  
	* Bars also suggest that there is "data at 0", which with many activities is patently untrue  
	* Also no bars above the mean (where there is certainly data)  
* One alternative is to just plot the data points using a jitter/alpha so they can all be seen  
	* The error bars with the jittered points are a much cleaner visulaization of the data  
  
Recall that pie charts are just bar charts wrapped on to polar coordinates:  
  
* The pie chart is often prone to misues since people interpret it differently - angle vs. area vs. arc-length vs. etc.  
* Best practice is to reserve the pie chart for where there is a large, macro difference and only looking at ~3 groups  
* The better practice is the stacked bar that sums to 100%; no need to rotate it on to polar coordinates  
  
Suggestion that heat maps are "one of the least effective forms of communication":  
  
* Perceptions of colors can change based on the context  
* Tendency for the heat map to convey a "wow factor" even though there is precious little information communicated  
* Much better to facet and dot-plot the data on a continuous axis  
    * It is very good for exploratory analysis; may be too dense for publication  
* Final choices will depend on the data density and the composition of the audience  
  
Example code includes:  
```{r}
data(mtcars)
mtcars$cyl <- factor(mtcars$cyl)
mtcars$am <- factor(mtcars$am)
m <- ggplot(mtcars, aes(x = cyl, y = wt))


# Draw dynamite plot
m +
  stat_summary(fun.y = mean, geom = "bar", fill = "skyblue") +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", width = 0.1)


# Base layers
m <- ggplot(mtcars, aes(x = cyl, y = wt, col = am, fill = am))

# Plot 1: Draw dynamite plot
m +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", width = 0.1)

# Plot 2: Set position dodge in each stat function
m +
  stat_summary(fun.y = mean, geom = "bar", position = "dodge") +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), 
               geom = "errorbar", width = 0.1, position = "dodge")

# Set your dodge posn manually
posn.d <- position_dodge(0.9)

# Plot 3:  Redraw dynamite plot
m +
  stat_summary(fun.y = mean, geom = "bar", position = posn.d) +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", 
               width = 0.1, position = posn.d
               )


cyl_means <- tapply(mtcars$wt, mtcars$cyl, FUN=mean)
cyl_sd <- tapply(mtcars$wt, mtcars$cyl, FUN=sd)
cyl_ct <- tapply(mtcars$wt, mtcars$cyl, FUN=length)
mtcars.cyl <- data.frame(cyl=as.factor(names(cyl_means)), 
                         wt.avg=cyl_means, sd=cyl_sd, prop=cyl_ct/sum(cyl_ct)
                         )


# Base layers
m <- ggplot(mtcars.cyl, aes(x = cyl, y = wt.avg))

# Plot 1: Draw bar plot
m + geom_bar(stat = "identity", fill="skyblue")

# Plot 2: Add width aesthetic
m + geom_bar(stat = "identity", fill="skyblue", aes(width=prop))

# Plot 3: Add error bars
m + geom_bar(stat = "identity", fill="skyblue", aes(width=prop)) +
    geom_errorbar(aes(ymin = wt.avg - sd, ymax = wt.avg + sd), width=0.1)

ggplot(mtcars, aes(x = factor(1), fill = am)) +
  geom_bar(position = "fill", width=1) + 
  facet_grid(. ~ cyl) + 
  coord_polar(theta="y")



library(GGally)

# All columns except am
group_by_am <- match("am", names(mtcars))
my_names_am <- (1:11)[-group_by_am]

# Basic parallel plot - each variable plotted as a z-score transformation
ggparcoord(mtcars, my_names_am, groupColumn = group_by_am, alpha = 0.8)


barley <- lattice::barley
str(barley)

# Create color palette
myColors <- brewer.pal(9, "Reds")

# Build the heat map from scratch
ggplot(barley, aes(x=year, y=variety, fill=yield)) + 
    geom_tile() + 
    facet_wrap(~ site, ncol=1) + 
    scale_fill_gradientn(colors = myColors)

# Line plots
ggplot(barley, aes(x=year, y=yield, col=variety, group=variety)) + 
    geom_line() + 
    facet_wrap(~ site, nrow=1)

ggplot(barley, aes(x=year, y=yield, col=site, group=site, fill=site)) + 
    stat_summary(fun.y = mean, geom="line") + 
    stat_summary(fun.data = mean_sdl, fun.args=list(mult=1), geom="ribbon", col=NA, alpha=0.1)

```
  
CHIS (California Health Information Study) - Descriptive Statistics Case Study:  
  
* Data available at healthpolicy.ucla.edu  
* Data set "adult" from the 2009 version, starts with 47614 x 536  
	* Reduced data set "adult" from the 2009 version will have 44346 x 10  
		* RBMI, BMI_P, RACEHPR2, SRSEX, SRAGE_P, MARIT2, AB1, ASTCUR, AB51, POVLL  
		* Filtered to ethnicities White, Black, Asian, Latino  
  
The mosaic plot is a sequence of rectangles, each sized based on the size of the group:  
  
* X-axis is then the total number of individuals  
* Mosaic plots are a good means of looking at chi-squared data  
* Table can report on both the size and the underlying statistics  
	* See for example the mosaic() command available in library vcd  
	* Could then color each rectangle based on whether it has been over or under represented  
  
Case study code - creating a Merimeko plot:  
```{r}
# TBD - need the raw data first!
```
  
###_Data Visualization (ggplot2 part 3)_  
Advanced course that builds on concepts from the previous modules:  
  
* Chapter 1 - graphs for the statistically savvy (box plots, density plots, violin plots, etc.)  
* Chapter 2 - graphs for very specific data types (large data, networks, etc.)  
* Chapter 3 - graphing with maps (cartography) and animations  
* Chapter 4 - internals of the ggplot2 package (grid, manipulations for efficiency)  
* Chapter 5 - case studies (building geoms from scratch; and a Tufte-plot)  
  
Refresher code from previous modules includes:  
```{r}
# Create movies_small
library(ggplot2movies)
set.seed(123)
movies_small <- movies[sample(nrow(movies), 1000), ]
movies_small$rating <- factor(round(movies_small$rating))

# Create movies_small
library(ggplot2movies)
set.seed(123)
movies_small <- movies[sample(nrow(movies), 1000), ]
movies_small$rating <- factor(round(movies_small$rating))

# Explore movies_small with str()
str(movies_small)

# Build a scatter plot with mean and 95% CI
ggplot(movies_small, aes(x = rating, y = votes)) +
  geom_point() +
  stat_summary(fun.data = "mean_cl_normal",
               geom = "crossbar",
               width = 0.2,
               col = "red") +
  scale_y_log10()

# Reproduce the plot
ggplot(diamonds, aes(x=carat, y=price, col=color)) + 
  geom_point(alpha=0.5, size=0.5, shape=16) + 
  scale_x_log10(limits=c(0.1, 10)) + 
  xlab(expression(log[10](Carat))) + 
  scale_y_log10(limits=c(100, 100000)) +  
  ylab(expression(log[10](Price))) + 
  scale_color_brewer(palette="YlOrRd") + 
  coord_equal() + 
  theme_classic()

# Add smooth layer and facet the plot
ggplot(diamonds, aes(x = carat, y = price, col = color)) +
  geom_point(alpha = 0.5, size = .5, shape = 16) +
  scale_x_log10(expression(log[10](Carat)), limits = c(0.1,10)) +
  scale_y_log10(expression(log[10](Price)), limits = c(100,100000)) +
  scale_color_brewer(palette = "YlOrRd") +
  coord_equal() +
  theme_classic() + 
  stat_smooth(method="lm") + 
  facet_grid(. ~ cut)
```

  
Box plots creation and usage (more for an academic audience).  John Tukey - "Exploratory Data Analysis" (median, IQR, max, min, extremes outside of 1.5 * IQR).  Whiskers are only drawn up to the "fence" (1.5 * IQR); everything above this is the outlier.
  
Example code includes:  
```{r}
# Add a boxplot geom
d <- ggplot(movies_small, aes(x = rating, y = votes)) +
  geom_point() +
  geom_boxplot() +
  stat_summary(fun.data = "mean_cl_normal",
               geom = "crossbar",
               width = 0.2,
               col = "red")

# Untransformed plot
d

# Transform the scale
d + scale_y_log10()

# Transform the coordinates (produces error in RStudio - commented out)
# d + coord_trans(y = "log10")

# Plot object p
p <- ggplot(diamonds, aes(x = carat, y = price))

# Use cut_interval
p + geom_boxplot(aes(group = cut_interval(carat, n=10)))

# Use cut_number
p + geom_boxplot(aes(group = cut_number(carat, n=10)))

# Use cut_width
p + geom_boxplot(aes(group = cut_width(carat, width=0.25)))

```
  
Density plots are a good way to describe univariate data.  There are many statistics (e.g., PDF or Probability Density Function), including the "Kernel Density Estimate" (KDE) - "sum of bumps placed at the observations; kernel function determines the shape of the bumps, while window width, h, determines their width":  
  
* x <- c(0.0, 1.0, 1.1, 1.5, 1.9, 2.8, 2.9, 3.5)  
* ggplot(data.frame(x=x), aes(x=x)) + geom_rug(col="dark green") + lims(x=c(-2, 6))  
* The normal KDE would build a normal around each point, then aggregate all the normals as the distribution  
* The mode of a continuous distribution is defined as the peak of the KDE  
* Generally, the defaults can be used for the bandwidth, h, though it can be adjusted if wanted  
* The plots can extend beyond the limits of the data, which is mathematically correct but may be off-putting  
    * The geom_density() function will cut off at the observed maximum and minimum (the area under the curve will no longer be 1, though)  
  
Example code includes:
```{r}
# Set up keys
t_norm <- c(-0.560475646552213,-0.23017748948328,1.55870831414912,0.070508391424576,0.129287735160946,1.71506498688328,0.460916205989202,-1.26506123460653,-0.686852851893526,-0.445661970099958,1.22408179743946,0.359813827057364,0.400771450594052,0.11068271594512,-0.555841134754075,1.78691313680308,0.497850478229239,-1.96661715662964,0.701355901563686,-0.472791407727934,-1.06782370598685,-0.217974914658295,-1.02600444830724,-0.72889122929114,-0.625039267849257,-1.68669331074241,0.837787044494525,0.153373117836515,-1.13813693701195,1.25381492106993,0.426464221476814,-0.295071482992271,0.895125661045022,0.878133487533042,0.821581081637487,0.688640254100091,0.553917653537589,-0.0619117105767217,-0.305962663739917,-0.380471001012383,-0.694706978920513,-0.207917278019599,-1.26539635156826,2.16895596533851,1.20796199830499,-1.12310858320335,-0.402884835299076,-0.466655353623219,0.779965118336318,-0.0833690664718293,0.253318513994755,-0.028546755348703,-0.0428704572913161,1.36860228401446,-0.225770985659268,1.51647060442954,-1.54875280423022,0.584613749636069,0.123854243844614,0.215941568743973,0.379639482759882,-0.502323453109302,-0.33320738366942,-1.01857538310709,-1.07179122647558,0.303528641404258,0.448209778629426,0.0530042267305041,0.922267467879738,2.05008468562714,-0.491031166056535,-2.30916887564081,1.00573852446226,-0.709200762582393,-0.688008616467358,1.0255713696967,-0.284773007051009,-1.22071771225454,0.18130347974915,-0.138891362439045,0.00576418589988693,0.38528040112633,-0.370660031792409,0.644376548518833,-0.220486561818751,0.331781963915697,1.09683901314935,0.435181490833803,-0.325931585531227,1.14880761845109,0.993503855962119,0.54839695950807,0.238731735111441,-0.627906076039371,1.36065244853001,-0.600259587147127,2.18733299301658,1.53261062618519,-0.235700359100477,-1.02642090030678,-0.710406563699301,0.25688370915653,-0.246691878462374,-0.347542599397733,-0.951618567265016,-0.0450277248089203,-0.784904469457076,-1.66794193658814,-0.380226520287762,0.918996609060766,-0.575346962608392,0.607964322225033,-1.61788270828916,-0.0555619655245394,0.519407203943462,0.301153362166714,0.105676194148943,-0.640706008305376,-0.849704346033582,-1.02412879060491,0.117646597100126,-0.947474614184802,-0.490557443700668,-0.256092192198247,1.84386200523221,-0.651949901695459,0.235386572284857,0.0779608495637108,-0.961856634130129,-0.0713080861235987,1.44455085842335,0.451504053079215,0.0412329219929399,-0.422496832339625,-2.05324722154052,1.13133721341418,-1.46064007092482,0.739947510877334,1.90910356921748,-1.4438931609718,0.701784335374711,-0.262197489402468,-1.57214415914549,-1.51466765378175,-1.60153617357459,-0.530906522170303,-1.4617555849959,0.687916772975828,2.10010894052567,-1.28703047603518,0.787738847475178,0.76904224100091,0.332202578950118,-1.00837660827701,-0.119452606630659,-0.280395335170247,0.56298953322048,-0.372438756103829,0.976973386685621,-0.374580857767014,1.05271146557933,-1.04917700666607,-1.26015524475811,3.2410399349424,-0.416857588160432,0.298227591540715,0.636569674033849,-0.483780625708744,0.516862044313609,0.368964527385086,-0.215380507641693,0.0652930335253153,-0.034067253738464,2.12845189901618,-0.741336096272828,-1.09599626707466,0.0377883991710788,0.310480749443137,0.436523478910183,-0.458365332711106,-1.06332613397119,1.26318517608949,-0.349650387953555,-0.865512862653374,-0.236279568941097,-0.197175894348552,1.10992028971364,0.0847372921971965,0.754053785184521,-0.499292017172261,0.214445309581601,-0.324685911490835,0.0945835281735714,-0.895363357977542,-1.31080153332797,1.99721338474797,0.600708823672418,-1.25127136162494,-0.611165916680421,-1.18548008459731)
t_bimodal <- c(0.19881034888372,-0.68758702356649,-2.26514505669635,-1.45680594076791,-2.41433994791886,-2.47624689461558,-2.78860283785024,-2.59461726745951,-0.34909253266331,-2.05402812508544,-1.88075476357242,-1.75631257040091,-0.767524121514662,-2.51606383094478,-2.99250715039204,-0.32430306759681,-2.44116321690529,-2.72306596993987,-3.23627311888329,-3.2847157223178,-2.57397347929799,-1.38201418283347,-0.890151861070282,-1.29241164616441,-2.36365729709525,-1.9402500626154,-2.70459646368007,-2.71721816157401,-1.11534950102308,-3.01559257860354,-0.0447060345075361,-2.09031959396585,-1.78546117337078,-2.73852770473957,-2.57438868976327,-3.31701613230524,-2.18292538837273,-1.58101759507554,-1.67569565583862,-2.78153648705475,-2.788621970854,-2.50219871834286,-0.503939330153649,-3.13730362066574,-2.1790515943802,-0.0976381783210729,-2.10097488532881,-3.35984070382139,-2.66476943527406,-1.51454002109512,-2.37560287166977,-2.56187636354978,-2.34391723412846,-1.90950335286078,-0.401491228854174,-2.08856511213888,-0.919200503848483,-1.36924588434943,-2.11363989550614,-3.5329020028906,-2.52111731755252,-2.48987045313847,-1.95284556723847,-0.699801322333179,0.293078973831094,-0.452418941016231,-2.13315096432894,-3.75652739555764,-2.38877986407174,-1.91079277692671,-1.15498699593256,-1.03747203151573,-1.31569057058354,-3.39527434979947,-1.15035695436664,-2.44655721642722,-1.82519729983874,-1.92544882282627,-1.57183323502949,-1.97532501717386,-3.66747509758566,-1.26350403522656,-1.61397343165032,-2.26565162527822,-1.88185548895332,-1.86596135463154,-1.778980531439,-0.359153834022514,-2.21905037893348,-1.83193461611534,-0.831616126930907,-0.945818976623081,-0.854736889619643,-2.57746800105956,0.00248273029282942,-1.93329912906982,-0.133148155293138,-3.35090268603071,-1.97901641364576,-0.750085429030784,1.2847578127772,1.24731103178226,1.06146129639311,0.947486720661263,1.5628404668196,2.33117917295898,-0.01421049792072,2.21198043337229,3.23667504641657,4.03757401824044,3.30117599220059,2.75677476379596,0.27326960088567,1.39849329199322,1.64795354341738,2.70352390275689,1.89432866599623,0.741351371939828,3.68443570809411,2.91139129179596,2.23743027249103,3.21810861032581,0.661225712765028,2.6608202977898,1.47708762368658,2.68374552185071,1.93917804533993,2.63296071303145,3.33551761505939,2.0072900903169,3.01755863695209,0.811565964852021,1.27839555956398,3.51921771138818,2.37738797302393,-0.0522228204337298,0.635962547917625,1.79921898441088,2.86577940433449,1.89811674428478,2.62418747202065,2.95900537778783,3.67105482886294,2.05601673327496,1.9480180938191,0.24676264085773,2.09932759408783,1.42814994210444,1.02599041719591,1.82009376895246,3.01494317274366,0.00725151131141755,1.57272071279457,2.11663728358271,1.10679242994505,2.33390294249923,2.41142992061573,1.96696384072401,-0.465898193760027,4.57145814586664,1.7947007425318,2.65119328158767,2.27376649103655,3.02467323481835,2.81765944637409,1.79020682877149,2.37816777220851,1.05459116887611,2.85692301089932,1.53896166111565,4.41677335378821,0.348951104311813,1.53601275703399,2.82537986275924,2.51013254687866,1.410518961485,1.00321925779248,2.1444757047107,1.98569258683309,0.20971876273594,2.03455106713385,2.19023031569246,2.17472639698184,0.944982957397319,2.47613327830263,3.37857013695924,2.45623640317981,0.864411529625657,1.5643545303081,2.34610361955361,1.35295436868173,-0.157646335015277,2.88425082002821,1.17052238837548,1.42643972923172,3.50390060900454,1.22585507039459,2.8457315401893,0.739317121181225,1.64545759692602)
t_uniform <- c(-0.117272665724158,-0.536618106998503,-1.51491178292781,-1.81202527787536,-0.948814783245325,1.87456467188895,-0.0460180705413222,-0.0887118810787797,0.995171524584293,0.670560925267637,-1.80233761295676,0.780420977622271,-0.546953733079135,1.53653442952782,1.10118891857564,-1.44318543467671,-0.819962915033102,-1.49566885828972,0.359606495127082,0.246702435426414,0.754884480498731,-0.754916763864458,0.422347365878522,1.96413727570325,0.972819660790265,-1.69657147862017,-0.195324374362826,-1.78585226181895,-0.641777943819761,0.935808595269918,-1.98357223812491,1.08763792924583,-0.148099155165255,0.883361400105059,0.666022868826985,0.288294882513583,0.815251800231636,0.628884241916239,-0.842591419816017,-1.61104217730463,1.84968528430909,0.945336116477847,0.450893874280155,-1.52028447668999,0.201036185957491,-0.948974888771772,1.5934433247894,-1.96328021679074,-1.05506025627255,-1.47982161864638,-0.695067413151264,0.90559555310756,1.96698094159365,0.86053411103785,0.0177592439576983,-0.255809312686324,1.79530100245029,-1.51927404943854,-1.69941929355264,1.55608597118407,-0.302191619761288,-1.8305572848767,0.5897685745731,-0.125523350201547,0.471704493276775,-0.916738269850612,-1.3708188533783,-1.54298291914165,0.0307314526289701,0.192129150032997,-1.43741524219513,-1.32092379964888,1.0479412926361,0.1095797624439,1.44395743031055,0.69421995151788,-1.94783588126302,0.77279560547322,1.56685492862016,0.52740070130676,-1.57082138024271,1.684260815382,0.701449524611235,-1.40562514960766,0.981367369182408,1.77039544750005,-0.316866497509181,-0.809107687324286,-0.962293319404125,-1.10847473237664,0.262617373839021,1.02660053130239,0.678414183668792,0.186091177165508,1.24585463106632,1.03666720818728,-1.9197261352092,-0.476385199464858,-1.79647954553366,1.19161174912006,1.69479685276747,0.170393481850624,1.40945840068161,0.33425145316869,0.673294574022293,0.0452583860605955,1.05100235715508,1.61344915162772,1.28189804870635,-1.71427260152996,-1.98441462777555,-1.79120553657413,1.46624071896076,0.304980675689876,-0.744629711844027,1.83786313515157,0.364775028079748,0.125637343153358,-0.464253313839436,-0.721787082031369,1.23354502115399,-1.83232200611383,-0.545029221102595,1.4263878678903,0.791786313988268,0.737945891916752,-0.607939789071679,0.218727317638695,-1.45102552976459,1.13972622528672,1.54745028633624,-1.18361646682024,1.08249184582382,0.385451843030751,1.83067896962166,-1.36524640955031,0.103897096589208,1.49260547012091,1.47882428113371,-1.90524542704225,1.90355877391994,-0.0391032071784139,-0.443318707868457,-0.329780160449445,-1.62829671800137,-1.35276315920055,-0.378334019333124,-0.632742223329842,-0.33897018712014,-0.783790123648942,0.241122149862349,-1.37650339771062,1.82631905656308,-1.82413349859416,-0.511369029060006,1.85046136565506,0.581710062921047,-1.75494954269379,-0.360216286033392,-0.296379463747144,0.0326323388144374,-0.201600356958807,0.493045534007251,-1.44008827582002,1.63178560324013,0.277773099020123,0.193122295662761,-1.53268950991333,1.04811332933605,-0.0865222131833434,1.12787565868348,-1.8158938055858,1.27937867119908,-0.922366210259497,-0.868598341941833,0.572886237874627,1.79247535113245,-1.97200618218631,-0.593529311940074,-0.323817100375891,-0.168492811731994,0.846770317293704,1.67939223069698,0.508442802354693,1.60872711334378,1.02931663673371,-1.44856653735042,-1.38698048796505,-1.2347512524575,-0.267259437590837,-1.65112279262394,-1.10487999580801,0.28859471809119,-0.399323286488652,0.261861522682011,1.31849551480263,0.568455277942121,-0.43400499317795,0.838319418951869,-1.56470370758325)
test_data <- data.frame(norm=t_norm, bimodal=t_bimodal, uniform=t_uniform)
small_data <- data.frame(x=c(-3.5, 0, 0.5, 6))

# Calculating density: d
d <- density(test_data$norm)

# Use which.max() to calculate mode
mode <- d$x[which.max(d$y)]

# Finish the ggplot call
ggplot(test_data, aes(x = norm)) +
  geom_rug() +
  geom_density() +
  geom_vline(xintercept = mode, col = "red")

# Arguments you'll need later on
fun_args <- list(mean = mean(test_data$norm), sd = sd(test_data$norm))

# Finish the ggplot
ggplot(test_data, aes(x = norm)) + 
  geom_histogram(aes(y=..density..)) + 
  geom_density(col="red") + 
  stat_function(fun=dnorm, args=fun_args, col="blue")

# Get the bandwith
get_bw <- density(small_data$x)$bw

# Basic plotting object
p <- ggplot(small_data, aes(x = x)) +
  geom_rug() +
  coord_cartesian(ylim = c(0,0.5))

# Create three plots
p + geom_density()
p + geom_density(adjust=0.25)
# p + geom_density(bw = 0.25*get_bw)  ** does not work with my version of R/ggplot2 **

# Create two plots
p + geom_density(kernel="r")
p + geom_density(kernel="e")

```
  
  
  
###_Data Visualization (ggvis)_  
The ggvis is based on a "grammar of graphics" and is closely linked to ggplot2 (both designed by Wickham).  The objective for ggvis is to combine the analytic power of R with the visual power of Javascript.  
  
Broadly, the "grammar of graphics" includes several layers such as Data, Coordinate System, Marks, Properties, and the like.  This is similar to ggplot2 though with a modified syntax that is more in line with dplyr chaining:  
  
myData  %>% 
  ggvis(~myX, ~myY, fill = ~myFill, ...) %>% 
  layer_myMarkChoice()

Note that the := operator is the static assignment operator, ensuring that a call to "red" means the color "red" and not merely a character vector coerced to the required length with every entry being "red".  The ~ symbolizes that this is a variable in my dataset.  So ~red would mean the variable red in the dataset undergoing plotting.  

Some basic example code includes:  
```{r}
library(ggvis)
data(mtcars)
str(mtcars)


# Change the code below to make a graph with red points
mtcars %>% ggvis(~wt, ~mpg, fill := "red") %>% layer_points()

# Change the code below draw smooths instead of points
mtcars %>% ggvis(~wt, ~mpg) %>% layer_smooths()

# Change the code below to make a graph containing both points and a smoothed summary line
mtcars %>% ggvis(~wt, ~mpg) %>% layer_points() %>% layer_smooths()


data(pressure)
str(pressure)

# Adapt the code: show bars instead of points
pressure %>% ggvis(~temperature, ~pressure) %>% layer_bars()

# Adapt the codee: show lines instead of points
pressure %>% ggvis(~temperature, ~pressure) %>% layer_lines()

# Extend the code: map the fill property to the temperature variable
pressure %>% ggvis(~temperature, ~pressure, fill=~temperature) %>% layer_points()

# Extend the code: map the size property to the pressure variable
pressure %>% ggvis(~temperature, ~pressure, size=~pressure) %>% layer_points()

```

There are three main new operators in ggvis (relative to ggplot):
  
* %>% (piping from magrittr)  
* ~ (for defining variables as being from the dataset)  
* := (for setting of properties)  
	* Can think of having a data space (e.g., species or size) and a visual space (e.g., species is filled by color or size is represented by shape)  
	* The straight-up equal sign (=) and the tilde tell ggvis that the variables exists in the data space  
	* Using the setting operator (:=) you can make an assignment to something that is not in the data space; for example fill := "red" or size := 100 (purely a visual space assignment)  
  
The line is a special type of mark (second most common after points) - stroke, strokeWidth, strokeOpacity, strokeDash, fill, fillOpacity  
  
Other forms include:  
  
* Paths, sort of like lines, but connecting points in their same order as the dataset  
* Ribbons, which have bounding  
* Smooths, which have loess (or lm or etc.) added to the data  
* Model predictions, which add a general model prediction  
  
Example code includes:  
```{r}
data(faithful)
str(faithful)


faithful %>% ggvis(~waiting, ~eruptions) %>% layer_points()

faithful %>% 
    ggvis(~waiting, ~eruptions, size = ~eruptions) %>% 
    layer_points(opacity := 0.5, fill := "blue", stroke := "black")

faithful %>% 
    ggvis(~waiting, ~eruptions, fillOpacity = ~eruptions) %>% 
    layer_points(size := 100, fill := "red", stroke := "red", shape := "cross")


data(pressure)
str(pressure)


# Modify this graph to map the size property to the pressure variable
pressure %>% ggvis(~temperature, ~pressure, size = ~pressure) %>% layer_points()

# Modify this graph by setting the size property
pressure %>% ggvis(~temperature, ~pressure, size := 100) %>% layer_points()

# Fix this code to set the fill property to red
pressure %>% ggvis(~temperature, ~pressure, fill := "red") %>% layer_points()

pressure %>% 
    ggvis(~temperature, ~pressure) %>% 
    layer_lines(stroke := "red", strokeWidth := 2, strokeDash := 6)

# texas %>% ggvis(~long, ~lat) %>% layer_paths(fill := "darkorange")


data(mtcars)
str(mtcars)


mtcars %>% compute_smooth(mpg ~ wt)

# Extend with ggvis() and layer_lines()
mtcars %>% compute_smooth(mpg ~ wt) %>% ggvis(~pred_, ~resp_) %>% layer_lines()

# Extend with layer_points() and layer_smooths()
mtcars %>% ggvis(~wt, ~mpg) %>% layer_points() %>% layer_smooths()

```
  
Behind the scenes, ggvis uses several compute functions to help with visualizations:  
  
* Using a computation such as a loess (smooth) means that you then map the marks to the smooth data and not the original data  
* The ggvis library only has five key marks - points, paths, ribbons, rects, and text  
* These are extended by a library of compute functions - smooths, model_predictions, bars, histograms, densities, etc.  
* Typically, the compute functions are called automatically by a layer called by the user  
  
The ggvis library is especially well designed to interact with the dplyr library (Hadley Wickham):  
  
* For example, layer_lines() is just arrange() %>% layer_paths(), since the arrange puts it in the proper x-axis order  
* The dplyr::group_by() offers the opportunities to treat different subsets of the data differently  
  
Example code includes:  
```{r}
data(faithful)
str(faithful)


faithful %>% ggvis(~waiting) %>% layer_histograms(width = 5)

# Finish the command
faithful %>%
  compute_bin(~waiting, width = 5) %>%
  ggvis(x = ~xmin_, x2 = ~xmax_, y = 0, y2 = ~count_) %>%
  layer_rects()

# Build the density plot
faithful %>% ggvis(~waiting, fill := "green") %>% layer_densities()


data(mtcars)
str(mtcars)


mtcars %>% 
  ggvis(x = ~factor(cyl)) %>% 
  layer_bars()

# Instruction 1
mtcars %>% 
    group_by(cyl) %>% 
    ggvis(~mpg, ~wt, stroke = ~factor(cyl)) %>% 
    layer_smooths()

# Instruction 2
mtcars %>% 
    group_by(cyl) %>% 
    ggvis(~mpg, fill = ~factor(cyl)) %>% 
    layer_densities()

mtcars %>% 
    group_by(cyl, am) %>% 
    ggvis(~mpg, fill = ~interaction(cyl, am)) %>% 
    layer_densities()

```
  
Can add interactivity to plots in ggvis:  
  
* For example, a slider to change a key parameter such as "span"  
* Can actually add any portion of ggvis to an interactive user control  
* While the interactive ggvis is active, R will remain busy waiting to re-plot on new user inputs  
	* Need to hit escape and/or the red stop sign to return control to R  
* Based on the Shiny framework  
	* Can send to other R users  
	* Can use Shiny server  
  
Multi-layered ggvis plots:  
  
* With ggvis, this is as simple as adding the second layer after the first layer  
* Keep adding layers with the piping operator  
  
Example code includes:  
```{r}
data(faithful)
str(faithful)

data(mtcars)
str(mtcars)

# Adapt the code: set fill with a select box
faithful %>% 
  ggvis(~waiting, ~eruptions, fillOpacity := 0.5, 
        shape := input_select(label = "Choose shape:", 
                              choices = c("circle", "square", "cross", 
                                          "diamond", "triangle-up", "triangle-down"
                                          )
                              ), 
        fill := input_select(label = "Choose color:", 
                             choices = c("black", "red", "blue", "green")
                             )
        ) %>% 
  layer_points()

# Add radio buttons to control the fill of the plot
mtcars %>% 
  ggvis(~mpg, ~wt,
        fill := input_radiobuttons(label = "Choose color:", 
                                   choices = c("black", "red", "blue", "green")
                                   )
        ) %>% 
  layer_points()

mtcars %>% 
  ggvis(~mpg, ~wt, 
        fill := input_text(label = "Choose color:", value = "black")) %>% 
  layer_points()

# Map the fill property to a select box that returns variable names
mtcars %>% 
  ggvis(~mpg, ~wt, fill = input_select(label = "Choose fill variable:", 
                                       choices = names(mtcars), map=as.name
                                       )
        ) %>% 
  layer_points()

# Map the bindwidth to a numeric field ("Choose a binwidth:")
mtcars %>% 
  ggvis(~mpg) %>% 
  layer_histograms(width = input_numeric(label = "Choose a binwidth:", value = 1))

# Map the binwidth to a slider bar ("Choose a binwidth:") with the correct specifications
mtcars %>% 
  ggvis(~mpg) %>% 
  layer_histograms(width = input_slider(label = "Choose a binwidth:", 1, 20))

# Add a layer of points to the graph below.
pressure %>% 
  ggvis(~temperature, ~pressure, stroke := "skyblue") %>% 
  layer_lines() %>%
  layer_points()

# Copy and adapt so that only the lines layer uses a skyblue stroke.
pressure %>% 
  ggvis(~temperature, ~pressure) %>% 
  layer_lines(stroke := "skyblue") %>%
  layer_points()

# Rewrite the code below so that only the points layer uses the shape property.
pressure %>% 
  ggvis(~temperature, ~pressure) %>% 
  layer_lines(stroke := "skyblue") %>% 
  layer_points(shape := "triangle-up")

# Refactor the code for the graph below to make it as concise as possible
pressure %>% 
  ggvis(~temperature, ~pressure, stroke := "skyblue", strokeOpacity := 0.5, strokeWidth := 5) %>% 
  layer_lines() %>% 
  layer_points(fill = ~temperature, 
              shape := "triangle-up", 
              size := 300)

# Add more layers to the line plot
pressure %>%
  ggvis(~temperature, ~pressure) %>%
  layer_lines(opacity := 0.5) %>%
  layer_points() %>%
  layer_model_predictions(model = "lm", stroke := "navy") %>%
  layer_smooths(stroke := "skyblue")

```
  
The add_axis() function can be used to change the titles and axis labels:  
  
* Can also change the ticks and the location (e.g., top/bottom) of where the axes appear on the plot  
* add_axis("x", title = "axis title", values = c(1, 2, 3), subdivide = 5, orient = "top")  
* The first argument specifies which axis to customize  
    * title - the title of the axis you specified in the first argument  
    * values - determine where labelled tick marks will appear on each axis  
    * subdivide - insert unlabelled tick marks between the labelled tick marks on an axis  
    * orient - control where the axis appears. For the x axis, you can use "top" or "bottom", for the y axis, you can use "left" or "right"  
  
The add_legends() function can help with cleaning up legends (make them look tidier).  This is similar to the arguments passed to the "adding an axis" above.
  
Can also customize the scales (relationships between data spaces and visual spaces) for the data:  
  
* Mappings go between the data space (e.g., species) and the visual space (e.g., fill color)  
* The default mapping is handled in ggvis by a function called a scale  
* The scale_nominal() command can change the mapping  
    * scale_nominal("fill", range = c("yellow", "orange", "red"))  
  
Example code includes:  
```{r}
data(faithful)
str(faithful)

# Defaulted axis
faithful %>% 
  ggvis(~waiting, ~eruptions) %>% 
  layer_points()

# Customized axis
faithful %>% 
  ggvis(~waiting, ~eruptions) %>% 
  layer_points() %>%
  add_axis("x", title="Time since previous eruption (m)", 
           values=c(50, 60, 70, 80, 90), subdivide=9, orient="top"
           ) %>%
  add_axis("y", title="Duration of eruption (m)", values=c(2, 3, 4, 5), 
           subdivide=9, orient="right"
           )


data(pressure)
str(pressure)

# Add a legend
faithful %>% 
  ggvis(~waiting, ~eruptions, opacity := 0.6, 
        fill = ~factor(round(eruptions))) %>% 
  layer_points() %>%
  add_legend("fill", title="~ duration (m)", orient="left")

# Original code with jumbled legends
faithful %>% 
  ggvis(~waiting, ~eruptions, opacity := 0.6, 
        fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), 
        size = ~round(eruptions))  %>%
  layer_points()

# Fix the legend
faithful %>% 
  ggvis(~waiting, ~eruptions, opacity := 0.6, 
        fill = ~factor(round(eruptions)), shape = ~factor(round(eruptions)), 
        size = ~round(eruptions))  %>%
  layer_points() %>%
  add_legend(c("fill", "shape", "size"), title="~ duration (m)")


data(mtcars)
str(mtcars)

# Add a scale_numeric()
mtcars %>% 
  ggvis(~wt, ~mpg, fill = ~disp, stroke = ~disp, strokeWidth := 2) %>%
  layer_points() %>%
  scale_numeric("fill", range = c("red", "yellow")) %>%
  scale_numeric("stroke", range = c("darkred", "orange"))

# Add a scale_numeric()
mtcars %>% ggvis(~wt, ~mpg, fill = ~hp) %>%
  layer_points() %>%
  scale_numeric("fill", range=c("green", "beige"))

# Add a scale_nominal()
mtcars %>% ggvis(~wt, ~mpg, fill = ~factor(cyl)) %>%
  layer_points() %>%
  scale_nominal("fill", range=c("purple", "blue", "green"))


# Original plot becomes too transparent
mtcars %>% ggvis(x = ~wt, y = ~mpg, fill = ~factor(cyl), opacity = ~hp) %>%
  layer_points()

# Range to prevent overly transparent data points
mtcars %>% ggvis(x = ~wt, y = ~mpg, fill = ~factor(cyl), opacity = ~hp) %>%
  layer_points() %>%
  scale_numeric("opacity", range=c(0.2, 1))

mtcars %>% ggvis(~wt, ~mpg, fill = ~disp) %>%
  layer_points() %>%
  scale_numeric("y", domain = c(0, NA)) %>%  # NA means top-of-data-range
  scale_numeric("x", domain = c(0, 6))

mtcars$color <- c('red' , 'teal' , '#cccccc' , 'tan' , 'red' , 'teal' , '#cccccc' , 'tan' , 
                  'red' , 'teal' , '#cccccc' , 'tan' , 'red' , 'teal' , '#cccccc' , 'tan' , 
                  'red' , 'teal' , '#cccccc' , 'tan' , 'red' , 'teal' , '#cccccc' , 'tan' , 
                  'red' , 'teal' , '#cccccc' , 'tan' , 'red' , 'teal' , '#cccccc' , 'tan'
                  )

# Using fill by mapping the "color" variable to the ggvis scales
mtcars %>% 
  ggvis(x = ~wt, y = ~mpg, fill = ~color) %>% 
  layer_points()

# Using fill based directly on the values in the "color" variable
mtcars %>% 
  ggvis(x = ~wt, y = ~mpg, fill := ~color) %>% 
  layer_points()

```
  
