---
title: "Data Camp Insights"
author: "davegoblue"
date: "July 7, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background and Overview  
DataCamp offers several interactive courses related to R Programming.  While much of it is review, it is always helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  This document summarizes a few key insights from:  
  
* Introduction to R  
* Intermediate R  
* Writing Functions in R  
* Importing Data in to R  
* Cleaning Data in R  
  
## Key Insights and Findings  
###_Introduction to R and Intermediate R_  
There are a few nuggest from within these beginning modules, including:  
  
####_Generic statements_  
* factor(x, ordered=TRUE, levels=c(myLevels)) creates ordinal factors (e.g., a > b > c)  
* subset(a, b) is functionally the same as a[a$b, ] but easier to read  
* & looks at each element while && looks only at the first element (same for | and ||)  
* Inside of a for loop, break kills the loop entirely while next moves back to the top for the next item  
* args(function) shows the arguments (with defaults) for function  
* search() shows the current search path (all auto-load packages and all attached packages)  
* cat("expression") will print the expression or direct it to a file; this is a way to allow \n and \t to take effect in a print statement  
* unique() keeps only the non-duplicated elements of a vector  
* unlist() converts a list back to a vector, somewhat similar to as.vector() on a matrix  
* sort() will sort a vector, but not a data frame  
* rep(a, times=m, each=n) replicates each element of a n times, and then the whole string m times  
* append(x, values, after=length(x)) will insert values in to vector x after point after  
* rev() reverses a vector  
* Inside a grep, "\\1" captures what is inside the ()  
    
####_Apply usages_  
* lapply() operates on a vector/list and always returns a list  
* sapply() is lapply but converted to a vector/array when possible (same as lapply if not possible); if USE.NAMES=FALSE then the vector will be unnamed, though the default is USE.NAMES=TRUE for a named vector  
* vapply(X, FUN, FUN.VALUE, ... , USE.NAMES=TRUE) is safer than sapply in that you specify what type of vector each iteration should produce; e.g., FUN.VALUE=character(1) or FUN.VALUE=numeric(3), with an error if the vector produced by an iteration is not exactly that  
  
####_Dates and times_  
* Sys.Date() grabs the system date as class "Date", with units of days  
* Sys.time() grabs the system time as class "POSIXct", with units of seconds  
* Sys.timezone() shows the system timezone  
* Years are formatted as %Y (4-digit) or %y (2-digit)  
* Months are formatted as %m (2-digit) or %B (full character) or %b (3-character)  
* Days are formatted as %d (2-digit)  
* Weekdays are formatted as %A (full name) or %a (partial name)  
* Times include %H (24-hour hour), %M (minutes), %S (seconds)  
* ?strptime will provide a lot more detail on the formats  
  
Below is some sample code showing examples for the generic statements:  
```{r}
# Factors
xRaw = c("High", "High", "Low", "Low", "Medium", "Very High", "Low")

xFactorNon = factor(xRaw, levels=c("Low", "Medium", "High", "Very High"))
xFactorNon
xFactorNon[xFactorNon == "High"] > xFactorNon[xFactorNon == "Low"][1]

xFactorOrder = factor(xRaw, ordered=TRUE, levels=c("Low", "Medium", "High", "Very High"))
xFactorOrder
xFactorOrder[xFactorOrder == "High"] > xFactorOrder[xFactorOrder == "Low"][1]


# Subsets
data(mtcars)
subset(mtcars, mpg>=25)
identical(subset(mtcars, mpg>=25), mtcars[mtcars$mpg>=25, ])
subset(mtcars, mpg>25, select=c("mpg", "cyl", "disp"))


# & and && (same as | and ||)
compA <- c(2, 3, 4, 1, 2, 3)
compB <- c(1, 2, 3, 4, 5, 6)
(compA > compB) & (compA + compB < 6)
(compA > compB) | (compA + compB < 6)
(compA > compB) && (compA + compB < 6)
(compA > compB) || (compA + compB < 6)


# Loops and cat()
# for (a in b) {
#     do stuff
#     if (exitCond) { break }
#     if (nextCond) { next }
#     do some more stuff
# }
for (myVal in compA*compB) {
    print(paste0("myVal is: ", myVal))
    if ((myVal %% 3) == 0) { cat("Divisible by 3, not happy about that\n\n"); next }
    print("That is not divisible by 3")
    if ((myVal %% 5) == 0) { cat("Exiting due to divisible by 5 but not divisible by 3\n\n"); break }
    cat("Onwards and upwards\n\n")
}


# args() and search()
args(plot.default)
search()


# unique()
compA
unique(compA)


# unlist()
listA <- as.list(compA)
unlist(listA)
identical(compA, unlist(listA))


# sort()
sort(mtcars$mpg)
sort(mtcars$mpg, decreasing=TRUE)

# rep()
rep(1:6, times=2)  # 1:6 followed by 1:6
rep(1:6, each=2)  # 1 1 2 2 3 3 4 4 5 5 6 6
rep(1:6, times=2, each=3)  # 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 6 6 6 repeated twice (each comes first)
rep(1:6, times=6:1)  # 1 1 1 1 1 1 2 2 2 2 2 3 3 3 3 4 4 4 5 5 6


# append()
myWords <- c("The", "cat", "in", "the", "hat")
paste(append(myWords, c("is", "fun", "to", "read")), collapse=" ")
paste(append(myWords, "funny", 4), collapse=" ")

# grep("//1")
sampMsg <- "This is from myname@subdomain.mydomain.com again"
gsub("(^.*\\w*[a-zA-Z0-9]+@)([a-zA-Z0-9]+\\.[a-zA-Z0-9.]+)(.*$)", "\\1", sampMsg)
gsub("(^.*\\w*[a-zA-Z0-9]+@)([a-zA-Z0-9]+\\.[a-zA-Z0-9.]+)(.*$)", "\\2", sampMsg)
gsub("(^.*\\w*[a-zA-Z0-9]+@)([a-zA-Z0-9]+\\.[a-zA-Z0-9.]+)(.*$)", "\\3", sampMsg)

# rev()
compA
rev(compA)

```
  
Below is some sample code showing examples for the apply statements:  
```{r}
# lapply
args(lapply)
lapply(1:5, FUN=sqrt)
lapply(1:5, FUN=function(x, y=2) { c(x=x, y=y, pow=x^y) }, y=3)
lapply(1:5, FUN=function(x, y=2) { if (x <= 3) {c(x=x, y=y, pow=x^y) } else { c(pow=x^y) } }, y=3)

# sapply (defaults to returning a named vector/array if possible; is lapply otherwise)
args(sapply)
args(simplify2array)
sapply(1:5, FUN=sqrt)
sapply(1:5, FUN=function(x, y=2) { c(x=x, y=y, pow=x^y) }, y=3)
sapply(1:5, FUN=function(x, y=2) { if (x <= 3) {c(x=x, y=y, pow=x^y) } else { c(pow=x^y) } }, y=3)

# vapply (tells sapply exactly what should be returned; errors out otherwise)
args(vapply)
vapply(1:5, FUN=sqrt, FUN.VALUE=numeric(1))
vapply(1:5, FUN=function(x, y=2) { c(x=x, y=y, pow=x^y) }, FUN.VALUE=numeric(3), y=3)

```
  
Below is some sample code for handing dates and times in R:  
```{r}
Sys.Date()
Sys.time()
args(strptime)

rightNow <- as.POSIXct(Sys.time())
format(rightNow, "%Y**%M-%d %H hours and %M minutes", usetz=TRUE)

lastChristmasNoon <- as.POSIXct("2015-12-25 12:00:00", format="%Y-%m-%d %X")
rightNow - lastChristmasNoon

nextUMHomeGame <- as.POSIXct("16/SEP/3 12:00:00", format="%y/%b/%d %H:%M:%S", tz="America/Detroit")
nextUMHomeGame - rightNow

# Time zones available in R
OlsonNames()

# From ?strptime (excerpted)
#
# ** General formats **
# %c Date and time. Locale-specific on output, "%a %b %e %H:%M:%S %Y" on input.
# %F Equivalent to %Y-%m-%d (the ISO 8601 date format).
# %T Equivalent to %H:%M:%S.
# %D Date format such as %m/%d/%y: the C99 standard says it should be that exact format
# %x Date. Locale-specific on output, "%y/%m/%d" on input.
# %X Time. Locale-specific on output, "%H:%M:%S" on input.
# 
# ** Key Components **
# %y Year without century (00-99). On input, values 00 to 68 are prefixed by 20 and 69 to 99 by 19
# %Y Year with century
# %m Month as decimal number (01-12).
# %b Abbreviated month name in the current locale on this platform.
# %B Full month name in the current locale.
# %d Day of the month as decimal number (01-31).
# %e Day of the month as decimal number (1-31), with a leading space for a single-digit number.
# %a Abbreviated weekday name in the current locale on this platform.
# %A Full weekday name in the current locale.
# %H Hours as decimal number (00-23)
# %I Hours as decimal number (01-12)
# %M Minute as decimal number (00-59).
# %S Second as integer (00-61), allowing for up to two leap-seconds (but POSIX-compliant implementations will ignore leap seconds).
# 
# ** Additional Options **
# %C Century (00-99): the integer part of the year divided by 100.
# 
# %g The last two digits of the week-based year (see %V). (Accepted but ignored on input.)
# %G The week-based year (see %V) as a decimal number. (Accepted but ignored on input.)
# 
# %h Equivalent to %b.
# 
# %j Day of year as decimal number (001-366).
# 
# %n Newline on output, arbitrary whitespace on input.
# 
# %p AM/PM indicator in the locale. Used in conjunction with %I and not with %H. An empty string in some locales (and the behaviour is undefined if used for input in such a locale).  Some platforms accept %P for output, which uses a lower-case version: others will output P.
# 
# %r The 12-hour clock time (using the locale's AM or PM). Only defined in some locales.
# 
# %R Equivalent to %H:%M.
# 
# %t Tab on output, arbitrary whitespace on input.
# 
# %u Weekday as a decimal number (1-7, Monday is 1).
# 
# %U Week of the year as decimal number (00-53) using Sunday as the first day 1 of the week (and typically with the first Sunday of the year as day 1 of week 1). The US convention.
# 
# %V Week of the year as decimal number (01-53) as defined in ISO 8601. If the week (starting on Monday) containing 1 January has four or more days in the new year, then it is considered week 1. Otherwise, it is the last week of the previous year, and the next week is week 1. (Accepted but ignored on input.)
# 
# %w Weekday as decimal number (0-6, Sunday is 0).
# 
# %W Week of the year as decimal number (00-53) using Monday as the first day of week (and typically with the first Monday of the year as day 1 of week 1). The UK convention.
# 
# For input, only years 0:9999 are accepted.
# 
# %z Signed offset in hours and minutes from UTC, so -0800 is 8 hours behind UTC. Values up to +1400 are accepted as from R 3.1.1: previous versions only accepted up to +1200. (Standard only for output.)
# 
# %Z (Output only.) Time zone abbreviation as a character string (empty if not available). This may not be reliable when a time zone has changed abbreviations over the years.

```
  
###_Writing Functions in R_  
Hadley and Charlotte Wickham led a course on writing functions in R.  Broadly, the course includes advice on when/how to use functions, as well as specific advice about commands available through library(purrr).  
  
Key pieces of advice include:  
  
* Write a function once you have cut and paste some code twice or more  
* Solve a simple problem before writing the function  
* A good function is both correct and understandable  
* Abstract away the for loops when possible (focus on data/actions, solve iteration more easily, have more understandable code), for example using purrr::map() or purr::map_<type>() where type can be dbl, chr, lgl, int, forcing a type-certain output  
* Use purrr::safely() and purrr::possibly() for better error handling  
* Use purr::pmap or purr::walk2 to iterate over 2+ arguments  
* Iterate functions for their side effects (printing, plotting, etc.) using purrr::walk()  
* Use stop() and stopifnot() for error catching of function arguments/output formats  
* Avoid type-inconsistent functions (e.g., sapply)  
* Avoid non-standard functions  
* Never rely on global options (e.g., how the user will have set stringsAsFactors)  
  
John Chambers gave a few useful slogans about functions:  
  
* Everything that exists is an object  
* Everything that happens is a function call  
  
Each function has three components:  
  
* formals(x) are in essence the arguments as in args(), but as a list  
* body(x) is the function code  
* environment(x) is where it was defined
  
Only the LAST evaluated expression is returned.  The use of return() is recommended only for early-returns in a special case (for example, when a break() will be called).  
  
Further, functions can be written anonymously on the command line, such as (function (x) {x + 1}) (1:5).  A function should only depend on arguments passed to it, not variables from a parent enviornment.  Every time the function is called, it receives a clean working environment.  Once it finishes, its variables are no longer available unless they were returned (either by default as the last operation, or by way of return()):  
  
```{r}
# Components of a function
args(rnorm)
formals(rnorm)
body(rnorm)
environment(rnorm)


# What is passed back
funDummy <- function(x) {
    if (x <= 2) {
        print("That is too small")
        return(3)  # This ends the function by convention
    }
    ceiling(x)  # This is the defaulted return() value if nothing happened to prevent the code getting here
}

funDummy(1)
funDummy(5)


# Anonymous functions
(function (x) {x + 1}) (1:5)

```
  
The course includes some insightful discussion of vectors.  As it happens, lists and data frames are just special collections of vectors in R.  Each column of a data frame is a vector, while each element of a list is either 1) an embedded data frame (which is eventually a vector by way of columns), 2) an embedded list (which is eventually a vector by way of recursion), or 3) an actual vector.  
  
The atomic vectors are of types logical, integer, character, and double; complex and raw are rarer types that are also available.  Lists are just recursive vectors, which is to say that lists can contain other lists and can be hetergeneous.  To explore vectors, you have:  
  
* typeof() for the type  
* length() for the length  
  
Note that NULL is the absence of a vector and has length 0.  NA is the absence of an element in the vector and has length 1.  All math operations with NA return NA; for example NA == NA will return NA.  
  
There are some good tips on extracting element from a list:  
  
* [] is to extract a sub-list  
* [[]] and $ more common and extract elements while removing an element of hierachy  
* seq_along(mtcars) will return 1:11 since there are 11 elements.  Helfpully, is applied to a frame with no columns, this returns integer(0) which means the for() loop does not crash  
* mtcars[[11]] will return the 11th element (11th column) of mtcars  
* vector("type", "length") will create a n empty vector of the requested type and length  
* range(x, na.rm=FALSE) gives vector c(xmin, xmax) which can be handy for plotting, scaling, and the like  
  
```{r}
# Data types
data(mtcars)
str(mtcars)
typeof(mtcars)  # n.b. that this is technically a "list"
length(mtcars)


# NULL and NA
length(NULL)
typeof(NULL)
length(NA)
typeof(NA)
NULL == NULL
NULL == NA
NA == NA
is.null(NULL)
is.null(NA)
is.na(NULL)
is.na(NA)


# Extraction
mtcars[["mpg"]][1:5]
mtcars[[2]][1:5]
mtcars$hp[1:5]


# Relevant lengths
seq_along(mtcars)
x <- data.frame()
seq_along(x)
length(seq_along(x))

foo <- function(x) { for (eachCol in seq_along(x)) { print(typeof(x[[eachCol]])) }}
foo(mtcars)
foo(x)  # Note that this does nothing!
data(airquality)
str(airquality)
foo(airquality)


# Range command
mpgRange <- range(mtcars$mpg)
mpgRange
mpgScale <- (mtcars$mpg - mpgRange[1]) / (mpgRange[2] - mpgRange[1])
summary(mpgScale)
```
  
The typical arguments in a function use a consistent, simple naming function:  
  
* x, y, z: vectors  
* df: data frame  
* i, j: numeric indices (generally rows and columns)  
* n: length of number of rows  
* p: number of columns  
  
Data arguments should come before detail arguments, and detail arguments should be given reasonable default values.  See for example rnorm(n, mean=0, sd=1).  The number requested (n) must be specified, but defaults are available for the details (mean and standard deviation).  
  
####_Functional Programming and library(purrr)_  
Functions can be passed as arguments to other functions, which is at the core of functional programming.  For example:  
```{r}
do_math <- function(x, fun) { fun(x) }
do_math(1:10, fun=mean)
do_math(1:10, fun=sd)
```
  
The library(purrr) takes advantage of this, and in a type-consistent manner.  There are functions for:  
  
* map() will create a list as the output  
* map_chr() will create a character vector as the output  
* map_dbl() will create a double vector as the output  
* map_int() will create an integer vector as the output  
* map_lgl() will create a logical (boolean) vector as the output  
  
The general arguments are .x (a list or an atomic vector) and .f which can be either a function, an anonymous function (formula with ~), or an extractor .x[[.f]].  For example:  
```{r}
library(purrr)

data(mtcars)

# Create output as a list
map(.x=mtcars, .f=sum)

# Create same output as a double
map_dbl(.x=mtcars, .f=sum)

# Create same output as integer
# map_int(.x=mtcars, .f=sum) . . . this would bomb since it is not actually an integere
map_int(.x=mtcars, .f=function(x) { as.integer(round(sum(x), 0)) } )

# Same thing but using an anonymous function with ~ and .
map_int(.x=mtcars, .f = ~ as.integer(round(sum(.), 0)) )

# Create a boolean vector
map_lgl(.x=mtcars, .f = ~ ifelse(sum(.) > 200, TRUE, FALSE) )

# Create a character vector
map_chr(.x=mtcars, .f = ~ ifelse(sum(.) > 200, "Large", "Not So Large") )

# Use the extractor [pulls the first row]
map_dbl(.x=mtcars, .f=1)

# Example from help file using chaining
mtcars %>%
  split(.$cyl) %>%
  map(~ lm(mpg ~ wt, data = .x)) %>%
  map(summary) %>%
  map_dbl("r.squared")

# Using sapply
sapply(split(mtcars, mtcars$cyl), FUN=function(.x) { summary(lm(mpg ~ wt, data=.x))$r.squared } )

# Use the extractor from a list
cylSplit <- split(mtcars, mtcars$cyl)
map(cylSplit, "mpg")
map(cylSplit, "cyl")
```
  
The purrr library has several additional interesting functions:  
  
* safely() is a wrapper for any functions that traps the errors and returns a relevant list  
* possibly() is similar to safely() with the exception that a default value for error cases is supplied  
* quietly() is a wrapper to suppress verbosity  
* transpose() reverses the order of lists (making the inner-most lists the outer-most lists), which is an easy way to extract either all the answers or all the error cases  
* map2(.x, .y, .f) allows two inputs to be passed to map()  
* pmap(.l, .f) allows passing a named list with as many inputs as needed to function .f  
* invoke_map(.f, .x, ...) lets you iterate over a list of functions .f  
* walk() is like map() but called solely to get function side effects (plot, save, etc.); it also returns the object that is passed to it, which can be convenient for chaining (piping)  
  
Some example code includes:  
```{r}
library(purrr)  # Called again for clarity; all these key functions belong to purrr

# safely(.f, otherwise = NULL, quiet = TRUE)
safe_log10 <- safely(log10)
map(list(0, 1, 10, "a"), .f=safe_log10)

# possibly(.f, otherwise, quiet = TRUE)
poss_log10 <- possibly(log10, otherwise=NaN)
map_dbl(list(0, 1, 10, "a"), .f=poss_log10)

# transpose() - note that this can become masked by data.table::transpose() so be careful
purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))
purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$result
unlist(purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$result)
purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$error
map_lgl(purrr::transpose(map(list(0, 1, 10, "a"), .f=safe_log10))$error, is.null)

# map2(.x, .y, .f)
map2(list(5, 10, 20), list(1, 2, 3), .f=rnorm) # rnorm(5, 1), rnorm(10, 2), and rnorm(20, 3)

# pmap(.l, .f)
pmap(list(n=list(5, 10, 20), mean=list(1, 5, 10), sd=list(0.1, 0.5, 0.1)), rnorm)

# invoke_map(.f, .x, ...)
invoke_map(list(rnorm, runif, rexp), n=5)

# walk() is for the side effects of a function
x <- list(1, "\n\ta\n", 3)
x %>% walk(cat)

# Chaining is available by way of the %>% operator
pretty_titles <- c("N(0, 1)", "Uniform(0, 1)", "Exponential (rate=1)")
set.seed(1607120947)
x <- invoke_map(list(rnorm, runif, rexp), n=5000)
foo <- function(x) { map(x, .f=summary) }
par(mfrow=c(1, 3))
pwalk(list(x=x, main=pretty_titles), .f=hist, xlab="", col="light blue") %>% map(.f=foo)
par(mfrow=c(1, 1))

```
  
####_Writing Robust Functions_  
There are two potentially desirable behaviors with functions:  
  
* Relaxed (default R approach) - make reasonable guesses about what you mean, which is particularly useful for interactive analyses  
* Robust (programming) - strict functions that throw errors rather than guessing in light of uncertainty  
  
As a best practice, R functions that will be used for programming (as opposed to interactive command line work) should be written in a robust manner.  Three standard problems should be avoided/mitigated:  
  
* Type-unstable - may return a vector one time, and a list the next  
* Non-standard evaluation - can use succinct API, but can introduce ambiguity  
* Hidden arguments - dependence on global functions/environments  
  
There are several methods available for throwing errors within an R function:  
  
* stopifnot(expression) will stop and throw an error unless expression is TRUE  
* if (expression) { stop("Error", call.=FALSE) }  
* if (expression) { stop(" 'x' should be a character vector", call.=FALSE) }  
    * call.=FALSE means that the call to the function should not be shown (???) - Hadley recommends this  
  
One example that commonly creates surprises is the [,] operator for extraction.  Adding [ , , drop=FALSE] ensures that you will still have what you passed (e.g., a matrix or data frame) rather than conversion of achunk of data to a vector.

Another common source of error is sapply() which will return a vector when it can and a list otherwise.  The map() and map_typ() functions in purrr are designed to be type-stable; if the output is not as expected, they will error out.  
  
Non-standard evaluations take advantage of the existence of something else (e.g., a variable in the parent environment that has not been passed).  This can cause confusion and improper results.  
  
* subset(mtcars, disp > 400) takes advantage of disp being an element of mtcars; disp would crash if called outside subset  
* This can cause problems when it is embedded inside a function  
* ggplot and dplyr frequently have these behaviors also  
    * The risk is that you can also put variables from the global environment in to the same call  
  
Pure functions have the key properties that 1) their output depends only on their inputs, and 2) they do not impact the outside world other than by way of their return value.  Specifically, the function should not depend on how the user has configured their global options as shown in options(), nor should it modify those options() settings upon return of control to the parent environment.  
  
A few examples are shown below:  
```{r}
# Throwing errors to stop a function (cannot actually run these!)
# stopifnot(FALSE)
# if (FALSE) { stop("Error: ", call.=FALSE) }
# if (FALSE) { stop("Error: This condition needed to be set as TRUE", call.=FALSE) }

# Behavior of [,] and [,,drop=FALSE]
mtxTest <- matrix(data=1:9, nrow=3, byrow=TRUE)
class(mtxTest)
mtxTest[1, ]
class(mtxTest[1, ])
mtxTest[1, , drop=FALSE]
class(mtxTest[1, , drop=FALSE])

# Behavior of sapply() - may not get what you are expecting
foo <- function(x) { x^2 }
sapply(1:5, FUN=foo)
class(sapply(1:5, FUN=foo))
sapply(c(1, list(1.5, 2, 2.5), 3, 4, 5), FUN=foo)
class(sapply(c(1, list(1.5, 2, 2.5), 3, 4, 5), FUN=foo))
sapply(list(1, c(1.5, 2, 2.5), 3, 4, 5), FUN=foo)
class(sapply(list(1, c(1.5, 2, 2.5), 3, 4, 5), FUN=foo))

```
  
This was a very enjoyable and instructive course.  
  
###_Importing Data in to R_  
This course provides an overview of loading data in to R from five main sources:  
  
* Flat files  
* Excel files  
* Statistical software  
* Databases  
* Web data  
  
####_Reading Flat Files_  
At the most basic level, the utlis library easily handles reading most types of flat files:  
  
* read.table(file, header=FALSE, sep="", stringsAsFactors=default.stringsAsFactors(), <many more>)  
* read.csv(file, header = TRUE, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "", ...)  
* read.delim(file, header = TRUE, sep = "\t", quote = "\"",  dec = ".", fill = TRUE, comment.char = "", ...)  
  
There are also European equivalents in case the decimal needs to be set as "," to read in the file:  
  
* read.csv2(file, header = TRUE, sep = ";", quote = "\"", dec = ",", fill = TRUE, comment.char = "", ...)  
* read.delim2(file, header = TRUE, sep = "\t", quote = "\"", dec = ",", fill = TRUE, comment.char = "", ...)  
  
The file.path() command is a nice way to put together file paths.  It is more or less equivalent to paste(, sep="/"), but with the benefit that sep is machine/operating-system dependent, so it may be easier to use across platforms.  
  
Further, there is the option to use colClasses() to specify the type in each column, with NULL meaning do not import.  Abbreviations can be used for these as well:  
```{r}
# colClasses (relevant abbreviations)
R.utils::colClasses("-?cdfilnrzDP")

# file.path example
file.path("..", "myplot.pdf")

# Key documentation for reading flat files
# 
# read.table(file, header = FALSE, sep = "", quote = "\"'",
#            dec = ".", numerals = c("allow.loss", "warn.loss", "no.loss"),
#            row.names, col.names, as.is = !stringsAsFactors,
#            na.strings = "NA", colClasses = NA, nrows = -1,
#            skip = 0, check.names = TRUE, fill = !blank.lines.skip,
#            strip.white = FALSE, blank.lines.skip = TRUE,
#            comment.char = "#",
#            allowEscapes = FALSE, flush = FALSE,
#            stringsAsFactors = default.stringsAsFactors(),
#            fileEncoding = "", encoding = "unknown", text, skipNul = FALSE)
# 
# read.csv(file, header = TRUE, sep = ",", quote = "\"",
#          dec = ".", fill = TRUE, comment.char = "", ...)
# 
# read.csv2(file, header = TRUE, sep = ";", quote = "\"",
#           dec = ",", fill = TRUE, comment.char = "", ...)
# 
# read.delim(file, header = TRUE, sep = "\t", quote = "\"",
#            dec = ".", fill = TRUE, comment.char = "", ...)
# 
# read.delim2(file, header = TRUE, sep = "\t", quote = "\"",
#             dec = ",", fill = TRUE, comment.char = "", ...)
```
  
There are also two libraries that can be especially helpful for reading in flat files - readr and data.table.  
  
* readr::read_delim() handles many data types  
* readr::read_delim(file, delim=",") will read a CSV
    * assumes col_names=TRUE (eq to header=TRUE)
    * assumes col_types=NULL (imputed from first 100 rows, side effect - no need for stringAsFactors = FALSE)  
* col_types can use short type, where c=character, d=double (numeric), i=integer, l=logical (boolean), _=skip  
    * col_names = FALSE means make your own  
    * col_names = c() means here are the column names you should use  
* skip=<number to skip>  
* n_max=<number to read>  
* read_csv() is for CSV  
* read_tsv is for tab-separated values	
  
* data.table() is designed for speed  
    * data.table::fread() is for fast reading  
    * The fread() automatically handles the column names and also infers the column separators  
    * This is a faster, more convenients, and easier to customize version of read.table()  
  
* Wrappers for the readr() function  
    * fac <- col_factor(levels = c("Beef", "Meat", "Poultry"))  
    * int <- col_integer()  
	* hotdogsFactor <- read_tsv("hotdogs.txt",  
	                            col_names = c("type", "calories", "sodium"), 
	                            col_types = list(fac, int, int)
	                            )
  
####_Reading Excel Files_    
Further, the library(readxl) is handy for loading Excel sheets:  
  
* readxl::excel_sheets() will list the sheets  
    * excel_sheets(path)  
* readxl::read_excel() will read in a specific sheet  
    * read_excel(path, sheet = 1, col_names = TRUE, col_types = NULL, na = "", skip=0)	
        * col_names: Either TRUE to use the first row as column names, FALSE to number columns sequentially     from X1 to Xn, or a character vector giving a name for each column  
        * col_types: Either NULL to guess from the spreadsheet or a character vector containing "blank", "numeric", "date" or "text"  
* lapply(excel_sheets(myXLS), FUN=read_excel, path=myXLS) provide all data from all sheets in a list  
  
####_Reading Statistical Software Files_  
R can also load files from common statistical software such as SAS, STATA, SPSS, and MATLAB/Octave.  The packages haven() by Wickham and foreign() by the R core team are two common examples.  The R.matlab() allows for reading to/from MATLAB/Octave:  
  
The library(haven) contains wrappers to the ReadStat package, a C library by Evan Miller, for reading files from SAS, STATA, and SPSS:  
  
* read_sas(filename)	
* read_stata(filename)  
* read_dta(filename)  
    * as_factor(R_column) will help if the type is "labelled"  
    * as.character(as_factor(R_column)) will turn it back to a character vector  
* read_spss(filename) which is a wrapper to read_por() and read_sav()	
  
The library(foreign) can read/write all types of foreign formats, with some caveats:  
  
* Only SAS libraries (.xport) can be read in; seems quite a drawback to not be able to read SAS files!  
* read.dta(file, convert.factors = TRUE, convert.dates=TRUE, missing.type=FALSE)  
* read.spss(file, use.value.labels = TRUE, to.data.frame = FALSE)  
  
Finally, the R.matlab() library is available for reading/writing MATLAB/Octave files.  Per the help file:   
  
* Methods readMat() and writeMat() for reading and writing MAT files. For user with MATLAB v6 or newer installed (either locally or on a remote host), the package also provides methods for controlling MATLAB (trademark) via R and sending and retrieving data between R and MATLAB.  
  
* In brief, this package provides a one-directional interface from R to MATLAB, with communication taking place via a TCP/IP connection and with data transferred either through another connection or via the file system. On the MATLAB side, the TCP/IP connection is handled by a small Java add-on.  
  
* The methods for reading and writing MAT files are stable. The R to MATLAB interface, that is the Matlab class, is less prioritized and should be considered a beta version.  
  
* readMat(con, maxLength=NULL, fixNames=TRUE, drop=c("singletonLists"), sparseMatrixClass=c("Matrix", "SparseM", "matrix"), verbose=FALSE, ...)  
    * Returns a named list structure containing all variables in the MAT file structure.  
  
* writeMat(con, ..., matVersion="5", onWrite=NULL, verbose=FALSE)  
    * con: Binary connection to which the MAT file structure should be written to. A string is interpreted as filename, which then will be opened (and closed afterwards).  
    * ...: Named variables to be written where the names must be unique.  
  
####_Reading Relational DB Files_  
Relational databases in R (DBMS tend to use SQL for queries), including libraries:  
  
* RMySQL  
* RPostgresSQL  
* ROracle  
* RSQLite  

Conventions are specified in DBI; see library(DBI):  
  
* dbConnect(drv, ...)  
* drv  
  
Create the connection as "con" (or whatever) and then use that elsewhere:  
  
* dbListTables(con)  # What tables are in this  
* dbReadTable(con, tablename)  
  
When finished, dbDisconnect(con) as a courtesy so as to not tie up resources.  
  
SQL queries from inside R - per previous, library(DBI) and then create the connection "con":  
  
* dbGetQuery(con, valid_SQL_code)  
    * Appears that \" is an escape character to use quotes inside quotes  
    * dbGetQuery(con, "SELECT name FROM employees WHERE started_at > \"2012-09-01\"")  
    * dbGetQuery(con, "SELECT * FROM products WHERE contract=1")  
* res <- dbSendQuery(con, validSQLcode) # If instead you want to grab only chunks of records  
    * dbFetch(res)  # Can specify chunking, such as dbFetch(res, n=1) for one line at a time  
    * dbHasCompleted(res) # see whether it is done  
        * while(!dbHasCompleted(res)) { chunk <- dbFetch(res, n=1); print(chunk) }  
    * dbClearResult(res)  
  
For example, using "./SQLforDataCampRMD_v01.db", run a few SQL commands:  
```{r}
# uses libraries DBI for the connection and RSQLite to interface with SQLite Browser on my machine
con <- DBI::dbConnect(RSQLite::SQLite(), "SQLforDataCampRMD_v01.db")

# List the tables, and drop dummy if it already exists
DBI::dbListTables(con)
DBI::dbGetQuery(con, "DROP TABLE IF EXISTS dummy")

# Create blank table
DBI::dbListTables(con)
DBI::dbGetQuery(con, "CREATE TABLE IF NOT EXISTS dummy (id PRIMARY KEY, name CHAR)")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (1, 'Amy')")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (2, 'Bill')")
DBI::dbGetQuery(con, "INSERT OR IGNORE INTO dummy (id, name) VALUES (2, 'Jen')") # Should do nothing
DBI::dbGetQuery(con, "SELECT * FROM dummy")
DBI::dbListTables(con)

# Can continue passing SQL commands back and forth as needed

# Close the connection
DBI::dbDisconnect(con)

```
  
####_Reading Web Data_  
Many of the R read-in libraries already work well with web data.  For example, read.csv("mywebsite.com", stringAsFactors=FALSE) will read a CSV right off the internet.  Further, there are options for:  
  
* download.file(url, dest_path) # Reproducibility advantages over right-click and save  
* library(httr) by Hadley Wickham, including GET() and content()  
  
The jsonlite library is good for working with JSON:  
  
* fromJSON(url) will create a named R list (often creates a data frame also)  
    * JSON objects are name:value pairs  
    * JSON arrays convert to vectors  
    * JSON can also created nested arrays  
* toJSON()  
  
Prettify adds indentation to a JSON string; minify removes all indentation/whitespace:  
  
* prettify()    prettify(txt, indent = 4)  
* minify()	    minify(txt)  
  
```{r}
jsonLoc <- file.path("../../..", "PythonDirectory", "UMModule04", "roster_data.json")
jsonData <- jsonlite::fromJSON(jsonLoc)
str(jsonData)
head(jsonData)
```
  
###_Cleaning Data in R_  
The general analysis pipeline is Collect -> Clean -> Analyze -> Report.  Cleaning is needed so the raw data can work with more traditional tools (e.g., packages in Python or R).  50% - 80% of time is spent in the Collect/Clean realm, even though this is not the most exciting (and thus taught) part of data analysis.  There are generally three stages of data cleaning:  Explore -> Tidy -> Prepare
  
Exploring the Data:  
  
* class()  
* dim()  
* names()  # column names  
* str()  
* dplyr::glimpse()  # a nicer version of str()  
* summary()  
  
Viewing the Data:  
  
* head(file, n=6)  
* tail(file, n=6)  
* hist(variable)  
* plot(x, y)  
* print()  # not recommended for larger datasets  
  
Tidy data - Wickham 2014, Principles of Tidy Data:  
  
* Each row should be an observation	
* Each column should be a variable (attribute)	
* Column headers should not be variables; e.g., eye color should be a single column, not many columns of 0/1  
* Each intersection should be a value (intersection of the observation and attribute)  
* Only one type of observational unit (e.g., each row is a person) per table  
  
The principles of tidy data can be implemented using library(tidyr):  
  
* tidyr::gather(data, key, value, ..., na.rm = FALSE, convert = FALSE, factor_key = FALSE)  # gather the data in to key-value pairs  
    * key, value	Names of key and value columns to create in output.  
    * ...	Specification of columns to gather. Use bare variable names. Select all variables between x and z with x:z, exclude y with -y. For more options, see the select documentation.  
  
* tidyr::spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE)  # spread the key-value pairs in to columns  
    * key	The bare (unquoted) name of the column whose values will be used as column headings.  
    * value	The bare (unquoted) name of the column whose values will populate the cells.  
    * fill	If set, missing values will be replaced with this value. Note that there are two types of missing in the input: explicit missing values (i.e. NA), and implicit missings, rows that simply aren't present. Both types of missing value will be replaced by fill.  
  
* tidyr::separate(data, col, into, sep = "[^[:alnum:]]+", remove = TRUE, convert = FALSE, extra = "warn", fill = "warn", ...)  
    * col	Bare column name.  
    * into	Names of new variables to create as character vector.  
    * sep	Separator between columns.  If character, is interpreted as a regular expression. The default value is a regular expression that matches any sequence of non-alphanumeric values.  If numeric, interpreted as positions to split at. Positive values start at 1 at the far-left of the string; negative value start at -1 at the far-right of the string. The length of sep should be one less than into.  
  
* tidyr::unite(data, col, ..., sep = "_", remove = TRUE)  
    * col	(Bare) name of column to add  
    * ...	Specification of columns to unite. Use bare variable names. Select all variables between x and z with x:z, exclude y with -y. For more options, see the select documentation.  
    * sep	Separator to use between values.  
  
Common symptoms of messy data include:  
  
* Column headers are values rather than variable names -- use tidyr::gather()  
* Variables stored in both rows and columns -- use tidyr::spread()  
* Multiple variables are stored in a single column -- use tidyr::separate()  
* Singe type of observational unit (e.g., people) stored in 2+ tables  
* Multiple types of observational units (e.g., people and pets) stored in a single table  
  
Example code includes:  
```{r}
# tidyr::gather()
stocks <- data.frame(time = as.Date('2009-01-01') + 0:4, 
                     X = rnorm(5, 0, 1), Y = rnorm(5, 0, 2), Z = rnorm(5, 0, 4)
                     )
stocks
# will create new columns stock (each of X, Y, Z) and price (the values that had been in X, Y, and Z), 
# while not gathering the time variable; final table will be time-stock-price
stockGather <- tidyr::gather(stocks, stock, price, -time)  
stockGather

# tidyr::spread()
tidyr::spread(stockGather, stock, price)
# TRUE (this fully reverses what the gather function has done)
identical(tidyr::spread(stockGather, stock, price), stocks)  


# tidyr::separate()
df <- data.frame(x = c(NA, "a.b", "a.d", "b.c"))
df
# by default, the splits occur on anything that is not alphanumeric, 
# so you get column A as whatever is before the dot and column B as whatever is after the dot
dfSep <- tidyr::separate(df, x, c("A", "B"))
dfSep

# tidyr::unite()
tidyr::unite(dfSep, united, c(A, B), sep="")
is.na(dfSep) # caution . . . 
is.na(tidyr::unite(dfSep, united, c(A, B), sep="")) # caution . . . 
```
  
* The library(lubridate) can be helpful for coercing strings to dates  
    * ymd()  # coerces a character string that is in year-month-day originally  
    * mdy() # coerces a character string that is in month-day-year originally  
    * hms() # coerces a character string that is in hours-minutes-seconds originally  
    * ymd_hms() # coerces a character string that is in year-month-day-hours-minutes-seconds  
  
* The library(stringr) can be helpful for working with strings  
	* stringr::str_trim()  # trails the leading and trailing white space  
	* stringr::str_pad(char, width, side, pad)  # adds characters in place of white space at the start/end  
	* stringr::str_detect(inVector, searchTerm) # is searchTerm in each iterm of vector (boolean, same length as inVector)  
	* stringr::str_replace(inVector, searchTerm, replaceTerm) # searchTerm will be replaced by replaceTerm in each iterm of vector (output same length/type as inVector)  
        * social_df$status <- str_replace(social_df$status, "^$", NA) # Nice way to make the blanks in to NA  
  
* The tolower() and toupper() commands can be very useful also  
  
* Missing and special values  
    * May be randomly missing, but very dangerous to assume!  
    * In R, these are NA  
        * Excel may have #N/A  
        * SPSS and SAS may have .  
        * Sometimes just shows up as a missing string  
    * Inf is for infinite  
    * NaN is for not a number  
  
* Finding these special values  
    * is.na()  
	* any(is.na())  
	* sum(is.na())  
	* summary()  # will tell the number of NA  
	* complete.cases()  # TRUE is the entire row is non-NA; FALSE otherwise  
	* na.omit()  # equivalent to x[complete.cases(x), ]  
  
Example code includes:  
```{r}
# lubridate::ymd()
lubridate::ymd("160720")
lubridate::ymd("2016-7-20")
lubridate::ymd("16jul20")
lubridate::ymd("16/07/20")

# lubridate::hms()
lubridate::hms("07h15:00")
lubridate::hms("17 hours, 15 minutes 00 seconds")
lubridate::hms("07-15-00")

# From ?stringr::str_detect
# 
# str_detect(string, pattern)
#   string	Input vector. Either a character vector, or something coercible to one.
#   pattern	Pattern to look for.  The default interpretation is a regular expression, as described in stringi-search-regex. Control options with regex().  Match a fixed string (i.e. by comparing only bytes), using fixed(x). This is fast, but approximate. Generally, for matching human text, you'll want coll(x) which respects character matching rules for the specified locale.  Match character, word, line and sentence boundaries with boundary(). An empty pattern, "", is equivalent to boundary("character").
# 

fruit <- c("apple", "banana", "pear", "pinapple")

stringr::str_detect(fruit, "a")
stringr::str_detect(fruit, "^a")
stringr::str_detect(fruit, "a$")
stringr::str_detect(fruit, "b")
stringr::str_detect(fruit, "[aeiou]")

# Also vectorised over pattern
stringr::str_detect("aecfg", letters)


# From ?stringr::str_replace
#
# str_replace(string, pattern, replacement)
# str_replace_all(string, pattern, replacement)
#   string	Input vector. Either a character vector, or something coercible to one.
#   pattern, replacement	Supply separate pattern and replacement strings to vectorise over the patterns. References of the form \1, \2 will be replaced with the contents of the respective matched group (created by ()) within the pattern.  For str_replace_all only, you can perform multiple patterns and replacements to each string, by passing a named character to pattern.
#

someNA <- c(letters, "", LETTERS, "")
someNA[someNA==""] <- NA
someNA

fruits <- c("one apple", "two pears", "three bananas")
stringr::str_replace(fruits, "[aeiou]", "-")  # Replace FIRST instance
stringr::str_replace_all(fruits, "[aeiou]", "-")  # Replace ALL instances

stringr::str_replace(fruits, "([aeiou])", "\\1\\1\\1")  # Triple up on the first vowel
stringr::str_replace(fruits, "[aeiou]", c("1", "2", "3"))  # First vowel to 1, 2, 3 in word 1, 2, 3
stringr::str_replace(fruits, c("a", "e", "i"), "-")  # First a -> - in word 1, first e -> - in word 2 . . . 

stringr::str_replace_all(fruits, "([aeiou])", "\\1\\1")  # Double up on all vowels
stringr::str_replace_all(fruits, "[aeiou]", c("1", "2", "3"))  # All vowels to 1, 2, 3, in word 1, 2, 3
stringr::str_replace_all(fruits, c("a", "e", "i"), "-")  # All a -> - in word 1, . . . 

```
  
Further, the outline from the weather gathering data cleaning challenge is noted:  
  
* The weather data tidying challenge  
	* Historical weather data from Boston  
	    * 12 months beginning December 2014  
	    * Columns are values (X1 means day 1, X2 means day 2, etc.), while measure (Max, Min, etc.) should be variables  
		* Variables coded incorrectly  
		* Missing and extreme values  
		* Etc.  
	
* STEP 1: UNDERSTAND STRUCTURE  
	* class(), dim(), names(), str(), dplyr::glimpse(), summary()  
* STEP 2: LOOK AT DATA  
	* head(), tail(), print(), hist(), plot()  
* STEP 3: TIDY DATA  
	* gather(), spread(), separate()  
* STEP 4: CONVERT TYPES  
    * lubridate() and variants, as.character() and variants, stringr() and variants, tidyr::unite()  
* STEP 5: MANAGE MISSING and EXTREME (OUTLIER) VALUES  
    * is.na(), sum(is.na()), any(is.na()), which(is.na())  
    * which(a, arr.ind=TRUE) returns a little matrix of rows and columns - nice!	
  
###_Data Manipulation (dplyr)_  
The library(dplyr) is a grammar of data manipulation.  It is written in C++ so you get the speed of C with the convenience of R.  It is in essence the data frame to data frame portion of plyr (plyr was the original Split-Apply-Combine).  May want to look in to count, transmute, and other verbs added post this summary.  
  
The examples use data(hflights) from library(hflights):  
```{r}
library(dplyr)
library(hflights)
data(hflights)
head(hflights)
summary(hflights)
```
  
The "tbl" is a special type of data frame, which is very helpful for printing:  
  
* tbl_df(myFrame)  # can store or whatever - will be a tbl_df, tbl, and data.frame  
    * Display is modified to fit the window display - will scale with the window  
* glimpse(myFrame) # lets you see al the variables and first few records for each (sort of like str)  
* as.data.frame(tbl_df(myFrame)) # this will be the data frame  
    * identical(as.data.frame(tbl_df(hflights)), hflights)  # FALSE  
    * sum(is.na(as.data.frame(tbl_df(hflights))) != is.na(hflights))  # 0  
    * sum(as.data.frame(tbl_df(hflights)) != hflights, na.rm=TRUE)  # 0  
  
An interesting way to do a lookup table:  
  
* two <- c("AA", "AS")  
* lut <- c("AA" = "American",  "AS" = "Alaska",  "B6" = "JetBlue")  
* two <- lut[two]  
* two  
  
See for example:  
```{r}
lut <- c("AA" = "American", "AS" = "Alaska", "B6" = "JetBlue", "CO" = "Continental", 
         "DL" = "Delta", "OO" = "SkyWest", "UA" = "United", "US" = "US_Airways", 
         "WN" = "Southwest", "EV" = "Atlantic_Southeast", "F9" = "Frontier", 
         "FL" = "AirTran", "MQ" = "American_Eagle", "XE" = "ExpressJet", "YV" = "Mesa"
         )
hflights$Carrier <- lut[hflights$UniqueCarrier]  
glimpse(hflights)  
```
  
There are five main verbs in dplyr:  
  
* select - subset of columns from a dataset  
    * select(df, . . . ) where . . . Are the columns to be kept  
	* starts_with("X"): every name that starts with "X",  
	* ends_with("X"): every name that ends with "X",  
	* contains("X"): every name that contains "X",  
	* matches("X"): every name that matches "X", where "X" can be a regular expression,  
	* num_range("x", 1:5): the variables named x01, x02, x03, x04 and x05,  
	* one_of(x): every name that appears in x, which should be a character vector.  
	* filter - subset of rows from a dataset  
        * filter(df, .)  where ... are 1+ logical tests (so make sure to use == or all.equal() or the like)  
* arrange - reorder rows in a dataset  
    * arrange(df, .) where . are the colunns to reorder by  
* mutate - create new columns in a dataset  
    * mutate(df, .) where each . is a formula for a new variable to be created  
* summarize - create summary statistics for a dataset  
    * summarize(df, .) where each . is a formula like newVar = thisEquation  
        * only aggregate functions (vector as input, single number as output) should be used  
    * dplyr adds several additional aggregate functions such as first, last, nth, n, n_distinct  
		* first(x) - The first element of vector x.  
		* last(x) - The last element of vector x.  
		* nth(x, n) - The nth element of vector x.  
		* n() - The number of rows in the data.frame or group of observations that summarise() describes.  
		* n_distinct(x) - The number of unique values in vector x.  
  
* In general:  
    * select and mutate operate on the variables  
    * filter and arrange operate on the observations  
    * summarize operates on groups of observations  
	* All of these are much cleaner if the data are tidy  
  
* There is also the option to use chaining %>% to process multiple commands		
    * Especially useful for memory storage and readability  
	* The pipe operator (%>%) comes from the magrittr package by Stefan Bache  
	* object %>% function(object will go first)  
	* c(1, 2, 3) %>% sum() # 6  
	* c(1, 2, 3, NA) %>% mean(na.rm=TRUE) # 2  
  
There is also the group_by capability for summaries of sub-groups:  
  
* group_by(df, .) where the . is what to group the data by  
    * The magic is when you run summarize() on data with group_by run on it; results will be by group  
	* If you have group_by(df, a, b) %>% summarize(timeSum = sum(time)) # all observations by a-b  
	* If you have group_by(df, a, b) %>% summarize(timeSum = sum(time)) %>% summarize(timeA = sum(timeSum)) # all observations by a  
	* If you have group_by(df, a, b) %>% summarize(timeSum = sum(time)) %>% summarize(timeA = sum(timeSum)) %>% summarize(timeAll = sum(timeA)) # all observations  
  
The dplyr library can also work with databases.  It only loads the data that you need, and you do not need to know the relevant SQL code -- dplyr writes the SQL code for you.  
  
Basic select and mutate examples include:  
```{r}
data(hflights)

# Make it faster, as well as a prettier printer
hflights <- tbl_df(hflights)
hflights
class(hflights)

# Select examples
select(hflights, ActualElapsedTime, AirTime, ArrDelay, DepDelay)
select(hflights, Origin:Cancelled)
select(hflights, Year:DayOfWeek, ArrDelay:Diverted)
select(hflights, ends_with("Delay"))
select(hflights, UniqueCarrier, ends_with("Num"), starts_with("Cancel"))
select(hflights, ends_with("Time"), ends_with("Delay"))

# Mutate example
m1 <- mutate(hflights, loss = ArrDelay - DepDelay, loss_ratio = loss / DepDelay)
class(m1)
m1
glimpse(m1)

```
  
