---
title: "Weather Modeling"
author: "davegoblue"
date: "8/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
The file 'WeatherEDA_202005_v002.Rmd' contains exploratory data analysis for historical weather data as contained in METAR archives hosted by Iowa State University.

Data have been dowloaded, processed, cleaned, and integrated for several stations (airports) and years, with .rds files saved in "./RInputFiles/ProcessedMETAR".

This module will perform initial modeling on the processed weather files.  It builds on the previous 'WeatherModeling_202006_v001.Rmd' and 'WeatherModeling_202006_v002.Rmd' as well as leveraging functions in 'WeatherModelingFunctions_v001.R'.

This file focuses on:  
  
1.  Random forest classification of select locales using 2014-2019 data  
2.  Random forest classification of locales in 2016  
3.  Random forest regression of temperatures in select locales for 2014-2019  
4.  XGB regression of temperatures in select locales for 2014-2019  
5.  XGB classification of locales in 2016  
  
There are numerous other models available in 'WeatherModeling_202006_v002.Rmd'.
  
#### _Data Availability_  
There are three main processed files available for further exploration:  
  
_metar_postEDA_20200617.rds and metar_postEDA_extra_20200627.rds_  
  
* source (chr) - the reporting station and time  
* locale (chr) - the descriptive name for source  
* dtime (dttm) - the date-time for the observation  
* origMETAR (chr) - the original METAR associated with the observation at that source and date-time  
* year (dbl) - the year, extracted from dtime  
* monthint (dbl) - the month, extracted from dtime, as an integer  
* month (fct) - the month, extracted from dtime, as a three-character abbreviation (factor)  
* day (int) - the day of the month, extracted from dtime  
* WindDir (chr) - previaling wind direction in degrees, stored as a character since 'VRB' means variable  
* WindSpeed (int) - the prevailing wind speed in knots  
* WindGust (dbl) - the wind gust speed in knots (NA if there is no recorded wind gust at that hour)  
* predomDir (chr) - the predominant wind direction as NE-E-SE-S-SW-W-NW-N-VRB-000-Error  
* Visibility (dbl) - surface visibility in statute miles  
* Altimeter (dbl) - altimeter in inches of mercury  
* TempF (dbl) - the Fahrenheit temperature  
* DewF (dbl) - the Fahrenheit dew point  
* modSLP (dbl) - Sea-Level Pressure (SLP), adjusted to reflect that SLP is recorded as 0-1000 but reflects data that are 950-1050  
* cTypen (chr) - the cloud type of the nth cloud layer (FEW, BKN, SCT, OVC, or VV)  
* cLeveln (dbl) - the cloud height in feet of the nth cloud layer  
* isRain (lgl) - was rain occurring at the moment the METAR was captured?  
* isSnow (lgl) - was snow occurring at the moment the METAR was captured?  
* isThunder (lgl) - was thunder occurring at the moment the METAR was captured?  
* p1Inches (dbl) - how many inches of rain occurred in the past hour?  
* p36Inches (dbl) - how many inches of rain occurred in the past 3/6 hours (3-hour summaries at 3Z-9Z-15Z-21Z and 6-hour summaries at 6Z-12Z-18Z-24Z and NA at any other Z times)?  
* p24Inches (dbl) - how many inches of rain occurred in the past 24 hours (at 12Z, NA at all other times)  
* tempFHi (dbl) - the high temperature in the past 24 hours, in Fahrenheit (reported once per day)  
* tempFLo (dbl) - the low temperature in the past 24 hours, in Fahrenheit (reported once per day)  
* minHeight (dbl) - the minimum cloud height in feet (-100 means 'no clouds')  
* minType (fct) - amount of obscuration at the minimum cloud height (VV > OVC > BKN > SCT > FEW > CLR)  
* ceilingHeight (dbl) - the minimum cloud ceiling in feet (-100 means 'no ceiling')  
* ceilingType (fct) - amount of obscuration at the minimum ceiling height (VV > OVC > BKN)  
  
_metar_modifiedClouds_20200617.rds and metar_modifiedclouds_extra_20200627.rds_  
  
* source (chr) - the reporting station and time  
* sourceName (chr) - the descriptive name for source  
* dtime (dttm) - the date-time for the observation  
* level (dbl) - cloud level (level 0 is inserted for every source-dtime as a base layer of clear)  
* height (dbl) - level height (height -100 is inserted for every source-dtime as a base layer of clear)  
* type (dbl) - level type (type CLR is inserted for every source-dtime as a base layer of clear)  
  
_metar_precipLists_20200617.rds and metar_precipLists_extra_20200627.rds_  
  
* Contains elements for each of rain/snow/thunder for each of 2015/2016/2017  
* Each element contains a list and a tibble  
* The tibble is precipLength and contains precipitation by month as source-locale-month-hours-events  
* The list is precipList and contains data on each precipitation interval  
  
Several mapping files are defined for use in plotting; tidyverse, lubridate, and caret are loaded; and the relevant functions are sourced:  
```{r}

# The process frequently uses tidyverse, lubridate, caret, and randomForest
library(tidyverse)
library(lubridate)
library(caret)
library(randomForest)


# The main path for the files
filePath <- "./RInputFiles/ProcessedMETAR/"


# Sourcing functions
source("./WeatherModelingFunctions_v001.R")


# Descriptive names for key variables
varMapper <- c(source="Original source file name", 
               locale="Descriptive name",
               dtime="Date-Time (UTC)",
               origMETAR="Original METAR",
               year="Year",
               monthint="Month",
               month="Month", 
               day="Day of Month",
               WindDir="Wind Direction (degrees)", 
               WindSpeed="Wind Speed (kts)",
               WindGust="Wind Gust (kts)",
               predomDir="General Prevailing Wind Direction",
               Visibility="Visibility (SM)", 
               Altimeter="Altimeter (inches Hg)",
               TempF="Temperature (F)",
               DewF="Dew Point (F)", 
               modSLP="Sea-Level Pressure (hPa)", 
               cType1="First Cloud Layer Type", 
               cLevel1="First Cloud Layer Height (ft)",
               isRain="Rain at Observation Time",
               isSnow="Snow at Observation Time",
               isThunder="Thunder at Obsevation Time",
               tempFHi="24-hour High Temperature (F)",
               tempFLo="24-hour Low Temperature (F)",
               minHeight="Minimum Cloud Height (ft)",
               minType="Obscuration Level at Minimum Cloud Height",
               ceilingHeight="Minimum Ceiling Height (ft)",
               ceilingType="Obscuration Level at Minimum Ceiling Height", 
               hr="Hour of Day (Zulu time)",
               hrfct="Hour of Day (Zulu time)",
               hrBucket="Hour of Day (Zulu time) - rounded to nearest 3",
               locNamefct="Locale Name"
               )


# File name to city name mapper
cityNameMapper <- c(katl_2016="Atlanta, GA (2016)",
                    kbos_2016="Boston, MA (2016)", 
                    kdca_2016="Washington, DC (2016)", 
                    kden_2016="Denver, CO (2016)", 
                    kdfw_2016="Dallas, TX (2016)", 
                    kdtw_2016="Detroit, MI (2016)", 
                    kewr_2016="Newark, NJ (2016)",
                    kgrb_2016="Green Bay, WI (2016)",
                    kgrr_2016="Grand Rapids, MI (2016)",
                    kiah_2016="Houston, TX (2016)",
                    kind_2016="Indianapolis, IN (2016)",
                    klas_2014="Las Vegas, NV (2014)",
                    klas_2015="Las Vegas, NV (2015)",
                    klas_2016="Las Vegas, NV (2016)", 
                    klas_2017="Las Vegas, NV (2017)", 
                    klas_2018="Las Vegas, NV (2018)",
                    klas_2019="Las Vegas, NV (2019)",
                    klax_2016="Los Angeles, CA (2016)", 
                    klnk_2016="Lincoln, NE (2016)",
                    kmia_2016="Miami, FL (2016)", 
                    kmke_2016="Milwaukee, WI (2016)",
                    kmsn_2016="Madison, WI (2016)",
                    kmsp_2016="Minneapolis, MN (2016)",
                    kmsy_2014="New Orleans, LA (2014)",
                    kmsy_2015="New Orleans, LA (2015)",
                    kmsy_2016="New Orleans, LA (2016)", 
                    kmsy_2017="New Orleans, LA (2017)", 
                    kmsy_2018="New Orleans, LA (2018)",
                    kmsy_2019="New Orleans, LA (2019)",
                    kord_2014="Chicago, IL (2014)",
                    kord_2015="Chicago, IL (2015)",
                    kord_2016="Chicago, IL (2016)", 
                    kord_2017="Chicago, IL (2017)", 
                    kord_2018="Chicago, IL (2018)",
                    kord_2019="Chicago, IL (2019)",
                    kphl_2016="Philadelphia, PA (2016)", 
                    kphx_2016="Phoenix, AZ (2016)", 
                    ksan_2014="San Diego, CA (2014)",
                    ksan_2015="San Diego, CA (2015)",
                    ksan_2016="San Diego, CA (2016)",
                    ksan_2017="San Diego, CA (2017)",
                    ksan_2018="San Diego, CA (2018)",
                    ksan_2019="San Diego, CA (2019)",
                    ksat_2016="San Antonio, TX (2016)", 
                    ksea_2016="Seattle, WA (2016)", 
                    ksfo_2016="San Francisco, CA (2016)", 
                    ksjc_2016="San Jose, CA (2016)",
                    kstl_2016="Saint Louis, MO (2016)", 
                    ktpa_2016="Tampa Bay, FL (2016)", 
                    ktvc_2016="Traverse City, MI (2016)"
                    )

# File names in 2016, based on cityNameMapper
names_2016 <- grep(names(cityNameMapper), pattern="[a-z]{3}_2016", value=TRUE)

```
  
The main data will be from the metar_postEDA files.  They are integrated below, cloud and ceiling heights are converted to factors, hour is added as both a factor/numeric variable, and locale is added as a factor variable:  
```{r}

# Main weather data
metarData <- readRDS("./RInputFiles/ProcessedMETAR/metar_postEDA_20200617.rds") %>%
    bind_rows(readRDS("./RInputFiles/ProcessedMETAR/metar_postEDA_extra_20200627.rds")) %>%
    mutate(orig_minHeight=minHeight, 
           orig_ceilingHeight=ceilingHeight, 
           minHeight=mapCloudHeight(minHeight), 
           ceilingHeight=mapCloudHeight(ceilingHeight), 
           hr=lubridate::hour(lubridate::round_date(dtime, unit="1 hour")),
           hrfct=factor(hr), 
           locNamefct=factor(str_replace(locale, pattern=" \\(\\d{4}\\)", replacement=""))
           )
glimpse(metarData)

```

#### _Random Forest Classification (Select Locales for 2014-2019)_  
Models are run on all 2014-2019 data for Chicago, Las Vegas, New Orleans, and San Diego:  
```{r}

# Create the subset for Chicago, Las Vegas, New Orleans, San Diego
sub_2014_2019_data <- metarData %>%
    filter(str_sub(source, 1, 4) %in% c("kord", "klas", "kmsy", "ksan"), 
           year %in% c(2014, 2015, 2016, 2017, 2018, 2019)
           ) %>%
    mutate(city=str_replace(locale, pattern=" .\\d{4}.", replacement=""), 
           hr=lubridate::hour(dtime)
           )

# Check that proper locales are included
sub_2014_2019_data %>% 
    count(city, locale)

```
  
The random forest model is run and cached:  
```{r cache=TRUE}

# Run random forest for 2014-2019 data
rf_types_2014_2019_TDmcwha <- rfMultiLocale(sub_2014_2019_data, 
                                            vrbls=c("TempF", "DewF", 
                                                    "month", "hr",
                                                    "minHeight", "ceilingHeight", 
                                                    "WindSpeed", "predomDir", 
                                                    "modSLP"
                                                    ),
                                            locs=NULL, 
                                            locVar="city",
                                            pred="city",
                                            ntree=50, 
                                            seed=2006301420, 
                                            mtry=4
                                            )

```
  
```{r}

evalPredictions(rf_types_2014_2019_TDmcwha, 
                plotCaption = "Temp, Dew Point, Month, Hour of Day, Cloud Height, Wind, SLP", 
                keyVar="city"
                )

```
  
Even with a small forest (50 trees), the model is almost always separating Las Vegas, Chicago, San Diego, and New Orleans.  While the climates are very different in these cities, it is striking that the model has so few misclassifications.

How do other cities map against these classifications?  
```{r}

# Predictions on 2014-2019 data
helperPredictPlot(rf_types_2014_2019_TDmcwha$rfModel, 
                  df=filter(mutate(metarData, hr=lubridate::hour(dtime)), 
                            !(str_sub(source, 1, 4) %in% c("kord", "klas", "kmsy", "ksan"))
                            ), 
                  predOrder=c("Chicago, IL", "San Diego, CA", "New Orleans, LA", "Las Vegas, NV")
                  )

```
  
Classifications are broadly as expected based on climates by locale.  Variable importances are plotted:  
```{r}

helperPlotVarImp(rf_types_2014_2019_TDmcwha$rfModel)

```
  
Dew point and temperature are strong factors for separating the four cities in this analysis.  Month, SLP, minimum cloud height, and prevailing wind direction are also meaningful.
  
An assessment can be run for the 2014-2019 model:  
```{r}

# Run for the full model including SLP
probs_2014_2019_TDmcwha <- 
    assessPredictionCertainty(rf_types_2014_2019_TDmcwha, 
                              keyVar="city", 
                              plotCaption="Temp, Dew Point, Month/Hour, Clouds, Wind, SLP", 
                              showAcc=TRUE
                              )

```
  
* Predictions with 80%+ of the votes are made ~75% of the time, and these predictions are ~99% accurate  
* Predictions with <80% of the votes are made ~25% of the times, and these predictions are ~80% accurate  
* The percentage of votes received appears to be a reasonable proxy for the confidence of the prediction  
  
A similar process can be run for assessing the classification of the other cities against the 2014-2019 data for Chicago, Las Vegas, New Orleans, and San Diego:  
```{r}

useData <- metarData %>%
    filter(!(str_sub(source, 1, 4) %in% c("kord", "klas", "kmsy", "ksan"))) %>%
    mutate(hr=lubridate::hour(dtime))
    
# Run for the model excluding SLP
probs_allcities_2014_2019_TDmcwh <- 
    assessPredictionCertainty(rf_types_2014_2019_TDmcwha, 
                              testData=useData,
                              keyVar="locale", 
                              plotCaption="Temp, Dew Point, Month/Hour, Clouds, Wind, modSLP", 
                              showHists=TRUE
                              )

```
  
The model is frequently not so confident in assigning an archetype to related cities, though it frequently gets the most sensible assignment.

#### _Random Forest Classification (2016)_  
Next, an attempt is made to compare every grouping of two cities, using all variables, mtry of 4, and a very small forest of 15 trees:  
```{r cache=TRUE}

# Create a container list to hold the output
list_varimp_2016 <- vector("list", 0.5*length(names_2016)*(length(names_2016)-1))

# Set a random seed
set.seed(2007031342)

# Loop through all possible combinations
n <- 1
for (ctr in 1:(length(names_2016)-1)) {
    for (ctr2 in (ctr+1):length(names_2016)) {
        list_varimp_2016[[n]] <- rfTwoLocales(mutate(metarData, hr=lubridate::hour(dtime)), 
                                              loc1=names_2016[ctr], 
                                              loc2=names_2016[ctr2], 
                                              vrbls=c("TempF", "DewF", 
                                                      "month", "hr",
                                                      "minHeight", "ceilingHeight", 
                                                      "WindSpeed", "predomDir", 
                                                      "modSLP", "Altimeter"
                                                      ),
                                              ntree=15, 
                                              mtry=4
                                              )
        n <- n + 1
        if ((n %% 40) == 0) { cat("Through number:", n, "\n")}
    }
}

```
  
```{r}

# Create a tibble from the underlying accuracy data
acc_varimp_2016 <- map_dfr(list_varimp_2016, .f=helperAccuracyLocale)

# Assess the top 20 classification accuracies
acc_varimp_2016 %>%
    arrange(-accOverall) %>%
    head(20)

# Assess the bottom 20 classification accuracies
acc_varimp_2016 %>%
    arrange(accOverall) %>%
    head(20)

```
  
The best accuracies are obtained when comparing cities in very different climates (e.g., Denver vs. Humid/Marine or Miami vs. Desert/Cold), while the worst accuracies are obtained when comparing very similar cities (e.g., Chicago and Milwaukee or Newar and Philadelphia).

Variable importance can then be assessed across all 1:1 classifications:  
```{r cache=TRUE}

# Create a tibble of all the variable importance data
val_varimp_2016 <- map_dfr(list_varimp_2016, 
                           .f=function(x) { x$rfModel %>% 
                                   caret::varImp() %>% 
                                   t() %>% 
                                   as.data.frame()
                               }
                           ) %>% 
    tibble::as_tibble()

```
  
```{r}

# Create boxplot of overall variable importance
val_varimp_2016 %>% 
    mutate(num=1:nrow(val_varimp_2016)) %>% 
    pivot_longer(-num, names_to="variable", values_to="varImp") %>% 
    ggplot(aes(x=fct_reorder(variable, varImp), y=varImp)) + 
    geom_boxplot(fill="lightblue") + 
    labs(x="", 
         y="Variable Importance", 
         title="Variable Importance for 1:1 Random Forest Classifications"
         )

# Attach the city names and OOB error rate
tbl_varimp_2016 <- sapply(list_varimp_2016, 
                          FUN=function(x) { c(names(x$errorRate[2:3]), x$errorRate["OOB"]) }
                          ) %>%
    t() %>% 
    as.data.frame() %>% 
    bind_cols(val_varimp_2016) %>% 
    tibble::as_tibble() %>% 
    mutate(OOB=as.numeric(as.character(OOB))) %>%
    rename(locale1=V1, 
           locale2=V2
           )

# Plot accuracy vs. spikiness of variable importance
tbl_varimp_2016 %>%
    pivot_longer(-c(locale1, locale2, OOB), names_to="var", values_to="varImp") %>% 
    group_by(locale1, locale2, OOB) %>% 
    summarize(mean=mean(varImp), max=max(varImp)) %>% 
    mutate(maxMean=max/mean) %>%
    ggplot(aes(x=maxMean, y=1-OOB)) + 
    geom_point() + 
    geom_smooth(method="loess") +
    labs(x="Ratio of Maximum Variable Importance to Mean Variable Importance", 
         y="OOB Accuracy", 
         title="Accuracy vs. Spikiness of Variable Importance"
         )

```
  
Broadly speaking, the same variables that drive overall classification are important in driving 1:1 classifications.  There is meaningful spikiness, suggesting that different 1:1 classifications rely on different variables.

There is a strong trend where the best accuracies are obtained where there is a single spiky dimension that drives the classifications.  This suggests that while the model can take advantage of all 10 variables, it has the easiest tome when there is a single, well-differentiated variable.  No surprise.
  
#### _Random forest regression of temperatures in select locales for 2014-2019_  
Random forests can also be used to run regressions, such as on variables like temperature or dew point.  Models are run for the 2014-2019 data for the locales that have data availability (Chicago, IL; Las Vegas, NV; New Orleans, LA; San Diego, CA):  
```{r cache=TRUE}

# Create list of locations
fullDataLocs <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA")

# Create a main list, one per locale
lstFullData <- vector("list", length(fullDataLocs))


# Create a list of relevant dependent variables and variables to keep
depVarFull <- c('hrfct', 'DewF', 'modSLP', 'Altimeter', 'WindSpeed', 
                'predomDir', 'minHeight', 'ceilingHeight'
                )
keepVarFull <- c('source', 'dtime', 'locNamefct', 'year', 'month', 'hrfct', 'DewF', 'modSLP', 
                 'Altimeter', 'WindSpeed', 'predomDir', 'minHeight', 'ceilingHeight'
                 )


# Run the regressions by locale and month
nLoc <- 1
for (loc in fullDataLocs) {
    
    # Pull data for only this locale, and where TempF is not missing
    pullData <- metarData %>%
        filter(locNamefct==loc, !is.na(TempF))
    
    # Create the months to be run
    fullDataMonths <- pullData %>%
        count(month) %>%
        pull(month)
    
    # Create containers for each run
    lstFullData[[nLoc]] <- vector("list", length(fullDataMonths))
    
    # Run random forest regression for each month for the locale
    cat("\nBeginning to process:", loc)
    nMonth <- 1
    for (mon in fullDataMonths) {
        
        # Run the regression
        lstFullData[[nLoc]][[nMonth]] <- rfRegression(pullData, 
                                                      depVar="TempF", 
                                                      predVars=depVarFull, 
                                                      otherVar=keepVarFull,
                                                      critFilter=list(locNamefct=loc, month=mon), 
                                                      seed=2007271252, 
                                                      ntree=100, 
                                                      mtry=4, 
                                                      testSize=0.3
                                                      )
        
        # Increment the counter
        nMonth <- nMonth + 1
        cat("\nFinished month:", mon)
    }
    
    # Incerement the counter
    nLoc <- nLoc + 1
    
}

```
  
The relevant 'testData' files can then be combined for an assessment of overall prediction accuracy:  
```{r}

# Helper function to extract testData from inner list
combineTestData <- function(lst, elem="testData") {
    map_dfr(lst, .f=function(x) x[[elem]])
}

# Combine all of the test data files
fullTestData <- map_dfr(lstFullData, .f=combineTestData) %>%
    mutate(err=predicted-TempF, 
           year=factor(year)
           )

# Helper function to create RMSE data
helperCreateRMSE <- function(df, byVar, depVar, errVar="err") {
    
    df %>%
        group_by_at(vars(all_of(byVar))) %>%
        summarize(varTot=var(get(depVar)), varModel=mean(get(errVar)**2)) %>%
        mutate(rmseTot=varTot**0.5, rmseModel=varModel**0.5, rsq=1-varModel/varTot)
    
}

# Create plot for a given by-variable and facet-variable
helperRMSEPlot <- function(df, byVar, depVar, facetVar=NULL) {

    # Create a copy of the original by variable
    byVarOrig <- byVar
    
    # Expand byVar to include facetVar if facetVar is not null
    if (!is.null(facetVar)) {
        byVar <- unique(c(byVar, facetVar))
    }
    
    # Create 
    p1 <- df %>%
        helperCreateRMSE(byVar=byVar, depVar=depVar) %>%
        select_at(vars(all_of(c(byVar, "rmseTot", "rmseModel")))) %>%
        pivot_longer(c(rmseTot, rmseModel), names_to="model", values_to="rmse") %>%
        group_by_at(vars(all_of(byVar))) %>%
        mutate(dRMSE=ifelse(row_number()==n(), rmse, rmse-lead(rmse)), 
               model=factor(model, levels=c("rmseTot", "rmseModel"))
               ) %>%
        ggplot(aes_string(x=byVarOrig, y="dRMSE", fill="model")) + 
        geom_col() + 
        geom_text(data=~filter(., model=="rmseModel"), aes(y=dRMSE/2, label=round(dRMSE, 1))) +
        coord_flip() + 
        labs(x="", y="RMSE", title="RMSE before and after modelling") + 
        scale_fill_discrete("", 
                            breaks=c("rmseModel", "rmseTot"), 
                            labels=c("Final", "Explained by Model")
                            ) + 
        theme(legend.position="bottom")
    # Add facetting if the argument was passed
    if (!is.null(facetVar)) { p1 <- p1 + facet_wrap(as.formula(paste("~", facetVar))) }
    print(p1)
    
}

# Stand-alone on three main dimensions
helperRMSEPlot(fullTestData, byVar="locNamefct", depVar="TempF")
helperRMSEPlot(fullTestData, byVar="year", depVar="TempF")
helperRMSEPlot(fullTestData, byVar="month", depVar="TempF")

# Facetted by locale
helperRMSEPlot(fullTestData, byVar="year", depVar="TempF", facetVar="locNamefct")
helperRMSEPlot(fullTestData, byVar="month", depVar="TempF", facetVar="locNamefct")

```
  
Further, an overall decline in MSE can be estimated as the average of the MSE declines in each locale-month:  
```{r}

# Function to extract MSE data from inner lists
helperMSETibble <- function(x) { 
    map_dfr(x, .f=function(y) tibble::tibble(ntree=1:length(y$mse), mse=y$mse)) 
}

map_dfr(lstFullData, .f=function(x) { helperMSETibble(x) }, .id="source") %>%
    group_by(source, ntree) %>%
    summarize(meanmse=mean(mse)) %>%
    ungroup() %>%
    mutate(source=fullDataLocs[as.integer(source)]) %>%
    ggplot(aes(x=ntree, y=meanmse, group=source, color=source)) + 
    geom_line() + 
    ylim(c(0, NA)) + 
    labs(x="# Trees", y="MSE", title="Evolution of Average MSE by Number of Trees")

```
  
At 100 trees, the model appears to have largely completed learning, with no more material declines in MSE.  Overall, model predictions average 3-4 degrees different from actual temperatures.  Deviations are greater in Las Vegas (4-5 degrees), and in the spring in Chicago (4-5 degrees).  Deviations are lesser in San Diego (2-3 degrees) and winter in Chicago (2-3 degrees).
  
The model is then run for all months combined for a single locale, to compare results when month is a trained explanatory variable rather than a segment modelled separately:  
```{r cache=TRUE}

# Create list of locations
fullDataLocs <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA")

# Create a main list, one per locale
lstFullData_002 <- vector("list", length(fullDataLocs))


# Create a list of relevant dependent variables and variables to keep
depVarFull_002 <- c('month', 'hrfct', 'DewF', 'modSLP', 
                    'Altimeter', 'WindSpeed', 'predomDir', 
                    'minHeight', 'ceilingHeight'
                    )
keepVarFull_002 <- c('source', 'dtime', 'locNamefct', 'year', 'month', 'hrfct', 
                     'DewF', 'modSLP', 'Altimeter', 'WindSpeed', 'predomDir', 
                     'minHeight', 'ceilingHeight'
                     )


# Run the regressions by locale and month
nLoc <- 1
for (loc in fullDataLocs) {
    
    # Pull data for only this locale, and where TempF is not missing
    pullData <- metarData %>%
        filter(locNamefct==loc, !is.na(TempF))
    
    # To be parallel with previous runs, make a length-one list inside locale
    lstFullData_002[[nLoc]] <- vector("list", 1)
    
    # Run random forest regression for each locale
    cat("\nBeginning to process:", loc)
    lstFullData_002[[nLoc]][[1]] <- rfRegression(pullData, 
                                                 depVar="TempF", 
                                                 predVars=depVarFull_002, 
                                                 otherVar=keepVarFull_002,
                                                 critFilter=list(locNamefct=loc), 
                                                 seed=2007281307, 
                                                 ntree=25, 
                                                 mtry=4, 
                                                 testSize=0.3
                                                 )
    
    # Incerement the counter
    nLoc <- nLoc + 1
    
}

```
  
The results can then be compared to the results of the regressions run using month as a segment:  
```{r}

# Combine all of the test data files
fullTestData_002 <- map_dfr(lstFullData_002, .f=combineTestData) %>%
    mutate(err=predicted-TempF, 
           year=factor(year)
           )

# Stand-alone on three main dimensions
helperRMSEPlot(fullTestData_002, byVar="locNamefct", depVar="TempF")
helperRMSEPlot(fullTestData_002, byVar="year", depVar="TempF")
helperRMSEPlot(fullTestData_002, byVar="month", depVar="TempF")

# Facetted by locale
helperRMSEPlot(fullTestData_002, byVar="year", depVar="TempF", facetVar="locNamefct")
helperRMSEPlot(fullTestData_002, byVar="month", depVar="TempF", facetVar="locNamefct")

# Evolution of RMSE
map_dfr(lstFullData_002, .f=function(x) { helperMSETibble(x) }, .id="source") %>%
    group_by(source, ntree) %>%
    summarize(meanmse=mean(mse)) %>%
    ungroup() %>%
    mutate(source=fullDataLocs[as.integer(source)]) %>%
    ggplot(aes(x=ntree, y=meanmse, group=source, color=source)) + 
    geom_line() + 
    ylim(c(0, NA)) + 
    labs(x="# Trees", y="MSE", title="Evolution of Average MSE by Number of Trees")

```
  
The prediction qualities and evolution of MSE by number of trees look broadly similar to the results run by locale-month.  Notably, month scores high on variable importance:  
```{r}

impList <- lapply(lstFullData_002, FUN=function(x) { 
    locName <- x[[1]]$testData$locNamefct %>% as.character() %>% unique()
    x[[1]]$rfModel$importance %>% 
        as.data.frame() %>%
        rownames_to_column("variable") %>%
        rename_at(vars(all_of("IncNodePurity")), ~locName) %>%
        tibble::as_tibble()
    }
    )

impDF <- Reduce(function(x, y) merge(x, y, all=TRUE), impList)

# Overall variable importance
impDF %>%
    pivot_longer(-variable, names_to="locale", values_to="incPurity") %>%
    ggplot(aes(x=fct_reorder(varMapper[variable], incPurity), y=incPurity)) + 
    geom_col() + 
    coord_flip() + 
    facet_wrap(~locale) + 
    labs(x="", y="Importance", title="Variable Importance by Locale")

# Relative variable importance
impDF %>%
    pivot_longer(-variable, names_to="locale", values_to="incPurity") %>%
    group_by(locale) %>%
    mutate(incPurity=incPurity/sum(incPurity)) %>%
    ggplot(aes(x=fct_reorder(varMapper[variable], incPurity), y=incPurity)) + 
    geom_col() + 
    coord_flip() + 
    facet_wrap(~locale) + 
    labs(x="", y="Relative Importance", title="Relative Variable Importance by Locale")

```
  
There is much more underlying variance in the Chicago data, thus greater overall variable importance in Chicago.  On a relative basis, locale predictions are driven by:  
  
* Chicago - Dew Point, Month  
* Las Vegas - Month, Sea-Level Pressure, Hour, Altimeter  
* New Orleans - Dew Point, Month  
* San Diego - Month, Hour, Dew Point  
  
It is interesting to see the similarities in Chicago and New Orleans, with both having strong explanatory power from the combination of dew point and month, despite meaningfully different climates.  As in previous analyses, Las Vegas and San Diego look different from each other and also different from Chicago/New Orleans.
  
#### _XGB regression of temperatures in select locales for 2014-2019_  
Next, the xgboost (extreme gradient boosting) algorithm is attempted on the METAR dataset.  The general recipe from [CRAN](https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html) is followed, which includes several processing steps:  
  
1.  Convert factor variable(s) to binary variables using one-hot-encoding without intercept  
2.  Capture the target output variable as a vector  
3.  Train the model using xgboost::xgboost (can handle regression or classification)  
4.  Check feature importances  
5.  Plot the evolution in training RMSE and R-squared  
6.  Assess accuracy on test dataset  
  
Next, a very basic xgb model is attempted for predicting temperature.  First, data are prepared:  
```{r}

# Take metarData and limit to 4 sources with 2014-2019 data
baseXGBData_big4 <- metarData %>%
    filter(locNamefct %in% c("Las Vegas, NV", "New Orleans, LA", "Chicago, IL", "San Diego, CA"), 
           !is.na(TempF)
           )

# Split in to test and train datasets
idxTrain_big4 <- sample(1:nrow(baseXGBData_big4), size=round(0.7*nrow(baseXGBData_big4)), replace=FALSE)
baseXGBTrain_big4 <- baseXGBData_big4[idxTrain_big4, ]
baseXGBTest_big4 <- baseXGBData_big4[-idxTrain_big4, ]

# Select only variables of interest
xgbTrainInput_big4 <- baseXGBTrain_big4 %>%
    select(TempF, 
           locNamefct, month, hrfct, 
           DewF, modSLP, Altimeter, WindSpeed, 
           predomDir, minHeight, ceilingHeight
           ) %>%
    mutate(locNamefct=fct_drop(locNamefct))

```
  
Then, the three modeling steps are run:  
```{r}

# Step 1: Convert to sparse matrix format using one-hot encoding with no intercept
xgbTrainSparse_big4 <- Matrix::sparse.model.matrix(TempF ~ . - 1, data=xgbTrainInput_big4)
str(xgbTrainSparse_big4)
xgbTrainSparse_big4[1:6, ]

# Step 2: Create the target output variable as a vector
xgbTrainOutput_big4 <- xgbTrainInput_big4$TempF
str(xgbTrainOutput_big4)

# Step 3: Train the model using xgboost::xgboost, as regression
xgbModel_big4 <- xgboost::xgboost(data=xgbTrainSparse_big4, 
                                  label=xgbTrainOutput_big4, 
                                  nrounds=200, 
                                  print_every_n=20, 
                                  objective="reg:squarederror"
                                  )

```
  
Then, the three assessment steps are run:  
```{r}

# Step 4: Assess feature importances
xgbImportance_big4 <- xgboost::xgb.importance(feature_names=colnames(xgbTrainSparse_big4), 
                                              model=xgbModel_big4
                                              )

xgbImportance_big4 %>% 
    column_to_rownames("Feature") %>% 
    round(3)

xgbImportance_big4 %>%
    ggplot(aes(x=fct_reorder(Feature, Gain), y=Gain)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=Gain+0.02, label=round(Gain, 3))) + 
    coord_flip() + 
    labs(x="", title="Gain by Variable for TempF modeling with xgboost")

# Step 5: Plot evolution in training data RMSE and R-squared
xgbModel_big4$evaluation_log %>%
    filter(iter %% 5 == 0) %>%
    ggplot(aes(x=iter, y=train_rmse)) + 
    geom_text(aes(label=round(train_rmse, 1)), size=3) + 
    labs(x="Number of iterations", y="Training Set RMSE", title="Evolution of RMSE on training data")

xgbModel_big4$evaluation_log %>%
    filter(iter %% 10 == 0) %>%
    mutate(overall_rmse=sd(baseXGBTrain_big4$TempF), rsq=1-train_rmse**2/overall_rmse**2) %>%
    ggplot(aes(x=iter, y=rsq)) + 
    geom_text(aes(y=rsq, label=round(rsq,3)), size=3) +
    labs(x="Number of iterations", y="Training Set R-squared", title="Evolution of R-squared on training data")

# Step 6: Assess accuracy on test dataset
xgbTestInput_big4 <- baseXGBTest_big4 %>%
    select(TempF, 
           locNamefct, month, hrfct, 
           DewF, modSLP, Altimeter, WindSpeed, 
           predomDir, minHeight, ceilingHeight
           ) %>%
    mutate(locNamefct=fct_drop(locNamefct))

xgbTestSparse_big4 <- Matrix::sparse.model.matrix(TempF ~ . - 1, data=xgbTestInput_big4)

xgbTest_big4 <- xgbTestInput_big4 %>%
    mutate(xgbPred=predict(xgbModel_big4, newdata=xgbTestSparse_big4), err=xgbPred-TempF)

xgbTest_big4 %>%
    group_by(locNamefct) %>%
    summarize(rmse_orig=sd(TempF), rmse_xgb=mean(err**2)**0.5) %>%
    mutate(rsq=1-rmse_xgb**2/rmse_orig**2)

xgbTest_big4 %>%
    group_by(month) %>%
    summarize(rmse_orig=sd(TempF), rmse_xgb=mean(err**2)**0.5) %>%
    mutate(rsq=1-rmse_xgb**2/rmse_orig**2)

xgbTest_big4 %>%
    group_by(TempF, rndPred=round(xgbPred)) %>%
    summarize(n=n()) %>%
    ggplot(aes(x=TempF, y=rndPred)) + 
    geom_point(aes(size=n), alpha=0.1) + 
    geom_smooth(aes(weight=n)) + 
    geom_abline(lty=2, color="red") + 
    labs(title="XGB predictions vs. actual on test dataset", y="Predicted Temperature", x="Actual Temperature")

```
  
At a glance, initial prediction results are encouraging.  The model runs very quickly and gets to a comparable RMSE/R-squared on test data as the random forest.  Tuning parameters or adding cross-validation could potentially improve the algorithm further.
  
An initial conversion to functional form is made, leveraging some of the code already available in rfMultiLocale() and rfTwoLocales():  
```{r}

# Helper function to create sparse matrix without intercept (keep all factor levels)
# Need to adapt to keep all levels of all factors such as caret::dummyVars
helperMakeSparse <- function(tbl, depVar, predVars) {
    
    # FUNCTION ARGUMENTS
    # tbl: the tibble or data frame to be converted
    # depVar: the dependent variable (not to be included in the sparse matrix)
    # predVars: the predictor variables to be converted to sparse format
    
    # Filter to include only predVars then make sprase matrix modelling object
    # Include all contrast levels for every factor variable and exclude the intercept
    tbl %>%
        select_at(vars(all_of(c(predVars)))) %>%
        Matrix::sparse.model.matrix(~ . -1, 
                                    data=., 
                                    contrasts.arg=lapply(.[, sapply(., is.factor)], contrasts, contrasts=FALSE)
                                    )
    
}


# Run xgb model with desired parameters
xgbRunModel <- function(tbl, 
                        depVar, 
                        predVars,
                        otherVars=c("source", "dtime"),
                        critFilter=vector("list", 0),
                        dropEmptyLevels=TRUE,
                        seed=NULL, 
                        nrounds=200, 
                        print_every_n=nrounds, 
                        testSize=0.3, 
                        xgbObjective="reg:squarederror",
                        funcRun=xgboost::xgboost,
                        calcErr=TRUE,
                        ...
                        ) {
    
    # FUNCTION ARGUMENTS:
    # tbl: the data frame or tibble
    # depVar: the dependent variable that will be predicted
    # predVars: explanatory variables for modeling
    # otherVars: other variables to be kept in a final testData file, but not used in modeling
    # critFilter: named list of format list(varName=c(varValues))
    #             will include only observations where get(varName) %in% varValues
    #             vector("list", 0) creates a length-zero list, which never runs in the for loop
    # dropEmptyLevels: boolean, whether to run fct_drop on all variables of class factor after critFilter
    # seed: the random seed (NULL means no seed)
    # nrounds: the maximum number of boosting iterations
    # print_every_n: how frequently to print the progress of training error/accuracy while fitting
    # testSize: the fractional portion of data that should be used as the test dataset
    # xgbObjective: the objective function for xgboost
    # funcRun: the function to run, passed as a function
    # calcErr: boolean, whether to create variable err as predicted-get(depVar)
    # ...: additional arguments to be passed directly to xgboost
    
    # Check that funcName is valid and get the relevant function
    valFuncs <- c("xgboost", "xgb.cv")
    funcName <- as.character(substitute(funcRun))
    if (!(funcName[length(funcName)] %in% valFuncs)) {
        cat("\nFunction is currently only prepared for:", valFuncs, "\n")
        stop("Please change passed argument or update function\n")
    }
    
    # Filter such that only matches to critFilter are included
    for (xNum in seq_len(length(critFilter))) {
        tbl <- tbl %>%
            filter_at(vars(all_of(names(critFilter)[xNum])), ~. %in% critFilter[[xNum]])
    }
    
    # Keep only the depVar, predVar, and otherVars
    tbl <- tbl %>%
        select_at(vars(all_of(c(depVar, predVars, otherVars))))
    
    # Drop empty levels from factors if requested
    if (dropEmptyLevels) {
        tbl <- tbl %>%
            mutate_if(is.factor, .funs=fct_drop)
    }
    
    # Create test-train split
    ttLists <- createTestTrain(tbl, testSize=testSize, seed=seed)
    
    # Set the seed if requested
    if (!is.null(seed)) { set.seed(seed) }
    
    # Pull the dependent variable
    yTrain <- ttLists$trainData[, depVar, drop=TRUE]
    
    # Convert the dependent variable to be integers 0 to (n-1) if xgbObjective is "multi:.*"
    if (str_detect(xgbObjective, pattern="^multi:")) {
        # Convert to factor if passed as anything else
        if (!is.factor(yTrain)) yTrain <- factor(yTrain)
        # Save the factor levels so they can be added back later
        yTrainLevels <- levels(yTrain)
        # Convert to numeric 0 to n-1
        yTrain <- as.integer(yTrain) - 1
    } else {
        yTrainLevels <- NULL
    }
    
    # Convert predictor variables to sparse matrix format keeping only modeling variables
    sparseTrain <- helperMakeSparse(ttLists$trainData, depVar=depVar, predVars=predVars)
    sparseTest <- helperMakeSparse(ttLists$testData, depVar=depVar, predVars=predVars)
    
    # Train model
    xgbModel <- funcRun(data=sparseTrain, 
                        label=yTrain, 
                        nrounds=nrounds, 
                        print_every_n=print_every_n, 
                        objective=xgbObjective, 
                        ...
                        )

    # Create a base testData file that is NULL (will be for xgb.cv) and predData file that is NULL
    testData <- NULL
    predData <- NULL
    
    # Extract testData and add predictions if the model passed is xgboost
    if (funcName[length(funcName)] %in% c("xgboost")) {
        if (xgbObjective=="multi:softprob") {
            predData <- matrix(data=predict(xgbModel, newdata=sparseTest), 
                               nrow=nrow(ttLists$testData), 
                               ncol=length(yTrainLevels), 
                               byrow=TRUE
                               )
            maxCol <- apply(predData, 1, FUN=which.max)
            testData <- ttLists$testData %>%
                mutate(predicted=yTrainLevels[maxCol], probPredicted=apply(predData, 1, FUN=max))
            predData <- predData %>%
                as_tibble() %>%
                purrr::set_names(yTrainLevels)
        } else {
            testData <- ttLists$testData %>%
                mutate(predicted=predict(xgbModel, newdata=sparseTest))
            if (calcErr) { 
                testData <- testData %>% mutate(err=predicted-get(depVar))
            }
        }
    }
    
    # Return list containing funcName, trained model, and testData
    list(funcName=funcName[length(funcName)], 
         xgbModel=xgbModel, 
         testData=testData, 
         predData=predData,
         yTrainLevels=yTrainLevels
         )
    
}

```
  
```{r cache=TRUE}

# Define key predictor variables for base XGB runs
baseXGBPreds <- c("locNamefct", "month", "hrfct", 
                  "DewF", "modSLP", "Altimeter", "WindSpeed", 
                  "predomDir", "minHeight", "ceilingHeight"
                  )

# Core multi-year cities
multiYearLocales <- c("Las Vegas, NV", "New Orleans, LA", "Chicago, IL", "San Diego, CA")

# Run the function shell
xgbInit <- xgbRunModel(filter(metarData, !is.na(TempF)), 
                       depVar="TempF", 
                       predVars=baseXGBPreds, 
                       otherVars=c("source", "dtime"), 
                       critFilter=list(locNamefct=multiYearLocales),
                       seed=2008011825,
                       nrounds=2000,
                       print_every_n=50
                       )

```
  
Functions can then be written to assess the quality of the predictions:  
```{r}

# Create and plot importance for XGB model
plotXGBImportance <- function(mdl, 
                              subList="xgbModel", 
                              featureStems=NULL, 
                              showMainPlot=TRUE, 
                              showStemPlot=!is.null(featureStems), 
                              stemMapper=NULL,
                              plotTitle="Gain by Variable for xgboost", 
                              plotSubtitle=NULL
                              ) {
    
    # FUNCTION ARGUMENTS:
    # mdl: the xgb.Booster model file, or a list containing the xgb.Booster model file
    # subList: if mdl is a list, attempt to pull out item named in subList
    # featureStems: aggregate features starting with this vector of stems, and plot sum of gain 
    #               (NULL means do not do this and just plot the gains "as is" )
    # showMainPlot: boolean, whether to create the full importance plot (just return importance data otherwise)
    # showStemPlot: boolean, whether to create the plot summed by stems
    # stemMapper: mapping file to convert stem variables to descriptive names (NULL means leave as-is)
    # plotTitle: title to be included on the importance plots
    # plotSubtitle: subtitle to be included on the importance plots (NULL means no subtitle)

    # Pull out the modeling data from the list if needed
    if (!("xgb.Booster" %in% class(mdl))) {
        mdl <- mdl[[subList]]
    }
    
    # Pull out the feature importances
    xgbImportance <- xgboost::xgb.importance(model=mdl)
    
    # Helper function to sum data to stem (called below if featureStems is not NULL)
    helperStemTotal <- function(pattern, baseData=xgbImportance) {
        baseData %>%
            filter(grepl(pattern=paste0("^", pattern), x=Feature)) %>%
            select_if(is.numeric) %>%
            colSums()
    }
    
    # Create sums by stem if requested
    if (!is.null(featureStems)) {
        stemTotals <- sapply(featureStems, FUN=function(x) { helperStemTotal(pattern=x) } ) %>%
            t() %>%
            as.data.frame() %>%
            rownames_to_column("Feature") %>%
            tibble::as_tibble()
    } else {
        stemTotals <- NULL
    }

    # Helper function to plot gain by Feature
    helperPlotGain <- function(df, title, subtitle, mapper=NULL, caption=NULL) {
        # Add descriptive name if mapper is passed
        if (!is.null(mapper)) df <- df %>% mutate(Feature=paste0(Feature, "\n", mapper[Feature]))
        p1 <- df %>%
            ggplot(aes(x=fct_reorder(Feature, Gain), y=Gain)) + 
            geom_col(fill="lightblue") + 
            geom_text(aes(y=Gain+0.02, label=round(Gain, 3))) + 
            coord_flip() + 
            labs(x="", title=title)
        if (!is.null(subtitle)) { p1 <- p1 + labs(subtitle=subtitle) }
        if (!is.null(caption)) { p1 <- p1 + labs(caption=caption) }
        print(p1)
    }
    
    # Create and display the plots if requested
    if (showMainPlot) helperPlotGain(xgbImportance, title=plotTitle, subtitle=plotSubtitle)
    if (showStemPlot) { 
        helperPlotGain(stemTotals, 
                       title=plotTitle, 
                       subtitle=plotSubtitle,
                       mapper=stemMapper,
                       caption="Factor variables summed by stem"
                       )
    }
    
    # Return a list containing the raw data and the stemmed data (can be NULL)
    list(importanceData=xgbImportance, stemData=stemTotals)
    
}

# Find and plot importances
xgbInit_importance <- plotXGBImportance(xgbInit, 
                                        featureStems=baseXGBPreds, 
                                        stemMapper = varMapper, 
                                        plotTitle="Gain by variable in xgboost", 
                                        plotSubtitle="Modeling Temperature (F) in 4 Locales 2014-2019"
                                        )


# Function to create and plot RMSE and R-squared evolution of training data
plotXGBTrainEvolution <- function(mdl, 
                                  fullSD,
                                  subList="xgbModel", 
                                  plotRMSE=TRUE,
                                  plotR2=TRUE, 
                                  plot_every=10
                                  ) {
    
    # FUNCTION ARGUMENTS:
    # mdl: the xgb.Booster model file, or a list containing the xgb.Booster model file
    # fullSD: the overall stansard deviation for the training variable
    #         if passed as numeric, use as-is
    #         if passed as character, try to extract sd of that variable from 'testData' in mdl
    # subList: if mdl is a list, attempt to pull out item named in subList
    # plotRMSE: boolean, whether to create the RMSE evolution plot
    # plotR2: boolean, whether to create the R2 evolution plot
    # plot_every: how often to plot the RMSE/R-squared data (e.g., 10 means print iter 10, 20, 30, etc.)

    # Create the full standard deviation from 'testData' if passed as character
    # Must be run before mdl is converted out of list format
    if (is.character(fullSD)) {
        fullSD <- mdl[["testData"]] %>% pull(fullSD) %>% sd()
    }
    
    # Pull out the modeling data from the list if needed
    if (!("xgb.Booster" %in% class(mdl))) {
        mdl <- mdl[[subList]]
    }
    
    # Extract the evaluation log and add an approximated R-squared
    rmseR2 <- mdl[["evaluation_log"]] %>%
        mutate(overall_rmse=fullSD, rsq=1-train_rmse**2/overall_rmse**2) %>%
        tibble::as_tibble()
    
    # Helper function to create requested plot(s)
    helperPlotEvolution <- function(df, yVar, rnd, desc, size=3, plot_every=1) {
        p1 <- df %>%
            filter((iter %% plot_every) == 0) %>%
            ggplot(aes_string(x="iter", y=yVar)) + 
            geom_text(aes(label=round(get(yVar), rnd)), size=size) + 
            labs(x="Number of iterations", 
                 y=paste0("Training Set ", desc), 
                 title=paste0("Evolution of ", desc, " on training data")
                 )
        print(p1)
    }
    
    # Create the RMSE and R-squared plots if requested
    if (plotRMSE) helperPlotEvolution(rmseR2, yVar="train_rmse", rnd=1, desc="RMSE", plot_every=plot_every)
    if (plotR2) helperPlotEvolution(rmseR2, yVar="rsq", rnd=3, desc="R-squared", plot_every=plot_every)
    
    # Return the full evolution data frame
    rmseR2
    
}

# Plot the evolution
xgbInitEvolution <- plotXGBTrainEvolution(xgbInit, fullSD="TempF", plot_every=50)


# Function to report on, and plot, prediction caliber on testData
plotXGBTestData <- function(mdl, 
                            depVar,
                            predVar="predicted",
                            subList="testData", 
                            reportOverall=TRUE,
                            reportBy=NULL, 
                            showPlot=TRUE
                            ) {
    
    # FUNCTION ARGUMENTS:
    # mdl: the test data file, or a list containing the test data file
    # depVar: the variable that was predicted
    # predVar: the variable containing the prediction for depVar
    # subList: if mdl is a list, attempt to pull out item named in subList
    # reportOverall: boolean, whether to report an overall RMSE/R2 on test data
    # reportBy: variable for sumarizing RMSE/R2 by (NULL means no RMSE/R2 by any grouping variables)
    # showPlot: boolean, whether to create/show the plot of predictions vs actuals
    
    # Pull out the modeling data from the list if needed
    if ("list" %in% class(mdl)) {
        mdl <- mdl[[subList]]
    }
    
    # Helper function to print RMSE and R2
    helperReportRMSER2 <- function(df, depVar, errVar) {
        df %>%
            summarize(rmse_orig=sd(get(depVar)), 
                      rmse_xgb=mean((get(predVar)-get(depVar))**2)**0.5
                      ) %>%
            mutate(rsq=1-rmse_xgb**2/rmse_orig**2) %>%
            print()
        cat("\n")
    }
    
    # Report overall RMSE/R2 if requested
    if (reportOverall) {
        cat("\nOVERALL PREDICTIVE PERFORMANCE:\n\n")
        helperReportRMSER2(mdl, depVar=depVar, errVar=errVar)
        cat("\n")
    }
    
    # Report by grouping variables if any
    if (!is.null(reportBy)) {
        cat("\nPREDICTIVE PERFORMANCE BY GROUP(S):\n\n")
        sapply(reportBy, FUN=function(x) { 
            mdl %>% group_by_at(x) %>% helperReportRMSER2(depVar=depVar, errVar=errVar)
        }
        )
        cat("\n")
    }

    # Show overall model performance using rounded temperature and predictions
    if (showPlot) {
        p1 <- mdl %>%
            mutate(rndPred=round(get(predVar))) %>%
            group_by_at(vars(all_of(c(depVar, "rndPred")))) %>%
            summarize(n=n()) %>%
            ggplot(aes_string(x=depVar, y="rndPred")) + 
            geom_point(aes(size=n), alpha=0.1) + 
            geom_smooth(aes(weight=n)) + 
            geom_abline(lty=2, color="red") + 
            labs(title="XGB predictions vs. actual on test dataset", y="Predicted", x="Actual")
        print(p1)
    }
    
}

# Assess performance on test data
plotXGBTestData(xgbInit, depVar="TempF", reportBy=c("locNamefct", "month"))

```
  
The XGB model appears to be over-fitting the training data, as reflected by the RMSE gap between the test data set and the training data set:  
```{r}

# Get the random forest by locale RMSE reported by the training process
rfByLocaleRMSE <- sapply(lstFullData_002, FUN=function(x) { x[[1]][["rfModel"]][["mse"]] }) %>%
    as.data.frame() %>%
    tibble::as_tibble() %>%
    mutate(train_rmse=apply(., 1, FUN=mean)**0.5)

# Get the random forest RMSE reported by the test data
rfTestRMSE <- fullTestData_002 %>% 
    summarize(rfTestrmse=mean(err**2)**0.5) %>% 
    pull(rfTestrmse)

# Get the XGB RMSE reported by the training process
xgbInitRMSE <- xgbInit$xgbModel$evaluation_log

# Get the XGB RMSE reported by the test data
xgbTestRMSE <- xgbInit$testData %>%
    summarize(xgbTestrmse=mean(err**2)**0.5) %>%
    pull(xgbTestrmse)

# Plot differences in train/test RMSE
tibble::tibble(model=c("Random Forest", "XGB"), 
               train=c(tail(rfByLocaleRMSE$train_rmse, 1), tail(xgbInitRMSE$train_rmse, 1)), 
               test=c(rfTestRMSE, xgbTestRMSE)
               ) %>%
    pivot_longer(-model, names_to="type", values_to="RMSE") %>%
    mutate(type=factor(type, levels=c("train", "test"))) %>%
    ggplot(aes(x=model, y=RMSE, fill=type)) + 
    geom_col(position="dodge") + 
    geom_text(aes(y=RMSE+0.2, label=round(RMSE, 1)), position=position_dodge(width=1)) +
    labs(x="", title="XGB test RMSE is meaningfully higher than XGB train RMSE (overfitting)") + 
    scale_fill_discrete("") + 
    theme(legend.position="bottom")

```
  
So, the XGB model either needs to be tuned, or it needs to run for a much smaller number of iterations.  The function xgb.cv allows for cross-validation during the training process, which means that a running count of train/test RMSE is kept by iteration.  Suppose that this function is run:  
```{r cache=TRUE}

# Create the same training dataset as would have been passed to the previous XGB modeling
xgbInput_big4 <- metarData %>%
    filter(!is.na(TempF), locNamefct %in% multiYearLocales) %>%
    anti_join(xgbInit$testData %>% select(source, dtime)) %>%
    select_at(vars(all_of(c("TempF", baseXGBPreds, "source", "dtime")))) %>%
    mutate_if(is.factor, .funs=fct_drop)


# Step 1: Convert to sparse matrix format using one-hot encoding with no intercept
xgbTrain_big4 <- helperMakeSparse(xgbInput_big4, predVars=baseXGBPreds)

# Step 2: Create the target output variable as a vector
xgbTarget_big4 <- xgbInput_big4$TempF

# Step 3: Train the model using xgboost::xgb.cv, as regression
xgbInit_cv <- xgboost::xgb.cv(data=xgbTrain_big4, 
                              label=xgbTarget_big4, 
                              nrounds=2000, 
                              nfold=5,
                              print_every_n=50, 
                              objective="reg:squarederror"
                              )

```
  
The evolution of RMSE can then be plotted:  
```{r}

xgbInit_cv$evaluation_log %>%
    select(iter, train=train_rmse_mean, test=test_rmse_mean) %>%
    pivot_longer(-iter, names_to="type", values_to="rmse") %>%
    filter(iter > 10) %>%
    ggplot(aes(x=iter, y=rmse, color=type, group=type)) + 
    geom_line() + 
    scale_color_discrete("") +
    labs(x="# Iterations", 
         y="RMSE of Temperature (F)", 
         title="XGB Model RMSE when trained using Cross Validation (5-fold)", 
         subtitle="plot excludes first 10 iterations"
         )

```
  
There is a significant divergence between train RMSE and test RMSE, with test RMSE mostly being optimized already after 500-1000 iterations, while train RMSE continues to improve almost linearly even after 1000 iterations.  Unlike random forests, which minimize variance through bootstrap resampling and variable shuffling at each split, the XGB model continues to tune on residuals in the same dataset even after it has learned every pattern that can be generalized to unseen data.  Proper use of CV and early-stopping will be important.

Notably, XGB has achieved roughly 3.0 degrees RMSE with the test data while random forest achieved roughly 3.5 degrees with the test data.  In part this is driven by the very fast optimization of XGB allowing for many more iterations than can be run in the same time for a random forest.

Potential next steps are to incorporate xgb.cv as an option in the training function, and to explore hyperparameters such as eta (learning rate) and maximum depth, and their impacts on the test set RMSE.

Function xgbRunModel has been updated to accept either xgboost::xgboost or xgboost::xgb.cv.  This allows for running several versions of the hyperparameters through the CV process.  For example:  
```{r cache=TRUE}

# Create hyperparameter space
hypGrid <- expand.grid(eta=c(0.1, 0.3, 0.6), 
                       max_depth=c(3, 6, 10), 
                       nrounds=500, 
                       nfold=5
                       )

# Create containers for results
xgbSmall <- vector("list", nrow(hypGrid))

# Run xgb.cv once for each combination of hyper-parameters
for (rowNum in 1:nrow(hypGrid)) {

    # Extract the relevant hyperparameter data row
    params <- hypGrid[rowNum, ] %>% unlist()
    
    # Run the function for 5-fold CV for a few values of eta and max_depth
    xgbTemp <- xgbRunModel(filter(metarData, !is.na(TempF)), 
                           depVar="TempF", 
                           predVars=baseXGBPreds, 
                           otherVars=c("source", "dtime"), 
                           critFilter=list(locNamefct=multiYearLocales),
                           seed=2008041345,
                           nrounds=params["nrounds"],
                           eta=params["eta"], 
                           max_depth=params["max_depth"],
                           print_every_n=50, 
                           funcRun=xgboost::xgb.cv, 
                           nfold=params["nfold"]
                           )
    
    xgbSmall[[rowNum]] <- list(params=params, results=xgbTemp)
    
}

```
  
The test and train RMSE by iteration can be extracted and plotted by combination of parameters:  
```{r}

# Extract the key parameters as a character vector
allParams <- sapply(xgbSmall, FUN=function(x) x[["params"]])
keyParamVec <- allParams %>% 
    apply(2, FUN=function(x) paste0(names(x)[1], ": ", x[1], ", ", names(x)[2], ": ", x[2]))

# Extract RMSE and attach character vector
allRMSE <- map_dfr(xgbSmall, 
                   .f=function(x) x[["results"]][["xgbModel"]]$evaluation_log, 
                   .id="source"
                   ) %>%
    mutate(desc=factor(keyParamVec[as.numeric(source)], levels=keyParamVec)) %>%
    tibble::as_tibble()

# Get the minimum test RMSE
minTestRMSE <- allRMSE %>% 
    select(test_rmse_mean) %>% 
    min()

# Create plot for final test RMSE
allRMSE %>%
    select(desc, iter, train=train_rmse_mean, test=test_rmse_mean) %>%
    filter(iter==max(iter)) %>%
    ggplot(aes(x=desc, y=test)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=test/2, label=round(test, 1))) + 
    labs(x="", y="Test RMSE after 500 iterations", title="Test RMSE at 500 iterations by hyperparameters") +
    coord_flip()

# Create plot for RMSE evolution
allRMSE %>%
    select(desc, iter, train=train_rmse_mean, test=test_rmse_mean) %>%
    pivot_longer(-c(desc, iter), names_to="type", values_to="RMSE") %>%
    filter(RMSE <= 8) %>%
    ggplot(aes(x=iter, y=RMSE, group=type, color=type)) + 
    geom_line() + 
    labs(x="# Iterations", title="RMSE Evolution by Hyperparameters", subtitle="Only RMSE <= 8 plotted") +
    geom_hline(aes(yintercept=minTestRMSE), color="red", lty=2) +
    geom_vline(aes(xintercept=10), lty=2) +
    facet_wrap(~desc)

```
  
As expected, increasing eta and max_depth have a tendency to induce over-fitting while also driving to the optimal RMSE quicker.  A trade-off.  The default eta=0.3 and max_depth=6 appear to be close to the minimum RMSE at 500 observations with a rather modest delta between test/train RMSE.  The high over-fit model (eta 0.6 and max_depth 10) appears to have converged with a high test RMSE.  The slowest model (eta 0.1, max_depth 3) appears to still have significant room to learn even after 500 observations.  

The top performing models at 500 iterations appear to be blends of parameters that average out to "moderate-high" learning ability (low eta=0.1 with high max_depth=10, medium eta=0.3 with medium max_depth=6, medium eta=0.3 with high max_depth=10, high eta=0.6 with medium max_depth=6).

Suppose the slowest model is run for 2500 iterations to check on convergence:  
```{r cache=TRUE}

# Define key predictor variables for base XGB runs
baseXGBPreds <- c("locNamefct", "month", "hrfct", 
                  "DewF", "modSLP", "Altimeter", "WindSpeed", 
                  "predomDir", "minHeight", "ceilingHeight"
                  )

# Core multi-year cities
multiYearLocales <- c("Las Vegas, NV", "New Orleans, LA", "Chicago, IL", "San Diego, CA")

# Run the function shell
xgbSlow <- xgbRunModel(filter(metarData, !is.na(TempF)), 
                       depVar="TempF", 
                       predVars=baseXGBPreds, 
                       otherVars=c("source", "dtime"), 
                       critFilter=list(locNamefct=multiYearLocales),
                       seed=2008041432,
                       nrounds=2500,
                       print_every_n=50, 
                       eta=0.1, 
                       max_depth=3
                       )

```
  
The evalutaion functions can then be run:  
```{r}

# Plot the evolution
xgbSlowEvolution <- plotXGBTrainEvolution(xgbSlow, fullSD="TempF", plot_every=100)

# Assess performance on test data
plotXGBTestData(xgbSlow, depVar="TempF", reportBy=c("locNamefct", "month"))

```
  
Even at 2500 iterations, the model has almost no over-fitting (train RMSE 3.5, test RMSE 3.5).  However, the model appears to be converging at several tenths of a degree higher test RMSE than some of the other models achieve in as little as 500 iterations.
  
#### _XGB classification of locales for 2016_  
The XGB approach can also be followed for classifications, by passing a factor variable for y and updating the objective function and metric to be appropriate for classification.  The general recipe is similar to what is followed for regression.
  
To begin, a simple classification will be run using 2016 data to assess "is this data from Las Vegas?".  Las Vegas is selected because it is largely distinct from the other climate types.  For further simplification of the first pass, Phoenix data will be deleted, and the "all other" cities will be reduced to being the same data volume as Las Vegas:  
```{r}

set.seed(2008051312)

# Extract a balanced sample of Las Vegas 2016 data and all-other 2016 data (excluding Phoenix)
las2016Data <- metarData %>%
    filter(year==2016, !is.na(TempF), !(locNamefct %in% c("Phoenix, AZ"))) %>%
    mutate(isLAS=factor(ifelse(locNamefct=="Las Vegas, NV", "Las Vegas", "All Other"), 
                        levels=c("Las Vegas", "All Other")
                        ), 
           nLAS=sum(isLAS=="Las Vegas")
           ) %>%
    group_by(isLAS) %>%
    sample_n(min(nLAS)) %>% # not ideal coding, but it works
    ungroup()

```
  
First, the random forest approach is run on the Las Vegas 2016 data:  
```{r}

# Define key predictor variables for base XGB runs
locXGBPreds <- c("month", "hrfct", 
                 "TempF", "DewF", "modSLP", "Altimeter", "WindSpeed", 
                 "predomDir", "minHeight", "ceilingHeight"
                 )

# Run random forest for Las Vegas 2016 classifications
rf_las2016 <- rfMultiLocale(las2016Data, 
                            vrbls=locXGBPreds,
                            otherVar=keepVarFull,
                            locs=NULL, 
                            locVar="isLAS",
                            pred="isLAS",
                            ntree=100, 
                            seed=2008051316, 
                            mtry=4
                            )

```
  
The variable importances and quality of the predictions can then be assessed:  
```{r}

# Plot variable importances
helperPlotVarImp(rf_las2016$rfModel)

# Evaluate prediction accuracy
evalPredictions(rf_las2016, 
                plotCaption = "Temp, Dew Point, Month, Hour of Day, Cloud Height, Wind, SLP, Altimeter", 
                keyVar="isLAS"
                )

```
  
Using majority-rules classification, the model achieves balanced 96% accuracy in classifying locales as being Las Vegas or All Other.  The main variables that assist in the classification are dew point, minimum cloud height, and temperature.  This seems plausible as Las Vegas is rarely cloudy (and almost never at low levels) and shows a significantly different density on a dewpoint/temperature plot than most other locales:  
```{r}

plotOrder <- rev(levels(rf_las2016$testData$minHeight))

p1_las2016 <- rf_las2016$testData %>%
    ggplot(aes(x=isLAS, fill=factor(minHeight, levels=plotOrder))) + 
    geom_bar(position="fill") + 
    coord_flip() + 
    labs(x="", y="Percentage of observations", title="Minimum cloud heights") + 
    scale_fill_discrete("Min Cloud Height", guide=guide_legend(reverse=TRUE, nrow=2, byrow=TRUE), 
                        labels=plotOrder, breaks=plotOrder
                        ) + 
    theme(legend.position="bottom")

p2_las2016 <- rf_las2016$testData %>%
    ggplot(aes(x=TempF, y=DewF)) + 
    geom_point(alpha=0.1) + 
    labs(x="Temperature (F)", y="Dew Point (F)", title="Temp/Dew Point Distribution") + 
    facet_wrap(~isLAS)

gridExtra::grid.arrange(p1_las2016, p2_las2016, nrow=1)

```
  
The modesl has 80%+ votes in one direction or ther other about 80% of the time.  These classifications are ~99% accurate.  When the model is less confident, the prediction quality declines to the 70%-80% range:  
```{r}

# Evaluate prediction certainty
probs_las2016 <- 
    assessPredictionCertainty(rf_las2016, 
                              keyVar="isLAS", 
                              plotCaption="Temp, Dew Point, Month/Hour, Clouds, Wind, SLP, Altimeter", 
                              showAcc=TRUE
                              )

```
  
The process is then run using xgb, with the following modifications:  
  
1. Convert the isLAS variable to 0/1  
2. Update the objective function to be logistic  
  
First, the data are reshaped and the CV process is run to see where the test error stabilizes:  
```{r}

las2016Data <- las2016Data %>%
    mutate(binLAS=ifelse(isLAS=="Las Vegas", 1, 0))

```
  
```{r cache=TRUE}

# Run the function shell
xgb_las2016_cv <- xgbRunModel(las2016Data, 
                              depVar="binLAS", 
                              predVars=locXGBPreds, 
                              otherVars=keepVarFull, 
                              seed=2008051405,
                              nrounds=1000,
                              print_every_n=50, 
                              xgbObjective="binary:logistic", 
                              funcRun=xgboost::xgb.cv, 
                              nfold=5
                              )

```
  
Error rate evolution can be plotted:  
```{r}

# Calculate minimum test error
minTestError <- min(xgb_las2016_cv$xgbModel$evaluation_log$test_error_mean)

# Create plot for RMSE evolution
xgb_las2016_cv$xgbModel$evaluation_log %>%
    select(-contains("std")) %>%
    select(iter, train=train_error_mean, test=test_error_mean) %>%
    pivot_longer(-c(iter), names_to="type", values_to="RMSE") %>%
    ggplot(aes(x=iter, y=RMSE, group=type, color=type)) + 
    geom_line() + 
    geom_hline(aes(yintercept=minTestError), color="red", lty=2) +
    labs(x="# Iterations", title="RMSE Evolution")

```
  
Test error appears to be minimized in the 250-500 iterations range, while train error has been driven to zero within the first few hundred iterations.  A full XGB process is run using 500 iterations:  
```{r}

# Run the function shell
xgb_las2016 <- xgbRunModel(las2016Data, 
                           depVar="binLAS", 
                           predVars=locXGBPreds, 
                           otherVars=keepVarFull, 
                           seed=2008051405,
                           nrounds=500,
                           print_every_n=25, 
                           xgbObjective="binary:logistic"
                           )

```
  
Accuracy of the classifications can then be assessed by investigating the "predicted" column of the testData frame:  
```{r}

# Overall classification probability histogram
xgb_las2016$testData %>%
    ggplot(aes(x=predicted)) + 
    geom_histogram(fill="lightblue") + 
    labs(x="Predicted Probability of Las Vegas", y="", 
         title="Prediction Probabilities - Las Vegas vs. Other"
         )

# Overall accuracy by bin
xgb_las2016$testData %>%
    mutate(binPred=ifelse(predicted>=0.5, 1, 0)) %>%
    group_by(binLAS) %>%
    summarize(accuracy=mean(binLAS==binPred))

# Accuracy by predicted probability (whether certainty is at least 80%)
xgb_las2016$testData %>%
    mutate(binPred=ifelse(predicted>=0.5, 1, 0), 
           confPred=ifelse(abs(predicted-0.5)>=0.3, "High", "Low")
           ) %>%
    group_by(confPred, binLAS) %>%
    summarize(accuracy=mean(binLAS==binPred), n=n())

```
  
Overall accuracy is in the 97% range, roughly 1% higher than is achieved using the random forest algorithm.  The XGB model is slightly more likely to make a high-confidence error in classifying a non-Las Vegas locale as Las Vegas, and much less likely to have a low confidence in its predictions.

Variable importance can also be assessed:  
```{r}

# Find and plot importances
xgb_las2016_importance <- plotXGBImportance(xgb_las2016, 
                                            featureStems=locXGBPreds, 
                                            stemMapper = varMapper, 
                                            plotTitle="Gain by variable in xgboost", 
                                            plotSubtitle="Modeling 2016 Locale (Las Vegas vs, Other)"
                                            )

```
  
At a glance, the XGB algorithm makes high use of dew point and temperature, consistent with the random forest algorithm.  In contrast, the XGB algorithm prefers to use sea-level pressure next while the random forest model prefers to use minimum cloud height next.

Lastly, predictions are plotted against actual values:  
```{r}

xgb_las2016$testData %>%
    mutate(rndPred=round(2*predicted, 1)/2) %>%
    group_by(rndPred) %>%
    summarize(n=n(), meanPred=mean(binLAS)) %>%
    ggplot(aes(x=rndPred, y=meanPred)) + 
    geom_point(aes(size=n)) + 
    geom_abline(lty=2, color="red") + 
    geom_text(aes(y=meanPred+0.1, label=paste0((round(meanPred, 2)), "\n(n=", n, ")")), size=3) + 
    labs(x="Predicted Probability Las Vegas", y="Actual Proportion Las Vegas")

```
  
Broadly, there is a strong association between the predicted probabilities and the actual proportions.  As noted previously, almost all of the predictions are correctly made with high confidence.

Next steps are to explore some trickier single-locale classifications and then to explore multi-locale classifications.
  
Suppose that every locale is run through the process of self vs. other, with other under-sampled to be the same size as self, using 500 rounds:  
```{r}

# Function to run one vs all using XGB
helperXGBOnevAll <- function(df, 
                             keyLoc, 
                             critFilter=vector("list", 0),
                             underSample=TRUE,
                             predVars=locXGBPreds, 
                             otherVars=keepVarFull, 
                             seed=NULL, 
                             nrounds=500, 
                             print_every_n=100, 
                             xgbObjective="binary:logistic", 
                             ...
                             ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble
    # keyLoc: the value of locNamefct to use as the 'one' for 'one' vs. all-other
    # critFilter: named list, of format name=values, where filtering will be (get(name) %in% values)
    # underSample: boolean, if TRUE take 'all other' and randomly under-sample to be the same size as keyLoc
    # predVars: explanatory variables for modeling
    # otherVars: other variables to be kept, but not used in modeling
    # seed: the random seed (NULL means no seed)
    # nrounds: the number of XGB training rounds
    # print_every_n: the frequency of printing the XGB training error
    # xgbObjective: the objective function to be used in XGB modeling
    # ...: any other arguments to be passed (eventually) to xgboost::xgboost
    
    # Set the seed if it has been passed (drives consistency of under-sampling)
    if (!is.null(seed)) set.seed(seed)
    
    # Generate the descriptive name for keyLoc
    descName <- str_replace(keyLoc, pattern=", \\w{2}$", replacement="")
    
    # Announce the locale being run
    cat("\n\n **************")
    cat("\nRunning for", keyLoc, "with decription", descName, "\n")
    
    # Filter for non-NA data across all of 'locNamefct', predVars, otherVars, names(critFilter) are included
    subDF <- df %>%
        filter_at(vars(all_of(c("locNamefct", predVars, otherVars, names(critFilter)))), 
                  all_vars(!is.na(.))
                  )
    
    # Filter such that only matches to critFilter are included
    for (xNum in seq_len(length(critFilter))) {
        subDF <- subDF %>%
            filter_at(vars(all_of(names(critFilter)[xNum])), ~. %in% critFilter[[xNum]])
    }

    # Create the variables needed for modeling
    subDF <- subDF %>%
        mutate(isKEY=factor(ifelse(locNamefct==keyLoc, descName, "All Other"), 
                            levels=c(descName, "All Other")
                            ), 
               nKEY=sum(isKEY==descName), 
               binKEY=ifelse(isKEY==descName, 1, 0)
               )
    
    # Extract a balanced sample of data matching keyLoc and all-other data, if underSample==TRUE
    if (isTRUE(underSample)) {
        subDF <- subDF %>%
            group_by(isKEY) %>%
            sample_n(min(nKEY)) %>% # not ideal coding, but it works
            ungroup()
    }

    # Run the function shell
    outData <- xgbRunModel(subDF, 
                           depVar="binKEY", 
                           predVars=predVars, 
                           otherVars=otherVars, 
                           seed=seed,
                           nrounds=nrounds,
                           print_every_n=print_every_n, 
                           xgbObjective=xgbObjective, 
                           ...
                           )
    
    # Calculate overall accuracy
    accOverall <- outData$testData %>%
        mutate(correct=round(predicted)==binKEY) %>%
        pull(correct) %>%
        mean()
    
    # Report on finishing
    cat("Finished processing", keyLoc, "with overall test set accuracy", round(accOverall, 3), "\n")
    
    # Return the list
    outData
    
}

```
  
The function is then run for Las Vegas, including all 2016 cities:  
```{r}

xgb_las2016_check <- helperXGBOnevAll(metarData, 
                                      keyLoc="Las Vegas, NV", 
                                      critFilter=list(year=2016), 
                                      seed=2008061322
                                      )

# Report accuracy without Phoenix
xgb_las2016_check$testData %>%
    filter(locNamefct != "Phoenix, AZ") %>%
    mutate(correct=round(predicted)==binKEY) %>%
    pull(correct) %>%
    mean() %>%
    round(3)

```
  
Overall test set accuracy as reported is 96.3%.  Excluding Phoenix, which will often classify as Las Vegas and was excluded in the previous run, accuracy is roughly 97% as before.

Suppose that every locale is run, with the objective of finding cities that are most and least unique:  
```{r cache=TRUE}

# Extract the locales that are available in 2016
locs2016 <- metarData %>%
    filter(year==2016) %>%
    pull(locNamefct) %>%
    as.character() %>%
    unique() %>%
    sort()

# Create a container to hold each of the results
xgbOnevAll <- vector("list", length=length(locs2016))
names(xgbOnevAll) <- locs2016

# Run through the XGB process
n <- 1
for (loc in locs2016) {

    # Run and save the XGB model
    xgbOnevAll[[n]] <- helperXGBOnevAll(metarData, 
                                        keyLoc=loc, 
                                        critFilter=list(year=2016), 
                                        seed=2008061340+n
                                        )
    
    # Index the counter
    n <- n + 1
    
}

```
  
Overall accuracy by locale can then be assessed:  
```{r}

# Function to calculate and extract overall accuracy from the XGB list
helperXGBListAccuracyOverall <- function(lst) {
    
    lst[["testData"]] %>%
        select(binKEY, predicted) %>%
        mutate(correct=binKEY==round(predicted)) %>%
        pull(correct) %>%
        mean()
    
}

# Extract overall accuracy by locale and format as a tibble
accList <- sapply(xgbOnevAll, FUN=helperXGBListAccuracyOverall) %>% 
    as.data.frame() %>% 
    rename("accOverall"=".") %>% 
    rownames_to_column("locale") %>% 
    tibble::as_tibble()

# Plot the overall accuracy by locale
ggplot(accList, aes(x=fct_reorder(locale, accOverall), y=accOverall)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=accOverall+0.02, label=round(accOverall, 3)), hjust=0) + 
    coord_flip() + 
    labs(x="", y="Overall Accuracy", title="Accuracy of Classifying Locale vs. All Other")

```
  
Interestingly, San Antonio stands out as the most distinct of the cities in the analysis.  Several of the locales used previously as distinct archetypes - New Orleans, Las Vegas, San Diego - also score reasonably high in this exercise.  As expected, the cold-weather cities are the most difficult to distinguish, with classification success in the 80% range (null success 50% given deliberate under-sampling of all-other).

Accuracy can also be assessed as "classifying self correctly" and "classifying all-other correctly":  
```{r}

# Function to calculate and extract overall accuracy from the XGB list
helperXGBListAccuracySubset <- function(lst) {
    
    lst[["testData"]] %>%
        select(binKEY, predicted) %>%
        mutate(correct=binKEY==round(predicted)) %>%
        group_by(binKEY) %>%
        summarize(acc=mean(correct))
    
}

# Extract accuracy by subset by locale
accListSubset <- map_dfr(xgbOnevAll, .f=helperXGBListAccuracySubset, .id="locale") %>% 
    mutate(type=factor(ifelse(binKEY==1, "Classifying Self", "Classifying All Other"), 
                       levels=c("Classifying Self", "Classifying All Other")
                       )
           )

# Plot the accuracies by locale, facetted by seld vs. all other
ggplot(accListSubset, aes(x=fct_reorder(locale, acc), y=acc)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=acc/2, label=round(acc, 3)), hjust=0) + 
    coord_flip() + 
    labs(x="", y="Accuracy", title="Accuracy of Classifying Locale vs. All Other") + 
    facet_wrap(~type)

```
  
Models are generally more successful in classifying self that in classifying all-other.  Put differently, the model is more likely to incorrectly flag an All Other locale as Self than it is to incorrectly flag Self as All Other.

Errors in classifying All Other as Self can then be explored further:  
```{r}

# Function to calculate and extract accuracy by locName fct from the XGB list
helperXGBListErrorByTrueLocation <- function(lst) {
    
    lst[["testData"]] %>%
        select(locNamefct, binKEY, predicted) %>%
        mutate(correct=binKEY==round(predicted)) %>%
        filter(binKEY==0) %>%
        group_by(locNamefct) %>%
        summarize(acc=mean(correct))
    
}

# Extract by locale
accListTrueLocation <- map_dfr(xgbOnevAll, .f=helperXGBListErrorByTrueLocation, .id="modelLocale")

# Plot accuracy
accFactors <- accListTrueLocation %>% 
    group_by(modelLocale) %>% 
    summarize(meanAcc=mean(acc)) %>%
    arrange(meanAcc) %>%
    pull(modelLocale)

accListTrueLocation %>%
    ggplot(aes(x=factor(modelLocale, levels=accFactors), 
               y=factor(locNamefct, levels=accFactors)
               )
           ) + 
    geom_tile(aes(fill=1-acc)) + 
    geom_text(aes(label=paste0(round(100*(1-acc)), "%")),size=3) + 
    scale_x_discrete(position="top") + 
    labs(x="Modeling Locale", 
         y="True Locale", 
         title="Rate of Misclassifying True Locale as Modeling Locale"
         ) + 
    theme(axis.text.x=element_text(angle=90)) + 
    scale_fill_continuous("Error Rate", low="white", high="red")

```
  
When Seattle and Denver are just a small part of the deliberately under-sampled All Other class, they are routinely misclassified as being part of many other locales.  When they are the full-sampled class, almost no other city is misclassified with any frequency as being them.  This suggests that Seattle and Denver may each be distinct, but in a rather nuanced manner that requires significant training data volumes to learn.

Next steps are to explore some of the more challenging one-vs-one classifications suggested by this grid, then to extend the analysis to explore the multi-class classification capabilities of XGB.

Locale pairs that are difficult to classify are identified based on the preceding analysis.  There are two types of "difficult to classify":  
  
1.  Generally difficult - A is often classified as B when the model is trained as B vs. all, while B is often classified as A when the model is trained as A vs. all  
2.  Directionally difficult - A is often classified as B when the model is trained as B vs. all, while B is often correctly classified as "not A" when the model is trained as A vs. all  
  
The file accListTrueLocation is merged to itself to identify these cases:  
```{r}

# Create the frame of mean error and mean difference in error for A vs all and B vs all
accSummary <- accListTrueLocation %>%
    mutate(x=factor(modelLocale), modelLocale=locNamefct, locNamefct=x) %>%
    select(modelLocale, locNamefct, acc1=acc) %>%
    inner_join(mutate(accListTrueLocation, modelLocale=factor(modelLocale))) %>%
    mutate(meanError=1-(acc1 + acc)/2, deltaError=abs(acc-acc1))

# The file contains two (pragmatically identical) records for each city pair - once as A/B, once as B/A
# Keep only the records where modelLocale is "smaller" than locNamefct
accSummaryUnique <- accSummary %>%
    filter(pmin(as.character(modelLocale), as.character(locNamefct))==as.character(modelLocale))

# Highest mean error
highMeanError <- accSummaryUnique %>%
    arrange(-meanError) %>%
    filter(meanError >= 0.5)
highMeanError
highMeanError %>% mutate(n=n()) %>% select_if(is.numeric) %>% summarize_all(mean)

# Highest delta error
highDeltaError <- accSummaryUnique %>%
    arrange(-deltaError) %>%
    filter(deltaError >= 0.2)
highDeltaError
highDeltaError %>% mutate(n=n()) %>% select_if(is.numeric) %>% summarize_all(mean)

```
  
There are 29 locale pairs where the average error rate is worse than a coin flip (mean error for these pairs in 60%).  There are 18 locale pairs where the directional error rate differs by at least 20%.

Each of these groupings is run through the prediction process individually, with the goal of seeing how the error rates evolve when the pairs are compared individually.  Function xgbOnevAll() is modified slightly so that there is no under-sampling if a parameter is passed as FALSE.  This is due to the one vs. one comparisons being of the same size, and an attempt to under-sample potentially causing errors due to very small differences in METAR data capture (bad data) by locale:  
```{r cache=TRUE}

# Create container for each pairing in highMeanError
# Name for modelLocale
highMeanErrorList <- vector("list", nrow(highMeanError))
names(highMeanErrorList) <- highMeanError$modelLocale

# Run model for each pairing in highMeanError
for (n in 1:nrow(highMeanError)) {
    
    # Extract the key locale and the other locale
    # Note that which locale is defined as key is arbitrary and unimportant since this is a full 1:1 comparison
    keyLoc <- as.character(highMeanError$modelLocale)[n]
    otherLoc <- as.character(highMeanError$locNamefct)[n]
    
    # Run XGB for 500 rounds using only two locales and 2016 data; do not under-sample 'all other'
    highMeanErrorList[[n]] <- helperXGBOnevAll(metarData, 
                                               keyLoc=keyLoc, 
                                               critFilter=list(year=2016, 
                                                               locNamefct=c(keyLoc, otherLoc)
                                                               ), 
                                               underSample=FALSE,
                                               seed=2008071346
                                               )
    
}

```
  
The error rates are then extracted (overall and by subtype:  
```{r}

# Helper function to extract one vs one accuracy
helperExtractOnevOneAccuracy <- function(lst) {
    
    # Create accuracy for each sub-group and overall
    rawSummary <- lst[["testData"]] %>%
        count(locNamefct, correct=round(predicted)==binKEY) %>%
        mutate(locNamefct=as.character(locNamefct)) %>%
        bind_rows(mutate(., locNamefct="Overall")) %>%
        group_by(locNamefct) %>%
        summarize(nTotal=sum(n), nCorrect=sum(n*(correct==TRUE))) %>%
        ungroup() %>%
        mutate(acc=nCorrect/nTotal)
    
    # Create one-row tibble with locA, locB, accA, accB, accOverall
    locs=rawSummary$locNamefct %>% setdiff("Overall")
    tibble::tibble(locA=min(locs), 
                   locB=max(locs), 
                   accA=rawSummary %>% filter(locNamefct==locA) %>% pull(acc),
                   accB=rawSummary %>% filter(locNamefct==locB) %>% pull(acc), 
                   accOverall=rawSummary %>% filter(locNamefct=="Overall") %>% pull(acc)
                   )
    
}

# Extract accuracy by subset by locale
accHighMeanError <- map_dfr(highMeanErrorList, .f=helperExtractOnevOneAccuracy)

# Combine with the original file, highMeanError
highMeanOutput <- highMeanError %>%
    mutate(locA=as.character(modelLocale), 
           locB=as.character(locNamefct), 
           original=1-meanError
           ) %>%
    select(locA, locB, original) %>%
    inner_join(accHighMeanError, by=c("locA", "locB")) %>%
    select(-accA, -accB, new=accOverall) %>%
    pivot_longer(-c(locA, locB), names_to="model", values_to="accuracy") %>%
    mutate(desc=paste0(locA, " vs. ", locB)) 

# Plot the accuracy data
highMeanOutput %>%
    ggplot(aes(x=fct_reorder(desc, accuracy), y=accuracy, color=model)) + 
    geom_point() + 
    geom_text(aes(y=accuracy-0.02, label=round(accuracy, 2)), hjust=1, size=4) +
    coord_flip() + 
    labs(x="", y="Accuracy", title="Change in Accuracy - Original One vs. All, New One vs, One") + 
    geom_hline(aes(yintercept=0.5), lty=2) + 
    ylim(c(0, 1))

# Plot the change in accuracy data
highMeanOutput %>%
    group_by(desc) %>%
    summarize(accuracyGain=max(accuracy)-min(accuracy)) %>%
    ggplot(aes(x=fct_reorder(desc, accuracyGain), y=accuracyGain)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=accuracyGain+0.02, label=round(accuracyGain, 2)), hjust=0) + 
    coord_flip() + 
    labs(x="", y="Gain in Accuracy", title="Gain in Accuracy (One vs One compared to One vs. All")

# Check the highest delta error records
accHighMeanError %>%
    mutate(deltaAccuracy=abs(accA-accB)) %>%
    arrange(-deltaAccuracy)

```
  
Accuracy gains are significant, with many comparisons gaining ~40% accuracy when trained one vs. one rather than being just a small component of a one vs. all training.  Gains are especially notable for Las Vegas/Phoenix, which soars from 36% accuracy (worse than null) to 95% accuracy.  Large gains are also noted for Miami/Tampa and Los Angeles/San Diego, suggesting that a focused one-on-one training can help pull apart similar archetype cities.  

That said, there are still a large number of pairings where the differentiation appears to be OK (accuracy around 80% vs. null accuracy 50%) but still with meaningful confusion.

There is little delta error whether looking at A/B or B/A, not surprising given the balanced nature of the data used for modeling and the generally small delta error for these pairings even in the previous one vs all.

The process is run again for the high delta-error pairings:  
```{r cache=TRUE}

# Create container for each pairing in highDeltaError
# Name for modelLocale
highDeltaErrorList <- vector("list", nrow(highDeltaError))
names(highDeltaErrorList) <- highDeltaError$modelLocale

# Run model for each pairing in highDeltaError
for (n in 1:nrow(highDeltaError)) {
    
    # Extract the key locale and the other locale
    # Note that which locale is defined as key is arbitrary and unimportant since this is a full 1:1 comparison
    keyLoc <- as.character(highDeltaError$modelLocale)[n]
    otherLoc <- as.character(highDeltaError$locNamefct)[n]
    
    # Run XGB for 500 rounds using only two locales and 2016 data; do not under-sample 'all other'
    highDeltaErrorList[[n]] <- helperXGBOnevAll(metarData, 
                                                keyLoc=keyLoc, 
                                                critFilter=list(year=2016, 
                                                                locNamefct=c(keyLoc, otherLoc)
                                                                ), 
                                                underSample=FALSE,
                                                seed=2008071439
                                                )
    
}

```
  
The comparisons can be run again also:  
```{r}

# Extract accuracy by subset by locale
accHighDeltaError <- map_dfr(highDeltaErrorList, .f=helperExtractOnevOneAccuracy)

# Combine with the original file, highMeanError
highDeltaOutput <- highDeltaError %>%
    mutate(locA=as.character(modelLocale), 
           locB=as.character(locNamefct), 
           original=1-meanError
           ) %>%
    select(locA, locB, original) %>%
    inner_join(accHighDeltaError, by=c("locA", "locB")) %>%
    select(-accA, -accB, new=accOverall) %>%
    pivot_longer(-c(locA, locB), names_to="model", values_to="accuracy") %>%
    mutate(desc=paste0(locA, " vs. ", locB)) 

# Plot the accuracy data
highDeltaOutput %>%
    ggplot(aes(x=fct_reorder(desc, accuracy), y=accuracy, color=model)) + 
    geom_point() + 
    geom_text(aes(y=accuracy-0.02, label=round(accuracy, 2)), hjust=1, size=4) +
    coord_flip() + 
    labs(x="", y="Accuracy", title="Change in Accuracy - Original One vs. All, New One vs, One") + 
    geom_hline(aes(yintercept=0.5), lty=2) + 
    ylim(c(0, 1))

# Plot the change in accuracy data
highDeltaOutput %>%
    group_by(desc) %>%
    summarize(accuracyGain=max(accuracy)-min(accuracy)) %>%
    ggplot(aes(x=fct_reorder(desc, accuracyGain), y=accuracyGain)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=accuracyGain+0.02, label=round(accuracyGain, 2)), hjust=0) + 
    coord_flip() + 
    labs(x="", y="Gain in Accuracy", title="Gain in Accuracy (One vs One compared to One vs. All")

# Check the highest delta error records
accHighDeltaError %>%
    mutate(deltaAccuracy=abs(accA-accB)) %>%
    arrange(-deltaAccuracy)

```
  
There is no longer any meaningful difference in the A/B and B/A accuracy for these pairings, and the overall accuracy has in almost all cases climbed in to the 90%+ range.  This is suggestive that these pairings are largely distinct but need a large training samplt to tease out the distinctions.  The one exception is Detroit vs. Traverse City, which is notably the only pairing that was also part of the "low overall accuracy" runs.

Next steps are to explore what predictions look like for unrelated cities using the one vs. one models.  This may induce significant extrapolation errors, and is intended solely as an exploratory step.  The next main model will be the multiclass version of the XGB, taking care to have sufficient training data volume in an attempt to meaningfully capture the accuracy gains seen in the preceding analysis.
  
Converting the XGB approach to work for multiclass classification will require the following:  
  
1.  Convert the target variable to an integer that indexes from 0 to n-1 (where there are n classes); note that technically this works for n-2, since the XGB algorithm uses 0 and 1 for that  
2.  Convert the objective to "multi:softprob"  
3.  Convert the evaluation metric (though this should happen automatically)  
4.  Convert the predictions back to a normal value  
  
The function xgbRunModel() is updated to accept xgbObjective="multi:softprob", and then data are created for the four locales with 2014-2019 data - Chicago, Las Vegas, New Orleans, San Diego:  
```{r cache=TRUE}

fourLocales <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA")

xgbFourLocalesCV <- xgbRunModel(filter(metarData, !is.na(TempF)), 
                                depVar="locNamefct", 
                                predVars=locXGBPreds, 
                                otherVars=keepVarFull, 
                                critFilter=list(year=2016, locNamefct=fourLocales), 
                                seed=2008081315,
                                nrounds=1000,
                                print_every_n=50,
                                xgbObjective="multi:softprob", 
                                funcRun=xgboost::xgb.cv,
                                nfold=5,
                                calcErr=FALSE,
                                num_class=length(fourLocales)
                                )

```
  
Error evolution can then be plotted:  
```{r}

minTestError <- xgbFourLocalesCV$xgbModel$evaluation_log %>%
    pull(test_merror_mean) %>%
    min()

xgbFourLocalesCV$xgbModel$evaluation_log %>%
    filter(test_merror_mean <= 0.08) %>%
    ggplot(aes(x=iter, y=test_merror_mean)) +
    geom_line() +
    labs(x="# Iterations", 
         y="Test Error", 
         title="Evolution of Test Error", 
         subtitle="Filtered to Test Error <= 0.08"
         ) + 
    ylim(c(0, NA)) + 
    geom_vline(aes(xintercept=0), lty=2) + 
    geom_hline(aes(yintercept=minTestError), color="red", lty=2)
    
```
  
Test error appears to be near the minimum at around 750 iterations.  The xgboost::xgboost algorithm is run with 750 iterations, and with predictions made for the best locale:  
```{r cache=TRUE}

fourLocales <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA")

xgbFourLocales <- xgbRunModel(filter(metarData, !is.na(TempF)), 
                              depVar="locNamefct", 
                              predVars=locXGBPreds, 
                              otherVars=keepVarFull, 
                              critFilter=list(year=2016, locNamefct=fourLocales), 
                              seed=2008081315,
                              nrounds=750,
                              print_every_n=50,
                              xgbObjective="multi:softprob", 
                              funcRun=xgboost::xgboost,
                              calcErr=FALSE,
                              num_class=length(fourLocales)
                              )

```
  
Classification performance on test data can then be assessed:  
```{r}

# Overall success
xgbFourLocales$testData %>%
    summarize(mean(locNamefct==predicted))

# Histogram of predicted probabilities for selected class
xgbFourLocales$testData %>%
    mutate(correct=locNamefct==predicted) %>%
    ggplot() + 
    stat_count(aes(x=round(probPredicted, 2), y=..prop.., group=correct)) + 
    facet_wrap(~correct) + 
    labs(x="Probability Given to Prediction", 
         y="Proportion of Predictions", 
         title="Probability of Prediction vs. Accuracy of Prediction"
         )

# Confusion matrix
xgbFourLocales$testData %>%
    count(locNamefct, predicted) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n)) %>%
    ggplot(aes(x=predicted, y=locNamefct)) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("", low="white", high="green") + 
    labs(title="Predicted vs. Actual Locale Frequency", y="Actual Locale", x="Predicted Locale")

# Find and plot importances
xgb_fourLocales_importance <- plotXGBImportance(xgbFourLocales, 
                                                featureStems=locXGBPreds, 
                                                stemMapper = varMapper, 
                                                plotTitle="Gain by variable in xgboost", 
                                                plotSubtitle="Modeling 2016 Locale (LAS, MSY, ORD, SAN)"
                                                )

```
  
Overall prediction accuracy is ~98%, with incorrect predictions having much lower probabilities than correct predictions.  Predictions for every locale appear to be of about equal (and very high) accuracy.  Dew point stands out as the best differentiato, with temperature, month, and sea-level pressure next highest in gain.  This is consistent with previous analysis on these locales.

Overall, the base XGB parameters seem to be driving increased multiclass accuracy in a much shorter run time than random forest.

How well does the 2016 model predict data from 2014-2015 and 2017-2019 for these same locales?  
```{r}

# Extract data for the four locales, excluding 2016
non2016FourLocalesData <- metarData %>%
    filter(!is.na(TempF), year != 2016, locNamefct %in% fourLocales)

# Make the predictions
non2016FourLocalesPred <- non2016FourLocalesData %>%
    mutate_if(is.factor, .funs=fct_drop) %>%
    helperMakeSparse(predVars=locXGBPreds) %>%
    predict(xgbFourLocales$xgbModel, newdata=.)

# Create the prediction matrix
non2016FourLocalesMatrix <- matrix(data=non2016FourLocalesPred, 
                                   ncol=length(fourLocales), 
                                   nrow=nrow(non2016FourLocalesData), 
                                   byrow=TRUE
                                   )

# Get the predictions and probabilities, and add them to non2016FourLocalesData
maxCol <- apply(non2016FourLocalesMatrix, 1, FUN=which.max)
non2016FourLocalesData <- non2016FourLocalesData %>%
    mutate(predicted=xgbFourLocales$yTrainLevels[maxCol], 
           probPredicted=apply(non2016FourLocalesMatrix, 1, FUN=max), 
           correct=locNamefct==predicted
           )

# Assess overall prediction accuracy by year
helperNon2016Accuracy <- function(df, grpVar, grpLabel) {
    p1 <- df %>%
        group_by_at(grpVar) %>%
        summarize(pctCorrect=mean(correct)) %>%
        ggplot(aes(x=factor(get(grpVar)), y=pctCorrect)) + 
        geom_col(fill="lightblue") + 
        geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect), "%"))) +
        coord_flip() + 
        ylim(c(0, 1)) + 
        labs(x=grpLabel, y="Percent Correct", title="Accuracy of predictions to non-2016 data")
    print(p1)
}

helperNon2016Accuracy(non2016FourLocalesData, grpVar="year", grpLabel="Year")
helperNon2016Accuracy(non2016FourLocalesData, grpVar="locNamefct", grpLabel="Actual Locale")
helperNon2016Accuracy(non2016FourLocalesData, grpVar="month", grpLabel="Month")
helperNon2016Accuracy(non2016FourLocalesData, grpVar="hrfct", grpLabel="Hour (Zulu Time)")

# Confusion matrix
non2016FourLocalesData %>%
    count(locNamefct, predicted) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n)) %>%
    ggplot(aes(x=predicted, y=locNamefct)) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("", low="white", high="green") + 
    labs(title="Predicted vs. Actual Locale Frequency", y="Actual Locale", x="Predicted Locale")

```
  
Prediction accuracy dips from ~98% on the 2016 test data to ~92% on the 2014-2015 and 2017-2019 data from the same locales.  This suggests that while the model is learning significant features that differentiate these locales, it is also learning features specific to 2016.

Chicago and Las Vegas are better classified in years other than 2016, retaining ~95% accuracy.  Predictions are generally better in the summer months than in the winter months, and there is no meaningful difference in accuracy by hour.

Next steps are to train the model on a larger tranche of data (2015-2018) to see if this can be better generalized to out-of-sample years (2014, 2019) for the same locales:  
```{r cache=TRUE}

fourLocales <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA")

xgbFourLocalesCV_20152018 <- xgbRunModel(filter(metarData, !is.na(TempF)), 
                                         depVar="locNamefct", 
                                         predVars=locXGBPreds, 
                                         otherVars=keepVarFull, 
                                         critFilter=list(year=2015:2018, locNamefct=fourLocales), 
                                         seed=2008091301,
                                         nrounds=1000,
                                         print_every_n=50,
                                         xgbObjective="multi:softprob", 
                                         funcRun=xgboost::xgb.cv,
                                         nfold=5,
                                         calcErr=FALSE,
                                         num_class=length(fourLocales)
                                         )

```
  
Results of the cross-validation can be assessed:  
```{r}

minTestError_20152018 <- xgbFourLocalesCV_20152018$xgbModel$evaluation_log %>%
    pull(test_merror_mean) %>%
    min()

xgbFourLocalesCV_20152018$xgbModel$evaluation_log %>%
    filter(test_merror_mean <= 0.08) %>%
    ggplot(aes(x=iter, y=test_merror_mean)) +
    geom_line() +
    labs(x="# Iterations", 
         y="Test Error", 
         title="Evolution of Test Error", 
         subtitle="Filtered to Test Error <= 0.08"
         ) + 
    ylim(c(0, NA)) + 
    geom_vline(aes(xintercept=0), lty=2) + 
    geom_hline(aes(yintercept=minTestError_20152018), color="red", lty=2)

```
  
The model using 2015-2018 data appears to drive slightly lower test RMSE than the 2016-only modeling.  Test RMSE evolution appears to be stable by around 1000 iterations.  The model is run for 1000 rounds:  
```{r cache=TRUE}

fourLocales <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA")

xgbFourLocales_20152018 <- xgbRunModel(filter(metarData, !is.na(TempF)), 
                                       depVar="locNamefct", 
                                       predVars=locXGBPreds, 
                                       otherVars=keepVarFull, 
                                       critFilter=list(year=2015:2018, locNamefct=fourLocales), 
                                       seed=2008081315,
                                       nrounds=1000,
                                       print_every_n=50,
                                       xgbObjective="multi:softprob", 
                                       funcRun=xgboost::xgboost,
                                       calcErr=FALSE,
                                       num_class=length(fourLocales)
                                       )

```
  
Model performance can be evaluated:  
```{r}

# Overall success
xgbFourLocales_20152018$testData %>%
    summarize(mean(locNamefct==predicted))

# Histogram of predicted probabilities for selected class
xgbFourLocales_20152018$testData %>%
    mutate(correct=locNamefct==predicted) %>%
    ggplot() + 
    stat_count(aes(x=round(probPredicted, 2), y=..prop.., group=correct)) + 
    facet_wrap(~correct) + 
    labs(x="Probability Given to Prediction", 
         y="Proportion of Predictions", 
         title="Probability of Prediction vs. Accuracy of Prediction"
         )

# Confusion matrix
xgbFourLocales_20152018$testData %>%
    count(locNamefct, predicted) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n)) %>%
    ggplot(aes(x=predicted, y=locNamefct)) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("", low="white", high="green") + 
    labs(title="Predicted vs. Actual Locale Frequency", y="Actual Locale", x="Predicted Locale")

# Find and plot importances
xgb_fourLocales_importance_20152018 <- plotXGBImportance(xgbFourLocales_20152018, 
                                                         featureStems=locXGBPreds, 
                                                         stemMapper = varMapper, 
                                                         plotTitle="Gain by variable in xgboost", 
                                                         plotSubtitle="2015-2018 Locale (LAS, MSY, ORD, SAN)"
                                                         )

```
  
Accuracy and variable importances seem comparable to the previous model run using only 2016 data.  Performance by dimension and the confusion matrix can be evaluated:  
```{r}

# Extract data for the four locales, excluding 2016
non20152018FourLocalesData <- metarData %>%
    filter(!is.na(TempF), !(year %in% 2015:2018), locNamefct %in% fourLocales)

# Make the predictions
non20152018FourLocalesPred <- non20152018FourLocalesData %>%
    mutate_if(is.factor, .funs=fct_drop) %>%
    helperMakeSparse(predVars=locXGBPreds) %>%
    predict(xgbFourLocales_20152018$xgbModel, newdata=.)

# Create the prediction matrix
non20152018FourLocalesMatrix <- matrix(data=non20152018FourLocalesPred, 
                                       ncol=length(fourLocales), 
                                       nrow=nrow(non20152018FourLocalesData), 
                                       byrow=TRUE
                                       )

# Get the predictions and probabilities, and add them to non2016FourLocalesData
maxCol <- apply(non20152018FourLocalesMatrix, 1, FUN=which.max)
non20152018FourLocalesData <- non20152018FourLocalesData %>%
    mutate(predicted=xgbFourLocales_20152018$yTrainLevels[maxCol], 
           probPredicted=apply(non20152018FourLocalesMatrix, 1, FUN=max), 
           correct=locNamefct==predicted
           )

# Assess overall prediction accuracy by year
helperNon2016Accuracy <- function(df, grpVar, grpLabel) {
    p1 <- df %>%
        group_by_at(grpVar) %>%
        summarize(pctCorrect=mean(correct)) %>%
        ggplot(aes(x=factor(get(grpVar)), y=pctCorrect)) + 
        geom_col(fill="lightblue") + 
        geom_text(aes(y=pctCorrect/2, label=paste0(round(100*pctCorrect), "%"))) +
        coord_flip() + 
        ylim(c(0, 1)) + 
        labs(x=grpLabel, y="Percent Correct", title="Accuracy of predictions to non-2016 data")
    print(p1)
}

helperNon2016Accuracy(non20152018FourLocalesData, grpVar="year", grpLabel="Year")
helperNon2016Accuracy(non20152018FourLocalesData, grpVar="locNamefct", grpLabel="Actual Locale")
helperNon2016Accuracy(non20152018FourLocalesData, grpVar="month", grpLabel="Month")
helperNon2016Accuracy(non20152018FourLocalesData, grpVar="hrfct", grpLabel="Hour (Zulu Time)")

# Confusion matrix
non20152018FourLocalesData %>%
    count(locNamefct, predicted) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n)) %>%
    ggplot(aes(x=predicted, y=locNamefct)) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("", low="white", high="green") + 
    labs(title="Predicted vs. Actual Locale Frequency", y="Actual Locale", x="Predicted Locale")

```
  
Prediction quality for out-of-sample years increases meaningfully, from ~90% when the model is based only on 2016 data to ~95% when the model is based on 2015-2018 data.  This suggests that modeling on multiple years helps train the model on recurring climate differences as opposed to sporadic local weather anomalies in a given year.  chicago appears to almost always be predicted as itself, though each other locale predicts as Chicago on occasion.

Data for all other locales is then run through the model to see which of the four cities each of the other cities is "most like":  
```{r}

# Extract data for the four locales, excluding 2016
otherLocalesData <- metarData %>%
    filter(!is.na(TempF), year == 2016, !(locNamefct %in% fourLocales))

# Make the predictions
otherLocalesPred <- otherLocalesData %>%
    mutate_if(is.factor, .funs=fct_drop) %>%
    helperMakeSparse(predVars=locXGBPreds) %>%
    predict(xgbFourLocales_20152018$xgbModel, newdata=.)

# Create the prediction matrix
otherLocalesMatrix <- matrix(data=otherLocalesPred, 
                                   ncol=length(fourLocales), 
                                   nrow=nrow(otherLocalesData), 
                                   byrow=TRUE
                                   )

# Get the predictions and probabilities, and add them to non2016FourLocalesData
maxCol <- apply(otherLocalesMatrix, 1, FUN=which.max)
otherLocalesData <- otherLocalesData %>%
    mutate(predicted=xgbFourLocales_20152018$yTrainLevels[maxCol], 
           probPredicted=apply(otherLocalesMatrix, 1, FUN=max), 
           correct=locNamefct==predicted
           )

# Confusion matrix
otherLocalesData %>%
    count(locNamefct, predicted) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n), pctChi=ifelse(predicted=="Chicago, IL", pct, 0)) %>%
    ggplot(aes(x=predicted, y=fct_reorder(locNamefct, pctChi, .fun=max))) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("", low="white", high="green") + 
    labs(title="Predicted vs. Actual Locale Frequency", y="Actual Locale", x="Predicted Locale")

```
  
Findings include:  
  
* Cold weather cities tend to be 90%+ assigned to Chicago  
* Phoenix is highly associated with Las Vegas, while somewhat surprisingly both Denver and San Antonio align primarily with Las Vegas also  
* Houston, Tampa, and Miami align primarily with New Orleans, with some association also to San Diego  
* Coastal California cities tend to align to San Diego and Chicago, with the proportion of Chicago-like days higher in northern California than in Los Angeles  
  
Next steps are to consolidate the functions for data preparation, modeling (including CV), and prediction for XGB such that the process is better aligned with functional programming.
  
Broadly, the XGB modeling process includes several steps:  
  
1.  Filtering an initial dataset so that all records meet certain criteria (key variables not missing, locale subsetted, empty factor levels dropped, etc.)  
2.  Splitting the initial dataset in to test and train  
3.  Separating a dataset in to response variable (recoded to 0 to n-1) and predictor variables (converted to sparse format with factors as dummy columns)  
4.  Making predictions using a trained model  
5.  Training a model and making predictions - can be xgboost::xgboost, xgboost::xgb.cv and use a user-specified objective function  
  
Code from above is copied and adapted as needed for achieving these steps.  The first function filters an initial data frame to return data that are ready for preprocessing and then modeling:  
```{r}

# Function to filter an initial data frame based on specified criteria
helperFilterData <- function(df, 
                             keepVars=NULL,
                             noNA=keepVars, 
                             critFilter=vector("list", 0), 
                             critFilterNot=vector("list", 0), 
                             dropEmptyLevels=TRUE
                             ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble to filter
    # keepVars: the variables to be kept (NULL means keep all)
    # noNA: character string of variables that should have a "not NA" rule enforced (NULL means skip step)
    #       default is to apply noNA to every variable passed to keepVars
    # critFilter: filtering criteria passed as a named list (list(varName01=allowedValues01, ...))
    # critFilterNot: filtering criteria passed as a named list (list(varName01=disallowedValues01, ...))
    #                note that critFilterNot uses !disallowedValues01 while critFilter uses allowedValues01
    # dropEmptyLevels: whether to drop the empty levels of all factor variables
    
    # Remove the NA variables where passed as arguments
    if (!is.null(noNA)) {
        df <- df %>%
            filter_at(vars(all_of(noNA)), all_vars(!is.na(.)))
    }
    
    # Filter such that only matches to critFilter are included
    for (xNum in seq_len(length(critFilter))) {
        df <- df %>%
            filter_at(vars(all_of(names(critFilter)[xNum])), ~. %in% critFilter[[xNum]])
    }
    
    # Filter such that all matches to critFilterNot are excluded
    for (xNum in seq_len(length(critFilterNot))) {
        df <- df %>%
            filter_at(vars(all_of(names(critFilterNot)[xNum])), ~!(. %in% critFilterNot[[xNum]]))
    }
    
    # Keep only the requested variables if keepVars is not NULL
    if (!is.null(keepVars)) {
        df <- df %>%
            select_at(vars(all_of(keepVars)))
    }
    
    # Drop empty levels from factors if requested
    if (dropEmptyLevels) {
        df <- df %>%
            mutate_if(is.factor, .funs=fct_drop)
    }
    
    # Return the modified frame
    df
    
}

# Confirm that function does nothing to dataset with default parameters and dropEmptyLevels=FALSE
all.equal(metarData, helperFilterData(metarData, dropEmptyLevels=FALSE))

# Example creation of a dataset for weather modeling
sampData <- helperFilterData(metarData, 
                             keepVars=c("source", "dtime", "TempF", "DewF", "month", "predomDir"), 
                             critFilter=list(year=2015, month=c("Jan", "Jun", "Jul", "Dec")), 
                             critFilterNot=list(predomDir=c("VRB", "000", "Error"), locNamefct="Las Vegas, NV")
                             ) %>%
    mutate(isMSY=source=="kmsy_2015")
sampData %>% count(source, isMSY)
sampData %>% count(month)
sampData %>% count(predomDir)
str(sampData)

```
  
The second step is already achieved by function createTestTrain() which creates a list with elements 'trainData' and 'testData':  
```{r}

sampList <- createTestTrain(sampData)
sampList

```
  
The third function recodes data as needed so that the response variable is either a numeric or a factor that has been recoded as 0 to n-1, while the predictor variables are a sparse matrix where every column is either a numeric or a factor that has been converted to have its contrasts in each column:  
```{r}

# Recode the data as appropriate for XGB modeling
helperXGBRecode <- function(df, 
                            depVar, 
                            predVars, 
                            depIsFactor=FALSE
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: the data frame or tibble for processing
    # depVar: the dependent variable
    # predVars: the predictor variables
    # depIsFactor: whether the dependent variable should be treated as a factor (classification)
    #              NULL means set TRUE if !is.numeric(depVar)
    
    # Pull the dependent variable
    y <- df[, depVar, drop=TRUE]
    
    # Set depIsFactor automatically if it was passed as NULL
    if (is.null(depIsFactor)) depIsFactor <- !is.numeric(y)
    
    # Convert y to factor if flag is set
    if (depIsFactor) {
        # Convert y to a factor variable if it is not already of that class
        if (!is.factor(y)) y <- factor(y)
        # Store the factor levels so they can be reconstituted if desired
        yLevels <- levels(y)
        # Convert the dependent variable to integers running from 0 to n-1
        y <- as.integer(y) - 1
    } else {
        # Set yLevels to NULL if it is not relevant
        yLevels <- NULL
    }

    # Create the sparse matrix for 
    x <- df %>%
        select_at(vars(all_of(c(predVars)))) %>%
        Matrix::sparse.model.matrix(~ . -1, 
                                    data=., 
                                    contrasts.arg=lapply(.[, sapply(., is.factor)], contrasts, contrasts=FALSE)
                                    )
    
    # Return the key elements
    list(y=y, x=x, yLevels=yLevels)
    
}

# Create a factor modeling list for multi-class
sampTrainFactor <- helperXGBRecode(sampList$trainData, 
                                   depVar="source", 
                                   predVars=c("TempF", "DewF", "month", "predomDir"), 
                                   depIsFactor=TRUE
                                   )
str(sampTrainFactor)

# Create a factor modeling list for single-class
sampTrainSingle <- helperXGBRecode(sampList$trainData, 
                                   depVar="isMSY", 
                                   predVars=c("TempF", "DewF", "month", "predomDir"), 
                                   depIsFactor=TRUE
                                   )
str(sampTrainSingle)

# Create a numeric modeling list
sampTrainFactor <- helperXGBRecode(sampList$trainData, 
                                   depVar="TempF", 
                                   predVars=c("source", "DewF", "month", "predomDir"), 
                                   depIsFactor=FALSE
                                   )
str(sampTrainFactor)

```
  
A function to make predictions using an XGB model is created:  
```{r}

# Function to make predictions given a trained XGB model and an aligned sparse matrix
helperXGBPredict <- function(mdl, 
                             dfSparse, 
                             objective,
                             probMatrix=FALSE, 
                             yLevels=NULL
                             ) {
    
    # FUNCTION ARGUMENTS:
    # mdl: the trained XGB model
    # dfSparse: a sparse data frame that matches the columns used in mdl
    # objective: the objective function that was used (drives the prediction approach)
    # probMatrix: boolean, whether to create a probability matrix by class
    # yLevels: if probMatrix is TRUE, yLevels should be passed (names associated to classes 0 through n-1)
    
    # Check that yLevels is passed so the probability matrix can be properly consituted
    if (probMatrix & is.null(yLevels)) {
        stop("\nMust pass the yLevels variable if a probability matrix is requested\n")
    }
    
    # Create a base probMatrix file that is NULL
    probData <- NULL

    # Create the testData tibble (and predData matrix if probMatrix=TRUE)
    # If logistic, the probability is the prediction and a tentative class can be set using 50%
    if (probMatrix) {
        # If logistic, the probability is the prediction and a tentative class can be set using 50%
        if (objective=="binary:logistic") {
            probData <- matrix(data=c(1-predict(mdl, newdata=dfSparse), predict(mdl, newdata=dfSparse)), 
                               nrow=nrow(dfSparse), 
                               ncol=length(yLevels), 
                               byrow=FALSE
                               )
        } else {
            probData <- matrix(data=predict(mdl, newdata=dfSparse), 
                               nrow=nrow(dfSparse), 
                               ncol=length(yLevels), 
                               byrow=TRUE
                               )
        }
        maxCol <- apply(probData, 1, FUN=which.max)
        predData <- tibble::tibble(predicted=factor(yLevels[maxCol], levels=yLevels), 
                                   probPredicted=apply(probData, 1, FUN=max)
                                   )
        probData <- probData %>%
            as_tibble() %>%
            purrr::set_names(yLevels)
    } else {
        predData <- tibble::tibble(predicted=predict(mdl, newdata=dfSparse))
    }
    
    # Return a list of predData and probData
    list(predData=predData, probData=probData)

}

```
  
The function xgbRunModel() is modified to call the helper functions just created:  
```{r}

# Run xgb model with desired parameters
xgbRunModel_002 <- function(tbl, 
                            depVar, 
                            predVars,
                            otherVars=c("source", "dtime"),
                            critFilter=vector("list", 0),
                            critFilterNot=vector("list", 0),
                            dropEmptyLevels=TRUE,
                            seed=NULL, 
                            nrounds=200, 
                            print_every_n=nrounds, 
                            testSize=0.3, 
                            xgbObjective="reg:squarederror",
                            funcRun=xgboost::xgboost,
                            calcErr=TRUE,
                            ...
                            ) {
    
    # FUNCTION ARGUMENTS:
    # tbl: the data frame or tibble
    # depVar: the dependent variable that will be predicted
    # predVars: explanatory variables for modeling
    # otherVars: other variables to be kept in a final testData file, but not used in modeling
    # critFilter: named list of format list(varName=c(varValues))
    #             will include only observations where get(varName) %in% varValues
    #             vector("list", 0) creates a length-zero list, which never runs in the for loop
    # critFilterNot: named list of format list(varName=c(varValues))
    #                same as critFilter operation, but will apply !(varName %in% varValues)
    # dropEmptyLevels: boolean, whether to run fct_drop on all variables of class factor after critFilter
    # seed: the random seed (NULL means no seed)
    # nrounds: the maximum number of boosting iterations
    # print_every_n: how frequently to print the progress of training error/accuracy while fitting
    # testSize: the fractional portion of data that should be used as the test dataset
    # xgbObjective: the objective function for xgboost
    # funcRun: the function to run, passed as a function
    # calcErr: boolean, whether to create variable err as predicted-get(depVar)
    # ...: additional arguments to be passed directly to xgboost
    
    # Check that funcName is valid and get the relevant function
    valFuncs <- c("xgboost", "xgb.cv")
    funcName <- as.character(substitute(funcRun))
    if (!(funcName[length(funcName)] %in% valFuncs)) {
        cat("\nFunction is currently only prepared for:", valFuncs, "\n")
        stop("Please change passed argument or update function\n")
    }
    
    # Check that the objective function is programmed
    valObjectiveFull <- c("reg:squarederror", "binary:logistic", "multi:softprob")
    valObjectiveCVOnly <- c("multi:softmax")  # This is not implemented in helperXGBPredict, so it is CV only
    if (!(xgbObjective %in% valObjectiveFull) & funcName[length(funcName)]=="xgboost") {
        cat("\nFunction runs xgboost with predict only for:", valObjectiveFull, "\n")
        stop("Please change passed argument or update function (need to also updated helperXGBPredict()\n")
    }
    if (!(xgbObjective %in% c(valObjectiveFull, valObjectiveCVOnly))) {
        cat("\nFunction runs xgb.cv only for:", valObjectiveFull, valObjectiveCVOnly, "\n")
        stop("Please change passed argument or update function (need to also updated helperXGBPredict()\n")
    }
    
    # Filter such that only matches to critFilter are included
    tbl <- helperFilterData(tbl, 
                            keepVars=c(depVar, predVars, otherVars), 
                            noNA=c(depVar, predVars), 
                            critFilter=critFilter, 
                            critFilterNot=critFilterNot,
                            dropEmptyLevels=dropEmptyLevels
                            )
    
    # Create test-train split
    ttLists <- createTestTrain(tbl, testSize=testSize, seed=seed)
    
    # Set the seed if requested
    if (!is.null(seed)) { set.seed(seed) }
    
    # Recode the training data and testing data
    # Treat (and convert) depVar as factor unless it is numeric (signified by NULL)
    recodeTrain <- helperXGBRecode(ttLists$trainData, 
                                   depVar=depVar, 
                                   predVars=predVars, 
                                   depIsFactor=NULL
                                   )
    recodeTest <- helperXGBRecode(ttLists$testData, 
                                  depVar=depVar, 
                                  predVars=predVars, 
                                  depIsFactor=NULL
                                  )
        
    # Pull the dependent variable, dependent variable levels, and sparse matrix x for training
    yTrain <- recodeTrain$y
    yTrainLevels <- recodeTrain$yLevels
    sparseTrain <- recodeTrain$x
    
    # Train model
    xgbModel <- funcRun(data=sparseTrain, 
                        label=yTrain, 
                        nrounds=nrounds, 
                        print_every_n=print_every_n, 
                        objective=xgbObjective, 
                        ...
                        )

    # Make predictions, including getting the probability matrix if yTrainLevels exists (yTrain is factor)
    # Run only if xgboost was passed (no predictions for CV)
    if (funcName[length(funcName)] %in% c("xgboost")) {
        # Make the predictions
        xgbPredsList <- helperXGBPredict(xgbModel, 
                                         dfSparse=recodeTest$x, 
                                         probMatrix=!is.null(yTrainLevels), 
                                         yLevels=yTrainLevels, 
                                         objective=xgbObjective
                                         )
    
        # Combine the xgbPredList output with existing testData file
        testData <- ttLists$testData %>%
            bind_cols(xgbPredsList$predData)
        
        # Extract probData
        probData <- xgbPredsList$probData
        
    } else {
        # Create NULL for testData and probData
        testData <- NULL
        probData <- NULL
    }
    
    # Return list containing funcName, trained model, and testData
    list(funcName=funcName[length(funcName)], 
         xgbModel=xgbModel, 
         testData=testData, 
         predData=probData,
         yTrainLevels=yTrainLevels
         )
    
}

```
  
The function is then tested on a numeric response variable:  
```{r cache=TRUE}

# Define key predictor variables for base XGB runs
baseXGBPreds <- c("locNamefct", "month", "hrfct", 
                  "DewF", "modSLP", "Altimeter", "WindSpeed", 
                  "predomDir", "minHeight", "ceilingHeight"
                  )

# Core multi-year cities
multiYearLocales <- c("Las Vegas, NV", "New Orleans, LA", "Chicago, IL", "San Diego, CA")

# Run the function shell
xgbInit_002 <- xgbRunModel_002(metarData, 
                               depVar="TempF", 
                               predVars=baseXGBPreds, 
                               otherVars=c("source", "dtime"), 
                               critFilter=list(locNamefct=multiYearLocales),
                               seed=2008011825,
                               nrounds=2000,
                               print_every_n=50
                               )

```
  
The function is then tested on a single-class factor variable with the CV capability:  
```{r cache=TRUE}

# Run the function shell
xgb_las2016_cv_002 <- xgbRunModel_002(las2016Data, 
                                      depVar="isLAS", 
                                      predVars=locXGBPreds, 
                                      otherVars=keepVarFull, 
                                      seed=2008051405,
                                      nrounds=1000,
                                      print_every_n=50, 
                                      xgbObjective="binary:logistic", 
                                      funcRun=xgboost::xgb.cv, 
                                      nfold=5
                                      )

```
  
Test RMSE is a bit higher than previous.  The disconnect should be explored further.

And then function is then tested on multi-class classification:  
```{r cache=TRUE}

fourLocales <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA")

xgbFourLocales_002 <- xgbRunModel_002(filter(metarData, !is.na(TempF)), 
                                      depVar="locNamefct", 
                                      predVars=locXGBPreds, 
                                      otherVars=keepVarFull, 
                                      critFilter=list(year=2016, locNamefct=fourLocales), 
                                      seed=2008081315,
                                      nrounds=1000,
                                      print_every_n=50,
                                      xgbObjective="multi:softprob", 
                                      funcRun=xgboost::xgboost,
                                      calcErr=FALSE,
                                      num_class=length(fourLocales)
                                      )

# Overall success
xgbFourLocales_002$testData %>%
    summarize(mean(locNamefct==predicted))

```
  
Overall accuracy is the same as when run using the xgbRunModel function.

Next steps are to check whether the logistic regression approach is being properly implemented in the updated function:  
```{r}

select(xgb_las2016_cv$xgbModel$evaluation_log, iter, test_rmse_orig=test_error_mean) %>%
    inner_join(select(xgb_las2016_cv_002$xgbModel$evaluation_log, iter, test_rmse_new=test_error_mean)) %>%
    pivot_longer(-iter, names_to="model", values_to="rmse") %>%
    filter(rmse <= 0.06) %>%
    ggplot(aes(x=iter, y=rmse, group=model, color=model)) + 
    geom_line() + 
    ylim(c(0, NA)) + 
    labs(x="# Iterations", 
         y="RMSE", 
         title="CV Test RMSE by Function", 
         subtitle="New Function vs. Original Function"
         )

```
  
RMSE is slightly higher when the data were passed to the function as 0/1 rather than having the function convert a factor.  Is the same true when running the xgboost::xgboost?  

Exploration shows there was a logic error in helperXGBPredict that output nonsensical probabilities.  Broadly speaking, since the logit only outputs the probability of the positive class by default, creating a yes/no probability matrix from this data is both duplicative and erroneous in an unintended manner.

Function xgbRunModel() is updated to check that only a specified group of objective functions are allowed.  Adding an objective function may require updating the associated helper functions.  Currently allowed objective functions include:  
  
* reg:squarederror (default for numeric regression)  
* binary:logistic (default for 1/0 classification)  
* multi:softprob (default for multi-class classification)
  
Function helperXGBPredict is updated to account for the type of objective function, with binary:logistic separated so that the probability matrix is 1-predicted (probability of 0) and predicted (probability of 1).  The modeling process can then be run on the Las Vegas data:  
```{r cache=TRUE}

# Run the function shell
xgb_las2016_002 <- xgbRunModel_002(las2016Data, 
                                   depVar="isLAS", 
                                   predVars=locXGBPreds, 
                                   otherVars=keepVarFull, 
                                   seed=2008051405,
                                   nrounds=500,
                                   print_every_n=50, 
                                   xgbObjective="binary:logistic", 
                                   funcRun=xgboost::xgboost
                                   )

```
  
And, the confusion matrices for the original and new approach can be compared:  
```{r}

# Original model
xgb_las2016$testData %>%
    count(binLAS, round(predicted))

# Updated model
xgb_las2016_002$testData %>%
    count(isLAS, predicted)

```
  
Predictions for the model run with the same seed are identical.  The ability to pass factor variables to binary:logistic is useful, as it returning output with the predicted class and probability of that class (such as would happen in softmax).

Suppose that instead the Las Vegas model were run directly using softprob:  
```{r cache=TRUE}

# Run the function shell
xgb_las2016_002_softprob <- xgbRunModel_002(las2016Data, 
                                            depVar="isLAS", 
                                            predVars=locXGBPreds, 
                                            otherVars=keepVarFull, 
                                            seed=2008051405,
                                            nrounds=500,
                                            print_every_n=50, 
                                            xgbObjective="multi:softprob", 
                                            funcRun=xgboost::xgboost, 
                                            calcErr=FALSE,
                                            num_class=2
                                            )

```
  
The confusion matrix can again be checked:  
```{r}

# Updated model
xgb_las2016_002_softprob$testData %>%
    count(isLAS, predicted)

```
  
Classification success is nearly identical, with just some small rounding differences.

A bigger classification is run using the four key locales plus a selection of locales that seem to map to more than one of the four key locales:  
  
* Chicago  
* Las Vegas  
* New Orleans  
* San Diego  
* Seattle  
* Atlanta  
* San Francisco  
* Denver  
* San Antonio  
* Tampa Bay  
  
```{r cache=TRUE}

keyLocales <- c("Chicago, IL", "Las Vegas, NV", "New Orleans, LA", "San Diego, CA", "Seattle, WA", 
                "Atlanta, GA", "San Francisco, CA", "Denver, CO", "San Antonio, TX", "Tampa Bay, FL"
                )

xgbKeyLocales_002 <- xgbRunModel_002(metarData, 
                                     depVar="locNamefct", 
                                     predVars=locXGBPreds, 
                                     otherVars=keepVarFull, 
                                     critFilter=list(year=2016, locNamefct=keyLocales), 
                                     seed=2008111413,
                                     nrounds=1000,
                                     print_every_n=50,
                                     xgbObjective="multi:softprob", 
                                     funcRun=xgboost::xgboost,
                                     calcErr=FALSE,
                                     num_class=length(keyLocales)
                                     )

```
  
Model performance can be evaluated:  
```{r}

# Overall success
xgbKeyLocales_002$testData %>%
    summarize(mean(locNamefct==predicted))

# Histogram of predicted probabilities for selected class
xgbKeyLocales_002$testData %>%
    mutate(correct=locNamefct==predicted) %>%
    ggplot() + 
    stat_count(aes(x=round(probPredicted, 2), y=..prop.., group=correct)) + 
    facet_wrap(~correct) + 
    labs(x="Probability Given to Prediction", 
         y="Proportion of Predictions", 
         title="Probability of Prediction vs. Accuracy of Prediction"
         )

# Confusion matrix
xgbKeyLocales_002$testData %>%
    count(locNamefct, predicted) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n)) %>%
    ggplot(aes(x=stringr::str_replace(predicted, pattern=", ", replacement="\n"), y=locNamefct)) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("", low="white", high="green") + 
    labs(title="Predicted vs. Actual Locale Frequency", y="Actual Locale", x="Predicted Locale")

# Find and plot importances
xgbKeyLocales_importance_002 <- plotXGBImportance(xgbKeyLocales_002, 
                                                  featureStems=locXGBPreds, 
                                                  stemMapper = varMapper, 
                                                  plotTitle="Gain by variable in xgboost", 
                                                  plotSubtitle="2016 Locale (Subset of 10 locales modeled)"
                                                  )

```
  
The numeric variables (dew point, SLP, altimeter, temperature) dominate the variable importance (gain) in the XGB model, and the combination drives roughly 95% accuracy in predictions.  There are a few clusters that drive misclassifications of over 1% - Chicago/Atlanta, New Orleans/Tampa, San Diego/San Francisco.
  
Predictions can then be extended to the non-modeled locales:  
```{r cache=TRUE}

# Extract data for the non-modeled locales
otherLocalesData_002 <- metarData %>%
    filter(!is.na(TempF), year == 2016, !(locNamefct %in% keyLocales))

# Make the predictions
otherLocalesPred_002 <- otherLocalesData_002 %>%
    mutate_if(is.factor, .funs=fct_drop) %>%
    helperMakeSparse(predVars=locXGBPreds) %>%
    predict(xgbKeyLocales_002$xgbModel, newdata=.)

# Create the prediction matrix
otherLocalesMatrix_002 <- matrix(data=otherLocalesPred_002, 
                                 ncol=length(keyLocales), 
                                 nrow=nrow(otherLocalesData_002), 
                                 byrow=TRUE
                                 )

# Get the predictions and probabilities, and add them to non2016FourLocalesData
maxCol <- apply(otherLocalesMatrix_002, 1, FUN=which.max)
otherLocalesData_002 <- otherLocalesData_002 %>%
    mutate(predicted=xgbKeyLocales_002$yTrainLevels[maxCol], 
           probPredicted=apply(otherLocalesMatrix_002, 1, FUN=max), 
           correct=locNamefct==predicted
           )

# Confusion matrix
otherLocalesData_002 %>%
    count(locNamefct, predicted) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n), pctChi=ifelse(predicted=="Chicago, IL", pct, 0)) %>%
    ggplot(aes(x=stringr::str_replace(predicted, pattern=", ", replacement="\n"), 
               y=fct_reorder(locNamefct, pctChi, .fun=max)
               )
           ) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous("", low="white", high="green") + 
    labs(title="Predicted vs. Actual Locale Frequency", y="Actual Locale", x="Predicted Locale")

```
  
As the number of categories has increased, so has the likelihood of a locale being classified as having meaningful probability of being in 2+ categories.  Some of the larger splits seem sensible - Saint Louis/Indianapolis/Lincoln as Atlanta/Chicago, Dallas as Atlanta/Chicago/San Antonio, Newark/Philadelpia/DC as Chicago/Atlanta/Denver/Tampa, San Jose/Los Angeles as San Diego/San Francisco, Houston as Tampa/New Orleans.

Next steps are to consolidate the evaluation functions to assess model quality for both regression and classification exercises.

The evaluation process includes:  
  
* Assessment of variable importance from modeling - plotXGBImportance()  
* Assessment of the train (and sometimes test) error during modeling as shown in evaluation_log - plotXGBTrainEvolution() with modifications  
* Assessment of model predictions on hold-out data from the test data that was split from the training data - plotXGBTestData() should be updated for linear and confusion matrix  
* Assessment of prediction confidence vs. prediction accuracy on hold-out data  
* Assessment of model predictions on data that was not originally part of the train/test methodology  
  
Function plotXGBImportance() is already programmed.  It produces a full feature importance by every contrast of the factor and a summarized feature importance based on the stems associated with the factor:  
```{r}

# Find and plot importances
xgbKeyLocales_importance_002 <- plotXGBImportance(xgbKeyLocales_002, 
                                                  featureStems=locXGBPreds, 
                                                  stemMapper = varMapper, 
                                                  plotTitle="Gain by variable in xgboost", 
                                                  plotSubtitle="2016 Locale (Subset of 10 locales modeled)"
                                                  )

# Example of data available in the output list
str(xgbKeyLocales_importance_002)

```
  
Next, the function plotXGBTrainEvolution() is updated to plotXGBEvolution(), updated to handle both 1) regression vs. classification, and 2) train-only vs. train and test:  
```{r}

# Function to create and plot key metrics for an XGB model
# Currently, the R-squared capability is disabled, and this will only plot RMSE/Error
plotXGBEvolution <- function(mdl, 
                             subList="xgbModel", 
                             isRegression=TRUE,
                             isClassification=!isRegression,
                             first_iter_plot=10,
                             label_every=10, 
                             show_line=FALSE,
                             show_dashed=isClassification,
                             rounding=1, 
                             yLim=NULL
                             ) {
    
    # FUNCTION ARGUMENTS:
    # mdl: the xgb.Booster model file, or a list containing the xgb.Booster model file
    # subList: if mdl is a list, attempt to pull out item named in subList
    # isRegression: boolean, should this have regression metrics (RMSE, R2) plotted?
    # isClassification: boolean, should this have classification metrics (accuracy) plotted?
    # first_iter_plot: the first iteration to plot (for avoiding the very early burn-in errors)
    # label_every: how often to add text for the error data (e.g., 10 means print iter 10, 20, 30, etc.)
    # show_line: boolean, whether to include a line in the plot, or just the labels
    # rounding: level of precision for rounding when text is displayed on the plot
    # yLim: user-specified y-limits

    # Check that exactly one of isRegression and isClassification has been chosen
    if (!xor(isRegression, isClassification)) {
        cat("\nParemeters passed for isRegression:", isRegression, " and isClassification:", isClassification)
        stop("\nThese parameters must be logical and pass an exclusive or gate; please retry\n")
    }
    
    # Pull out the modeling data from the list if needed
    if (!("xgb.Booster" %in% class(mdl))) {
        mdl <- mdl[[subList]]
    }
    
    # Pull the error data
    errData <- mdl$evaluation_log
    
    # Convert names so that _merror becomes _error
    errNames <- names(errData)
    errNames <- str_replace(errNames, pattern="_merror", replacement="_error")
    names(errData) <- errNames
    
    # Keep only iter and then items that end in _rmse, _error, or _mean
    keepVars <- errNames[grepl(x=errNames, pattern="[iter|_rmse|_error|_mean]$")]
    errUse <- select_at(errData, vars(all_of(keepVars)))
    
    # Convert names so that train.* becomes train and test.* becomes test
    errUseNames <- names(errUse)
    errUseNames <- str_replace(errUseNames, pattern="train.*", replacement="train")
    errUseNames <- str_replace(errUseNames, pattern="test.*", replacement="test")
    names(errUse) <- errUseNames
    
    # Pivot the data longer, making the new descriptive column 'type' and the new numeric column 'error'
    errUse <- errUse %>%
        pivot_longer(-iter, names_to="type", values_to="error")
    
    # Helper function to create requested plot(s)
    helperPlotEvolution <- function(df, yVar, rnd, desc, size=3, label_every=1) {
        # Horizontal lines to be drawn at minimum by group
        groupMin <- df %>% group_by(type) %>% summarize(minError=min(error)) %>% pull(minError)
        # Create the base plot
        p1 <- df %>%
            filter(iter >= first_iter_plot) %>%
            ggplot(aes_string(x="iter", y=yVar, group="type", color="type")) + 
            labs(x="Number of iterations", 
                 y=paste0(desc), 
                 title=paste0("Evolution of ", desc)
                 ) + 
            scale_color_discrete("Data set") + 
            geom_vline(aes(xintercept=0), lty=2)
        if (show_dashed) p1 <- p1 + geom_hline(yintercept=groupMin, lty=2)
        if (!is.null(label_every)) {
            p1 <- p1 + geom_text(data=~filter(., (iter %% label_every)==0), 
                                 aes(label=round(get(yVar), rnd)), 
                                 size=size
                                 )
        }
        if (show_line) p1 <- p1 + geom_line()
        if (!is.null(yLim)) p1 <- p1 + ylim(yLim)
        print(p1)
    }
    
    # Create plot for regression (RMSE) or classification (error)
    helperPlotEvolution(errUse, 
                        yVar="error", 
                        rnd=rounding, 
                        desc=if(isRegression) "RMSE" else "Error", 
                        label_every=label_every
                        )

    # Return the relevant data file
    errUse
    
}

# Test for regression data
plotXGBEvolution(xgbInit_002, isRegression=TRUE, label_every=50, yLim=c(0, NA))

# Test for classification data - single-class
plotXGBEvolution(xgb_las2016, isRegression=FALSE, label_every=NULL, yLim=c(0, NA), show_line=TRUE)

# Test for classification data - single-class with CV
plotXGBEvolution(xgb_las2016_cv, isRegression=FALSE, label_every=NULL, yLim=c(0, NA), show_line=TRUE)

# Test for classification data - multi-class with CV
plotXGBEvolution(xgbFourLocalesCV_20152018, 
                 isRegression=FALSE, 
                 label_every=NULL, 
                 first_iter_plot = 50,
                 yLim=c(0, NA), 
                 show_line=TRUE
                 )

```
  
Next steps are to continue with evaluating model predictions on test data (RMSE or confusion, confidence level) and to extend to predictions on other data sets.  Function plotXGBTestData() is updated so that it can show either the linear trend of the relationships or the confusion matrix:  
```{r}

# Helper function to calculate and display RMSE/R2 for continuous data
helperCalculateRMSER2 <- function(df, depVar, predVar) {
    
    df %>%
        summarize(rmse_overall=sd(get(depVar)), 
                  rmse_model=mean((get(predVar)-get(depVar))**2)**0.5
                  ) %>%
        mutate(rsq=1-rmse_model**2/rmse_overall**2)

}


# Helper function to plot predicted vs. actual for continuous data
helperPlotTestContinuous <- function(df, predVar, depVar, roundSig=0) {
    
    # Show overall model performance using rounded temperature and predictions
    p1 <- df %>%
        mutate(rndPred=round(get(predVar), roundSig), rndDep=round(get(depVar), roundSig)) %>%
        group_by_at(vars(all_of(c("rndDep", "rndPred")))) %>%
        summarize(n=n()) %>%
        ggplot(aes_string(x="rndDep", y="rndPred")) + 
        geom_point(aes(size=n), alpha=0.1) + 
        geom_smooth(aes(weight=n)) + 
        geom_abline(lty=2, color="red") + 
        labs(title="Predictions vs. actual on test dataset", y="Predicted", x="Actual")
    print(p1)

}


# Helper function to create a confusion matrix
helperConfusion <- function(df, actual, predVar="predicted", ySortVar=NULL, rotateOn=11) {

    # Number of categories that will be on the x-axis
    nX <- df %>% 
        pull(predVar) %>% 
        as.character() %>% 
        unique() %>% 
        length()
    
    # Confusion matrix
    p1 <- df %>%
        group_by_at(vars(all_of(c(actual, predVar)))) %>%
        summarize(n=n()) %>%
        group_by_at(vars(all_of(actual))) %>%
        mutate(pct=n/sum(n), 
               pctSort=if (is.null(ySortVar)) 0 else ifelse(get(predVar)==ySortVar, pct, 0)
               ) %>%
        ggplot(aes(x=if (nX < rotateOn) stringr::str_replace(get(predVar), pattern=", ", replacement="\n") else get(predVar), 
                   y=if (is.null(ySortVar)) get(actual) else fct_reorder(get(actual), pctSort, .fun=max)
                   )
               ) + 
        geom_tile(aes(fill=pct)) + 
        geom_text(aes(label=paste0(round(100*pct), "%"))) + 
        scale_fill_continuous("", low="white", high="green") + 
        labs(title="Predicted vs. Actual Frequency", y="Actual Locale", x="Predicted Locale")
    # If the rotateOn criteria is exceeded, rotate the x-axis by 90 degrees and place axes at top
    if (nX >= rotateOn) {
        p1 <- p1 + 
            theme(axis.text.x=element_text(angle=90, hjust=0.1)) + # Rotate by 90 degrees
            scale_x_discrete(position = "top") # Put at the top of the graph
    }
    # Print the plot
    print(p1)
    
}


# Updated function to report on, and plot, prediction quality
assessTestData <- function(df, 
                           depVar,
                           predVar="predicted",
                           subList="testData", 
                           isRegression=FALSE, 
                           isClassification=FALSE,
                           reportOverall=TRUE,
                           reportBy=NULL, 
                           roundSig=0,
                           showPlot=TRUE, 
                           ySortVar=NULL
                           ) {
    
    # FUNCTION ARGUMENTS:
    # df: the test data file, or a list containing the test data file
    # depVar: the variable that was predicted
    # predVar: the variable containing the prediction for depVar
    # subList: if mdl is a list, attempt to pull out item named in subList
    # reportOverall: boolean, whether to report an overall RMSE/R2 on test data
    # reportBy: variable for sumarizing RMSE/R2 by (NULL means no RMSE/R2 by any grouping variables)
    # roundSig: rounding significance for numeric plotting (points at x/y will be sized by n after rounding)
    # showPlot: boolean, whether to create/show the plot of predictions vs actuals
    # ySortVar: character, if a confusion matrix is plotted, sort y high-low by percent in this category
    
    # Pull out the modeling data from the list if needed
    if ("list" %in% class(df)) {
        df <- df[[subList]]
    }

    # Report overall RMSE/R2 if regression model and requested
    if (isRegression & reportOverall) {
        cat("\nOVERALL PREDICTIVE PERFORMANCE:\n\n")
        helperCalculateRMSER2(df, depVar=depVar, predVar=predVar) %>%
            print()
        cat("\n")
    }    

    # Report by grouping variables if any provided and regression model
    if (!is.null(reportBy) & isRegression) {
        cat("\nPREDICTIVE PERFORMANCE BY GROUP(S):\n\n")
        sapply(reportBy, FUN=function(x) { 
            df %>% 
                group_by_at(x) %>% 
                helperCalculateRMSER2(depVar=depVar, predVar=predVar) %>%
                print()
            }
        )
        cat("\n")
    }

    # Report accuracy by grouping variable and overall if reportBy and classification model
    if (!is.null(reportBy) & isClassification) {
        dfPlot <- df %>%
            mutate(isCorrect=(get(depVar)==get(predVar)))
        overallAcc <- mean(dfPlot$isCorrect)
        p1 <- dfPlot %>%
            group_by_at(vars(all_of(reportBy))) %>%
            summarize(pctCorrect=mean(isCorrect)) %>%
            ggplot(aes(x=fct_reorder(get(reportBy), pctCorrect), y=pctCorrect)) + 
            geom_col(fill="lightblue") + 
            geom_text(aes(y=pctCorrect+0.02, label=paste0(round(100*pctCorrect), "%")), hjust=0) +
            coord_flip() + 
            geom_hline(aes(yintercept=overallAcc), lty=2) + 
            annotate("text", 
                     x=2, 
                     y=overallAcc+0.02, 
                     label=paste0("Overall Accuracy:\n", round(100*overallAcc, 1), "%"), 
                     hjust=0
                     ) +
            labs(title="Accuracy of Predictions by Actual", x="Actual", y="Percent Accurately Predicted") + 
            ylim(c(0, 1.05))
        print(p1)
    }
    
    # Plot numerical summary
    if (showPlot & isRegression) {
        helperPlotTestContinuous(df, predVar=predVar, depVar=depVar, roundSig=roundSig)
    }
    
    # Plot categorical summary
    if (showPlot & isClassification) {
        helperConfusion(df, actual=depVar, predVar=predVar, ySortVar=ySortVar)
    }
    
}


# Test for regression data
assessTestData(xgbInit_002, depVar="TempF", reportBy="locNamefct", isRegression=TRUE)

# Test for classification data - single-class
assessTestData(xgb_las2016_002, depVar="isLAS", isClassification=TRUE)

# Test for classification data - multi-class
assessTestData(xgbFourLocales_002, depVar="locNamefct", isClassification=TRUE)

```
  
Next, a function is written to assess prediction quality by confidence (applicable only for classification):  
```{r}

# Function to plot predictuon accuracy vs. prediction confidence
plotPredictionConfidencevQuality <- function(df, 
                                             depVar,
                                             predVar="predicted", 
                                             probVar="probPredicted", 
                                             subList="testData", 
                                             roundProb=0.05, 
                                             dataLim=1
                                             ) {
    
    # FUNCTION ARGUMENTS:
    # df: the test data file, or a list containing the test data file
    # depVar: the variable that was predicted
    # predVar: the variable containing the prediction for depVar
    # probVar: the variable containing the probability associated to the prediction
    # subList: if mdl is a list, attempt to pull out item named in subList
    # roundProb: round predicted probabilities to the nearest this
    # dataLim: only report for points with at least this quantity of data
    
    # Pull out the modeling data from the list if needed
    if ("list" %in% class(df)) {
        df <- df[[subList]]
    }
    
    # Pull a main plotting database together
    plotData <- df %>%
        mutate(prob=round(get(probVar)/roundProb)*roundProb, 
               correct=get(depVar)==get(predVar)
               ) %>%
        count(prob, correct) %>%
        mutate(nCorrect=ifelse(correct, n, 0)) %>%
        group_by(prob) %>%
        summarize(n=sum(n), nCorrect=sum(nCorrect)) %>%
        mutate(pctCorrect=nCorrect/n) %>%
        ungroup()
    
    # Create overall plot of accuracy by predicted probability
    p1 <- plotData %>%
        filter(n >= dataLim) %>%
        ggplot(aes(x=prob, y=pctCorrect)) + 
        geom_point(aes(size=n)) + 
        geom_text(aes(y=pctCorrect-0.04, label=paste0(round(100*pctCorrect, 1), "%\n(n=", n, ")"))) + 
        geom_abline(lty=2) + 
        labs(x="Probability Predicted", y="Percent Correct", title="Accuracy vs. Probability Predicted")
    print(p1)
    
}

# Assess for classification data - single-class
plotPredictionConfidencevQuality(xgb_las2016_002, depVar="isLAS")

# Assess for classification data - multi-class
plotPredictionConfidencevQuality(xgbFourLocales_002, depVar="locNamefct", dataLim=5)

```
  
Functions can then be written to assess model quality on data that is not yet part of the test dataset.  This requires as inputs a trained model and a dataset that can be converted to the same variables as the trained model.  The process is currently implemented only for classification:  
```{r}

# Function to apply predictions to other data
assessNonModelDataPredictions <- function(mdl, 
                                          df, 
                                          depVar,
                                          predVars, 
                                          yLevels, 
                                          ySortVar=NULL, 
                                          diagnose=FALSE
                                          ) {
    
    # FUNCTION ARGUMENTS:
    # mdl: the trained model
    # df: the data file for predictions
    # depVar: the actual category as contained in df
    # predVars: the variable containing the prediction for depVar
    # yLevels: the levels that the prediction can be made to (sort order is important)
    # ySortVar: character, if a confusion matrix is plotted, sort y high-low by percent in this category    
    #           NULL means no factor reordering, plot as-is
    # diagnose: boolean, whether to print mdl and dfSparse to help with debugging
    
    # Create the sparse data
    dfSparse <- df %>%
        mutate_if(is.factor, .funs=fct_drop) %>%
        helperMakeSparse(predVars=predVars)
    
    # Debugging, if needed
    if (diagnose) {
        print(mdl$feature_names)
        print(colnames(dfSparse))
    }
    
    # Create the predictions
    predsList <- helperXGBPredict(mdl=mdl, 
                                  dfSparse=dfSparse, 
                                  objective=mdl$params$objective, 
                                  probMatrix=TRUE, 
                                  yLevels=yLevels
                                  )
    
    # Add the predictions and associated probabilities to df
    df <- df %>%
        mutate(predicted=predsList$predData$predicted, 
               probPredicted=predsList$predData$probPredicted
               )
    
    # Plot the confusion matrix
    helperConfusion(df, actual=depVar, predVar="predicted", ySortVar=ySortVar)
    
    # Return the data frame
    df
    
}

# Assess for classification data - single-class
df_las2016_002 <- assessNonModelDataPredictions(mdl=xgb_las2016_002$xgbModel, 
                                                df=filter(metarData, !is.na(TempF), year!=2016), 
                                                depVar="locNamefct", 
                                                predVars=locXGBPreds, 
                                                yLevels=xgb_las2016_002$yTrainLevels, 
                                                ySortVar="Las Vegas"
                                                )

# Assess for classification data - multi-class
df_FourLocales_002 <- assessNonModelDataPredictions(mdl=xgbFourLocales_002$xgbModel, 
                                                    df=filter(metarData, !is.na(TempF), year==2016), 
                                                    depVar="locNamefct", 
                                                    predVars=locXGBPreds, 
                                                    yLevels=xgbFourLocales_002$yTrainLevels, 
                                                    ySortVar="Chicago, IL"
                                                    )

```
  
Next steps are to better organize the functions so the process can be run from start to finish for a specific data set and modeling technique.

Suppose for example that the goal is to predict the dew point based on the other information in a METAR file.  The function xgbRunModel_002() will achieve this by integrating all of the prediction functions:  
```{r cache=TRUE}

# Define key predictor variables for base XGB runs
dewXGBPreds <- c("locNamefct", "month", "hrfct", 
                 "TempF", "modSLP", "Altimeter", "WindSpeed", 
                 "predomDir", "minHeight", "ceilingHeight"
                 )

# Core multi-year cities
multiYearLocales <- c("Las Vegas, NV", "New Orleans, LA", "Chicago, IL", "San Diego, CA")

# Run the function shell
xgb_dewpoint_cv <- xgbRunModel_002(metarData, 
                                   depVar="DewF", 
                                   predVars=dewXGBPreds, 
                                   otherVars=keepVarFull, 
                                   critFilter=list(locNamefct=multiYearLocales),
                                   critFilterNot=list(year=2016),
                                   seed=2008141315,
                                   nrounds=500,
                                   print_every_n=50, 
                                   xgbObjective="reg:squarederror", 
                                   funcRun=xgboost::xgb.cv, 
                                   nfold=5
                                   )

```
  
An assesment of the train-test error can be performed:  
```{r}

plotXGBEvolution(xgb_dewpoint_cv, isRegression=TRUE, label_every=25, yLim=c(0, NA), show_line=FALSE)

```
  
Test error is fairly stable after 200 rounds, at just under 6 degrees.  The full model can then be run using 200 rounds:  
```{r cache=TRUE}

# Run the function shell
xgb_dewpoint <- xgbRunModel_002(metarData, 
                                depVar="DewF", 
                                predVars=dewXGBPreds, 
                                otherVars=keepVarFull, 
                                critFilter=list(locNamefct=multiYearLocales),
                                critFilterNot=list(year=2016),
                                seed=2008141325,
                                nrounds=200,
                                print_every_n=50, 
                                xgbObjective="reg:squarederror", 
                                funcRun=xgboost::xgboost
                                )

```
  
The full series of assessment functions can then be run:  
```{r}

# ASSESSMENT 1: Variable Importance
xgb_dewpoint_importance <- plotXGBImportance(xgb_dewpoint, 
                                             featureStems=dewXGBPreds, 
                                             stemMapper = varMapper, 
                                             plotTitle="Gain by variable in xgboost", 
                                             plotSubtitle="Dewpoint (four key locales modeled)"
                                             )

# ASSESSMENT 2: Evolution of training error
plotXGBEvolution(xgb_dewpoint, isRegression=TRUE, label_every=10, yLim=c(0, NA), show_line=FALSE)

# ASSESSMENT 3: Assess performance on test dataset
assessTestData(xgb_dewpoint, depVar="DewF", reportBy="locNamefct", isRegression=TRUE)

# ASSESSMENT 4: Prediction quality vs. confidence (currently only implemented for classification)

# ASSESSMENT 5: Performance on other data (currently only implemented for classification)

```
  
The model primarily estimates dewpoint based on temperature and locale.  Predictions are most accurate for Chicago and New Orleans, and least accurate for Las Vegas.  There does not appear to be much learning that can generalize to unseen data after the first hundred rounds.
  
The process is repeated, with an attempt made to model the ceiling height for the four key locales.  The minimum cloud height is included, which may be a leak:  
```{r cache=TRUE}

# Define key predictor variables for base XGB runs
ceilXGBPreds <- c("locNamefct", "month", "hrfct", 
                  "TempF", "DewF", "modSLP", "Altimeter", "WindSpeed", 
                  "predomDir", "minHeight"
                  )

# Core multi-year cities
multiYearLocales <- c("Las Vegas, NV", "New Orleans, LA", "Chicago, IL", "San Diego, CA")

# Ceiling types
ceilTypes <- sort(as.character(unique(metarData$ceilingHeight)))
ceilTypes

# Run the function shell
xgb_ceiling_cv <- xgbRunModel_002(metarData, 
                                  depVar="ceilingHeight", 
                                  predVars=ceilXGBPreds, 
                                  otherVars=keepVarFull, 
                                  critFilter=list(locNamefct=multiYearLocales),
                                  critFilterNot=list(year=2016),
                                  seed=2008141340,
                                  nrounds=500,
                                  print_every_n=50, 
                                  xgbObjective="multi:softprob", 
                                  funcRun=xgboost::xgb.cv, 
                                  nfold=5, 
                                  num_class=length(ceilTypes)
                                  )

```
  
An assesment of the train-test error can be performed:  
```{r}

plotXGBEvolution(xgb_ceiling_cv, isRegression=FALSE, yLim=c(0, NA), show_line=TRUE, label_every=NULL)

```
  
Test error is fairly stable after 200 rounds, at just under 20%.  The full model is run using 200 rounds:  
```{r cache=TRUE}

# Run the function shell
xgb_ceiling <- xgbRunModel_002(metarData, 
                               depVar="ceilingHeight", 
                               predVars=ceilXGBPreds, 
                               otherVars=keepVarFull, 
                               critFilter=list(locNamefct=multiYearLocales),
                               critFilterNot=list(year=2016),
                               seed=2008141350,
                               nrounds=200,
                               print_every_n=50, 
                               xgbObjective="multi:softprob", 
                               funcRun=xgboost::xgboost, 
                               num_class=length(ceilTypes)
                               )

```
  
The full series of assessment functions can then be run:  
```{r}

# ASSESSMENT 1: Variable Importance
xgb_ceiling_importance <- plotXGBImportance(xgb_ceiling, 
                                            featureStems=ceilXGBPreds, 
                                            stemMapper = varMapper, 
                                            plotTitle="Gain by variable in xgboost", 
                                            plotSubtitle="Ceiling Height (four key locales modeled)"
                                            )

# ASSESSMENT 2: Evolution of training error
plotXGBEvolution(xgb_ceiling, isRegression=FALSE, label_every=NULL, yLim=c(0, NA), show_line=TRUE)

# ASSESSMENT 3: Assess performance on test dataset
assessTestData(xgb_ceiling, depVar="ceilingHeight", isClassification=TRUE)

# ASSESSMENT 4: Prediction quality vs. confidence (currently only implemented for classification)
plotPredictionConfidencevQuality(xgb_ceiling, depVar="ceilingHeight", dataLim=5)

# ASSESSMENT 5: Performance on other data (currently only implemented for classification)
df_ceiling_pred <- assessNonModelDataPredictions(mdl=xgb_ceiling$xgbModel, 
                                                 df=filter(metarData, 
                                                           !is.na(TempF), 
                                                           year==2016, 
                                                           locNamefct %in% fourLocales
                                                           ), 
                                                 depVar="ceilingHeight", 
                                                 predVars=ceilXGBPreds, 
                                                 yLevels=xgb_ceiling$yTrainLevels, 
                                                 ySortVar="None"
                                                 )

# Additional assessments
xgb_ceiling$testData %>%
    mutate(correct=(ceilingHeight==predicted)) %>%
    count(ceilingHeight, correct) %>%
    ggplot(aes(x=ceilingHeight, y=n, fill=correct)) + 
    geom_col(position="stack")

xgb_ceiling$testData %>%
    mutate(correct=(ceilingHeight==predicted)) %>%
    count(ceilingHeight, correct) %>%
    ggplot(aes(x=ceilingHeight, y=n, fill=correct)) + 
    geom_col(position="fill")

xgb_ceiling$testData %>%
    mutate(correct=(ceilingHeight==predicted)) %>%
    count(minHeight, correct) %>%
    ggplot(aes(x=minHeight, y=n, fill=correct)) + 
    geom_col(position="stack")

xgb_ceiling$testData %>%
    mutate(correct=(ceilingHeight==predicted)) %>%
    count(minHeight, correct) %>%
    ggplot(aes(x=minHeight, y=n, fill=correct)) + 
    geom_col(position="fill")

xgb_ceiling$testData %>%
    mutate(correct=(ceilingHeight==predicted)) %>%
    count(minHeight, predicted) %>%
    group_by(minHeight) %>%
    mutate(pct=n/sum(n)) %>%
    ggplot(aes(x=minHeight, y=predicted)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=paste0(round(100*pct), "%"))) + 
    scale_fill_continuous(low="white", high="green")

```
  
The model prediction probabilities are well aligned with the model percent of correct predictions.  Minimum cloud height is, as expected, a key driver of ceiling height, followed by temperature, dewpoint, and locale.  

Due to significant class imbalance, overall prediction accuracy is high even though prediction accuracy is poor for cases where the ceiling height is medium or high.  The model generally does well in classifying Surface ceilings and None (cases of no ceilings).  This is driven by the somewhat trivial prediction that there is no ceiling when there are no clouds.  Prediction accuracies are about 75% in the other cases where there are at least some clouds.  In general, the model will either predict minHeight or None as the ceiling.

Next, the process is run to classify each of the locales in the 2016 data:  
```{r cache=TRUE}

# Extract the locales that are available in 2016
locs2016 <- metarData %>%
    filter(year==2016) %>%
    pull(locNamefct) %>%
    as.character() %>%
    unique() %>%
    sort()

# Run the function shell for CV
xgb_alllocales_cv <- xgbRunModel_002(metarData, 
                                     depVar="locNamefct", 
                                     predVars=locXGBPreds, 
                                     otherVars=keepVarFull, 
                                     critFilter=list(year=2016),
                                     seed=2008151247,
                                     nrounds=250,
                                     print_every_n=10, 
                                     xgbObjective="multi:softmax", 
                                     funcRun=xgboost::xgb.cv, 
                                     nfold=5, 
                                     num_class=length(locs2016)
                                     )

# Plot the error evolution
plotXGBEvolution(xgb_alllocales_cv, 
                 isRegression=FALSE, 
                 yLim=c(0, NA), 
                 show_line=TRUE, 
                 label_every=NULL, 
                 first_iter_plot = 1
                 )

```
  
The function can then be run for 250 rounds:  
```{r cache=TRUE}

# Run the function shell
xgb_alllocales <- xgbRunModel_002(metarData, 
                                  depVar="locNamefct", 
                                  predVars=locXGBPreds, 
                                  otherVars=keepVarFull, 
                                  critFilter=list(year=2016),
                                  seed=2008151315,
                                  nrounds=250,
                                  print_every_n=10, 
                                  xgbObjective="multi:softprob", 
                                  funcRun=xgboost::xgboost, 
                                  num_class=length(locs2016)
                                  )

```
  
And the evaluation functions can then be run:  
```{r}

# ASSESSMENT 1: Variable Importance
xgb_alllocales_importance <- plotXGBImportance(xgb_alllocales, 
                                               featureStems=locXGBPreds, 
                                               stemMapper = varMapper, 
                                               plotTitle="Gain by variable in xgboost", 
                                               plotSubtitle="Locale (2016)"
                                               )

# ASSESSMENT 2: Evolution of training error
plotXGBEvolution(xgb_alllocales, isRegression=FALSE, label_every=NULL, yLim=c(0, NA), show_line=TRUE)

# ASSESSMENT 3: Assess performance on test dataset
assessTestData(xgb_alllocales, depVar="locNamefct", reportBy="locNamefct", isClassification=TRUE)

# ASSESSMENT 4: Prediction quality vs. confidence (currently only implemented for classification)
plotPredictionConfidencevQuality(xgb_alllocales, depVar="locNamefct", dataLim=5)

# ASSESSMENT 5: Performance on other data (currently only implemented for classification)
df_alllocales_pred <- assessNonModelDataPredictions(mdl=xgb_alllocales$xgbModel, 
                                                    df=filter(metarData, 
                                                              !is.na(TempF), 
                                                              year!=2016, 
                                                              locNamefct %in% fourLocales
                                                              ), 
                                                    depVar="locNamefct", 
                                                    predVars=locXGBPreds, 
                                                    yLevels=xgb_alllocales$yTrainLevels, 
                                                    ySortVar="None"
                                                    )

```
  
The model is reasonably effective at classifying many of the more "distinct" locales, but struggles with the cold weather locales like Chicago.  Next steps are to build on the previous one vs. all approach to see if that can speed up the process or enhance the predictive power (overall or for trickier locales).
  
Two approaches are taken to model individual locales in 2016:  
  
* Locale vs. all-other, with under-sampling of all-other so that classes are balanced  
* Locale vs. locale for a specific city pairing  
  
Test data can then be run through the locale vs. all-other algorithms in an attempt at classification.  Close calls can be further run through the relevant locale vs. locale classification.  Example code includes:  
```{r}

# Create the overall test and train data
fullDataSplit <- createTestTrain(filter(metarData, year==2016, !is.na(TempF)), 
                                 noNA=FALSE, 
                                 seed=2008161312
                                 )

# Extract the training data for the remainder of the process
localeTrainData <- fullDataSplit$trainData

```
  
The models for one vs. all can then be run and cached:  
```{r cache=TRUE}

# Create the locations to be modeled
useLocs <- locs2016

# Create a container for storing all relevant objects
localeOnevAll <- vector("list", length(useLocs))

# Run the modeling process once for each locale
n <- 1
for (thisLoc in useLocs) {
    
    # Announce the progress
    cat("\nProcess for:", thisLoc, "\n")
    
    # Create the training data to use for the model
    thisTrain <- localeTrainData %>%
        mutate(curLocale=factor(ifelse(locNamefct==thisLoc, thisLoc, "All Other"), 
                                levels=c(thisLoc, "All Other")
                                )
               )
    
    # Find the samllest group size
    smallN <- thisTrain %>%
        count(curLocale) %>%
        pull(n) %>%
        min()
    
    # Balance the samples in thisTrain
    thisTrain <- thisTrain %>%
        group_by(curLocale) %>%
        sample_n(smallN) %>%
        ungroup()
    
    # Run the CV process with a callback for early stopping if 5 iterations show no improvement
    xgb_thislocale_cv <- xgbRunModel_002(thisTrain, 
                                         depVar="curLocale", 
                                         predVars=locXGBPreds, 
                                         otherVars=keepVarFull, 
                                         seed=2008161330+n,
                                         nrounds=500,
                                         print_every_n=500, 
                                         xgbObjective="multi:softmax", 
                                         funcRun=xgboost::xgb.cv, 
                                         nfold=5, 
                                         num_class=2, 
                                         early_stopping_rounds=5
                                         )

    # The best iteration can then be pulled
    bestN <- xgb_thislocale_cv$xgbModel$best_iteration
    
    # And the model can be run for that number of iterations
    xgb_thislocale <- xgbRunModel_002(thisTrain, 
                                      depVar="curLocale", 
                                      predVars=locXGBPreds, 
                                      otherVars=keepVarFull, 
                                      seed=2008161330+100+n,
                                      nrounds=bestN,
                                      print_every_n=500, 
                                      xgbObjective="multi:softprob", 
                                      funcRun=xgboost::xgboost, 
                                      num_class=2
                                      )
    
    
    # Place the trained CV object in the relevant list
    localeOnevAll[[n]] <- list(cvResult=xgb_thislocale_cv, 
                               mdlResult=xgb_thislocale, 
                               bestN=bestN
                               )
    
    # Increment the counter
    n <- n + 1
    
}


# Name the list
names(localeOnevAll) <- useLocs

```
  
Error statistics can then be calculated and reported:  
```{r}

# Report on summary statistics
# Test error by CV file
cvError <- map_dfr(localeOnevAll, 
                   .f=function(x) {x$cvResult$xgbModel$evaluation_log[x$bestN, ]}, 
                   .id="locale"
                   )

# Function to extract key error data
dummyError <- function(x) {
    df <- x$mdlResult$testData %>%
        mutate(correct=ifelse(predicted==curLocale, 1, 0), 
               self=ifelse(curLocale=="All Other", 0, 1), 
               pctCorrectOverall=mean(correct)
               ) %>%
        group_by(self) %>%
        summarize(pctCorrectGroup=mean(correct), pctCorrectOverall=mean(pctCorrectOverall)) %>%
        ungroup()
}

# Test error by model file
mdlError <- map_dfr(localeOnevAll, 
                    .f=dummyError,
                    .id="locale"
                    )

# Plot of error rates - overall for CV and model
mdlError %>%
    inner_join(cvError) %>%
    filter(self==1) %>%
    mutate(pctCorrectCV=1-test_merror_mean) %>%
    select(locale, Overall=pctCorrectOverall, CV=pctCorrectCV) %>%
    pivot_longer(-locale) %>%
    ggplot(aes(x=fct_reorder(locale, value), y=value)) + 
    geom_point(aes(color=name)) + 
    coord_flip() + 
    labs(y="Test Accuracy", title="CV and Model Accuracy on Test Data", x="") + 
    scale_color_discrete("Model Type") + 
    ylim(c(0, 1))

# Plot of error rates - self vs. other for model
mdlError %>%
    ggplot(aes(x=fct_reorder(locale, pctCorrectOverall), y=pctCorrectGroup)) + 
    geom_point(aes(color=factor(self))) + 
    coord_flip() + 
    labs(y="Test Accuracy", title="Model Accuracy on Test Data", subtitle="Self=1, All Other=0", x="") + 
    scale_color_discrete("Classifying Self?") + 
    ylim(c(0, 1))


```
  
CV errors are generally well aligned with modeling errors, as expected.  As seen previously, the model is generally more successful in classifying self and less successful in classifying all other.  Suppose that just this model were used to make predictions for all of the test data:  
```{r}

# Prepare the test data
testData <- fullDataSplit$testData %>%
    select_at(vars(all_of(c(locXGBPreds, keepVarFull)))) %>%
    filter(locNamefct %in% useLocs) %>%
    mutate_if(is.factor, .funs=fct_drop)

# Function to create the probabilities by locale
predictLocaleProbs <- function(x) {
    helperXGBPredict(x$mdlResult$xgbModel, 
                     dfSparse=helperMakeSparse(testData, 
                                               depVar="locNamefct", 
                                               predVars=locXGBPreds
                                               ), 
                     objective="multi:softprob", 
                     probMatrix=TRUE, 
                     yLevels=x$mdlResult$yTrainLevels
                     )$probData %>%
        select(-`All Other`)
}

# Extract from list
locData <- testData %>%
    select(locNamefct, source, dtime) %>%
    bind_cols(map_dfc(localeOnevAll, .f=predictLocaleProbs))

# Convert to prediction based purely on maximum probability
sampPreds <- locData %>%
    mutate(record=row_number()) %>%
    pivot_longer(-c(record, locNamefct, source, dtime)) %>%
    group_by(record, locNamefct, source, dtime) %>%
    filter(value==max(value)) %>%
    ungroup() %>%
    mutate(correct=(as.character(locNamefct)==name))

# Overall accuracy by locale
sampPreds %>%
    group_by(locNamefct) %>%
    summarize(pctCorrect=mean(correct), n=n()) %>%
    ggplot(aes(x=fct_reorder(locNamefct, pctCorrect), y=pctCorrect)) + 
    geom_point() + 
    geom_text(aes(y=pctCorrect+0.02, label=paste0(round(100*pctCorrect), "%")), hjust=0, size=3.5) + 
    coord_flip() + 
    labs(x="Actual Locale", y="Percent Correctly Predicted", 
         title="Predictions based on Maximum Probability"
         ) + 
    ylim(c(0, 1.1))

# Confusion matrix
sampPreds %>%
    mutate(name=factor(name)) %>%
    count(locNamefct, name) %>%
    group_by(locNamefct) %>%
    mutate(pct=n/sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=locNamefct, y=name)) + 
    geom_tile(aes(fill=pct)) + 
    geom_text(aes(label=paste0(round(100*pct), "%")), size=2) + 
    coord_flip() + 
    labs(x="Actual Locale", y="Predicted Locale", 
         title="Predictions based on Maximum Probability"
         ) + 
    scale_fill_continuous(low="white", high="lightgreen") + 
    theme(axis.text.x=element_text(angle=90, hjust=1))

```
  
There are meaningful classification issues with the base predictor.  Next steps are to find cases where a city has multiple "likely" predictions and to use one vs. one modeling to help make those decisions.
  
Each predicted locale has an associated probability that can be used to help model the data.  For example, a look at twenty random records shows:  
```{r}

# Initialize a seed so that the random sampling is consistent
set.seed(2008171324)

# Take 20 random records and see the assigned probabilities
locData %>%
    mutate(record=row_number()) %>%
    sample_n(size=20) %>%
    pivot_longer(-c(record, locNamefct, source, dtime)) %>%
    ggplot(aes(x=paste0(locNamefct, " - ", record), y=factor(name))) + 
    geom_tile(aes(fill=value)) + 
    geom_text(aes(label=paste0(round(100*value), "%")), size=2.5) + 
    coord_flip() + 
    labs(x="Actual Locale", y="Potential Locale", title="Probability Associated to Potential Locale") + 
    theme(axis.text.x=element_text(angle=90, hjust=1)) + 
    scale_fill_continuous(low="white", high="green")

```
  
It is common for a locale to have decent potential matches to several potential locales, and the highest probability is not always associated to the best prediction.  Suppose that for every actual locale, the following statistics are gathered:  
  
* Highest predicted probability  
* Second highest predicted probability  
* Accuracy of highest predicted probability locale  
* Predicted probability of correct locale  
  
```{r}

# Calculate key data for every prediction
predStats <- locData %>%
    mutate(record=row_number()) %>%
    pivot_longer(-c(record, locNamefct, source, dtime)) %>%
    mutate(correct=(locNamefct==name)) %>%
    group_by(record, locNamefct, source, dtime) %>%
    arrange(record, locNamefct, source, dtime, -value) %>%
    summarize(highProb=nth(value, 1), 
              nextProb=nth(value, 2), 
              accHigh=nth(correct, 1),
              actualLocaleProb=sum(ifelse(correct, value, 0))
              ) %>%
    ungroup()

```
  
Summaries by locale can then be generated, to answer questions such as:
  
* How often is the actual locale the highest prediction or the second highest prediction?  
* What is the distribution of the probabilities associated to prediction of actual locale?  
* What is the distribution of difference in probability between highest predicted probability and prediction of actual locale?  
  
```{r}

# Frequency by Prediction Caliber
predStats %>%
    mutate(isFirst=as.integer(accHigh), 
           isSecond=pmax(0, (actualLocaleProb==nextProb)-isFirst), 
           isClose=pmax(0, (actualLocaleProb >= highProb-0.25)-isFirst-isSecond)
           ) %>% 
    select(record, locNamefct, isFirst, isSecond, isClose) %>%
    pivot_longer(-c(record, locNamefct), names_to="type", values_to="boolean") %>%
    mutate(type=factor(type, levels=c("isClose", "isSecond", "isFirst"))) %>%
    group_by(locNamefct, type) %>%
    summarize(pct=mean(boolean)) %>%
    ggplot(aes(x=fct_reorder(locNamefct, pct, .fun=sum), y=pct)) + 
    geom_col(aes(fill=type), position="stack") + 
    coord_flip() + 
    labs(x="Actual Locale", y="Percentage of Predictions", title="Prediction Caliber by Actual Locale") + 
    theme(legend.position="bottom") + 
    scale_fill_discrete("Prediction Caliber", 
                         breaks=c("isFirst", "isSecond", "isClose"), 
                         labels=c("Highest Prob", "Second Highest Prob", "Prob Within 25%")
                         )

# Histogram of Prediction Probabilities by Actual Locale
predStats %>%
    ggplot(aes(x=actualLocaleProb)) + 
    geom_histogram(fill="lightblue") + 
    facet_wrap(~locNamefct) + 
    labs(x="", y="n", title="Predicted Probability for Actual Locale")

# Histogram of Highest Probability minus Correct Locale Probability by Actual Locale
predStats %>%
    ggplot(aes(x=actualLocaleProb-highProb)) + 
    geom_histogram(fill="lightblue") + 
    facet_wrap(~locNamefct) + 
    labs(x="", 
         y="n", 
         title="Error in Predicted Probability for Actual Locale", 
         subtitle="Error is probability for actual locale minus highest probability for any locale"
         )

```
  
For some of the locales, the predicted probability is very frequently either the highest probability prediction or close to the highest probability prediction.  These locales may be further enhanced by exploring the one vs one model when there is more than one high probability prediction to this subset of locales.

For other locales, the predicted probability is frequently middling, which creates challenges in pulling apart the observation from other locales.  In particular, this challenge is observed for the four-season locales, which can be misclassified as each other and also as a different archetype during a peak and spiky season.

Further exploration can be made for prediction quality given the highest probability and the difference in the highest probability and the second highest probability:  
```{r}

roundStats <- predStats %>%
    mutate(roundProb=round(20*highProb)/20, 
           deltaProb=round(10*(highProb-nextProb))/10
           ) 

roundStats %>%
    group_by(roundProb, deltaProb) %>%
    summarize(n=n(), acc=mean(accHigh)) %>%
    ungroup() %>%
    filter(n >= 10) %>%
    ggplot(aes(x=factor(roundProb), y=factor(deltaProb))) + 
    geom_tile(aes(fill=acc)) + 
    geom_text(aes(label=paste0(round(100*acc), "%\n(n=", n, ")")), size=2.5) + 
    scale_fill_continuous(low="white", high="green") + 
    labs(x="Highest Predicted Probability", 
         y="Gap to Next Highest Predicted Probability", 
         title="Accuracy by Highest Probability vs. Next Highest Probability"
         )

```
  
There is a small subset of predictions where the predicted probability is high (97.5% plus) and the gap to the next highest probability is also high (25% plus).  These predictions have very high accuracy:  
```{r}

# Plot of accuracy by condition
predStats %>%
    mutate(isLikely=(highProb >= 0.975) & (highProb-nextProb >= 0.25)) %>%
    group_by(isLikely) %>%
    summarize(acc=mean(accHigh), n=n()) %>%
    ungroup() %>%
    ggplot(aes(x=isLikely, y=acc)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=acc/2, label=paste0(round(100*acc), "%\n(n=", n, ")"))) + 
    labs(x="Highest Probability >= 0.975 and Delta Probability >= 0.25?", y="Accuracy")

# Plot of accuracy by condition
predStats %>%
    mutate(isLikely=(highProb >= 0.975) & (highProb-nextProb >= 0.25)) %>%
    group_by(locNamefct, isLikely) %>%
    summarize(n=n()) %>%
    ungroup() %>%
    ggplot(aes(x=fct_reorder(locNamefct, n, .fun=min), y=n)) + 
    geom_col(aes(fill=isLikely), position="fill") + 
    labs(y="", x="") + 
    coord_flip() + 
    theme(legend.position="bottom") + 
    scale_fill_discrete("Highest Probability >= 0.975 and Delta Probability >= 0.25?", 
                        breaks=c(TRUE, FALSE)
                        )

```
  
Further work is needed to explore the one vs all predictor to see if it has potential to be expanded using one vs one predictors or if it is overly faulty and prone to making confidently inaccurate predictions,
  

