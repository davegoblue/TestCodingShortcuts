---
title: "Open Meteo Weather Exploration"
author: "davegoblue"
date: "2023-06-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
Open-Meteo maintains an [API for historical weather](https://open-meteo.com/en/docs/historical-weather-api) that allows for non-commercial usage of historical weather data maintained by the website.

This file runs exploratory analysis on some of the historical weather data.

## Exploratory Analysis
The exploration process uses tidyverse and several generic custom functions:  
```{r}

library(tidyverse) # tidyverse functionality is included throughout

source("./Generic_Added_Utility_Functions_202105_v001.R") # Basic functions

```
  
A sample of data for 365 days has been downloaded as a CSV. The downloaded data has three separate files included in a single tab, separated by a blank row. The first file is location data, the second file is hourly data, and the third file is daily data. For initial exploration, parameters specific to this file are used:  
```{r, fig.height=9, fig.width=9}

omFileLoc <- "./RInputFiles/openmeteo_20230612_example.csv"

# Location data
omLocation <- readr::read_csv(omFileLoc, n_max=1, skip=0) 
omLocation

# Hourly data 
# Elements: time, 2m temp (C), 2m dew point (C), 2m relative humidity (%), precip (mm), rain (mm), and snow (cm)
omHourlyRaw <- readr::read_csv(omFileLoc, n_max=8760, skip=3) 
omHourlyProcess <- omHourlyRaw %>%
    purrr::set_names(c("time", "temp2m_C", "relH2m", "dew2m_C", "precip_mm", "rain_mm", "snow_cm")) %>% 
    mutate(date=date(time))
omHourlyProcess
summary(omHourlyProcess)

# Daily data 
# Elements: date, sum of precip (mm), sum of rain (mm), and sum of snow (cm)
omDailyRaw <- readr::read_csv(omFileLoc, n_max=365, skip=8765) 
omDailyProcess <- omDailyRaw %>%
    purrr::set_names(c("date", "precip_mm", "rain_mm", "snow_cm"))
omDailyProcess
summary(omDailyProcess)

```
  
A function is written to read a portion of a CSV file:  
```{r, fig.height=9, fig.width=9}

partialCSVRead <- function(loc, firstRow=1L, lastRow=+Inf, col_names=TRUE, ...) {
    
    # FUNCTION arguments
    # loc: file location
    # firstRow: first row that is relevant to the partial file read (whether header line or data line)
    # last Row: last row that is relevant to the partial file read (+Inf means read until last line of file)
    # col_names: the col_names parameter passed to readr::read_csv
    #            TRUE means header=TRUE (get column names from file, read data starting on next line)
    #            FALSE means header=FALSE (auto-generate column names, read data starting on first line)
    #            character vector means use these as column names (read data starting on first line)
    # ...: additional arguments passed to read_csv

    # Read the file and return
    # skip: rows to be skipped are all those prior to firstRow
    # n_max: maximum rows read are lastRow-firstRow, with an additional data row when col_names is not TRUE
    readr::read_csv(loc, 
                    skip=firstRow-1, 
                    n_max=lastRow-firstRow+ifelse(isTRUE(col_names), 0, 1), 
                    ...
                    )
    
}

# Double check that data are the same
partialCSVRead(omFileLoc, firstRow=1L, lastRow=2L) %>% all.equal(omLocation)
partialCSVRead(omFileLoc, firstRow=4L, lastRow=8764L) %>% all.equal(omHourlyRaw)
partialCSVRead(omFileLoc, firstRow=8766L, lastRow=+Inf) %>% all.equal(omDailyRaw)

```
  
The blank lines are assessed, allowing for all tables to be read at the same time:  
```{r, fig.height=9, fig.width=9}

# Get the break points for gaps in a vector (e.g., 0, 3, 5:8, 20 has break points 0, 3, 5, 20 and 0, 3, 8, 30)
vecGaps <- function(x, addElements=c(), sortUnique=TRUE) {
    
    if(length(addElements)>0) x <- c(addElements, x)
    if(isTRUE(sortUnique)) x <- unique(sort(x))
    list("starts"=c(x[is.na(lag(x)) | x-lag(x)>1], +Inf), 
         "ends"=x[is.na(lead(x)) | lead(x)-x>1]
         )
    
}

vecGaps(c(3, 5:8, 20), addElements=0)

# Find the break points in a single file
flatFileGaps <- function(loc) {

    which(stringr::str_length(readLines(loc))==0) %>% vecGaps(addElements=0)
    
}

flatFileGaps(omFileLoc)


# Read all relevant data as CSV with header
readMultiCSV <- function(loc, col_names=TRUE, ...) {

    gaps <- flatFileGaps(loc)
    
    lapply(seq_along(gaps$ends), 
           FUN=function(x) partialCSVRead(loc, 
                                          firstRow=gaps$ends[x]+1, 
                                          lastRow=gaps$starts[x+1]-1, 
                                          col_names=col_names, 
                                          ...
                                          )
           )
    
}

tstMultiCSV <- readMultiCSV(omFileLoc)

all.equal(tstMultiCSV[[1]], omLocation)
all.equal(tstMultiCSV[[2]], omHourlyRaw)
all.equal(tstMultiCSV[[3]], omDailyRaw)

```
  
Data can also be downloaded through the Open-Meteo API, returning a JSON file. The data download has been completed off-line to minimize repeated hits against the server. The JSON file can then be read:  
```{r}

# Example download sequence
# download.file("https://archive-api.open-meteo.com/v1/archive?latitude=41.85&longitude=-87.65&start_date=2022-06-01&end_date=2023-06-08&hourly=temperature_2m,relativehumidity_2m,dewpoint_2m,precipitation,rain,snowfall&daily=precipitation_sum,rain_sum,snowfall_sum&timezone=America%2FChicago", "tempOM")

# Create hourly data tibble
jsonHourly <- jsonlite::read_json("tempOM", simplifyVector = TRUE)[["hourly"]] %>% 
    tibble::as_tibble() %>% 
    mutate(tm=lubridate::ymd_hm(time), date=date(tm))
jsonHourly

# Create daily data tibble
jsonDaily <- jsonlite::read_json("tempOM", simplifyVector = TRUE)[["daily"]] %>% 
    tibble::as_tibble()
jsonDaily

# Extract other elements
jsonNames <- jsonlite::read_json("tempOM", simplifyVector = TRUE) %>% names
for (jsonName in jsonNames[!(jsonNames %in% c("daily", "hourly", "daily_units", "hourly_units"))]) {
    cat("\n", jsonName, ":", jsonlite::read_json("tempOM", simplifyVector = TRUE)[[jsonName]])
}
for (jsonName in jsonNames[jsonNames %in% c("daily_units", "hourly_units")]) {
    cat("\n", jsonName, ":\n")
    print(jsonlite::read_json("tempOM", simplifyVector = TRUE)[[jsonName]] %>% tibble::as_tibble() %>% t())
}

```
  
Daily data read from JSON and CSV are compared:  
```{r}

# Convert variable names in JSON daily data
jsonDailyProcess <- jsonDaily %>%
    colRenamer(vecRename=c("precipitation_sum"="precip_mm", 
                           "rain_sum"="rain_mm", 
                           "snowfall_sum"="snow_cm", 
                           "time"="date"
                           )
               ) %>%
    mutate(date=as.Date(date))
jsonDailyProcess

# Check dates included
omDailyProcess %>% 
    select(date) %>% 
    mutate(inCSV=1) %>% 
    full_join(mutate(select(jsonDailyProcess, "date"), inJSON=1), by="date") %>%
    filter(!complete.cases(.))

# Check column names
all.equal(names(omDailyProcess), names(jsonDailyProcess))

# Check data elements from 2022-06-08 through 2023-06-04 (last full day of data)
all.equal(omDailyProcess %>% tibble::as_tibble() %>% filter(date>="2022-06-08", date<="2023-06-04"), 
          jsonDailyProcess %>% filter(date>="2022-06-08", date<="2023-06-04")
          )

```
  
Hourly data read from JSON and CSV are compared:  
```{r}

# Convert variable names in JSON hourly data
jsonHourlyProcess <- jsonHourly %>% 
    select(-time) %>%
    colRenamer(vecRename=c("temperature_2m"="temp2m_C", 
                           "relativehumidity_2m"="relH2m", 
                           "dewpoint_2m"="dew2m_C", 
                           "precipitation"="precip_mm", 
                           "rain"="rain_mm", 
                           "snowfall"="snow_cm",
                           "tm"="time"
                           )
               ) %>% 
    select(time, everything())
jsonHourlyProcess

# Check dates included
omHourlyProcess %>% 
    count(date, name="nCSV") %>% 
    full_join(count(jsonHourlyProcess, date, name="nJSON"), by="date") %>%
    filter(!complete.cases(.))

# Check column names
all.equal(names(omHourlyProcess), names(jsonHourlyProcess))

# Check data elements from 2022-06-08 through 2023-06-04 (last full day of data)
all.equal(omHourlyProcess %>% tibble::as_tibble() %>% filter(date>="2022-06-08", date<="2023-06-04"), 
          jsonHourlyProcess %>% filter(date>="2022-06-08", date<="2023-06-04")
          )

```
  
Metrics that can be reuested for hourly and daily data include:  
```{r}

hourlyMetrics <- "temperature_2m,relativehumidity_2m,dewpoint_2m,apparent_temperature,pressure_msl,surface_pressure,precipitation,rain,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,shortwave_radiation,direct_radiation,direct_normal_irradiance,diffuse_radiation,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m,windgusts_10m,et0_fao_evapotranspiration,weathercode,vapor_pressure_deficit,soil_temperature_0_to_7cm,soil_temperature_7_to_28cm,soil_temperature_28_to_100cm,soil_temperature_100_to_255cm,soil_moisture_0_to_7cm,soil_moisture_7_to_28cm,soil_moisture_28_to_100cm,soil_moisture_100_to_255cm"
dailyMetrics <- "weathercode,temperature_2m_max,temperature_2m_min,apparent_temperature_max,apparent_temperature_min,precipitation_sum,rain_sum,snowfall_sum,precipitation_hours,sunrise,sunset,windspeed_10m_max,windgusts_10m_max,winddirection_10m_dominant,shortwave_radiation_sum,et0_fao_evapotranspiration"

hourlyDescription <- "Air temperature at 2 meters above ground\nRelative humidity at 2 meters above ground\nDew point temperature at 2 meters above ground\nApparent temperature is the perceived feels-like temperature combining wind chill factor, relative humidity and solar radiation\nAtmospheric air pressure reduced to mean sea level (msl) or pressure at surface. Typically pressure on mean sea level is used in meteorology. Surface pressure gets lower with increasing elevation.\nAtmospheric air pressure reduced to mean sea level (msl) or pressure at surface. Typically pressure on mean sea level is used in meteorology. Surface pressure gets lower with increasing elevation.\nTotal precipitation (rain, showers, snow) sum of the preceding hour. Data is stored with a 0.1 mm precision. If precipitation data is summed up to monthly sums, there might be small inconsistencies with the total precipitation amount.\nOnly liquid precipitation of the preceding hour including local showers and rain from large scale systems.\nSnowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent\nTotal cloud cover as an area fraction\nLow level clouds and fog up to 2 km altitude\nMid level clouds from 2 to 6 km altitude\nHigh level clouds from 6 km altitude\nShortwave solar radiation as average of the preceding hour. This is equal to the total global horizontal irradiation\nDirect solar radiation as average of the preceding hour on the horizontal plane and the normal plane (perpendicular to the sun)\nDirect solar radiation as average of the preceding hour on the horizontal plane and the normal plane (perpendicular to the sun)\nDiffuse solar radiation as average of the preceding hour\nWind speed at 10 or 100 meters above ground. Wind speed on 10 meters is the standard level.\nWind speed at 10 or 100 meters above ground. Wind speed on 10 meters is the standard level.\nWind direction at 10 or 100 meters above ground\nWind direction at 10 or 100 meters above ground\nGusts at 10 meters above ground of the indicated hour. Wind gusts in CERRA are defined as the maximum wind gusts of the preceding hour. Please consult the ECMWF IFS documentation for more information on how wind gusts are parameterized in weather models.\nET0 Reference Evapotranspiration of a well watered grass field. Based on FAO-56 Penman-Monteith equations ET0 is calculated from temperature, wind speed, humidity and solar radiation. Unlimited soil water is assumed. ET0 is commonly used to estimate the required irrigation for plants.\nWeather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details. Weather code is calculated from cloud cover analysis, precipitation and snowfall. As barely no information about atmospheric stability is available, estimation about thunderstorms is not possible.\nVapor Pressure Deificit (VPD) in kilopascal (kPa). For high VPD (>1.6), water transpiration of plants increases. For low VPD (<0.4), transpiration decreases\nAverage temperature of different soil levels below ground.\nAverage temperature of different soil levels below ground.\nAverage temperature of different soil levels below ground.\nAverage temperature of different soil levels below ground.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths."
dailyDescription <- "The most severe weather condition on a given day\nMaximum and minimum daily air temperature at 2 meters above ground\nMaximum and minimum daily air temperature at 2 meters above ground\nMaximum and minimum daily apparent temperature\nMaximum and minimum daily apparent temperature\nSum of daily precipitation (including rain, showers and snowfall)\nSum of daily rain\nSum of daily snowfall\nThe number of hours with rain\nSun rise and set times\nSun rise and set times\nMaximum wind speed and gusts on a day\nMaximum wind speed and gusts on a day\nDominant wind direction\nThe sum of solar radiaion on a given day in Megajoules\nDaily sum of ET0 Reference Evapotranspiration of a well watered grass field"

# Create tibble for hourly metrics
tblMetricsHourly <- tibble::tibble(metric=hourlyMetrics %>% str_split_1(","), 
                                   description=hourlyDescription %>% str_split_1("\n")
                                   )
tblMetricsHourly %>% 
    print(n=50)

# Create tibble for daily metrics
tblMetricsDaily <- tibble::tibble(metric=dailyMetrics %>% str_split_1(","), 
                                  description=dailyDescription %>% str_split_1("\n")
                                   )
tblMetricsDaily

```

Data can then be assembled into a string that is compatible with the Open-Meteo API format:  
```{r}

openMeteoURLCreate <- function(mainURL="https://archive-api.open-meteo.com/v1/archive", 
                               lat=45, 
                               lon=-90, 
                               startDate=paste(year(Sys.Date())-1, "01", "01", sep="-"), 
                               endDate=paste(year(Sys.Date())-1, "12", "31", sep="-"), 
                               hourlyMetrics=NULL, 
                               dailyMetrics=NULL,
                               tz="GMT", 
                               ...
                               ) {
    
    # Create formatted string
    fString <- paste0(mainURL, 
                      "?latitude=", 
                      lat, 
                      "&longitude=", 
                      lon, 
                      "&start_date=", 
                      startDate, 
                      "&end_date=", 
                      endDate
                      )
    if(!is.null(hourlyMetrics)) fString <- paste0(fString, "&hourly=", hourlyMetrics)
    if(!is.null(dailyMetrics)) fString <- paste0(fString, "&daily=", dailyMetrics)
    
    # Return the formatted string
    paste0(fString, "&timezone=", stringr::str_replace(tz, "/", "%2F"), ...)
    
}

# Blank example
openMeteoURLCreate()

# Matching previous CSV data pull
openMeteoURLCreate(lat=41.85, 
                   lon=-87.65, 
                   startDate="2022-06-01", 
                   endDate="2023-06-08", 
                   hourlyMetrics="temperature_2m,relativehumidity_2m,dewpoint_2m,precipitation,rain,snowfall", 
                   dailyMetrics="precipitation_sum,rain_sum,snowfall_sum", 
                   tz="America/Chicago"
                   )

```
  
A helper function is created to convert cities to lat/lon and to allow for selection of hourly and daily metrics by index number:  
```{r}

helperOpenMeteoURL <- function(cityName=NULL,
                               lat=NULL,
                               lon=NULL,
                               hourlyMetrics=NULL,
                               hourlyIndices=NULL,
                               hourlyDesc=tblMetricsHourly,
                               dailyMetrics=NULL,
                               dailyIndices=NULL,
                               dailyDesc=tblMetricsDaily,
                               startDate=NULL, 
                               endDate=NULL, 
                               tz=NULL,
                               ...
                               ) {
    
    # Convert city to lat/lon if lat/lon are NULL
    if(is.null(lat) | is.null(lon)) {
        if(is.null(cityName)) stop("\nMust provide lat/lon or city name available in maps::us.cities\n")
        cityData <- maps::us.cities %>% tibble::as_tibble() %>% filter(name==cityName)
        if(nrow(cityData)!=1) stop("\nMust provide city name that maps uniquely to maps::us.cities$name\n")
        lat <- cityData$lat[1]
        lon <- cityData$long[1]
    }
    
    # Get hourly metrics by index if relevant
    if(is.null(hourlyMetrics) & !is.null(hourlyIndices)) {
        hourlyMetrics <- hourlyDesc %>% slice(hourlyIndices) %>% pull(metric)
        hourlyMetrics <- paste0(hourlyMetrics, collapse=",")
        cat("\nHourly metrics created from indices:", hourlyMetrics, "\n\n")
    }
    
    # Get daily metrics by index if relevant
    if(is.null(dailyMetrics) & !is.null(dailyIndices)) {
        dailyMetrics <- dailyDesc %>% slice(dailyIndices) %>% pull(metric)
        dailyMetrics <- paste0(dailyMetrics, collapse=",")
        cat("\nDaily metrics created from indices:", dailyMetrics, "\n\n")
    }
    
    # Use default values from OpenMeteoURLCreate() for startDate, endDate, and tz if passed as NULL
    if(is.null(startDate)) startDate <- eval(formals(openMeteoURLCreate)$startDate)
    if(is.null(endDate)) endDate <- eval(formals(openMeteoURLCreate)$endDate)
    if(is.null(tz)) tz <- eval(formals(openMeteoURLCreate)$tz)
    
    # Create and return URL
    openMeteoURLCreate(lat=lat,
                       lon=lon, 
                       startDate=startDate, 
                       endDate=endDate, 
                       hourlyMetrics=hourlyMetrics, 
                       dailyMetrics=dailyMetrics, 
                       tz=tz,
                       ...
                       )
    
}

```
  
The URL is tested for file download, cached to avoid multiple hits to the server:  
```{r cache=TRUE}

testURL <- helperOpenMeteoURL(cityName="Chicago IL", 
                              hourlyIndices=c(1:3, 7:9),
                              dailyIndices=6:8,
                              startDate="2022-06-01", 
                              endDate="2023-06-08", 
                              tz="America/Chicago"
                              )
testURL

# Download file
if(!file.exists("notuse_testOM.json")) {
    fileDownload(fileName="notuse_testOM.json", url=testURL)
} else {
    cat("\nFile notuse_testOM.json already exists, skipping download\n")
}

```
  
Code is created to read the JSON return object:  
```{r}

readOpenMeteoJSON <- function(js) {
    
    # FUNCTION arguments: 
    # js: JSON list returned by download from Open-Meteo
    
    # Get the object and names
    jsObj <- jsonlite::read_json(js, simplifyVector = TRUE)
    nms <- jsObj %>% names()
    cat("\nObjects in JSON include:", paste(nms, collapse=", "), "\n\n")
    
    # Set default objects as NULL
    tblDaily <- NULL
    tblHourly <- NULL
    tblUnitsDaily <- NULL
    tblUnitsHourly <- NULL
    
    # Get daily and hourly as tibble if relevant
    if("daily" %in% nms) tblDaily <- jsObj$daily %>% tibble::as_tibble()
    if("hourly" %in% nms) tblHourly <- jsObj$hourly %>% tibble::as_tibble()
    
    # Helper function for unit conversions
    helperMetricUnit <- function(x, mapper, desc) {
        x %>% 
            tibble::as_tibble() %>% 
            pivot_longer(cols=everything()) %>% 
            left_join(mapper, by=c("name"="metric")) %>% 
            mutate(value=stringr::str_replace(value, "\u00b0", "deg ")) %>% 
            mutate(metricType=desc) %>% 
            select(metricType, everything())
    }
    
    # Get the unit descriptions
    if("daily_units" %in% nms) 
        tblUnitsDaily <- helperMetricUnit(jsObj$daily_units, tblMetricsDaily, desc="daily_units")
    if("hourly_units" %in% nms) 
        tblUnitsHourly <- helperMetricUnit(jsObj$hourly_units, tblMetricsHourly, desc="hourly_units")
    if(is.null(tblUnitsDaily) & !is.null(tblUnitsHourly)) tblUnits <- tblUnitsHourly
    else if(!is.null(tblUnitsDaily) & is.null(tblUnitsHourly)) tblUnits <- tblUnitsDaily
    else if(!is.null(tblUnitsDaily) & !is.null(tblUnitsHourly)) 
        tblUnits <- bind_rows(tblUnitsHourly, tblUnitsDaily)
    else tblUnits <- NULL
    
    # Put everything else together
    tblDescription <- jsObj[setdiff(nms, c("hourly", "hourly_units", "daily", "daily_units"))] %>%
        tibble::as_tibble()
    
    # Return the list objects
    list(tblDaily=tblDaily, tblHourly=tblHourly, tblUnits=tblUnits, tblDescription=tblDescription)
    
}

prettyOpenMeteoMeta <- function(df, extr="tblDescription") {
    if("list" %in% class(df)) df <- df[[extr]]
    for(name in names(df)) {
        cat("\n", name, ": ", df %>% pull(name), sep="")
    }
    cat("\n\n")
}


tmpOM <- readOpenMeteoJSON("notuse_testOM.json")
tmpOM
prettyOpenMeteoMeta(tmpOM)

```
  
Conversion functions are written for hourly and daily data:  
```{r}

omProcessDaily <- function(tbl, extr="tblDaily") {
    if("list" %in% class(tbl)) tbl <- tbl[[extr]]
    tbl %>% mutate(date=lubridate::ymd(time)) %>% select(date, everything())
}

omProcessHourly <- function(tbl, extr="tblHourly") {
    if("list" %in% class(tbl)) tbl <- tbl[[extr]]
    tbl %>% 
        mutate(origTime=time, 
               time=lubridate::ymd_hm(time), 
               date=lubridate::date(time), 
               hour=lubridate::hour(time)
               ) %>% 
        select(time, date, hour, everything())
}

omProcessDaily(tmpOM)
omProcessHourly(tmpOM)

```
  
Function readOpenMeteoJSON() is updated to automatically incorporate date conversions:  
```{r}

readOpenMeteoJSON <- function(js, mapDaily=tblMetricsDaily, mapHourly=tblMetricsHourly) {
    
    # FUNCTION arguments: 
    # js: JSON list returned by download from Open-Meteo
    # mapDaily: mapping file for daily metrics
    # mapHourly: mapping file for hourly metrics
    
    # Get the object and names
    jsObj <- jsonlite::read_json(js, simplifyVector = TRUE)
    nms <- jsObj %>% names()
    cat("\nObjects in JSON include:", paste(nms, collapse=", "), "\n\n")
    
    # Set default objects as NULL
    tblDaily <- NULL
    tblHourly <- NULL
    tblUnitsDaily <- NULL
    tblUnitsHourly <- NULL
    
    # Get daily and hourly as tibble if relevant
    if("daily" %in% nms) tblDaily <- jsObj$daily %>% tibble::as_tibble() %>% omProcessDaily()
    if("hourly" %in% nms) tblHourly <- jsObj$hourly %>% tibble::as_tibble() %>% omProcessHourly()
    
    # Helper function for unit conversions
    helperMetricUnit <- function(x, mapper, desc=NULL) {
        if(is.null(desc)) 
            desc <- as.list(match.call())$x %>% 
                deparse() %>% 
                stringr::str_replace_all(pattern=".*\\$", replacement="")
        x %>% 
            tibble::as_tibble() %>% 
            pivot_longer(cols=everything()) %>% 
            left_join(mapper, by=c("name"="metric")) %>% 
            mutate(value=stringr::str_replace(value, "\u00b0", "deg ")) %>% 
            mutate(metricType=desc) %>% 
            select(metricType, everything())
    }
    
    # Get the unit descriptions
    if("daily_units" %in% nms) tblUnitsDaily <- helperMetricUnit(jsObj$daily_units, mapDaily)
    if("hourly_units" %in% nms) tblUnitsHourly <- helperMetricUnit(jsObj$hourly_units, mapHourly)
    if(is.null(tblUnitsDaily) & !is.null(tblUnitsHourly)) tblUnits <- tblUnitsHourly
    else if(!is.null(tblUnitsDaily) & is.null(tblUnitsHourly)) tblUnits <- tblUnitsDaily
    else if(!is.null(tblUnitsDaily) & !is.null(tblUnitsHourly)) 
        tblUnits <- bind_rows(tblUnitsHourly, tblUnitsDaily)
    else tblUnits <- NULL
    
    # Put everything else together
    tblDescription <- jsObj[setdiff(nms, c("hourly", "hourly_units", "daily", "daily_units"))] %>%
        tibble::as_tibble()
    
    # Return the list objects
    list(tblDaily=tblDaily, tblHourly=tblHourly, tblUnits=tblUnits, tblDescription=tblDescription)
    
}

tmpOM2 <- readOpenMeteoJSON("notuse_testOM.json")
tmpOM2
prettyOpenMeteoMeta(tmpOM2)

identical(tmpOM$tblUnits, tmpOM2$tblUnits)
identical(tmpOM$tblDescription, tmpOM2$tblDescription)
identical(tmpOM$tblDaily %>% omProcessDaily(), tmpOM2$tblDaily)
identical(tmpOM$tblHourly %>% omProcessHourly(), tmpOM2$tblHourly)

```
  
The daily data is tested for file download, cached to avoid multiple hits to the server:  
```{r cache=TRUE}

testURLDaily <- helperOpenMeteoURL(cityName="Chicago IL", 
                                   dailyIndices=1:nrow(tblMetricsDaily),
                                   startDate="2010-01-01", 
                                   endDate="2023-06-15", 
                                   tz="America/Chicago"
                                   )
testURLDaily

# Download file
if(!file.exists("notuse_testOM_daily.json")) {
    fileDownload(fileName="notuse_testOM_daily.json", url=testURLDaily)
} else {
    cat("\nFile notuse_testOM_daily.json already exists, skipping download\n")
}

```
  
Data are read and stored as a list:  
```{r, fig.height=9, fig.width=9}

tmpOMDaily <- readOpenMeteoJSON("notuse_testOM_daily.json")
tmpOMDaily
prettyOpenMeteoMeta(tmpOMDaily)

# Exploration of precipitation hours by day
tmpOMDaily$tblDaily %>% count(precipitation_hours) %>% print(n=30)
tmpOMDaily$tblDaily %>%
    filter(lubridate::year(date)<=2022) %>%
    ggplot(aes(x=precipitation_hours)) + 
    geom_density(aes(group=lubridate::year(date), color=as.factor(lubridate::year(date)))) + 
    scale_color_discrete("Year") + 
    labs(title="Hours of Precipitation per Day", x="Hours of Precipitation", y="Annual density")
tmpOMDaily$tblDaily %>%
    filter(lubridate::year(date)<=2022) %>%
    ggplot(aes(x=precipitation_hours)) + 
    geom_histogram(aes(fill=as.factor(lubridate::year(date))), bins=25) + 
    scale_fill_discrete("Year") + 
    facet_wrap(~lubridate::year(date)) +
    labs(title="Hours of Precipitation per Day", x="Hours of Precipitation", y="# Days")

```
  
Precipitation by month is explored:  
```{r, fig.height=9, fig.width=9}

dfPrecip <- tmpOMDaily$tblDaily %>%
    filter(lubridate::year(date)<=2022) %>%
    select(date, precipitation_sum, rain_sum, snowfall_sum) %>%
    mutate(month=factor(month.abb[lubridate::month(date)], levels=month.abb), 
           yyyymm=customYYYYMM(date)
           ) %>%
    group_by(yyyymm, month) %>%
    summarize(across(where(is.numeric), sum), n=n(), .groups="drop")
dfPrecip

# Boxplot of precipitation by month
dfPrecip %>%
    select(-n) %>%
    pivot_longer(-c(yyyymm, month)) %>%
    ggplot(aes(x=month, y=ifelse(name=="snowfall_sum", 10*value, value))) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~name, scales="free_y") + 
    labs(title="Precipitation by month (2010-2022)", y="Precipitation (mm)", x=NULL) + 
    theme(axis.text.x = element_text(angle = 90)) + 
    lims(y=c(0, NA))

# Mean precipitation by month
dfPrecip %>%
    group_by(month) %>%
    summarize(across(where(is.numeric), mean)) %>%
    ggplot(aes(x=month)) + 
    geom_col(aes(y=precipitation_sum), fill="green") + 
    geom_col(aes(y=rain_sum), fill="lightblue") + 
    geom_text(aes(y=rain_sum/2, label=round(rain_sum))) +
    geom_text(aes(y=rain_sum/2 + precipitation_sum/2, 
                  label=ifelse(precipitation_sum>rain_sum+3, round(precipitation_sum-rain_sum), "")
                  )
              ) + 
    geom_text(aes(y=precipitation_sum+5, label=round(precipitation_sum))) +
    labs(x=NULL, 
         y="Precipitation (mm)", 
         title="Mean precipitation by month (2010-2022)", 
         subtitle="Light blue is mm falling as rain, green is liquid equivalent of other"
         )

```
  
Average temperatures by month are also explored:  
```{r, fig.height=9, fig.width=9}

dfTemp <- tmpOMDaily$tblDaily %>%
    filter(lubridate::year(date)<=2022) %>%
    select(date, 
           temperature_2m_max, 
           temperature_2m_min, 
           apparent_temperature_max, 
           apparent_temperature_min
           ) %>%
    mutate(month=factor(month.abb[lubridate::month(date)], levels=month.abb), 
           yyyymm=customYYYYMM(date)
           ) %>%
    group_by(yyyymm, month) %>%
    summarize(across(where(is.numeric), mean), n=n(), .groups="drop")
dfTemp

# Boxplot of precipitation by month
dfTemp %>%
    select(-n) %>%
    pivot_longer(-c(yyyymm, month)) %>%
    ggplot(aes(x=month, y=value)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~name) + 
    labs(title="Average temperature by month (2010-2022)", y="Average temperature (C)", x=NULL) + 
    theme(axis.text.x = element_text(angle = 90))

# Mean temperatures by month
dfTemp %>%
    select(-n) %>%
    group_by(month) %>%
    summarize(across(where(is.numeric), mean)) %>%
    pivot_longer(cols=-c(month)) %>%
    mutate(measType=stringr::str_replace(name, ".*_", ""), 
           meas=ifelse(str_detect(name, "apparent"), "apparent", "actual")
           ) %>%
    select(-name) %>%
    pivot_wider(id_cols=c(month, meas), names_from="measType", values_from="value") %>%
    ggplot(aes(x=month)) + 
    geom_tile(aes(y=(max+min)/2, height=max-min), width=0.5, fill="lightblue") +
    geom_text(aes(y=max+1, label=round(max, 1)), size=2.5) +
    geom_text(aes(y=min-1, label=round(min, 1)), size=2.5) +
    labs(x=NULL, 
         y="Temperature (C)", 
         title="Mean high and low temperature by month (2010-2022)", 
         subtitle="Actual temperature and apparent temperature"
         ) + 
    facet_wrap(~meas)

```
  
Sunrise and sunset times are explored:  
```{r, fig.height=9, fig.width=9}

dfSun <- tmpOMDaily$tblDaily %>%
    filter(lubridate::year(date)<=2022) %>%
    select(date, sunrise, sunset) %>%
    mutate(month=factor(month.abb[lubridate::month(date)], levels=month.abb), 
           yyyymm=customYYYYMM(date),
           across(c(sunrise, sunset), lubridate::ymd_hm), 
           sr=hms::as_hms(sunrise), 
           ss=hms::as_hms(sunset), 
           doy=lubridate::yday(date), 
           year=lubridate::year(date)
           ) 
dfSun

# Plot of sunrise and sunset by day of year
dfSun %>%
    select(date, month, year, doy, sr, ss) %>%
    ggplot(aes(x=doy, group=factor(year), color=factor(year))) + 
    geom_line(aes(y=sr)) + 
    geom_line(aes(y=ss)) + 
    geom_line(aes(y=(ss+sr)/2)) +
    labs(x="Day of year", y="Time (always on DST)", title="Sunrise, sunset, and solar noon by day of year") + 
    scale_color_discrete("Year")

# Plot of minutes gained from earliest/latest
dfSun %>%
    select(date, month, year, doy, sr, ss) %>%
    group_by(year) %>%
    mutate(dsr=max(sr)-sr, dss=ss-min(ss)) %>%
    ungroup() %>%
    rename(sunrise_change=dsr, sunset_change=dss) %>%
    pivot_longer(cols=c(sunrise_change, sunset_change)) %>%
    ggplot(aes(x=doy)) + 
    geom_point(aes(y=as.numeric(value)/60, color=name), size=0.5) +
    labs(x="Day of year", y="Minutes", title="Delta from latest sunrise / earliest sunset") + 
    scale_color_discrete("Metric")

```
  
Wind data is explored:  
```{r, fig.height=9, fig.width=9}

dfWind <- tmpOMDaily$tblDaily %>% 
    select(date, 
           dir=winddirection_10m_dominant, 
           spd=windspeed_10m_max, 
           gst=windgusts_10m_max
           ) %>% 
    mutate(month=lubridate::month(date), 
           year=lubridate::year(date), 
           dir10=round(dir/10)*10, 
           spd5=round(spd/5)*5, 
           gst5=round(gst/5)*5
           ) 
dfWind

# Plot of wind direction and speed
dfWind %>%
    ggplot(aes(x=dir, y=spd)) + 
    geom_point(alpha=0.2, size=0.5) + 
    coord_polar() + 
    facet_wrap(~factor(month.abb[month], levels=month.abb), nrow=2) + 
    geom_vline(xintercept=c(0, 90, 180, 270), lty=2, color="red") + 
    labs(title="Maximum wind speed and predominant direction (measured daily)", 
         y="Maximum Wind speed (km/h)", 
         x="Predominant Wind direction"
         ) + 
    scale_x_continuous(breaks=c(0, 90, 180, 270))

dfWind %>%
    filter(lubridate::year(date)<=2022) %>%
    count(month, dir10, spd5) %>%
    ggplot(aes(x=dir10, y=spd5)) + 
    geom_point(aes(size=n), alpha=0.2) + 
    coord_polar() + 
    facet_wrap(~factor(month.abb[month], levels=month.abb)) + 
    geom_vline(xintercept=c(0, 90, 180, 270), lty=2, color="red") + 
    labs(title="Maximum wind speed and predominant direction (measured daily)", 
         subtitle="Wind speed rounded to nearest 5 km/h, wind direction rounded to nearest 10 degrees",
         y="Maximum Wind speed (km/h)", 
         x="Predominant Wind direction"
         ) + 
    scale_x_continuous(breaks=c(0, 90, 180, 270))

# Plot of predominant wind direction
dfWind %>% 
    ggplot(aes(x=dir10)) + 
    geom_histogram(binwidth=10) + 
    facet_wrap(~factor(month.abb[month], levels=month.abb)) + 
    geom_vline(xintercept=c(0, 90, 180, 270, 360), lty=2, color="red") + 
    labs(title="Predominant wind direction (measured daily)", 
         y="# Days", 
         x="Predominant wind direction (rounded to nearest 10 degrees)"
         ) + 
    scale_x_continuous(breaks=c(0, 90, 180, 270, 360))

# Plot of maximum wind speed
dfWind %>% 
    ggplot(aes(x=spd5)) + 
    geom_histogram(binwidth=5) + 
    facet_wrap(~factor(month.abb[month], levels=month.abb)) + 
    labs(title="Maximum wind speed (measured daily)", 
         y="# Days", 
         x="Maximum wind speed (km/h, rounded to nearest 5 km/h)"
         )

# Mean maximum wind speed by month
dfWind %>%
    filter(year<=2022) %>%
    select(date, month, year, spd, gst) %>%
    pivot_longer(cols=-c(date, month, year)) %>%
    ggplot(aes(x=factor(month.abb[month], levels=month.abb), y=value)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~c("gst"="2. Maximum wind gust", "spd"="1. Maximum wind speed")[name]) + 
    labs(title="Wind speed measured daily (2010-2022)", y="Wind speed (km/h)", x=NULL) + 
    theme(axis.text.x = element_text(angle = 90)) + 
    lims(y=c(0, NA))

```
  
Weather codes, radiation, and evapotranspiration are explored:  
```{r, fig.height=9, fig.width=9}

dfOther <- tmpOMDaily$tblDaily %>% 
    select(date, wc=weathercode, sw=shortwave_radiation_sum, et=et0_fao_evapotranspiration) %>%
    mutate(wc=factor(wc, levels=sort(unique(wc))), 
           year=lubridate::year(date), 
           month=factor(month.abb[lubridate::month(date)], levels=month.abb), 
           yyyymm=customYYYYMM(date)
           )
dfOther

# Histogram of weather code
dfOther %>%
    filter(year<=2022) %>%
    ggplot(aes(x=wc)) + 
    geom_bar() + 
    facet_wrap(~month) + 
    labs(title="Weather codes by month (2010-2022)", y="Count", x="Weather code") + 
    theme(axis.text.x = element_text(angle = 90))

# Mean radiation and evapotranspiration by month
dfOther %>%
    select(-year) %>%
    group_by(month) %>%
    summarize(across(where(is.numeric), mean)) %>%
    pivot_longer(cols=-c(month)) %>%
    ggplot(aes(x=month)) + 
    geom_point(aes(y=value)) +
    geom_line(aes(y=value, group=1)) +
    labs(x=NULL, 
         y=NULL, 
         title="Mean radiation and evapotranspiration by month (2010-2022)",
         subtitle="Evapotranspiration (mm) and Radiation (MegaJoules)"
         ) + 
    facet_wrap(~c("et"="Evapotranspiration (mm)", "sw"="Radiation (MJ)")[name], scales="free_y") + 
    lims(y=c(0, NA))

# Boxplot for radiation and evapotranspiration by month
dfOther %>%
    select(date, month, sw, et) %>%
    pivot_longer(-c(date, month)) %>%
    ggplot(aes(x=month)) + 
    geom_boxplot(aes(y=value), fill="lightblue") +
    labs(x=NULL, 
         y=NULL, 
         title="Daily radiation and evapotranspiration (2010-2022)",
         subtitle="Evapotranspiration (mm) and Radiation (MegaJoules)"
         ) + 
    facet_wrap(~c("et"="Evapotranspiration (mm)", "sw"="Radiation (MJ)")[name], scales="free_y") + 
    lims(y=c(0, NA))

```
  
The hourly data is tested for file download, cached to avoid multiple hits to the server:  
```{r cache=TRUE}

testURLHourly <- helperOpenMeteoURL(cityName="Chicago IL", 
                                    hourlyIndices=1:nrow(tblMetricsHourly),
                                    startDate="2010-01-01", 
                                    endDate="2023-06-15", 
                                    tz="America/Chicago"
                                    )
testURLHourly

# Download file
if(!file.exists("notuse_testOM_hourly.json")) {
    fileDownload(fileName="notuse_testOM_hourly.json", url=testURLHourly)
} else {
    cat("\nFile notuse_testOM_hourly.json already exists, skipping download\n")
}

```
  
Data are read and stored as a list:  
```{r, fig.height=9, fig.width=9}

tmpOMHourly <- readOpenMeteoJSON("notuse_testOM_hourly.json")
tmpOMHourly
prettyOpenMeteoMeta(tmpOMHourly)

```
  
Consistency of data between daily and hourly is explored:  
```{r}

# Variables where maximum of hourly should be created
vrblMax <- c("weathercode", "temperature_2m", "apparent_temperature", "windspeed_10m", "windgusts_10m")

# Variables where minimum of hourly should be created
vrblMin <- c("temperature_2m", "apparent_temperature")

# Variables where sum of hourly should be created
vrblSum <- c("precipitation", "rain", "snowfall", "shortwave_radiation", "et0_fao_evapotranspiration")

# Variables in daily not to explore
# date, time, sunrise, sunset

# Variables that require a different approach
# winddirection_10m_dominant, precipitation_hours

# Check that all variables are included in hourly data
c(vrblMax, vrblMin, vrblSum) %in% (tmpOMHourly$tblHourly %>% names)

# Create daily data from hourly
dfDailyFromHourly <- tmpOMHourly$tblHourly %>%
    group_by(date) %>%
    summarize(across(.cols=all_of(vrblMax), .fns=max, .names="{.col}_max"),
              across(.cols=all_of(vrblMin), .fns=min, .names="{.col}_min"), 
              across(.cols=all_of(vrblSum), .fns=sum, .names="{.col}_sum"), 
              precipitation_hours=sum(precipitation>0)
              ) %>%
    rename(weathercode=weathercode_max, et0_fao_evapotranspiration=et0_fao_evapotranspiration_sum)
dfDailyFromHourly
names(dfDailyFromHourly)
names(dfDailyFromHourly) %in% names(tmpOMDaily$tblDaily)

# Check data consistency
for (colName in names(dfDailyFromHourly)) {
    cat("\n", 
        colName, 
        ":", 
        all.equal(dfDailyFromHourly %>% pull(colName), tmpOMDaily$tblDaily %>% pull(colName))
        )
}

# Plot for differences in radiation
dfRadiation <- dfDailyFromHourly %>%
    select(date, shortwave_radiation_sum) %>%
    bind_rows(select(tmpOMDaily$tblDaily, date, shortwave_radiation_sum), .id="src") %>%
    mutate(src=c("1"="Daily from Hourly", "2"="Daily as Reported")[src])
dfRadiation %>%
    ggplot(aes(x=date, y=shortwave_radiation_sum)) + 
    geom_line(aes(group=src, color=src)) + 
    labs(x=NULL, y="Sum of radiation", title="Comparison of shortwave radiation by day by source")

# Exploration of units
tmpOMDaily$tblUnits %>% filter(name=="shortwave_radiation_sum")
tmpOMHourly$tblUnits %>% filter(name=="shortwave_radiation")

# Conversion of Watts per hour to MegaJoules
# 0.0036 megajoules/watt-hour
dfRadiation %>%
    ggplot(aes(x=date, y=ifelse(src=="Daily from Hourly", 0.0036, 1)*shortwave_radiation_sum)) + 
    geom_line(aes(group=src, color=src)) + 
    labs(x=NULL, 
         y="Sum of radiation", 
         title="Comparison of shortwave radiation by day by source", 
         subtitle="Summed from hourly multiplied by 0.0036 to convert Watt-hours to MegaJoules"
         )
dfRadiation %>%
    pivot_wider(id_cols="date", names_from="src", values_from="shortwave_radiation_sum") %>%
    mutate(rat=`Daily as Reported`/`Daily from Hourly`) %>%
    summary()

```
  
With the exception of radiation (reported in different units causing slight rounding differences), the reported daily data matches the expected aggregate of the reported hourly data
  
Precipitation by hour is explored:  
```{r, fig.height=9, fig.width=9}

# Hourly precipitation data
dfHourlyPrecip <- tmpOMHourly$tblHourly %>%
    select(time, hour, precipitation, snowfall, rain) %>%
    mutate(year=lubridate::year(time), 
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, year, month, hour))
dfHourlyPrecip

# Nil precipitation percent
dfNilPrecip <- dfHourlyPrecip %>%
    group_by(month, name) %>%
    summarize(pctNil=mean(value==0), .groups="drop")
dfNilPrecip

# Graphs of precipitation amount by month
for(metric in unique(dfHourlyPrecip$name)) {
    p1 <- dfHourlyPrecip %>%
        filter(name==metric, value>0, year<=2022) %>%
        ggplot() + 
        geom_histogram(aes(x=value), bins = 50) + 
        facet_wrap(~month) + 
        labs(x=NULL, 
             y=NULL, 
             title=paste0(metric, 
                          ": hourly total (", 
                          tmpOMHourly$tblUnits %>% filter(name==metric) %>% pull(value), 
                          ") from 2010-2022"
                          )
             ) + 
        geom_text(data=dfNilPrecip %>% filter(name==metric), 
                  aes(x=Inf, 
                      y=Inf, 
                      label=paste0("Excludes ", round(100*pctNil, 1), "%\nof observations at 0")
                      ), 
                  size=2.5, 
                  hjust=1, 
                  vjust=1
                  )
    print(p1)
}

```
  
Temperature by hour is explored:  
```{r, fig.height=9, fig.width=9}

# Hourly temperature data
dfHourlyTemp <- tmpOMHourly$tblHourly %>%
    select(time, hour, temperature_2m, apparent_temperature, dewpoint_2m) %>%
    mutate(year=lubridate::year(time), 
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, year, month, hour))
dfHourlyTemp

# Graphs of precipitation amount by month
for(metric in unique(dfHourlyTemp$name)) {
    p1 <- dfHourlyTemp %>%
        filter(name==metric, year<=2022) %>%
        ggplot() + 
        geom_boxplot(aes(x=factor(hour), y=value), fill = "lightblue") + 
        facet_wrap(~month) + 
        labs(x=NULL, 
             y=NULL, 
             title=paste0(metric, 
                          ": hourly boxplot (", 
                          tmpOMHourly$tblUnits %>% filter(name==metric) %>% pull(value), 
                          ") from 2010-2022"
                          )
             )
    print(p1)
    
}

# Spread of temperature by day
dfHourlyTemp %>%
    mutate(date=lubridate::date(time)) %>%
    group_by(year, month, date, name) %>%
    summarize(maxValue=max(value), minValue=min(value), mdnValue=median(value), .groups="drop") %>%
    mutate(spd=maxValue-minValue) %>%
    group_by(month, name) %>%
    summarize(across(where(is.numeric), mean), .groups="drop") %>%
    ggplot(aes(x=fct_rev(month), y=spd)) + 
    geom_point() + 
    coord_flip() +
    facet_wrap(~name) + 
    labs(title="Average high/low spread of key metrics by month (deg C)", x=NULL, y="deg C") + 
    lims(y=c(0, NA))

```
  
Hours with maximum/minimum temperature and precipitation are explored:  
```{r, fig.height=9, fig.width=9}

# Create temperature and precipitation data
dfHourlyTempPrecip <- dfHourlyTemp %>%
    bind_rows(dfHourlyPrecip) %>%
    mutate(date=lubridate::date(time)) %>% 
    arrange(time, name) 
dfHourlyTempPrecip

# Limit to temperature, dewpoint, and precipitation
# Limit precipitation to only days with precipitation > 0
tmpDF <- dfHourlyTempPrecip %>%
    filter(name %in% c("dewpoint_2m", "precipitation", "temperature_2m")) %>%
    group_by(date, name) %>% 
    filter(name!="precipitation" | sum(value)>0) %>%
    mutate(isMax=ifelse(value==max(value), 1, 0), isMin=ifelse(value==min(value), 1, 0)) %>% 
    group_by(name, month, hour) %>% 
    summarize(across(c(isMax, isMin), mean), .groups="drop") %>% 
    pivot_longer(-c(name, month, hour), names_to="metric")
tmpDF

# Plot max/min for metric
for(keyMetric in unique(tmpDF$name)) {

    p1 <- tmpDF %>% 
        filter(name==keyMetric) %>% 
        ggplot(aes(x=hour, y=value)) + 
        geom_line(aes(color=metric, group=metric)) + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y="% of time as max/min", 
             title=paste0(keyMetric, ": maximum and minimum by hour"), 
             subtitle=paste0("Ties included as full value", 
                             ifelse(keyMetric=="precipitation", " (days with no precipitation excluded)", "")
                             )
             ) + 
        scale_color_discrete("Metric:")
    print(p1)
}

# Plot percent of hours with precipitation
dfHourlyTempPrecip %>% 
    filter(name=="precipitation") %>% 
    group_by(month, hour) %>%
    summarize(pct0=mean(value>0), pct05=mean(value>=0.5), .groups="drop") %>%
    pivot_longer(-c(month, hour)) %>%
    mutate(name=ifelse(name=="pct0", ">=0.1 mm", ">=0.5 mm")) %>%
    ggplot(aes(x=hour, y=value)) + 
    geom_line(aes(group=name, color=name)) + 
    facet_wrap(~month) + 
    labs(x="Hour of day", 
         y="% at/above precipitation hurdle", 
         title=paste0("% of observations with precipitation in past hour")
         ) + 
    lims(y=c(0, NA))

```
  
Wind by hour is explored:  
```{r, fig.height=9, fig.width=9}

# Hourly wind data
dfHourlyWind <- tmpOMHourly$tblHourly %>%
    select(time, hour, windspeed_10m, windgusts_10m, winddirection_10m) %>%
    mutate(year=lubridate::year(time), 
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, year, month, hour))
dfHourlyWind

# Graphs of wind speed/gust by month
for(metric in setdiff(unique(dfHourlyWind$name), "winddirection_10m")) {
    p1 <- dfHourlyWind %>%
        filter(name==metric, year<=2022) %>%
        ggplot() + 
        geom_boxplot(aes(x=factor(hour), y=value), fill = "lightblue") + 
        facet_wrap(~month) + 
        labs(x=NULL, 
             y=NULL, 
             title=paste0(metric, 
                          ": hourly boxplot (", 
                          tmpOMHourly$tblUnits %>% filter(name==metric) %>% pull(value), 
                          ") from 2010-2022"
                          )
             )
    print(p1)
    
}

# Average wind speed and gust by hour
dfHourlyWind %>%
    filter(!(name %in% c("winddirection_10m")), year<=2022) %>%
    group_by(month, hour, name) %>%
    summarize(across("value", mean), .groups="drop") %>%
    ggplot(aes(x=factor(hour), y=value)) + 
    geom_point(aes(group=name, color=name)) + 
    facet_wrap(~month) + 
    labs(title="Average wind speed and gust (km/h)", x=NULL, y="km/h") + 
    lims(y=c(0, NA))

# Average wind direction by hour
dfHourlyWind %>%
    filter((name %in% c("winddirection_10m")), year<=2022) %>%
    mutate(preDom=case_when(value<45|value>=315~"N", 
                            value<135~"E", 
                            value<225~"S", 
                            value<315~"W", 
                            TRUE~"error"
                            )
           ) %>%
    count(month, hour, preDom) %>%
    ggplot(aes(x=factor(hour), y=n)) + 
    geom_col(aes(fill=factor(preDom, levels=c("N", "W", "S", "E"))), position="fill") + 
    facet_wrap(~month) + 
    labs(title="Distribution of wind direction", x=NULL, y="Wind direction (%)") + 
    scale_fill_discrete("")

```
  
Wind by N/S and E/W is also explored:  
```{r, fig.height=9, fig.width=9}

# Average wind direction by hour
tmpWindDir <- dfHourlyWind %>%
    filter((name %in% c("winddirection_10m")), year<=2022) %>%
    mutate(ew=case_when(value>30&value<150~"E", 
                        value>210&value<=330~"W", 
                        TRUE~"none"
                        ), 
           ns=case_when(value>300|value<=60~"N", 
                        value>120&value<=240~"S", 
                        TRUE~"none"
                        )
    ) 
tmpWindDir

tmpWindDir %>%
    count(month, hour, ew) %>%
    group_by(month, hour) %>%
    mutate(pct=n/sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=hour, y=pct)) + 
    geom_line(aes(color=factor(ew, levels=c("W", "none", "E")))) + 
    facet_wrap(~month) + 
    labs(title="Wind direction", 
         x="Hour of day", 
         y="% of observations", 
         subtitle="(030-150 deg defined as East, 210-330 deg defined as West)"
         ) + 
    scale_color_discrete("") + 
    lims(y=c(0, NA))

tmpWindDir %>%
    count(month, hour, ns) %>%
    group_by(month, hour) %>%
    mutate(pct=n/sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=hour, y=pct)) + 
    geom_line(aes(color=factor(ns, levels=c("S", "none", "N")))) + 
    facet_wrap(~month) + 
    labs(title="Wind direction", 
         x="Hour of day", 
         y="% of observations", 
         subtitle="(300-060 deg defined as North, 120-240 deg defined as South)"
         ) + 
    scale_color_discrete("") + 
    lims(y=c(0, NA))

```
  
Hourly wind directions are averaged using arctan. The formula is arctan2(y=sum-of-sin, x=sum-of-cos):  
```{r, fig.height=9, fig.width=9}

# Unweighted by wind speed
tmpWindatan_uw <- tmpWindDir %>%
    mutate(date=lubridate::date(time), cosine=cos(2*pi*value/360), sine=sin(2*pi*value/360)) %>%
    group_by(date) %>%
    summarize(across(c(cosine, sine), sum), .groups="drop") %>%
    mutate(arctangent=atan(sine/cosine), 
           arctangent2=atan2(y=sine, x=cosine), 
           avgdir=((arctangent2/2)*(360/pi)), 
           avgdir=ifelse(avgdir<0, 360+avgdir, avgdir)
           ) %>%
    left_join(select(tmpOMDaily$tblDaily, date, wdd=winddirection_10m_dominant), by="date")
tmpWindatan_uw

tmpWindatan_uw %>%
    mutate(delta=avgdir-wdd) %>%
    summary()

tmpWindatan_uw %>%
    count(wdd, awdd=round(avgdir)) %>%
    ggplot(aes(x=wdd, y=awdd)) + 
    geom_point(aes(size=n)) + 
    labs(x="Reported dominant wind direction (daily data)", 
         y="Calculated dominant wind direction (hourly data)", 
         title="Relationship between reported and calculated dominant wind direction", 
         subtitle="Unweighted by wind speed"
         ) + 
    scale_size_continuous("# days")

# Weighted by wind speed
tmpWindatan_wtd <- tmpOMHourly$tblHourly %>%
    select(date, time, wd=winddirection_10m, ws=windspeed_10m) %>%
    mutate(cosine=cos(2*pi*wd/360), sine=sin(2*pi*wd/360)) %>%
    group_by(date) %>%
    summarize(across(c(cosine, sine), .fns=function(x) sum(x*ws)/sum(ws)), sws=sum(ws), .groups="drop") %>%
    mutate(arctangent=atan(sine/cosine), 
           arctangent2=atan2(y=sine, x=cosine), 
           avgdir=((arctangent2/2)*(360/pi)), 
           avgdir=ifelse(avgdir<0, 360+avgdir, avgdir), 
           avgspd=sws/24, 
           dist=sws*sqrt(cosine**2+sine**2)
           ) %>%
    left_join(select(tmpOMDaily$tblDaily, date, wdd=winddirection_10m_dominant), by="date")
tmpWindatan_wtd

tmpWindatan_wtd %>%
    mutate(delta=avgdir-wdd) %>%
    summary()

tmpWindatan_wtd %>%
    count(wdd, awdd=round(avgdir)) %>%
    ggplot(aes(x=wdd, y=awdd)) + 
    geom_point(aes(size=n)) + 
    labs(x="Reported dominant wind direction (daily data)", 
         y="Calculated dominant wind direction (hourly data)", 
         title="Relationship between reported and calculated dominant wind direction", 
         subtitle="Weighted by wind speed"
         ) + 
    scale_size_continuous("# days")

tmpWindatan_wtd %>%
    count(rdist=round(dist), rspd=round(avgspd)) %>%
    ggplot(aes(x=rspd, y=rdist/24)) + 
    geom_point(aes(size=n)) + 
    labs(title="Average wind speed (total and weighted by direction) by day", 
         x="Average wind speed per day", 
         y="Weighted average wind speed\n(total distance on average angle, divided by 24)"
         ) + 
    geom_abline(slope=1, intercept=0, lty=2) +
    scale_size_continuous("# days")

tmpWindatan_wtd %>%
    mutate(rdist=round(dist), rspd=round(avgspd)) %>%
    ggplot(aes(x=rspd)) + 
    geom_boxplot(aes(y=dist/24/rspd, group=rspd), fill="lightblue") + 
    labs(title="Average wind speed (total and weighted by direction) by day", 
         x="Average wind speed per day", 
         y="Average weighted wind speed\n(as ratio of gross average)"
         )

tmpWindatan_wtd %>%
    filter(abs(avgdir-wdd)>1)

```
  
The wind direction averaging of hourly data, weighted by wind speed, is consistent with the reported dominant wind direction in the daily data.
  
Weather codes are explored:  
```{r, fig.height=9, fig.width=9}

# Hourly weather codes, evapotranspiration, and shortwave
dfHourlyCode <- tmpOMHourly$tblHourly %>%
    select(time, hour, wc=weathercode, et=et0_fao_evapotranspiration, sw=shortwave_radiation) %>%
    mutate(year=lubridate::year(time), 
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, year, month, hour))
dfHourlyCode

# Exploration of weather codes overall
dfHourlyCode %>%
    filter(name=="wc", year<=2022) %>%
    count(value) %>%
    ggplot(aes(x=fct_rev(factor(value)), y=n/1000)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(label=round(n/1000, 1)), hjust=0, size=3) +
    labs(title="Weather codes in hourly data (2010-2022)", y="Count (000)", x="Weather Code") + 
    coord_flip()

# Exploration of weather codes by month
dfHourlyCode %>%
    filter(name=="wc", year<=2022) %>%
    count(month, value) %>%
    group_by(month) %>%
    mutate(pct=n/sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=fct_rev(factor(value)), y=pct)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(label=paste0(round(100*pct, 1), "%")), hjust=0, size=3) +
    labs(title="Weather codes in hourly data (2010-2022)", y="Frequency", x="Weather Code") + 
    coord_flip() + 
    facet_wrap(~month)

# Exploration of weather codes by month
dfHourlyCode %>%
    filter(name=="wc", year<=2022) %>%
    mutate(wType=case_when(value<=3~"Dry", 
                           value>=51 & value<=55~"Drizzle", 
                           value>=61 & value<=65~"Rain", 
                           value>=71 & value<=75~"Snow", 
                           TRUE~"error"
                           )
           ) %>%
    count(month, wType) %>%
    group_by(month) %>%
    mutate(pct=n/sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=month, y=pct)) + 
    geom_col(aes(fill=factor(wType, levels=c("Dry", "Drizzle", "Rain", "Snow"))), position="stack") + 
    labs(title="Precipitation types in hourly data (2010-2022)", y="Frequency", x=NULL) + 
    coord_flip() + 
    scale_fill_discrete("")

# Weather codes from WMO Code Table 4677 (select examples)
# 00	Cloud development not observed or not observable
# 01	Clouds generally dissolving or becoming less developed
# 02	State of sky on the whole unchanged
# 03	Clouds generally forming or developing
# 51	Drizzle, not freezing, continuous (slight)
# 53	Drizzle, not freezing, continuous (moderate)
# 55	Drizzle, not freezing, continuous (heavy)
# 61	Rain, not freezing, continuous (slight)
# 63	Rain, not freezing, continuous (moderate)
# 65	Rain, not freezing, continuous (heavy)
# 71	Continuous fall of snowflakes (slight)
# 73	Continuous fall of snowflakes (moderate)
# 75	Continuous fall of snowflakes (heavy)

```
  
Shortwave radiation is explored:  
```{r, fig.height=9, fig.width=9}

dfHourlyCode %>%
    filter(name=="sw", year<=2022) %>%
    ggplot(aes(x=factor(hour), y=value)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~month) + 
    labs(title="Average shortwave solar radiation over the past hour", x=NULL, y="Watts per sqaure meter")

dfHourlyCode %>%
    filter(name=="sw", year<=2022) %>%
    group_by(hour, month) %>%
    summarize(value=mean(value), .groups="drop") %>%
    ggplot(aes(x=factor(hour), y=value)) + 
    geom_point() + 
    facet_wrap(~month) + 
    labs(title="Average hourly shortwave solar radiation by hour and month (2010-2022)", 
         x=NULL, 
         y="Watts per sqaure meter"
         )

dfHourlyCode %>%
    filter(name %in% c("sw"), year<=2022) %>%
    mutate(date=lubridate::date(time)) %>%
    group_by(date, name) %>% 
    mutate(isMax=ifelse(value==max(value), 1, 0), isMin=ifelse(value==min(value), 1, 0)) %>% 
    group_by(name, month, hour) %>% 
    summarize(across(c(isMax, isMin), mean), .groups="drop") %>% 
    pivot_longer(-c(name, month, hour), names_to="metric") %>%
    ggplot(aes(x=hour, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    facet_wrap(~month) + 
    labs(x="Hour of day", 
         y="% of time as max/min", 
         title=paste0("Shortwave radiation", ": maximum and minimum by hour"), 
         subtitle=paste0("Ties included as full value")
         ) + 
    scale_color_discrete("Metric:")

```
  

Evapotranspiration is explored:  
```{r, fig.height=9, fig.width=9}

dfHourlyCode %>%
    filter(name=="et", year<=2022) %>%
    ggplot(aes(x=factor(hour), y=value)) + 
    geom_boxplot(fill="lightblue") + 
    facet_wrap(~month) + 
    labs(title="Evapotranspiration of a well-watered grass field", x="Hour of day", y="mm")

dfHourlyCode %>%
    filter(name=="et", year<=2022) %>%
    group_by(hour, month) %>%
    summarize(value=mean(value), .groups="drop") %>%
    ggplot(aes(x=factor(hour), y=value)) + 
    geom_point() + 
    facet_wrap(~month) + 
    labs(title="Mean evapotranspiration of a well-watered grass field by hour and month (2010-2022)", 
         x="Hour of day", 
         y="mm"
         )

dfHourlyCode %>%
    filter(name %in% c("et"), year<=2022) %>%
    mutate(date=lubridate::date(time)) %>%
    group_by(date, name) %>% 
    mutate(isMax=ifelse(value==max(value), 1, 0), isMin=ifelse(value==min(value), 1, 0)) %>% 
    group_by(name, month, hour) %>% 
    summarize(across(c(isMax, isMin), mean), .groups="drop") %>% 
    pivot_longer(-c(name, month, hour), names_to="metric") %>%
    ggplot(aes(x=hour, y=value)) + 
    geom_line(aes(color=metric, group=metric)) + 
    facet_wrap(~month) + 
    labs(x="Hour of day", 
         y="proportion of time as max/min", 
         title=paste0("Evapotranspiration", ": maximum and minimum by hour"), 
         subtitle=paste0("Ties included as full value")
         ) + 
    scale_color_discrete("Metric:")

```
  
Cloud cover is explored:  
```{r, fig.height=9, fig.width=9}

# Create cloud cover data
dfHourlyCloud <- tmpOMHourly$tblHourly %>% 
    select(time, hour, contains("cloud")) %>% 
    mutate(year=lubridate::year(time),
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, year, month, hour))
dfHourlyCloud

# Boxplot for cloud cover types
for(keyMetric in unique(dfHourlyCloud$name)) {

    p1 <- dfHourlyCloud %>% 
        filter(name==keyMetric, year<=2022) %>% 
        ggplot(aes(x=factor(hour), y=value)) + 
        geom_boxplot(fill="lightblue") + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y="% sky covered with cloud", 
             title=paste0(keyMetric, ": % sky covered with cloud")
             )
    print(p1)

}

# Create max/min for metric
tmpDFCloud <- dfHourlyCloud %>%
    filter(year<=2022) %>%
    mutate(date=lubridate::date(time)) %>%
    group_by(date, name) %>% 
    mutate(isMax=ifelse(value==max(value), 1, 0), isMin=ifelse(value==min(value), 1, 0)) %>% 
    group_by(name, month, hour) %>% 
    summarize(across(c(isMax, isMin), mean), .groups="drop") %>% 
    pivot_longer(-c(name, month, hour), names_to="metric")
tmpDFCloud

# Plot max/min for metric
for(keyMetric in unique(tmpDFCloud$name)) {

    p1 <- tmpDFCloud %>% 
        filter(name==keyMetric) %>% 
        ggplot(aes(x=hour, y=value)) + 
        geom_line(aes(color=metric, group=metric)) + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y="% of time as max/min", 
             title=paste0(keyMetric, ": maximum and minimum by hour"), 
             subtitle=paste0("Ties included as full value")
             ) + 
        scale_color_discrete("Metric:")
    print(p1)
    
}

```
  
Atmospheric pressure is explored:  
```{r, fig.height=9, fig.width=9}

# Create pressure data
dfHourlyPressure <- tmpOMHourly$tblHourly %>% 
    select(time, hour, contains("pressure")) %>% 
    mutate(year=lubridate::year(time),
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, year, month, hour))
dfHourlyPressure

# Boxplot for pressure types
for(keyMetric in unique(dfHourlyPressure$name)) {

    tmpUnits <- tmpOMHourly$tblUnits %>% filter(name==keyMetric) %>% pull(value)
    
    p1 <- dfHourlyPressure %>% 
        filter(name==keyMetric, year<=2022) %>% 
        ggplot(aes(x=factor(hour), y=value)) + 
        geom_boxplot(fill="lightblue") + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y=paste0(keyMetric, " (", tmpUnits, ")"), 
             title=paste0(keyMetric, ": ", tmpUnits)
             )
    print(p1)

}

dfHourlyPressure %>%
    pivot_wider(id_cols=c(time, hour, year, month)) %>%
    count(pressure_msl, surface_pressure) %>%
    ggplot(aes(x=pressure_msl, y=surface_pressure)) + 
    geom_point(aes(size=n)) + 
    geom_smooth(aes(weight=n), method="lm") +
    labs(title="Surface pressure vs. MSL", x="MSL", y="Surface Pressure")

# Create max/min for metric
tmpDFPressure <- dfHourlyPressure %>%
    filter(year<=2022) %>%
    mutate(date=lubridate::date(time)) %>%
    group_by(date, name) %>% 
    mutate(isMax=ifelse(value==max(value), 1, 0), isMin=ifelse(value==min(value), 1, 0)) %>% 
    group_by(name, month, hour) %>% 
    summarize(across(c(isMax, isMin), mean), .groups="drop") %>% 
    pivot_longer(-c(name, month, hour), names_to="metric")
tmpDFPressure

# Plot max/min for metric
for(keyMetric in unique(tmpDFPressure$name)) {

    p1 <- tmpDFPressure %>% 
        filter(name==keyMetric) %>% 
        ggplot(aes(x=hour, y=value)) + 
        geom_line(aes(color=metric, group=metric)) + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y="% of time as max/min", 
             title=paste0(keyMetric, ": maximum and minimum by hour"), 
             subtitle=paste0("Ties included as full value")
             ) + 
        scale_color_discrete("Metric:")
    print(p1)
    
}

```
  
Soil temperature is explored:  
```{r, fig.height=9, fig.width=9}

# Create soil temperature data
dfHourlySoilTemp <- tmpOMHourly$tblHourly %>% 
    select(time, date, hour, starts_with("soil_temp")) %>% 
    mutate(year=lubridate::year(time),
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, date, year, month, hour))
dfHourlySoilTemp

# Boxplot for soil temperature
for(keyMetric in unique(dfHourlySoilTemp$name)) {

    tmpUnits <- tmpOMHourly$tblUnits %>% filter(name==keyMetric) %>% pull(value)
    
    p1 <- dfHourlySoilTemp %>% 
        filter(name==keyMetric, year<=2022) %>% 
        ggplot(aes(x=factor(hour), y=value)) + 
        geom_boxplot(fill="lightblue") + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y=paste0(keyMetric, " (", tmpUnits, ")"), 
             title=paste0(keyMetric, ": ", tmpUnits)
             )
    print(p1)

}

# Mean and standard deviation by month
dfHourlySoilTemp %>%
    group_by(date, name) %>%
    summarize(across(value, .fns=list(mu=mean, sigma=sd)), .groups="drop") %>%
    mutate(doy=lubridate::yday(date)) %>%
    group_by(doy, name) %>%
    summarize(across(starts_with("value"), .fns=list(mu=mean)), .groups="drop") %>%
    pivot_longer(cols=-c(doy, name), names_to="metric") %>%
    ggplot(aes(x=doy, y=value)) + 
    geom_line(aes(group=name, color=stringr::str_replace(name, "soil_temperature_", ""))) + 
    facet_wrap(~c("value_mu_mu"="Daily mean", "value_sigma_mu"="Mean daily standard deviation")[metric], 
               nrow=2, 
               scales="free_y"
               ) + 
    labs(x="Day of Year", 
         y="Degrees (C)", 
         title="Soil temperature mean and average daily standard deviation"
         ) + 
    scale_color_discrete("Soil depth")

# Create max/min for metric
tmpDFSoilTemp <- dfHourlySoilTemp %>%
    filter(year<=2022) %>%
    mutate(date=lubridate::date(time)) %>%
    group_by(date, name) %>% 
    mutate(isMax=ifelse(value==max(value), 1, 0), isMin=ifelse(value==min(value), 1, 0)) %>% 
    group_by(name, month, hour) %>% 
    summarize(across(c(isMax, isMin), mean), .groups="drop") %>% 
    pivot_longer(-c(name, month, hour), names_to="metric")
tmpDFSoilTemp

# Plot max/min for metric
for(keyMetric in unique(tmpDFSoilTemp$name)) {

    p1 <- tmpDFSoilTemp %>% 
        filter(name==keyMetric) %>% 
        ggplot(aes(x=hour, y=value)) + 
        geom_line(aes(color=metric, group=metric)) + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y="% of time as max/min", 
             title=paste0(keyMetric, ": maximum and minimum by hour"), 
             subtitle=paste0("Ties included as full value")
             ) + 
        scale_color_discrete("Metric:")
    print(p1)
    
}

```
  
Soil moisture is explored:  
```{r, fig.height=9, fig.width=9}

# Create soil moisture data
dfHourlySoilMoist <- tmpOMHourly$tblHourly %>% 
    select(time, date, hour, starts_with("soil_moist")) %>% 
    mutate(year=lubridate::year(time),
           month=factor(month.abb[lubridate::month(time)], levels=month.abb)
           ) %>%
    pivot_longer(cols=-c(time, date, year, month, hour))
dfHourlySoilMoist

# Boxplot for soil moisture
for(keyMetric in unique(dfHourlySoilMoist$name)) {

    tmpUnits <- tmpOMHourly$tblUnits %>% filter(name==keyMetric) %>% pull(value)
    
    p1 <- dfHourlySoilMoist %>% 
        filter(name==keyMetric, year<=2022) %>% 
        ggplot(aes(x=factor(hour), y=value)) + 
        geom_boxplot(fill="lightblue") + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y=paste0(keyMetric, " (", tmpUnits, ")"), 
             title=paste0(keyMetric, ": ", tmpUnits)
             )
    print(p1)

}

# Mean and standard deviation by month
dfHourlySoilMoist %>%
    group_by(date, name) %>%
    summarize(across(value, .fns=list(mu=mean, sigma=sd)), .groups="drop") %>%
    mutate(doy=lubridate::yday(date)) %>%
    group_by(doy, name) %>%
    summarize(across(starts_with("value"), .fns=list(mu=mean)), .groups="drop") %>%
    pivot_longer(cols=-c(doy, name), names_to="metric") %>%
    ggplot(aes(x=doy, y=value)) + 
    geom_line(aes(group=name, color=stringr::str_replace(name, "soil_moisture_", ""))) + 
    facet_wrap(~c("value_mu_mu"="Daily mean", "value_sigma_mu"="Mean daily standard deviation")[metric], 
               nrow=2, 
               scales="free_y"
               ) + 
    labs(x="Day of Year", 
         y="cubic meters per cubic meter\n(volumetric mixing ratio)", 
         title="Soil moisture mean and average daily standard deviation"
         ) + 
    scale_color_discrete("Soil depth")

# Create max/min for metric
tmpDFSoilMoist <- dfHourlySoilMoist %>%
    filter(year<=2022) %>%
    mutate(date=lubridate::date(time)) %>%
    group_by(date, name) %>% 
    mutate(isMax=ifelse(value==max(value), 1, 0), isMin=ifelse(value==min(value), 1, 0)) %>% 
    group_by(name, month, hour) %>% 
    summarize(across(c(isMax, isMin), mean), .groups="drop") %>% 
    pivot_longer(-c(name, month, hour), names_to="metric")
tmpDFSoilMoist

# Plot max/min for metric
for(keyMetric in unique(tmpDFSoilMoist$name)) {

    p1 <- tmpDFSoilMoist %>% 
        filter(name==keyMetric) %>% 
        ggplot(aes(x=hour, y=value)) + 
        geom_line(aes(color=metric, group=metric)) + 
        facet_wrap(~month) + 
        labs(x="Hour of day", 
             y="% of time as max/min", 
             title=paste0(keyMetric, ": maximum and minimum by hour"), 
             subtitle=paste0("Ties included as full value")
             ) + 
        scale_color_discrete("Metric:")
    print(p1)
    
}

```
  
Metrics are explored for their variation over months and over hours of day:  
```{r, fig.height=9, fig.width=9}

# Sample database
tmpTemp <- tmpOMHourly$tblHourly %>%
    select(time, date, temperature_2m) %>%
    mutate(month=factor(month.abb[lubridate::month(date)], levels=month.abb), 
           hour=lubridate::hour(time), 
           fct_hour=factor(hour), 
           rndTemp=round(2*temperature_2m, 0)/2
           )
tmpTemp

# Simple predictive model for temperature/month
prdTemp <- tmpTemp %>%
    count(rndTemp, month) %>%
    arrange(rndTemp, desc(n)) %>%
    group_by(rndTemp) %>%
    mutate(corr=row_number()==1, pred=first(month)) %>%
    ungroup()
prdTemp

# Confusion matrix and accuracy
prdTemp %>%
    count(month, corr, wt=n) %>%
    pivot_wider(id_cols=month, names_from=corr, values_from=n, values_fill=0) %>%
    bind_rows(summarize(., across(where(is.numeric), sum)) %>% 
                  mutate(month="All") %>% 
                  select(month, everything())
              ) %>%
    mutate(n=`TRUE`+`FALSE`, 
           pctCorrect=`TRUE`/n, 
           pctNaive=ifelse(month=="All", 1/(nrow(.)-1), 2*n/sum(n)), 
           lift=pctCorrect/pctNaive
           )

prdTemp %>%
    count(month, pred, corr, wt=n) %>%
    ggplot(aes(x=month, y=pred)) + 
    labs(x="Actual month", y="Predicted month", title="Actual vs. predicted month using temperature") + 
    geom_text(aes(label=n)) + 
    geom_tile(aes(fill=corr), alpha=0.25)

```
  
The simple predictive model is converted to functional form:  
```{r, fig.height=9, fig.width=9}

simpleOneVarPredict <- function(df, 
                                tgt, 
                                prd, 
                                nPrint=30, 
                                showPlot=TRUE, 
                                returnData=TRUE
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble with key elements
    # tgt: target variable
    # prd: predictor variable
    # nPrint: maximum number of lines of confusion matrix to print
    #         0 means do not print any summary statistics
    # showPlot: boolean, should overlap plot be created and shown?
    
    # Counts of predictor to target variable
    dfPred <- df %>%
        group_by(across(all_of(c(prd, tgt)))) %>%
        summarize(n=n(), .groups="drop") %>%
        arrange(across(all_of(prd)), desc(n)) %>%
        group_by(across(all_of(prd))) %>%
        mutate(correct=row_number()==1, predicted=first(get(tgt))) %>%
        ungroup()

    # Confusion matrix and accuracy
    dfConf <- dfPred %>%
        group_by(across(all_of(c(tgt, "correct")))) %>%
        summarize(n=sum(n), .groups="drop") %>%
        pivot_wider(id_cols=tgt, names_from=correct, values_from=n, values_fill=0) %>%
        mutate(n=`TRUE`+`FALSE`, 
               pctCorrect=`TRUE`/n, 
               pctNaive=1/(nrow(.)), 
               lift=pctCorrect/pctNaive-1
               )
    
    # Overall confusion matrix
    dfConfAll <- dfConf %>%
        summarize(nMax=max(n), across(c(`FALSE`, `TRUE`, "n"), sum)) %>%
        mutate(pctCorrect=`TRUE`/n, 
               pctNaive=nMax/n, 
               lift=pctCorrect/pctNaive-1, 
               nBucket=length(unique(dfPred[[prd]]))
               )
    
    # Print confusion matrices
    if(nPrint > 0) {
        cat("\nAccuracy by target subgroup:\n")
        dfConf %>% print(n=nPrint)
        cat("\nOverall Accuracy:\n")
        dfConfAll %>% print(n=nPrint)
    }
    
    # Plot of overlaps
    if(isTRUE(showPlot)) {
        p1 <- dfPred %>%
            group_by(across(c(all_of(tgt), "predicted", "correct"))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            ggplot(aes(x=get(tgt), y=predicted)) + 
            labs(x="Actual", 
                 y="Predicted", 
                 title=paste0("Actual vs. predicted ", tgt), 
                 subtitle=paste0("(using ", prd, ")")
                 ) + 
            geom_text(aes(label=n)) + 
            geom_tile(aes(fill=correct), alpha=0.25)
        print(p1)
    }
    
    # Return data if requested
    if(isTRUE(returnData)) list(dfPred=dfPred, dfConf=dfConf, dfConfAll=dfConfAll)
    
}

tstFunc <- simpleOneVarPredict(tmpTemp, tgt="month", prd="rndTemp")
tstFunc

all.equal(tstFunc$dfPred, rename(prdTemp, correct=corr, predicted=pred))

```
  
The function is tested for predictive power of numeric variables, bucketed in to percentiles, on month:  
```{r, fig.height=9, fig.width=9}

# Create percentiles for numeric variables
tmpTemp <- tmpOMHourly$tblHourly %>%
    mutate(month=factor(month.abb[lubridate::month(date)], levels=month.abb), 
           hour=lubridate::hour(time), 
           fct_hour=factor(hour), 
           across(where(is.numeric), .fns=function(x) round(100*percent_rank(x)), .names="pct_{.col}")
           )
tmpTemp

# Get key variable names
tmpNames <- tmpTemp %>% 
    select(starts_with("pct")) %>% 
    names()
tmpNames

# Get the key predictive metrics
tmpDFR <- map_dfr(.x=tmpNames, 
                  .f=function(x) simpleOneVarPredict(tmpTemp, tgt="month", prd=x, nPrint=0, showPlot=FALSE)$dfConfAll
                  ) %>% 
    mutate(vrbl=tmpNames) %>% 
    arrange(desc(lift))

# Print and plot lift by variable
tmpDFR %>% 
    print(n=50)
tmpDFR %>% 
    ggplot(aes(x=fct_reorder(stringr::str_replace_all(vrbl, "pct_", ""), lift), y=lift)) + 
    geom_col(fill="lightblue") + 
    coord_flip() + 
    labs(x=NULL, y="lift", title="Lift by hourly variable percentile in predicting month")

# Example for soil temperature and high clouds
simpleOneVarPredict(tmpTemp, tgt="month", prd="pct_soil_temperature_100_to_255cm", returnData=FALSE)
simpleOneVarPredict(tmpTemp, tgt="month", prd="pct_cloudcover_high", returnData=FALSE)

```
  
The function is tested for predictive power of numeric variables, bucketed in to percentiles, on hour:  
```{r, fig.height=9, fig.width=9}

# Remove hour from tmpNames
tmpNamesNoHour <- setdiff(tmpNames, "pct_hour")
tmpNamesNoHour

# Get the key predictive metrics
tmpDFRHour <- map_dfr(.x=tmpNamesNoHour, 
                  .f=function(x) simpleOneVarPredict(tmpTemp, tgt="fct_hour", prd=x, nPrint=0, showPlot=FALSE)$dfConfAll
                  ) %>% 
    mutate(vrbl=tmpNamesNoHour) %>% 
    arrange(desc(lift))

# Print and plot lift by variable
tmpDFRHour %>% 
    print(n=50)
tmpDFRHour %>% 
    ggplot(aes(x=fct_reorder(stringr::str_replace_all(vrbl, "pct_", ""), lift), y=lift)) + 
    geom_col(fill="lightblue") + 
    coord_flip() + 
    labs(x=NULL, y="lift", title="Lift by hourly variable percentile in predicting hour")

# Example for diffuse radiation and soil moisture
simpleOneVarPredict(tmpTemp, tgt="fct_hour", prd="pct_diffuse_radiation", returnData=FALSE)
simpleOneVarPredict(tmpTemp, tgt="fct_hour", prd="pct_soil_moisture_100_to_255cm", returnData=FALSE)

```
  
Random variables, split equally 0-5, 0-25, and 0-100, are included as an example null state:  
```{r, fig.height=9, fig.width=9}

# Add random variables
set.seed(23072413)
tmpTemp <- tmpTemp %>%
    mutate(rnd005=sample(0:5, size=n(), replace=TRUE), 
           rnd025=sample(0:25, size=n(), replace=TRUE), 
           rnd100=sample(0:100, size=n(), replace=TRUE)
           )

# Get key variable names
tmpNames_v2 <- tmpTemp %>% 
    select(starts_with("pct"), starts_with("rnd")) %>% 
    names()
tmpNames_v2

# Get the key predictive metrics
tmpDFR_v2 <- map_dfr(.x=tmpNames_v2, 
                     .f=function(x) simpleOneVarPredict(tmpTemp, tgt="month", prd=x, nPrint=0, showPlot=FALSE)$dfConfAll
                     ) %>% 
    mutate(vrbl=tmpNames_v2) %>% 
    arrange(desc(lift))

# Print and plot lift by variable
tmpDFR_v2 %>% 
    print(n=50)
tmpDFR_v2 %>% 
    mutate(fillColor=ifelse(str_detect(vrbl, pattern="pct_"), "lightblue", "red")) %>%
    ggplot(aes(x=fct_reorder(stringr::str_replace_all(vrbl, "pct_", ""), lift), y=lift)) + 
    geom_col(aes(fill=fillColor)) + 
    coord_flip() + 
    labs(x=NULL, y="lift", title="Lift by hourly variable percentile in predicting month") + 
    scale_fill_identity()


# Remove hour from tmpNames
tmpNamesNoHour_v2 <- setdiff(tmpNames_v2, "pct_hour")
tmpNamesNoHour_v2

# Get the key predictive metrics
tmpDFRHour_v2 <- map_dfr(.x=tmpNamesNoHour_v2, 
                         .f=function(x) simpleOneVarPredict(tmpTemp, tgt="fct_hour", prd=x, nPrint=0, showPlot=FALSE)$dfConfAll
                  ) %>% 
    mutate(vrbl=tmpNamesNoHour_v2) %>% 
    arrange(desc(lift))

# Print and plot lift by variable
tmpDFRHour_v2 %>% 
    print(n=50)
tmpDFRHour_v2 %>% 
    mutate(fillColor=ifelse(str_detect(vrbl, pattern="pct_"), "lightblue", "red")) %>%
    ggplot(aes(x=fct_reorder(stringr::str_replace_all(vrbl, "pct_", ""), lift), y=lift)) + 
    geom_col(aes(fill=fillColor)) + 
    coord_flip() + 
    labs(x=NULL, y="lift", title="Lift by hourly variable percentile in predicting hour") + 
    scale_fill_identity()

```
  
Function simpleOneVarPredict() is updated to allow for test-train:  
```{r, fig.height=9, fig.width=9}

simpleOneVarPredict <- function(df, 
                                tgt, 
                                prd, 
                                dfTest=NULL,
                                nPrint=30, 
                                showPlot=TRUE, 
                                returnData=TRUE
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble with key elements (training data set)
    # tgt: target variable
    # prd: predictor variable
    # dfTest: test dataset for applying predictions
    # nPrint: maximum number of lines of confusion matrix to print
    #         0 means do not print any summary statistics
    # showPlot: boolean, should overlap plot be created and shown?
    
    # Counts of predictor to target variable
    dfPred <- df %>%
        group_by(across(all_of(c(prd, tgt)))) %>%
        summarize(n=n(), .groups="drop") %>%
        arrange(across(all_of(prd)), desc(n)) %>%
        group_by(across(all_of(prd))) %>%
        mutate(correct=row_number()==1, predicted=first(get(tgt))) %>%
        ungroup()

    # Confusion matrix and accuracy
    dfConf <- dfPred %>%
        group_by(across(all_of(c(tgt, "correct")))) %>%
        summarize(n=sum(n), .groups="drop") %>%
        pivot_wider(id_cols=tgt, names_from=correct, values_from=n, values_fill=0) %>%
        mutate(n=`TRUE`+`FALSE`, 
               pctCorrect=`TRUE`/n, 
               pctNaive=1/(nrow(.)), 
               lift=pctCorrect/pctNaive-1
               )
    
    # Overall confusion matrix
    dfConfAll <- dfConf %>%
        summarize(nMax=max(n), across(c(`FALSE`, `TRUE`, "n"), sum)) %>%
        mutate(pctCorrect=`TRUE`/n, 
               pctNaive=nMax/n, 
               lift=pctCorrect/pctNaive-1, 
               nBucket=length(unique(dfPred[[prd]]))
               )
    
    # Print confusion matrices
    if(nPrint > 0) {
        cat("\nAccuracy by target subgroup (training data):\n")
        dfConf %>% print(n=nPrint)
        cat("\nOverall Accuracy (training data):\n")
        dfConfAll %>% print(n=nPrint)
    }
    
    # Plot of overlaps
    if(isTRUE(showPlot)) {
        p1 <- dfPred %>%
            group_by(across(c(all_of(tgt), "predicted", "correct"))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            ggplot(aes(x=get(tgt), y=predicted)) + 
            labs(x="Actual", 
                 y="Predicted", 
                 title=paste0("Training data - Actual vs. predicted ", tgt), 
                 subtitle=paste0("(using ", prd, ")")
                 ) + 
            geom_text(aes(label=n)) + 
            geom_tile(aes(fill=correct), alpha=0.25)
        print(p1)
    }
    
    # Create metrics for test dataset if requested
    if(!is.null(dfTest)) {
        # Get maximum category from training data
        mostPredicted <- count(dfPred, predicted, wt=n) %>% slice(1) %>% pull(predicted)
        # Get mapping of metric to prediction
        dfPredict <- dfPred %>% 
            group_by(across(all_of(c(prd, "predicted")))) %>% 
            summarize(n=sum(n), .groups="drop")
        # Create predictions for test data
        dfPredTest <- dfTest %>%
            select(all_of(c(prd, tgt))) %>%
            left_join(select(dfPredict, -n)) %>%
            replace_na(list(predicted=mostPredicted)) %>%
            group_by(across(all_of(c(prd, tgt, "predicted")))) %>%
            summarize(n=n(), .groups="drop") %>%
            mutate(correct=(get(tgt)==predicted))
        # Create confusion statistics for test data
        dfConfTest <- dfPredTest %>%
            group_by(across(all_of(c(tgt, "correct")))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            pivot_wider(id_cols=tgt, names_from=correct, values_from=n, values_fill=0) %>%
            mutate(n=`TRUE`+`FALSE`, 
                   pctCorrect=`TRUE`/n, 
                   pctNaive=1/(nrow(.)), 
                   lift=pctCorrect/pctNaive-1
                   )
        # Overall confusion matrix for test data
        dfConfAllTest <- dfConfTest %>%
            summarize(nMax=max(n), across(c(`FALSE`, `TRUE`, "n"), sum)) %>%
            mutate(pctCorrect=`TRUE`/n, 
                   pctNaive=nMax/n, 
                   lift=pctCorrect/pctNaive-1, 
                   nBucket=length(unique(dfConfTest[[prd]]))
               )
        # Print confusion matrices
        if(nPrint > 0) {
            cat("\nAccuracy by target subgroup (testing data):\n")
            dfConfTest %>% print(n=nPrint)
            cat("\nOverall Accuracy (testing data):\n")
            dfConfAllTest %>% print(n=nPrint)
            }
    } else {
        dfPredTest <- NULL
        dfConfTest <- NULL
        dfConfAllTest <- NULL
        
    }
    
    # Return data if requested
    if(isTRUE(returnData)) list(dfPred=dfPred, 
                                dfConf=dfConf, 
                                dfConfAll=dfConfAll, 
                                dfPredTest=dfPredTest, 
                                dfConfTest=dfConfTest, 
                                dfConfAllTest=dfConfAllTest
                                )
    
}

# Original format
simpleOneVarPredict(tmpTemp, tgt="month", prd="pct_soil_temperature_100_to_255cm", showPlot=FALSE)

# Train-test format
set.seed(23072514)
idxTrain <- sample(1:nrow(tmpTemp), size=round(.8*nrow(tmpTemp)), replace=FALSE)
simpleOneVarPredict(tmpTemp[idxTrain,], 
                    tgt="month", 
                    prd="pct_soil_temperature_100_to_255cm", 
                    showPlot=FALSE, 
                    dfTest=tmpTemp[-idxTrain,]
                    )

```
  
The function is split into components for better modularity:  
```{r, fig.height=9, fig.width=9}

# Fit a single predictor to a single categorical variable
simpleOneVarFit <- function(df, 
                            tgt, 
                            prd, 
                            rankType="last", 
                            naMethod=TRUE
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble with key elements (training data set)
    # tgt: target variable
    # prd: predictor variable
    # rankType: method for breaking ties of same n, passed to base::rank as ties.method=
    # naMethod: method for handling NA in ranks, passed to base::rank as na.last=
    
    # Counts of predictor to target variable, and associated predictions
    df %>%
        group_by(across(all_of(c(prd, tgt)))) %>%
        summarize(n=n(), .groups="drop") %>%
        arrange(across(all_of(prd)), desc(n), across(all_of(tgt))) %>%
        group_by(across(all_of(prd))) %>%
        mutate(rankN=n()+1-rank(n, ties.method=rankType, na.last=naMethod)) %>%
        arrange(across(all_of(prd)), rankN) %>%
        ungroup()

}

# Test that results are the same for a variable with many ties, and a variable with fewer ties
tstFit <- simpleOneVarFit(tmpTemp, tgt="month", prd="pct_snowfall")
tstOrig <- simpleOneVarPredict(tmpTemp, tgt="month", prd="pct_snowfall", nPrint=0, showPlot=FALSE)$dfPred
all.equal(tstOrig %>% select(-correct, -predicted), tstFit %>% select(-rankN))

tstFit <- simpleOneVarFit(tmpTemp, tgt="month", prd="pct_soil_temperature_100_to_255cm")
tstOrig <- simpleOneVarPredict(tmpTemp, 
                               tgt="month", 
                               prd="pct_soil_temperature_100_to_255cm", 
                               nPrint=0, 
                               showPlot=FALSE
                               )$dfPred
all.equal(tstOrig %>% select(-correct, -predicted), tstFit %>% select(-rankN))

```
  
A prediction mapper is created, along with the mapping for anything not in the data:  
```{r, fig.height=9, fig.width=9}

simpleOneVarMapper <- function(df, tgt, prd) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble from SimpleOneVarFit()
    # tgt: target variable
    # prd: predictor variable
    
    # Get the most common actual results
    dfCommon <- df %>% count(across(all_of(tgt)), wt=n, sort=TRUE)
    
    # Get the predictions
    dfPredictor <- df %>%
        group_by(across(all_of(prd))) %>%
        filter(row_number()==1) %>%
        select(all_of(c(prd, tgt))) %>%
        ungroup()
    
    list(dfPredictor=dfPredictor, dfCommon=dfCommon)
    
}

tstMapper <- simpleOneVarMapper(tstFit, tgt="month", prd="pct_soil_temperature_100_to_255cm")
tstMapper

```
  
A function to apply the prediction mapper is created:  
```{r, fig.height=9, fig.width=9}

simpleOneVarApplyMapper <- function(df, 
                                    tgt,
                                    prd, 
                                    mapper, 
                                    mapperDF="dfPredictor", 
                                    mapperDefault="dfCommon",
                                    prdName="predicted"
                                    ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame containing prd for predicting tgt
    # tgt: target variable in df
    # prd: predictor variable in df
    # mapper: mapping list from sinpleOneVarMapper()
    # mapperDF: element that can be used to merge mappings
    # mapperDefault: element that can be used for NA resulting from merging mapperDF
    # prdName: name for the prediction variable
    
    # Extract the mapper and default value
    vecRename <- c(prdName) %>% purrr::set_names(tgt)
    dfMap <- mapper[[mapperDF]] %>% select(all_of(c(prd, tgt))) %>% colRenamer(vecRename=vecRename)
    chrDefault <- mapper[[mapperDefault]] %>% slice(1) %>% pull(tgt)
    
    # Merge mappings to df
    df %>%
        left_join(dfMap, by=prd) %>%
        replace_na(list("predicted"=chrDefault))
    
}

# Example with mutated variable
tmpMutated <- tmpTemp %>% 
    select(date, hour, month, pct_soil_temperature_100_to_255cm) %>%
    mutate(pct_soil_temperature_100_to_255cm=ifelse(hour==0, -10, pct_soil_temperature_100_to_255cm))
tstApplied <- simpleOneVarApplyMapper(tmpMutated,
                                      tgt="month", 
                                      prd="pct_soil_temperature_100_to_255cm", 
                                      mapper=tstMapper
                                      )
tstApplied

# Example using tstFit to confirm same outputs
tstApplied <- simpleOneVarApplyMapper(tstFit,
                                      tgt="month", 
                                      prd="pct_soil_temperature_100_to_255cm", 
                                      mapper=tstMapper
                                      )
tstApplied
all.equal(tstOrig %>% select(-correct, correct), 
          tstApplied %>% select(-rankN) %>% mutate(correct=month==predicted)
          )

```
  
A function to create the confusion matrix data is written:  
```{r, fig.height=9, fig.width=9}

simpleOneVarConfusionData <- function(df, 
                                      tgtOrig,
                                      tgtPred, 
                                      otherVars=c(),
                                      weightBy="n"
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame from simpleOneVarApplyMapper()
    # tgtOrig: original target variable name in df
    # tgtPred: predicted target variable name in df
    # otherVars: other variables to be kept (will be grouping variables)
    # weightBy: weighting variable for counts in df (NULL means count each row of df as 1)
    
    # Confusion matrix data creation
    df %>%
        group_by(across(all_of(c(tgtOrig, tgtPred, otherVars)))) %>%
        summarize(n=if(!is.null(weightBy)) sum(get(weightBy)) else n(), .groups="drop") %>%
        mutate(correct=get(tgtOrig)==get(tgtPred))
    
}

# Example with and without weighting
simpleOneVarConfusionData(tstApplied, tgtOrig="month", tgtPred="predicted", weightBy=NULL)
simpleOneVarConfusionData(tstApplied, tgtOrig="month", tgtPred="predicted")

```
  
A function to report the confusion matrix data is written:  
```{r, fig.height=9, fig.width=9}

simpleOneVarConfusionReport <- function(df, 
                                        tgtOrig,
                                        tgtPred, 
                                        otherVars=c(), 
                                        printConf=TRUE,
                                        printConfOrig=printConf, 
                                        printConfPred=printConf,
                                        printConfOverall=printConf, 
                                        plotConf=TRUE, 
                                        plotDesc="",
                                        nBucket=NA, 
                                        predictorVarName="", 
                                        returnData=FALSE
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame from simpleOneVarConfusionData()
    # tgtOrig: original target variable name in df
    # tgtPred: predicted target variable name in df
    # otherVars: other variables to be kept (will be grouping variables) - NOT IMPLEMENTED
    # printConf: boolean, should confusion matrix data be printed? Applies to all three
    # printConfOrig: boolean, should confusion data be printed based on original target variable?
    # printConfPred: boolean, should confusion data be printed based on predicted target variable?
    # printConfOverall: boolean, should overall confusion data be printed?
    # plotConf: boolean, should confusion overlap data be plotted?
    # plotDesc: descriptive label to be included in front of plot title
    # nBucket: number of buckets used for prediction (pass from previous data)
    # predictorVarName: variable name to be included in chart description
    # returnData: boolean, should the confusion matrices be returned?
    
    # Confusion data based on original target variable
    if(isTRUE(printConfOrig) | isTRUE(returnData)) {
        dfConfOrig <- df %>%
            group_by(across(all_of(c(tgtOrig)))) %>%
            summarize(right=sum(n*correct), wrong=sum(n)-right, n=sum(n), .groups="drop") %>%
            mutate(pctRight=right/n, pctNaive=n/(sum(n)), lift=pctRight/pctNaive-1)
    }

    # Confusion data based on predicted target variable
    if(isTRUE(printConfPred) | isTRUE(returnData)) {
        dfConfPred <- df %>%
            group_by(across(all_of(c(tgtPred)))) %>%
            summarize(right=sum(n*correct), wrong=sum(n)-right, n=sum(n), .groups="drop") %>%
            mutate(pctRight=right/n)
    }

    # Overall confusion data
    if(isTRUE(printConfOverall) | isTRUE(returnData)) {
        maxNaive <- df %>%
            group_by(across(all_of(tgtOrig))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            arrange(desc(n)) %>%
            slice(1) %>%
            pull(n)
        dfConfOverall <- df %>%
            summarize(right=sum(n*correct), wrong=sum(n)-right, n=sum(n), .groups="drop") %>%
            mutate(maxN=maxNaive, pctRight=right/n, pctNaive=maxN/n, lift=pctRight/pctNaive-1, nBucket=nBucket)
    }
    
    # Confusion report based on original target variable
    if(isTRUE(printConfOrig)) {
        cat("\nConfusion data based on original target variable:", tgtOrig, "\n")
        dfConfOrig %>%
            print(n=50)
    }

    # Confusion report based on predicted target variable
    if(isTRUE(printConfPred)) {
        cat("\nConfusion data based on predicted target variable:", tgtPred, "\n")
        dfConfPred %>%
            print(n=50)
    }
    
    # Overall confusion matrix
    if(isTRUE(printConfOverall)) {
        cat("\nOverall confusion matrix\n")
        dfConfOverall %>%
            print(n=50)
    }
    
    # Plot of overlaps
    if(isTRUE(plotConf)) {
        p1 <- df %>%
            group_by(across(all_of(c(tgtOrig, tgtPred, "correct")))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            ggplot(aes(x=get(tgtOrig), y=get(tgtPred))) + 
            labs(x="Actual", 
                 y="Predicted", 
                 title=paste0(plotDesc, "Actual vs. predicted ", tgtOrig), 
                 subtitle=paste0("(using ", predictorVarName, ")")
                 ) + 
            geom_text(aes(label=n)) + 
            geom_tile(aes(fill=correct), alpha=0.25)
        print(p1)
    }
    
    # Return data if requested
    if(isTRUE(returnData)) list(dfConfOrig=dfConfOrig, dfConfPred=dfConfPred, dfConfOverall=dfConfOverall)
    
}

# Example with weighting
simpleOneVarConfusionData(tstApplied, tgtOrig="month", tgtPred="predicted") %>%
    simpleOneVarConfusionReport(tgtOrig="month", 
                                tgtPred="predicted", 
                                nBucket=length(unique(tstApplied$pct_soil_temperature_100_to_255cm)), 
                                predictorVarName=names(tstApplied)[1]
                                )

# Example with weighting and data returned without reporting
simpleOneVarConfusionData(tstApplied, tgtOrig="month", tgtPred="predicted") %>%
    simpleOneVarConfusionReport(tgtOrig="month", 
                                tgtPred="predicted", 
                                nBucket=length(unique(tstApplied$pct_soil_temperature_100_to_255cm)), 
                                predictorVarName=names(tstApplied)[1], 
                                printConf=FALSE, 
                                plotConf=FALSE,
                                returnData=TRUE
                                )

```
  
The functions are run together:  
```{r, fig.height=9, fig.width=9}

# Create fitting data
dfFit <- simpleOneVarFit(tmpTemp, tgt="month", prd="pct_temperature_2m") 

# Apply and report on fitting data
dfFit %>%
    simpleOneVarMapper(tgt="month", prd="pct_temperature_2m") %>%
    simpleOneVarApplyMapper(df=dfFit, tgt="month", prd="pct_temperature_2m", mapper=.) %>%
    simpleOneVarConfusionData(tgtOrig="month", tgtPred="predicted") %>%
    simpleOneVarConfusionReport(tgtOrig="month", 
                                tgtPred="predicted", 
                                nBucket=length(unique(dfFit$pct_temperature_2m))
                                )

```
  
A function is written to chain all of the functions:  
```{r, fig.height=9, fig.width=9}

simpleOneVarChain <- function(df,
                              tgt,
                              prd,
                              mapper=NULL, 
                              rankType="last", 
                              naMethod=TRUE, 
                              printReport=TRUE, 
                              plotDesc="",
                              returnData=TRUE, 
                              includeConfData=FALSE
                              ) {

    # FUNCTION ARGUMENTS:
    # df: data frame or tibble with key elements (training or testing data set)
    # tgt: target variable
    # prd: predictor variable
    # mapper: mapping file to be applied for predictions (NULL means create from simpleOneVarApply())
    # rankType: method for breaking ties of same n, passed to base::rank as ties.method=
    # naMethod: method for handling NA in ranks, passed to base::rank as na.last=    
    # printReport: boolean, should the confusion report data and plot be printed?
    # plotDesc: descriptive label to be included in front of plot title
    # returnData: boolean, should data elements be returned?
    # includeConfData: boolean, should confusion data be returned?
    
    # Create the summary of predictor-target-n
    dfFit <- simpleOneVarFit(df, tgt=tgt, prd=prd, rankType=rankType, naMethod=naMethod)     

    # Create the mapper if it does not already exist
    if(is.null(mapper)) mapper <- simpleOneVarMapper(dfFit, tgt=tgt, prd=prd)
    
    # Apply mapper to data
    dfApplied <- simpleOneVarApplyMapper(dfFit, tgt=tgt, prd=prd, mapper=mapper)

    # Create confusion data
    dfConfusion <- simpleOneVarConfusionData(dfApplied, tgtOrig=tgt, tgtPred="predicted")
    
    # Create confusion report if requested
    if(isTRUE(printReport) | isTRUE(includeConfData)) {
        dfConfReport <- simpleOneVarConfusionReport(df=dfConfusion, 
                                                    tgtOrig=tgt, 
                                                    tgtPred="predicted", 
                                                    nBucket=length(unique(dfApplied[[prd]])), 
                                                    predictorVarName=prd, 
                                                    printConf=printReport, 
                                                    plotConf=printReport,
                                                    plotDesc=plotDesc,
                                                    returnData=includeConfData
                                                    )
    }
    
    # Return data if requested
    if(isTRUE(returnData)) {
        ret <- list(dfFit=dfFit, mapper=mapper, dfApplied=dfApplied, dfConfusion=dfConfusion)
        if(isTRUE(includeConfData)) ret<-c(ret, list(dfConfData=dfConfReport))
        ret
    }
    
}

# Full process
tmpChain <- simpleOneVarChain(tmpTemp, tgt="month", prd="pct_temperature_2m")
str(tmpChain)

# Plots only
simpleOneVarChain(tmpTemp, tgt="month", prd="pct_temperature_2m", returnData=FALSE)

# Data only
tmpChain_v2 <- simpleOneVarChain(tmpTemp, tgt="month", prd="pct_temperature_2m", printReport=FALSE)
identical(tmpChain_v2, tmpChain)

# Data only using a mapper
tmpChain_v3 <- simpleOneVarChain(tmpTemp, 
                                 tgt="month", 
                                 prd="pct_temperature_2m", 
                                 mapper=tmpChain$mapper,
                                 printReport=FALSE
                                 )
identical(tmpChain_v3, tmpChain)

# Return confusion data
tmpChain_v4 <- simpleOneVarChain(tmpTemp, 
                                 tgt="month", 
                                 prd="pct_temperature_2m", 
                                 mapper=tmpChain$mapper,
                                 printReport=FALSE, 
                                 includeConfData=TRUE
                                 )
identical(tmpChain_v4[1:4], tmpChain)
tmpChain_v4$dfConfData

```
  
The process is then run on a train-test basis for a single variable:  
```{r, fig.height=9, fig.width=9}

# Add random variables to dataset, then split in to test and train
set.seed(23080412)
tmpTempRand <- tmpTemp %>%
    mutate(pct_0005=sample(0:5, size=nrow(.), replace=TRUE),
           pct_0025=sample(0:25, size=nrow(.), replace=TRUE), 
           pct_0100=sample(0:100, size=nrow(.), replace=TRUE), 
           pct_0250=sample(0:250, size=nrow(.), replace=TRUE),
           pct_0500=sample(0:500, size=nrow(.), replace=TRUE), 
           pct_1000=sample(0:1000, size=nrow(.), replace=TRUE), 
           pct_2500=sample(0:2500, size=nrow(.), replace=TRUE), 
           pct_5000=sample(0:5000, size=nrow(.), replace=TRUE), 
           )
idxTrain <- sort(sample(1:nrow(tmpTempRand), size=round(0.75*nrow(tmpTempRand)), replace=FALSE))
tmpTempTrain <- tmpTempRand[idxTrain, ]
tmpTempTest <- tmpTempRand[-idxTrain, ]

# Full process run on training data
tmpChainTrain <- simpleOneVarChain(tmpTempTrain, 
                                   tgt="month", 
                                   prd="pct_temperature_2m", 
                                   includeConfData=TRUE,
                                   plotDesc="Training data: "
                                   )

# Diagnostics run on testing data
tmpChainTest <- simpleOneVarChain(tmpTempTest, 
                                  tgt="month", 
                                  prd="pct_temperature_2m", 
                                  mapper=tmpChainTrain$mapper,
                                  includeConfData=TRUE,
                                  plotDesc="Testing data: "
                                  )

# Overall confusion
tmpChainTrain$dfConfData$dfConfOverall
tmpChainTest$dfConfData$dfConfOverall

```
  
The process for running train-test is created as a function:  
```{r, fig.height=9, fig.width=9}

simpleOneVarTrainTest <- function(dfTrain,
                                  dfTest,
                                  tgt,
                                  prd,
                                  rankType="last", 
                                  naMethod=TRUE, 
                                  printReport=FALSE, 
                                  includeConfData=TRUE, 
                                  returnData=TRUE
                              ) {

    # FUNCTION ARGUMENTS:
    # dfTrain: data frame or tibble with key elements (training data set)
    # dfTest: data frame or tibble with key elements (testing data set)
    # tgt: target variable
    # prd: predictor variable
    # rankType: method for breaking ties of same n, passed to base::rank as ties.method=
    # naMethod: method for handling NA in ranks, passed to base::rank as na.last=    
    # printReport: boolean, should the confusion report data and plot be printed?
    # includeConfData: boolean, should confusion data be returned?
    # returnData: boolean, should data elements be returned?
    
    # Fit the training data
    tmpTrain <- simpleOneVarChain(df=dfTrain, 
                                  tgt=tgt, 
                                  prd=prd,
                                  rankType=rankType,
                                  naMethod=naMethod,
                                  printReport=printReport,
                                  plotDesc="Training data: ",
                                  returnData=TRUE,
                                  includeConfData=includeConfData
                                  )
    
    # Fit the testing data
    tmpTest <- simpleOneVarChain(df=dfTest, 
                                 tgt=tgt, 
                                 prd=prd,
                                 mapper=tmpTrain$mapper,
                                 rankType=rankType,
                                 naMethod=naMethod,
                                 printReport=printReport,
                                 plotDesc="Testing data: ",
                                 returnData=TRUE,
                                 includeConfData=includeConfData
                                 )
    
    # Return data if requested
    if(isTRUE(returnData)) list(tmpTrain=tmpTrain, tmpTest=tmpTest)
    
}

# Full process without plotting
tmpVTT <- simpleOneVarTrainTest(dfTrain=tmpTempTrain, 
                                dfTest=tmpTempTest, 
                                tgt="month", 
                                prd="pct_temperature_2m"
                                )
str(tmpVTT)
# Extracting key elements of prediction accuracy
map_dfr(.x=tmpVTT, .f=function(x) x$dfConfData$dfConfOverall) %>%
    mutate(dataType=names(tmpVTT), 
           tgt=names(tmpVTT[[1]]$mapper$dfPredictor)[2], 
           prd=names(tmpVTT[[1]]$mapper$dfPredictor)[1]
           ) %>%
    select(dataType, tgt, prd, everything())

```
  
Predictive power for each variable on month is explored:  
```{r, fig.height=9, fig.width=9}

# Get all pct variables
pctVars <- tmpTempTrain %>% 
    select(starts_with("pct")) %>%
    names()
pctVars

# Run each variable and combine as dfr
tmpLiftPct <- map_dfr(.x=pctVars, 
        .f=function(x) {
            tmp <- simpleOneVarTrainTest(dfTrain=tmpTempTrain, dfTest=tmpTempTest, tgt="month", prd=x)
            map_dfr(.x=tmp, .f=function(y) y$dfConfData$dfConfOverall) %>%
                mutate(dataType=names(tmp), 
                       tgt=names(tmp[[1]]$mapper$dfPredictor)[2], 
                       prd=names(tmp[[1]]$mapper$dfPredictor)[1]
                       ) %>%
                select(dataType, tgt, prd, everything())
            }
        )
tmpLiftPct

```
  
Variables are plotted based on explanatory power on month:  
```{r, fig.height=9, fig.width=9}

tmpLiftPct %>% 
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point() + 
    coord_flip() + 
    facet_wrap(~c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType]) + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent month, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on month"
         )

```
  
Predictive power for each variable on hour (as factor) is explored:  
```{r, fig.height=9, fig.width=9}

# Run each variable and combine as dfr (pctVars derived previously for month)
tmpLiftHourPct <- map_dfr(.x=pctVars, 
                          .f=function(x) {
                              tmp <- simpleOneVarTrainTest(dfTrain=tmpTempTrain, 
                                                           dfTest=tmpTempTest, 
                                                           tgt="fct_hour", 
                                                           prd=x
                                                           )
                              map_dfr(.x=tmp, .f=function(y) y$dfConfData$dfConfOverall) %>%
                                  mutate(dataType=names(tmp), 
                                         tgt=names(tmp[[1]]$mapper$dfPredictor)[2], 
                                         prd=names(tmp[[1]]$mapper$dfPredictor)[1]
                                         ) %>%
                                  select(dataType, tgt, prd, everything())
                              }
                          )
tmpLiftHourPct

```
  
Variables are plotted based on explanatory power on hour:  
```{r, fig.height=9, fig.width=9}

tmpLiftHourPct %>% 
    filter(prd != "pct_hour") %>%
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point() + 
    coord_flip() + 
    facet_wrap(~c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType]) + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent hour, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on hour (as factor)"
         )

```
  
Data are converted to add aggregate elements:  
  
* Season (Dec-Feb=Winter, Mar-May=Spring, Jun-Aug=Summer, Sep-Nov=Fall)  
* Night/Day (07h-18h=Day, 00h-06h and 19h-23h=Night)  
* Combined Season and Night/Day  
  
```{r, fig.height=9, fig.width=9}

# Add random variables to dataset, then split in to test and train
set.seed(23080412) # Same seed as above
tmpTempRand <- tmpTemp %>%
    mutate(pct_0005=sample(0:5, size=nrow(.), replace=TRUE),
           pct_0025=sample(0:25, size=nrow(.), replace=TRUE), 
           pct_0100=sample(0:100, size=nrow(.), replace=TRUE), 
           pct_0250=sample(0:250, size=nrow(.), replace=TRUE),
           pct_0500=sample(0:500, size=nrow(.), replace=TRUE), 
           pct_1000=sample(0:1000, size=nrow(.), replace=TRUE), 
           pct_2500=sample(0:2500, size=nrow(.), replace=TRUE), 
           pct_5000=sample(0:5000, size=nrow(.), replace=TRUE), 
           tod=ifelse(hour>=7 & hour<=18, "Day", "Night"), 
           season=case_when(month %in% c("Mar", "Apr", "May") ~ "Spring", 
                            month %in% c("Jun", "Jul", "Aug") ~ "Summer", 
                            month %in% c("Sep", "Oct", "Nov") ~ "Fall", 
                            month %in% c("Dec", "Jan", "Feb") ~ "Winter", 
                            TRUE~"typo"
                            ), 
           todSeason=paste0(season, "-", tod), 
           tod=factor(tod, levels=c("Day", "Night")), 
           season=factor(season, levels=c("Spring", "Summer", "Fall", "Winter")), 
           todSeason=factor(todSeason, 
                            levels=paste0(rep(c("Spring", "Summer", "Fall", "Winter"), each=2), 
                                          "-", 
                                          c("Day", "Night")
                                          )
                            )
           )
tmpTempRand %>% count(tod)
tmpTempRand %>% count(season)
tmpTempRand %>% count(todSeason)
idxTrain <- sort(sample(1:nrow(tmpTempRand), size=round(0.75*nrow(tmpTempRand)), replace=FALSE))
tmpTempTrain <- tmpTempRand[idxTrain, ]
tmpTempTest <- tmpTempRand[-idxTrain, ]

# Example process for season
simpleOneVarTrainTest(dfTrain=tmpTempTrain, 
                      dfTest=tmpTempTest, 
                      tgt="season", 
                      prd="pct_temperature_2m", 
                      printReport=TRUE, 
                      returnData=FALSE
                      )

```
  
Predictive power for each variable on season is explored:  
```{r, fig.height=9, fig.width=9}

# Get all pct variables
pctVars <- tmpTempTrain %>% 
    select(starts_with("pct")) %>%
    names()
pctVars

# Run each variable and combine as dfr
tmpLiftPctSeason <- map_dfr(.x=pctVars, 
        .f=function(x) {
            tmp <- simpleOneVarTrainTest(dfTrain=tmpTempTrain, dfTest=tmpTempTest, tgt="season", prd=x)
            map_dfr(.x=tmp, .f=function(y) y$dfConfData$dfConfOverall) %>%
                mutate(dataType=names(tmp), 
                       tgt=names(tmp[[1]]$mapper$dfPredictor)[2], 
                       prd=names(tmp[[1]]$mapper$dfPredictor)[1]
                       ) %>%
                select(dataType, tgt, prd, everything())
            }
        )
tmpLiftPctSeason

```
  
Variables are plotted based on explanatory power on season:  
```{r, fig.height=9, fig.width=9}

tmpLiftPctSeason %>% 
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point() + 
    coord_flip() + 
    facet_wrap(~c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType]) + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent season, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on season"
         )

tmpLiftPctSeason %>% 
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point(aes(color=c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType])) + 
    coord_flip() + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent season, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on season"
         ) + 
    scale_color_discrete(NULL)

```
  
Predictive power for each variable on day-night is explored:  
```{r, fig.height=9, fig.width=9}

# Get all pct variables (exclude hour)
pctVars <- tmpTempTrain %>% 
    select(starts_with("pct")) %>%
    select(-pct_hour) %>%
    names()
pctVars

# Run each variable and combine as dfr
tmpLiftPctDayNight <- map_dfr(.x=pctVars, 
        .f=function(x) {
            tmp <- simpleOneVarTrainTest(dfTrain=tmpTempTrain, dfTest=tmpTempTest, tgt="tod", prd=x)
            map_dfr(.x=tmp, .f=function(y) y$dfConfData$dfConfOverall) %>%
                mutate(dataType=names(tmp), 
                       tgt=names(tmp[[1]]$mapper$dfPredictor)[2], 
                       prd=names(tmp[[1]]$mapper$dfPredictor)[1]
                       ) %>%
                select(dataType, tgt, prd, everything())
            }
        )
tmpLiftPctDayNight

```
  
Variables are plotted based on explanatory power on night-day:  
```{r, fig.height=9, fig.width=9}

tmpLiftPctDayNight %>% 
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point() + 
    coord_flip() + 
    facet_wrap(~c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType]) + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent day-night, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on day-night"
         )

tmpLiftPctDayNight %>% 
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point(aes(color=c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType])) + 
    coord_flip() + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent day-night, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on day-night"
         ) + 
    scale_color_discrete(NULL)

```
  
Lift by Variable is plotted for season and night-day:  
```{r, fig.height=9, fig.width=9}

tmpLiftPctSeason %>% 
    filter(prd != "pct_hour") %>%
    bind_rows(tmpLiftPctDayNight) %>%
    filter(dataType=="tmpTest") %>%
    select(tgt, prd, lift) %>%
    pivot_wider(id_cols="prd", names_from="tgt", values_from="lift") %>%
    mutate(prdType=case_when(str_detect(prd, "_\\d{4}$")~"4. Random", 
                             str_detect(prd, "radia")|str_detect(prd, "evapotrans")~"2. Radiation/et0",
                             str_detect(prd, "temper|dewp|vapor|soil")~"1. Temp/Dew/Vapor/Soil",
                             TRUE ~ "3. Other"
                             )
           ) %>%
    ggplot(aes(x=season, y=tod)) + 
    geom_point(aes(color=prdType)) + 
    labs(x="Lift (season)", 
         y="Lift (night-day)", 
         title="Lift on test data of single-variable predictor"
         ) +
    geom_hline(yintercept=c(0, 0.25), lty=2) +
    geom_vline(xintercept=c(0, 0.5), lty=2) +
    facet_wrap(~prdType) + 
    scale_color_discrete("Type")

```
  
Predictive power for each variable on day-night-season is explored:  
```{r, fig.height=9, fig.width=9}

# Get all pct variables (exclude hour)
pctVars <- tmpTempTrain %>% 
    select(starts_with("pct")) %>%
    select(-pct_hour) %>%
    names()
pctVars

# Run each variable and combine as dfr
tmpLiftPctDayNightSeason <- map_dfr(.x=pctVars, 
        .f=function(x) {
            tmp <- simpleOneVarTrainTest(dfTrain=tmpTempTrain, dfTest=tmpTempTest, tgt="todSeason", prd=x)
            map_dfr(.x=tmp, .f=function(y) y$dfConfData$dfConfOverall) %>%
                mutate(dataType=names(tmp), 
                       tgt=names(tmp[[1]]$mapper$dfPredictor)[2], 
                       prd=names(tmp[[1]]$mapper$dfPredictor)[1]
                       ) %>%
                select(dataType, tgt, prd, everything())
            }
        )
tmpLiftPctDayNightSeason

```
  
Variables are plotted based on explanatory power on night-day-season:  
```{r, fig.height=9, fig.width=9}

tmpLiftPctDayNightSeason %>% 
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point() + 
    coord_flip() + 
    facet_wrap(~c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType]) + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent day-night-season, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on day-night-season"
         )

tmpLiftPctDayNightSeason %>% 
    ggplot(aes(x=fct_reorder(prd, lift, min), y=lift)) + 
    geom_point(aes(color=c("tmpTest"="1. Test data", "tmpTrain"="2. Training data")[dataType])) + 
    coord_flip() + 
    geom_hline(yintercept=0, lty=2) +
    labs(y="Lift (percent correct divided by percent of most frequent day-night-season, minus 1)", 
         x=NULL, 
         title="Explanatory power of variable on day-night-season"
         ) + 
    scale_color_discrete(NULL)

# Example process for season
simpleOneVarTrainTest(dfTrain=tmpTempTrain, 
                      dfTest=tmpTempTest, 
                      tgt="season", 
                      prd="pct_temperature_2m", 
                      printReport=TRUE, 
                      returnData=FALSE
                      )

```
  
The top-performing Variable for night-day-season is plotted:  
```{r, fig.height=9, fig.width=9}

# Example process for night-day-season
simpleOneVarTrainTest(dfTrain=tmpTempTrain, 
                      dfTest=tmpTempTest, 
                      tgt="todSeason", 
                      prd="pct_soil_temperature_0_to_7cm", 
                      printReport=TRUE, 
                      returnData=FALSE
                      )

```
  
Data are explored for k-means:  
```{r, fig.height=9, fig.width=9}

# Create data
kmTrain <- tmpTempTrain %>%
    select(time, starts_with("pct")) %>%
    select(-pct_hour, -pct_weathercode, -ends_with("0"), -ends_with("5"))
names(kmTrain)

# Confirm that mean and sd are reasonably similar
kmSD <- kmTrain %>%
    summarize(across(starts_with("pct"), .fns=list(mean=mean, sd=sd))) %>%
    pivot_longer(cols=everything()) %>%
    mutate(metric=str_remove_all(name, pattern="pct_|_mean|_sd"), 
           type=str_extract(name, pattern="[a-zA-Z0-9]+$")
           ) %>%
    pivot_wider(id_cols="metric", names_from="type", values_from="value")
kmSD %>%
    ggplot(aes(x=mean, y=sd)) + 
    geom_point(alpha=0.5) + 
    labs(title="Mean and standard deviation for potential k-means variables", x="Mean", y="SD")
kmSD %>%
    filter(mean<=20)

# Initial k-means with two centers
set.seed(23081914)
kmTrain_002 <- kmTrain %>%
    select(-time) %>%
    kmeans(centers=2)
kmTrain_002$centers %>%
    tibble::as_tibble() %>%
    mutate(cluster=row_number()) %>%
    pivot_longer(cols=-c(cluster)) %>%
    ggplot(aes(x=fct_reorder(str_remove(name, "pct_"), value, .fun=function(a) a[2]-a[1]), y=value)) + 
    geom_point(aes(color=factor(cluster))) + 
    scale_color_discrete("Cluster") + 
    facet_wrap(~factor(cluster)) +
    labs(title="Cluster means (kmeans, centers=2)", x="Metric", y="Cluster mean") + 
    lims(y=c(0, 100)) + 
    geom_hline(yintercept=40, lty=2) +
    coord_flip()

```
  
Clusters are assessed:  
```{r, fig.height=9, fig.width=9}

kmAssess <- kmTrain %>% 
    mutate(cl=factor(kmTrain_002$cluster), 
           fct_month=factor(month.abb[month(time)], levels=month.abb), 
           hour=as.integer(hour(time)), 
           tod=ifelse(hour>=7 & hour<=18, "Day", "Night"), 
           season=case_when(fct_month %in% c("Mar", "Apr", "May") ~ "Spring", 
                            fct_month %in% c("Jun", "Jul", "Aug") ~ "Summer", 
                            fct_month %in% c("Sep", "Oct", "Nov") ~ "Fall", 
                            fct_month %in% c("Dec", "Jan", "Feb") ~ "Winter", 
                            TRUE~"typo"
                            ), 
           todSeason=paste0(season, "-", tod), 
           tod=factor(tod, levels=c("Day", "Night")), 
           season=factor(season, levels=c("Spring", "Summer", "Fall", "Winter")), 
           todSeason=factor(todSeason, 
                            levels=paste0(rep(c("Spring", "Summer", "Fall", "Winter"), each=2), 
                                          "-", 
                                          c("Day", "Night")
                                          )
                            )
           ) 

# Assessed by month and hour
kmAssess %>% 
    count(fct_month, hour, cl) %>% 
    group_by(fct_month, hour) %>% 
    mutate(pct=n/sum(n)) %>% 
    ungroup() %>% 
    ggplot(aes(y=fct_month, x=hour)) + 
    geom_tile(aes(fill=pct)) + 
    facet_wrap(~cl, nrow=1) + 
    scale_fill_continuous(low="white", high="green") + 
    labs(title="Percentage by cluster (kmeans with 2 centers)", x="Hour", y=NULL)

# Assessed by todSeason
kmAssess %>% 
    count(todSeason, cl) %>% 
    group_by(todSeason) %>% 
    mutate(pct=n/sum(n)) %>% 
    ungroup() %>% 
    ggplot(aes(y=fct_reorder(todSeason, pct, .fun=function(x) x[1]), x=factor(cl))) + 
    geom_tile(aes(fill=pct)) + 
    scale_fill_continuous(low="white", high="green") + 
    labs(title="Percentage by cluster (kmeans with 2 centers)", x="Hour", y=NULL)

```
  
A function is written for creating k-means:  
```{r, fig.height=9, fig.width=9}

plotClusterMeans <- function(km, nrow=NULL, ncol=NULL, scales="fixed") {

    # FUNCTION ARGUMENTS
    # km: object returned by stats::kmeans(...)
    # nrow: number of rows for faceting (NULL means default)
    # ncol: number of columns for faceting (NULL means default)
    # scales: passed to facet_wrap as scales=scales
    
    # Assess clustering by dimension
    p1 <- km$centers %>%
        tibble::as_tibble() %>%
        mutate(cluster=row_number()) %>%
        pivot_longer(cols=-c(cluster)) %>%
        ggplot(aes(x=fct_reorder(name, 
                                 value, 
                                 .fun=function(a) ifelse(length(a)==2, a[2]-a[1], diff(range(a)))
                                 ), 
                   y=value
                   )
               ) + 
        geom_point(aes(color=factor(cluster))) + 
        scale_color_discrete("Cluster") + 
        facet_wrap(~factor(cluster), nrow=nrow, ncol=ncol, scales=scales) +
        labs(title=paste0("Cluster means (kmeans, centers=", nrow(km$centers), ")"), 
             x="Metric", 
             y="Cluster mean"
             ) + 
        geom_hline(yintercept=median(km$centers), lty=2) +
        coord_flip()
    print(p1)
    
}

plotClusterPct <- function(df, km, keyVars, nRowFacet=1, printPlot=TRUE) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame initially passed to stats::kmeans(...)
    # km: object returned by stats::kmeans(...)
    # keyVars: character vector of length 1 (y-only, x will be cl) or length 2 (x, y, cl will facet)
    # nRowFacet: number of rows for facetting (only relevant if length(keyVars) is 2)
    # printPlot: boolean, should plot be printed? (if not true, plot will be returned)
    
    # Check length of keyVars
    if(!(length(keyVars) %in% c(1, 2))) stop("\nArgument keyVars must be length-1 or length-2\n")
    
    p1 <- df %>%
        mutate(cl=factor(km$cluster)) %>%
        group_by(across(c(all_of(keyVars), "cl"))) %>%
        summarize(n=n(), .groups="drop") %>%
        group_by(across(all_of(keyVars))) %>%
        mutate(pct=n/sum(n)) %>%
        ungroup() %>%
        ggplot() + 
        scale_fill_continuous(low="white", high="green") + 
        labs(title=paste0("Percentage by cluster (kmeans with ", nrow(km$centers), " centers)"), 
             x=ifelse(length(keyVars)==1, "Cluster", keyVars[1]), 
             y=ifelse(length(keyVars)==1, keyVars[1], keyVars[2])
             )
    if(length(keyVars)==1) p1 <- p1 + geom_tile(aes(fill=pct, x=cl, y=get(keyVars[1])))
    if(length(keyVars)==2) {
        p1 <- p1 + 
            geom_tile(aes(fill=pct, x=get(keyVars[1]), y=get(keyVars[2]))) + 
            facet_wrap(~cl, nrow=nRowFacet)
    }
    
    if(isTRUE(printPlot)) print(p1)
    else return(p1)
    
}

runKMeans <- function(df, 
                      vars=NULL, 
                      centers=2, 
                      nStart=1L, 
                      iter.max=10L, 
                      seed=NULL, 
                      plotMeans=FALSE,
                      nrowMeans=NULL,
                      plotPct=NULL, 
                      nrowPct=1
                      ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame for clustering
    # vars: variables to be used for clustering (NULL means everything in df)
    # centers: number of centers
    # nStart: passed to kmeans
    # iter.max: passed to kmeans
    # seed: seed to be set (if NULL, no seed is set)
    # plotMeans: boolean, plot variable means by cluster?
    # nrowMeans: argument passed as nrow for faceting rows in plotClusterMeans() - NULL is default ggplot2
    # plotPct: list of character vectors to be passed sequentially as keyVars to plotClusterPct()
    #          NULL means do not run
    #          pctByCluster=list(c("var1"), c("var2", "var3")) will run plotting twice
    # nrowPct: argument for faceting number of rows in plotClusterPct()
    
    # Set seed if requested
    if(!is.null(seed)) set.seed(seed)
    
    # Get the variable names if passed as NULL
    if(is.null(vars)) vars <- names(df)
    
    # Run the k-means process
    km <- df %>%
        select(all_of(vars)) %>% 
        kmeans(centers=centers, iter.max=iter.max, nstart=nStart)

    # Assess clustering by dimension if requested
    if(isTRUE(plotMeans)) plotClusterMeans(km, nrow=nrowMeans)
    if(!is.null((plotPct))) 
        for(ctr in 1:length(plotPct)) 
            plotClusterPct(df=df, km=km, keyVars=plotPct[[ctr]], nRowFacet=nrowPct)
    
    # Return the k-means object
    km
    
}

# Get relevant variables
varsTrain <- tmpTempTrain %>%
    select(starts_with("pct")) %>%
    select(-pct_hour, -pct_weathercode, -ends_with("0"), -ends_with("5")) %>%
    names()
varsTrain

km003 <- runKMeans(tmpTempTrain, 
                   vars=varsTrain, 
                   centers=3, 
                   nStart=25, 
                   seed=23082113, 
                   iter.max=20L,
                   plotMeans=TRUE, 
                   plotPct=list(c("todSeason"), c("hour", "month")), 
                   nrowPct=1
                   )
str(km003)

plotClusterMeans(km003)

plotClusterPct(df=tmpTempTrain, km=km003, keyVars=c("todSeason"))
plotClusterPct(df=tmpTempTrain, km=km003, keyVars=c("hour", "month"))

```
  
The first split with 2 clusters is explored:  
```{r, fig.height=9, fig.width=9}

km002 <- runKMeans(tmpTempTrain, 
                   vars=varsTrain, 
                   centers=2, 
                   nStart=25, 
                   seed=23082113, 
                   iter.max=20L,
                   plotMeans=TRUE, 
                   plotPct=list(c("todSeason"), c("hour", "month")), 
                   nrowPct=1
                   )
str(km002)

```
  
The first split appears to be day-night, based primarily on radiation and evapotranspiration.

Sum-squares are explored:
```{r, fig.height=9, fig.width=9}

km001 <- runKMeans(tmpTempTrain, 
                   vars=varsTrain, 
                   centers=1, 
                   nStart=25, 
                   seed=23082113, 
                   iter.max=20L,
                   plotMeans=TRUE, 
                   plotPct=list(c("todSeason"), c("hour", "month")), 
                   nrowPct=1
                   )
str(km001)

sapply(list(km001, km002, km003), FUN=function(x) c("k"=length(x$size), 
                                                    "totss"=x$totss, 
                                                    "betweenss"=x$betweenss, 
                                                    "tot.withinss"=x$tot.withinss,
                                                    "iter"=x$iter,
                                                    "ifault"=ifelse(is.null(x$ifault), 0, x$ifault)
                                                    )
       ) %>%
    t() %>%
    tibble::as_tibble() %>%
    mutate(pct=pmin(1, tot.withinss/totss)) %>%
    ggplot(aes(x=k, y=pct)) + 
    geom_line() + 
    geom_point() + 
    geom_text(aes(y=pct-0.05, label=round(pct, 3)), size=2.5) +
    labs(x="# Clusters", 
         y="SS-within / SS-total", 
         title="Sum-squares within as proportion of sum-squares total"
         ) + 
    lims(y=c(0, 1))

```
  
The first cluster (k=2) primarily splits day from night and accounts for ~20% of total sum-squares. The next cluster (k=3) splits colder-season from warmer-season and accounts for an additional ~10% of total sum-squares
  
Clusters are run for 1-15 centers, cached to reduce processing time:  
```{r, cache=TRUE}

kmList <- lapply(1:15, FUN=function(x) runKMeans(tmpTempTrain, 
                                                 vars=varsTrain, 
                                                 centers=x, 
                                                 nStart=25, 
                                                 seed=23082113, 
                                                 iter.max=50L,
                                                 plotMeans=FALSE, 
                                                 plotPct=NULL
                                                 )
                 )

```
  
Change in SS-between is explored based on number of clusters:  
```{r, fig.height=9, fig.width=9}

dfSS <- sapply(kmList, FUN=function(x) c("nCluster"=length(x$size), 
                                         "totss"=x$totss, 
                                         "betweenss"=x$betweenss, 
                                         "tot.withinss"=x$tot.withinss, 
                                         "iter"=x$iter, 
                                         "ifault"=unclass(ifelse(is.null(x$ifault), 0, x$ifault))
                                         )
               ) %>% 
    t() %>% 
    tibble::as_tibble() %>% 
    mutate(pct=tot.withinss/totss, dpct=pct-lag(pct)) 
dfSS

dfSS %>% 
    ggplot(aes(x=nCluster, y=pmin(1, pct))) + 
    geom_point() + 
    geom_line() + 
    lims(y=c(0, 1)) + 
    labs(x="# Clusters", 
         y="Within SS / Total SS", 
         title="Sum-squares ratio by number of clusters (k=means)"
         ) + 
    geom_text(aes(y=pct-0.05, label=round(pct, 3)), size=2.5)

dfSS %>% 
    filter(!is.na(dpct)) %>% 
    ggplot(aes(x=factor(nCluster), y=dpct)) + 
    geom_col(fill="lightblue") + 
    labs(x="When adding this cluster", 
         y="Change in (Within SS / Total SS)", 
         title="Sum-squares ratio by number of clusters (k=means)"
         ) + 
    geom_text(aes(y=dpct/2, label=round(dpct, 3)), size=2.5)

```
  
* The first cluster added (k=2) causes a 20% drop in SS-within  
* The next three clusters added (k=3, 4, 5) each cause a 5%-10% drop in SS-within  
* Clusters after k=5 have consistent, modest drops in SS-within  
  
This is suggestive that exploring evolution of data splits at k = 1, 2, 3, 4, 5 may be informative  
  
The runKMeans() function is updated to allow for passing a k-means object:  
```{r, fig.height=9, fig.width=9}

# Updated to allow passing a k-means object
runKMeans <- function(df, 
                      km=NULL,
                      vars=NULL, 
                      centers=2, 
                      nStart=1L, 
                      iter.max=10L, 
                      seed=NULL, 
                      plotMeans=FALSE,
                      nrowMeans=NULL,
                      plotPct=NULL, 
                      nrowPct=1, 
                      returnKM=is.null(km)
                      ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame for clustering
    # km: k-means object (will shut off k-means processing and run as plot-only)
    # vars: variables to be used for clustering (NULL means everything in df)
    # centers: number of centers
    # nStart: passed to kmeans
    # iter.max: passed to kmeans
    # seed: seed to be set (if NULL, no seed is set)
    # plotMeans: boolean, plot variable means by cluster?
    # nrowMeans: argument passed as nrow for faceting rows in plotClusterMeans() - NULL is default ggplot2
    # plotPct: list of character vectors to be passed sequentially as keyVars to plotClusterPct()
    #          NULL means do not run
    #          pctByCluster=list(c("var1"), c("var2", "var3")) will run plotting twice
    # nrowPct: argument for faceting number of rows in plotClusterPct()
    # returnKM: boolean, should the k-means object be returned?
    
    # Set seed if requested
    if(!is.null(seed)) set.seed(seed)
    
    # Get the variable names if passed as NULL
    if(is.null(vars)) vars <- names(df)
    
    # Run the k-means process if the object has not been passed
    if(is.null(km)) {
        km <- df %>%
            select(all_of(vars)) %>% 
            kmeans(centers=centers, iter.max=iter.max, nstart=nStart)
    }

    # Assess clustering by dimension if requested
    if(isTRUE(plotMeans)) plotClusterMeans(km, nrow=nrowMeans)
    if(!is.null((plotPct))) 
        for(ctr in 1:length(plotPct)) 
            plotClusterPct(df=df, km=km, keyVars=plotPct[[ctr]], nRowFacet=nrowPct)
    
    # Return the k-means object
    if(isTRUE(returnKM)) return(km)
    
}

# Function run on the 3-cluster k-means object
runKMeans(df=tmpTempTrain, 
          km=km003, 
          plotMeans=TRUE, 
          plotPct=list(c("todSeason"), c("hour", "month")), 
          nrowPct=1
          )

```
  
A function is written to assign points to the nearest cluster centroid:  
```{r, fig.height=9, fig.width=9}

assignKMeans <- function(km, df, returnAllDistanceData=FALSE) {
    
    # FUNCTION ARGUMENTS:
    # km: a k-means object
    # df: data frame or tibble
    # returnAllDistanceData: boolean, should the distance data and clusters be returned?
    #                        TRUE returns a data frame with distances as V1, V2, ..., and cluster as cl
    #                        FALSE returns a vector of cluster assignments as integers
    
    # Select columns from df to match km
    df <- df %>% select(all_of(colnames(km$centers)))
    if(!all.equal(names(df), colnames(km$centers))) stop("\nName mismatch in clustering and frame\n")
    
    # Create the distances and find clusters
    distClust <- sapply(seq_len(nrow(km$centers)), 
                        FUN=function(x) sqrt(rowSums(sweep(as.matrix(df), 
                                                           2, 
                                                           t(as.matrix(km$centers[x,,drop=FALSE]))
                                                           )**2
                                                     )
                                             )
                        ) %>% 
        as.data.frame() %>% 
        tibble::as_tibble() %>% 
        mutate(cl=apply(., 1, which.min))
    
    # Return the proper file
    if(isTRUE(returnAllDistanceData)) return(distClust)
    else return(distClust$cl)
    
}

# Example of returning distance data
glimpse(assignKMeans(km=km003, df=tmpTempTrain, returnAllDistanceData=TRUE))

# Confirmation that cluster assignments match (could occasionally have a tied distance and possible mismatch)
table(assignKMeans(km=km003, df=tmpTempTrain), km003$cluster)

```
  
Clustering with k=4 is explored:  
```{r, fig.height=9, fig.width=9}

# Function run on the 4-cluster k-means object
runKMeans(df=tmpTempTrain, 
          km=kmList[[4]], 
          plotMeans=TRUE, 
          plotPct=list(c("todSeason"), c("hour", "month")), 
          nrowPct=1, 
          nrowMeans=1
          )

```
  
With 4 clusters, data are broadly split as warm/cold season and day/night
  
Clustering with k=5 is explored:  
```{r, fig.height=9, fig.width=9}

# Function run on the 5-cluster k-means object
runKMeans(df=tmpTempTrain, 
          km=kmList[[5]], 
          plotMeans=TRUE, 
          plotPct=list(c("todSeason"), c("hour", "month")), 
          nrowPct=1, 
          nrowMeans=1
          )

```
  
With 5 clusters, data are broadly split as precipitation/no with "no" further split as warm/cold season and day/night
  
Principal component analysis is run to explore variance explained by number of components:  
```{r, fig.height=9, fig.width=9}

# Correlation analysis
corTrain <- cor(tmpTempTrain[, varsTrain])
hcTrain <- hclust(as.dist((1-corTrain)/2))
orderTrain <- hcTrain$order %>% purrr::set_names(hcTrain$labels)
tmpHeat <- as.data.frame(corTrain, row.names=rownames(corTrain)) %>% 
    rownames_to_column("var1") %>% 
    tibble::as_tibble() %>% 
    pivot_longer(cols=-c("var1"), names_to="var2") %>%
    mutate()
tmpHeat %>%
    ggplot(aes(x=fct_reorder(var1, orderTrain[var1]), y=fct_reorder(var2, orderTrain[var2]))) + 
    geom_tile(aes(fill=value)) + 
    geom_text(aes(label=round(value, 2)), size=2) + 
    scale_fill_gradient2(low="red", mid="white", high="green") + 
    labs(x=NULL, y=NULL, title="Correlations") + 
    theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1))

pcaTrain <- prcomp(tmpTempTrain[, varsTrain])
summary(pcaTrain)
tibble::tibble(sd=pcaTrain$sdev, var=sd**2, n=1:length(pcaTrain$sdev)) %>%
    ggplot(aes(x=n)) + 
    geom_col(aes(y=var/sum(var)), fill="lightblue") +
    geom_text(aes(y=cumsum(var)/sum(var), label=round(cumsum(var)/sum(var), 2)), hjust=0, size=2.5) + 
    geom_line(aes(y=cumsum(var)/sum(var))) +
    labs(x="Component", y="Variance Explained", title="Variance Explained (cumulative and incremental)")

```
  
A simple random forest is explored, for prediction of month:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model
rfTempTrainMonth <- ranger::ranger(month ~ ., 
                                   data=tmpTempTrain[, c('month', varsTrain)], 
                                   importance = "impurity"
                                   )
rfTempTrainMonth

# Variable importance
rfTempTrainMonth$variable.importance %>% 
    as.data.frame() %>% 
    purrr::set_names("imp") %>% 
    rownames_to_column("metric") %>% 
    tibble::as_tibble() %>%
    ggplot(aes(x=fct_reorder(metric, imp), y=imp/1000)) + 
    geom_col(fill="lightblue") + 
    labs(x=NULL, y="Variable Importance (000)", title="Simple random forest to predict month") +
    coord_flip()

# Performance on test data (confirm >99% accuracy)
rfTempTest <- tmpTempTest %>%
    mutate(pred=predict(rfTempTrainMonth, data=.)$predictions)
cat("\nAccuracy on test dataset is: ", round(100*mean(rfTempTest$pred==rfTempTest$month), 2), "%\n", sep="")
rfTempTest %>%
    count(month, pred) %>%
    ggplot(aes(x=pred, y=month)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted month", y="Actual month", title="Predicting month on test data")

```
  
The simple random forest has over 99% predictive accuracy on month, primarily focusing on metrics related to soil (soil temperature and soil moisture at various depths).

A portion of the predictive accuracy may be based on specific soil trends during a given year, as it is very unlikely that data consistently change right at 00h00 of a new month. Models are run using a holdout year:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model, holding out 2022 data
rfTempHoldout <- ranger::ranger(month ~ ., 
                                data=tmpTempTrain[year(tmpTempTrain$date) != 2022, c('month', varsTrain)], 
                                importance = "impurity"
                                )
rfTempHoldout

# Performance on holdout data
rfTempTest <- tmpTempTrain %>%
    bind_rows(tmpTempTest) %>%
    filter(year(date)==2022) %>%
    mutate(pred=predict(rfTempHoldout, data=.)$predictions)
cat("\nAccuracy on holdout 2022 data is: ", round(100*mean(rfTempTest$pred==rfTempTest$month), 2), "%\n", sep="")
rfTempTest %>%
    count(month, pred) %>%
    ggplot(aes(x=pred, y=month)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted month (2022)", 
         y="Actual month (2022)", 
         title="Applying random forest fit without 2022 data to 2022"
         )

```
  
Without access to training data including 2022, the model still makes good predictions for 2022 month. But, predictions are commonly off by +/- 1 month leading to overall accuracy of 85% (vs. 99%+ when able to train on soil heating patterns in the given year)
  
The random forest is run using only the four most important variables:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model, with only the four most important variables, holding out 2022 data
varsTop4 <- sort(rfTempHoldout$variable.importance, decreasing=TRUE)[1:4] %>% names
rfTempHoldTop4 <- ranger::ranger(month ~ ., 
                                 data=tmpTempTrain[year(tmpTempTrain$date) != 2022, c('month', varsTop4)], 
                                 importance = "impurity"
                                 )
rfTempHoldTop4

# Performance on holdout data
tmpPred <- tmpTempTrain %>%
    bind_rows(tmpTempTest) %>%
    filter(year(date)==2022) %>%
    mutate(pred=predict(rfTempHoldTop4, data=.)$predictions)
cat("\nAccuracy on holdout 2022 data is: ", 
    round(100*mean(tmpPred$pred==tmpPred$month), 2), 
    "%\n", 
    sep=""
    )
tmpPred %>%
    count(month, pred) %>%
    ggplot(aes(x=pred, y=month)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted month (2022)", 
         y="Actual month (2022)", 
         title="Applying random forest fit without 2022 data to 2022", 
         subtitle="Most important 4 variables only"
         )


```
  
Even with just 4 variables, the simple random forest retains 80% accuracy in predicting month for a holdout year, with all predictions being +/- 1 month of actual

Accuracy of predictions by day of month is explored:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Full plot
tmpPred %>%
    bind_rows(rfTempTest, .id="src") %>%
    mutate(day=day(date), src=c("1"="Top-4", "2"="All")[src]) %>% 
    group_by(src, day) %>% 
    summarize(mu=mean(month==pred), .groups="drop") %>% 
    ggplot(aes(x=factor(day), y=mu)) + 
    geom_line(aes(group=src, color=src)) + 
    geom_point(aes(color=src), size=1) + 
    lims(y=c(0, 1)) + 
    labs(x="Day of month", 
         y="Accuracy of predicting month", 
         title="Accuracy of predicting month by day of month"
         ) + 
    scale_color_discrete("Features")

# Plot facetted by month
tmpPred %>%
    bind_rows(rfTempTest, .id="src") %>%
    mutate(day=day(date), src=c("1"="Top-4", "2"="All")[src]) %>% 
    group_by(src, day, month) %>% 
    summarize(mu=mean(month==pred), .groups="drop") %>% 
    ggplot(aes(x=factor(day), y=mu)) + 
    geom_line(aes(group=src, color=src)) + 
    geom_point(aes(color=src), size=1) + 
    lims(y=c(0, 1)) + 
    labs(x="Day of month", 
         y="Accuracy of predicting month", 
         title="Accuracy of predicting month by day of month"
         ) + 
    scale_color_discrete("Features") + 
    facet_wrap(~month)

```
  
Predictions near mid-month tend to be more accurate than predictions near the borders between months, consistent with soil temperatures gradually increasing or decreasing as seasons progress
  
A simple random forest is explored, for prediction of year:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model
rfTempTrainYear <- ranger::ranger(fct_year ~ ., 
                                  data=tmpTempTrain %>%
                                      mutate(fct_year=factor(year(date))) %>%
                                      select(all_of(c("fct_year", 'month', varsTrain))),
                                  importance = "impurity"
                                  )
rfTempTrainYear

# Variable importance
rfTempTrainYear$variable.importance %>% 
    as.data.frame() %>% 
    purrr::set_names("imp") %>% 
    rownames_to_column("metric") %>% 
    tibble::as_tibble() %>%
    ggplot(aes(x=fct_reorder(metric, imp), y=imp/1000)) + 
    geom_col(fill="lightblue") + 
    labs(x=NULL, y="Variable Importance (000)", title="Simple random forest to predict year") +
    coord_flip()

# Performance on test data (confirm >99% accuracy)
rfTempTest <- tmpTempTest %>%
    mutate(fct_year=factor(year(date)), pred=predict(rfTempTrainYear, data=.)$predictions)
cat("\nAccuracy on test dataset is: ", round(100*mean(rfTempTest$pred==rfTempTest$fct_year), 2), "%\n", sep="")
rfTempTest %>%
    count(fct_year, pred) %>%
    ggplot(aes(x=pred, y=fct_year)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted year", y="Actual year", title="Predicting year on test data")

```
  
The years are sufficiently distinct that the random forest is able to separate them perfectly, with soil moisture being a primary explanatory variable
  
The random forest is re-run using only the four most important variables:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model, with only the four most important variables
varsTop4 <- sort(rfTempTrainYear$variable.importance, decreasing=TRUE)[1:4] %>% names
rfTempYearTop4 <- ranger::ranger(fct_year ~ ., 
                                 data=tmpTempTrain %>%
                                     mutate(fct_year=factor(year(date))) %>%
                                     select(all_of(c("fct_year", varsTop4))),
                                  importance = "impurity"
                                 )
rfTempYearTop4

# Variable importance
rfTempYearTop4$variable.importance %>% 
    as.data.frame() %>% 
    purrr::set_names("imp") %>% 
    rownames_to_column("metric") %>% 
    tibble::as_tibble() %>%
    ggplot(aes(x=fct_reorder(metric, imp), y=imp/1000)) + 
    geom_col(fill="lightblue") + 
    labs(x=NULL, 
         y="Variable Importance (000)", 
         title="Simple random forest to predict year", 
         subtitle="Restricted to top-4 importance variables from previous forest") +
    coord_flip()

# Performance on test data (confirm >99% accuracy)
rfTempTest <- tmpTempTest %>%
    mutate(fct_year=factor(year(date)), pred=predict(rfTempYearTop4, data=.)$predictions)
cat("\nAccuracy on test dataset is: ", round(100*mean(rfTempTest$pred==rfTempTest$fct_year), 2), "%\n", sep="")
rfTempTest %>%
    count(fct_year, pred) %>%
    ggplot(aes(x=pred, y=fct_year)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted year", 
         y="Actual year", 
         title="Predicting year on test data", 
         subtitle="Top-4 predictors by importance only"
         )

```
  
Over the course of the 13.5-year training data, there is sufficient variation in soil temperature and moisture for the model to be assess year almost perfectly. Further exploration is needed for how the model is able to make clean distinctions between, for example, very late on December 31 and very early on January 1
  
Data are further explored for uniqueness of the four key variables:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Full dataset of values, sorted
tmpTempFull <- tmpTempTrain %>%
    bind_rows(tmpTempTest, .id="src") %>%
    arrange(time) %>%
    mutate(src=c("1"="Train", "2"="Test")[src], yyyymm=customYYYYMM(date), year=year(date))
tmpTempFull

# Number of combinations of percentile (top-4 variables)
tmpTempFull %>%
    count(across(varsTop4), sort=TRUE)

# Number of combinations of percentile (top-4 variables) and year
tmpTempFull %>%
    count(across(c(varsTop4, "year")), sort=TRUE)

# Number of combinations of percentile (top-4 variables) and year-month
tmpTempFull %>%
    count(across(c(varsTop4, "yyyymm")), sort=TRUE)

# Plot of moisture and temperature
tmpTempFull %>% 
    mutate(fct_year=factor(year)) %>% 
    count(x255=pct_soil_moisture_100_to_255cm, y100=pct_soil_moisture_28_to_100cm, fct_year, month) %>% 
    filter(fct_year %in% 2014:2017) %>% 
    ggplot() + 
    geom_point(aes(x=x255, y=y100, color=month, size=n)) + 
    facet_wrap(~fct_year) + 
    labs(x="Soil moisture percentile (100-255 cm)", 
         y="Soil moisture percentile (28-100 cm)", 
         title="Soil moisture patterns by year"
         )

```
  
Of the 117,936 observations, there are 13,279 combinations of percentile for the top-4 variables. Only a very few combinations span across different years (21) or even months (160), explaining the very high explanatory power of the model. Forward-looking predictive power is likely to be very poor, as the model needs to be trained on the specific patterns of soil moisture and temperature that evolved in a given year. But, specific patterns of observed soil moisture appear to be a characteristic signature of a specific year in the training data

The model is re-run predicting year with all variables except for the top-4:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model, excluding the four most important variables
varsNonTop4 <- sort(rfTempTrainYear$variable.importance, decreasing=TRUE)[-c(1:4)] %>% names
rfTempYearNonTop4 <- ranger::ranger(fct_year ~ ., 
                                    data=tmpTempTrain %>%
                                        mutate(fct_year=factor(year(date))) %>%
                                        select(all_of(c("fct_year", varsNonTop4))),
                                    importance = "impurity"
                                    )
rfTempYearNonTop4

# Variable importance
rfTempYearNonTop4$variable.importance %>% 
    as.data.frame() %>% 
    purrr::set_names("imp") %>% 
    rownames_to_column("metric") %>% 
    tibble::as_tibble() %>%
    ggplot(aes(x=fct_reorder(metric, imp), y=imp/1000)) + 
    geom_col(fill="lightblue") + 
    labs(x=NULL, 
         y="Variable Importance (000)", 
         title="Simple random forest to predict year", 
         subtitle="Excludes top-4 importance variables from full forest") +
    coord_flip()

# Performance on test data (confirm >99% accuracy)
rfTempTest <- tmpTempTest %>%
    mutate(fct_year=factor(year(date)), pred=predict(rfTempYearNonTop4, data=.)$predictions)
cat("\nAccuracy on test dataset is: ", round(100*mean(rfTempTest$pred==rfTempTest$fct_year), 2), "%\n", sep="")
rfTempTest %>%
    count(fct_year, pred) %>%
    ggplot(aes(x=pred, y=fct_year)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted year", 
         y="Actual year", 
         title="Predicting year on test data", 
         subtitle="Excludes top-4 predictors by importance"
         )

```
  
There is still sufficient annual difference in the data to effectively determine year based on explanatory variables excluding the top-4 in importance
  
The model is re-run for predicting month with all variables except for the top-4:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model, excluding the four most important variables
varsNonTop4 <- sort(rfTempTrainMonth$variable.importance, decreasing=TRUE)[-c(1:4)] %>% names
rfTempMonthNonTop4 <- ranger::ranger(month ~ ., 
                                     data=tmpTempTrain %>%
                                         mutate(fct_year=factor(year(date))) %>%
                                         select(all_of(c("month", varsNonTop4))),
                                     importance = "impurity"
                                     )
rfTempMonthNonTop4

# Variable importance
rfTempMonthNonTop4$variable.importance %>% 
    as.data.frame() %>% 
    purrr::set_names("imp") %>% 
    rownames_to_column("metric") %>% 
    tibble::as_tibble() %>%
    ggplot(aes(x=fct_reorder(metric, imp), y=imp/1000)) + 
    geom_col(fill="lightblue") + 
    labs(x=NULL, 
         y="Variable Importance (000)", 
         title="Simple random forest to predict month", 
         subtitle="Excludes top-4 importance variables from full forest") +
    coord_flip()

# Performance on test data (confirm >95% accuracy)
rfTempTest <- tmpTempTest %>%
    mutate(fct_year=factor(year(date)), pred=predict(rfTempMonthNonTop4, data=.)$predictions)
cat("\nAccuracy on test dataset is: ", round(100*mean(rfTempTest$pred==rfTempTest$month), 2), "%\n", sep="")
rfTempTest %>%
    count(month, pred) %>%
    ggplot(aes(x=pred, y=month)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted month", 
         y="Actual month", 
         title="Predicting month on test data", 
         subtitle="Excludes top-4 predictors by importance"
         )

```
  
There is still sufficient annual difference in the data to effectively determine month based on explanatory variables excluding the top-4 in importance
  
The random forest is re-run excluding the four most important variables, with a holdout year:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Simple random forest model, excluding the four most important variables, holding out 2022 data
varsNonTop4 <- sort(rfTempTrainMonth$variable.importance, decreasing=TRUE)[-c(1:4)] %>% names
rfTempHoldNonTop4 <- ranger::ranger(month ~ ., 
                                    data=tmpTempTrain %>%
                                        mutate(fct_year=factor(year(date))) %>%
                                        filter(year(date)<2022) %>%
                                        select(all_of(c("month", varsNonTop4))),
                                 importance = "impurity"
                                 )
rfTempHoldNonTop4

# Performance on holdout data
tmpPred <- tmpTempTrain %>%
    bind_rows(tmpTempTest) %>%
    filter(year(date)==2022) %>%
    mutate(pred=predict(rfTempHoldNonTop4, data=.)$predictions)
cat("\nAccuracy on holdout 2022 data is: ", 
    round(100*mean(tmpPred$pred==tmpPred$month), 2), 
    "%\n", 
    sep=""
    )
tmpPred %>%
    count(month, pred) %>%
    ggplot(aes(x=pred, y=month)) + 
    geom_tile(aes(fill=n)) + 
    geom_text(aes(label=n), size=2.5) +
    scale_fill_continuous("", low="white", high="green") + 
    labs(x="Predicted month (2022)", 
         y="Actual month (2022)", 
         title="Applying random forest fit without 2022 data to 2022", 
         subtitle="Most important 4 variables only"
         )
tmpPred %>% 
    select(month, pred) %>% 
    mutate(across(.cols=everything(), as.integer), delta=((month-pred+6)%%12)-6) %>% 
    ggplot(aes(x=delta)) + 
    geom_bar(fill="lightblue") + 
    labs(title="Difference in months (predicted vs. actual)", 
         x="Difference in months", 
         y="Number"
         )

```
  
Excluding top-4 variables, the model successfully memorizes patterns, but is less successful in generalizing for forward-looking predictions. Predictions on a future year are ~50% accurate, compared with ~95% accuracy for predictions on unseen data in modeled years. This suggests high autocorrelation among data elements, such that an unseen data point at 12h00 is very similar to seen data points at 11h00 and 13h00, and similar (though less so) to seen data points at 12h00 exactly 1 year ago and/or 1 year in the future. Predictions on an unseen year are usually within +/- 1 month of actual, so the model is learning generalized trends about seasons

The random forest regression is run for predicting temperature:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Variables to include for modeling
varsTemp <- tmpTempTrain %>% 
    select(-matches("pct_\\d{4}$"), -pct_temperature_2m, -pct_weathercode) %>% 
    select(starts_with("pct_")) %>%
    names
varsTemp

# Simple random forest model, excluding the four most important variables, holding out 2022 data
rfTempTemp <- ranger::ranger(temperature_2m ~ ., 
                             data=tmpTempTrain %>%
                                 select(all_of(c("temperature_2m", varsTemp))),
                             importance = "impurity"
                             )
rfTempTemp

# Variable importance
rfTempTemp$variable.importance %>% 
    as.data.frame() %>% 
    purrr::set_names("imp") %>% 
    rownames_to_column("metric") %>% 
    tibble::as_tibble() %>%
    ggplot(aes(x=fct_reorder(metric, imp), y=imp/1000)) + 
    geom_col(fill="lightblue") + 
    labs(x=NULL, 
         y="Variable Importance (000)", 
         title="Simple random forest to predict temperature"
         ) +
    coord_flip()

# Performance on test data (confirm very low error)
rfTempTest <- tmpTempTest %>%
    mutate(pred=predict(rfTempTemp, data=.)$predictions)
cat("\nMSE on test dataset is: ", round(mean((rfTempTest$pred-rfTempTest$temperature_2m)**2), 3), "\n", sep="")

rfTempTest %>%
    count(ractual=round(temperature_2m, 1), rpred=round(pred, 1)) %>%
    ggplot(aes(x=ractual, y=rpred)) + 
    geom_point(aes(size=n)) + 
    geom_smooth(aes(weight=n), method="lm") +
    scale_size_continuous("") + 
    labs(x="Actual temperature", 
         y="Predicted temperature", 
         title="Applying random forest regression for temperature"
         )

```
  
Many variables are strongly correlated, making temperature a simple prediction. Apparent temperature in particular is derived from dewpoint and temperature
  
The random forest regression is re-run for predicting temperature, with 2022-2023 as holdout years:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Variables to include for modeling
varsTemp <- tmpTempTrain %>% 
    select(-matches("pct_\\d{4}$"), -pct_temperature_2m, -pct_weathercode) %>% 
    select(starts_with("pct_")) %>%
    names
varsTemp

# Simple random forest model, excluding the four most important variables, holding out 2022 data
rfTempTempHoldout <- ranger::ranger(temperature_2m ~ ., 
                                    data=tmpTempTrain %>%
                                        filter(year(date)<2022) %>%
                                        select(all_of(c("temperature_2m", varsTemp))),
                                    importance = "impurity"
                                    )
rfTempTempHoldout

# Performance on test data (confirm very low error)
rfTempTest <- tmpTempTest %>%
    mutate(pred=predict(rfTempTempHoldout, data=.)$predictions, year=year(date), delta=temperature_2m-pred)
cat("\nMSE on test dataset for 2022-2023 (holdout years) is: ", 
    round(mean(rfTempTest[rfTempTest$year>=2022,]$delta**2), 3), 
    "\n", 
    sep=""
    )

# Plot of MSE by year
rfTempTest %>%
    group_by(year) %>%
    summarize(mse=mean(delta**2)) %>%
    ggplot(aes(x=factor(year))) + 
    geom_col(aes(y=mse), fill="lightblue") + 
    geom_text(aes(y=mse/2, label=round(mse,2))) +
    labs(x=NULL, y="MSE", title="MSE of temperature predictions (modeled using 2021 and prior data)")

# Plot of predicted vs. actual temperature in holdout years
rfTempTest %>%
    filter(year>=2022) %>%
    count(ractual=round(temperature_2m, 1), rpred=round(pred, 1)) %>%
    ggplot(aes(x=ractual, y=rpred)) + 
    geom_point(aes(size=n)) + 
    geom_smooth(aes(weight=n), method="lm") +
    scale_size_continuous("") + 
    labs(x="Actual temperature", 
         y="Predicted temperature", 
         title="Applying random forest regression for temperature", 
         subtitle="Holdout years 2022-2023 plotted (modeled on 2021 and prior)"
         )

```
  
Given correlations among the variables, predictions remain very accurate in the holdout years, though less accurate than in years where the model has been able to see nearby data

The random forest regression is re-run for predicting temperature, with only the top 4 predictors, and with 2022-2023 as holdout years:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Variables to include for modeling
varsTop4 <- sort(rfTempTempHoldout$variable.importance, decreasing=TRUE)[c(1:4)] %>% names

# Simple random forest model, excluding the four most important variables, holding out 2022 data
rfTempTempTop4Holdout <- ranger::ranger(temperature_2m ~ ., 
                                        data=tmpTempTrain %>%
                                            filter(year(date)<2022) %>%
                                            select(all_of(c("temperature_2m", varsTop4))),
                                        importance = "impurity"
                                        )
rfTempTempTop4Holdout

# Performance on test data (confirm very low error)
rfTempTest <- tmpTempTest %>%
    mutate(pred=predict(rfTempTempTop4Holdout, data=.)$predictions, 
           year=year(date), 
           delta=temperature_2m-pred
           )
cat("\nMSE on test dataset for 2022-2023 (holdout years) is: ", 
    round(mean(rfTempTest[rfTempTest$year>=2022,]$delta**2), 3), 
    "\n", 
    sep=""
    )

# Plot of MSE by year
rfTempTest %>%
    group_by(year) %>%
    summarize(mse=mean(delta**2)) %>%
    ggplot(aes(x=factor(year))) + 
    geom_col(aes(y=mse), fill="lightblue") + 
    geom_text(aes(y=mse/2, label=round(mse,2))) +
    labs(x=NULL, 
         y="MSE", 
         title="MSE of temperature predictions (modeled using 2021 and prior data)", 
         subtitle="Top 4 predictors only"
         )

# Plot of predicted vs. actual temperature in holdout years
rfTempTest %>%
    filter(year>=2022) %>%
    count(ractual=round(temperature_2m, 1), rpred=round(pred, 1)) %>%
    ggplot(aes(x=ractual, y=rpred)) + 
    geom_point(aes(size=n)) + 
    geom_smooth(aes(weight=n), method="lm") +
    scale_size_continuous("") + 
    labs(x="Actual temperature", 
         y="Predicted temperature", 
         title="Applying random forest regression for temperature (top-4 variables only)", 
         subtitle="Holdout years 2022-2023 plotted (modeled on 2021 and prior)"
         )

```
  
With only the top-4 variables, MSE on the training dataset increases, while ratio between MSE for the holdout data and MSE for the training data decreases

The random forest regression is re-run for predicting temperature, excluding the top 4 predictors, and with 2022-2023 as holdout years:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Variables to include for modeling
varsNonTop4 <- sort(rfTempTempHoldout$variable.importance, decreasing=TRUE)[-c(1:4)] %>% names

# Simple random forest model, excluding the four most important variables, holding out 2022 data
rfTempTempNonTop4Holdout <- ranger::ranger(temperature_2m ~ ., 
                                        data=tmpTempTrain %>%
                                            filter(year(date)<2022) %>%
                                            select(all_of(c("temperature_2m", varsNonTop4))),
                                        importance = "impurity"
                                        )
rfTempTempNonTop4Holdout

# Performance on test data (confirm very low error)
rfTempTest <- tmpTempTest %>%
    mutate(pred=predict(rfTempTempNonTop4Holdout, data=.)$predictions, 
           year=year(date), 
           delta=temperature_2m-pred
           )
cat("\nMSE on test dataset for 2022-2023 (holdout years) is: ", 
    round(mean(rfTempTest[rfTempTest$year>=2022,]$delta**2), 3), 
    "\n", 
    sep=""
    )

# Plot of MSE by year
rfTempTest %>%
    group_by(year) %>%
    summarize(mse=mean(delta**2)) %>%
    ggplot(aes(x=factor(year))) + 
    geom_col(aes(y=mse), fill="lightblue") + 
    geom_text(aes(y=mse/2, label=round(mse,2))) +
    labs(x=NULL, 
         y="MSE", 
         title="MSE of temperature predictions (modeled using 2021 and prior data)", 
         subtitle="Excludes top-4 predictors"
         )

# Plot of predicted vs. actual temperature in holdout years
rfTempTest %>%
    filter(year>=2022) %>%
    count(ractual=round(temperature_2m, 1), rpred=round(pred, 1)) %>%
    ggplot(aes(x=ractual, y=rpred)) + 
    geom_point(aes(size=n)) + 
    geom_smooth(aes(weight=n), method="lm") +
    scale_size_continuous("") + 
    labs(x="Actual temperature", 
         y="Predicted temperature", 
         title="Applying random forest regression for temperature (excludes top-4 variables)", 
         subtitle="Holdout years 2022-2023 plotted (modeled on 2021 and prior)"
         )

```
  
Excluding the top-4 variables, MSE on the training dataset increases, and ratio between MSE for the holdout data and MSE for the training data increases significantly. The random forest excluding the top-4 variables is no longer as effective at learning general relationships between the predictors and temperature

The random forest regression is re-run for predicting temperature, with predictors of importance rank 2-5, and with 2022-2023 as holdout years:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Variables to include for modeling
varsRank25 <- sort(rfTempTempHoldout$variable.importance, decreasing=TRUE)[c(2:5)] %>% names

# Simple random forest model, including the four specified variables, holding out 2022-2023 data
rfTempTemp25Holdout <- ranger::ranger(temperature_2m ~ ., 
                                      data=tmpTempTrain %>%
                                          filter(year(date)<2022) %>%
                                          select(all_of(c("temperature_2m", varsRank25))),
                                      importance = "impurity"
                                      )
rfTempTemp25Holdout

# Performance on test data (confirm very low error)
rfTempTest <- tmpTempTest %>%
    mutate(pred=predict(rfTempTemp25Holdout, data=.)$predictions, 
           year=year(date), 
           delta=temperature_2m-pred
           )
cat("\nMSE on test dataset for 2022-2023 (holdout years) is: ", 
    round(mean(rfTempTest[rfTempTest$year>=2022,]$delta**2), 3), 
    "\n", 
    sep=""
    )

# Plot of MSE by year
rfTempTest %>%
    group_by(year) %>%
    summarize(mse=mean(delta**2)) %>%
    ggplot(aes(x=factor(year))) + 
    geom_col(aes(y=mse), fill="lightblue") + 
    geom_text(aes(y=mse/2, label=round(mse,2))) +
    labs(x=NULL, 
         y="MSE", 
         title="MSE of temperature predictions (modeled using 2021 and prior data)", 
         subtitle="Predictors with variable importance rank 2-5 only"
         )

# Plot of predicted vs. actual temperature in holdout years
rfTempTest %>%
    filter(year>=2022) %>%
    count(ractual=round(temperature_2m, 1), rpred=round(pred, 1)) %>%
    ggplot(aes(x=ractual, y=rpred)) + 
    geom_point(aes(size=n)) + 
    geom_smooth(aes(weight=n), method="lm") +
    scale_size_continuous("") + 
    labs(x="Actual temperature", 
         y="Predicted temperature", 
         title="Applying random forest regression for temperature (variables of importance rank 2-5 only)", 
         subtitle="Holdout years 2022-2023 plotted (modeled on 2021 and prior)"
         )

# Histogram of prediction accuracy
rfTempTest %>%
    mutate(holdout=ifelse(year(date)>=2022, "Yes", "No"), err=round(pred-temperature_2m, 0)) %>%
    count(holdout, err) %>%
    group_by(holdout) %>%
    mutate(pct=n/sum(n)) %>%
    ungroup() %>%
    ggplot(aes(x=err, y=pct)) + 
    geom_col(fill="lightblue") +
    geom_text(aes(label=paste0(round(100*pct, 1), "%"), vjust=ifelse(pct>0.1, 1, 0)), size=2.5) +
    facet_wrap(~holdout, ncol=1) + 
    labs(x="Predicted minus Actual", 
         y="Proportion of observations", 
         title="Predictions by holdout (2022-2023) vs. non-holdout (2021 and prior) year"
         )

```
  
Even excluding "apparent temperature", the model learns generalized trends that are meaningfully applicable in the holdout years. Predictions are generally within 2 degrees of actual

The random forest regression is run for predicting surface windspeed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Variables to include for modeling
varsWind <- tmpTempTrain %>% 
    select(-matches("pct_\\d{4}$"), -pct_windspeed_10m, -pct_weathercode) %>% 
    select(starts_with("pct_")) %>%
    names
varsWind

# Simple random forest model with all features and no holdout year
rfTempWind <- ranger::ranger(windspeed_10m ~ ., 
                             data=tmpTempTrain %>%
                                 select(all_of(c("windspeed_10m", varsWind))),
                             importance = "impurity"
                             )
rfTempWind

# Variable importance
rfTempWind$variable.importance %>% 
    as.data.frame() %>% 
    purrr::set_names("imp") %>% 
    rownames_to_column("metric") %>% 
    tibble::as_tibble() %>%
    ggplot(aes(x=fct_reorder(metric, imp), y=imp/1000)) + 
    geom_col(fill="lightblue") + 
    labs(x=NULL, 
         y="Variable Importance (000)", 
         title="Simple random forest to predict surface (10m) windspeed"
         ) +
    coord_flip()

# Performance on test data (confirm very low error)
rfTempTest <- tmpTempTest %>%
    mutate(pred=predict(rfTempWind, data=.)$predictions)
cat("\nMSE on test dataset is: ", round(mean((rfTempTest$pred-rfTempTest$windspeed_10m)**2), 3), "\n", sep="")

rfTempTest %>%
    count(ractual=round(windspeed_10m, 0), rpred=round(pred, 1)) %>%
    ggplot(aes(x=ractual, y=rpred)) + 
    geom_point(aes(size=n)) + 
    geom_smooth(aes(weight=n), method="lm") +
    scale_size_continuous("") + 
    labs(x="Actual windspeed (10m)", 
         y="Predicted windspeed (10m)", 
         title="Applying random forest regression for surface windspeed"
         )

```
  
Surface wind gusts and high-level windspeeds appear to accurately predict surface windspeeds
