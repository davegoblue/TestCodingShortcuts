---
title: "Open Meteo Weather Exploration"
author: "davegoblue"
date: "2023-06-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
Open-Meteo maintains an [API for historical weather](https://open-meteo.com/en/docs/historical-weather-api) that allows for non-commercial usage of historical weather data maintained by the website.

This file runs exploratory analysis on some of the historical weather data.

## Exploratory Analysis
The exploration process uses tidyverse and several generic custom functions:  
```{r}

library(tidyverse) # tidyverse functionality is included throughout

source("./Generic_Added_Utility_Functions_202105_v001.R") # Basic functions

```
  
A sample of data for 365 days has been downloaded as a CSV. The downloaded data has three separate files included in a single tab, separated by a blank row. The first file is location data, the second file is hourly data, and the third file is daily data. For initial exploration, parameters specific to this file are used:  
```{r, fig.height=9, fig.width=9}

omFileLoc <- "./RInputFiles/openmeteo_20230612_example.csv"

# Location data
omLocation <- readr::read_csv(omFileLoc, n_max=1, skip=0) 
omLocation

# Hourly data 
# Elements: time, 2m temp (C), 2m dew point (C), 2m relative humidity (%), precip (mm), rain (mm), and snow (cm)
omHourlyRaw <- readr::read_csv(omFileLoc, n_max=8760, skip=3) 
omHourlyProcess <- omHourlyRaw %>%
    purrr::set_names(c("time", "temp2m_C", "relH2m", "dew2m_C", "precip_mm", "rain_mm", "snow_cm")) %>% 
    mutate(date=date(time))
omHourlyProcess
summary(omHourlyProcess)

# Daily data 
# Elements: date, sum of precip (mm), sum of rain (mm), and sum of snow (cm)
omDailyRaw <- readr::read_csv(omFileLoc, n_max=365, skip=8765) 
omDailyProcess <- omDailyRaw %>%
    purrr::set_names(c("date", "precip_mm", "rain_mm", "snow_cm"))
omDailyProcess
summary(omDailyProcess)

```
  
A function is written to read a portion of a CSV file:  
```{r, fig.height=9, fig.width=9}

partialCSVRead <- function(loc, firstRow=1L, lastRow=+Inf, col_names=TRUE, ...) {
    
    # FUNCTION arguments
    # loc: file location
    # firstRow: first row that is relevant to the partial file read (whether header line or data line)
    # last Row: last row that is relevant to the partial file read (+Inf means read until last line of file)
    # col_names: the col_names parameter passed to readr::read_csv
    #            TRUE means header=TRUE (get column names from file, read data starting on next line)
    #            FALSE means header=FALSE (auto-generate column names, read data starting on first line)
    #            character vector means use these as column names (read data starting on first line)
    # ...: additional arguments passed to read_csv

    # Read the file and return
    # skip: rows to be skipped are all those prior to firstRow
    # n_max: maximum rows read are lastRow-firstRow, with an additional data row when col_names is not TRUE
    readr::read_csv(loc, 
                    skip=firstRow-1, 
                    n_max=lastRow-firstRow+ifelse(isTRUE(col_names), 0, 1), 
                    ...
                    )
    
}

# Double check that data are the same
partialCSVRead(omFileLoc, firstRow=1L, lastRow=2L) %>% all.equal(omLocation)
partialCSVRead(omFileLoc, firstRow=4L, lastRow=8764L) %>% all.equal(omHourlyRaw)
partialCSVRead(omFileLoc, firstRow=8766L, lastRow=+Inf) %>% all.equal(omDailyRaw)

```
  
The blank lines are assessed, allowing for all tables to be read at the same time:  
```{r, fig.height=9, fig.width=9}

# Get the break points for gaps in a vector (e.g., 0, 3, 5:8, 20 has break points 0, 3, 5, 20 and 0, 3, 8, 30)
vecGaps <- function(x, addElements=c(), sortUnique=TRUE) {
    
    if(length(addElements)>0) x <- c(addElements, x)
    if(isTRUE(sortUnique)) x <- unique(sort(x))
    list("starts"=c(x[is.na(lag(x)) | x-lag(x)>1], +Inf), 
         "ends"=x[is.na(lead(x)) | lead(x)-x>1]
         )
    
}

vecGaps(c(3, 5:8, 20), addElements=0)

# Find the break points in a single file
flatFileGaps <- function(loc) {

    which(stringr::str_length(readLines(loc))==0) %>% vecGaps(addElements=0)
    
}

flatFileGaps(omFileLoc)


# Read all relevant data as CSV with header
readMultiCSV <- function(loc, col_names=TRUE, ...) {

    gaps <- flatFileGaps(loc)
    
    lapply(seq_along(gaps$ends), 
           FUN=function(x) partialCSVRead(loc, 
                                          firstRow=gaps$ends[x]+1, 
                                          lastRow=gaps$starts[x+1]-1, 
                                          col_names=col_names, 
                                          ...
                                          )
           )
    
}

tstMultiCSV <- readMultiCSV(omFileLoc)

all.equal(tstMultiCSV[[1]], omLocation)
all.equal(tstMultiCSV[[2]], omHourlyRaw)
all.equal(tstMultiCSV[[3]], omDailyRaw)

```
  
Data can also be downloaded through the Open-Meteo API, returning a JSON file. The data download has been completed off-line to minimize repeated hits against the server. The JSON file can then be read:  
```{r}

# Example download sequence
# download.file("https://archive-api.open-meteo.com/v1/archive?latitude=41.85&longitude=-87.65&start_date=2022-06-01&end_date=2023-06-08&hourly=temperature_2m,relativehumidity_2m,dewpoint_2m,precipitation,rain,snowfall&daily=precipitation_sum,rain_sum,snowfall_sum&timezone=America%2FChicago", "tempOM")

# Create hourly data tibble
jsonHourly <- jsonlite::read_json("tempOM", simplifyVector = TRUE)[["hourly"]] %>% 
    tibble::as_tibble() %>% 
    mutate(tm=lubridate::ymd_hm(time), date=date(tm))
jsonHourly

# Create daily data tibble
jsonDaily <- jsonlite::read_json("tempOM", simplifyVector = TRUE)[["daily"]] %>% 
    tibble::as_tibble()
jsonDaily

# Extract other elements
jsonNames <- jsonlite::read_json("tempOM", simplifyVector = TRUE) %>% names
for (jsonName in jsonNames[!(jsonNames %in% c("daily", "hourly", "daily_units", "hourly_units"))]) {
    cat("\n", jsonName, ":", jsonlite::read_json("tempOM", simplifyVector = TRUE)[[jsonName]])
}
for (jsonName in jsonNames[jsonNames %in% c("daily_units", "hourly_units")]) {
    cat("\n", jsonName, ":\n")
    print(jsonlite::read_json("tempOM", simplifyVector = TRUE)[[jsonName]] %>% tibble::as_tibble() %>% t())
}

```
  
Daily data read from JSON and CSV are compared:  
```{r}

# Convert variable names in JSON daily data
jsonDailyProcess <- jsonDaily %>%
    colRenamer(vecRename=c("precipitation_sum"="precip_mm", 
                           "rain_sum"="rain_mm", 
                           "snowfall_sum"="snow_cm", 
                           "time"="date"
                           )
               ) %>%
    mutate(date=as.Date(date))
jsonDailyProcess

# Check dates included
omDailyProcess %>% 
    select(date) %>% 
    mutate(inCSV=1) %>% 
    full_join(mutate(select(jsonDailyProcess, "date"), inJSON=1), by="date") %>%
    filter(!complete.cases(.))

# Check column names
all.equal(names(omDailyProcess), names(jsonDailyProcess))

# Check data elements from 2022-06-08 through 2023-06-04 (last full day of data)
all.equal(omDailyProcess %>% tibble::as_tibble() %>% filter(date>="2022-06-08", date<="2023-06-04"), 
          jsonDailyProcess %>% filter(date>="2022-06-08", date<="2023-06-04")
          )

```
  
