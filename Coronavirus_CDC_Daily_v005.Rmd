---
title: "CDC Daily by State"
author: "davegoblue"
date: "2022-09-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
This file is designed to use CDC data to assess coronavirus disease burden by state, including creating and analyzing state-level clusters.

Through March 7, 2021, [The COVID Tracking Project](https://covidtracking.com/) collected and integrated data on tests, cases, hospitalizations, deaths, and the like by state and date.  The latest code for using this data is available in Coronavirus_Statistics_CTP_v004.Rmd.

The COVID Tracking Project suggest that [US federal data sources](https://covidtracking.com/analysis-updates/federal-covid-data-101-how-to-find-data) are now sufficiently robust to be used for analyses that previously relied on COVID Tracking Project.  This code is an attempt to update modules in Coronavirus_Statistics_CTP_v004.Rmd to leverage US federal data.

The code in this module builds on code available in _v004, with function and mapping files updated:  
  
* Generic_Added_Utility_Functions_202105_v001.R - generic functions that can be used in other areas  
* Coronavirus_CDC_Daily_Functions_v001.R - functions specific to coronavirus daily data  
  
Broadly, the CDC data analyzed by this module includes:  
  
* CDC case and death data by state and date are available for download on the [CDC website](https://data.cdc.gov/api/views/9mfq-cb36/rows.csv?accessType=DOWNLOAD)  
* CDC hospital data are available for download on the [healthdata.gov website](https://beta.healthdata.gov/api/views/g62h-syeh/rows.csv?accessType=DOWNLOAD)  
* CDC vaccines data are also available for download on the [CDC website](https://data.cdc.gov/api/views/unsk-b7fc/rows.csv?accessType=DOWNLOAD):  
  
## Functions and Mapping Files
The tidyverse package is loaded and functions are sourced:  
```{r}

# The tidyverse functions are routinely used without package::function format
library(tidyverse)
library(geofacet)

# Functions are available in source file
source("./Generic_Added_Utility_Functions_202105_v001.R")
source("./Coronavirus_CDC_Daily_Functions_v002.R")

```

A series of mapping files are also available to allow for parameterized processing.  Mappings include:  
  
* urlMapper - mapping file for urlType and url location to download data  
* renMapper - mapping file for renaming of variables in the raw data file  
* selfListMapper - mapping file for transformations by variable type  
* fullListMapper - mapping file for transformations across variable types  
* lstComboMapper - mapping file for elements to be combined by data type (most common is to combine NYC and NYS data to NY if the file provides them separately)  
* uqMapper - mapping file for fields that should combine to be unique keys for processed files  
* lstFilterMapper - mapping file for filtering to subset (most common is to keep 50 states and DC)  
* vecSelectMapper - mapping file for variables to keep  
* checkControlGroupMapper - mapping file for group_by() of control total checks  
* checkControlAggMapper - mapping file for numeric variables for control total checks  
* checkSimilarityMapper - mapping file for similarity checks to perform  
* plotSimilarityMaooer - mapping file for fields where differences in universe should be plotted
* keyAggMapper - mapping file for the aggregate-level control total checks to perform  
* perCapMapper - named vector that drives conversion from original field name to per capita field name  
* hhsMapper - named vector that drivers numerical variables to keep (and renaming) from HHS capacity data file
  
These default parameters are maintained in a separate .R file and can be sourced:  
```{r}

source("./Coronavirus_CDC_Daily_Default_Mappings_v002.R")

```

## Example Code Processing
The function is run to download and process the latest CDC case, hospitalization, and death data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcDaily"="./RInputFiles/Coronavirus/CDC_dc_downloaded_220907.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_220907.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_220907.csv"
                 )
compareList <- list("cdcDaily"=readFromRDS("cdc_daily_220805")$dfRaw$cdcDaily, 
                    "cdcHosp"=readFromRDS("cdc_daily_220805")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_220805")$dfRaw$vax
                    )

cdc_daily_220907 <- readRunCDCDaily(thruLabel="Sep 05, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_220907, ovrWriteError=FALSE)

```
  
The function is run to download and process the latest hospitalization data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run for latest data, save as RDS
indivHosp_20220907 <- downloadReadHospitalData(loc="./RInputFiles/Coronavirus/HHS_Hospital_20220907.csv")
saveToRDS(indivHosp_20220907, ovrWriteError=FALSE)

```
  
Post-processing is run, including hospital summaries:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create pivoted burden data
burdenPivotList_220907 <- postProcessCDCDaily(cdc_daily_220907, 
                                              dataThruLabel="Aug 2022", 
                                              keyDatesBurden=c("2022-08-31", "2022-02-28", 
                                                               "2021-08-31", "2021-02-28"
                                                               ),
                                              keyDatesVaccine=c("2022-08-31", "2022-03-31", 
                                                                "2021-10-31", "2021-05-31"
                                                                ), 
                                              returnData=TRUE
                                              )

# Create hospitalized per capita data
hospPerCap_220907 <- hospAgePerCapita(readFromRDS("dfStateAgeBucket2019"), 
                                      lst=burdenPivotList_220907, 
                                      popVar="pop2019", 
                                      excludeState=c(), 
                                      cumStartDate="2020-07-15"
                                      )

burdenPivotList_220907$hospAge %>%
    group_by(adultPed, confSusp, age, name) %>%
    summarize(value=sum(value, na.rm=TRUE), n=n(), .groups="drop")

saveToRDS(burdenPivotList_220907, ovrWriteError=FALSE)
saveToRDS(hospPerCap_220907, ovrWriteError=FALSE)

```
  
Peaks and valleys of key metrics are also updated:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

peakValleyCDCDaily(cdc_daily_220907)

```
  
Hospital capacity is updated using a mix of old data (for 2021) and new data:  
```{r, fig.height=9, fig.width=9}

identical(names(indivHosp_20220907), names(readFromRDS("indivHosp_20220704")))

modHospData <- bind_rows(filter(readFromRDS("indivHosp_20220704"), lubridate::year(collection_week)<2022), 
                         filter(indivHosp_20220907, lubridate::year(collection_week)>=2022), 
                         .id="src"
                         )
updated_modStateHosp_20220907 <- hospitalCapacityCDCDaily(modHospData, 
                                                          plotSub="Aug 2020 to Aug 2022\nOld data used pre-2022"
                                                          )

```
  
Data availability by source and time is assessed:  
```{r, fig.height=9, fig.width=9}

# Temporary function to aggregate data
tempCounter <- function(df) {
    df %>%
        select(hospital_pk, collection_week, all_of(names(hhsMapper))) %>%
        colRenamer(vecRename=hhsMapper) %>%
        pivot_longer(-c(hospital_pk, collection_week)) %>%
        filter(!is.na(value), value>0) %>%
        count(collection_week, name)
}

dfTemp <- bind_rows(tempCounter(indivHosp_20220907), tempCounter(readFromRDS("indivHosp_20220704")), .id="src")

dfTemp %>%
    select(collection_week, name) %>%
    unique() %>%
    bind_rows(., ., .id="src")  %>%
    full_join(dfTemp, by=c("src", "collection_week", "name")) %>%
    mutate(src=c("1"="SEP-2022", "2"="JUL-2022")[src]) %>%
    mutate(n=ifelse(is.na(n), 0, n)) %>%
    ggplot(aes(x=collection_week, y=n)) +
    geom_line(aes(group=src, color=src)) + 
    facet_wrap(~name) + 
    labs(title="Number of hospitals in US reporting >0 on metric by week", x=NULL, y="# Hospitals Reporting > 0") + 
    scale_color_discrete("Data Source:")

dfTemp %>%
    select(collection_week, name) %>%
    unique() %>%
    bind_rows(., ., .id="src")  %>%
    full_join(dfTemp, by=c("src", "collection_week", "name")) %>%
    mutate(src=c("1"="SEP-2022", "2"="JUL-2022")[src]) %>%
    mutate(n=ifelse(is.na(n), 0, n)) %>% 
    group_by(collection_week, name) %>% 
    summarize(delta=sum(ifelse(src=="SEP-2022", n, 0)-ifelse(src!="SEP-2022", n, 0)), .groups="drop") %>%
    ggplot(aes(x=collection_week, y=delta)) +
    geom_line(aes(color=case_when(delta>=0 ~ "darkgreen", TRUE ~ "red"))) +
    geom_hline(yintercept=0, lty=2) +
    geom_vline(xintercept=c(as.Date("2021-08-20"), as.Date("2022-06-24")), lty=2) +
    scale_color_identity(NULL) +
    facet_wrap(~name) + 
    labs(title="Delta in Number of hospitals in US reporting >0 on metric by week", 
         subtitle="Trend break dashed lines at 2021-08-20 and 2022-06-24",
         x=NULL, 
         y="Delta in # Hospitals Reporting > 0"
         )
    
```
  
The process is converted to functional form:  
```{r, fig.height=9, fig.width=9}

multiSourceDataCombine <- function(lst, timeVec, keyVar="collection_week", idName="src") {
    
    # FUNCTION ARGUMENTS:
    # lst: list of data frames to be combined
    # timeVec: vector of time cut points (data before timeVec[1] taken from lst[[1]], etc.)
    # keyVar: variable describing time in the data
    # idName: name of column for .id when files combined
    
    # Check list lengths
    if(length(lst)==0) {
        cat("\nEmpty list passed, returning 0x0 tibble\n")
        return(tibble::tibble())
    } else if (length(lst)==1) {
        cat("\nList of length 1 passed, returning item in list as-is")
        return(lst[[1]])
    }
    
    # Check that timeVec matches
    if(length(lst) != length(timeVec) + 1) stop("\nMismatch of lst and timeVec\n")
    
    # Check that all data frames have the same column names in the same order
    vecNames <- names(lst[[1]])
    for(n in 2:length(lst)) if(!isTRUE(identical(names(lst[[n]]), vecNames))) stop("\nName mismatch in files\n")
    
    # Combine data
    bind_rows(lst, .id=idName) %>%
        mutate(srcNum=as.integer(get(idName)), 
               dateMin=ifelse(srcNum==1, NA, timeVec[srcNum-1]), 
               dateMax=ifelse(srcNum==max(srcNum), NA, timeVec[srcNum])
               ) %>%
        filter(is.na(dateMin) | get(keyVar) >= dateMin, 
               is.na(dateMax) | get(keyVar) < dateMax
               ) %>%
        select(-srcNum, -dateMin, -dateMax)
    
}

# Create modified hospital data
multiSourceHosp_20220902 <- multiSourceDataCombine(list(readFromRDS("indivHosp_20220704"), 
                                                        indivHosp_20220907
                                                        ), 
                                                   timeVec=as.Date("2022-01-01")
                                                   )

# Confirm that function produces expected output
multiSourceHosp_20220902 %>%
    select(-src) %>%
    identical(modHospData %>% select(-src))

```
  
The updated hospital data are then plotted:
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run hospital plots
modStateHosp_20220902 <- hospitalCapacityCDCDaily(multiSourceHosp_20220902, 
                                                  plotSub="Aug 2020 to Aug 2022\nOld data used pre-2022"
                                                  )

```
  
## Data Refreshes
### _Data From 2022-10-02_
The latest CDC case, hospitalization, and death data are downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcDaily"="./RInputFiles/Coronavirus/CDC_dc_downloaded_221002.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_221002.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_221002.csv"
                 )
compareList <- list("cdcDaily"=readFromRDS("cdc_daily_220907")$dfRaw$cdcDaily, 
                    "cdcHosp"=readFromRDS("cdc_daily_220907")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_220907")$dfRaw$vax
                    )

cdc_daily_221002 <- readRunCDCDaily(thruLabel="Sep 30, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_221002, ovrWriteError=FALSE)

```
  
The function is run to download and process the latest hospitalization data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run for latest data, save as RDS
indivHosp_20221003 <- downloadReadHospitalData(loc="./RInputFiles/Coronavirus/HHS_Hospital_20221003.csv")
saveToRDS(indivHosp_20221003, ovrWriteError=FALSE)

```
  
Post-processing is run, including hospital summaries:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create pivoted burden data
burdenPivotList_221002 <- postProcessCDCDaily(cdc_daily_221002, 
                                              dataThruLabel="Sep 2022", 
                                              keyDatesBurden=c("2022-09-30", "2022-03-31", 
                                                               "2021-09-30", "2021-03-31"
                                                               ),
                                              keyDatesVaccine=c("2022-09-28", "2022-03-31", 
                                                                "2021-09-30", "2021-03-31"
                                                                ), 
                                              returnData=TRUE
                                              )

# Create hospitalized per capita data
hospPerCap_221002 <- hospAgePerCapita(readFromRDS("dfStateAgeBucket2019"), 
                                      lst=burdenPivotList_221002, 
                                      popVar="pop2019", 
                                      excludeState=c(), 
                                      cumStartDate="2020-07-15"
                                      )

burdenPivotList_221002$hospAge %>%
    group_by(adultPed, confSusp, age, name) %>%
    summarize(value=sum(value, na.rm=TRUE), n=n(), .groups="drop")

saveToRDS(burdenPivotList_221002, ovrWriteError=FALSE)
saveToRDS(hospPerCap_221002, ovrWriteError=FALSE)

```
  
Peaks and valleys of key metrics are also updated:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

peakValleyCDCDaily(cdc_daily_221002)

```
  
A function is written for the hospital data availability checks:  
```{r, fig.height=9, fig.width=9}

checkHospitalDataComplete <- function(df1=NULL, 
                                      df2=NULL,
                                      dfAll=NULL,
                                      lab1="DF1", 
                                      lab2="DF2", 
                                      trendBreaks=c(), 
                                      makeP1=TRUE, 
                                      makeP2=TRUE,
                                      returnData=FALSE
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # df1: the first data frame (NULL means integrated frame passed as dfAll)
    # df2: the second data frame (NULL means integrated frame passed as dfAll)
    # dfAll: integrated data frame from previous iteration of function (will ignore df1 and df2)
    # lab1: plot label for the first data frame
    # lab2: plot label for the second data frame
    # trendBreaks: character vector of trend break dates of form YYYY-MM-DD - if c(), no trend brek vlines plotted
    # makeP1: boolean, should the first plot be created and printed?
    # makeP2: boolean, should the first plot be created and printed?
    # returnData: boolean, should dfAll be returned?
    
    # Temporary function to aggregate data
    tempCountComplete <- function(df) {
        df %>%
            select(hospital_pk, collection_week, all_of(names(hhsMapper))) %>%
            colRenamer(vecRename=hhsMapper) %>%
            pivot_longer(-c(hospital_pk, collection_week)) %>%
            filter(!is.na(value), value>0) %>%
            count(collection_week, name)
    }
    
    # Create or use passed data
    if(is.null(dfAll)) {
        if(is.null(df1) | is.null(df2)) stop("dfAll not passed requires both df1 and df2 to be passed\n")
        dfAll <- bind_rows(tempCountComplete(df1), tempCountComplete(df2), .id="src")
    } else {
        if(!is.null(df1) | !is.null(df2)) warning("dfAll passed and will be used; df1 and/or df2 ignored\n")
    }
    
    # Plot data completeness - hospitals reporting >0 on metric by week
    if(isTRUE(makeP1)) {
        p1 <- dfAll %>%
            select(collection_week, name) %>%
            unique() %>%
            bind_rows(., ., .id="src")  %>%
            full_join(dfAll, by=c("src", "collection_week", "name")) %>%
            mutate(src=c("1"=lab1, "2"=lab2)[src]) %>%
            mutate(n=ifelse(is.na(n), 0, n)) %>%
            ggplot(aes(x=collection_week, y=n)) +
            geom_line(aes(group=src, color=src)) + 
            facet_wrap(~name) + 
            labs(title="Number of hospitals in US reporting >0 on metric by week", 
                 x=NULL, 
                 y="# Hospitals Reporting > 0"
                 ) + 
            scale_color_discrete("Data Source:")
        if(length(trendBreaks) > 0) {
            p1 <- p1 + geom_vline(xintercept=as.Date(all_of(trendBreaks)), lty=2) +
                labs(subtitle=paste0("Trend break dashed lines at ", 
                                     paste0(all_of(trendBreaks), collapse=" and ")
                                     )
                     )
        }
        print(p1)
    }
    

    # Plot data completeness - showing difference in reported data and trend break dates
    if(isTRUE(makeP2)) {
        p2 <- dfAll %>%
            select(collection_week, name) %>%
            unique() %>%
            bind_rows(., ., .id="src")  %>%
            full_join(dfAll, by=c("src", "collection_week", "name")) %>%
            mutate(src=c("1"=lab1, "2"=lab2)[src]) %>%
            mutate(n=ifelse(is.na(n), 0, n)) %>% 
            group_by(collection_week, name) %>% 
            summarize(delta=sum(ifelse(src==lab1, n, 0)-ifelse(src!=lab1, n, 0)), .groups="drop") %>%
            ggplot(aes(x=collection_week, y=delta)) +
            geom_line() +
            geom_point(data=~filter(., delta != 0), 
                       aes(color=case_when(delta>=0 ~ "darkgreen", TRUE ~ "red"))
                       ) +
            geom_hline(yintercept=0, lty=2) +
            scale_color_identity(NULL) +
            facet_wrap(~name) + 
            labs(title="Delta in Number of hospitals in US reporting >0 on metric by week", 
                 x=NULL, 
                 y="Delta in # Hospitals Reporting > 0"
                 )
        if(length(trendBreaks) > 0) {
            p2 <- p2 + geom_vline(xintercept=as.Date(all_of(trendBreaks)), lty=2) +
                labs(subtitle=paste0("Trend break dashed lines at ", 
                                     paste0(all_of(trendBreaks), collapse=" and ")
                                     )
                )
        }
        print(p2)
    }
    
    if(isTRUE(returnData)) return(dfAll)
    
}

# Create the data
dfTemp <- checkHospitalDataComplete(df1=readFromRDS("indivHosp_20221003"), 
                                    df2=readFromRDS("indivHosp_20220704"), 
                                    makeP1=FALSE, 
                                    makeP2=FALSE,
                                    returnData=TRUE
                                    )

# Create the first plot
checkHospitalDataComplete(dfAll=dfTemp,
                          lab1="OCT-2022", 
                          lab2="JUL-2022", 
                          makeP2=FALSE, 
                          trendBreaks=c("2021-09-25", "2022-06-24")
                          )

# Create the second plot
checkHospitalDataComplete(dfAll=dfTemp,
                          lab1="OCT-2022", 
                          lab2="JUL-2022", 
                          makeP1=FALSE, 
                          trendBreaks=c("2021-09-25", "2022-06-24")
                          )

```
  
The discontinuity issues due to occasional minor changes between positive and negative is resolved by using points colored by positive/negative and a solid line

Hospital data are pieced together as needed:
```{r, fig.height=9, fig.width=9}

# Create modified hospital data
multiSourceHosp_20221002 <- multiSourceDataCombine(list(readFromRDS("indivHosp_20220704"),
                                                        readFromRDS("indivHosp_20221003")
                                                        ),
                                                   timeVec=as.Date("2022-01-01")
                                                   )

```
  
The updated hospital data are then plotted:
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run hospital plots
modStateHosp_20221002 <- hospitalCapacityCDCDaily(multiSourceHosp_20221002, 
                                                  plotSub="Aug 2020 to Sep 2022\nOld data used pre-2022"
                                                  )

```
  
### _Data From 2022-11-02_
The latest CDC case, hospitalization, and death data are downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcDaily"="./RInputFiles/Coronavirus/CDC_dc_downloaded_221102.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_221102.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_221102.csv"
                 )
compareList <- list("cdcDaily"=readFromRDS("cdc_daily_221002")$dfRaw$cdcDaily, 
                    "cdcHosp"=readFromRDS("cdc_daily_221002")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_221002")$dfRaw$vax
                    )

cdc_daily_221102 <- readRunCDCDaily(thruLabel="Oct 31, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_221102, ovrWriteError=FALSE)

```
  
The CDC modified deaths and cases reporting to be weekly, based on summation of county-level data to states. Per their website, methodology has changed (inconsistency between datasets?) and the previous daily data is no longer updated as of October 20, 2022. Weekly deaths and cases data are available at [CDC website](https://data.cdc.gov/Case-Surveillance/Weekly-United-States-COVID-19-Cases-and-Deaths-by-/pwn4-m3yp)
  
The latest hospitalization data is also downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run for latest data, save as RDS
indivHosp_20221103 <- downloadReadHospitalData(loc="./RInputFiles/Coronavirus/HHS_Hospital_20221103.csv")
saveToRDS(indivHosp_20221103, ovrWriteError=FALSE)

```
  
Post-processing is run, including hospital summaries:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create pivoted burden data
burdenPivotList_221102 <- postProcessCDCDaily(cdc_daily_221102, 
                                              dataThruLabel="Oct 2022", 
                                              keyDatesBurden=c("2022-10-15", "2022-04-15", 
                                                               "2021-10-15", "2021-04-15"
                                                               ),
                                              keyDatesVaccine=c("2022-10-26", "2022-04-30", 
                                                                "2021-10-31", "2021-04-30"
                                                                ), 
                                              returnData=TRUE
                                              )

# Create hospitalized per capita data
hospPerCap_221102 <- hospAgePerCapita(readFromRDS("dfStateAgeBucket2019"), 
                                      lst=burdenPivotList_221102, 
                                      popVar="pop2019", 
                                      excludeState=c(), 
                                      cumStartDate="2020-07-15"
                                      )

burdenPivotList_221102$hospAge %>%
    group_by(adultPed, confSusp, age, name) %>%
    summarize(value=sum(value, na.rm=TRUE), n=n(), .groups="drop")

saveToRDS(burdenPivotList_221102, ovrWriteError=FALSE)
saveToRDS(hospPerCap_221102, ovrWriteError=FALSE)

```
  
Peaks and valleys of key metrics are also updated:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

peakValleyCDCDaily(cdc_daily_221102)

```
  
Hospital data are pieced together as needed:
```{r, fig.height=9, fig.width=9}

# Create modified hospital data
multiSourceHosp_20221102 <- multiSourceDataCombine(list(readFromRDS("indivHosp_20220704"),
                                                        readFromRDS("indivHosp_20221103")
                                                        ),
                                                   timeVec=as.Date("2022-01-01")
                                                   )

```
  
The updated hospital data are then plotted:
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run hospital plots
modStateHosp_20221102 <- hospitalCapacityCDCDaily(multiSourceHosp_20221102, 
                                                  plotSub="Aug 2020 to Oct 2022\nOld data used pre-2022"
                                                  )

```
  
An example of the new data file is downloaded manually and then read:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

tmpBurden <- fileRead("./RInputFiles/Coronavirus/Weekly_United_States_COVID-19_Cases_and_Deaths_by_State.csv")
glimpse(tmpBurden)

# Check for states
tmpStateBurden <- tmpBurden %>%
    arrange(end_date, state) %>%
    group_by(state) %>%
    summarize(across(c(new_cases, new_deaths), .fns=sum, na.rm=TRUE), 
              across(c(tot_cases, tot_deaths), .fns=max, na.rm=TRUE)
              ) %>%
    ungroup()

# Are all 50 states and DC included?
tmpStateBurden %>%
    filter(state %in% c(state.abb, "DC")) %>%
    arrange(desc(tot_deaths)) %>%
    print(n=+Inf)

# What other states are included?
tmpStateBurden %>%
    filter(!(state %in% c(state.abb, "DC"))) %>%
    arrange(desc(tot_deaths)) %>%
    print(n=+Inf)

# Are there disconnects between total and sum of new?
tmpStateBurden %>%
    filter((new_cases != tot_cases) | (new_deaths != tot_deaths)) %>%
    mutate(ratCase=tot_cases/new_cases, ratDeath=tot_deaths/new_deaths)

```
  
NYC data appear to be tracked separately from NY data, requiring combination. Otherwise, the expected geographical units appear to be included, and with totals and sum of new matching (exceptions for deaths in DC, NYC, and Kansas).

Similarity of total burden is compared:  
```{r, fig.height=9, fig.width=9}

tmpBurdenDate <- tmpBurden %>%
    select(date=end_date, state, where(is.numeric)) %>%
    bind_rows(cdc_daily_221102$dfProcess$cdcDaily, .id="src") %>%
    mutate(src=c("1"="CDC weekly (new)", "2"="CDC daily (old)")[src])
tmpBurdenDate

# Plot for total deaths and total cases
tmpBurdenDate %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(src, date) %>%
    summarize(across(where(is.numeric), sum, na.rm=TRUE), .groups="drop") %>%
    pivot_longer(where(is.numeric)) %>%
    filter(name %in% c("tot_cases", "tot_deaths")) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=src, group=src)) + 
    facet_wrap(~name, scales="free_y")

# Plot by state - latest burden
tmpBurdenDate %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(src, state) %>%
    summarize(across(where(is.numeric), last, order_by=date), .groups="drop") %>%
    pivot_longer(where(is.numeric)) %>%
    filter(name %in% c("tot_cases", "tot_deaths")) %>%
    ggplot(aes(x=fct_reorder(state, value), y=value)) + 
    geom_point(aes(color=src)) + 
    facet_wrap(~name, scales="free_x") + 
    coord_flip() + 
    labs(x=NULL, y=NULL, title="Most recent burden by state and data source") + 
    scale_color_discrete("Source")

# Plot by state - difference in burden
tmpBurdenDate %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(src, state) %>%
    summarize(across(where(is.numeric), last, order_by=date), .groups="drop") %>%
    pivot_longer(where(is.numeric)) %>%
    filter(name %in% c("tot_cases", "tot_deaths")) %>%
    group_by(state, name) %>%
    summarize(value=sum(ifelse(src!="CDC daily (old)", value, 0)) - sum(ifelse(src=="CDC daily (old)", value, 0)), 
              .groups="drop"
              ) %>%
    ggplot(aes(x=fct_reorder(state, value), y=value)) + 
    geom_point() + 
    facet_wrap(~name, scales="free_x") + 
    coord_flip() + 
    labs(x=NULL, y=NULL, title="Change in most recent burden by state (new minus old)") + 
    geom_hline(yintercept=0, lty=2)

```
  
At a first glance, national totals are well aligned between the existing daily data file and the new weekly data file. The newer data has two weeks of extra reporting, so most recent totals by state are generally slightly higher. The new data breaks apart NYC and NY, which need to be combined for processing the new file

Function readQCRawCDCDaily() is updated:  
```{r, fig.height=9, fig.width=9}

# Function to read and check a raw data file (last updated 16-NOV-2022, previously updated 02-AUG-2021)
readQCRawCDCDaily <- function(fileName, 
                              writeLog=NULL,
                              ovrwriteLog=TRUE,
                              dfRef=NULL,
                              urlType=NULL,
                              url=NULL, 
                              getData=TRUE,
                              ovrWriteDownload=FALSE, 
                              vecRename=NULL, 
                              selfList=NULL,
                              fullList=NULL,
                              uniqueBy=NULL, 
                              step3Group=NULL,
                              step3Vals=NULL, 
                              step4KeyVars=NULL, 
                              step5PlotItems=NULL,
                              step6AggregateList=NULL,
                              inferVars=list("url"=urlMapper, 
                                             "vecRename"=renMapper, 
                                             "selfList"=selfListMapper, 
                                             "fullList"=fullListMapper, 
                                             "uniqueBy"=uqMapper, 
                                             "step3Group"=checkControlGroupMapper,
                                             "step3Vals"=checkControlVarsMapper, 
                                             "step4KeyVars"=checkSimilarityMapper, 
                                             "step5PlotItems"=plotSimilarityMapper,
                                             "step6AggregateList"=keyAggMapper
                                             )
                              ) {
    
    # FUNCTION ARGUMENTS
    # fileName: the location where downloaded data either is, or will be, stored
    # writeLog: the external file location for printing (NULL means use the main log stdout)
    # ovrwriteLog: boolean, if using an external log, should it be started from scratch (overwritten)?
    # dfRef: a reference data frame for comparison (either NULL or NA means do not run comparisons)
    # urlType: character vector that can be mapped using urlMapper and keyVarMapper
    # url: direct URL passed as character string
    #      NOTE that if both url and urlType are NULL, no file will be downloaded
    # getData: boolean, should an attempt be made to get new data using urlType or url?
    # ovrWriteDownload: boolean, if fileName already exists, should it be overwritten?
    # vecRename: vector for renaming c('existing name'='new name'), can be any length from 0 to ncol(df)
    #            NULL means infer from urlType, if not available there use c()
    # selfList: list for functions to apply to self, list('variable'=fn) will apply variable=fn(variable)
    #           processed in order, so more than one function can be applied to self
    #           NULL means infer from urlType, if not available in mapping file use list()
    # fullList: list for general functions to be applied, list('new variable'=expression(code))
    #           will create 'new variable' as eval(expression(code))
    #           for now, requires passing an expression
    #           NULL means infer from urlType, use list() if not in mapping file
    # uniqueBy: combination of variables for checking uniqueness
    #           NULL means infer from data, keep as NULL (meaning use-all) if cannot be inferred
    # step3Group: variable to be used as the x-axis (grouping) for step 3 plots
    #             NULL means infer from data
    # step3Vals: values to be plotted on the y-axis for step 3 plots
    #            NULL means infer from data
    # step4KeyVars: list of parameters to be passed as keyVars= in step 4
    #               NULL means infer from urlType
    # step5PlotItems: items to be plotted in step 5
    #                 NULL means infer from urlType
    # step6AggregateList: drives the elements to be passed to compareAggregate() and flagLargeDelta()
    #                     NULL means infer from urlType
    # inferVars: vector of c('variable'='mapper') for inferring parameter values when passed as NULL
    
    # Step 0a: Use urlType to infer key variables if passed as NULL
    for (vrbl in names(inferVars)) {
        mapper <- inferVars[[vrbl]]
        if (is.null(get(vrbl))) {
            if (urlType %in% names(mapper)) assign(vrbl, mapper[[urlType]])
            else if ("default" %in% names(mapper)) assign(vrbl, mapper[["default"]])
        }
    }
    
    # Step 1: Download a new file (if requested)
    if (!is.null(url) & isTRUE(getData)) fileDownload(fileName=fileName, url=url, ovrWrite=ovrWriteDownload)
    else cat("\nNo file has been downloaded, will use existing file:", fileName, "\n")
    
    # Step 2: Read file, rename and mutate variables, confirm uniqueness by expected levels
    dfRaw <- fileRead(fileName) %>% 
        colRenamer(vecRename) %>% 
        colMutater(selfList=selfList, fullList=fullList) %>%
        checkUniqueRows(uniqueBy=uniqueBy)
    
    # Step 3: Plot basic control totals for new cases and new deaths by month
    dfRaw %>%
        checkControl(groupBy=step3Group, useVars=step3Vals, printControls=FALSE, na.rm=TRUE) %>%
        helperLinePlot(x=step3Group, y="newValue", facetVar="name", facetScales="free_y", groupColor="name")
    
    # If there is no file for comparison, return the data
    if (is.null(dfRef) | if(length(dfRef)==1) is.na(dfRef) else FALSE) return(dfRaw)
    
    # Step 4b: Check similarity of existing and reference file
    # ovrWriteLog=FALSE since everything should be an append after the opening text line in step 0
    diffRaw <- checkSimilarity(df=dfRaw, 
                               ref=dfRef, 
                               keyVars=step4KeyVars, 
                               writeLog=writeLog, 
                               ovrwriteLog=FALSE
                               )
    
    # Step 5: Plot the similarity checks
    plotSimilarity(diffRaw, plotItems=step5PlotItems)
    
    # Step 6: Plot and report on differences in aggregates
    helperAggMap <- function(x) {
        h1 <- compareAggregate(df=dfRaw, ref=dfRef, grpVar=x$grpVar, numVars=x$numVars, 
                               sameUniverse=x$sameUniverse, plotData=x$plotData, isLine=x$isLine, 
                               returnDelta=x$returnDelta)
        if (isTRUE(x$flagLargeDelta)) {
            h2 <- flagLargeDelta(h1, pctTol=x$pctTol, absTol=x$absTol, sortBy=x$sortBy, 
                                 dropNA=x$dropNA, printAll=x$printAll
                                 )
            if (is.null(writeLog)) print(h2)
            else {
                cat(nrow(h2), " records", sep="")
                txt <- paste0("\n\n***Differences of at least ", 
                              x$absTol, 
                              " and at least ", 
                              round(100*x$pctTol, 3), "%\n\n"
                              )
                printLog(h2, txt=txt, writeLog=writeLog)
            }
        }
    }
    lapply(step6AggregateList, FUN=helperAggMap)
    
    cat("\n\n")
    
    # Return the raw data file
    dfRaw
    
}

# Explore for only reading and returning new data
testFile <- "./RInputFiles/Coronavirus/Weekly_United_States_COVID-19_Cases_and_Deaths_by_State.csv"
dfTest <- readQCRawCDCDaily(fileName=testFile, 
                            getData=FALSE, 
                            urlType="cdcWeekly",
                            url=c(),
                            vecRename=c("end_date"="date"),
                            selfList=list(),
                            fullList=list(),
                            uniqueBy=c("state", "date"), 
                            step3Group=c("date"), 
                            step3Vals=c("new_cases", "new_deaths"),
                            step4KeyVars=list("date"=list("label"="date", "countOnly"=TRUE, "convChar"=TRUE), 
                                              "state"=list("label"="state", "countOnly"=FALSE)
                                              ), # will need to update checkSimilarityMapper
                            step5PlotItems=c("date"),
                            step6AggregateList=list() # will need to update keyAggMapper
                            )
all.equal(dfTest, rename(tmpBurden, date=end_date))

```
  
The function for processing downloaded data, without comparison to previous data, works as intended. Next steps are to update the appropriate mapping files to include "cdcWeeklyBurden" as an option, then check that the function runs as intended. Elements are added to the mapping list manually at first:  
```{r, fig.height=9, fig.width=9}

# Need to combine NYC as part of NY
lstComboMapper$cdcWeeklyBurden <- list("nyc"=list("comboVar"="state", 
                                                  "uqVars"="date", 
                                                  "vecCombo"=c("NY"="NY", "NYC"="NY"),
                                                  "fn"=specNA(sum)
                                                  )
                                       )

# Need to test URL mapping later
if("cdcWeeklyBurden" %in% names(urlMapper)) {
    urlMapper["cdcWeeklyBurden"] <- "https://data.cdc.gov/api/views/pwn4-m3yp/rows.csv?accessType=DOWNLOAD"
} else {
    origNames <- names(urlMapper)
    urlMapper <- c(urlMapper, "https://data.cdc.gov/api/views/pwn4-m3yp/rows.csv?accessType=DOWNLOAD")
    names(urlMapper) <- c(origNames, "cdcWeeklyBurden")
}
    

# Rename end_date to date
renMapper$cdcWeeklyBurden <- c('end_date'='date')

# No need for variable mapping (formats OK as-is)
selfListMapper$cdcWeeklyBurden <- list()
fullListMapper$cdcWeeklyBurden <- list()

# File should be unique by state-date
uqMapper$cdcWeeklyBurden <- c("state", "date")

# Keep only 50 states (after NYC mapping) and DC
lstFilterMapper$cdcWeeklyBurden <- list("state"=c(state.abb, "DC"))

# Keep date, state, tot_cases, new_cases, tot_deaths, new_deaths
vecSelectMapper$cdcWeeklyBurden <- c("date", "state", "tot_cases", "tot_deaths", "new_cases", "new_deaths")

# Checks for control groups
checkControlGroupMapper$cdcWeeklyBurden <- c("date")
checkControlVarsMapper$cdcWeeklyBurden <- c("new_cases", "new_deaths")

# Check for similarity mapping
checkSimilarityMapper$cdcWeeklyBurden <- list(date=list(label='date', countOnly=TRUE, convChar=TRUE), 
                                              state=list(label='state', countOnly=FALSE)
                                              )
plotSimilarityMapper$cdcWeeklyBurden <- c("date")

# Update keyAggMapper (use cdcDaily for now, probably need to update later)
keyAggMapper$cdcWeeklyBurden <- keyAggMapper$cdcDaily

# No variables need to be kept as-is (avoiding rolling 7-day)
asIsMapper$cdcWeeklyBurden <- c()

# No changes needed to perCapMapper or hhsMapper

# Run sample code
dfTest_v2 <- readQCRawCDCDaily(fileName=testFile, 
                               getData=FALSE, 
                               urlType="cdcWeeklyBurden",
                               url=c()
                               )
all.equal(dfTest, dfTest_v2)

```
  
Next steps are to enable URL downloads and enable checking against a reference file. Downloading of new data is attempted:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run sample code
dfTest_v3 <- readQCRawCDCDaily(fileName="./RInputFiles/Coronavirus/CDC_dc_downloaded_221118.csv", 
                               getData=TRUE, 
                               urlType="cdcWeeklyBurden"
                               )
dfTest_v3

```
  
Checking against a reference file is also enabled:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

dfTest_v3_ref <- readQCRawCDCDaily(fileName="./RInputFiles/Coronavirus/CDC_dc_downloaded_221118.csv", 
                                   getData=FALSE,
                                   dfRef=dfTest_v2,
                                   urlType="cdcWeeklyBurden"
                                   )
dfTest_v3_ref

```
  
New columns have been added to explain jumps in total cases/deaths that are not explained by that week's reported new cases/deaths. These data are explored:  
```{r, fig.height=9, fig.width=9}

# Full file
testFull <- read_csv("./RInputFiles/Coronavirus/CDC_dc_downloaded_221118.csv")

# Entries with non-zero for "new_historic_*"
testFull %>%
    filter(new_historic_cases != 0 | new_historic_deaths != 0)

# Cases where cumsum and total are not equal
testFull %>%
    arrange(state, end_date) %>% 
    group_by(state) %>% 
    mutate(delta=tot_deaths-cumsum(new_deaths)) %>% 
    filter(delta != 0)

```
  
The field appears to be working as intended. Next steps are to compare the cases and deaths data to the previous daily data files:  
```{r, fig.height=9, fig.width=9}

# Conversion of raw CDC daily data to cumsum
cdcDailyTest <- cdc_daily_221102$dfRaw$cdcDaily %>%
    select(date, state, new_cases, new_deaths) %>%
    arrange(state, date) %>%
    group_by(state) %>%
    mutate(cum_cases=cumsum(new_cases), cum_deaths=cumsum(new_deaths)) %>%
    ungroup()

# Integration of new weekly data, using end_date=date
testCombo <- testFull %>%
    select(date=end_date, state, wkly_tot_cases=tot_cases, wkly_tot_deaths=tot_deaths) %>%
    left_join(cdcDailyTest, by=c("date", "state"))
testCombo

# Calculation of RMSE by state for 'tot_deaths'
testCombo %>%
    filter(date <= "2022-10-12") %>%
    mutate(delta=wkly_tot_deaths-cum_deaths) %>%
    group_by(state) %>%
    summarize(rmse=sqrt(mean(delta**2)), r2=1-rmse/mean(cum_deaths)) %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    ggplot(aes(x=fct_reorder(state, r2), y=r2)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(label=round(r2, 2)), hjust=0) +
    coord_flip() + 
    labs(x=NULL, y="Similarity of reported deaths (weekly vs. daily)\n(1.0 means perfect alignment)")

# Select states plotted
testCombo %>%
    filter(state %in% c("OR", "LA", "DC", "PA", "NE", "WA", "OH", "MO", "AK")) %>%
    select(date, state, wkly_tot_deaths, cum_deaths) %>%
    pivot_longer(-c(state, date)) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("wkly_tot_deaths"="Weekly data (new)", "cum_deaths"="Daily data (old)")[name]
                  )
              ) + 
    facet_wrap(~state, scales="free_y") + 
    scale_color_discrete("Source:") + 
    labs(title="Comparison of cumulative deaths by source", y="Cumulative deaths", x=NULL)

# Overall data
testCombo %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(date) %>%
    summarize(across(where(is.numeric), specNA())) %>%
    select(date, wkly_tot_deaths, cum_deaths) %>%
    pivot_longer(-c(date)) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("wkly_tot_deaths"="Weekly data (new)", "cum_deaths"="Daily data (old)")[name]
                  )
              ) + 
    scale_color_discrete("Source:") + 
    labs(title="Comparison of cumulative deaths by source", 
         y="Cumulative deaths", 
         x=NULL, 
         subtitle="50 states plus DC"
         )

```
  
Alignment of reported deaths varies by state. In aggregate, the newer data appears to report slightly more deaths. The data file is processed using the existing function:  
```{r, fig.height=9, fig.width=9}

dfTestProcess <- processRawFile(dfTest_v3_ref, 
                                vecRename=c(), 
                                vecSelect=vecSelectMapper[["cdcWeeklyBurden"]], 
                                lstCombo=lstComboMapper[["cdcWeeklyBurden"]],
                                lstFilter=lstFilterMapper[["cdcWeeklyBurden"]]
                                )
dfTestProcess

```
  
Data need to be converted appropriately to daily format for 7-day rolling averages. Totals should be carried forward, while new values should be left as-is. This step needs to be added either to processRawFile() or createPerCapita():
```{r, fig.height=9, fig.width=9}

# Conversion to daily data
dfTestProcessDaily <- expand.grid(seq.Date(dfTestProcess %>% pull(date) %>% min, 
                                           dfTestProcess %>% pull(date) %>% max, 
                                           by=1
                                           ), 
                                  dfTestProcess %>% pull(state) %>% unique
                                  ) %>% 
    tibble::as_tibble() %>% 
    set_names(c("date", "state")) %>% 
    left_join(dfTestProcess, by=c("date", "state")) %>% 
    mutate(across(starts_with("tot"), .fns=function(x) zoo::na.locf(x)), 
           across(starts_with("new"), .fns=function(x) ifelse(is.na(x), 0, x))
           )
dfTestProcessDaily

dfTestPerCapita <- createPerCapita(dfTestProcessDaily, 
                                   uqBy=c("state", "date"), 
                                   popData=getStateData(), 
                                   mapper=perCapMapper, 
                                   asIsVars=if(isTRUE(exists("asIsMapper"))) asIsMapper[["cdcWeeklyBurden"]] else c()
                                   )
dfTestPerCapita

```
  
A function is written for the conversion of weekly data to daily data:  
```{r, fig.height=9, fig.width=9}

convertWeeklyDaily <- function(df, 
                               timeVar="date", 
                               otherVars=c("state"),
                               timeUnitDays=1, 
                               locfVars=c("tot_cases", "tot_deaths"), 
                               naVars=c("new_cases", "new_deaths")
                               ) {

    # FUNCTION ARGUMENTS:
    # df: the original daily frame
    # timeVar: the time variable in the original dataset
    # otherVars: other variables of importance (df should be unique by otherVars-timeVar)
    # timeUnitDays: final data should be every 'timeUnitDays' days (e.g., 2 would be every other day)
    # locfVars: variables for applying zoo::na.locf
    # naVars: variables for applying ifelse(is.na(x), 0, x)
    
    # Check that all variables are included
    nms1 <- sort(names(df))
    nms2 <- sort(c(timeVar, otherVars, locfVars, naVars))
    if(!isTRUE(all.equal(nms1, nms2))) {
        warning(paste0("\nSome variables not passed to convertWeeklyDaily() will be treated as-is:", 
                       "\ndf has variables: ", 
                       paste0(nms1, collapse=", "), 
                       "\nvariables passed as arguments are: ", 
                       paste0(nms2, collapse=", "), 
                       "\n"
                       )
                )
    }
    
    # Relevant times to include
    keyTimes <- seq.Date(df %>% pull(timeVar) %>% min, df %>% pull(timeVar) %>% max, by=timeUnitDays) %>%
        tibble::tibble() %>%
        purrr::set_names(timeVar)
    
    # Relevant levels to include
    keyLevels <- df %>% select(all_of(otherVars)) %>% unique()
    
    # Return cross-join tibble
    tibble::tibble(keyTimes) %>%
        full_join(keyLevels, by=character()) %>%
        purrr::set_names(c(timeVar, otherVars)) %>% 
        left_join(df, by=c(timeVar, otherVars)) %>% 
        arrange(across(c(all_of(otherVars), all_of(timeVar)))) %>%
        group_by(across(all_of(otherVars))) %>%
        mutate(across(all_of(locfVars), .fns=function(x) zoo::na.locf(x)), 
               across(all_of(naVars), .fns=function(x) ifelse(is.na(x), 0, x))
               ) %>%
        ungroup()

}

all.equal(dfTestProcessDaily %>% arrange(state, date), convertWeeklyDaily(dfTestProcess), check.attributes=FALSE)

```
  
