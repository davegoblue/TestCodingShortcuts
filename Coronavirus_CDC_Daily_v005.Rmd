---
title: "CDC Daily by State"
author: "davegoblue"
date: "2022-09-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
This file is designed to use CDC data to assess coronavirus disease burden by state, including creating and analyzing state-level clusters.

Through March 7, 2021, [The COVID Tracking Project](https://covidtracking.com/) collected and integrated data on tests, cases, hospitalizations, deaths, and the like by state and date.  The latest code for using this data is available in Coronavirus_Statistics_CTP_v004.Rmd.

The COVID Tracking Project suggest that [US federal data sources](https://covidtracking.com/analysis-updates/federal-covid-data-101-how-to-find-data) are now sufficiently robust to be used for analyses that previously relied on COVID Tracking Project.  This code is an attempt to update modules in Coronavirus_Statistics_CTP_v004.Rmd to leverage US federal data.

The code in this module builds on code available in _v004, with function and mapping files updated:  
  
* Generic_Added_Utility_Functions_202105_v001.R - generic functions that can be used in other areas  
* Coronavirus_CDC_Daily_Functions_v001.R - functions specific to coronavirus daily data  
  
Broadly, the CDC data analyzed by this module includes:  
  
* CDC case and death data by state and date are available for download on the [CDC website](https://data.cdc.gov/api/views/9mfq-cb36/rows.csv?accessType=DOWNLOAD)  
* CDC hospital data are available for download on the [healthdata.gov website](https://beta.healthdata.gov/api/views/g62h-syeh/rows.csv?accessType=DOWNLOAD)  
* CDC vaccines data are also available for download on the [CDC website](https://data.cdc.gov/api/views/unsk-b7fc/rows.csv?accessType=DOWNLOAD):  
  
## Functions and Mapping Files
The tidyverse package is loaded and functions are sourced:  
```{r}

# The tidyverse functions are routinely used without package::function format
library(tidyverse)
library(geofacet)

# Functions are available in source file
source("./Generic_Added_Utility_Functions_202105_v001.R")
source("./Coronavirus_CDC_Daily_Functions_v002.R")

```

A series of mapping files are also available to allow for parameterized processing.  Mappings include:  
  
* urlMapper - mapping file for urlType and url location to download data  
* renMapper - mapping file for renaming of variables in the raw data file  
* selfListMapper - mapping file for transformations by variable type  
* fullListMapper - mapping file for transformations across variable types  
* lstComboMapper - mapping file for elements to be combined by data type (most common is to combine NYC and NYS data to NY if the file provides them separately)  
* uqMapper - mapping file for fields that should combine to be unique keys for processed files  
* lstFilterMapper - mapping file for filtering to subset (most common is to keep 50 states and DC)  
* vecSelectMapper - mapping file for variables to keep  
* checkControlGroupMapper - mapping file for group_by() of control total checks  
* checkControlAggMapper - mapping file for numeric variables for control total checks  
* checkSimilarityMapper - mapping file for similarity checks to perform  
* plotSimilarityMaooer - mapping file for fields where differences in universe should be plotted
* keyAggMapper - mapping file for the aggregate-level control total checks to perform  
* perCapMapper - named vector that drives conversion from original field name to per capita field name  
* hhsMapper - named vector that drivers numerical variables to keep (and renaming) from HHS capacity data file
  
These default parameters are maintained in a separate .R file and can be sourced:  
```{r}

source("./Coronavirus_CDC_Daily_Default_Mappings_v002.R")

```

## Example Code Processing
The function is run to download and process the latest CDC case, hospitalization, and death data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcDaily"="./RInputFiles/Coronavirus/CDC_dc_downloaded_220907.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_220907.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_220907.csv"
                 )
compareList <- list("cdcDaily"=readFromRDS("cdc_daily_220805")$dfRaw$cdcDaily, 
                    "cdcHosp"=readFromRDS("cdc_daily_220805")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_220805")$dfRaw$vax
                    )

cdc_daily_220907 <- readRunCDCDaily(thruLabel="Sep 05, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_220907, ovrWriteError=FALSE)

```
  
The function is run to download and process the latest hospitalization data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run for latest data, save as RDS
indivHosp_20220907 <- downloadReadHospitalData(loc="./RInputFiles/Coronavirus/HHS_Hospital_20220907.csv")
saveToRDS(indivHosp_20220907, ovrWriteError=FALSE)

```
  
Post-processing is run, including hospital summaries:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create pivoted burden data
burdenPivotList_220907 <- postProcessCDCDaily(cdc_daily_220907, 
                                              dataThruLabel="Aug 2022", 
                                              keyDatesBurden=c("2022-08-31", "2022-02-28", 
                                                               "2021-08-31", "2021-02-28"
                                                               ),
                                              keyDatesVaccine=c("2022-08-31", "2022-03-31", 
                                                                "2021-10-31", "2021-05-31"
                                                                ), 
                                              returnData=TRUE
                                              )

# Create hospitalized per capita data
hospPerCap_220907 <- hospAgePerCapita(readFromRDS("dfStateAgeBucket2019"), 
                                      lst=burdenPivotList_220907, 
                                      popVar="pop2019", 
                                      excludeState=c(), 
                                      cumStartDate="2020-07-15"
                                      )

burdenPivotList_220907$hospAge %>%
    group_by(adultPed, confSusp, age, name) %>%
    summarize(value=sum(value, na.rm=TRUE), n=n(), .groups="drop")

saveToRDS(burdenPivotList_220907, ovrWriteError=FALSE)
saveToRDS(hospPerCap_220907, ovrWriteError=FALSE)

```
  
Peaks and valleys of key metrics are also updated:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

peakValleyCDCDaily(cdc_daily_220907)

```
  
Hospital capacity is updated using a mix of old data (for 2021) and new data:  
```{r, fig.height=9, fig.width=9}

identical(names(indivHosp_20220907), names(readFromRDS("indivHosp_20220704")))

modHospData <- bind_rows(filter(readFromRDS("indivHosp_20220704"), lubridate::year(collection_week)<2022), 
                         filter(indivHosp_20220907, lubridate::year(collection_week)>=2022), 
                         .id="src"
                         )
updated_modStateHosp_20220907 <- hospitalCapacityCDCDaily(modHospData, 
                                                          plotSub="Aug 2020 to Aug 2022\nOld data used pre-2022"
                                                          )

```
  
Data availability by source and time is assessed:  
```{r, fig.height=9, fig.width=9}

# Temporary function to aggregate data
tempCounter <- function(df) {
    df %>%
        select(hospital_pk, collection_week, all_of(names(hhsMapper))) %>%
        colRenamer(vecRename=hhsMapper) %>%
        pivot_longer(-c(hospital_pk, collection_week)) %>%
        filter(!is.na(value), value>0) %>%
        count(collection_week, name)
}

dfTemp <- bind_rows(tempCounter(indivHosp_20220907), tempCounter(readFromRDS("indivHosp_20220704")), .id="src")

dfTemp %>%
    select(collection_week, name) %>%
    unique() %>%
    bind_rows(., ., .id="src")  %>%
    full_join(dfTemp, by=c("src", "collection_week", "name")) %>%
    mutate(src=c("1"="SEP-2022", "2"="JUL-2022")[src]) %>%
    mutate(n=ifelse(is.na(n), 0, n)) %>%
    ggplot(aes(x=collection_week, y=n)) +
    geom_line(aes(group=src, color=src)) + 
    facet_wrap(~name) + 
    labs(title="Number of hospitals in US reporting >0 on metric by week", x=NULL, y="# Hospitals Reporting > 0") + 
    scale_color_discrete("Data Source:")

dfTemp %>%
    select(collection_week, name) %>%
    unique() %>%
    bind_rows(., ., .id="src")  %>%
    full_join(dfTemp, by=c("src", "collection_week", "name")) %>%
    mutate(src=c("1"="SEP-2022", "2"="JUL-2022")[src]) %>%
    mutate(n=ifelse(is.na(n), 0, n)) %>% 
    group_by(collection_week, name) %>% 
    summarize(delta=sum(ifelse(src=="SEP-2022", n, 0)-ifelse(src!="SEP-2022", n, 0)), .groups="drop") %>%
    ggplot(aes(x=collection_week, y=delta)) +
    geom_line(aes(color=case_when(delta>=0 ~ "darkgreen", TRUE ~ "red"))) +
    geom_hline(yintercept=0, lty=2) +
    geom_vline(xintercept=c(as.Date("2021-08-20"), as.Date("2022-06-24")), lty=2) +
    scale_color_identity(NULL) +
    facet_wrap(~name) + 
    labs(title="Delta in Number of hospitals in US reporting >0 on metric by week", 
         subtitle="Trend break dashed lines at 2021-08-20 and 2022-06-24",
         x=NULL, 
         y="Delta in # Hospitals Reporting > 0"
         )
    
```
  
The process is converted to functional form:  
```{r, fig.height=9, fig.width=9}

multiSourceDataCombine <- function(lst, timeVec, keyVar="collection_week", idName="src") {
    
    # FUNCTION ARGUMENTS:
    # lst: list of data frames to be combined
    # timeVec: vector of time cut points (data before timeVec[1] taken from lst[[1]], etc.)
    # keyVar: variable describing time in the data
    # idName: name of column for .id when files combined
    
    # Check list lengths
    if(length(lst)==0) {
        cat("\nEmpty list passed, returning 0x0 tibble\n")
        return(tibble::tibble())
    } else if (length(lst)==1) {
        cat("\nList of length 1 passed, returning item in list as-is")
        return(lst[[1]])
    }
    
    # Check that timeVec matches
    if(length(lst) != length(timeVec) + 1) stop("\nMismatch of lst and timeVec\n")
    
    # Check that all data frames have the same column names in the same order
    vecNames <- names(lst[[1]])
    for(n in 2:length(lst)) if(!isTRUE(identical(names(lst[[n]]), vecNames))) stop("\nName mismatch in files\n")
    
    # Combine data
    bind_rows(lst, .id=idName) %>%
        mutate(srcNum=as.integer(get(idName)), 
               dateMin=ifelse(srcNum==1, NA, timeVec[srcNum-1]), 
               dateMax=ifelse(srcNum==max(srcNum), NA, timeVec[srcNum])
               ) %>%
        filter(is.na(dateMin) | get(keyVar) >= dateMin, 
               is.na(dateMax) | get(keyVar) < dateMax
               ) %>%
        select(-srcNum, -dateMin, -dateMax)
    
}

# Create modified hospital data
multiSourceHosp_20220902 <- multiSourceDataCombine(list(readFromRDS("indivHosp_20220704"), 
                                                        indivHosp_20220907
                                                        ), 
                                                   timeVec=as.Date("2022-01-01")
                                                   )

# Confirm that function produces expected output
multiSourceHosp_20220902 %>%
    select(-src) %>%
    identical(modHospData %>% select(-src))

```
  
The updated hospital data are then plotted:
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run hospital plots
modStateHosp_20220902 <- hospitalCapacityCDCDaily(multiSourceHosp_20220902, 
                                                  plotSub="Aug 2020 to Aug 2022\nOld data used pre-2022"
                                                  )

```
  
## Data Refreshes
### _Data From 2022-10-02_
The latest CDC case, hospitalization, and death data are downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcDaily"="./RInputFiles/Coronavirus/CDC_dc_downloaded_221002.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_221002.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_221002.csv"
                 )
compareList <- list("cdcDaily"=readFromRDS("cdc_daily_220907")$dfRaw$cdcDaily, 
                    "cdcHosp"=readFromRDS("cdc_daily_220907")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_220907")$dfRaw$vax
                    )

cdc_daily_221002 <- readRunCDCDaily(thruLabel="Sep 30, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_221002, ovrWriteError=FALSE)

```
  
The function is run to download and process the latest hospitalization data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run for latest data, save as RDS
indivHosp_20221003 <- downloadReadHospitalData(loc="./RInputFiles/Coronavirus/HHS_Hospital_20221003.csv")
saveToRDS(indivHosp_20221003, ovrWriteError=FALSE)

```
  
Post-processing is run, including hospital summaries:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create pivoted burden data
burdenPivotList_221002 <- postProcessCDCDaily(cdc_daily_221002, 
                                              dataThruLabel="Sep 2022", 
                                              keyDatesBurden=c("2022-09-30", "2022-03-31", 
                                                               "2021-09-30", "2021-03-31"
                                                               ),
                                              keyDatesVaccine=c("2022-09-28", "2022-03-31", 
                                                                "2021-09-30", "2021-03-31"
                                                                ), 
                                              returnData=TRUE
                                              )

# Create hospitalized per capita data
hospPerCap_221002 <- hospAgePerCapita(readFromRDS("dfStateAgeBucket2019"), 
                                      lst=burdenPivotList_221002, 
                                      popVar="pop2019", 
                                      excludeState=c(), 
                                      cumStartDate="2020-07-15"
                                      )

burdenPivotList_221002$hospAge %>%
    group_by(adultPed, confSusp, age, name) %>%
    summarize(value=sum(value, na.rm=TRUE), n=n(), .groups="drop")

saveToRDS(burdenPivotList_221002, ovrWriteError=FALSE)
saveToRDS(hospPerCap_221002, ovrWriteError=FALSE)

```
  
Peaks and valleys of key metrics are also updated:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

peakValleyCDCDaily(cdc_daily_221002)

```
  
A function is written for the hospital data availability checks:  
```{r, fig.height=9, fig.width=9}

checkHospitalDataComplete <- function(df1=NULL, 
                                      df2=NULL,
                                      dfAll=NULL,
                                      lab1="DF1", 
                                      lab2="DF2", 
                                      trendBreaks=c(), 
                                      makeP1=TRUE, 
                                      makeP2=TRUE,
                                      returnData=FALSE
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # df1: the first data frame (NULL means integrated frame passed as dfAll)
    # df2: the second data frame (NULL means integrated frame passed as dfAll)
    # dfAll: integrated data frame from previous iteration of function (will ignore df1 and df2)
    # lab1: plot label for the first data frame
    # lab2: plot label for the second data frame
    # trendBreaks: character vector of trend break dates of form YYYY-MM-DD - if c(), no trend brek vlines plotted
    # makeP1: boolean, should the first plot be created and printed?
    # makeP2: boolean, should the first plot be created and printed?
    # returnData: boolean, should dfAll be returned?
    
    # Temporary function to aggregate data
    tempCountComplete <- function(df) {
        df %>%
            select(hospital_pk, collection_week, all_of(names(hhsMapper))) %>%
            colRenamer(vecRename=hhsMapper) %>%
            pivot_longer(-c(hospital_pk, collection_week)) %>%
            filter(!is.na(value), value>0) %>%
            count(collection_week, name)
    }
    
    # Create or use passed data
    if(is.null(dfAll)) {
        if(is.null(df1) | is.null(df2)) stop("dfAll not passed requires both df1 and df2 to be passed\n")
        dfAll <- bind_rows(tempCountComplete(df1), tempCountComplete(df2), .id="src")
    } else {
        if(!is.null(df1) | !is.null(df2)) warning("dfAll passed and will be used; df1 and/or df2 ignored\n")
    }
    
    # Plot data completeness - hospitals reporting >0 on metric by week
    if(isTRUE(makeP1)) {
        p1 <- dfAll %>%
            select(collection_week, name) %>%
            unique() %>%
            bind_rows(., ., .id="src")  %>%
            full_join(dfAll, by=c("src", "collection_week", "name")) %>%
            mutate(src=c("1"=lab1, "2"=lab2)[src]) %>%
            mutate(n=ifelse(is.na(n), 0, n)) %>%
            ggplot(aes(x=collection_week, y=n)) +
            geom_line(aes(group=src, color=src)) + 
            facet_wrap(~name) + 
            labs(title="Number of hospitals in US reporting >0 on metric by week", 
                 x=NULL, 
                 y="# Hospitals Reporting > 0"
                 ) + 
            scale_color_discrete("Data Source:")
        if(length(trendBreaks) > 0) {
            p1 <- p1 + geom_vline(xintercept=as.Date(all_of(trendBreaks)), lty=2) +
                labs(subtitle=paste0("Trend break dashed lines at ", 
                                     paste0(all_of(trendBreaks), collapse=" and ")
                                     )
                     )
        }
        print(p1)
    }
    

    # Plot data completeness - showing difference in reported data and trend break dates
    if(isTRUE(makeP2)) {
        p2 <- dfAll %>%
            select(collection_week, name) %>%
            unique() %>%
            bind_rows(., ., .id="src")  %>%
            full_join(dfAll, by=c("src", "collection_week", "name")) %>%
            mutate(src=c("1"=lab1, "2"=lab2)[src]) %>%
            mutate(n=ifelse(is.na(n), 0, n)) %>% 
            group_by(collection_week, name) %>% 
            summarize(delta=sum(ifelse(src==lab1, n, 0)-ifelse(src!=lab1, n, 0)), .groups="drop") %>%
            ggplot(aes(x=collection_week, y=delta)) +
            geom_line() +
            geom_point(data=~filter(., delta != 0), 
                       aes(color=case_when(delta>=0 ~ "darkgreen", TRUE ~ "red"))
                       ) +
            geom_hline(yintercept=0, lty=2) +
            scale_color_identity(NULL) +
            facet_wrap(~name) + 
            labs(title="Delta in Number of hospitals in US reporting >0 on metric by week", 
                 x=NULL, 
                 y="Delta in # Hospitals Reporting > 0"
                 )
        if(length(trendBreaks) > 0) {
            p2 <- p2 + geom_vline(xintercept=as.Date(all_of(trendBreaks)), lty=2) +
                labs(subtitle=paste0("Trend break dashed lines at ", 
                                     paste0(all_of(trendBreaks), collapse=" and ")
                                     )
                )
        }
        print(p2)
    }
    
    if(isTRUE(returnData)) return(dfAll)
    
}

# Create the data
dfTemp <- checkHospitalDataComplete(df1=readFromRDS("indivHosp_20221003"), 
                                    df2=readFromRDS("indivHosp_20220704"), 
                                    makeP1=FALSE, 
                                    makeP2=FALSE,
                                    returnData=TRUE
                                    )

# Create the first plot
checkHospitalDataComplete(dfAll=dfTemp,
                          lab1="OCT-2022", 
                          lab2="JUL-2022", 
                          makeP2=FALSE, 
                          trendBreaks=c("2021-09-25", "2022-06-24")
                          )

# Create the second plot
checkHospitalDataComplete(dfAll=dfTemp,
                          lab1="OCT-2022", 
                          lab2="JUL-2022", 
                          makeP1=FALSE, 
                          trendBreaks=c("2021-09-25", "2022-06-24")
                          )

```
  
The discontinuity issues due to occasional minor changes between positive and negative is resolved by using points colored by positive/negative and a solid line

Hospital data are pieced together as needed:
```{r, fig.height=9, fig.width=9}

# Create modified hospital data
multiSourceHosp_20221002 <- multiSourceDataCombine(list(readFromRDS("indivHosp_20220704"),
                                                        readFromRDS("indivHosp_20221003")
                                                        ),
                                                   timeVec=as.Date("2022-01-01")
                                                   )

```
  
The updated hospital data are then plotted:
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run hospital plots
modStateHosp_20221002 <- hospitalCapacityCDCDaily(multiSourceHosp_20221002, 
                                                  plotSub="Aug 2020 to Sep 2022\nOld data used pre-2022"
                                                  )

```
  
### _Data From 2022-11-02_
The latest CDC case, hospitalization, and death data are downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcDaily"="./RInputFiles/Coronavirus/CDC_dc_downloaded_221102.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_221102.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_221102.csv"
                 )
compareList <- list("cdcDaily"=readFromRDS("cdc_daily_221002")$dfRaw$cdcDaily, 
                    "cdcHosp"=readFromRDS("cdc_daily_221002")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_221002")$dfRaw$vax
                    )

cdc_daily_221102 <- readRunCDCDaily(thruLabel="Oct 31, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_221102, ovrWriteError=FALSE)

```
  
The CDC modified deaths and cases reporting to be weekly, based on summation of county-level data to states. Per their website, methodology has changed (inconsistency between datasets?) and the previous daily data is no longer updated as of October 20, 2022. Weekly deaths and cases data are available at [CDC website](https://data.cdc.gov/Case-Surveillance/Weekly-United-States-COVID-19-Cases-and-Deaths-by-/pwn4-m3yp)
  
The latest hospitalization data is also downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run for latest data, save as RDS
indivHosp_20221103 <- downloadReadHospitalData(loc="./RInputFiles/Coronavirus/HHS_Hospital_20221103.csv")
saveToRDS(indivHosp_20221103, ovrWriteError=FALSE)

```
  
Post-processing is run, including hospital summaries:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create pivoted burden data
burdenPivotList_221102 <- postProcessCDCDaily(cdc_daily_221102, 
                                              dataThruLabel="Oct 2022", 
                                              keyDatesBurden=c("2022-10-15", "2022-04-15", 
                                                               "2021-10-15", "2021-04-15"
                                                               ),
                                              keyDatesVaccine=c("2022-10-26", "2022-04-30", 
                                                                "2021-10-31", "2021-04-30"
                                                                ), 
                                              returnData=TRUE
                                              )

# Create hospitalized per capita data
hospPerCap_221102 <- hospAgePerCapita(readFromRDS("dfStateAgeBucket2019"), 
                                      lst=burdenPivotList_221102, 
                                      popVar="pop2019", 
                                      excludeState=c(), 
                                      cumStartDate="2020-07-15"
                                      )

burdenPivotList_221102$hospAge %>%
    group_by(adultPed, confSusp, age, name) %>%
    summarize(value=sum(value, na.rm=TRUE), n=n(), .groups="drop")

saveToRDS(burdenPivotList_221102, ovrWriteError=FALSE)
saveToRDS(hospPerCap_221102, ovrWriteError=FALSE)

```
  
Peaks and valleys of key metrics are also updated:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

peakValleyCDCDaily(cdc_daily_221102)

```
  
Hospital data are pieced together as needed:
```{r, fig.height=9, fig.width=9}

# Create modified hospital data
multiSourceHosp_20221102 <- multiSourceDataCombine(list(readFromRDS("indivHosp_20220704"),
                                                        readFromRDS("indivHosp_20221103")
                                                        ),
                                                   timeVec=as.Date("2022-01-01")
                                                   )

```
  
The updated hospital data are then plotted:
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run hospital plots
modStateHosp_20221102 <- hospitalCapacityCDCDaily(multiSourceHosp_20221102, 
                                                  plotSub="Aug 2020 to Oct 2022\nOld data used pre-2022"
                                                  )

```
  
An example of the new data file is downloaded manually and then read:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

tmpBurden <- fileRead("./RInputFiles/Coronavirus/Weekly_United_States_COVID-19_Cases_and_Deaths_by_State.csv")
glimpse(tmpBurden)

# Check for states
tmpStateBurden <- tmpBurden %>%
    arrange(end_date, state) %>%
    group_by(state) %>%
    summarize(across(c(new_cases, new_deaths), .fns=sum, na.rm=TRUE), 
              across(c(tot_cases, tot_deaths), .fns=max, na.rm=TRUE)
              ) %>%
    ungroup()

# Are all 50 states and DC included?
tmpStateBurden %>%
    filter(state %in% c(state.abb, "DC")) %>%
    arrange(desc(tot_deaths)) %>%
    print(n=+Inf)

# What other states are included?
tmpStateBurden %>%
    filter(!(state %in% c(state.abb, "DC"))) %>%
    arrange(desc(tot_deaths)) %>%
    print(n=+Inf)

# Are there disconnects between total and sum of new?
tmpStateBurden %>%
    filter((new_cases != tot_cases) | (new_deaths != tot_deaths)) %>%
    mutate(ratCase=tot_cases/new_cases, ratDeath=tot_deaths/new_deaths)

```
  
NYC data appear to be tracked separately from NY data, requiring combination. Otherwise, the expected geographical units appear to be included, and with totals and sum of new matching (exceptions for deaths in DC, NYC, and Kansas).

Similarity of total burden is compared:  
```{r, fig.height=9, fig.width=9}

tmpBurdenDate <- tmpBurden %>%
    select(date=end_date, state, where(is.numeric)) %>%
    bind_rows(cdc_daily_221102$dfProcess$cdcDaily, .id="src") %>%
    mutate(src=c("1"="CDC weekly (new)", "2"="CDC daily (old)")[src])
tmpBurdenDate

# Plot for total deaths and total cases
tmpBurdenDate %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(src, date) %>%
    summarize(across(where(is.numeric), sum, na.rm=TRUE), .groups="drop") %>%
    pivot_longer(where(is.numeric)) %>%
    filter(name %in% c("tot_cases", "tot_deaths")) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(color=src, group=src)) + 
    facet_wrap(~name, scales="free_y")

# Plot by state - latest burden
tmpBurdenDate %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(src, state) %>%
    summarize(across(where(is.numeric), last, order_by=date), .groups="drop") %>%
    pivot_longer(where(is.numeric)) %>%
    filter(name %in% c("tot_cases", "tot_deaths")) %>%
    ggplot(aes(x=fct_reorder(state, value), y=value)) + 
    geom_point(aes(color=src)) + 
    facet_wrap(~name, scales="free_x") + 
    coord_flip() + 
    labs(x=NULL, y=NULL, title="Most recent burden by state and data source") + 
    scale_color_discrete("Source")

# Plot by state - difference in burden
tmpBurdenDate %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(src, state) %>%
    summarize(across(where(is.numeric), last, order_by=date), .groups="drop") %>%
    pivot_longer(where(is.numeric)) %>%
    filter(name %in% c("tot_cases", "tot_deaths")) %>%
    group_by(state, name) %>%
    summarize(value=sum(ifelse(src!="CDC daily (old)", value, 0)) - sum(ifelse(src=="CDC daily (old)", value, 0)), 
              .groups="drop"
              ) %>%
    ggplot(aes(x=fct_reorder(state, value), y=value)) + 
    geom_point() + 
    facet_wrap(~name, scales="free_x") + 
    coord_flip() + 
    labs(x=NULL, y=NULL, title="Change in most recent burden by state (new minus old)") + 
    geom_hline(yintercept=0, lty=2)

```
  
At a first glance, national totals are well aligned between the existing daily data file and the new weekly data file. The newer data has two weeks of extra reporting, so most recent totals by state are generally slightly higher. The new data breaks apart NYC and NY, which need to be combined for processing the new file

Function readQCRawCDCDaily() is updated:  
```{r, fig.height=9, fig.width=9}

# Function to read and check a raw data file (last updated 16-NOV-2022, previously updated 02-AUG-2021)
readQCRawCDCDaily <- function(fileName, 
                              writeLog=NULL,
                              ovrwriteLog=TRUE,
                              dfRef=NULL,
                              urlType=NULL,
                              url=NULL, 
                              getData=TRUE,
                              ovrWriteDownload=FALSE, 
                              vecRename=NULL, 
                              selfList=NULL,
                              fullList=NULL,
                              uniqueBy=NULL, 
                              step3Group=NULL,
                              step3Vals=NULL, 
                              step4KeyVars=NULL, 
                              step5PlotItems=NULL,
                              step6AggregateList=NULL,
                              inferVars=list("url"=urlMapper, 
                                             "vecRename"=renMapper, 
                                             "selfList"=selfListMapper, 
                                             "fullList"=fullListMapper, 
                                             "uniqueBy"=uqMapper, 
                                             "step3Group"=checkControlGroupMapper,
                                             "step3Vals"=checkControlVarsMapper, 
                                             "step4KeyVars"=checkSimilarityMapper, 
                                             "step5PlotItems"=plotSimilarityMapper,
                                             "step6AggregateList"=keyAggMapper
                                             )
                              ) {
    
    # FUNCTION ARGUMENTS
    # fileName: the location where downloaded data either is, or will be, stored
    # writeLog: the external file location for printing (NULL means use the main log stdout)
    # ovrwriteLog: boolean, if using an external log, should it be started from scratch (overwritten)?
    # dfRef: a reference data frame for comparison (either NULL or NA means do not run comparisons)
    # urlType: character vector that can be mapped using urlMapper and keyVarMapper
    # url: direct URL passed as character string
    #      NOTE that if both url and urlType are NULL, no file will be downloaded
    # getData: boolean, should an attempt be made to get new data using urlType or url?
    # ovrWriteDownload: boolean, if fileName already exists, should it be overwritten?
    # vecRename: vector for renaming c('existing name'='new name'), can be any length from 0 to ncol(df)
    #            NULL means infer from urlType, if not available there use c()
    # selfList: list for functions to apply to self, list('variable'=fn) will apply variable=fn(variable)
    #           processed in order, so more than one function can be applied to self
    #           NULL means infer from urlType, if not available in mapping file use list()
    # fullList: list for general functions to be applied, list('new variable'=expression(code))
    #           will create 'new variable' as eval(expression(code))
    #           for now, requires passing an expression
    #           NULL means infer from urlType, use list() if not in mapping file
    # uniqueBy: combination of variables for checking uniqueness
    #           NULL means infer from data, keep as NULL (meaning use-all) if cannot be inferred
    # step3Group: variable to be used as the x-axis (grouping) for step 3 plots
    #             NULL means infer from data
    # step3Vals: values to be plotted on the y-axis for step 3 plots
    #            NULL means infer from data
    # step4KeyVars: list of parameters to be passed as keyVars= in step 4
    #               NULL means infer from urlType
    # step5PlotItems: items to be plotted in step 5
    #                 NULL means infer from urlType
    # step6AggregateList: drives the elements to be passed to compareAggregate() and flagLargeDelta()
    #                     NULL means infer from urlType
    # inferVars: vector of c('variable'='mapper') for inferring parameter values when passed as NULL
    
    # Step 0a: Use urlType to infer key variables if passed as NULL
    for (vrbl in names(inferVars)) {
        mapper <- inferVars[[vrbl]]
        if (is.null(get(vrbl))) {
            if (urlType %in% names(mapper)) assign(vrbl, mapper[[urlType]])
            else if ("default" %in% names(mapper)) assign(vrbl, mapper[["default"]])
        }
    }
    
    # Step 1: Download a new file (if requested)
    if (!is.null(url) & isTRUE(getData)) fileDownload(fileName=fileName, url=url, ovrWrite=ovrWriteDownload)
    else cat("\nNo file has been downloaded, will use existing file:", fileName, "\n")
    
    # Step 2: Read file, rename and mutate variables, confirm uniqueness by expected levels
    dfRaw <- fileRead(fileName) %>% 
        colRenamer(vecRename) %>% 
        colMutater(selfList=selfList, fullList=fullList) %>%
        checkUniqueRows(uniqueBy=uniqueBy)
    
    # Step 3: Plot basic control totals for new cases and new deaths by month
    dfRaw %>%
        checkControl(groupBy=step3Group, useVars=step3Vals, printControls=FALSE, na.rm=TRUE) %>%
        helperLinePlot(x=step3Group, y="newValue", facetVar="name", facetScales="free_y", groupColor="name")
    
    # If there is no file for comparison, return the data
    if (is.null(dfRef) | if(length(dfRef)==1) is.na(dfRef) else FALSE) return(dfRaw)
    
    # Step 4b: Check similarity of existing and reference file
    # ovrWriteLog=FALSE since everything should be an append after the opening text line in step 0
    diffRaw <- checkSimilarity(df=dfRaw, 
                               ref=dfRef, 
                               keyVars=step4KeyVars, 
                               writeLog=writeLog, 
                               ovrwriteLog=FALSE
                               )
    
    # Step 5: Plot the similarity checks
    plotSimilarity(diffRaw, plotItems=step5PlotItems)
    
    # Step 6: Plot and report on differences in aggregates
    helperAggMap <- function(x) {
        h1 <- compareAggregate(df=dfRaw, ref=dfRef, grpVar=x$grpVar, numVars=x$numVars, 
                               sameUniverse=x$sameUniverse, plotData=x$plotData, isLine=x$isLine, 
                               returnDelta=x$returnDelta)
        if (isTRUE(x$flagLargeDelta)) {
            h2 <- flagLargeDelta(h1, pctTol=x$pctTol, absTol=x$absTol, sortBy=x$sortBy, 
                                 dropNA=x$dropNA, printAll=x$printAll
                                 )
            if (is.null(writeLog)) print(h2)
            else {
                cat(nrow(h2), " records", sep="")
                txt <- paste0("\n\n***Differences of at least ", 
                              x$absTol, 
                              " and at least ", 
                              round(100*x$pctTol, 3), "%\n\n"
                              )
                printLog(h2, txt=txt, writeLog=writeLog)
            }
        }
    }
    lapply(step6AggregateList, FUN=helperAggMap)
    
    cat("\n\n")
    
    # Return the raw data file
    dfRaw
    
}

# Explore for only reading and returning new data
testFile <- "./RInputFiles/Coronavirus/Weekly_United_States_COVID-19_Cases_and_Deaths_by_State.csv"
dfTest <- readQCRawCDCDaily(fileName=testFile, 
                            getData=FALSE, 
                            urlType="cdcWeekly",
                            url=c(),
                            vecRename=c("end_date"="date"),
                            selfList=list(),
                            fullList=list(),
                            uniqueBy=c("state", "date"), 
                            step3Group=c("date"), 
                            step3Vals=c("new_cases", "new_deaths"),
                            step4KeyVars=list("date"=list("label"="date", "countOnly"=TRUE, "convChar"=TRUE), 
                                              "state"=list("label"="state", "countOnly"=FALSE)
                                              ), # will need to update checkSimilarityMapper
                            step5PlotItems=c("date"),
                            step6AggregateList=list() # will need to update keyAggMapper
                            )
all.equal(dfTest, rename(tmpBurden, date=end_date))

```
  
The function for processing downloaded data, without comparison to previous data, works as intended. Next steps are to update the appropriate mapping files to include "cdcWeeklyBurden" as an option, then check that the function runs as intended. Elements are added to the mapping list manually at first:  
```{r, fig.height=9, fig.width=9}

# Need to combine NYC as part of NY
lstComboMapper$cdcWeeklyBurden <- list("nyc"=list("comboVar"="state", 
                                                  "uqVars"="date", 
                                                  "vecCombo"=c("NY"="NY", "NYC"="NY"),
                                                  "fn"=specNA(sum)
                                                  )
                                       )

# Need to test URL mapping later
if("cdcWeeklyBurden" %in% names(urlMapper)) {
    urlMapper["cdcWeeklyBurden"] <- "https://data.cdc.gov/api/views/pwn4-m3yp/rows.csv?accessType=DOWNLOAD"
} else {
    origNames <- names(urlMapper)
    urlMapper <- c(urlMapper, "https://data.cdc.gov/api/views/pwn4-m3yp/rows.csv?accessType=DOWNLOAD")
    names(urlMapper) <- c(origNames, "cdcWeeklyBurden")
}
    

# Rename end_date to date
renMapper$cdcWeeklyBurden <- c('end_date'='date')

# No need for variable mapping (formats OK as-is)
selfListMapper$cdcWeeklyBurden <- list()
fullListMapper$cdcWeeklyBurden <- list()

# File should be unique by state-date
uqMapper$cdcWeeklyBurden <- c("state", "date")

# Keep only 50 states (after NYC mapping) and DC
lstFilterMapper$cdcWeeklyBurden <- list("state"=c(state.abb, "DC"))

# Keep date, state, tot_cases, new_cases, tot_deaths, new_deaths
vecSelectMapper$cdcWeeklyBurden <- c("date", "state", "tot_cases", "tot_deaths", "new_cases", "new_deaths")

# Checks for control groups
checkControlGroupMapper$cdcWeeklyBurden <- c("date")
checkControlVarsMapper$cdcWeeklyBurden <- c("new_cases", "new_deaths")

# Check for similarity mapping
checkSimilarityMapper$cdcWeeklyBurden <- list(date=list(label='date', countOnly=TRUE, convChar=TRUE), 
                                              state=list(label='state', countOnly=FALSE)
                                              )
plotSimilarityMapper$cdcWeeklyBurden <- c("date")

# Update keyAggMapper (use cdcDaily for now, probably need to update later)
keyAggMapper$cdcWeeklyBurden <- keyAggMapper$cdcDaily

# No variables need to be kept as-is (avoiding rolling 7-day)
asIsMapper$cdcWeeklyBurden <- c()

# No changes needed to perCapMapper or hhsMapper

# Run sample code
dfTest_v2 <- readQCRawCDCDaily(fileName=testFile, 
                               getData=FALSE, 
                               urlType="cdcWeeklyBurden",
                               url=c()
                               )
all.equal(dfTest, dfTest_v2)

```
  
Next steps are to enable URL downloads and enable checking against a reference file. Downloading of new data is attempted:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run sample code
dfTest_v3 <- readQCRawCDCDaily(fileName="./RInputFiles/Coronavirus/CDC_dc_downloaded_221118.csv", 
                               getData=TRUE, 
                               urlType="cdcWeeklyBurden"
                               )
dfTest_v3

```
  
Checking against a reference file is also enabled:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

dfTest_v3_ref <- readQCRawCDCDaily(fileName="./RInputFiles/Coronavirus/CDC_dc_downloaded_221118.csv", 
                                   getData=FALSE,
                                   dfRef=dfTest_v2,
                                   urlType="cdcWeeklyBurden"
                                   )
dfTest_v3_ref

```
  
New columns have been added to explain jumps in total cases/deaths that are not explained by that week's reported new cases/deaths. These data are explored:  
```{r, fig.height=9, fig.width=9}

# Full file
testFull <- read_csv("./RInputFiles/Coronavirus/CDC_dc_downloaded_221118.csv")

# Entries with non-zero for "new_historic_*"
testFull %>%
    filter(new_historic_cases != 0 | new_historic_deaths != 0)

# Cases where cumsum and total are not equal
testFull %>%
    arrange(state, end_date) %>% 
    group_by(state) %>% 
    mutate(delta=tot_deaths-cumsum(new_deaths)) %>% 
    filter(delta != 0)

```
  
The field appears to be working as intended. Next steps are to compare the cases and deaths data to the previous daily data files:  
```{r, fig.height=9, fig.width=9}

# Conversion of raw CDC daily data to cumsum
cdcDailyTest <- cdc_daily_221102$dfRaw$cdcDaily %>%
    select(date, state, new_cases, new_deaths) %>%
    arrange(state, date) %>%
    group_by(state) %>%
    mutate(cum_cases=cumsum(new_cases), cum_deaths=cumsum(new_deaths)) %>%
    ungroup()

# Integration of new weekly data, using end_date=date
testCombo <- testFull %>%
    select(date=end_date, state, wkly_tot_cases=tot_cases, wkly_tot_deaths=tot_deaths) %>%
    left_join(cdcDailyTest, by=c("date", "state"))
testCombo

# Calculation of RMSE by state for 'tot_deaths'
testCombo %>%
    filter(date <= "2022-10-12") %>%
    mutate(delta=wkly_tot_deaths-cum_deaths) %>%
    group_by(state) %>%
    summarize(rmse=sqrt(mean(delta**2)), r2=1-rmse/mean(cum_deaths)) %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    ggplot(aes(x=fct_reorder(state, r2), y=r2)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(label=round(r2, 2)), hjust=0) +
    coord_flip() + 
    labs(x=NULL, y="Similarity of reported deaths (weekly vs. daily)\n(1.0 means perfect alignment)")

# Select states plotted
testCombo %>%
    filter(state %in% c("OR", "LA", "DC", "PA", "NE", "WA", "OH", "MO", "AK")) %>%
    select(date, state, wkly_tot_deaths, cum_deaths) %>%
    pivot_longer(-c(state, date)) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("wkly_tot_deaths"="Weekly data (new)", "cum_deaths"="Daily data (old)")[name]
                  )
              ) + 
    facet_wrap(~state, scales="free_y") + 
    scale_color_discrete("Source:") + 
    labs(title="Comparison of cumulative deaths by source", y="Cumulative deaths", x=NULL)

# Overall data
testCombo %>%
    filter(state %in% c(state.abb, "DC", "NYC")) %>%
    group_by(date) %>%
    summarize(across(where(is.numeric), specNA())) %>%
    select(date, wkly_tot_deaths, cum_deaths) %>%
    pivot_longer(-c(date)) %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("wkly_tot_deaths"="Weekly data (new)", "cum_deaths"="Daily data (old)")[name]
                  )
              ) + 
    scale_color_discrete("Source:") + 
    labs(title="Comparison of cumulative deaths by source", 
         y="Cumulative deaths", 
         x=NULL, 
         subtitle="50 states plus DC"
         )

```
  
Alignment of reported deaths varies by state. In aggregate, the newer data appears to report slightly more deaths. The data file is processed using the existing function:  
```{r, fig.height=9, fig.width=9}

dfTestProcess <- processRawFile(dfTest_v3_ref, 
                                vecRename=c(), 
                                vecSelect=vecSelectMapper[["cdcWeeklyBurden"]], 
                                lstCombo=lstComboMapper[["cdcWeeklyBurden"]],
                                lstFilter=lstFilterMapper[["cdcWeeklyBurden"]]
                                )
dfTestProcess

```
  
Data need to be converted appropriately to daily format for 7-day rolling averages. Totals should be carried forward, while new values should be left as-is. This step needs to be added either to processRawFile() or createPerCapita():
```{r, fig.height=9, fig.width=9}

# Conversion to daily data
dfTestProcessDaily <- expand.grid(seq.Date(dfTestProcess %>% pull(date) %>% min, 
                                           dfTestProcess %>% pull(date) %>% max, 
                                           by=1
                                           ), 
                                  dfTestProcess %>% pull(state) %>% unique
                                  ) %>% 
    tibble::as_tibble() %>% 
    set_names(c("date", "state")) %>% 
    left_join(dfTestProcess, by=c("date", "state")) %>% 
    mutate(across(starts_with("tot"), .fns=function(x) zoo::na.locf(x)), 
           across(starts_with("new"), .fns=function(x) ifelse(is.na(x), 0, x))
           )
dfTestProcessDaily

dfTestPerCapita <- createPerCapita(dfTestProcessDaily, 
                                   uqBy=c("state", "date"), 
                                   popData=getStateData(), 
                                   mapper=perCapMapper, 
                                   asIsVars=if(isTRUE(exists("asIsMapper"))) asIsMapper[["cdcWeeklyBurden"]] else c()
                                   )
dfTestPerCapita

```
  
A function is written for the conversion of weekly data to daily data:  
```{r, fig.height=9, fig.width=9}

convertWeeklyDaily <- function(df, 
                               timeVar="date", 
                               otherVars=c("state"),
                               timeUnitDays=1, 
                               locfVars=c("tot_cases", "tot_deaths"), 
                               naVars=c("new_cases", "new_deaths")
                               ) {

    # FUNCTION ARGUMENTS:
    # df: the original daily frame
    # timeVar: the time variable in the original dataset
    # otherVars: other variables of importance (df should be unique by otherVars-timeVar)
    # timeUnitDays: final data should be every 'timeUnitDays' days (e.g., 2 would be every other day)
    # locfVars: variables for applying zoo::na.locf
    # naVars: variables for applying ifelse(is.na(x), 0, x)
    
    # Check that all variables are included
    nms1 <- sort(names(df))
    nms2 <- sort(c(timeVar, otherVars, locfVars, naVars))
    if(!isTRUE(all.equal(nms1, nms2))) {
        warning(paste0("\nSome variables not passed to convertWeeklyDaily() will be treated as-is:", 
                       "\ndf has variables: ", 
                       paste0(nms1, collapse=", "), 
                       "\nvariables passed as arguments are: ", 
                       paste0(nms2, collapse=", "), 
                       "\n"
                       )
                )
    }
    
    # Relevant times to include
    keyTimes <- seq.Date(df %>% pull(timeVar) %>% min, df %>% pull(timeVar) %>% max, by=timeUnitDays) %>%
        tibble::tibble() %>%
        purrr::set_names(timeVar)
    
    # Relevant levels to include
    keyLevels <- df %>% select(all_of(otherVars)) %>% unique()
    
    # Return cross-join tibble
    tibble::tibble(keyTimes) %>%
        full_join(keyLevels, by=character()) %>%
        purrr::set_names(c(timeVar, otherVars)) %>% 
        left_join(df, by=c(timeVar, otherVars)) %>% 
        arrange(across(c(all_of(otherVars), all_of(timeVar)))) %>%
        group_by(across(all_of(otherVars))) %>%
        mutate(across(all_of(locfVars), .fns=function(x) zoo::na.locf(x)), 
               across(all_of(naVars), .fns=function(x) ifelse(is.na(x), 0, x))
               ) %>%
        ungroup()

}

all.equal(dfTestProcessDaily %>% arrange(state, date), convertWeeklyDaily(dfTestProcess), check.attributes=FALSE)

```
  
The diagnostic functions are also run:  
```{r, fig.height=9, fig.width=9}

# Create plot data
plotDataTest <- diagnoseClusters(lst=list("stateData"=getStateData(), 
                                          "dfPerCapita"=dfTestPerCapita %>% mutate(hpm7=0), 
                                          "useClusters"=readFromRDS("cdc_daily_210528")$useClusters
                                          ), 
                                 lstExtract=list("stateData"=function(x) colSelector(x, vecSelect=c("state", "pop")),
                                                 "dfPerCapita"=NULL
                                                 ),
                                 wm_aggVars=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7"),
                                 detailAggVars=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7"),
                                 brewPalette="Paired"
                                 )

```
  
The missing hospital data need to be located and/or processing updated. Otherwise, existing functionality appears to work on weekly data converted to daily data. These should integrate naturally if included in processing, as hospital and vaccine data are still available from existing sources. Function readRunCDCDaily() is updated for conversions:  
```{r, fig.height=9, fig.width=9}

# Function to download/load, process, segment, and analyze data for CDC daily (last updated 02-AUG-2021)
readRunCDCDaily <- function(thruLabel, 
                            downloadTo=list("cdcDaily"=NA, "cdcHosp"=NA, "vax"=NA), 
                            readFrom=downloadTo, 
                            compareFile=list("cdcDaily"=NA, "cdcHosp"=NA, "vax"=NA),
                            writeLog=NULL,
                            ovrwriteLog=TRUE,
                            dfPerCapita=NULL,
                            useClusters=NULL,
                            hierarchical=TRUE,
                            returnList=!isTRUE(hierarchical), 
                            kCut=6,
                            reAssignState=vector("list", 0),
                            weightedMeanAggs=eval(formals(diagnoseClusters)$wm_aggVars),
                            detailedPlotAggs=weightedMeanAggs,
                            skipAssessmentPlots=FALSE,
                            brewPalette=NA,
                            convertWeekly=c("cdcWeeklyBurden"),
                            ...
                            ) {
    
    # FUNCTION ARGUMENTS:
    # thruLabel: the label for when the data are through (e.g., "Aug 30, 2020")
    # donwloadTo: named list for locations to download data (cdcDaily and cdcHosp)
    #             NA means do not download data for that particular element
    # readFrom: named list for locations to read data from (defaults to donwloadTo)
    # compareFile: named list for the reference file to be used for cdcDaily and cdcHosp 
    #              NA means do not use a reference file for that element
    # dateChangePlot: boolean, should changes in dates be captured as a plot rather than as a list?
    # dateMetricPrint: boolean, should the changes by date and metric be printed to the main log?
    # writeLog: name of a separate log file for capturing detailed data on changes between files
    #           NULL means no detailed data captured
    # ovrwriteLog: boolean, should the log file be overwritten and started again from scratch?
    # dfPerCapita: file can be passed directly, which bypasses the loading and processing steps
    #              default NULL means create dfPerCapita using steps 2-4
    # useClusters: file containing clusters by state (NULL means make the clusters from the data)
    # hierarchical: boolean, should hierarchical clusters be produced (if FALSE, will be k-means)?
    # returnList: boolean, should a list be returned or just the cluster object?
    #             refers to what is returned by clusterStates(); the main function always returns a list
    # kCut: number of segments when cutting the hierarchical tree
    # reAssignState: mapping file for assigning a state to another state's cluster
    #                format list("stateToChange"="stateClusterToAssign")
    # weightedMeanAggs: variables where a population-weighted cluster mean should be created
    # detailedPlotAggs: variables that should be included in the cluster-level disease evolution detailed plots
    # skipAssessmentPlots: boolean to skip the plots for assessClusters()
    #                      especially useful if just exploring dendrograms or silhouette widths
    # brewPalette: create plots using this color scheme (needs to be valid in ggplot2::scale_*_brewer())
    #              NA means use R default color schemes
    # convertWeekly: signals that dfProcess for this data frame should be passed through convertWeeklyDaily()
    # ...: arguments to be passed to clusterStates(), will be used only if useClusters is NULL
    
    # STEP 0: Function to create the return list
    createFinalList <- function(plots=TRUE) {
        list(stateData=stateData, 
             dfRaw=dfRawList, 
             dfProcess=dfProcessList, 
             dfPerCapita=dfPerCapita, 
             useClusters=useClusters, 
             plotDataList=if(plots) plotDataList else NULL
             )
    }
    
    # STEP 1: Get state data
    stateData <- getStateData()
    
    # If a log file is requested, create the log file (allows for append=TRUE for all downstream functions)
    if (!is.null(writeLog)) genNewLog(writeLog=writeLog, ovrwriteLog=ovrwriteLog)
    
    # Get the data types to be used (will be the elements of readFrom) and create a file storage list
    elemUsed <- names(readFrom)
    dfRawList <- vector("list", length=length(elemUsed)) %>% purrr::set_names(elemUsed)
    dfProcessList <- vector("list", length=length(elemUsed)) %>% purrr::set_names(elemUsed)
    
    # Steps 2-4 are run only is dfPerCapita has not been passed
    if (is.null(dfPerCapita)) {
        
        # Step 2: Download and QC all of the requested data
        for (elem in elemUsed) {
            dfRawList[[elem]] <- readQCRawCDCDaily(fileName=readFrom[[elem]], 
                                                   writeLog=writeLog, 
                                                   ovrwriteLog=FALSE,
                                                   urlType=elem, 
                                                   getData=if(is.na(downloadTo[[elem]])) FALSE else TRUE, 
                                                   dfRef=compareFile[[elem]]
                                                   )
            glimpseLog(dfRawList[[elem]], txt=paste0("\nRaw file for ", elem, ":\n"), logFile=writeLog)
        }
        
        # Step 3: Process all of the requested data
        for (elem in elemUsed) {
            dfProcessList[[elem]] <- processRawFile(dfRawList[[elem]], 
                                                    vecRename=c(), # already handled in readQCRawCDCDaily()
                                                    vecSelect=vecSelectMapper[[elem]], 
                                                    lstCombo=lstComboMapper[[elem]], 
                                                    lstFilter=lstFilterMapper[[elem]]
                                                    )
            # Run conversions of weekly to daily if needed
            if(elem %in% all_of(convertWeekly)) dfProcessList[[elem]] <- convertWeeklyDaily(dfProcessList[[elem]])
            glimpseLog(dfProcessList[[elem]], txt=paste0("\nProcessed for ", elem, ":\n"), logFile=writeLog)
        }
        
        # Step 4: Integrate in to a dfPerCapita file and glimpse (to specified log file)
        dfPerCapita <- createPerCapita(dfProcessList, 
                                       uqBy=c("state", "date"), 
                                       popData=stateData, 
                                       mapper=perCapMapper, 
                                       asIsVars=if(isTRUE(exists("asIsMapper"))) unname(unlist(asIsMapper)) else c()
                                       )
        glimpseLog(dfPerCapita, txt="\nIntegrated per capita data file:\n", logFile=writeLog)
        
    } else {
        dfRawList <- NULL
        dfProcessList <- NULL
    }
    
    # STEP 5: Create the clusters (if they have not been passed)
    if (is.null(useClusters)) {
        clData <- clusterStates(df=dfPerCapita, hierarchical=hierarchical, returnList=returnList, ...)
        useClusters <- getClusters(clData, hier=hierarchical, kCut=kCut, reAssign=reAssignState)
    }
    
    # STEP 5a: Stop the process and return what is available if skipAssessmentPlots is TRUE
    if (skipAssessmentPlots) 
        return(createFinalList(plots=FALSE))
    
    # STEP 6: Create the cluster assessments
    lstFuns <- list("stateData"=function(x) colSelector(x, vecSelect=c("state", "pop")), 
                    "dfPerCapita"=NULL
                    )
    plotDataList <- diagnoseClusters(lst=list("stateData"=stateData, 
                                              "dfPerCapita"=dfPerCapita, 
                                              "useClusters"=useClusters
                                              ), 
                                     lstExtract=lstFuns,
                                     wm_aggVars=weightedMeanAggs,
                                     detailAggVars=detailedPlotAggs,
                                     brewPalette=brewPalette
                                     )
    
    # STEP 7: Return a list of the key data
    return(createFinalList(plots=TRUE))
    
}

```
  
Next steps are to test the function on all data sources at the same time. The diagnostic functions are first run using an amalgamated dfProcess:  
```{r, fig.height=9, fig.width=9}

# Create per-capita data
asIsMapper <- list("cdcDaily"=c(),
                   "cdcWeekly"=c(),
                   "cdcHosp"=c(), 
                   "vax"=c("vxcpoppct", "vxcgte65", "vxcgte65pct", "vxcgte18", "vxcgte18pct")
                   )
dfTestPerCapita_v2 <- createPerCapita(list(cdcDaily=dfTestProcessDaily, 
                                           cdcHosp=cdc_daily_221102$dfProcess$cdcHosp, 
                                           vax=cdc_daily_221102$dfProcess$vax
                                           ), 
                                      uqBy=c("state", "date"), 
                                      popData=getStateData(), 
                                      mapper=perCapMapper, 
                                      asIsVars=if(isTRUE(exists("asIsMapper"))) unname(unlist(asIsMapper)) else c()
                                      )
dfTestPerCapita_v2
all.equal(dfTestPerCapita_v2 %>% 
              select(names(dfTestPerCapita)) %>% 
              filter(date >= "2020-01-22", date <= "2022-11-16"), 
          dfTestPerCapita
          )

# Create plot data
plotDataTest_v2 <- diagnoseClusters(lst=list("stateData"=getStateData(), 
                                             "dfPerCapita"=dfTestPerCapita_v2, 
                                             "useClusters"=readFromRDS("cdc_daily_210528")$useClusters
                                             ), 
                                    lstExtract=list("stateData"=function(x) colSelector(x, vecSelect=c("state", "pop")),
                                                    "dfPerCapita"=NULL
                                                    ),
                                    wm_aggVars=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7"),
                                    detailAggVars=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7"),
                                    brewPalette="Paired"
                                    )

```
  
Data appear to be integrating as expected. The asIsMapper call in readRunCDCDaily() has also been updated to correctly call every item in a vector in the list
  
The function is run to get the most recent weekly data and integrate with existing hospital and vaccines data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcWeeklyBurden"="./RInputFiles/Coronavirus/CDC_dc_wkly_downloaded_221128.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_221102.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_221102.csv"
                 )
compareList <- list("cdcWeeklyBurden"=dfTest_v3_ref, 
                    "cdcHosp"=readFromRDS("cdc_daily_221002")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_221002")$dfRaw$vax
                    )

cdc_daily_221128 <- readRunCDCDaily(thruLabel="Nov 24, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_221128, ovrWriteError=FALSE)

```

Data are checked for consistency with previous data:  
```{r, fig.height=9, fig.width=9}

tempGetBurden <- function(df, 
                          numVars=c("tcpm7", "cpm7", "tdpm7", "dpm7"), 
                          natPop=getStateData() %>% pull(pop) %>% sum()
                          ) {

    df %>%
        select(c("state", "date", all_of(numVars))) %>%
        pivot_longer(-c(state, date)) %>%
        filter(!is.na(value)) %>%
        left_join(getStateData() %>% select(state, pop), by="state") %>%
        group_by(date, name) %>%
        summarize(natPerCap=sum(value*pop)/natPop, .groups="drop")
    
}

# Plot for new weekly data
tempGetBurden(cdc_daily_221128$dfPerCapita) %>%
    ggplot(aes(x=date, y=natPerCap)) + 
    geom_line() + 
    facet_wrap(~name, scales="free_y") + 
    labs(x=NULL, y="Rolling 7 per million", title="Weekly data per million (rolling 7-day average)")
    
# Plot for old daily data
tempGetBurden(cdc_daily_221102$dfPerCapita) %>%
    ggplot(aes(x=date, y=natPerCap)) + 
    geom_line() + 
    facet_wrap(~name, scales="free_y") + 
    labs(x=NULL, y="Rolling 7 per million", title="Daily data per million (rolling 7-day average)")

# Integrated data
tempGetBurden(cdc_daily_221128$dfPerCapita) %>%
    bind_rows(tempGetBurden(cdc_daily_221102$dfPerCapita), .id="src") %>%
    mutate(src=c("1"="New Weekly", "2"="Old Daily")[src]) %>%
    ggplot(aes(x=date, y=natPerCap)) + 
    geom_line(aes(color=src, group=src)) + 
    facet_wrap(~name, scales="free_y") + 
    labs(x=NULL, 
         y="Rolling 7 per million", 
         title="Data per million comparison of new weekly and old daily (rolling 7-day average)"
         )
    
```
  
As expected, there are minor differences in rolling-7 totals by day for data reported on a daily and weekly basis. Overall, trends and data volumes are very similar, at a national level, between the data sources. Weekly data could be shifted "backwards" by a few days to reflect that a spike of 5 deaths per day will report as ...0-0-0-0-0-0-35-0-0..., which converts to rolling-7 as 0-0-0-5-5-5-5-5-5-5... Given the many issues in the data capture and reporting, this likely has minor, if any, practical impact
  
Comparisons are also made by state and metric:  
```{r, fig.height=9, fig.width=9}

dfSimilar <- cdc_daily_221102$dfPerCapita %>%
    select(c("state", "date", where(is.numeric))) %>%
    pivot_longer(-c(state, date)) %>%
    filter(!is.na(value)) %>%
    bind_rows(cdc_daily_221128$dfPerCapita %>%
                  select(c("state", "date", where(is.numeric))) %>%
                  pivot_longer(-c(state, date)) %>%
                  filter(!is.na(value)), 
              .id="src"
              ) %>%
    mutate(src=c("1"="Old Daily", "2"="New Weekly")[src]) %>%
    filter(name %in% c("tcpm7", "tdpm7", "cpm7", "dpm7"))
dfSimilar

# Ratio comparisons
dfSimilar %>%
    filter(date >= "2020-03-01", date <= "2022-10-15") %>%
    pivot_wider(c(state, date, name), names_from="src", values_fill=0) %>%
    group_by(state, name) %>%
    summarize(meanDelta=sqrt(mean((`Old Daily`-`New Weekly`)**2)), 
              meanDaily=mean(`Old Daily`), 
              meanWeekly=mean(`New Weekly`), 
              .groups="drop"
              ) %>%
    mutate(keyMetric=meanDelta*2/(meanDaily+meanWeekly)) %>%
    ggplot(aes(x=fct_reorder(state, keyMetric), y=keyMetric)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(label=round(keyMetric, 2))) + 
    coord_flip() + 
    facet_wrap(~name, scales="free_x", nrow=1) + 
    labs(x=NULL, y="RMSE divided by mean", title="Difference in weekly and daily data by state")

# Plotting data for Maine (high RMSE in cpm7)
dfSimilar %>%
    filter(state=="ME") %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=src, color=src)) + 
    facet_wrap(~name, scales="free_y") + 
    labs(title="Example data for Maine", x=NULL, y="Burden per million (rolling 7-day average)")

# Plotting data for Washington (high RMSE in tdpm7)
dfSimilar %>%
    filter(state=="WA") %>%
    ggplot(aes(x=date, y=value)) + 
    geom_line(aes(group=src, color=src)) + 
    facet_wrap(~name, scales="free_y") + 
    labs(title="Example data for Washington", x=NULL, y="Burden per million (rolling 7-day average)")

```
  
At a state level, there are some differences in timing of waves between the daily and weekly data. Washington is particularly anomalous in having spikes in deaths offset by several weeks or months, even as cases are well aligned.
  
The latest data are downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

readList <- list("cdcWeeklyBurden"="./RInputFiles/Coronavirus/CDC_dc_wkly_downloaded_221202.csv", 
                 "cdcHosp"="./RInputFiles/Coronavirus/CDC_h_downloaded_221202.csv", 
                 "vax"="./RInputFiles/Coronavirus/vaxData_downloaded_221202.csv"
                 )
compareList <- list("cdcWeeklyBurden"=dfTest_v3_ref, 
                    "cdcHosp"=readFromRDS("cdc_daily_221102")$dfRaw$cdcHosp, 
                    "vax"=readFromRDS("cdc_daily_221102")$dfRaw$vax
                    )

cdc_daily_221202 <- readRunCDCDaily(thruLabel="Nov 30, 2022", 
                                    downloadTo=lapply(readList, FUN=function(x) if(file.exists(x)) NA else x), 
                                    readFrom=readList,
                                    compareFile=compareList, 
                                    writeLog=NULL, 
                                    useClusters=readFromRDS("cdc_daily_210528")$useClusters, 
                                    weightedMeanAggs=c("tcpm7", "tdpm7", "cpm7", "dpm7", "hpm7", 
                                                       "vxcpm7", "vxcgte65pct"
                                                       ),
                                    skipAssessmentPlots=FALSE, 
                                    brewPalette="Paired"
                                    )
saveToRDS(cdc_daily_221202, ovrWriteError=FALSE)

```
  
The latest hospitalization data is also downloaded and processed:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run for latest data, save as RDS
indivHosp_20221203 <- downloadReadHospitalData(loc="./RInputFiles/Coronavirus/HHS_Hospital_20221203.csv")
saveToRDS(indivHosp_20221203, ovrWriteError=FALSE)

```
  
Post-processing is run, including hospital summaries:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create pivoted burden data
burdenPivotList_221202 <- postProcessCDCDaily(cdc_daily_221202, 
                                              dataThruLabel="Nov 2022", 
                                              keyDatesBurden=c("2022-11-30", "2022-05-31", 
                                                               "2021-11-30", "2021-05-31"
                                                               ),
                                              keyDatesVaccine=c("2022-11-30", "2021-11-30", 
                                                                "2021-07-31", "2021-03-31"
                                                                ), 
                                              returnData=TRUE
                                              )

# Create hospitalized per capita data
hospPerCap_221202 <- hospAgePerCapita(readFromRDS("dfStateAgeBucket2019"), 
                                      lst=burdenPivotList_221202, 
                                      popVar="pop2019", 
                                      excludeState=c(), 
                                      cumStartDate="2020-07-15"
                                      )

burdenPivotList_221202$hospAge %>%
    group_by(adultPed, confSusp, age, name) %>%
    summarize(value=sum(value, na.rm=TRUE), n=n(), .groups="drop")

saveToRDS(burdenPivotList_221202, ovrWriteError=FALSE)
saveToRDS(hospPerCap_221202, ovrWriteError=FALSE)

```
  
Peaks and valleys of key metrics are also updated:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

peakValleyCDCDaily(cdc_daily_221202)

```
  
The process should be updated to reflect that there can be multiple peaks around the same time period due to the underlying data being weekly rather than daily. Vaccines data should also be double-checked
  
Hospital data are pieced together as needed:
```{r, fig.height=9, fig.width=9}

# Create modified hospital data
multiSourceHosp_20221202 <- multiSourceDataCombine(list(readFromRDS("indivHosp_20220704"),
                                                        readFromRDS("indivHosp_20221203")
                                                        ),
                                                   timeVec=as.Date("2022-01-01")
                                                   )

```
  
The updated hospital data are then plotted:
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run hospital plots
modStateHosp_20221202 <- hospitalCapacityCDCDaily(multiSourceHosp_20221202, 
                                                  plotSub="Aug 2020 to Nov 2022\nOld data used pre-2022"
                                                  )

```
  
The original makePeakValley() and findPeaks() functions are copied for reference:  
```{r, fig.height=9, fig.width=9}

# Annotate peaks and valleys in CDC daily data
# makePeakValley <- function(df, 
#                            numVar, 
#                            windowWidth,
#                            rollMean=NULL, 
#                            uqBy=c("date"), 
#                            facetVar=c(), 
#                            fnNumVar=function(x) x, 
#                            fnPeak=function(x) x+100, 
#                            fnValley=function(x) x-100, 
#                            fnGroupFacet=FALSE,
#                            useTitle="", 
#                            yLab=""
#                            ) {
#     
#     # FUNCTION ARGUMENTS
#     # df: a data frame or tibble
#     # numVar: the numeric variable of interest
#     # windowWidth: width of the window for calculating peaks and valleys
#     # rollMean: the number of days for rolling mean (NULL means no rolling mean)
#     # uqBy: variable that the resutling data should be unique by
#     # facetVar: variable for faceting (c() means no facets)
#     # fnNumVar: what function should be applied to numVar (e.g., function(x) x/1000)
#     # fnPeak: function for plotting the peak labels
#     # fnValley: function for plotting the valley labels
#     # fnGroupFacet: boolean, should the functions be run separatelt for each facet as a grouping variable?
#     #               useful for labeling if the goal is to use 0.1*max(yVar) rather than a global peak and valley
#     # useTitle: title for plots
#     # yLab: y-axis label for plots
#     
#     # Create named vectors for useTitle and yLab if not passed
#     if(is.null(names(useTitle))) 
#         useTitle <- rep(useTitle, times=length(numVar)) %>% purrr::set_names(all_of(numVar))
#     if(is.null(names(yLab))) 
#         yLab <- rep(yLab, times=length(numVar)) %>% purrr::set_names(all_of(numVar))
#     
#     # Create named lists for fnNumVar, fnPeak, and fnValley
#     tempMakeList <- function(f, n, nms) {
#         tempList <- vector("list", length=n)
#         for(a in 1:n) tempList[[a]] <- f
#         names(tempList) <- nms
#         tempList
#     }
#     if(is.null(names(fnNumVar))) fnNumVar <- tempMakeList(fnNumVar, n=length(numVar), nms=numVar)
#     if(is.null(names(fnPeak))) fnPeak <- tempMakeList(fnPeak, n=length(numVar), nms=numVar)
#     if(is.null(names(fnValley))) fnValley <- tempMakeList(fnValley, n=length(numVar), nms=numVar)
#     
#     # Create the relevant data frame
#     newDF <- df %>% 
#         group_by_at(all_of(c(uqBy, facetVar))) %>%
#         summarize(across(all_of(numVar), .fns=sum, na.rm=TRUE), .groups="drop") %>%
#         group_by_at(all_of(facetVar)) %>%
#         mutate(if(!is.null(rollMean)) across(all_of(numVar), .fns=zoo::rollmean, k=rollMean, fill=NA),
#                across(all_of(numVar), .fns=findPeaks, width=windowWidth, gt=1, .names="{.col}_isPeak"),
#                across(all_of(numVar), 
#                       .fns=findPeaks, 
#                       width=windowWidth, 
#                       FUN=min, 
#                       gt=1, 
#                       lt=NULL, 
#                       fillVal=NA 
#                       ,.names="{.col}_isValley"
#                )
#         ) %>% 
#         ungroup()
#     
#     # Group by the facet variable(s) if not NULL and separate function by facet requested
#     if(!is.null(facetVar) & isTRUE(fnGroupFacet)) newDF <- newDF %>% group_by_at(all_of(facetVar))
#     
#     # Create the relevant plots
#     for(keyVar in numVar) {
#         
#         p1 <- newDF %>%
#             mutate(posPeak=fnPeak[[keyVar]](fnNumVar[[keyVar]](get(keyVar))), 
#                    posValley=fnValley[[keyVar]](fnNumVar[[keyVar]](get(keyVar)))
#             ) %>%
#             ggplot(aes(x=get(uqBy), y=fnNumVar[[keyVar]](get(keyVar)))) + 
#             geom_line() + 
#             geom_point(data=~filter(., get(paste0(keyVar, "_isPeak"))), color="red", size=3) +
#             geom_point(data=~filter(., get(paste0(keyVar, "_isValley"))), color="green", size=3) + 
#             geom_text(data=~filter(., get(paste0(keyVar, "_isPeak"))), 
#                       aes(y=posPeak, 
#                           label=paste0(get(uqBy), "\n", round(fnNumVar[[keyVar]](get(keyVar))))
#                       ), 
#                       color="red", 
#                       size=3
#             ) + 
#             geom_text(data=~filter(., get(paste0(keyVar, "_isValley"))), 
#                       aes(y=posValley, 
#                           label=paste0(get(uqBy), "\n", round(fnNumVar[[keyVar]](get(keyVar))))
#                       ), 
#                       color="black", 
#                       size=3
#             ) + 
#             labs(x=NULL, 
#                  y=yLab[[keyVar]], 
#                  title=useTitle[[keyVar]], 
#                  subtitle="Red (peaks) and green (valleys)"
#             )
#         if(length(facetVar) > 0) p1 <- p1 + facet_wrap(~get(facetVar), scales="free_y")
#         
#         print(p1)
#         
#     }
#     
#     # Return the data, removing any grouping
#     newDF %>% ungroup()
#     
# }

# # Function to find local extrema in a vector
# findPeaks <- function(x, 
#                       width=1, 
#                       align="center", 
#                       FUN=max, 
#                       gt=if(identical(FUN, max)) 0 else NULL, 
#                       lt=if(identical(FUN, min)) 0 else NULL, 
#                       fillVal=if(identical(FUN, max)) gt else if(identical(FUN, min)) lt else NA, 
#                       epsTol=1e-12,
#                       returnBool=TRUE, 
#                       ...
#                       ) {
#     
#     # FUNCTION ARGUMENTS:
#     # x: a numeric vector
#     # width: the width of the window to use
#     # align: whether the window should be "center", "left", or "right"
#     # FUN: the function to be used (max to find peaks, min to find valleys)
#     # gt: to be defined, the value must be greater than gt (NULL means use any value)
#     # lt: to be defined, the value must be less than lt (NULL means use any value)
#     # fillVal: value to use as output if a window does not exist (too close to boundary)
#     # epsTol: the epsilon value for considering two values to be the same
#     # returnBool: should the boolean be returned? TRUE means return TRUE/FALSE for peaks, FALSE means return vector
#     # ...: any other arguments to be passed to zoo::rollapply()
#     
#     # Create the rolling data
#     rolls <- zoo::rollapply(x, width=width, align=align, FUN=FUN, fill=fillVal, ...)
#     
#     # No post-processing applied unless returnBool is TRUE
#     if(!isTRUE(returnBool)) return(rolls)
#     
#     # Post-processing managed for gt and lt
#     if(!is.null(gt)) rolls <- ifelse(rolls<=gt, NA, rolls)
#     if(!is.null(lt)) rolls <- ifelse(rolls>=lt, NA, rolls)
#     
#     # Return the boolean vector
#     !is.na(rolls) & (abs(rolls-x) <= epsTol)
#     
# }

# Example for issue with weekly data converted to daily
testVec <- c(rep(c(3,2,4,5,2,3), each=7))
tibble::tibble(x=1:42, y1=testVec, y2=zoo::rollmean(testVec, 7, fill=NA), c=findPeaks(testVec, width=7)) %>%
    pivot_longer(-c(x, c)) %>%
    ggplot(aes(x=x, y=value, color=c)) + 
    geom_point() +
    labs(title="Example for findPeaks()", x="Index", y="Value") +
    facet_wrap(~c("y1"="Original", "y2"="Rolling 7")[name]) +
    scale_fill_discrete("Identified as peak?")

# Example for rolling-7 data
testVec2 <- zoo::rollmean(testVec, k=7, fill=0)
tibble::tibble(x=1:42, y1=testVec2, c=findPeaks(testVec2, width=7)) %>%
    pivot_longer(-c(x, c)) %>%
    ggplot(aes(x=x, y=value, color=c)) + 
    geom_point() +
    labs(title="Example for findPeaks()", x="Index", y="Value") +
    scale_fill_discrete("Identified as peak?")

```

Issues to address include 1) the midpoint always identified as a maximum, and 2) multiple tied values all identified as peaks. The issue appears to be addressed by first passing the data as rolling-7

The definition for maximum can be updated for improved precision. Using the concept of window k (assuming an odd number and "center", this means looking (k-1)/2 in each direction), then a peak can be defined as:  
  
* The value is greater than all of the previous (k-1)/2 points  
* The value is at least as high as all of the next (k-1)/2 points  
  
This should preclude finding points in the middle of an equal range even when they are clearly not peaks. An example includes:  
```{r, fig.height=9, fig.width=9}

# Example for plain data
tibble::tibble(x=1:42, 
               y=testVec, 
               testGT=testVec-zoo::rollapply(lag(testVec, 1), width=3, FUN=max, fill=NA, align="right"), 
               testGTE=testVec-zoo::rollapply(lead(testVec, 1), width=3, FUN=max, fill=NA, align="left"), 
               isPeak=(testGT>0) & (testGTE>=0)
               ) %>%
    pivot_longer(-c(x, y)) %>%
    mutate(color=ifelse(name=="isPeak", 
                        ifelse(!is.na(value) & value>0, "01. Peak", "02. Not Peak"), 
                        case_when(is.na(value) ~ "06. NA", 
                                  value==0 ~ "04. Zero", 
                                  value>0 ~ "03. GT", 
                                  value<0 ~ "05. LT"
                                  )
                        )
           ) %>%
    ggplot(aes(x=x, y=y, color=color)) + 
    geom_point() + 
    geom_point(data=~filter(., name=="isPeak", color=="01. Peak"), shape=21, size=10) +
    facet_wrap(~c("testGT"="Previous 3", "testGTE"="Next 3", "isPeak"="Identified as Peak")[name]) +
    scale_color_discrete("Versus closest 3?") + 
    labs(title="Exploring options for findPeaks()", x="Index", y="Value")

# Example for rolling-7 data
tibble::tibble(x=1:42, 
               y=testVec2, 
               testGT=testVec2-zoo::rollapply(lag(testVec2, 1), width=3, FUN=max, fill=NA, align="right"), 
               testGTE=testVec2-zoo::rollapply(lead(testVec2, 1), width=3, FUN=max, fill=NA, align="left"), 
               isPeak=(testGT>0) & (testGTE>=0)
               ) %>%
    pivot_longer(-c(x, y)) %>%
    mutate(color=ifelse(name=="isPeak", 
                        ifelse(!is.na(value) & value>0, "01. Peak", "02. Not Peak"), 
                        case_when(is.na(value) ~ "06. NA", 
                                  value==0 ~ "04. Zero", 
                                  value>0 ~ "03. GT", 
                                  value<0 ~ "05. LT"
                                  )
                        )
           ) %>%
    ggplot(aes(x=x, y=y, color=color)) + 
    geom_point() + 
    geom_point(data=~filter(., name=="isPeak", color=="01. Peak"), shape=21, size=10) +
    facet_wrap(~c("testGT"="Previous 3", "testGTE"="Next 3", "isPeak"="Identified as Peak")[name]) +
    scale_color_discrete("Versus closest 3?") + 
    labs(title="Exploring options for findPeaks()", x="Index", y="Value")

```
  
The approaches are compared:  
```{r, fig.height=9, fig.width=9}

# Example for plain data
tibble::tibble(x=1:42, 
               y=testVec, 
               testGT=testVec-zoo::rollapply(lag(testVec, 1), width=3, FUN=max, fill=NA, align="right"), 
               testGTE=testVec-zoo::rollapply(lead(testVec, 1), width=3, FUN=max, fill=NA, align="left"), 
               isPeak=(testGT>0) & (testGTE>=0), 
               findPeak=findPeaks(testVec, width=7)
               ) %>%
    pivot_longer(-c(x, y)) %>%
    mutate(color=ifelse(name %in% c("isPeak", "findPeak"), 
                        ifelse(!is.na(value) & value>0, "01. Peak", "02. Not Peak"), 
                        case_when(is.na(value) ~ "06. NA", 
                                  value==0 ~ "04. Zero", 
                                  value>0 ~ "03. GT", 
                                  value<0 ~ "05. LT"
                                  )
                        )
           ) %>%
    ggplot(aes(x=x, y=y, color=color)) + 
    geom_point() + 
    geom_point(data=~filter(., name %in% c("isPeak", "findPeak"), color=="01. Peak"), shape=21, size=10) +
    facet_wrap(~c("testGT"="Previous 3", 
                  "testGTE"="Next 3", 
                  "isPeak"="Identified as Peak (new)", 
                  "findPeak"="Identified as Peak (fnPeak)"
                  )[name]) +
    scale_color_discrete("Versus closest 3?") + 
    labs(title="Exploring options for findPeaks()", x="Index", y="Value")

# Example for rolling-7 data
tibble::tibble(x=1:42, 
               y=testVec2, 
               testGT=testVec2-zoo::rollapply(lag(testVec2, 1), width=3, FUN=max, fill=NA, align="right"), 
               testGTE=testVec2-zoo::rollapply(lead(testVec2, 1), width=3, FUN=max, fill=NA, align="left"), 
               isPeak=(testGT>0) & (testGTE>=0), 
               findPeak=findPeaks(testVec2, width=7)
               ) %>%
    pivot_longer(-c(x, y)) %>%
    mutate(color=ifelse(name %in% c("isPeak", "findPeak"), 
                        ifelse(!is.na(value) & value>0, "01. Peak", "02. Not Peak"), 
                        case_when(is.na(value) ~ "06. NA", 
                                  value==0 ~ "04. Zero", 
                                  value>0 ~ "03. GT", 
                                  value<0 ~ "05. LT"
                                  )
                        )
           ) %>%
    ggplot(aes(x=x, y=y, color=color)) + 
    geom_point() + 
    geom_point(data=~filter(., name %in% c("isPeak", "findPeak"), color=="01. Peak"), shape=21, size=10) +
    facet_wrap(~c("testGT"="Previous 3", 
                  "testGTE"="Next 3", 
                  "isPeak"="Identified as Peak (new)", 
                  "findPeak"="Identified as Peak (fnPeak)"
                  )[name]) +
    scale_color_discrete("Versus closest 3?") + 
    labs(title="Exploring options for findPeaks()", x="Index", y="Value")

```
  
Both methods identify the same peaks when the data are clearly rising and falling. The newer approach is more robust to long periods of flat data, as are more likely to occur when the underlying dataset is weekly converted to daily converted to rolling rather than daily converted directly to rolling
  
Peak identification is converted to functional form:  
```{r, fig.height=9, fig.width=9}

# Function to find local extrema in a vector
updatedFindPeaks <- function(x,
                             width=1,
                             FUN=max,
                             # gt=if(identical(FUN, max)) 0 else NULL,
                             # lt=if(identical(FUN, min)) 0 else NULL,
                             fillVal=NA,
                             epsTol=1e-12,
                             ...
                      ) {

    # FUNCTION ARGUMENTS:
    # x: a numeric vector
    # width: the width of the window to use
    # FUN: the function to be used (max to find peaks, min to find valleys)
    # gt: to be defined, the value must be greater than gt (NULL means use any value)
    # lt: to be defined, the value must be less than lt (NULL means use any value)
    # fillVal: value to use as output if a window does not exist (too close to boundary)
    # epsTol: the epsilon value for considering two values to be the same
    # ...: any other arguments to be passed to zoo::rollapply()

    # If width is 1 or under, everything is a maximum or minimum
    if(width <= 1) return(rep(TRUE, length(x)))
    
    # Check for window width (if even number, look further left than right)
    lookLeft <- ceiling((width-1)/2)
    lookRight <- floor((width-1)/2)

    # Run the look to the left
    xLeft <- x-zoo::rollapply(lag(x, 1), width=lookLeft, FUN=FUN, fill=fillVal, align="right")
    
    # Run the look to the right, unless lookRight is 0
    if(lookRight==0) xRight <- rep(0, length(x))
    else xRight <- x-zoo::rollapply(lead(x, 1), width=lookRight, FUN=FUN, fill=fillVal, align="left")
    
    # Return the boolean vector (only enabled for MAX and MIN for now)
    if(identical(FUN, max)) { !is.na(xLeft) & !is.na(xRight) & (xLeft>epsTol) & (xRight>=(-epsTol)) }
    else if(identical(FUN, min)) { !is.na(xLeft) & !is.na(xRight) & (xLeft<(-epsTol)) & (xRight<=epsTol) }
    else rep(NA, length(x))

}

which(updatedFindPeaks(testVec, width=7))
which(updatedFindPeaks(testVec2, width=7))

which(updatedFindPeaks(testVec, width=2))
which(updatedFindPeaks(testVec2, width=2))

which(updatedFindPeaks(testVec, width=7, FUN=min))
which(updatedFindPeaks(testVec2, width=7, FUN=min))

```
  
The function is identifying peaks as expected. The function is tested against existing coronavirus deaths data:  
```{r, fig.height=9, fig.width=9}

dfTestBurden <- cdc_daily_221202$dfPerCapita %>%
    select(date, state, new_cases, new_deaths) %>%
    mutate(regn=c(as.character(state.region), "South")[match(state, c(state.abb, "DC"))]) %>%
    group_by(regn, date) %>%
    summarize(across(where(is.numeric), specNA(sum)), .groups="drop")

dfTestBurden %>%
    group_by(regn) %>%
    mutate(useDeaths=zoo::rollmean(new_deaths, k=7, fill=NA)) %>%
    mutate(isPeak=findPeaks(new_deaths, width=71, align="center", FUN=max)) %>%
    filter(isPeak)

dfTestBurden %>%
    group_by(regn) %>%
    mutate(useDeaths=zoo::rollmean(new_deaths, k=7, fill=NA)) %>%
    mutate(isPeak=updatedFindPeaks(new_deaths, width=71, align="center", FUN=max)) %>%
    filter(isPeak)

```
  
The methodologies find the same peaks, so overplotting is not caused by findPeaks. The function should still be updated for better behavior with ties. The overplotting issue needs further exploration as well

An example for deaths is included:  
```{r, fig.height=9, fig.width=9}

dfPeakDeath <- cdc_daily_221202$dfPerCapita %>%
    mutate(regn=c(as.character(state.region), "South")[match(state, c(state.abb, "DC"))]) %>%
    makePeakValley(numVar="new_deaths", 
                   windowWidth = 71, 
                   rollMean=7, 
                   facetVar=c("regn"), 
                   fnNumVar=list("new_deaths"=function(x) x), 
                   fnPeak=list("new_deaths"=function(x) x+100),
                   fnValley=list("new_deaths"=function(x) x-100),
                   useTitle=c("new_deaths"="US coronavirus deaths"), 
                   yLab=c("new_deaths"=paste0("Rolling ", 7, "-day mean deaths"))
                   )

dfPeakDeath %>%
    filter(new_deaths_isPeak) %>%
    group_by(regn) %>%
    mutate(idx=1:n()) %>%
    ggplot() + 
    geom_text(aes(x=date, y=idx, label=date)) +
    facet_wrap(~regn, scales="free_y") + 
    labs(x=NULL, y="Index", "Peaks identified in deaths data")

```
  
The updated peaks function should resolve the identification of 7 consecutive peaks. Example code:  
```{r, fig.height=9, fig.width=9}

updatedMakePeakValley <- function(df, 
                                  numVar, 
                                  windowWidth,
                                  rollMean=NULL, 
                                  uqBy=c("date"), 
                                  facetVar=c(), 
                                  fnNumVar=function(x) x, 
                                  fnPeak=function(x) x+100, 
                                  fnValley=function(x) x-100, 
                                  fnGroupFacet=FALSE,
                                  useTitle="", 
                                  yLab=""
                                  ) {
    
    # FUNCTION ARGUMENTS
    # df: a data frame or tibble
    # numVar: the numeric variable of interest
    # windowWidth: width of the window for calculating peaks and valleys
    # rollMean: the number of days for rolling mean (NULL means no rolling mean)
    # uqBy: variable that the resutling data should be unique by
    # facetVar: variable for faceting (c() means no facets)
    # fnNumVar: what function should be applied to numVar (e.g., function(x) x/1000)
    # fnPeak: function for plotting the peak labels
    # fnValley: function for plotting the valley labels
    # fnGroupFacet: boolean, should the functions be run separatelt for each facet as a grouping variable?
    #               useful for labeling if the goal is to use 0.1*max(yVar) rather than a global peak and valley
    # useTitle: title for plots
    # yLab: y-axis label for plots
    
    # Create named vectors for useTitle and yLab if not passed
    if(is.null(names(useTitle))) 
        useTitle <- rep(useTitle, times=length(numVar)) %>% purrr::set_names(all_of(numVar))
    if(is.null(names(yLab))) 
        yLab <- rep(yLab, times=length(numVar)) %>% purrr::set_names(all_of(numVar))
    
    # Create named lists for fnNumVar, fnPeak, and fnValley
    tempMakeList <- function(f, n, nms) {
        tempList <- vector("list", length=n)
        for(a in 1:n) tempList[[a]] <- f
        names(tempList) <- nms
        tempList
    }
    if(is.null(names(fnNumVar))) fnNumVar <- tempMakeList(fnNumVar, n=length(numVar), nms=numVar)
    if(is.null(names(fnPeak))) fnPeak <- tempMakeList(fnPeak, n=length(numVar), nms=numVar)
    if(is.null(names(fnValley))) fnValley <- tempMakeList(fnValley, n=length(numVar), nms=numVar)
    
    # Create the relevant data frame
    newDF <- df %>% 
        group_by_at(all_of(c(uqBy, facetVar))) %>%
        summarize(across(all_of(numVar), .fns=sum, na.rm=TRUE), .groups="drop") %>%
        group_by_at(all_of(facetVar)) %>%
        mutate(if(!is.null(rollMean)) across(all_of(numVar), .fns=zoo::rollmean, k=rollMean, fill=NA),
               across(all_of(numVar), .fns=updatedFindPeaks, width=windowWidth, FUN=max, .names="{.col}_isPeak"),
               across(all_of(numVar), .fns=updatedFindPeaks, width=windowWidth, FUN=min, .names="{.col}_isValley")
               ) %>% 
        ungroup()
    
    # Group by the facet variable(s) if not NULL and separate function by facet requested
    if(!is.null(facetVar) & isTRUE(fnGroupFacet)) newDF <- newDF %>% group_by_at(all_of(facetVar))
    
    # Create the relevant plots
    for(keyVar in numVar) {
        
        p1 <- newDF %>%
            mutate(posPeak=fnPeak[[keyVar]](fnNumVar[[keyVar]](get(keyVar))), 
                   posValley=fnValley[[keyVar]](fnNumVar[[keyVar]](get(keyVar)))
            ) %>%
            ggplot(aes(x=get(uqBy), y=fnNumVar[[keyVar]](get(keyVar)))) + 
            geom_line() + 
            geom_point(data=~filter(., get(paste0(keyVar, "_isPeak"))), color="red", size=3) +
            geom_point(data=~filter(., get(paste0(keyVar, "_isValley"))), color="green", size=3) + 
            geom_text(data=~filter(., get(paste0(keyVar, "_isPeak"))), 
                      aes(y=posPeak, 
                          label=paste0(get(uqBy), "\n", round(fnNumVar[[keyVar]](get(keyVar))))
                      ), 
                      color="red", 
                      size=3
            ) + 
            geom_text(data=~filter(., get(paste0(keyVar, "_isValley"))), 
                      aes(y=posValley, 
                          label=paste0(get(uqBy), "\n", round(fnNumVar[[keyVar]](get(keyVar))))
                      ), 
                      color="black", 
                      size=3
            ) + 
            labs(x=NULL, 
                 y=yLab[[keyVar]], 
                 title=useTitle[[keyVar]], 
                 subtitle="Red (peaks) and green (valleys)"
            )
        if(length(facetVar) > 0) p1 <- p1 + facet_wrap(~get(facetVar), scales="free_y")
        
        print(p1)
        
    }
    
    # Return the data, removing any grouping
    newDF %>% ungroup()
    
}

dfPeakDeath_v2 <- cdc_daily_221202$dfPerCapita %>%
    mutate(regn=c(as.character(state.region), "South")[match(state, c(state.abb, "DC"))]) %>%
    updatedMakePeakValley(numVar="new_deaths", 
                          windowWidth = 71, 
                          rollMean=7, 
                          facetVar=c("regn"), 
                          fnNumVar=list("new_deaths"=function(x) x), 
                          fnPeak=list("new_deaths"=function(x) x+100),
                          fnValley=list("new_deaths"=function(x) x-100),
                          useTitle=c("new_deaths"="US coronavirus deaths"), 
                          yLab=c("new_deaths"=paste0("Rolling ", 7, "-day mean deaths"))
                          )

dfPeakDeath_v2 %>%
    filter(new_deaths_isPeak) %>%
    group_by(regn) %>%
    mutate(idx=1:n()) %>%
    ggplot() + 
    geom_text(aes(x=date, y=idx, label=date)) +
    facet_wrap(~regn, scales="free_y") + 
    labs(x=NULL, y="Index", "Peaks identified in deaths data")

```
  
The overplotting issue is addressed. There are still issues of how much lift should be required to declare a peak, especially as a time series becomes long. The function is run for all the burden variables:  
```{r, fig.height=9, fig.width=9}

dfPeaks <- cdc_daily_221202$dfPerCapita %>%
    mutate(regn=c(as.character(state.region), "South")[match(state, c(state.abb, "DC"))]) %>%
    updatedMakePeakValley(numVar=c("new_deaths", "new_cases", "inp"), 
                          windowWidth = 71, 
                          rollMean=7, 
                          facetVar=c("regn"), 
                          fnNumVar=list("new_deaths"=function(x) x, 
                                        "new_cases"=function(x) x/1000,
                                        "inp"=function(x) x/1000
                                        ), 
                          fnPeak=list("new_deaths"=function(x) x+100, 
                                      "new_cases"=function(x) x+10, 
                                      "inp"=function(x) x+10
                                      ),
                          fnValley=list("new_deaths"=function(x) x-100, 
                                        "new_cases"=function(x) x-5, 
                                        "inp"=function(x) x-5
                                        ),
                          useTitle=c("new_deaths"="US coronavirus deaths", 
                                     "new_cases"="US coronavirus cases", 
                                     "inp"="US coronavirus total hospitalized"
                                     ), 
                          yLab=c("new_deaths"=paste0("Rolling ", 7, "-day mean deaths"), 
                                 "new_cases"=paste0("Rolling ", 7, "-day mean cases (000)"), 
                                 "inp"=paste0("Rolling ", 7, "-day mean in hospital (000)")
                                 )
                          )

dfPeaks %>%
    select(date, regn, contains("isPeak")) %>%
    pivot_longer(-c(date, regn)) %>%
    filter(value) %>%
    group_by(regn, name) %>%
    mutate(idx=1:n()) %>%
    ggplot() + 
    geom_text(aes(x=date, y=factor(idx), label=date), size=3) +
    facet_grid(name~regn) + 
    labs(x=NULL, y="Index", "Peaks identified in burden data")

```
  
