---
title: "Data Camp Python Notes"
author: "davegoblue"
date: "May 11, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(engine.path=list(python="C:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python36-32\\python.exe"))
```

## Background and Overview  
DataCamp offer interactive courses related to Python Programming.  Since R Markdown documents can run simple Python code chunks (though the data is not accessible to future chunks, a large difference from R Markdown for R), this document attempts to summarize notes from the first module.
  
## Python Programming  
###_Intro to Python for Data Science_#
  
Chapter 1 - Python Basics  
  
Hello Python! - focusing on Python specific to data science:  
  
* Designed by Guido Van Rossum (started as a hobby), but has become a general purpose language that can build anything  
* Python is open-source, free, and has packages for data science  
* This course will focus on Python 3.x given that support for Python 2.7 has been (and will continue to) decreasing  
	* Available at https://www.python.org/downloads (have downloaded Python 3.6.1 for Windows)  
    * The DataCamp module uses interactive Python (iPython)  
* Python scripts are simply text files with a .py extension - must use print() inside scripts in order to force printing  
  
Variables and Types - variables names are case-sensitive in Python:  
  
* The single equals sign is the assignment operator  
* The type(myVar) call will return the type of the variable - float, integer ("int"), string ("str"), boolean ("bool"), etc.  
	* The booleans are represented as proper-noun capitalization - True and False  
* String summation is concatenation without spacing (roughly the same as paste0() in R) -- "ab" + "cd" = "abcd" ; note also that "ab" * 2 = "abab"  
	* In general, different types of data will respond differently to the same function  
  
Example code includes:  
```{r engine='python'}

# Example, do not modify!
print(5 / 8)

# Put code below here
print(7 + 10)

# Recall that commented lines are marked by the hash-sign, same as R
# Exponentiation is ** and modulo division is %

# Addition and subtraction
print(5 + 5)
print(5 - 5)

# Multiplication and division
print(3 * 5)
print(10 / 2)

# Exponentiation
print(4 ** 2)

# Modulo
print(18 % 7)

# How much is your $100 worth after 7 years?
print(100 * 1.1**7)


# Create a variable savings
savings = 100

# Print out savings
print(savings)


# Create a variable savings
savings = 100

# Create a variable factor
factor = 1.10

# Calculate result
result = savings * factor ** 7

# Print out result
print(result)


# Create a variable desc
desc = "compound interest"

# Create a variable profitable
profitable = True


# Several variables to experiment with
savings = 100
factor = 1.1
desc = "compound interest"

# Assign product of factor and savings to year1
year1 = savings * factor

# Print the type of year1
print(type(year1))

# Assign sum of desc and desc to doubledesc
doubledesc = desc + desc

# Print out doubledesc
print(doubledesc)


# Definition of savings and result
savings = 100
result = 100 * 1.10 ** 7

# Fix the printout
print("I started with $" + str(savings) + " and now have $" + str(result) + ". Awesome!")

# Definition of pi_string
pi_string = "3.1415926"

# Convert pi_string into float: pi_float
pi_float = float(pi_string)


```
  
The output all comes at once, another difference from R Markdown for R.  In combination with being unable to access any of the variables later in the same document, there are tangible limitations to this approach.

Using Python within R Markdown may be more useful if I install "feather" for both Python and R.  Feather allows for running code in Python, then quick-saving pandas in a way that is quick-readable as frames for the next R chunk.  See https://blog.rstudio.org/2016/03/29/feather/.  

Getting feather for R took just a few seconds using install.packages().  Getting feather for Python 3.6 using Windows seems to require a C++ 14.0 compiler from MS Visual Studio.  So far, that is easier said than done.
  
***

Chapter 2 - Lists  
  
What are lists?  Multiple vales in one variable, formed using square brackets such as myList = [a, b, c]:  
  
* The elements of a list may be of any type, including lists  
  
Subsetting lists - the first element in the list is defined as element 0:  
  
* Subsetting can be done as myList[myIndex]  
* Alternately, subsetting can be done using negative numbers, with -1 being the last element of the list  
* List slicing can be run using the colon operator  
	* myList[a:b] will start with index a and end with index b-1  
    * myList[:b] means go from start to index b-1, while myList[a:] means go from a to the end of the list  
  
List manipulation - changing, adding, or removing elements:  
  
* Changing elements is based on using the indices and the equal sign - myList[myIndex] = myNewValue  
* The addition operator will concatenate the various lists  
    * myList + [a, b] will produce a new list [myList, a, b]  
* Deleting elements from a list uses the del() operator - for example, del(myList[2]) will delete the third item of myList which occupies index 2  
* Behind the scense, Python is storing the data and the references to the data  
	* Importantly, this means that copying a list and then editing the copy will edit the original list also; the pointers are to the same underlying data  
    * Basically, myNewList = myList is copying the references to the data that are contained in myList, rather than copying all the data and the references  
    * On the other hand, myNewList = myList[:] or myNewList = list(myList) will make the full, independent copy of the data with new references  
  
Example code includes:  
```{r engine='python'}

# area variables (in square meters)
hall = 11.25
kit = 18.0
liv = 20.0
bed = 10.75
bath = 9.50

# Create list areas
areas = [hall, kit, liv, bed, bath]

# Print areas
print(areas)


# area variables (in square meters)
hall = 11.25
kit = 18.0
liv = 20.0
bed = 10.75
bath = 9.50

# Adapt list areas
areas = ["hallway", hall, "kitchen", kit, "living room", liv, "bedroom", bed, "bathroom", bath]

# Print areas
print(areas)


# area variables (in square meters)
hall = 11.25
kit = 18.0
liv = 20.0
bed = 10.75
bath = 9.50

# house information as list of lists
house = [["hallway", hall],
         ["kitchen", kit],
         ["living room", liv],
         ["bedroom", bed], 
         ["bathroom", bath]
         ]

# Print out house
print(house)

# Print out the type of house
print(type(house))


# Create the areas list
areas = ["hallway", 11.25, "kitchen", 18.0, "living room", 20.0, "bedroom", 10.75, "bathroom", 9.50]

# Print out second element from areas
print(areas[1])

# Print out last element from areas
print(areas[-1])

# Print out the area of the living room
print(areas[5])


# Create the areas list
areas = ["hallway", 11.25, "kitchen", 18.0, "living room", 20.0, "bedroom", 10.75, "bathroom", 9.50]

# Sum of kitchen and bedroom area: eat_sleep_area
eat_sleep_area = areas[3] + areas[7]

# Print the variable eat_sleep_area
print(eat_sleep_area)


# Create the areas list
areas = ["hallway", 11.25, "kitchen", 18.0, "living room", 20.0, "bedroom", 10.75, "bathroom", 9.50]

# Use slicing to create downstairs
downstairs = areas[:6]

# Use slicing to create upstairs
upstairs = areas[6:]

# Print out downstairs and upstairs
print(downstairs)
print(upstairs)


# Create the areas list
areas = ["hallway", 11.25, "kitchen", 18.0, "living room", 20.0, "bedroom", 10.75, "bathroom", 9.50]

# Correct the bathroom area
areas[-1] = 10.5

# Change "living room" to "chill zone"
areas[4] = "chill zone"


# Create the areas list and make some changes
areas = ["hallway", 11.25, "kitchen", 18.0, "chill zone", 20.0,
         "bedroom", 10.75, "bathroom", 10.50]

# Add poolhouse data to areas, new list is areas_1
areas_1 = areas + ["poolhouse", 24.5]

# Add garage data to areas_1, new list is areas_2
areas_2 = areas_1 + ["garage", 15.45]


# Create list areas
areas = [11.25, 18.0, 20.0, 10.75, 9.50]

# Create areas_copy
areas_copy = list(areas)

# Change areas_copy
areas_copy[0] = 5.0

# Print areas
print(areas)


```
  
  
***

Chapter 3 - Functions and Packages  
  
Introduction to functions - pieces of reusable code for solving a particular task:  
  
* Built-in functions are things like max() or type() or round(myNum, myDecimals)  
* Can use help(builtInFunction) to get the help page for builtInFunction  
  
Methods - all objects of a specific type have default access to the methods for that object:  
  
* Methods are functions that belong to an object  
* For example, myList.index("mySearch") will return the index that matches to "mySearch" (if a number, should not be quoted)  
	* Alternately, myList.count("mySearch") will return the number of matches to "mySearch"  
* The methods will behave differently (perhaps even not existing) for different object types  
* Further, some methods modify the object that they are associated with; for example .append()  
  
Packages are directoried of pyhton scripts, each a module specifying functions, methods, and types:  
  
* Thousands of Python packages are available, including Numpy, Matplotlib, and Scikit-learn  
* Installing packages is based on the "pip" system - download get-pip.py from http://pip.readthedocs.org/en/stable/installing  
	* Then, uses "pip3 install myPackage" (unquoted) at the command line  
    * On my machine, needs to be at command line, then [PythonPath]\python.exe -m pip install myPackage  
* Packages can then be imported using "import myPackage" (unquoted) at the command line  
* The package always needs to be attached to its command, for example numpy.array() rather than just array()  
	* As a result, it is often helpful to use import numpy as np, so that np.array() can serve as a shortcut for numpy.array()  
* Alternately, can ask for "from numpy import array" if only wanting to import the function array()  
	* Now, array() can also be called without any prefix; for example, as array(myNumbers) rather than numpy.array(myNumbers)  
  
Example code includes:  
```{r engine='python'}

# Create variables var1 and var2
var1 = [1, 2, 3, 4]
var2 = True

# Print out type of var1
print(type(var1))

# Print out length of var1
print(len(var1))

# Convert var2 to an integer: out2
out2 = int(var2)


# Create lists first and second
first = [11.25, 18.0, 20.0]
second = [10.75, 9.50]

# Paste together first and second: full
full = first + second

# Sort full in descending order: full_sorted
full_sorted = sorted(full, reverse=True)

# Print out full_sorted
print(full_sorted)


# string to experiment with: room
room = "poolhouse"

# Use upper() on room: room_up
room_up = room.upper()

# Print out room and room_up
print(room)
print(room_up)

# Print out the number of o's in room
print(room.count("o"))


# Create list areas
areas = [11.25, 18.0, 20.0, 10.75, 9.50]

# Print out the index of the element 20.0
print(areas.index(20.0))

# Print out how often 14.5 appears in areas
print(areas.count(14.5))


# Create list areas
areas = [11.25, 18.0, 20.0, 10.75, 9.50]

# Use append twice to add poolhouse and garage size
areas.append(24.5)
areas.append(15.45)

# Print out areas
print(areas)

# Reverse the orders of the elements in areas
areas.reverse()

# Print out areas
print(areas)


# Definition of radius
r = 0.43

# Import the math package
import math

# Calculate C
C = 2 * math.pi * r

# Calculate A
A = math.pi * (r ** 2)

# Build printout
print("Circumference: " + str(C))
print("Area: " + str(A))


# Definition of radius
r = 192500

# Import radians function of math package
from math import radians

# Travel distance of Moon over 12 degrees. Store in dist.
dist = r * radians(12)

# Print out dist
print(dist)

```
  
  
***

Chapter 4 - Numpy  
  
Numpy extends list operations using "Numerical Python" (collections of values, optimized for speed):  
  
* The Numpy array is like a list, but you can run mathematical calculations with it  
	* For example, [1, 2, 3] * 2 is [1, 2, 3, 1, 2, 3] while [1, 2, 3] **2 throws an error  
    * However, numpy.array([1, 2, 3]) * 2 is array([2, 4, 6]) while numpy.array([1, 2, 3]) ** 2 is array([1, 4, 9]), both as expected  
* The basic structure of numpy.array() is a vector, which will operate element-wise  
	* Numpy arrays must be of a single-type, converted to the "most flexible" (e.g., string is more flexible than float is more flexible than boolean)  
* The plus sign with a numpy.array() will add element-wise rather than pasting (as it would with lists)  
* Can also use logical subsetting; for example, bmi[bmi > 23] will return all bmi that are greater than 23  
  
2D Numpy Arrays - extending the vector to be multi-dimensional:  
  
* For a numpy vector/array, the type will be numpy.ndarray (stands for n-dimensional array)  
* Can create a two-dimensional array much like an array of lists; numpy.array( [ [1, 2, 3], [4, 5, 6] ] )  
    * The .shape() method will give the dimensions of the array as rows, columns  
* Selecting a row is just based on myArray[rowIndex], so a specific cell can be extracted with myArray[rowIndex][colIndex]  
	* Alternately, myArray[rowIndex, colIndex] will also return the specified row and column  
    * Can also use myArray[:, colIndex] to get just the specified column(s)  
* The 2D Numpy arrays can also be used for element-wise operations
* The 2D Numpy arrays can also be used for element-wise operations  
  
Numpy Basic Statistics - basic data exploration:  
  
* numpy.mean() will take the mean of the relevant data  
* numpy.median() will take the median of the relevant data  
* numpy.corrcoef() will create the correlation coefficients  
* numpy.std() will take the standard deviation  
* numpy.sum() and numpy.sort() are faster than the base versions since numpy has enforced common data types within the array  
* Note that Filip manufactured the MLB data as follows  
    * height = numpy.round(numpy.random.normal(1.75, 0.20, 5000), 2)  
    * weight = numpy.round(numpy.random.normal(60.32, 15, 5000), 2)  
    * np_baseball = np.column_stack((height, weight))  
  
Example code includes:  
```{r engine='python'}

# Create list baseball
baseball = [180, 215, 210, 210, 188, 176, 209, 200]

# Import the numpy package as np
import numpy as np

# Create a Numpy array from baseball: np_baseball
np_baseball = np.array(baseball)

# Print out type of np_baseball
print(type(np_baseball))


# DO NOT HAVE THE HEIGHT OR WEIGHT DATA - it is MLB data on 1000 players
# Create dummy data
height = np.round(np.random.normal(1.75, 0.20, 5000), 2)  
weight = np.round(np.random.normal(60.32, 15, 5000), 2)  


# Create a Numpy array from height: np_height
np_height = np.array(height)

# Print out np_height
print(np_height)

# Convert np_height to m: np_height_m
np_height_m = np_height * 0.0254

# Print np_height_m
print(np_height_m)


# Create array from height with correct units: np_height_m
np_height_m = np.array(height) * 0.0254

# Create array from weight with correct units: np_weight_kg
np_weight_kg = np.array(weight) * 0.453592

# Calculate the BMI: bmi
bmi = np_weight_kg / (np_height_m ** 2)

# Print out bmi
print(bmi)


# Calculate the BMI: bmi
np_height_m = np.array(height) * 0.0254
np_weight_kg = np.array(weight) * 0.453592
bmi = np_weight_kg / np_height_m ** 2

# Create the light array
light = bmi < 21

# Print out light
print(light)

# Print out BMIs of all baseball players whose BMI is below 21
print(bmi[light])


# Store weight and height lists as numpy arrays
np_weight = np.array(weight)
np_height = np.array(height)

# Print out the weight at index 50
print(np_weight[50])

# Print out sub-array of np_height: index 100 up to and including index 110
print(np_height[100:111])


# Create baseball, a list of lists
baseball = [[180, 78.4],
            [215, 102.7],
            [210, 98.5],
            [188, 75.2]]

# Import numpy
import numpy as np

# Create a 2D Numpy array from baseball: np_baseball
np_baseball = np.array(baseball)

# Print out the type of np_baseball
print(type(np_baseball))

# Print out the shape of np_baseball
print(np_baseball.shape)


# DO NOT HAVE baseball, which is a list of lists of the 1015 MLB players with their height/weight
# Create a 2D Numpy array from baseball: np_baseball
# np_baseball = np.array(baseball)
# Dummy up the data instead
np_baseball = np.column_stack((height, weight))  

# Print out the shape of np_baseball
print(np_baseball.shape)  # 1015 x 2


# Create np_baseball (2 cols)
# np_baseball = np.array(baseball)

# Print out the 50th row of np_baseball
print(np_baseball[49])

# Select the entire second column of np_baseball: np_weight
np_weight = np_baseball[:, 1]

# Print out height of 124th player
print(np_baseball[123, 0])


# DO NOT HAVE baseball OR updated ; each should be 1,015 x 3 (height, weight, bmi)
# Create np_baseball (3 cols)
# np_baseball = np.array(baseball)

# Print out addition of np_baseball and updated
# print(np_baseball + updated)

# Create Numpy array: conversion
# conversion = np.array([0.0254, 0.453592, 1])

# Print out product of np_baseball and conversion
# print(np_baseball * conversion)


# Create np_height from np_baseball
np_height = np_baseball[:, 0]

# Print out the mean of np_height
print(np.mean(np_height))

# Print out the median of np_height
print(np.median(np_height))


# Print mean height (first column)
avg = np.mean(np_baseball[:,0])
print("Average: " + str(avg))

# Print median height. Replace 'None'
med = np.median(np_baseball[:,0])
print("Median: " + str(med))

# Print out the standard deviation on height. Replace 'None'
stddev = np.std(np_baseball[:,0])
print("Standard Deviation: " + str(stddev))

# Print out correlation between first and second column. Replace 'None'
corr = np.corrcoef(np_baseball[:, 0], np_baseball[:, 1])
print("Correlation: " + str(corr))


# DO NOT HAVE DATA for positions or heights (soccer data . . . )
# Convert positions and heights to numpy arrays: np_positions, np_heights
# np_positions = np.array(positions)
# np_heights = np.array(heights)

# Heights of the goalkeepers: gk_heights
# gk_heights = np_heights[np_positions == "GK"]

# Heights of the other players: other_heights
# other_heights = np_heights[np_positions != "GK"]

# Print out the median height of goalkeepers. Replace 'None'
# print("Median height of goalkeepers: " + str(np.median(gk_heights)))

# Print out the median height of other players. Replace 'None'
# print("Median height of other players: " + str(np.median(other_heights)))


```
  

###_Intermediate Python for Data Science_#  
  
Chapter 1 - Matplotlib for Data Visualization  
  
Basic plots with matplotlib - generally, the heart of visualization within Python:  
  
* Need to import the key functions; for example import matplotlib.pyplot as plt  
* Then, plt.plot(list1, list2) will create a line plot with list1 being x and list2 being y  
	* If you want to actually see the plot, use plt.show(), somewhat like plt.plot() just being a saved ggplot2 object  
* Alternately, plt.scatter() to create a scatter plot  
  
Histograms are useful for exploring a dataset (getting an idea about the distribution):  
  
* import matplotlib.pyplot as plt  # help(plt.hist) will show all the options for a histogram  
* plt.hist(x, bins=myBins)  # default for myBins is 10  
	* Needs plt.show() as per the above  
  
Customization for changing the base plot types in Python:  
  
* Can label x-axis with plt.xlabel('X Label')  
* Can label y-axis with plt.xlabel('Y Label')  
* Can add title with plt.title('My Title')  
* Can add plt.yticks([myList], [myNames]) # myList can be 2+ elements which will define the y-range; optional list myNames must be the same length as myList and will label the y-axis  
	* All of these must be run PRIOR to the plt.show() command  
  
Example code includes:  
```{r engine="python"}

# Define the reading data path
readPath = "C:/Users/Dave/Documents/Personal/Learning/Coursera/RDirectory/RHomework/DataCamp/"

# This is world population 1950-2100 (DO NOT HAVE FILE)
# Import some wikipedia data from CSV as panda
import pandas as pd

globalPop = pd.read_csv(readPath + "GlobalPopYear_1950_2100_v001.csv")

year = globalPop["year"]
pop = globalPop["pop"]

# Print the last item from year and pop
print(year.iloc[-1])
print(pop.iloc[-1])

# Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Make a line plot: year on the x-axis, pop on the y-axis
plt.plot(year, pop)

# Display the plot with plt.show()
# Need to use a proper Python IDE for plt.show() - otherwise just pops up the images "live"
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy001.png", bbox_inches="tight")

```
  
**The population plot saved from Python is**:  
![](_dummyPy001.png)
  
Next, the Hans Rosling Data is explored:  
```{r engine = "python"}

# Using the Hans Rosling Data (2007 life expectancy and GDP for 142 countries)
# Create from Wikipedia, World Bank, and the like
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# readPath = "C:\\Users\\Dave\\Documents\\Personal\\Learning\\Coursera\\RDirectory\\RHomework\\DataCamp\\"
readPath = "C:/Users/Dave/Documents/Personal/Learning/Coursera/RDirectory/RHomework/DataCamp/"


globalData = pd.read_csv(readPath + "GlobalGDPLifeExpectancy_v001.csv")

gdp_cap = 1000000 * np.array(globalData["gdp"]) / np.array(globalData["pop"])
life_exp = globalData["le_2015"]
pop = globalData["pop"]
life_exp1950 = globalData["le_1960"]  # Much easier to get 1960 than 1950 online - KLUGE
regn = globalData["region"]

# Print the last item of gdp_cap and life_exp
print(gdp_cap[-1])  # Since it is a numpy
print(life_exp.iloc[-1])  # Since it is a panda

# Make a line plot, gdp_cap on the x-axis, life_exp on the y-axis
plt.plot(gdp_cap, life_exp)

# Display the plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy002.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Change the line plot below to a scatter plot
plt.scatter(gdp_cap, life_exp)

# Put the x-axis on a logarithmic scale
plt.xscale('log')

# Show plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy003.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Brings in yet another variable, population

# Build Scatter plot
plt.scatter(pop, life_exp)
plt.xscale("log")

# Show plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy004.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Create histogram of life_exp data
plt.hist(life_exp)

# Display histogram
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy005.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Build histogram with 5 bins
plt.hist(life_exp, bins=5)

# Show and clean up plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# plt.clf()
# Save as dummy PNG instead
plt.savefig("_dummyPy006.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Build histogram with 20 bins
plt.hist(life_exp, bins=20)

# Show and clean up again
# Need to use a proper Python IDE for plt.show()
# plt.show()
# plt.clf()
# Save as dummy PNG instead
plt.savefig("_dummyPy007.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Histogram of life_exp, 15 bins
plt.hist(life_exp, bins=15)

# Show and clear plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# plt.clf()
# Save as dummy PNG instead
plt.savefig("_dummyPy008.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Histogram of life_exp1950, 15 bins
plt.hist(life_exp1950, bins=15)

# Show and clear plot again
# Need to use a proper Python IDE for plt.show()
# plt.show()
# plt.clf()
# Save as dummy PNG instead
plt.savefig("_dummyPy009.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Basic scatter plot, log scale
plt.scatter(gdp_cap, life_exp)
plt.xscale('log') 

# Strings
xlab = 'GDP per Capita [in USD]'
ylab = 'Life Expectancy [in years]'
title = 'World Development in 2007'

# Add axis labels
plt.xlabel(xlab)
plt.ylabel(ylab)

# Add title
plt.title(title)

# After customizing, display the plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy010.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Scatter plot
plt.scatter(gdp_cap, life_exp)

# Previous customizations
plt.xscale('log') 
plt.xlabel('GDP per Capita [in USD]')
plt.ylabel('Life Expectancy [in years]')
plt.title('World Development in 2007')

# Definition of tick_val and tick_lab
tick_val = [1000,10000,100000]
tick_lab = ['1k','10k','100k']

# Adapt the ticks on the x-axis
plt.xticks(tick_val, tick_lab)

# After customizing, display the plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy011.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Import numpy as np
import numpy as np

# Store pop as a numpy array: np_pop
np_pop = np.array(pop) / 1000000  # Population in millions

# Double np_pop
np_pop = np_pop * 2 # Doubled for larger bubbles

# Update: set s argument to np_pop
plt.scatter(gdp_cap, life_exp, s = np_pop)

# Previous customizations
plt.xscale('log') 
plt.xlabel('GDP per Capita [in USD]')
plt.ylabel('Life Expectancy [in years]')
plt.title('World Development in 2007')
plt.xticks([1000, 10000, 100000],['1k', '10k', '100k'])

# Display the plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy012.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Color is based on continent, using the below dictionary
colDict = {
    'Asia':'red',
    'Europe':'green',
    'Africa':'blue',
    'Americas':'yellow',
    'Oceania':'black'
}

col=[]

for eachRegion in regn :
    col.append(colDict[eachRegion])

# Specify c and alpha inside plt.scatter()
plt.scatter(x = gdp_cap, y = life_exp, s = np_pop , c=col, alpha=0.8)

# Previous customizations
plt.xscale('log') 
plt.xlabel('GDP per Capita [in USD]')
plt.ylabel('Life Expectancy [in years]')
plt.title('World Development in 2007')
plt.xticks([1000,10000,100000], ['1k','10k','100k'])

# Show the plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# plt.clf()
# Save as dummy PNG instead
plt.savefig("_dummyPy013.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Scatter plot
plt.scatter(x = gdp_cap, y = life_exp, s = np_pop, c = col, alpha = 0.4)

# Previous customizations
plt.xscale('log') 
plt.xlabel('GDP per Capita [in USD]')
plt.ylabel('Life Expectancy [in years]')
plt.title('World Development in 2007')
plt.xticks([1000,10000,100000], ['1k','10k','100k'])

# Additional customizations
plt.text(1550, 71, 'India')
plt.text(5700, 80, 'China')

# Add grid() call
plt.grid(True)

# Show the plot
# Need to use a proper Python IDE for plt.show()
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy014.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

```
  
**GDP vs Life Expectancy by Country as Line Graph (not good . . . )**:  
![](_dummyPy002.png)
  
**GDP vs Life Expectancy by Country as Scatter Plot**:  
![](_dummyPy003.png)

**GDP vs Life Expectancy by Country as Scatter Plot with Log Scale**:  
![](_dummyPy004.png)

**Life Expectancy Histogram (default 10 bins)**:  
![](_dummyPy005.png)

**Life Expectancy Histogram (5 bins)**:  
![](_dummyPy006.png)

**Life Expectancy Histogram (20 bins)**:  
![](_dummyPy007.png)

**Life Expectancy Histogram for 2015 (15 bins)**:  
![](_dummyPy008.png)

**Life Expectancy Histogram for 1960 (15 bins)**:  
![](_dummyPy009.png)

**Base Rosling-like graph (GDP vs Life Expectancy by Country Scatter)**:  
![](_dummyPy010.png)

**Rosling-like graph (enhanced tick labels)**:  
![](_dummyPy011.png)

**Rosling-like graph (bubble size ~ population)**:  
![](_dummyPy012.png)

**Rosling-like graph (bubble color based on region)**:  
![](_dummyPy013.png)
  
**Rosling-like graph (semit-transparent bubbles)**:  
![](_dummyPy014.png)
  
  
***
  
Chapter 2 - Dictionaries and Pandas  
  
Dictionaries, Part I - key-value pairs:  
  
* The dictionary is created with curly brackets, with key-value pairs denoted by a colon and separated by a comma  
	* world = { "afghanistan":31, "albania":2.8, "algeria":39 }  # sets up three key-value pairs as the dictionary called world  
    * Now, world["albania"] will return 2.8, the value that is associated with key "albania"  
* Dictionary look-ups are extremely fast even for enormous dictionaries  
  
Dictionaries, Part II:  
  
* Dictionaries need to have unique keys; if duplicate keys are included, the value associated with the LAST key is retained  
* The keys also need to be immutable objects, which is to say strings or booleans or integers or floats (but not lists, since you can change their contents dynamically)  
* Assigning (or changing) key-value pairs in a dictionary is myDict[myKey] = myValue  
* To test whether a key is in the dictionary, use myKey in myDict # returns boolean True or False  
* To delete an item from the dictionary, use del(myDict[myKey]) # the full key-value pair is removed  
* Lists and dictionaries have many similarities, but also some key differences  
	* Lists are indexed by a range of numbers, making them ideal for collections of values where the order matters  
    * Dictionaries are indexed by unique keys, making them ideal for fast look-ups (they are also inherently completely unordered/unsorted based on how they are hashed)  
  
Pandas, Part I - tabular dataset storage and manipulation:  
  
* Same general philosophy where rows are observations and columns are attributes/variables  
* Basically, need a form of numpy.array() that allows for different variable types in different columns  
* The pandas package provides a high-level data-manipulation tool (built on numpy by Wes McKinney)  
	* The pandas package conveniently stores data as a DataFrame  
    * Generally, the rows and columns will all have unique names  
    * Further, the columns can all be of different types  
* Suppose that you create a dictionary where the keys are the desired column labels while the values are a list of the desired values for the column  
	* import pandas as pd  
    * myFrame = pd.DataFrame(myDict)  
    * myFrame.index = labelList # optional, if wanting to provide row-names  
* Alternately, the data can be imported such as from a CSV  
	* pd.read_csv(myCSVPath, index_col=myIndex)  # index_col is optional and needed only if an index column has been provided  
  
Pandas, Part II - indexing and selecting data from a DataFrame using square brackets, loc, and iloc:  
  
* myFrame[colNameQuoted] will return a subset of the panda with type pandas.core.series.Series  
* myFrame[[colNameQuoted]] will return a single-column panda with type pandas.core.frame.DataFrame  
* myFrame[[colName1Quoted, colName2Quoted]] will return a two-column panda  
* myFrame[a:b] will return rows rather than columns, starting with index a and ending at index b-1  
* The loc and iloc tools are designed to extend Pandas data extraction to be more similar to numpy extractions such as [ rows, columns ]  
	* myFrame.loc[rowNameQuoted] will return a panda series matching the ROW  
    * myFrame.loc[[rowNameQuoted]] will return a panda frame containing just that ROW  
    * myFrame.loc[[rowName1Quoted, rowName2Quoted, rowName3Quoted]] will return a panda frame containing the requested ROWS  
    * myFrame.loc[[rowListQuoted], [colListQuoted]] will return just the specified rows and columns  
    * myFrame.loc[:, [colListQuoted]] will return all rows and just the specified columns  
* The iloc function is the index-based version of loc for data access and extraction  
	* myFrame.iloc[[rowIndices]] will return a panda frame containing just these ROWS  
    * myFrame.iloc[[rowIndices], [colIndices]] will return a panda frame containing just these COLUMNS  
  
Example code includes:  
```{r engine="python"}

# Definition of countries and capital
countries = ['spain', 'france', 'germany', 'norway']
capitals = ['madrid', 'paris', 'berlin', 'oslo']

# Get index of 'germany': ind_ger
ind_ger = countries.index("germany")

# Use ind_ger to print out capital of Germany
print(capitals[ind_ger])


# Definition of countries and capital
countries = ['spain', 'france', 'germany', 'norway']
capitals = ['madrid', 'paris', 'berlin', 'oslo']

# From string in countries and capitals, create dictionary europe
europe = {
   'spain':'madrid', 
   'france':'paris', 
   'germany':'berlin', 
   'norway':'oslo'
}

# Print europe
print(europe)


# Definition of dictionary
europe = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }

# Print out the keys in europe
print(europe.keys())

# Print out value that belongs to key 'norway'
print(europe['norway'])


# Definition of dictionary
europe = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }

# Add italy to europe
europe['italy'] = 'rome'

# Print out italy in europe
print('italy' in europe)

# Add poland to europe
europe['poland'] = 'warsaw'

# Print europe
print(europe)


# Definition of dictionary
europe = {'spain':'madrid', 'france':'paris', 'germany':'bonn',
          'norway':'oslo', 'italy':'rome', 'poland':'warsaw',
          'australia':'vienna' }

# Update capital of germany
europe['germany'] = 'berlin'

# Remove australia
del(europe['australia'])

# Print europe
print(europe)


# Dictionary of dictionaries
europe = { 'spain': { 'capital':'madrid', 'population':46.77 },
           'france': { 'capital':'paris', 'population':66.03 },
           'germany': { 'capital':'berlin', 'population':80.62 },
           'norway': { 'capital':'oslo', 'population':5.084 } }


# Print out the capital of France
print(europe['france']['capital'])

# Create sub-dictionary data
data = { 'capital':'rome', 'population':59.83 }

# Add data to europe under key 'italy'
europe['italy'] = data

# Print europe
print(europe)


# Pre-defined lists
names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']
dr =  [True, False, False, False, True, True, True]
cpc = [809, 731, 588, 18, 200, 70, 45]

# Import pandas as pd
import pandas as pd

# Create dictionary my_dict with three key:value pairs: my_dict
my_dict = { 'country': names, 'drives_right': dr, 'cars_per_cap': cpc }

# Build a DataFrame cars from my_dict: cars
cars = pd.DataFrame(my_dict)

# Print cars
print(cars)


# Build cars DataFrame
names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']
dr =  [True, False, False, False, True, True, True]
cpc = [809, 731, 588, 18, 200, 70, 45]
dict = { 'country':names, 'drives_right':dr, 'cars_per_cap':cpc }
cars = pd.DataFrame(dict)
print(cars)

# Definition of row_labels
row_labels = ['US', 'AUS', 'JAP', 'IN', 'RU', 'MOR', 'EG']

# Specify row labels of cars
cars.index = row_labels

# Print cars again
print(cars)


# DO NOT HAVE FILE "cars.csv" - cars_per_cap , country , drives_right
# Created as cars.to_csv("cars.csv")
# Import the cars.csv data: cars
cars = pd.read_csv("cars.csv")

# Print out cars
print(cars)


# SLIGHTLY DIFFERENT VERSION WITH ROW NAMES AS THE FIRST COLUMN
# Import pandas as pd
import pandas as pd

# Fix import by including index_col
cars = pd.read_csv('cars.csv', index_col=0)

# Print out cars
print(cars)


# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Print out country column as Pandas Series
print(cars["country"])

# Print out country column as Pandas DataFrame
print(cars[["country"]])

# Print out DataFrame with country and drives_right columns
print(cars[["country", "drives_right"]])


# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Print out first 3 observations
print(cars[0:3])

# Print out fourth, fifth and sixth observation
print(cars[3:6])


# Print out observation for Japan
print(cars.loc["JAP"])

# Print out observations for Australia and Egypt
print(cars.loc[["AUS", "EG"]])


# Print out drives_right value of Morocco
print(cars.loc[["MOR"], ["drives_right"]])

# Print sub-DataFrame
print(cars.loc[["RU", "MOR"], ["country", "drives_right"]])


# Print out drives_right column as Series
print(cars.loc[:, "drives_right"])

# Print out drives_right column as DataFrame
print(cars.loc[:, ["drives_right"]])

# Print out cars_per_cap and drives_right as DataFrame
print(cars.loc[:, ["cars_per_cap", "drives_right"]])


```
  
  
***

Chapter 3 - Logic, Control Flow, and Filtering  
  
Comparison Operators - how two values relate (tests for equality, greater, lesser, etc.):  
  
* Less than (<), greater than (>), equals (==), less than or equal (<=), greater than or equal (>=), and not equals (!=) are as per R  
* Need to have comparisons between objects of the same type (specifically, not comparing strings and floats)  
  
Boolean operators - most commonly used are and, or, and not:  
  
* In Python, the word "and" is used rather than & or &&  
* In Python, the word "or" is used rather than | or ||  
* In Python, the word "not" is used rather than - or !  
* If comparisons will be run on an array, then use np.logical_and(), np.logical_or(), and np.logical_not()  
    * np.logical_and(bmi > 27, bmi < 30)  
  
If, elif, else:  
  
* General syntax is "if condition : action" followed optionally by "elif condition : action" or "else condition : action"
	* If written on multiple lines, the action should be indented by 4 spaces and may include block instructions  
    * Any code without the indentation will be known to no longer be part of the if block  
  
Filtering Pandas DataFrame - generally a three-step process of 1) select key column as panda.series, 2) run test, and 3) use to grab relevant rows:  
  
* If you pass myFrame[myBool] where myBool is the same size (number of rows) as myFrame, then it will automatically pull back the rows where myBool == True  
* Because pandas are built on the numpy infrastructure, np.logical_and() and the related terms will work on the pandas also  
  
Example code includes:  
```{r engine='python'}

# Comparison of booleans
print(True == False)

# Comparison of integers
print((-5 * 15) != 75)

# Comparison of strings
print("pyscript" == "PyScript")

# Compare a boolean with an integer
print(True == 1)


# Comparison of integers
x = -3 * 6
print(x >= -10)

# Comparison of strings
y = "test"
print("test" <= y)

# Comparison of booleans
print(True > False)


# Create arrays
import numpy as np
my_house = np.array([18.0, 20.0, 10.75, 9.50])
your_house = np.array([14.0, 24.0, 14.25, 9.0])

# my_house greater than or equal to 18
print(my_house >= 18)

# my_house less than your_house
print(my_house < your_house)


# Define variables
my_kitchen = 18.0
your_kitchen = 14.0

# my_kitchen bigger than 10 and smaller than 18?
print(my_kitchen > 10 and my_kitchen < 18)

# my_kitchen smaller than 14 or bigger than 17?
print(my_kitchen < 14 or my_kitchen > 17)

# Double my_kitchen smaller than triple your_kitchen?
print(2 * my_kitchen < 3 * your_kitchen)


# Create arrays
import numpy as np
my_house = np.array([18.0, 20.0, 10.75, 9.50])
your_house = np.array([14.0, 24.0, 14.25, 9.0])

# my_house greater than 18.5 or smaller than 10
print(np.logical_or(my_house > 18.5, my_house < 10))

# Both my_house and your_house smaller than 11
print(np.logical_and(my_house <11, your_house < 11))


# Define variables
room = "kit"
area = 14.0

# if statement for room
if room == "kit" :
    print("looking around in the kitchen.")

# if statement for area
if area > 15 :
    print("big place!")


# Define variables
room = "kit"
area = 14.0

# if-else construct for room
if room == "kit" :
    print("looking around in the kitchen.")
else :
    print("looking around elsewhere.")

# if-else construct for area
if area > 15 :
    print("big place!")
else :
    print("pretty small.")


# Define variables
room = "bed"
area = 14.0

# if-elif-else construct for room
if room == "kit" :
    print("looking around in the kitchen.")
elif room == "bed":
    print("looking around in the bedroom.")
else :
    print("looking around elsewhere.")

# if-elif-else construct for area
if area > 15 :
    print("big place!")
elif area > 10 :
    print("medium size, nice!")
else :
    print("pretty small.")


# AS PER ABOVE, DO NOT HAVE THIS DATASET
# That has since been worked around . . . 
# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Extract drives_right column as Series: dr
dr = cars["drives_right"]

# Use dr to subset cars: sel
sel = cars[dr]

# Print sel
print(sel)


# Convert code to a one-liner
sel = cars[cars['drives_right']]

# Print sel
print(sel)


# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Create car_maniac: observations that have a cars_per_cap over 500
cpc = cars["cars_per_cap"]
many_cars = cpc > 500
car_maniac = cars[many_cars]

# Print car_maniac
print(car_maniac)


# Create medium: observations with cars_per_cap between 100 and 500
cpc = cars['cars_per_cap']
between = np.logical_and(cpc > 100, cpc < 500)
medium = cars[between]

# Print medium
print(medium)


```
  
  
***

Chapter 4 - Loops  
  
The while loop - alternative to the if/elif/else process:  
  
* The while loop will continue to execute as long as the condition is met  
* These loops are typically rare (and can easily cause an infinite loop), but can be powerful in certain circumstances  
* The syntax is while condition : expression # If expression is placed on the next line(s), then it should be indented by 4 spaces  
* CTRL-C will typically kill a Python infinte loop  
  
The for loop - alternative to the while loop:  
  
* The basic syntax is for var in seq : expression # as per previous, if expression is on the next line(s), it should be indented by 4 spaces  
	* The seq can be a list or a dictionary or the like, which will iterate by item in the list or disctionary or the like  
* Using a list, the enumerate() command will pull out a tuple which can be used in the iterations  
	for a, b in enumerate(myList) : expression # a will be the index and b will be the value  
* If iterating over a string, the for loop will extract character by character  
  
Looping data structures - Part I - extension to dictionaries, numpy arrays, and the like:  
  
* Looping through a dictionary requires calling the .items() method on the dictionary  
	* for key, value in myDict.items() : expression # will extract key, value as tuples  
    * Since dictionaries are unordered, the key will not be sorted and can come out in any order (hash-table dependent)  
* Looping through a 1-D numpy array will work the same as looping through a list; standard for-loop syntax  
* Looping through a 2-D numpy array will extract the 1-D numpy arrays underlying the 2-D numpy array (which may or may not be the desired output)  
	* Alternately, using np.nditer(myNumpy2D) will extract the items one at a time
  
Looping data structures - Part II - extension to pandas DataFrame:  
  
* The basic expression for x in myPanda : expression # the x will just iterate across the column names  
* To extract the rows, use for lab, row in myPanda.iterrows() : expression # the lab will be the row name and the row will be the row data, iterated over all the rows  
* Rather than using a loop, the apply function can be used to create new columns in the panda  
	* myPanda["myNewCol"] = myPanda["myOldCol"].apply(len)  # will create new variable myNewCol as len(myOldCol)  
  
Example code includes:  
```{r engine='python'}

# Initialize offset
offset = 8

# Code the while loop
while offset != 0 :
    print("correcting...")
    offset = offset - 1
    print(offset)


# Initialize offset
offset = -6

# Code the while loop
while offset != 0 :
    print("correcting...")
    if offset > 0 :
        offset = offset - 1
    else :
        offset = offset + 1
    print(offset)


# areas list
areas = [11.25, 18.0, 20.0, 10.75, 9.50]

# Code the for loop
for x in areas :
    print(x)


# areas list
areas = [11.25, 18.0, 20.0, 10.75, 9.50]

# Change for loop to use enumerate()
for a, b in enumerate(areas) :
    print("room " + str(a) + ": " + str(b))


# areas list
areas = [11.25, 18.0, 20.0, 10.75, 9.50]

# Code the for loop
for index, area in enumerate(areas) :
    print("room " + str(index + 1) + ": " + str(area))


# house list of lists
house = [["hallway", 11.25], 
         ["kitchen", 18.0], 
         ["living room", 20.0], 
         ["bedroom", 10.75], 
         ["bathroom", 9.50]]
         
# Build a for loop from scratch
for rooms in house :
    print("the " + str(rooms[0]) + " is " + str(rooms[1]) + " sqm")


# Definition of dictionary
europe = {'spain':'madrid', 'france':'paris', 'germany':'bonn', 
          'norway':'oslo', 'italy':'rome', 'poland':'warsaw', 'australia':'vienna' }
          
# Iterate over europe
for country, capital in europe.items() :
    print("the capital of " + str(country) + " is " + str(capital))


# Import numpy as np
import numpy as np

# DO NOT HAVE EITHER DATASET
# Create np_height
height = np.round(np.random.normal(1.75, 0.20, 50), 2)  
np_height = np.array(height)

# Create np_baseball
# baseball = [180, 215, 210, 210, 188, 176, 209, 200]
# np_baseball = np.array(baseball)

weight = np.round(np.random.normal(60.32, 15, 50), 2)
np_baseball = np.column_stack((height, weight))


# For loop over np_height
for height in np_height :
    print(str(height) + " inches")

# The end= argument over-rides the default to move to a new line
# For loop over np_baseball
for item in np.nditer(np_baseball) :
    print(item, end=" ")


# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Iterate over rows of cars
for lab, dat in cars.iterrows() :
    print(lab)
    print(dat)


# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Adapt for loop
for lab, row in cars.iterrows() :
    print(lab + ": " + str(row['cars_per_cap']))


# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Code for loop that adds COUNTRY column
for lab, row in cars.iterrows() :
    cars.loc[lab, "COUNTRY"] = row['country'].upper()

# Print cars
print(cars)


# Import cars data
import pandas as pd
cars = pd.read_csv('cars.csv', index_col = 0)

# Use .apply(str.upper)
cars["COUNTRY"] = cars["country"].apply(str.upper)
print(cars)


```
  
  
***
  
Chapter 5 - Case Study: Hacker Statistics  
  
Random numbers - random walk using a 6-sided dice where 1/2 means -1, 3/4/5 means +1, and 6 means roll again and go up the number of the next roll:  
  
* Further, set a floor of step 0 and also add a 0.1% chance of falling down the stairs (presumably reverting to floor 0) at any given move  
* Assume that this is a 100-move game, and assess the odds of ending at floor 60+  
* Hacker statistics is simulating the game to assess the probabilities, as opposed to solving the game analytically  
* The np.random.rand() function will return a random number between 0 and 1  
	* Calling np.random.seed(mySeed) will set the seed for the upcoming trials, useful for reproducibility  
    * Calling np.random.int(a, b) will generate random integers between a and b-1 (both inclusive) assuming equal probabilities  
  
Random walk - well-known pattern in science:  
  
* Initializing an empty list can be done with the square brackets; myEmptyList = []  
	* Appending items to the list can be done with myEmptyList.append(myEntry)  
* For the random walk, start at 0 making myList = [0]  
	* Then, can run for x in range(runs) : myList.append(myList[x] + myRandom)  
    * Note that range(x) will generate an integer list from 0 to x-1  
  
Distribution of random walks - expanding on the 100-trial random walk:  
  
* Simulating many times allows for building a distribution and then making calculations based on that distribution  
  
Example code includes:  
```{r engine='python'}

# Import numpy as np
import numpy as np

# Set the seed
np.random.seed(123)

# Generate and print random float
print(np.random.rand())


# Import numpy and set seed
import numpy as np
np.random.seed(123)

# Use randint() to simulate a dice
print(np.random.randint(1, 7))

# Use randint() again
print(np.random.randint(1, 7))


# Import numpy and set seed
import numpy as np
np.random.seed(123)

# Starting step
step = 50

# Roll the dice
dice = np.random.randint(1, 7)

# Finish the control construct
if dice <= 2 :
    step = step - 1
elif dice < 6 :
    step = step + 1
else :
    step = step + np.random.randint(1,7)

# Print out dice and step
print(dice)
print(step)


# Import numpy and set seed
import numpy as np
np.random.seed(123)

# Initialize random_walk
random_walk = [0]

# Complete the ___
for x in range(100) :
    # Set step: last element in random_walk
    step = random_walk[-1]

    # Roll the dice
    dice = np.random.randint(1,7)

    # Determine next step
    if dice <= 2:
        step = step - 1
    elif dice <= 5:
        step = step + 1
    else:
        step = step + np.random.randint(1,7)

    # append next_step to random_walk
    random_walk.append(step)

# Print random_walk
print(random_walk)


# Import numpy and set seed
import numpy as np
np.random.seed(123)

# Initialize random_walk
random_walk = [0]

for x in range(100) :
    step = random_walk[-1]
    dice = np.random.randint(1,7)

    if dice <= 2:
        # Replace below: use max to make sure step can't go below 0
        step = max(0, step - 1)
    elif dice <= 5:
        step = step + 1
    else:
        step = step + np.random.randint(1,7)

    random_walk.append(step)

print(random_walk)


# Initialization
import numpy as np
np.random.seed(123)
random_walk = [0]

for x in range(100) :
    step = random_walk[-1]
    dice = np.random.randint(1,7)

    if dice <= 2:
        step = max(0, step - 1)
    elif dice <= 5:
        step = step + 1
    else:
        step = step + np.random.randint(1,7)

    random_walk.append(step)

# Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Plot random_walk
plt.plot(random_walk)

# Show the plot
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy015.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting


# Initialization
import numpy as np
np.random.seed(123)

# Initialize all_walks
all_walks = []

# Simulate random walk 10 times
for i in range(10) :

    # Code from before
    random_walk = [0]
    for x in range(100) :
        step = random_walk[-1]
        dice = np.random.randint(1,7)

        if dice <= 2:
            step = max(0, step - 1)
        elif dice <= 5:
            step = step + 1
        else:
            step = step + np.random.randint(1,7)
        random_walk.append(step)

    # Append random_walk to all_walks
    all_walks.append(random_walk)

# Print all_walks
print(all_walks)


import matplotlib.pyplot as plt
import numpy as np
np.random.seed(123)
all_walks = []
for i in range(10) :
    random_walk = [0]
    for x in range(100) :
        step = random_walk[-1]
        dice = np.random.randint(1,7)
        if dice <= 2:
            step = max(0, step - 1)
        elif dice <= 5:
            step = step + 1
        else:
            step = step + np.random.randint(1,7)
        random_walk.append(step)
    all_walks.append(random_walk)

# Convert all_walks to Numpy array: np_aw
np_aw = np.array(all_walks)

# Plot np_aw and show
plt.plot(np_aw)
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy016.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting


# Transpose np_aw: np_aw_t
np_aw_t = np.transpose(np_aw)

# Plot np_aw_t and show
plt.plot(np_aw_t)
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy017.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting


import matplotlib.pyplot as plt
import numpy as np
np.random.seed(123)
all_walks = []

# Simulate random walk 250 times
for i in range(250) :
    random_walk = [0]
    for x in range(100) :
        step = random_walk[-1]
        dice = np.random.randint(1,7)
        if dice <= 2:
            step = max(0, step - 1)
        elif dice <= 5:
            step = step + 1
        else:
            step = step + np.random.randint(1,7)

        # Implement clumsiness
        if np.random.rand() <= 0.001 :
            step = 0

        random_walk.append(step)
    all_walks.append(random_walk)

# Create and plot np_aw_t
np_aw_t = np.transpose(np.array(all_walks))
plt.plot(np_aw_t)
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy018.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting


import matplotlib.pyplot as plt
import numpy as np
np.random.seed(123)
all_walks = []

# Simulate random walk 500 times
for i in range(500) :
    random_walk = [0]
    for x in range(100) :
        step = random_walk[-1]
        dice = np.random.randint(1,7)
        if dice <= 2:
            step = max(0, step - 1)
        elif dice <= 5:
            step = step + 1
        else:
            step = step + np.random.randint(1,7)
        if np.random.rand() <= 0.001 :
            step = 0
        random_walk.append(step)
    all_walks.append(random_walk)

# Create and plot np_aw_t
np_aw_t = np.transpose(np.array(all_walks))

# Select last row from np_aw_t: ends
ends = np_aw_t[-1]

# Plot histogram of ends, display plot
plt.hist(ends)
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy019.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting


```
  
**Single random walk**:  
![](_dummyPy015.png)

**10 full walks**:  
![](_dummyPy016.png)

**10 full walks transposed**:  
![](_dummyPy017.png)

**250 random walks with "clumsiness"**:  
![](_dummyPy018.png)

**500 random walks with "clumsiness"**:  
![](_dummyPy019.png)

  
  
***
  
###_Python Data Science Toolbox (Part I)_#  
  
Chapter 1 - Writing your own functions  
  
User-defined functions - with/without parameters, and with/without returning values:  
  
* The general syntax is def myFunction(myParams) : commands  # commands can be on additional lines provided that they are indented by 4 spaces  
* Parameters are defined in the function body, while arguments are passed to the function; so def myFunction(myParams) has a parameter while myFunction(myArg) provides an argument  
* If the command return myReturn is included in the function, then myReturn will be passed back to the main body of the code and the function will stop and return  
* Docstrings serve as documentation for a function, and are included in the first line after the function call, surrounded by triple quotes ("""myComment""")  
  
Multiple parameters and return values:  
  
* Functions can accept multiple arguments, such as def myFunction(myParam1, myParam2)  
* Can also return multiple values using tuples (like a list, but immutable and defined using parentheses rather than square brackets)  
* Tuples can be unpacked using just variables separated by commans, so for example:
	* myTuple = (2, 4, 6)  
    * a, b, c = myTuple # a will be 2, b will be 4, c will be 6  
    * myTuple[1] will be 4, so the tuples can be accessed by way of an index  
  
Bringing it all together - practical examples using Twitter data:  
  
* Function header, function body including a docstring, and (optionally) function returns  
  
Example code includes:  
```{r engine='python'}

# Define the function shout
def shout():
    """Print a string with three exclamation marks"""
    # Concatenate the strings: shout_word
    shout_word = "congratulations" + "!!!"
    
    # Print shout_word
    print(shout_word)

# Call shout
shout()


# Define shout with the parameter, word
def shout(word):
    """Print a string with three exclamation marks"""
    # Concatenate the strings: shout_word
    shout_word = word + '!!!'

    # Print shout_word
    print(shout_word)

# Call shout with the string 'congratulations'
shout("congratulations")


# Define shout with the parameter, word
def shout(word):
    """Return a string with three exclamation marks"""
    # Concatenate the strings: shout_word
    shout_word = word + "!!!"

    # Replace print with return
    return(shout_word)

# Pass 'congratulations' to shout: yell
yell = shout("congratulations")

# Print yell
print(yell)


# Define shout with parameters word1 and word2
def shout(word1, word2):
    """Concatenate strings with three exclamation marks"""
    # Concatenate word1 with '!!!': shout1
    shout1 = word1 + "!!!"
    
    # Concatenate word2 with '!!!': shout2
    shout2 = word2 + "!!!"
    
    # Concatenate shout1 with shout2: new_shout
    new_shout = shout1 + shout2

    # Return new_shout
    return new_shout

# Pass 'congratulations' and 'you' to shout(): yell
yell = shout("congratulations", "you")

# Print yell
print(yell)


# Set up the nums tuple for later access
nums = (3, 4, 6)

# Unpack nums into num1, num2, and num3
num1, num2, num3 = nums

# Construct even_nums
even_nums = (2, num2, num3)


# Define shout_all with parameters word1 and word2
def shout_all(word1, word2):
    
    # Concatenate word1 with '!!!': shout1
    shout1 = word1 + "!!!"
    
    # Concatenate word2 with '!!!': shout2
    shout2 = word2 + "!!!"
    
    # Construct a tuple with shout1 and shout2: shout_words
    shout_words = (shout1, shout2)

    # Return shout_words
    return shout_words

# Pass 'congratulations' and 'you' to shout_all(): yell1, yell2
yell1, yell2 = shout_all("congratulations", "you")

# Print yell1 and yell2
print(yell1)
print(yell2)


# Import pandas
import pandas as pd

# DO NOT HAVE THIS CSV; CAN JUST MAKE A COLUMN WITH A SINGLE WORD FOR THE EXAMPLE
# Import Twitter data as DataFrame: df
df = pd.read_csv("tweets.csv")

# Initialize an empty dictionary: langs_count
langs_count = {}

# Extract column from DataFrame: col
col = df['lang']

# Iterate over lang column in DataFrame
for entry in col:

    # If the language is in langs_count, add 1
    if entry in langs_count.keys():
        langs_count[entry] = langs_count[entry] + 1
    # Else add the language to langs_count, set the value to 1
    else:
        langs_count[entry] = 1

# Print the populated dictionary
print(langs_count)


# Define count_entries()
def count_entries(df, col_name):
    """Return a dictionary with counts of 
    occurrences as value for each key."""

    # Initialize an empty dictionary: langs_count
    langs_count = {}
    
    # Extract column from DataFrame: col
    col = df[col_name]
    
    # Iterate over lang column in DataFrame
    for entry in col:

        # If the language is in langs_count, add 1
        if entry in langs_count.keys():
            langs_count[entry] = langs_count[entry] + 1
        # Else add the language to langs_count, set the value to 1
        else:
            langs_count[entry] = 1

    # Return the langs_count dictionary
    return(langs_count)

# NEED TO CREATE tweets_df such that it contains a column 'lang'
# Call count_entries(): result
tweets_df = df
result = count_entries(tweets_df, "lang")

# Print the result
print(result)


```
  

***

Chapter 2 - Default arguments and variable-length arguments  
  
Scope (where are objects or names accessible) and user-defined functions:  
  
* Global scope - defined in the main body of the script  
* Local scope - defined inside a function (once the function ends, the names and objects from the function disappear)  
* Built-in scope - names in the pre-defined built-ins module (e.g., print)  
    * To access the builtins, type "import builtins" followed by "dir(builtins)" - long story as to why  
* Search path for a name/object is local scope, then global scope, then built-in scope  
* Can use the key word "global" within a function to access the global scope, meaning that variable will be edited in the global scope, not in the local scope  
	* def square(value) : global new_val ; new_val = new_val ** 2; return(new_ val)  
    * new_val = 10 ; square(3) # returns 100, but now the global variable new_val is also 100  
  
Nested functions - one function defined inside another function:  
  
* With nested functions, the search is first local, then to the enclosing function, then to the global scope, then to the builtins  
* The inner function can be helpful if the outer function will need to repeat certain actions to achieve its objectives  
* Can return an inner function as the output of a function (example being raising to a user-specified power)  
* There is a computer-science term "closure" that defines exactly how the scopes work during this process  
	* Per DataCamp, "One other pretty cool reason for nesting functions is the idea of a closure. This means that the nested or inner function remembers the state of its enclosing scope when called."  
    * Continuing the DataCamp quote "Thus, anything defined locally in the enclosing scope is available to the inner function even when the outer function has finished execution."  
* The keyword "nonlocal" is available for changing names/values in the enclosing scope (not the global scope; that is keyword "global")  
  
Default and flexible arguments - arguments used when they are not specified, or when a flexible number of arguments can be passed:  
  
* The default arguments are defined using the equal sign, same as R (can be over-ridden if passed by the user, otherwise the default value will be used)  
* Using a parameter *args (anything with a single star) will create a tuple called "args" out of whatever the user-passed (1 or more arguments)  
	* It appears from the example that Python has the += command (as well as -=, *= and /=)  
* Using parameter **kwargs (anything with a double-star) will create a dictionary kwargs with key, value pairs off whatever the user has entered  
  
Bringing it all together - case study on processing a data frame to get word counts, defaulted to column 'lang':  
  
* Objective is to further generalize the process to be able to work on any number (arbitrary, user-specified) of columns in the DataFrame  
  
Example code includes:  
```{r engine='python'}

# Create a string: team
team = "teen titans"

# Define change_team()
def change_team():
    """Change the value of the global variable team."""

    # Use team in global scope
    global team

    # Change the value of team in global: team
    team = "justice league"

# Print team
print(team)

# Call change_team()
change_team()

# Print team
print(team)


# Define three_shouts
def three_shouts(word1, word2, word3):
    """Returns a tuple of strings
    concatenated with '!!!'."""

    # Define inner
    def inner(word):
        """Returns a string concatenated with '!!!'."""
        return word + '!!!'

    # Return a tuple of strings
    return (inner(word1), inner(word2), inner(word3))

# Call three_shouts() and print
print(three_shouts('a', 'b', 'c'))


# Define echo
def echo(n):
    """Return the inner_echo function."""

    # Define inner_echo
    def inner_echo(word1):
        """Concatenate n copies of word1."""
        echo_word = word1 * n
        return echo_word

    # Return inner_echo
    return inner_echo

# Call echo: twice
twice = echo(2)

# Call echo: thrice
thrice = echo(3)

# Call twice() and thrice() then print
print(twice('hello'), thrice('hello'))


# Define echo_shout()
def echo_shout(word):
    """Change the value of a nonlocal variable"""
    
    # Concatenate word with itself: echo_word
    echo_word = word + word
    
    #Print echo_word
    print(echo_word)
    
    # Define inner function shout()
    def shout():
        """Alter a variable in the enclosing scope"""    
        #Use echo_word in nonlocal scope
        nonlocal echo_word
        
        #Change echo_word to echo_word concatenated with '!!!'
        echo_word = echo_word + "!!!"
    
    # Call function shout()
    shout()
    
    #Print echo_word
    print(echo_word)

#Call function echo_shout() with argument 'hello'    
echo_shout("hello")


# Define shout_echo
def shout_echo(word1, echo=1):
    """Concatenate echo copies of word1 and three
     exclamation marks at the end of the string."""

    # Concatenate echo copies of word1 using *: echo_word
    echo_word = word1 * echo

    # Concatenate '!!!' to echo_word: shout_word
    shout_word = echo_word + '!!!'

    # Return shout_word
    return shout_word

# Call shout_echo() with "Hey": no_echo
no_echo = shout_echo("Hey")

# Call shout_echo() with "Hey" and echo=5: with_echo
with_echo = shout_echo("Hey", 5)

# Print no_echo and with_echo
print(no_echo)
print(with_echo)


# Define shout_echo
def shout_echo(word1, echo=1, intense=False):
    """Concatenate echo copies of word1 and three
    exclamation marks at the end of the string."""

    # Concatenate echo copies of word1 using *: echo_word
    echo_word = word1 * echo

    # Capitalize echo_word if intense is True
    if intense is True:
        # Capitalize and concatenate '!!!': echo_word_new
        echo_word_new = echo_word.upper() + '!!!'
    else:
        # Concatenate '!!!' to echo_word: echo_word_new
        echo_word_new = echo_word + '!!!'

    # Return echo_word_new
    return echo_word_new

# Call shout_echo() with "Hey", echo=5 and intense=True: with_big_echo
with_big_echo = shout_echo("Hey", 5, True)

# Call shout_echo() with "Hey" and intense=True: big_no_echo
big_no_echo = shout_echo("Hey", intense=True)

# Print values
print(with_big_echo)
print(big_no_echo)


# Define gibberish
def gibberish(*args):
    """Concatenate strings in *args together."""

    # Initialize an empty string: hodgepodge
    hodgepodge = ""

    # Concatenate the strings in args
    for word in args:
        hodgepodge += word

    # Return hodgepodge
    return(hodgepodge)

# Call gibberish() with one string: one_word
one_word = gibberish("luke")

# Call gibberish() with five strings: many_words
many_words = gibberish("luke", "leia", "han", "obi", "darth")

# Print one_word and many_words
print(one_word)
print(many_words)


# Define report_status
def report_status(**kwargs):
    """Print out the status of a movie character."""

    print("\nBEGIN: REPORT\n")

    # Iterate over the key-value pairs of kwargs
    for key, value in kwargs.items():
        # Print out the keys and values, separated by a colon ':'
        print(key + ": " + value)

    print("\nEND REPORT")

# First call to report_status()
report_status(name="luke", affiliation="jedi", status="missing")

# Second call to report_status()
report_status(name="anakin", affiliation="sith lord", status="deceased")


# DO NOT HAVE file tweets_df (may need to create some dummy data . . . )
import pandas as pd
tweets_df = pd.read_csv("tweets.csv")


# Define count_entries()
def count_entries(df, col_name="lang"):
    """Return a dictionary with counts of
    occurrences as value for each key."""

    # Initialize an empty dictionary: cols_count
    cols_count = {}

    # Extract column from DataFrame: col
    col = df[col_name]
    
    # Iterate over the column in DataFrame
    for entry in col:

        # If entry is in cols_count, add 1
        if entry in cols_count.keys():
            cols_count[entry] += 1

        # Else add the entry to cols_count, set the value to 1
        else:
            cols_count[entry] = 1

    # Return the cols_count dictionary
    return cols_count

# Call count_entries(): result1
result1 = count_entries(tweets_df)

# Call count_entries(): result2
result2 = count_entries(tweets_df, "source")

# Print result1 and result2
print(result1)
print(result2)


# Define count_entries()
def count_entries(df, *args):
    """Return a dictionary with counts of
    occurrences as value for each key."""
    
    #Initialize an empty dictionary: cols_count
    cols_count = {}
    
    # Iterate over column names in args
    for col_name in args:
    
        # Extract column from DataFrame: col
        col = df[col_name]
    
        # Iterate over the column in DataFrame
        for entry in col:
    
            # If entry is in cols_count, add 1
            if entry in cols_count.keys():
                cols_count[entry] += 1
    
            # Else add the entry to cols_count, set the value to 1
            else:
                cols_count[entry] = 1

    # Return the cols_count dictionary
    return cols_count

# Call count_entries(): result1
result1 = count_entries(tweets_df, "lang")

# Call count_entries(): result2
result2 = count_entries(tweets_df, "lang", "source")

# Print result1 and result2
print(result1)
print(result2)

```
  
  
***

Chapter 3 - Lambda functions and error handling  
  
Lambda functions - quicker way to write functions on the fly:  
  
* The general syntax is lambda : expression  
	* raise_to_power = lambda x, y : x ** y  
    * raise_to_power(2, 3) = 8 # runs as 2 ** 3  
* The lambda function is "quick and dirty", so it should be limited to key areas where that is appropriate  
* An example is map(func, seq) which will apply the function over all elements of the sequence  
	* The lambda function can be valuable here, since it allows for a custom function to be applied quickly across a sequence  
    * square_all = map(lambda num: num ** 2, nums)  
    * Need to use print(list(square_all)) since print(square_all) will just define that it is an object at a designated point in memory  
  
Introduction to error handling - functions generally return an error if something is wrong, though that can be trapped/over-ridden:  
  
* Endeavor to provide useful error messages rather than just a trace-back default from Python  
* The typical approach in Python is try-except, where try will try the command and except will run if the try produces an error  
	* try : command to try  # The commands to try are typically on a new line(s) and indented by 4 spaces  
    * except : do otherwise  # The except lines up with the try, while the do otherwise are typically on a new line(s) and indented by 4 spaces  
* Can add types of errors to be trapped, for example "type errors only" using except TypeError :   
* To generate an error (for example, if negative inputs are not desired), can use raise ValueError("quotedMessage")  
    * This will throw a trace-back message, with the quotedMessage appearing at the bottom  
  
Bringing it all together:  
  
* Case study for error handling on the tweets data frame  
  
Example code includes:  
```{r engine='python'}

# Define echo_word as a lambda function: echo_word
echo_word = (lambda word1, echo : word1 * echo)

# Call echo_word: result
result = echo_word("hey", 5)

# Print result
print(result)


# Create a list of strings: spells
spells = ["protego", "accio", "expecto patronum", "legilimens"]

# Use map() to apply a lambda function over spells: shout_spells
shout_spells = map(lambda a : a + "!!!", spells)

# Convert shout_spells to a list: shout_spells_list
shout_spells_list = list(shout_spells)

# Convert shout_spells into a list and print it
print(shout_spells_list)


# Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

# Use filter() to apply a lambda function over fellowship: result
result = filter(lambda a : len(a) > 6, fellowship)

# Convert result to a list: result_list
result_list = list(result)

# Convert result into a list and print it
print(result_list)


# Import reduce from functools
from functools import reduce

# Create a list of strings: stark
stark = ['robb', 'sansa', 'arya', 'eddard', 'jon']

# Use reduce() to apply a lambda function over stark: result
result = reduce(lambda item1, item2 : item1 + item2, stark)

# Print the result
print(result)


# Define shout_echo
def shout_echo(word1, echo=1):
    """Concatenate echo copies of word1 and three
    exclamation marks at the end of the string."""

    # Initialize empty strings: echo_word, shout_words
    echo_word = ""
    shout_words = ""

    # Add exception handling with try-except
    try:
        # Concatenate echo copies of word1 using *: echo_word
        echo_word = word1 * echo

        # Concatenate '!!!' to echo_word: shout_words
        shout_words = echo_word + "!!!"
    except:
        # Print error message
        print("word1 must be a string and echo must be an integer.")

    # Return shout_words
    return shout_words

# Call shout_echo
shout_echo("particle", echo="accelerator")


# Define shout_echo
def shout_echo(word1, echo=1):
    """Concatenate echo copies of word1 and three
    exclamation marks at the end of the string."""

    # Raise an error with raise
    if echo < 0:
        raise ValueError('echo must be greater than 0')

    # Concatenate echo copies of word1 using *: echo_word
    echo_word = word1 * echo

    # Concatenate '!!!' to echo_word: shout_word
    shout_word = echo_word + '!!!'

    # Return shout_word
    return shout_word

# Call shout_echo
shout_echo("particle", echo=5)


# DO NOT HAVE file tweets_df (made "tweets.csv" using R)
import pandas as pd
tweets_df = pd.read_csv("tweets.csv")

# Select retweets from the Twitter DataFrame: result
result = filter(lambda x : x[0:2] == "RT", tweets_df["text"])

# Create list from filter object result: res_list
res_list = list(result)

# Print all retweets in res_list
for tweet in res_list:
    print(tweet)


# Define count_entries()
def count_entries(df, col_name='lang'):
    """Return a dictionary with counts of
    occurrences as value for each key."""

    # Initialize an empty dictionary: cols_count
    cols_count = {}

    # Add try block
    try:
        # Extract column from DataFrame: col
        col = df[col_name]
        
        # Iterate over the column in dataframe
        for entry in col:
    
            # If entry is in cols_count, add 1
            if entry in cols_count.keys():
                cols_count[entry] += 1
            # Else add the entry to cols_count, set the value to 1
            else:
                cols_count[entry] = 1
    
        # Return the cols_count dictionary
        return cols_count

    # Add except block
    except:
        print('The DataFrame does not have a ' + col_name + ' column.')

# DO NOT HAVE file tweets_df
# Call count_entries(): result1
result1 = count_entries(tweets_df, 'lang')

# Print result1
print(result1)

# Call count_entries(): result2
result2 = count_entries(tweets_df, 'lang1')


# Define count_entries()
def count_entries(df, col_name='lang'):
    """Return a dictionary with counts of
    occurrences as value for each key."""
    
    # Raise a ValueError if col_name is NOT in DataFrame
    if col_name not in df.columns:
        raise ValueError('The DataFrame does not have a ' + col_name + ' column.')

    # Initialize an empty dictionary: cols_count
    cols_count = {}
    
    # Extract column from DataFrame: col
    col = df[col_name]
    
    # Iterate over the column in DataFrame
    for entry in col:

        # If entry is in cols_count, add 1
        if entry in cols_count.keys():
            cols_count[entry] += 1
            # Else add the entry to cols_count, set the value to 1
        else:
            cols_count[entry] = 1
        
        # Return the cols_count dictionary
    return cols_count

# Call count_entries(): result1
result1 = count_entries(tweets_df, "lang")

# Print result1
print(result1)


# CAREFUL, THIS ONE IS DESIGNED TO RAISE THE ERROR!
# count_entries(tweets_df, 'lang1')


```


###_Python Data Science Toolbox (Part II)_#  
  
Chapter 1 - Using iterators in PythonLand  
  
Introduction to iterators - for loops and the like:  
  
* For loops can be used for iterating over strings, lists, dictionaries, range() objects, and the like  
* Anything that can be looped over is called an "iterable", and will have an associated iter() method  
* An "iterator" is something that produces the next value with a next() call  
* For loops are using the "iterable" property of objects under-the-hood, with an associated (if silent) "next" call  
	* word = "Da"; it = iter(word); next(it); next(it)  # "D" then "a"  
* The "star" (*) operator will impact all elements of an iterator at once  
	* word = "Data"; it = iter(word); print(*it)  # single-line of "D" "a" "t" "a"  
    * Note that if print(*it) were then called again, there would just be a blank line; there is nothing left to iterate over  
* To unpack a dictionary, use myDict.items()  
* To iterate over a file, use file=open("file.txt"); it = iter(file); print(next(it))  
  
Playing with iterators - enumerate and zip:  
  
* The function enumerate(myIterable) returns an "enumerate" class object with both items and their indices  
	* Running list() on the "enumerate" class object will make a list out of the tuples [(index1, item1), (index2, item2), . . . ]  
    * The default is for index1=0, though the argument start= may be included in the enumerate() call for a different starting index  
* The zip(myIter01, myIter02) will create a "zip" class object of the iterators  
	* list(zip()) will return a list of tuples, starting with (allItem1), (allIterm2), . . . 
    * Seems to require that the iterators all be the same length, else only items until running out of the shortest iterator will be tupled  
  
Using iterators to load large files in to memory - loading data in chunks:  
  
* Common strategy with large files is to read in some data, process it, save the results, discard the data, and then repeat  
* The pandas.read_csv() has an option for chunksize= that allows for reading chunks of any given size  
  
Example code includes:  
```{r engine='python'}

# Create a list of strings: flash
flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']

# Print each list item in flash using a for loop
for person in flash : print(person)

# Create an iterator for flash: superspeed
superspeed = iter(flash)

# Print each item from the iterator
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))
print(next(superspeed))


# Create an iterator for range(3): small_value
small_value = iter(range(3))

# Print the values in small_value
print(next(small_value))
print(next(small_value))
print(next(small_value))

# Loop over range(3) and print the values
for num in range(3) : print(num)


# Create an iterator for range(10 ** 100): googol
googol = iter(range(10 ** 100))

# Print the first 5 values from googol
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))
print(next(googol))


# Create a range object: values
values = range(10, 21)

# Print the range object
print(values)

# Create a list of integers: values_list
values_list = list(values)

# Print values_list
print(values_list)

# Get the sum of values: values_sum
values_sum = sum(values)

# Print values_sum
print(values_sum)


# Create a list of strings: mutants
mutants = ['charles xavier', 
            'bobby drake', 
            'kurt wagner', 
            'max eisenhardt', 
            'kitty pride']

# Create a list of tuples: mutant_list
mutant_list = list(enumerate(mutants))

# Print the list of tuples
print(mutant_list)

# Unpack and print the tuple pairs
for index1, value1 in mutant_list :
    print(index1, value1)

# Change the start index
for index2, value2 in list(enumerate(mutants, start=1)) :
    print(index2, value2)


aliases = ['prof x', 'iceman', 'nightcrawler', 'magneto', 'shadowcat']
powers = ['telepathy', 'thermokinesis', 'teleportation', 'magnetokinesis', 'intangibility' ]

# Create a list of tuples: mutant_data
mutant_data = list(zip(mutants, aliases, powers))

# Print the list of tuples
print(mutant_data)

# Create a zip object using the three lists: mutant_zip
mutant_zip = zip(mutants, aliases, powers)

# Print the zip object
print(mutant_zip)

# Unpack the zip object and print the tuple values
for value1, value2, value3 in mutant_zip :
    print(value1, value2, value3)


# Create a zip object from mutants and powers: z1
z1 = zip(mutants, powers)

# Print the tuples in z1 by unpacking with *
print(*z1)

# Re-create a zip object from mutants and powers: z1
z1 = zip(mutants, powers)

# 'Unzip' the tuples in z1 by unpacking with * and zip(): result1, result2
result1, result2 = zip(*z1)

# Check if unpacked tuples are equivalent to original tuples
print(result1 == tuple(mutants))
print(result2 == tuple(powers))


import pandas as pd

# Initialize an empty dictionary: counts_dict
counts_dict = dict()

# DO NOT HAVE FILE tweets.csv
# Created in R - see above for code
# Iterate over the file chunk by chunk
for chunk in pd.read_csv("tweets.csv", chunksize=10):
    # Iterate over the column in DataFrame
    for entry in chunk['lang']:
        if entry in counts_dict.keys():
            counts_dict[entry] += 1
        else:
            counts_dict[entry] = 1

# Print the populated dictionary
print(counts_dict)


# Define count_entries()
def count_entries(csv_file, c_size, colname):
    """Return a dictionary with counts of
    occurrences as value for each key."""
    
    # Initialize an empty dictionary: counts_dict
    counts_dict = {}

    # Iterate over the file chunk by chunk
    for chunk in pd.read_csv(csv_file, chunksize=c_size):

        # Iterate over the column in DataFrame
        for entry in chunk[colname]:
            if entry in counts_dict.keys():
                counts_dict[entry] += 1
            else:
                counts_dict[entry] = 1

    # Return counts_dict
    return counts_dict

# Call count_entries(): result_counts
result_counts = count_entries("tweets.csv", 10, "lang")

# Print result_counts
print(result_counts)

```
  
  
***
  
Chapter 2 - List comprehensions and generators  
  
List comprehensions help address some of the inefficiencies (coding, run time, etc.) of using for loops for some tasks:  
  
* The syntax is [myDesiredCalcs for myVar in myIter]  # should be square-bracketed  
* List comprehension may be used over any iterable; for example, [num for num in range(11)] will return [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  
* List comprehension can also be valuable in lieu of nested for loops; for example, with [(num1, num2) for num1 in range(0, 2) for num2 in range(6, 8)]  
* There is sometimes a trade-off for readability to keep in mind  
  
Advanced comprehensions - additional functionality available:  
  
* [myDesiredCalcs for myVar in myIter if myCond] # allows for myCond to limit the myVar that are available to myDesiredCalcs (thus limiting the output list)  
* [myDesiredCalcs if myCond else myDefault for myVar in myIter]  # allows for extracting where myCond is met and replacing with myDefault otherwise  
* {myCalc01 : myCalc02 for myVar in myIter} # produces a dictionary with key myCalc01 and value myCalc02  
  
Introduction to generator expressions - creating generator objects rather than list/dictionaries:  
  
* (myDesiredCalcs for myVar in myIter)  # will create a generator object rather than a list  
* Using a generator expression can help significantly with large sequences due to "lazy evaluation" (not evaluated until needed, such as next() being called)  
	* While [num for num in range(10 ** 1000000)] will bomb out of memory, (num for num in range(10 ** 1000000)) is OK!  
* An additional nice feature is that all the conditionals can be run in the generator expression also  
* Note that using "yield" rather than "return" in a def (function) will build a generator function (it will return a generator object when called)  
  
Wrapping up comprehensions and generators - helps with wrangling data:  
  
* Basic form - enclosed in brackets, output will be a list  
* More advanced forms - conditions on the iterator and/or iterable  
* Dictionaries - enclosed in braces  
* Generators - enclosed in parentheses  
  
Example code includes:  
```{r engine='python'}

doctor = ['house', 'cuddy', 'chase', 'thirteen', 'wilson']
[doc[0] for doc in doctor]

# Create list comprehension: squares
squares = [i ** 2 for i in range(0, 10)]


# Create a 5 x 5 matrix using a list of lists: matrix
matrix = [[col for col in range(5)] for row in range(5)]

# Print the matrix
for row in matrix:
    print(row)


# Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

# Create list comprehension: new_fellowship
new_fellowship = [member for member in fellowship if len(member) >= 7]

# Print the new list
print(new_fellowship)


# Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

# Create list comprehension: new_fellowship
new_fellowship = [member if len(member) >= 7 else "" for member in fellowship]

# Print the new list
print(new_fellowship)


# Create a list of strings: fellowship
fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']

# Create dict comprehension: new_fellowship
new_fellowship = {member : len(member) for member in fellowship}

# Print the new list
print(new_fellowship)


# Create generator object: result
result = (num for num in range(16))

# Print the first 5 values
print(next(result))
print(next(result))
print(next(result))
print(next(result))
print(next(result))

# Print the rest of the values
# NOTE - only will print 5-15 since 0-4 have previously been "consumed" above
for value in result:
    print(value)


# Create a list of strings: lannister
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Create a generator object: lengths
lengths = (len(person) for person in lannister)

# Iterate over and print the values in lengths
for value in lengths:
    print(value)


# Create a list of strings
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Define generator function get_lengths
def get_lengths(input_list):
    """Generator function that yields the
    length of the strings in input_list."""
    # Yield the length of a string
    for person in input_list:
        yield len(person)

# Print the values generated by get_lengths()
for value in get_lengths(lannister):
    print(value)


# DO NOT HAVE panda "df"
# Extract the created_at column from df: tweet_time
# tweet_time = df["created_at"]

# Extract the clock time: tweet_clock_time
# tweet_clock_time = [entry[11:19] for entry in tweet_time]

# Print the extracted times
# print(tweet_clock_time)


# Extract the created_at column from df: tweet_time
# tweet_time = df['created_at']

# Extract the clock time: tweet_clock_time
# tweet_clock_time = [entry[11:19] for entry in tweet_time if entry[17:19] == "19"]

# Print the extracted times
# print(tweet_clock_time)

```
  
  
***

Chapter 3 - Bringing it all together (case study)  
  
Welcome to the case study - previous two course techniques:  
  
* Wrangle and extract data from the World Bank Indicators dataset (1960-2015 data on 227 countries)  
* Recall that zip(a, b, . . . ) creates an iterable of tuples conmtaining (a1, b1, …), (a2, b2, …), …  
  
Using Python generators for streaming data:  
  
* Generators are helpful for reading large files - in fact, they work even on files that are being written (as long as the read stays behind the write)  
* Generator functions are written like regular functions, but they have a "yield" (put it in to the generator and keep going) rather than a "return" (return the value and stop)  
* Goal will be to write a generator to read streaming data  
  
Reading files in chunks with pandas.read_csv():  
  
* pandas.read_csv(file, chunksize= ) # allows the file to be read in chunks of size chunksize  
  
Example code includes:  
```{r engine='python'}

row_vals = [ 'Arab World', 'ARB', 'Adolescent fertility rate (births per 1,000 women ages 15-19)', 'SP.ADO.TFRT', '1960', '133.56090740552298' ]

feature_names = [ 'CountryName', 'CountryCode', 'IndicatorName', 'IndicatorCode', 'Year', 'Value' ]

# Zip lists: zipped_lists
zipped_lists = zip(feature_names, row_vals)

# Create a dictionary: rs_dict
rs_dict = dict(zipped_lists)

# Print the dictionary
print(rs_dict)


# Define lists2dict()
def lists2dict(list1, list2):
    """Return a dictionary where list1 provides
    the keys and list2 provides the values."""
    
    # Zip lists: zipped_lists
    zipped_lists = zip(list1, list2)
    
    # Create a dictionary: rs_dict
    rs_dict = dict(zipped_lists)
    
    # Return the dictionary
    return rs_dict

# Call lists2dict: rs_fxn
rs_fxn = lists2dict(feature_names, row_vals)

# Print rs_fxn
print(rs_fxn)


# Create list row_lists
regn = ['Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World', 'Arab World']

abb = ['ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB', 'ARB']

indName = ['Adolescent fertility rate (births per 1,000 women ages 15-19)', 'Age dependency ratio (% of working-age population)', 'Age dependency ratio, old (% of working-age population)', 'Age dependency ratio, young (% of working-age population)', 'Arms exports (SIPRI trend indicator values)', 'Arms imports (SIPRI trend indicator values)', 'Birth rate, crude (per 1,000 people)', 'CO2 emissions (kt)', 'CO2 emissions (metric tons per capita)', 'CO2 emissions from gaseous fuel consumption (% of total)', 'CO2 emissions from liquid fuel consumption (% of total)', 'CO2 emissions from liquid fuel consumption (kt)', 'CO2 emissions from solid fuel consumption (% of total)', 'Death rate, crude (per 1,000 people)', 'Fertility rate, total (births per woman)', 'Fixed telephone subscriptions', 'Fixed telephone subscriptions (per 100 people)', 'Hospital beds (per 1,000 people)', 'International migrant stock (% of population)', 'International migrant stock, total' ]

indCode = ['SP.ADO.TFRT', 'SP.POP.DPND', 'SP.POP.DPND.OL', 'SP.POP.DPND.YG', 'MS.MIL.XPRT.KD', 'MS.MIL.MPRT.KD', 'SP.DYN.CBRT.IN', 'EN.ATM.CO2E.KT', 'EN.ATM.CO2E.PC', 'EN.ATM.CO2E.GF.ZS', 'EN.ATM.CO2E.LF.ZS', 'EN.ATM.CO2E.LF.KT', 'EN.ATM.CO2E.SF.ZS', 'SP.DYN.CDRT.IN', 'SP.DYN.TFRT.IN', 'IT.MLT.MAIN', 'IT.MLT.MAIN.P2', 'SH.MED.BEDS.ZS', 'SM.POP.TOTL.ZS', 'SM.POP.TOTL']

year = ['1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960', '1960']

value = ['133.56090740552298', '87.7976011532547', '6.634579191565161', '81.02332950839141', '3000000.0', '538000000.0', '47.697888095096395', '59563.9892169935', '0.6439635478877049', '5.041291753975099', '84.8514729446567', '49541.707291032304', '4.72698138789597', '19.7544519237187', '6.92402738655897', '406833.0', '0.6167005703199', '1.9296220724398703', '2.9906371279862403', '3324685.0']

row_lists=list(zip(regn, abb, indName, indCode, year, value))

# Print the first two lists in row_lists
print(row_lists[0])
print(row_lists[1])

# Turn list of lists into list of dicts: list_of_dicts
list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]

# Print the first two dictionaries in list_of_dicts
print(list_of_dicts[0])
print(list_of_dicts[1])

# Import the pandas package
import pandas as pd

# Turn list of lists into list of dicts: list_of_dicts
list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]

# Turn list of dicts into a DataFrame: df
df = pd.DataFrame(list_of_dicts)

# Print the head of the DataFrame
print(df.head())

# REFERENCE DATA POSSIBLY AT http://data.worldbank.org/data-catalog/world-development-indicators
# Created relevant file "world_dev_ind.csv" using Python and World Bank download
# Open a connection to the file
with open("world_dev_ind.csv") as file:
    
    # Skip the column names
    file.readline()
    
    # Initialize an empty dictionary: counts_dict
    counts_dict = {}
    
    # Process only the first 1000 rows
    for j in range(1000):
        
        # Split the current line into a list: line
        line = file.readline().split(',')
        
        # Get the value for the first column: first_col
        first_col = line[0]
        
        # If the column value is in the dict, increment its value
        if first_col in counts_dict.keys():
            counts_dict[first_col] += 1
        
        # Else, add to the dict and set value to 1
        else:
            counts_dict[first_col] = 1

# Print the resulting dictionary
print(counts_dict)

# Define read_large_file()
def read_large_file(file_object):
    """A generator function to read a large file lazily."""
    
    # Loop indefinitely until the end of the file
    while True:
        
        # Read a line from the file: data
        data = file_object.readline()
        
        # Break if this is the end of the file
        if not data:
            break
        
        # Yield the line of data
        yield data
        
# Open a connection to the file
with open('world_dev_ind.csv') as file:
    
    # Create a generator object for the file: gen_file
    gen_file = read_large_file(file)
    
    # Print the first three lines of the file
    print(next(gen_file))
    print(next(gen_file))
    print(next(gen_file))


# Initialize an empty dictionary: counts_dict
counts_dict = {}


# Open a connection to the file
with open("world_dev_ind.csv") as file:
    
    # Iterate over the generator from read_large_file()
    for line in read_large_file(file):
        row = line.split(',')
        first_col = row[0]
        
        if first_col in counts_dict.keys():
            counts_dict[first_col] += 1
        else:
            counts_dict[first_col] = 1

# Print            
print(counts_dict)


# DO NOT HAVE FILE ind_pop.csv (CountryName,CountryCode,IndicatorName,IndicatorCode,Year,Value\n)
# Value for regions of CountryName/CountryCode - fixing Urban population (% of total), SP.URB.TOTL.IN.ZS , 1960
# Just changed it to use "world_dev_ind.csv"
# Import the pandas package
import pandas as pd
import matplotlib.pyplot as plt


# Initialize reader object: df_reader
df_reader = pd.read_csv("world_dev_ind.csv", chunksize=10)

# Print two chunks
print(next(df_reader))
print(next(df_reader))


# DO NOT HAVE FILE ind_pop_data.csv 
# ('CountryName,CountryCode,Year,Total Population,Urban population (% of total)\n)
# Appears to be 1960-1964
# Initialize reader object: urb_pop_reader
# Create file using Python, needs to read in using encoding="latin-1"
urb_pop_reader = pd.read_csv("ind_pop_data.csv", chunksize=2500, encoding="latin-1")

# Get the first DataFrame chunk: df_urb_pop
df_urb_pop = next(urb_pop_reader)

# Check out the head of the DataFrame
print(df_urb_pop.head())

# Check out specific country: df_pop_ceb
idxCeb = df_urb_pop[df_urb_pop["CountryCode"] == "CEB"].index
df_pop_ceb = df_urb_pop.loc[idxCeb, :]  # Make sure it is not just a reference . . . 

# Zip DataFrame columns of interest: pops
pops = zip(df_pop_ceb["Total Population"], df_pop_ceb["Urban population (% of total)"])

# Turn zip object into list: pops_list
pops_list = list(pops)

# Print pops_list
print(pops_list)


# Initialize reader object: urb_pop_reader
urb_pop_reader = pd.read_csv("ind_pop_data.csv", chunksize=2500, encoding="latin-1")

# Get the first DataFrame chunk: df_urb_pop
df_urb_pop = next(urb_pop_reader)

# Check out specific country: df_pop_ceb
idxCeb = df_urb_pop[df_urb_pop["CountryCode"] == "CEB"].index
df_pop_ceb = df_urb_pop.loc[idxCeb, :]  # Make sure it is not just a reference . . . 
# df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']

# Zip DataFrame columns of interest: pops
pops = zip(df_pop_ceb['Total Population'], 
            df_pop_ceb['Urban population (% of total)'])

# Turn zip object into list: pops_list
pops_list = list(pops)

# Use list comprehension to create new DataFrame column 'Total Urban Population'
# df_pop_ceb["Total Urban Population"] = df_pop_ceb["Total Population"]
# a = [int(0.01 * tup[0] * tup[1]) for tup in pops_list]
df_pop_ceb['Total Urban Population'] = [int(0.01 * tup[0] * tup[1]) for tup in pops_list]


# Plot urban population data
df_pop_ceb.plot(kind="scatter", x="Year", y="Total Urban Population")
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy020.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting

# Initialize reader object: urb_pop_reader
urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000, encoding="latin-1")

# Initialize empty DataFrame: data
data = pd.DataFrame()

# Iterate over each DataFrame chunk
for df_urb_pop in urb_pop_reader:

    # Check out specific country: df_pop_ceb
    idxCeb = df_urb_pop[df_urb_pop["CountryCode"] == "CEB"].index
    df_pop_ceb = df_urb_pop.loc[idxCeb, :]  # Make sure it is not just a reference . . . 

    # Zip DataFrame columns of interest: pops
    pops = zip(df_pop_ceb['Total Population'],
                df_pop_ceb['Urban population (% of total)'])

    # Turn zip object into list: pops_list
    pops_list = list(pops)

    # Use list comprehension to create new DataFrame column 'Total Urban Population'
    # df_pop_ceb["Total Urban Population"] = df_pop_ceb["Total Population"]
    # a = [int(0.01 * tup[0] * tup[1]) for tup in pops_list]
    df_pop_ceb['Total Urban Population'] = [int(0.01 * tup[0] * tup[1]) for tup in pops_list]
    
    # Append DataFrame chunk to data: data
    data = data.append(df_pop_ceb)

# Plot urban population data
data.plot(kind='scatter', x='Year', y='Total Urban Population')
# plt.show()
# Save as dummy PNG instead
plt.savefig("_dummyPy021.png", bbox_inches="tight")
plt.clf()  # Required to prevent continued over-plotting


# Define plot_pop()
def plot_pop(filename, country_code, pngCode=False):
    
    # Initialize reader object: urb_pop_reader
    urb_pop_reader = pd.read_csv(filename, chunksize=1000, encoding="latin-1")
    
    # Initialize empty DataFrame: data
    data = pd.DataFrame()
    
    # Iterate over each DataFrame chunk
    for df_urb_pop in urb_pop_reader:
        # Check out specific country: df_pop_ceb
        idxCeb = df_urb_pop[df_urb_pop["CountryCode"] == country_code].index
        df_pop_ceb = df_urb_pop.loc[idxCeb, :]  # Make sure it is not just a reference . . . 
        
        # Zip DataFrame columns of interest: pops
        pops = zip(df_pop_ceb['Total Population'],
                    df_pop_ceb['Urban population (% of total)'])
        
        # Turn zip object into list: pops_list
        pops_list = list(pops)
        
        # Use list comprehension to create new DataFrame column 'Total Urban Population'
        # df_pop_ceb["Total Urban Population"] = df_pop_ceb["Total Population"]
        # a = [int(0.01 * tup[0] * tup[1]) for tup in pops_list]
        # df_pop_ceb.loc[df_pop_ceb.index, 'Total Urban Population'] = a
        df_pop_ceb['Total Urban Population'] = [int(0.01 * tup[0] * tup[1]) for tup in pops_list]
        
        # Append DataFrame chunk to data: data
        data = data.append(df_pop_ceb)
        
    # Plot urban population data
    data.plot(kind='scatter', x='Year', y='Total Urban Population')
    if pngCode == False :
        plt.show()  # Plot by default
    else :
        plt.savefig(pngCode, bbox_inches="tight") # Save as dummy PNG instead
    
    plt.clf()  # Required to prevent continued over-plotting

# Set the filename: fn
fn = 'ind_pop_data.csv'

# Call plot_pop for country code 'CEB'
plot_pop(fn, "CEB", "_dummyPy022.png")

# Call plot_pop for country code 'ARB'
plot_pop(fn, "ARB", "_dummyPy023.png")

```
  
Plots 20 and 21 are not displayed as they are redundant with plot 22.


**Urban Population by Year for Country Code CEB**:  
![](_dummyPy022.png)

**Urban Population by Year for Country Code ARB**:  
![](_dummyPy023.png)
  
###_Network Analysis in Python (Part I)_#  
  
Chapter 1 - Introduction to Networks  
  
Introduction to networks - examples like social networks, transportation networks, etc.:  
  
* Networks are a useful tool for modeling relationships between entities  
* Networks are defined by two sets of attributes; nodes and edges (these form a network, known in mathematics as a "graph")  
* The "networkx" library is frequently imported as nx  
* The start of an empty network (Graph) can be defined as G = nx.Graph()  
* Nodes can be added using G.add_nodes_from([nodeList])  
	* The call to G.nodes() provides a list of the nodes currently in the Graph  
* The call to G.add_edge(myTuple) will create a link (edge) as defined by myTuple  
	* The call to G.edges will be a tuple showing all the edges currently defined  
* Metadata can further be added to the nodes, such as using G.node[1]["label"] = "blue"  
	* The call to G.nodes(data=True) will then bring back the nodes and also the associated metadata (as dictionaries)  
* The nx.draw() function will draw out the Graph (requires plt.show() where plt is matplotlab.pyplot)  
  
Types of graphs:  
  
* Undirected graphs (e.g., Facebook) are typically drawn as a line with no arrows between two circles  
	* These are created empty as per above using nx.Graph()  
* Directed graphs (e.g., Twitter) are typically drawn as a line with an arrow (uni or bi directional depending on follow/follower) between two circles  
	* These are created empty using nx.DiGraph()  
* Multi graphs (e.g., trips between bike stations) are typically drawn as many arrows between two circles  
	* These are created empty using nx.MultiGraph()  
* Multi graphs can instead be created as weighted arrows, where the weight represents the frequency of occurrence (save memory, plotting, etc. vs. baseline)  
	* The weight may just be included as part of the metadata dictionary  
* Self-loops are nodes that connect to themselves, such as bike trips that start and end at the same station  
  
Network visualization - irrational ("looks like a hairball") and rational visualizations:  
  
* Three primary types of plots - Matrix plot, Arc plot, Circos plot  
* The Matrix plot is a simple row-column, with the square filled in if the edge between the nodes exists  
	* With an undirected Graph, the matrix will be symmetrical around the diagonal  
    * With a directed Graph, the matrix need not be symmetrical around the diagonal (the columns are what the arrow hits, the rows are what it is from)  
* The Arc plot is a transformation where the nodes are all along a single axis of the plot, with connections drawn as semi-circles  
* The Circos plot is a trasnformation of the Arc plot, but where the "axis" is converted in to a circle  
* The "nxviz" package, typically imported as "nv", allows for visualizing the Graphs the above types  
	* ap = nv.ArcPlot(G) ; ap.draw() ; plt.show() will create the ArcPlot  
  
Example code includes:  
```{r engine='python'}

## NEED TO MOCK UP T_sub from the above
import networkx as nx
import datetime

T_sub = nx.DiGraph()

T_sub.add_nodes_from([1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])

T_sub.add_edges_from([(1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (1, 32), (1, 33), (1, 34), (1, 35), (1, 36), (1, 37), (1, 38), (1, 39), (1, 40), (1, 41), (1, 42), (1, 43), (1, 44), (1, 45), (1, 46), (1, 47), (1, 48), (1, 49), (16, 48), (16, 18), (16, 35), (16, 36), (18, 16), (18, 24), (18, 35), (18, 36), (19, 35), (19, 36), (19, 5), (19, 8), (19, 11), (19, 13), (19, 15), (19, 48), (19, 17), (19, 20), (19, 21), (19, 24), (19, 37), (19, 30), (19, 31), (28, 1), (28, 5), (28, 7), (28, 8), (28, 11), (28, 14), (28, 15), (28, 17), (28, 20), (28, 21), (28, 24), (28, 25), (28, 27), (28, 29), (28, 30), (28, 31), (28, 35), (28, 36), (28, 37), (28, 44), (28, 48), (28, 49), (36, 24), (36, 35), (36, 5), (36, 37), (37, 24), (37, 35), (37, 36), (39, 1), (39, 35), (39, 36), (39, 38), (39, 33), (39, 40), (39, 41), (39, 45), (39, 24), (42, 1), (43, 48), (43, 35), (43, 36), (43, 37), (43, 24), (43, 29), (43, 47), (45, 1), (45, 39), (45, 41)])

node_meta = [{'occupation': 'scientist', 'category': 'I'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'P'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'P'}]

for x in range(len(T_sub.nodes())) :
    T_sub.node[T_sub.nodes()[x]]["occupation"] = node_meta[x]["occupation"]
    T_sub.node[T_sub.nodes()[x]]["category"] = node_meta[x]["category"]

edge_meta = [{'date': datetime.date(2012, 11, 17)}, {'date': datetime.date(2007, 6, 19)}, {'date': datetime.date(2014, 3, 18)}, {'date': datetime.date(2007, 3, 18)}, {'date': datetime.date(2011, 12, 19)}, {'date': datetime.date(2013, 12, 7)}, {'date': datetime.date(2009, 11, 9)}, {'date': datetime.date(2008, 10, 7)}, {'date': datetime.date(2008, 8, 14)}, {'date': datetime.date(2011, 3, 22)}, {'date': datetime.date(2014, 8, 3)}, {'date': datetime.date(2007, 5, 19)}, {'date': datetime.date(2009, 12, 13)}, {'date': datetime.date(2011, 4, 7)}, {'date': datetime.date(2013, 8, 2)}, {'date': datetime.date(2014, 11, 17)}, {'date': datetime.date(2013, 5, 20)}, {'date': datetime.date(2010, 12, 15)}, {'date': datetime.date(2010, 11, 27)}, {'date': datetime.date(2013, 9, 5)}, {'date': datetime.date(2013, 3, 1)}, {'date': datetime.date(2007, 7, 8)}, {'date': datetime.date(2010, 5, 23)}, {'date': datetime.date(2007, 9, 14)}, {'date': datetime.date(2013, 1, 24)}, {'date': datetime.date(2013, 6, 21)}, {'date': datetime.date(2010, 6, 28)}, {'date': datetime.date(2011, 12, 2)}, {'date': datetime.date(2010, 7, 24)}, {'date': datetime.date(2010, 7, 4)}, {'date': datetime.date(2013, 9, 28)}, {'date': datetime.date(2007, 3, 17)}, {'date': datetime.date(2013, 11, 7)}, {'date': datetime.date(2012, 8, 13)}, {'date': datetime.date(2009, 2, 19)}, {'date': datetime.date(2007, 3, 17)}, {'date': datetime.date(2011, 11, 15)}, {'date': datetime.date(2011, 12, 26)}, {'date': datetime.date(2010, 2, 14)}, {'date': datetime.date(2014, 4, 16)}, {'date': datetime.date(2010, 2, 28)}, {'date': datetime.date(2007, 11, 2)}, {'date': datetime.date(2008, 5, 17)}, {'date': datetime.date(2013, 11, 18)}, {'date': datetime.date(2010, 11, 14)}, {'date': datetime.date(2007, 8, 19)}, {'date': datetime.date(2012, 5, 11)}, {'date': datetime.date(2007, 10, 27)}, {'date': datetime.date(2009, 11, 14)}, {'date': datetime.date(2009, 4, 19)}, {'date': datetime.date(2007, 7, 14)}, {'date': datetime.date(2012, 5, 7)}, {'date': datetime.date(2014, 5, 4)}, {'date': datetime.date(2012, 6, 16)}, {'date': datetime.date(2012, 4, 25)}, {'date': datetime.date(2012, 6, 25)}, {'date': datetime.date(2010, 10, 14)}, {'date': datetime.date(2013, 4, 18)}, {'date': datetime.date(2013, 10, 6)}, {'date': datetime.date(2009, 8, 2)}, {'date': datetime.date(2008, 9, 23)}, {'date': datetime.date(2011, 11, 26)}, {'date': datetime.date(2010, 1, 22)}, {'date': datetime.date(2012, 6, 23)}, {'date': datetime.date(2013, 11, 20)}, {'date': datetime.date(2008, 7, 6)}, {'date': datetime.date(2009, 4, 12)}, {'date': datetime.date(2011, 12, 28)}, {'date': datetime.date(2012, 1, 22)}, {'date': datetime.date(2009, 1, 26)}, {'date': datetime.date(2012, 1, 13)}, {'date': datetime.date(2010, 9, 26)}, {'date': datetime.date(2013, 11, 14)}, {'date': datetime.date(2010, 7, 22)}, {'date': datetime.date(2013, 3, 17)}, {'date': datetime.date(2008, 10, 18)}, {'date': datetime.date(2008, 12, 9)}, {'date': datetime.date(2012, 1, 14)}, {'date': datetime.date(2012, 6, 28)}, {'date': datetime.date(2011, 10, 5)}, {'date': datetime.date(2007, 5, 19)}, {'date': datetime.date(2013, 1, 24)}, {'date': datetime.date(2008, 6, 28)}, {'date': datetime.date(2008, 5, 16)}, {'date': datetime.date(2013, 5, 8)}, {'date': datetime.date(2007, 7, 23)}, {'date': datetime.date(2010, 8, 4)}, {'date': datetime.date(2011, 10, 18)}, {'date': datetime.date(2011, 6, 2)}, {'date': datetime.date(2009, 5, 23)}, {'date': datetime.date(2010, 10, 14)}, {'date': datetime.date(2013, 7, 17)}, {'date': datetime.date(2008, 5, 19)}, {'date': datetime.date(2008, 3, 19)}, {'date': datetime.date(2010, 8, 14)}, {'date': datetime.date(2012, 6, 19)}, {'date': datetime.date(2013, 8, 12)}, {'date': datetime.date(2013, 7, 6)}, {'date': datetime.date(2014, 10, 11)}, {'date': datetime.date(2012, 7, 1)}, {'date': datetime.date(2013, 11, 5)}, {'date': datetime.date(2009, 11, 6)}, {'date': datetime.date(2009, 4, 19)}, {'date': datetime.date(2008, 8, 12)}, {'date': datetime.date(2012, 8, 8)}, {'date': datetime.date(2009, 8, 12)}, {'date': datetime.date(2012, 5, 27)}, {'date': datetime.date(2011, 9, 15)}, {'date': datetime.date(2013, 12, 19)}, {'date': datetime.date(2007, 12, 7)}, {'date': datetime.date(2008, 3, 4)}, {'date': datetime.date(2013, 9, 16)}, {'date': datetime.date(2009, 11, 22)}, {'date': datetime.date(2014, 9, 19)}, {'date': datetime.date(2008, 10, 20)}, {'date': datetime.date(2010, 12, 16)}, {'date': datetime.date(2013, 3, 15)}, {'date': datetime.date(2012, 4, 25)}, {'date': datetime.date(2009, 5, 10)}]

for x in range(len(T_sub.edges())) :
    a, b = T_sub.edges()[x]
    T_sub.edge[a][b]["date"] = edge_meta[x]["date"]


# Import necessary modules
import matplotlib.pyplot as plt


# Draw the graph to screen
nx.draw(T_sub)
# plt.show()
plt.savefig("_dummyPy024.png", bbox_inches="tight")



# Also need to mock up T
# Use T_sub for these
# Use a list comprehension to get the nodes of interest: noi
noi = [n for n, d in T_sub.nodes(data=True) if d['occupation'] == 'scientist']

# Use a list comprehension to get the edges of interest: eoi
eoi = [(u, v) for u, v, d in T_sub.edges(data=True) if d["date"] < datetime.date(2010, 1, 1)]


# Set the weight of the edge
T_sub.edge[1][10]["weight"] = 2

# Iterate over all the edges (with metadata)
for u, v, d in T_sub.edges(data=True):
    
    # Check if node 293 is involved
    # Make it node 23 instead
    if 23 in [u, v]:
        # Set the weight to 1.1
        T_sub.edge[u][v]["weight"] = 1.1


# Define find_selfloop_nodes()
def find_selfloop_nodes(G):
    """
    Finds all nodes that have self-loops in the graph G.
    """
    nodes_in_selfloops = []
    
    # Iterate over all the edges of G
    for u, v in G.edges():
    # Check if node u and node v are the same
        if u == v:
            # Append node u to nodes_in_selfloops
            nodes_in_selfloops.append(u)
            
    return nodes_in_selfloops

# Check whether number of self loops equals the number of nodes in self loops
# The mock-up above has no self-loops, so this is just for reference on how to find them
assert T_sub.number_of_selfloops() == len(find_selfloop_nodes(T_sub))


# Import nxviz
import nxviz as nv

# Create the MatrixPlot object: m
m = nv.MatrixPlot(T_sub)

# Draw m to the screen
m.draw()

# Display the plot
# plt.show()
plt.savefig("_dummyPy025.png", bbox_inches="tight")


# Convert T to a matrix format: A
A = nx.to_numpy_matrix(T_sub)

# Convert A back to the NetworkX form as a directed graph: T_conv
T_conv = nx.from_numpy_matrix(A, create_using=nx.DiGraph())

# Check that the `category` metadata field is lost from each node
for n, d in T_conv.nodes(data=True):
    assert 'category' not in d.keys()


# Import necessary modules
import matplotlib.pyplot as plt
from nxviz import CircosPlot

# Create the CircosPlot object: c
c = CircosPlot(T_sub)

# Draw c to the screen
c.draw()

# Display the plot
# plt.show()
plt.savefig("_dummyPy026.png", bbox_inches="tight")


# Import necessary modules
from nxviz import ArcPlot

# Create the un-customized ArcPlot object: a
a = ArcPlot(T_sub)

# Draw a to the screen
a.draw()

# Display the plot
# plt.show()
plt.savefig("_dummyPy027.png", bbox_inches="tight")


# Create the customized ArcPlot object: a2
a2 = ArcPlot(T_sub, node_order="category", node_color="category")

# Draw a2 to the screen
a2.draw()

# Display the plot
# plt.show()
plt.savefig("_dummyPy028.png", bbox_inches="tight")


```
  
**Example network plot**:  
![](_dummyPy024.png)

**Example MatrixPlot (network)**:  
![](_dummyPy025.png)

**Example CircosPlot (network)**:  
![](_dummyPy026.png)

**Example ArcPlot (network)**:  
![](_dummyPy027.png)

**Example ArcPlot (network) colored by category**:  
![](_dummyPy028.png)


***

Chapter 2 - Important Nodes  
  
Degree centrality - one method of determining important nodes:  
  
* Being connected to another node makes you a "neighbor" of that node  
* Degree centrality for a node is defined as "# neighbors I have" divided by "number of possible neighbors"  
	* Depending on self-loops, the "number of possible neighbors" may or may not include itself  
* Examples of high degree centrality include Twitter broadcasters, airport hubs, disease super-spreaders, and the like  
* Within the "networkx" package, G.neighbors(1) will give a list of all the neighbors of node 1  
	* Can instead run nx.degree_centrality(G)  # outputs a dictionary of node:centrality; self-loops are not considered  
  
Graph algorithms - path finding for optimization (e.g., shortest path between nodes, information or disease spread, etc.):  
  
* Breadth-first search (BFS) algorithm first developed in the 1950s for finding the shortest path out of a maze  
* Basically, take one of the points, then find its neighbors, then its neighbors' neighbors, etc., until the second point is found  
* Use G.edges(), G.nodes(), and automate the search for finding paths between any two given points  
  
Betweeness centrality - including the key concept of "all shortest paths":  
  
* All shortest paths is based on finding all shortest paths between all pairs of nodes  
* Betweeness centrality is defined as "# shortest paths running THROUGH node" divide by "all possible shortest paths"  
	* n.b. that a node and its neighbor is not counted in the numerator or the denominator; only paths of length 2+ are relevant  
* This helps to identify "bottleneck" nodes - points that if eliminated would significantly slow or even stop connections  
	* Can use nx.barbell_graph(m1=, m2=) # m1 will be the size of the barbells, m2 will be the number of connector nodes (zero would just connect a point on each dumbbell)  
    * Can use nx.betweenness_centrality(G) to get a dictionary of node:betweeness  
  
Example code includes:  
```{r engine='python'}

import networkx as nx
import matplotlib.pyplot as plt
import datetime


# DO NOT HAVE Graph T
# Make the same as above
T = nx.DiGraph()

T.add_nodes_from([1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])

T.add_edges_from([(1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (1, 32), (1, 33), (1, 34), (1, 35), (1, 36), (1, 37), (1, 38), (1, 39), (1, 40), (1, 41), (1, 42), (1, 43), (1, 44), (1, 45), (1, 46), (1, 47), (1, 48), (1, 49), (16, 48), (16, 18), (16, 35), (16, 36), (18, 16), (18, 24), (18, 35), (18, 36), (19, 35), (19, 36), (19, 5), (19, 8), (19, 11), (19, 13), (19, 15), (19, 48), (19, 17), (19, 20), (19, 21), (19, 24), (19, 37), (19, 30), (19, 31), (28, 1), (28, 5), (28, 7), (28, 8), (28, 11), (28, 14), (28, 15), (28, 17), (28, 20), (28, 21), (28, 24), (28, 25), (28, 27), (28, 29), (28, 30), (28, 31), (28, 35), (28, 36), (28, 37), (28, 44), (28, 48), (28, 49), (36, 24), (36, 35), (36, 5), (36, 37), (37, 24), (37, 35), (37, 36), (39, 1), (39, 35), (39, 36), (39, 38), (39, 33), (39, 40), (39, 41), (39, 45), (39, 24), (42, 1), (43, 48), (43, 35), (43, 36), (43, 37), (43, 24), (43, 29), (43, 47), (45, 1), (45, 39), (45, 41)])

node_meta = [{'occupation': 'scientist', 'category': 'I'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'P'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'P'}]

for x in range(len(T.nodes())) :
    T.node[T.nodes()[x]]["occupation"] = node_meta[x]["occupation"]
    T.node[T.nodes()[x]]["category"] = node_meta[x]["category"]

edge_meta = [{'date': datetime.date(2012, 11, 17)}, {'date': datetime.date(2007, 6, 19)}, {'date': datetime.date(2014, 3, 18)}, {'date': datetime.date(2007, 3, 18)}, {'date': datetime.date(2011, 12, 19)}, {'date': datetime.date(2013, 12, 7)}, {'date': datetime.date(2009, 11, 9)}, {'date': datetime.date(2008, 10, 7)}, {'date': datetime.date(2008, 8, 14)}, {'date': datetime.date(2011, 3, 22)}, {'date': datetime.date(2014, 8, 3)}, {'date': datetime.date(2007, 5, 19)}, {'date': datetime.date(2009, 12, 13)}, {'date': datetime.date(2011, 4, 7)}, {'date': datetime.date(2013, 8, 2)}, {'date': datetime.date(2014, 11, 17)}, {'date': datetime.date(2013, 5, 20)}, {'date': datetime.date(2010, 12, 15)}, {'date': datetime.date(2010, 11, 27)}, {'date': datetime.date(2013, 9, 5)}, {'date': datetime.date(2013, 3, 1)}, {'date': datetime.date(2007, 7, 8)}, {'date': datetime.date(2010, 5, 23)}, {'date': datetime.date(2007, 9, 14)}, {'date': datetime.date(2013, 1, 24)}, {'date': datetime.date(2013, 6, 21)}, {'date': datetime.date(2010, 6, 28)}, {'date': datetime.date(2011, 12, 2)}, {'date': datetime.date(2010, 7, 24)}, {'date': datetime.date(2010, 7, 4)}, {'date': datetime.date(2013, 9, 28)}, {'date': datetime.date(2007, 3, 17)}, {'date': datetime.date(2013, 11, 7)}, {'date': datetime.date(2012, 8, 13)}, {'date': datetime.date(2009, 2, 19)}, {'date': datetime.date(2007, 3, 17)}, {'date': datetime.date(2011, 11, 15)}, {'date': datetime.date(2011, 12, 26)}, {'date': datetime.date(2010, 2, 14)}, {'date': datetime.date(2014, 4, 16)}, {'date': datetime.date(2010, 2, 28)}, {'date': datetime.date(2007, 11, 2)}, {'date': datetime.date(2008, 5, 17)}, {'date': datetime.date(2013, 11, 18)}, {'date': datetime.date(2010, 11, 14)}, {'date': datetime.date(2007, 8, 19)}, {'date': datetime.date(2012, 5, 11)}, {'date': datetime.date(2007, 10, 27)}, {'date': datetime.date(2009, 11, 14)}, {'date': datetime.date(2009, 4, 19)}, {'date': datetime.date(2007, 7, 14)}, {'date': datetime.date(2012, 5, 7)}, {'date': datetime.date(2014, 5, 4)}, {'date': datetime.date(2012, 6, 16)}, {'date': datetime.date(2012, 4, 25)}, {'date': datetime.date(2012, 6, 25)}, {'date': datetime.date(2010, 10, 14)}, {'date': datetime.date(2013, 4, 18)}, {'date': datetime.date(2013, 10, 6)}, {'date': datetime.date(2009, 8, 2)}, {'date': datetime.date(2008, 9, 23)}, {'date': datetime.date(2011, 11, 26)}, {'date': datetime.date(2010, 1, 22)}, {'date': datetime.date(2012, 6, 23)}, {'date': datetime.date(2013, 11, 20)}, {'date': datetime.date(2008, 7, 6)}, {'date': datetime.date(2009, 4, 12)}, {'date': datetime.date(2011, 12, 28)}, {'date': datetime.date(2012, 1, 22)}, {'date': datetime.date(2009, 1, 26)}, {'date': datetime.date(2012, 1, 13)}, {'date': datetime.date(2010, 9, 26)}, {'date': datetime.date(2013, 11, 14)}, {'date': datetime.date(2010, 7, 22)}, {'date': datetime.date(2013, 3, 17)}, {'date': datetime.date(2008, 10, 18)}, {'date': datetime.date(2008, 12, 9)}, {'date': datetime.date(2012, 1, 14)}, {'date': datetime.date(2012, 6, 28)}, {'date': datetime.date(2011, 10, 5)}, {'date': datetime.date(2007, 5, 19)}, {'date': datetime.date(2013, 1, 24)}, {'date': datetime.date(2008, 6, 28)}, {'date': datetime.date(2008, 5, 16)}, {'date': datetime.date(2013, 5, 8)}, {'date': datetime.date(2007, 7, 23)}, {'date': datetime.date(2010, 8, 4)}, {'date': datetime.date(2011, 10, 18)}, {'date': datetime.date(2011, 6, 2)}, {'date': datetime.date(2009, 5, 23)}, {'date': datetime.date(2010, 10, 14)}, {'date': datetime.date(2013, 7, 17)}, {'date': datetime.date(2008, 5, 19)}, {'date': datetime.date(2008, 3, 19)}, {'date': datetime.date(2010, 8, 14)}, {'date': datetime.date(2012, 6, 19)}, {'date': datetime.date(2013, 8, 12)}, {'date': datetime.date(2013, 7, 6)}, {'date': datetime.date(2014, 10, 11)}, {'date': datetime.date(2012, 7, 1)}, {'date': datetime.date(2013, 11, 5)}, {'date': datetime.date(2009, 11, 6)}, {'date': datetime.date(2009, 4, 19)}, {'date': datetime.date(2008, 8, 12)}, {'date': datetime.date(2012, 8, 8)}, {'date': datetime.date(2009, 8, 12)}, {'date': datetime.date(2012, 5, 27)}, {'date': datetime.date(2011, 9, 15)}, {'date': datetime.date(2013, 12, 19)}, {'date': datetime.date(2007, 12, 7)}, {'date': datetime.date(2008, 3, 4)}, {'date': datetime.date(2013, 9, 16)}, {'date': datetime.date(2009, 11, 22)}, {'date': datetime.date(2014, 9, 19)}, {'date': datetime.date(2008, 10, 20)}, {'date': datetime.date(2010, 12, 16)}, {'date': datetime.date(2013, 3, 15)}, {'date': datetime.date(2012, 4, 25)}, {'date': datetime.date(2009, 5, 10)}]

for x in range(len(T.edges())) :
    a, b = T.edges()[x]
    T.edge[a][b]["date"] = edge_meta[x]["date"]



# Define nodes_with_m_nbrs()
def nodes_with_m_nbrs(G, m):
    """
    Returns all nodes in graph G that have m neighbors.
    """
    nodes = set()
    
    # Iterate over all nodes in G
    for n in G.nodes():
        # Check if the number of neighbors of n matches m
        if len(G.neighbors(n)) == m:
            # Add the node n to the set
            nodes.add(n)
    # Return the nodes with m neighbors
    return nodes

# Compute and print all nodes in T that have 3 neighbors
three_nbrs = nodes_with_m_nbrs(T, 3)
print(three_nbrs)


# Compute the degree of every node: degrees
degrees = [len(T.neighbors(n)) for n in T.nodes()]

# Print the degrees
print(degrees)


# Compute the degree centrality of the Twitter network: deg_cent
deg_cent = nx.degree_centrality(T)

# Plot a histogram of the degree centrality distribution of the graph.
plt.figure()
plt.hist(list(deg_cent.values()))
# plt.show()
plt.savefig("_dummyPy029.png", bbox_inches="tight")
plt.clf()

# Plot a histogram of the degree distribution of the graph
plt.figure()
plt.hist(degrees)
# plt.show()
plt.savefig("_dummyPy030.png", bbox_inches="tight")
plt.clf()

# Plot a scatter plot of the centrality distribution and the degree distribution
plt.figure()
plt.scatter(degrees, list(deg_cent.values()))
# plt.show()
plt.savefig("_dummyPy031.png", bbox_inches="tight")
plt.clf()


def path_exists(G, node1, node2):
    """
    This function checks whether a path exists between two nodes (node1, node2) in graph G.
    """
    visited_nodes = set()
    queue = [node1]
    
    for node in queue:  
        neighbors = G.neighbors(node)
        if node2 in neighbors:
            print('Path exists between nodes {0} and {1}'.format(node1, node2))
            return True
            break
            
        else:
            visited_nodes.add(node)
            queue.extend([n for n in neighbors if n not in visited_nodes])
            
        # Check to see if the final element of the queue has been reached
        if node == queue[-1]:
            print('Path does not exist between nodes {0} and {1}'.format(node1, node2))
            
            # Place the appropriate return statement
            return False


# Compute the betweenness centrality of T: bet_cen
bet_cen = nx.betweenness_centrality(T)

# Compute the degree centrality of T: deg_cen
deg_cen = nx.degree_centrality(T)

# Create a scatter plot of betweenness centrality and degree centrality
plt.scatter(list(bet_cen.values()), list(deg_cen.values()))

# Display the plot
# plt.show()
plt.savefig("_dummyPy032.png", bbox_inches="tight")
plt.clf()


# Define find_nodes_with_highest_deg_cent()
def find_nodes_with_highest_deg_cent(G):
    # Compute the degree centrality of G: deg_cent
    deg_cent = nx.degree_centrality(G)
    
    # Compute the maximum degree centrality: max_dc
    max_dc = max(list(deg_cent.values()))
    
    nodes = set()
    
    # Iterate over the degree centrality dictionary
    for k, v in deg_cent.items():
        # Check if the current value has the maximum degree centrality
        if v == max_dc:
            # Add the current node to the set of nodes
            nodes.add(k)
            
    return nodes
    
# Find the node(s) that has the highest degree centrality in T: top_dc
top_dc = find_nodes_with_highest_deg_cent(T)
print(top_dc)

# Write the assertion statement
for node in top_dc:
    assert nx.degree_centrality(T)[node] == max(nx.degree_centrality(T).values())


# Define find_node_with_highest_bet_cent()
def find_node_with_highest_bet_cent(G):
    # Compute betweenness centrality: bet_cent
    bet_cent = nx.betweenness_centrality(G)
    
    # Compute maximum betweenness centrality: max_bc
    max_bc = max(list(bet_cent.values()))
    
    nodes = set()
    
    # Iterate over the betweenness centrality dictionary
    for k, v in bet_cent.items():
        # Check if the current value has the maximum betweenness centrality
        if v == max_bc:
            # Add the current node to the set of nodes
            nodes.add(k)
            
    return nodes

# Use that function to find the node(s) that has the highest betweenness centrality in the network: top_bc
top_bc = find_node_with_highest_bet_cent(T)
print(top_bc)

# Write an assertion statement that checks that the node(s) is/are correctly identified.
for node in top_bc:
    assert nx.betweenness_centrality(T)[node] == max(nx.betweenness_centrality(T).values())


```
  
  
**Histogram of degree centrality**:  
![](_dummyPy029.png)
  
**Histogram of degree distribution**:  
![](_dummyPy030.png)

**Scatter plot of degree centrality vs degree distribution**:  
![](_dummyPy031.png)

**Scatter plot of degree centrality vs between centrality**:  
![](_dummyPy032.png)
  
  
***

Chapter 3 - Structures
  
Cliques and communities - idea of tightly-knit groups:  
  
* In network theory, a "clique" is a set of nodes that are full connected to each other by way of an edge  
* Triangle closures are the idea that if A and B are connected and if A and C are connected, but that B and C are not connected, then connecting B and C will form a "clique"  
* A helpful package "itertools" has a function "combinations" that can help to iterate over many combinations (rather than a double for loop)  
	* For example, combinations("ABC", 2) will create ("A", "B"), ("A", "C"), ("B", "C")  
    * This will be an iterable, but it will not print by itself  
  
Maximal cliques - defined as a clique that when expanded by one node is no longer a clique:  
  
* Basically, there is no single extension (one extra edge) that would make the clique larger  
* Communities are an expansion of the idea of communities  
* One possible definition of "communities" would be maximal cliques that are of at least size x and that have at least y members in common  
* The find_cliques() function will find all of the maximal cliques in the network data  
  
Sub-graphs - sometimes helpful to view just a small portion of a larger graph:  
  
* Can use commands such as Gnew = G.subgraph(myNodes)  # will just contain the nodes of interest, as well as their edges to each other  
* Can then look at nx.draw(Gnew, with_labels=True) to request that labels be included on the visual  
  
Example code includes:  
```{r engine='python'}

from itertools import combinations

# Define is_in_triangle() 
def is_in_triangle(G, n):
    """
    Checks whether a node `n` in graph `G` is in a triangle relationship or not. 
    Returns a boolean.
    """
    in_triangle = False
    
    # Iterate over all possible triangle relationship combinations
    for n1, n2 in combinations(G.neighbors(n), 2):
        # Check if an edge exists between n1 and n2
        if G.has_edge(n1, n2):
            in_triangle = True
            break
    return in_triangle

# DO NOT HAVE T (make randomly, minus metadata)
import networkx as nx
import random
import numpy as np
import matplotlib.pyplot as plt

T = nx.Graph()
T.add_nodes_from([x for x in range(1, 31)])
np.random.seed(170530)
n1 = np.random.choice(range(1, 31), size=100, replace=True)
n2 = np.random.choice(range(1, 31), size=100, replace=True)

# Require that first be less than second
edge_list = [(min(x, y), max(x, y)) for x, y in zip(n1, n2) if x != y]
T.add_edges_from(edge_list)


# APPEARS THAT the set() makes sure to keep a sorted and unique list; if a = set(1, 2) and a.add(1) is run, than a will still be {1, 2}
# Can remove items from the set using a.remove() and can add items to the set using a.add()

# Write a function that identifies all nodes in a triangle relationship with a given node.
def nodes_in_triangle(G, n):
    """
    Returns the nodes in a graph `G` that are involved in a triangle relationship with the node `n`.
    """
    triangle_nodes = set([n])
    
    # Iterate over all possible triangle relationship combinations
    for n1, n2 in combinations(G.neighbors(n), 2):
        # Check if n1 and n2 have an edge between them
        if G.has_edge(n1, n2):
            # Add n1 to triangle_nodes
            triangle_nodes.add(n1)
            
            # Add n2 to triangle_nodes
            triangle_nodes.add(n2)
            
    return triangle_nodes
    
# Write the assertion statement
assert len(nodes_in_triangle(T, 1)) == 5  # happens to be what the RNG generated in this case


# Define node_in_open_triangle()
def node_in_open_triangle(G, n):
    """
    Checks whether pairs of neighbors of node `n` in graph `G` are in an 'open triangle' relationship with node `n`.
    """
    in_open_triangle = False
    
    # Iterate over all possible triangle relationship combinations
    for n1, n2 in combinations(G.neighbors(n), 2):
        # Check if n1 and n2 do NOT have an edge between them
        if not G.has_edge(n1, n2):
            in_open_triangle = True
            break
            
    return in_open_triangle

# Compute the number of open triangles in T
num_open_triangles = 0

# Iterate over all the nodes in T
for n in T.nodes():
    
    # Check if the current node is in an open triangle
    if node_in_open_triangle(T, n):
        
        # Increment num_open_triangles
        num_open_triangles += 1
    

print(num_open_triangles)


# Define maximal_cliques()
def maximal_cliques(G, size):
    """
    Finds all maximal cliques in graph `G` that are of size `size`.
    """
    mcs = []
    for clique in nx.find_cliques(G):
        if len(clique) == size:
            mcs.append(clique)
    return mcs

# Check that there are 33 maximal cliques of size 3 in the graph T
assert len(maximal_cliques(T, 3)) == 26  # happens to be what the RNG returns in this case


# Define get_nodes_and_nbrs()
def get_nodes_and_nbrs(G, nodes_of_interest):
    """
    Returns a subgraph of the graph `G` with only the `nodes_of_interest` and their neighbors.
    """
    nodes_to_draw = []
    
    # Iterate over the nodes of interest
    for n in nodes_of_interest:
        # Append the nodes of interest to nodes_to_draw
        nodes_to_draw.append(n)
        
        # Iterate over all the neighbors of node n
        for nbr in G.neighbors(n):
            # Append the neighbors of n to nodes_to_draw
            nodes_to_draw.append(nbr)
        
    return G.subgraph(nodes_to_draw)


# Extract the subgraph with the nodes of interest: T_draw
nodes_of_interest = [8, 24, 26]
T_draw = get_nodes_and_nbrs(T, nodes_of_interest)

# Draw the subgraph to the screen
nx.draw(T_draw, with_labels=True)
# plt.show()
plt.savefig("_dummyPy033.png", bbox_inches="tight")


# Extract the nodes of interest: nodes
node_meta = [{'occupation': 'scientist', 'category': 'I'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'scientist', 'category': 'P'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'politician', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'scientist', 'category': 'D'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'celebrity', 'category': 'D'}, {'occupation': 'politician', 'category': 'D'}, {'occupation': 'politician', 'category': 'P'}, {'occupation': 'celebrity', 'category': 'I'}, {'occupation': 'celebrity', 'category': 'P'}, {'occupation': 'scientist', 'category': 'I'}, {'occupation': 'scientist', 'category': 'P'}]

for x in range(len(T.nodes())) :
    T.node[T.nodes()[x]]["occupation"] = node_meta[x]["occupation"]
    T.node[T.nodes()[x]]["category"] = node_meta[x]["category"]


nodes = [n for n, d in T.nodes(data=True) if d['occupation'] == 'celebrity']

# Create the set of nodes: nodeset
nodeset = set(nodes)

# Iterate over nodes
for n in nodeset:
    
    # Compute the neighbors of n: nbrs
    nbrs = T.neighbors(n)
    
    # Compute the union of nodeset and nbrs: nodeset
    nodeset = nodeset.union(nbrs)


# Compute the subgraph using nodeset: T_sub
T_sub = T.subgraph(nodeset)

# Draw T_sub to the screen
nx.draw(T_sub, with_labels=True)
# plt.show()
plt.savefig("_dummyPy034.png", bbox_inches="tight")

```
  
  
**Example Sub-graph (anything touching any of [8, 24, 26]**:  
![](_dummyPy033.png)

**Example Sub-graph (specified "occupation" in metadata)**:  
![](_dummyPy034.png)
  
***
  
Chapter 4 - Case Study  
  
Case study introduction - GitHub collaborator data:  
  
* The data will be a GitHub use collaboration network  
* The nodes will be the users and the edges will reflect collaboration on 1+ GitHub repositories  
* Goals include 1) analyze structure, 2) visualize graph, and 3) build simple recommendation systems  
  
Case Study Part II - Visualization using the nxviz API:  
  
* circ = nv.CircosPlot(G) ; circ.draw()  # Create the Circos plot  
* Additionally, will use the "connected component subgraph" features of networkx  
* A connected component subragph is defined as a group of nodes connected to each other (perhaps not as a clique; may be through hubs) but with no connection to some other group of nodes  
	* nx.connected_component_subgraph(G)  # forms a generator object; cast as list to read them  
  
Case Study Part III: Cliques:  
  
* Simplest clique is an edge  
* Simplest complex clique is a triangle  
* Maximal clique is a clique that cannot be extended just be adding one additional node  
* The nx.find_cliques(G) will find all of the maximal cliques in G  
  
Case Study Part IV: Additional Tasks (building a recommender):  
  
* Find important users (share with the most other users - degree_centrality)  
* Find largest communities of collaborators (maximal cliques)  
* Build a collaboration recommendation system (open triangles)  
  
Example code includes:  
```{r engine='python'}

# Import necessary modules
import matplotlib.pyplot as plt
import networkx as nx 
import numpy as np
import random


# DO NOT HAVE Github collaborator graph "G"
# Dummy up the data - 20 each of 2 "flavors"
G = nx.Graph()
G.add_nodes_from([x for x in range(1, 41)])
np.random.seed(170531)

# Add edges for 1-20 with preference that they match to themselves
n1 = np.random.choice(range(1, 21), size=100, replace=True)
n2 = np.random.choice(range(1, 21), size=90, replace=True)
n3 = np.random.choice(range(21, 41), size=10, replace=True)

# Require that first be less than second
edge_list = [(min(x, y), max(x, y)) for x, y in zip(n1, np.append(n2, n3)) if x != y]
G.add_edges_from(edge_list)


# Add edges for 21-40 with preference that they match to themselves
n1 = np.random.choice(range(21, 41), size=50, replace=True)
n2 = np.random.choice(range(21, 41), size=40, replace=True)
n3 = np.random.choice(range(1, 21), size=10, replace=True)

# Require that first be less than second
edge_list = [(min(x, y), max(x, y)) for x, y in zip(n1, np.append(n2, n3)) if x != y]
G.add_edges_from(edge_list)

# Create two groupings for the nodes
node_meta = [{'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type01'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}, {'grouping': 'type02'}]

for x in range(len(G.nodes())) :
    G.node[G.nodes()[x]]["grouping"] = node_meta[x]["grouping"]



# Plot the degree distribution of the GitHub collaboration network
plt.hist(list(nx.degree_centrality(G).values()))
# plt.show()
plt.savefig("_dummyPy035.png", bbox_inches="tight")
plt.clf()



# Plot the degree distribution of the GitHub collaboration network
plt.hist(list(nx.betweenness_centrality(G).values()))
# plt.show()
plt.savefig("_dummyPy036.png", bbox_inches="tight")
plt.clf()


# Import necessary modules
from nxviz import MatrixPlot


# Calculate the largest connected component subgraph: largest_ccs
largest_ccs = sorted(nx.connected_component_subgraphs(G), key=lambda x: len(x))[-1]

# Create the customized MatrixPlot object: h
h = MatrixPlot(largest_ccs, node_grouping="grouping")

# Draw the MatrixPlot to the screen
h.draw()
# plt.show()
plt.savefig("_dummyPy037.png", bbox_inches="tight")


# Import necessary modules
from nxviz.plots import ArcPlot


# Iterate over all the nodes in G, including the metadata
for n, d in G.nodes(data=True):
    
    # Calculate the degree of each node: G.node[n]['degree']
    G.node[n]['degree'] = nx.degree(G, n)
    
# Create the ArcPlot object: a
a = ArcPlot(G, node_order="degree")

# Draw the ArcPlot to the screen
a.draw()
# plt.show()
plt.savefig("_dummyPy038.png", bbox_inches="tight")


# Import necessary modules
from nxviz import CircosPlot
 
 
# Iterate over all the nodes, including the metadata
for n, d in G.nodes(data=True):
    
    # Calculate the degree of each node: G.node[n]['degree']
    G.node[n]['degree'] = nx.degree(G, n)

# Create the CircosPlot object: c
c = CircosPlot(G, node_order="degree", node_grouping="grouping", node_color="grouping")

# Draw the CircosPlot object to the screen
c.draw()
# plt.show()
plt.savefig("_dummyPy039.png", bbox_inches="tight")


# Calculate the maximal cliques in G: cliques
cliques = nx.find_cliques(G)

# Count and print the number of maximal cliques in G
print(len(list(cliques)))


# Find the author(s) that are part of the largest maximal clique: largest_clique
largest_clique = sorted(nx.find_cliques(G), key=lambda x:len(x))[-1]

# Create the subgraph of the largest_clique: G_lc
G_lc = G.subgraph(largest_clique)

# Create the CircosPlot object: c
c = CircosPlot(G_lc)

# Draw the CircosPlot to the screen
c.draw()
# plt.show()
plt.savefig("_dummyPy040.png", bbox_inches="tight")


# Compute the degree centralities of G: deg_cent
deg_cent = nx.degree_centrality(G)

# Compute the maximum degree centrality: max_dc
max_dc = max(deg_cent.values())

# Find the user(s) that have collaborated the most: prolific_collaborators
prolific_collaborators = [n for n, dc in deg_cent.items() if dc == max_dc]

# Print the most prolific collaborator(s)
print(prolific_collaborators)


# Identify the largest maximal clique: largest_max_clique
largest_max_clique = set(sorted(nx.find_cliques(G), key=lambda x: len(x))[-1])

# Create a subgraph from the largest_max_clique: G_lmc
G_lmc = G.subgraph(largest_max_clique)

# Go out 1 degree of separation
for node in G_lmc.nodes():
    G_lmc.add_nodes_from(G.neighbors(node))
    G_lmc.add_edges_from(zip([node]*len(G.neighbors(node)), G.neighbors(node)))

# Record each node's degree centrality score
for n in G_lmc.nodes():
    G_lmc.node[n]['degree centrality'] = nx.degree_centrality(G_lmc)[n]
        
# Create the ArcPlot object: a
a = ArcPlot(G_lmc, node_order = "degree centrality")

# Draw the ArcPlot to the screen
a.draw()
# plt.show()
plt.savefig("_dummyPy041.png", bbox_inches="tight")


# Import necessary modules
from itertools import combinations
from collections import defaultdict

# Initialize the defaultdict: recommended
recommended = defaultdict(int)

# Iterate over all the nodes in G
for n, d in G.nodes(data=True):
    
    # Iterate over all possible triangle relationship combinations
    for n1, n2 in combinations(G.neighbors(n), 2):
        
        # Check whether n1 and n2 do not have an edge
        if not G.has_edge(n1, n2):
            
            # Increment recommended
            recommended[(n1, n2)] += 1


# Identify the top 10 pairs of users
all_counts = sorted(recommended.values())
top10_pairs = [pair for pair, count in recommended.items() if count > all_counts[-10]]
print(top10_pairs)

```
  
  
**Case study - degree distribution**:  
![](_dummyPy035.png)

**Case study - betweenness centrality**:  
![](_dummyPy036.png)

**Case study - MatrixPlot**:  
![](_dummyPy037.png)

**Case study - ArcPlot**:  
![](_dummyPy038.png)

**Case study - CircosPlot**:  
![](_dummyPy039.png)

**Case Study - CircosPlot (for largest clique)**:  
![](_dummyPy040.png)

**Case Study - ArcPlot (ordered by degree centrality)**:  
![](_dummyPy041.png)
  
  
## Python Import and Clean Data  
###_Importing Data in Python (Part I)_#
  
Chapter 1 - Introduction and flat files  
  
Welcome to the course - importing from 1) flat files, 2) other native data, and 3) relational databases:  
  
* Begin by looking at text files - plain text and table data (each row is an observation)  
* The python "open()" function is the easiest way to look at a file  
	* filename = "myFile" ; fPointer = open(filename, mode="r"), fText = fPointer.read(); file.close()  
    * print(fText)  # All the text will be printed to the console  
* Alternately, can use "with open("myFile", mode="r") as fPointer:  # the file will close when the with ends  
	* The "with" statement is known as a context manager  
    * The use of a context manager is a best practice, since you never have to worry about closing a file  
  
The importance of flat files in data science:  
  
* Flat files are text files containing records (which is to say "table data" with each row being an observation and each column being an attribute)  
* Flat files may also have a header describing the columns of the data (important to know for the data import process)  
* Flat files are especially relevant for data science since they are a nice way to store tidy data  
* Flat files may be separated by delimitors (comma, tab, etc.)   
* Imports may be done through numpy or pandas  
  
Importing flat files using numpy (only for data that is purely numerical):  
  
* numpy arrays are the Python standard for storing numerical data; efficient, fast, and clean, and also often essential for other packages  
* numpy.loadtxt() - import numpy as np; myData=np.loadtxt("myFile", delimiter=<myDelim>, skiprows=0, usecols=myList, dtype=)  # default delimiter is any whitespace, default skip-rows is 0, default usecols is ALL, dtype=str will load as strings  
	* Tends to break down when loading mixed data types; these are typically better for pandas  
* numpy.genfromtxt() is another option, though only briefly mentioned in this course  
  
Importing flat files using pandas - create 2-D data structures with columns of different data types:  
  
* The pandas package is designed to help elevate Python from data munging (where it has always been excellent) to the full data analysis workflow (which might otherwise require R)  
* The pandas DataFrame is modeled off the data frame in R; same idea of observations (rows) and variables (columns)  
* The pandas package is the current best practice for loading data from flat files in to Python  
* In the most basic usage, myData = pd.read_csv("myFile")  # assumes import pandas as pd called previously  
	* myData.head()  # shows the first 5 rows of the data  
    * myData.values # This will be the associated numpy array  
  
Example code includes:  
```{r engine='python'}

# put in directory ./PythonInputFiles/
# moby_dick.txt (converted to romeo-full.txt)
# digits.csv (using mnist_test.csv)
# digits_header.txt (skipped)
# seaslug.txt (downloaded)
# titanic.csv (converted from R)
# titanic_corrupt.txt (skipped)

myPath = "./PythonInputFiles/"


# NEED FILE "moby_dick.txt" (used "romeo-full.txt" instead)
# Open a file: file
file = open(myPath + "romeo-full.txt", mode="r")

# Print it
print(file.read())

# Check whether file is closed
print(file.closed)

# Close file
file.close()

# Check whether file is closed
print(file.closed)


# Read & print the first 3 lines
with open(myPath + "romeo-full.txt") as file:
    print(file.readline())
    print(file.readline())
    print(file.readline())


# NEED DIGIT RECOGNITION SITE - see http://yann.lecun.com/exdb/mnist/
# Import package
import numpy as np

# Assign filename to variable: file
file = myPath + 'mnist_test.csv'

# Load file as array: digits
digits = np.loadtxt(file, delimiter=",")

# Print datatype of digits
print(type(digits))

# Select and reshape a row
im = digits[21, 1:]
im_sq = np.reshape(im, (28, 28))


import matplotlib.pyplot as plt  # so the plotting below can be done

# Plot reshaped data (matplotlib.pyplot already loaded as plt)
plt.imshow(im_sq, cmap='Greys', interpolation='nearest')
# plt.show()


# File should be tab-delimited and with a header row (for the skiprows=1)
# Assign the filename: file
# file = 'digits_header.txt'

# Load the data: data
# data = np.loadtxt(file, delimiter="\t", skiprows=1, usecols=[0, 2])

# Print data
# print(data)


# NEED FILE FROM http://www.stat.ucla.edu/projects/datasets/seaslug-explanation.html
# Should be floats with a single text header row, and tab-delimited

# Assign filename: file
file = myPath + 'seaslug.txt'

# Import file: data
data = np.loadtxt(file, delimiter='\t', dtype=str)

# Print the first element of data
print(data[0])

# Import data as floats and skip the first row: data_float
data_float = np.loadtxt(file, delimiter="\t", dtype=float, skiprows=1)

# Print the 10th element of data_float
print(data_float[9])

# Plot a scatterplot of the data
plt.scatter(data_float[:, 0], data_float[:, 1])
plt.xlabel('time (min.)')
plt.ylabel('percentage of larvae')
# plt.show()


# NEED FILE "titanic.csv"
# Idea is that np.genfromtxt() and np.recfromcsv() can accept mixed data types through making each row its own array; dtype=None lets Python pick the data type by column

# Assign the filename: file
# file = myPath + 'titanic.csv'

# Import file using np.recfromcsv: d
# d=np.recfromcsv(file)   # This is like np.genfromtxt() with defaults set to dtype=None, delimiter=",", names=True

# Print out first three entries of d
# print(d[:3])


# PassengerId-Survived-Pclass-Sex-Age-SibSp-Parch-Ticket-Fare-Cabin-Embarked
# Import pandas as pd
import pandas as pd

# Assign the filename: file
file = myPath + 'titanic.csv'

# Read the file into a DataFrame: df
df = pd.read_csv(file)

# View the head of the DataFrame
print(df.head())



# Assign the filename: file
file = myPath + 'mnist_test.csv'

# Read the first 5 rows of the file into a DataFrame: data
data=pd.read_csv(file, nrows=5, header=None)

# Build a numpy array from the DataFrame: data_array
data_array = data.values

# Print the datatype of data_array to the shell
print(type(data_array))


# Assign filename: file
# file = 'titanic_corrupt.txt'

# Import file: data
# data = pd.read_csv(file, sep="\t", comment="#", na_values=["Nothing"])

# Print the head of the DataFrame
# print(data.head())

# Plot 'Age' variable in a histogram
# pd.DataFrame.hist(data[['Age']])
# plt.xlabel('Age (years)')
# plt.ylabel('count')
# plt.show()


```
  
  
***
  
Chapter 2 - Importing data from other file types  
  
Introduction to other files types - Excel spreadsheets, MATLAB, SAS, Stata, HDF5 (becoming a more relevant format for saving data):  
  
* There are also "pickled" files which are native to Python; idea is that you can serialize files like dictionaries or lists for later use in Python (rather than using json which is more human-readable)  
* Opening a pickled file: import pickle; with open("myFile,pkl", mode="rb") as file: data=pickle.load(file)  
* Excel files can generally be opened using data=pd.ExcelFile("myExcel.xlsx")  # assumes previous import pandas as pd; automatically loads the Excel sheet as a data frame  
    * data.sheet_names  # provides a list of the sheet names  
    * df1 = data.parse("sheetName")  # can pass either the index as a float or the sheet name as a string  
    * Can also skip rows and import only certain columns  
  
Importing SAS/Stata files using pandas:  
  
* SAS: Statistical Analysis System is common for business analytics and biostatistics  
* Stata: Statistics + Data is common for academic social sciences research  
* The most common SAS files have the extensions .sas7bdat and .sas7cdat  
	* from sas7bdat import SAS7BDAT  
    * with SAS7BDAT("mySASfile.sas7bdat") as file: df_sas=file.to_data_frame()  # as per previous examples  
* The Stata files can be imported directly using pd  
	* pd.read_stata("myStataFile.dta")  
  
Importing HDF5 (Hierarchical Data Format 5) files, quickly becoming the Python standard for storing large quantities of numerical data:  
  
* HDF5 can scale up to exabytes of data, and is commonly used for files of hundereds of gigabytes or even terabytes  
* import h5py; data=h5py.File("myHD5.hd5", "r"); for key in data.keys(): print(key)  
	* might have "meta", "quality", and "strain" for a specific LIGO data file  
    * could dive further in to any of the keys, for example for key in data["meta"].keys(): print(key)  
* The HDF project is formally managed by the HDF group, a Champaign-based spinoff of the University of Illinois  
  
Importing MATLAB (MATrix LABoratory) files - industry standard in engineering and science:  
  
* The library scipy has functions scipy.io.loadmat() and scipy.io.savemat()  
	* The loaded file will be a dictionary (keys are the variable names and values are the objects assigned to the variables)  
* COULD NOT GET scipy to import (lack of blas???)
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

# Import pickle package
import pickle

# NEED PICKLE DATA - {'Mar': '84.4', 'June': '69.4', 'Airline': '8', 'Aug': '85'}
# Created using with open(myPath + "data.pkl", "wb") as file: pickle.dump(myDict, file)
# Open pickle file and load data: d
with open(myPath + 'data.pkl', mode="rb") as file:
    d = pickle.load(file)

# Print d
print(d)

# Print datatype of d
print(type(d))


# NEED BATTLE DEATHS DATA - https://www.prio.org/Data/Armed-Conflict/Battle-Deaths/The-Battle-Deaths-Dataset-version-30/ (downloaded and converted name to "battledeath.xlsx")
# Import pandas
import pandas as pd

# Assign spreadsheet filename: file
file = myPath + "battledeath.xlsx"

# Load spreadsheet: xl
xl = pd.ExcelFile(file)

# Print sheet names
print(xl.sheet_names)


# Load a sheet into a DataFrame by name: df1
# There is only one sheet absent converting "bdonly" to a file by year
df1 = xl.parse("bdonly")

# Print the head of the DataFrame df1
print(df1.head())

# Load a sheet into a DataFrame by index: df2
df2 = xl.parse(0)

# Print the head of the DataFrame df2
print(df2.head())


# Parse the first sheet and rename the columns: df1
df1 = xl.parse(0, skiprows=[0], parse_cols=[2, 9], names=["AAM due to War (2002)", "Country"])

# Print the head of the DataFrame df1
print(df1.head())

# Parse the tenth column of the first sheet and rename the column: df2
df2 = xl.parse(0, parse_cols=[9], skiprows=[0], names=["Country"])

# Print the head of the DataFrame df2
print(df2.head())


# DO NOT HAVE THIS FILE EITHER
# Import sas7bdat package
from sas7bdat import SAS7BDAT

# Save file to a DataFrame: df_sas
# with SAS7BDAT('sales.sas7bdat') as file:
#     df_sas = file.to_data_frame()

# Print head of DataFrame
# print(df_sas.head())

import matplotlib.pyplot as plt

# Plot histogram of DataFrame features (pandas and pyplot already imported)
# pd.DataFrame.hist(df_sas[['P']])
# plt.ylabel('count')
# plt.show()


# DO NOT HAVE THIS FILE EITHER
# Import pandas

# Load Stata file into a pandas DataFrame: df
# df = pd.read_stata("disarea.dta")

# Print the head of the DataFrame df
# print(df.head())

# Plot histogram of one column of the DataFrame
# pd.DataFrame.hist(df[['disa10']])
# plt.xlabel('Extent of disease')
# plt.ylabel('Number of coutries')
# plt.show()


# DO NOT HAVE THIS FILE EITHER
# Import packages
import numpy as np
import h5py

# Assign filename: file
# file = 'LIGO_data.hdf5'

# Load file: data
# data = h5py.File(file, "r")

# Print the datatype of the loaded file
# print(type(data))

# Print the keys of the file
# for key in data.keys():
#     print(key)


# Get the HDF5 group: group
# group = data["strain"]

# Check out keys of group
# for key in group.keys():
#     print(key)

# Set variable equal to time series data: strain
# strain = data['strain']['Strain'].value

# Set number of time points to sample: num_samples
# num_samples = 10000

# Set time vector
# time = np.arange(0, 1, 1/num_samples)

# Plot data
# plt.plot(time, strain[:num_samples])
# plt.xlabel('GPS Time (s)')
# plt.ylabel('strain')
# plt.show()


# DO NOT HAVE THIS FILE EITHER - see https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm
# Import package (cannot get to download)
# import scipy.io

# Load MATLAB file: mat
# mat = scipy.io.loadmat('albeck_gene_expression.mat')

# Print the datatype type of mat
# print(type(mat))


# Print the keys of the MATLAB dictionary
# print(mat.keys())

# Print the type of the value corresponding to the key 'CYratioCyt'
# print(type(mat["CYratioCyt"]))

# Print the shape of the value corresponding to the key 'CYratioCyt'
# print(np.shape(mat["CYratioCyt"]))

# Subset the array and plot it
# data = mat['CYratioCyt'][25, 5:]
# fig = plt.figure()
# plt.plot(data)
# plt.xlabel('time (min.)')
# plt.ylabel('normalized fluorescence (measure of expression)')
# plt.show()

```
  
  
***
  
Chapter 3 - Relational databases  
  
Introduction to relational databases - standard discussion of how a relational database (system of tables) works:  
  
* Each of the tables is a data frame, keyed by a primary key (unique identifier for the row in question)  
* The tables are all linked by way of the primary keys, and the existence of these keys as columns in some of the other tables  
* The relational linking process saves a great deal of space  
* Many systems exist, such as PostgreSQL, MySQL, SQLite, and the like  
* SQL is an acronym for "Structured Query Language" which is a standard way for interacting with the relational databases  
  
Creating a database engine in Python - goal is to get data out of the relational database using SQL:  
  
* SQLite is nice since it is fast and simple, though other databases may have additional valuable features  
* The package "SQLAlchemy" works with many other RDBMS (relational database management systems)  
	* from sqlalchemy import create_engine  
    * engine = create_engine("mySQLDatabase.sqlite")  # may have different extensions if a different type of database  
    * engine.table_names()  # provides the names of all the tables in engine  
  
Querying relational databases in Python - connecting to the engine and then querying (getting data out from) the database:  
  
* SELECT * FROM myTable will bring over all columns of all rows  
* General workflow for SQL in Python include: 1) import packages, 2) create the DB engine, 3) connect to the engine, 4) query the database, 5) save query results to a DataFrame, and 6) close the connection  
	* Step 3: con = engine.connect()  
    * Step 4: rs = con.execute("valid SQL queries")  
    * Step 5: df = pd.DataFrame(rs.fetchall()) ; df.columns = rs.keys() # if wanting to bring over meaningful column names  
    * Step 6: con.close()  
* A context manager (with engine.connect() as con) can save the hassle of con.close(), or worse forgetting to close the connection  
* Note that rs.fetchmany(size=5) is an option for bringing over just 5 lines from the query (can use numbers other than 5 also)  
  
Querying relational databases directly with pandas - shortcut to the above process:  
  
* df = pd.read_sql_query("valid SQL code", engine)  # where import pandas as pd and engine = create_engine("mySQLConnection") have previously been run  
  
Advanced querying - exploiting table relationships (combining mutliple tables):  
  
* The SQL join to bring 2+ tables together  
* SELECT myVars FROM Table1 INNER JOIN Table2 ON joinCriteria  
    * Note that the format for variables is Table.Variable, so Orders.CustomerID = Customers.CustomerID  
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

# NEED FILE - may be able to get at http://chinookdatabase.codeplex.com/
# Downloaded the ZIP, extracted the SQLite, and renamed to Chinook.sqlite
# Import necessary module
from sqlalchemy import create_engine

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')  # The sqlite:/// is called the 'connection string'


# Save the table names to a list: table_names
table_names = engine.table_names()

# Print the table names to the shell
print(table_names)


# Import packages
from sqlalchemy import create_engine
import pandas as pd

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Open engine connection: con
con = engine.connect()

# Perform query: rs
rs = con.execute("SELECT * FROM Album")

# Save results of the query to DataFrame: df
df = pd.DataFrame(rs.fetchall())

# Close connection
con.close()

# Print head of DataFrame df
print(df.head())


# Open engine in context manager
# Perform query and save results to DataFrame: df
with engine.connect() as con:
    rs = con.execute("SELECT LastName, Title FROM Employee")
    df = pd.DataFrame(rs.fetchmany(size=3))
    df.columns = rs.keys()

# Print the length of the DataFrame df
print(len(df))

# Print the head of the DataFrame df
print(df.head())


# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Open engine in context manager
# Perform query and save results to DataFrame: df
with engine.connect() as con:
    rs = con.execute("SELECT * FROM Employee WHERE EmployeeID >= 6")
    df = pd.DataFrame(rs.fetchall())
    df.columns = rs.keys()

# Print the head of the DataFrame df
print(df.head())


# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Open engine in context manager
with engine.connect() as con:
    rs = con.execute("SELECT * FROM Employee ORDER BY BirthDate")
    df = pd.DataFrame(rs.fetchall())
    
    # Set the DataFrame's column names
    df.columns = rs.keys()

# Print head of DataFrame
print(df.head())


# Import packages
from sqlalchemy import create_engine
import pandas as pd

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Execute query and store records in DataFrame: df
df = pd.read_sql_query("SELECT * FROM Album", engine)

# Print head of DataFrame
print(df.head())

# Open engine in context manager
# Perform query and save results to DataFrame: df1
with engine.connect() as con:
    rs = con.execute("SELECT * FROM Album")
    df1 = pd.DataFrame(rs.fetchall())
    df1.columns = rs.keys()

# Confirm that both methods yield the same result: does df = df1 ?
print(df.equals(df1))


# Import packages
from sqlalchemy import create_engine
import pandas as pd

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Execute query and store records in DataFrame: df
df = pd.read_sql_query("SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate", engine)

# Print head of DataFrame
print(df.head())


# Open engine in context manager
# Perform query and save results to DataFrame: df
with engine.connect() as con:
    rs = con.execute("SELECT Title, Name FROM Album INNER JOIN Artist ON Album.ArtistID = Artist.ArtistID")
    df = pd.DataFrame(rs.fetchall())
    df.columns = rs.keys()

# Print head of DataFrame df
print(df.head())


# Execute query and store records in DataFrame: df
df = pd.read_sql_query("SELECT * FROM PlaylistTrack INNER JOIN Track ON PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000", engine)

# Print head of DataFrame
print(df.head())


```
  
  
###_Importing Data in Python (Part II)_#
  
Chapter 1 - Importing Data from the Internet  
  
Importing flat files from the web - non-local files:  
  
* Clicking on URL and downloading files creates reproducibility problems and is non-scalable  
* Course covers 1) import and locally save from the web, 2) load datasets in to pandas DataFrames, 3) make HTTP requests, 4) scrape HTML (BeustifulSoup)  
* This course will particularly focus on "urllib" and "requests" packages  
* The "urllib" package has an interface for fetching data from across the web  
	* urllib.urlopen("myURL")  # Very similar to open() but takes an URL rather than a local file name  
    * from urllib.request import urlretrieve ; url = "myQuotedURL" ; urlretrieve(url, "myLocalFileName")  
  
HTTP requests to import files from the web - unpacking the urlretrieve from urllib.request:  
  
* URL is an acronym for Uniform/Universal Resource Locator (reference to web resources such as web addresses, FTP, and the like)  
* Ingredients for an URL include 1) protocol identifier (such as "http:") and a resource name (such as "datacamp.com")  
* HTTP is an acronym for Hyper-Text Transfer Protocol which is the foundation for data communication on the web  
	* Going to a website is the process of sending a GET request through HTTP ; the urlretrieve does this automatically  
* HTML is an acronym for HyperText Markup Language, which is the standard mark-up language used on the internet  
* Example process for GET requests using urllib  
	* from urllib.request import urlopen, Request  
    * url = "https://www.wikipedia.org/" ; request = Request(url) ; response = urlopen(request) ; html = response.read() ; response.close()  
* Can also send GET requests using "requests"", a commonly used package that simplifies the process  
	* import requests  
    * url = "https://www.wikipedia.org/" ; r = requests.get(url) ; text = r.text  
  
Scraping the web in Python using BeautifulSoup - make sense of the jumbled, unstructured HTML data:  
  
* Structured data has either 1) a pre-defined data model, or 2) organization in a defined manner  
* HTML is unstructured data, possessing neither of these properties  
* BeautifulSoup parses and extracts structured data from HTML  
* General usage would include  
	* from bs4 import BeautifulSoup ; import requests  
    * url = "https://www.crummy.com/software/BeautifulSoup/"  
    * r = requests.get(url) ; html_doc = r.text  
    * soup = BeautifulSoup(html_doc)  
    * print(soup.prettify()) # printes properly indented html code, easier for human parsing  
  
Example code includes:  
```{r engine='python'}

# Import package
from urllib.request import urlretrieve
import pandas as pd

# Assign url of file: url
url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
urlretrieve(url, 'winequality-red.csv')

# Read file into a DataFrame and print its head
df = pd.read_csv('winequality-red.csv', sep=';')
print(df.head())


# Import packages
import matplotlib.pyplot as plt
import pandas as pd

# Assign url of file: url
url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Read file into a DataFrame: df
df = pd.read_csv(url, sep=";")

# Print the head of the DataFrame
print(df.head())

# Plot first column of df
pd.DataFrame.hist(df.ix[:, 0:1])
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
# plt.show()
plt.clf()


# Assign url of file: url
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file: xl
xl = pd.read_excel(url, sheetname=None)

# Print the sheetnames to the shell
print(xl.keys())

# Print the head of the first sheet (using its name, NOT its index)
print(xl["1700"].head())


# Import packages
from urllib.request import urlopen, Request

# Specify the url
url = "http://www.datacamp.com/teach/documentation"

# This packages the request: request
request = Request(url)

# Sends the request and catches the response: response
response = urlopen(request)

# Print the datatype of response
print(type(response))

# Be polite and close the response!
response.close()


# Specify the url
url = "http://docs.datacamp.com/teach/"

# This packages the request
request = Request(url)

# Sends the request and catches the response: response
response = urlopen(request)

# Extract the response: html
html = response.read()

# Print the html
print(html)

# Be polite and close the response!
response.close()


import requests

# Specify the url: url
url = "http://docs.datacamp.com/teach/"

# Packages the request, send the request and catch the response: r
r = requests.get(url)

# Extract the response: text
text = r.text

# Print the html
print(text)


# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url: url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extracts the response as html: html_doc
html_doc = r.text

# Create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Prettify the BeautifulSoup object: pretty_soup
pretty_soup = soup.prettify()

# Print the response
print(pretty_soup)


# Get the title of Guido's webpage: guido_title
guido_title = soup.title

# Print the title of Guido's webpage to the shell
print(guido_title)

# Get Guido's text: guido_text
guido_text = soup.get_text()

# Print Guido's text to the shell
print(guido_text)


# Find all 'a' tags (which define hyperlinks): a_tags
a_tags = soup.find_all("a")

# Print the URLs to the shell
for link in a_tags:
    print(link.get("href"))


```
  
  
***

Chapter 2 - Interacting with APIs  
  
Introduction to APIs (Application Programming Interface) and JSON (JavaScript Object Notation):  
  
* API is a protocol and routine for building and interacting with software applications  
* JSON helps with rel-time browser to server communication, developed by Douglas Crockford  
* JSON has name-value pairs, very similar to a Python dictionary  
* General process might include  
	* import json  
    * with open("snakes.json", "r") as json_file: json_data = json.load(json_file)  # json_data will be imported as a dictionary  
  
APIs and interacting with the world-wide web - what APIs are and why they are important:  
  
* The API is a set of protocols and routines for interacting with software programs  
* The "Open Movies Database" (OMDB) has an API, as do most websites that might be data sources  
* Example usage might include  
	* import requests  
    * url = "http://www.omdbapi.com/?t=hackers"  # the ? Represents a "query string", in this case asking for "t" (title) equals "hackers" (the movie "Hackers")  
    * r = requests.get(url)  
    * json_data = r.json()  
* Can get the OMDB API webpage for how they allow their data to be queried/used and how to fomat the relevant "query strings"  
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

# DO NOT HAVE FILE a_movie.json, which appears to be JSON for the movie Social Network (2010)
# Created and saved file
import json

# Load JSON: json_data
with open(myPath + "a_movie.json") as json_file:
    json_data = json.load(json_file)

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])


# PROBABLY DO NOT RUN; NEED API KEY
# Import requests package
import requests

# Assign URL to variable: url
url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Print the text of the response
print(r.text)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])


# Assign URL to variable: url
url = "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza"

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print the Wikipedia page extract
pizza_extract = json_data['query']['pages']['24768']['extract']
print(pizza_extract)

```
  
  
***
  
Chapter 3 - Diving deeper in to the Twitter API  
  


	
	
	

	


