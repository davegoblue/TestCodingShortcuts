---
title: "Data Camp Python Import and Clean Data Notes"
author: "davegoblue"
date: "August 1, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(engine.path=list(python="C:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python36-32\\python.exe"))
```

## Background and Overview  

DataCamp offer interactive courses related to Python Programming.  Since R Markdown documents can run simple Python code chunks (though the data is not accessible to future chunks, a large difference from R Markdown for R), this document attempts to summarize notes from the modules when possible.

Topic areas summarized include:  
  
* Python Programming (Introduction, Intermediate, Toolbox I/II, Network Analysis I/II)  
* Python Import and Clean Data (Import I/II, Clean)  
* Python Data Manipulation (pandas Foundations, Manipulating DF with pandas, Merging DF with pandas, Databases in pandas, Data Types)  
  
This document contains the "Import and Clean Data" components.  A separate file DataCamp_Python_Programming_v002 contains the "Programming" components.  A separate file DataCamp_PythonNotes_v002 is the evolving working version.
  
## Python Import and Clean Data  
###_Importing Data in Python (Part I)_#
  
Chapter 1 - Introduction and flat files  
  
Welcome to the course - importing from 1) flat files, 2) other native data, and 3) relational databases:  
  
* Begin by looking at text files - plain text and table data (each row is an observation)  
* The python "open()" function is the easiest way to look at a file  
	* filename = "myFile" ; fPointer = open(filename, mode="r"), fText = fPointer.read(); file.close()  
    * print(fText)  # All the text will be printed to the console  
* Alternately, can use "with open("myFile", mode="r") as fPointer:  # the file will close when the with ends  
	* The "with" statement is known as a context manager  
    * The use of a context manager is a best practice, since you never have to worry about closing a file  
  
The importance of flat files in data science:  
  
* Flat files are text files containing records (which is to say "table data" with each row being an observation and each column being an attribute)  
* Flat files may also have a header describing the columns of the data (important to know for the data import process)  
* Flat files are especially relevant for data science since they are a nice way to store tidy data  
* Flat files may be separated by delimitors (comma, tab, etc.)   
* Imports may be done through numpy or pandas  
  
Importing flat files using numpy (only for data that is purely numerical):  
  
* numpy arrays are the Python standard for storing numerical data; efficient, fast, and clean, and also often essential for other packages  
* numpy.loadtxt() - import numpy as np; myData=np.loadtxt("myFile", delimiter=<myDelim>, skiprows=0, usecols=myList, dtype=)  # default delimiter is any whitespace, default skip-rows is 0, default usecols is ALL, dtype=str will load as strings  
	* Tends to break down when loading mixed data types; these are typically better for pandas  
* numpy.genfromtxt() is another option, though only briefly mentioned in this course  
  
Importing flat files using pandas - create 2-D data structures with columns of different data types:  
  
* The pandas package is designed to help elevate Python from data munging (where it has always been excellent) to the full data analysis workflow (which might otherwise require R)  
* The pandas DataFrame is modeled off the data frame in R; same idea of observations (rows) and variables (columns)  
* The pandas package is the current best practice for loading data from flat files in to Python  
* In the most basic usage, myData = pd.read_csv("myFile")  # assumes import pandas as pd called previously  
	* myData.head()  # shows the first 5 rows of the data  
    * myData.values # This will be the associated numpy array  
  
Example code includes:  
```{r engine='python'}

# put in directory ./PythonInputFiles/
# moby_dick.txt (converted to romeo-full.txt)
# digits.csv (using mnist_test.csv)
# digits_header.txt (skipped)
# seaslug.txt (downloaded)
# titanic.csv (converted from R)
# titanic_corrupt.txt (skipped)

myPath = "./PythonInputFiles/"


# NEED FILE "moby_dick.txt" (used "romeo-full.txt" instead)
# Open a file: file
file = open(myPath + "romeo-full.txt", mode="r")

# Print it
print(file.read())

# Check whether file is closed
print(file.closed)

# Close file
file.close()

# Check whether file is closed
print(file.closed)


# Read & print the first 3 lines
with open(myPath + "romeo-full.txt") as file:
    print(file.readline())
    print(file.readline())
    print(file.readline())


# NEED DIGIT RECOGNITION SITE - see http://yann.lecun.com/exdb/mnist/
# Import package
import numpy as np

# Assign filename to variable: file
file = myPath + 'mnist_test.csv'

# Load file as array: digits
digits = np.loadtxt(file, delimiter=",")

# Print datatype of digits
print(type(digits))

# Select and reshape a row
im = digits[21, 1:]
im_sq = np.reshape(im, (28, 28))


import matplotlib.pyplot as plt  # so the plotting below can be done

# Plot reshaped data (matplotlib.pyplot already loaded as plt)
plt.imshow(im_sq, cmap='Greys', interpolation='nearest')
# plt.show()
plt.savefig("_dummyPy042.png", bbox_inches="tight")
plt.clf()

# File should be tab-delimited and with a header row (for the skiprows=1)
# Assign the filename: file
# file = 'digits_header.txt'

# Load the data: data
# data = np.loadtxt(file, delimiter="\t", skiprows=1, usecols=[0, 2])

# Print data
# print(data)


# NEED FILE FROM http://www.stat.ucla.edu/projects/datasets/seaslug-explanation.html
# Should be floats with a single text header row, and tab-delimited

# Assign filename: file
file = myPath + 'seaslug.txt'

# Import file: data
data = np.loadtxt(file, delimiter='\t', dtype=str)

# Print the first element of data
print(data[0])

# Import data as floats and skip the first row: data_float
data_float = np.loadtxt(file, delimiter="\t", dtype=float, skiprows=1)

# Print the 10th element of data_float
print(data_float[9])

# Plot a scatterplot of the data
plt.scatter(data_float[:, 0], data_float[:, 1])
plt.xlabel('time (min.)')
plt.ylabel('percentage of larvae')
# plt.show()
plt.savefig("_dummyPy043.png", bbox_inches="tight")
plt.clf()

# NEED FILE "titanic.csv"
# Idea is that np.genfromtxt() and np.recfromcsv() can accept mixed data types through making each row its own array; dtype=None lets Python pick the data type by column

# Assign the filename: file
# file = myPath + 'titanic.csv'

# Import file using np.recfromcsv: d
# d=np.recfromcsv(file)   # This is like np.genfromtxt() with defaults set to dtype=None, delimiter=",", names=True

# Print out first three entries of d
# print(d[:3])


# PassengerId-Survived-Pclass-Sex-Age-SibSp-Parch-Ticket-Fare-Cabin-Embarked
# Import pandas as pd
import pandas as pd

# Assign the filename: file
file = myPath + 'titanic.csv'

# Read the file into a DataFrame: df
df = pd.read_csv(file)

# View the head of the DataFrame
print(df.head())



# Assign the filename: file
file = myPath + 'mnist_test.csv'

# Read the first 5 rows of the file into a DataFrame: data
data=pd.read_csv(file, nrows=5, header=None)

# Build a numpy array from the DataFrame: data_array
data_array = data.values

# Print the datatype of data_array to the shell
print(type(data_array))


# Assign filename: file
# file = 'titanic_corrupt.txt'

# Import file: data
# data = pd.read_csv(file, sep="\t", comment="#", na_values=["Nothing"])

# Print the head of the DataFrame
# print(data.head())

# Plot 'Age' variable in a histogram
# pd.DataFrame.hist(data[['Age']])
# plt.xlabel('Age (years)')
# plt.ylabel('count')
# plt.show()


```
  
**Example Image Recognition Digit**:  
![](_dummyPy042.png)

**Sea Slug Data**:  
![](_dummyPy043.png)
  
  
  
***
  
Chapter 2 - Importing data from other file types  
  
Introduction to other files types - Excel spreadsheets, MATLAB, SAS, Stata, HDF5 (becoming a more relevant format for saving data):  
  
* There are also "pickled" files which are native to Python; idea is that you can serialize files like dictionaries or lists for later use in Python (rather than using json which is more human-readable)  
* Opening a pickled file: import pickle; with open("myFile,pkl", mode="rb") as file: data=pickle.load(file)  
* Excel files can generally be opened using data=pd.ExcelFile("myExcel.xlsx")  # assumes previous import pandas as pd; automatically loads the Excel sheet as a data frame  
    * data.sheet_names  # provides a list of the sheet names  
    * df1 = data.parse("sheetName")  # can pass either the index as a float or the sheet name as a string  
    * Can also skip rows and import only certain columns  
  
Importing SAS/Stata files using pandas:  
  
* SAS: Statistical Analysis System is common for business analytics and biostatistics  
* Stata: Statistics + Data is common for academic social sciences research  
* The most common SAS files have the extensions .sas7bdat and .sas7cdat  
	* from sas7bdat import SAS7BDAT  
    * with SAS7BDAT("mySASfile.sas7bdat") as file: df_sas=file.to_data_frame()  # as per previous examples  
* The Stata files can be imported directly using pd  
	* pd.read_stata("myStataFile.dta")  
  
Importing HDF5 (Hierarchical Data Format 5) files, quickly becoming the Python standard for storing large quantities of numerical data:  
  
* HDF5 can scale up to exabytes of data, and is commonly used for files of hundereds of gigabytes or even terabytes  
* import h5py; data=h5py.File("myHD5.hd5", "r"); for key in data.keys(): print(key)  
	* might have "meta", "quality", and "strain" for a specific LIGO data file  
    * could dive further in to any of the keys, for example for key in data["meta"].keys(): print(key)  
* The HDF project is formally managed by the HDF group, a Champaign-based spinoff of the University of Illinois  
  
Importing MATLAB (MATrix LABoratory) files - industry standard in engineering and science:  
  
* The library scipy has functions scipy.io.loadmat() and scipy.io.savemat()  
	* The loaded file will be a dictionary (keys are the variable names and values are the objects assigned to the variables)  
* COULD NOT GET scipy to import (lack of blas???)
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

# Import pickle package
import pickle

# NEED PICKLE DATA - {'Mar': '84.4', 'June': '69.4', 'Airline': '8', 'Aug': '85'}
# Created using with open(myPath + "data.pkl", "wb") as file: pickle.dump(myDict, file)
# Open pickle file and load data: d
with open(myPath + 'data.pkl', mode="rb") as file:
    d = pickle.load(file)

# Print d
print(d)

# Print datatype of d
print(type(d))


# NEED BATTLE DEATHS DATA - https://www.prio.org/Data/Armed-Conflict/Battle-Deaths/The-Battle-Deaths-Dataset-version-30/ (downloaded and converted name to "battledeath.xlsx")
# Import pandas
import pandas as pd

# Assign spreadsheet filename: file
file = myPath + "battledeath.xlsx"

# Load spreadsheet: xl
xl = pd.ExcelFile(file)

# Print sheet names
print(xl.sheet_names)


# Load a sheet into a DataFrame by name: df1
# There is only one sheet absent converting "bdonly" to a file by year
df1 = xl.parse("bdonly")

# Print the head of the DataFrame df1
print(df1.head())

# Load a sheet into a DataFrame by index: df2
df2 = xl.parse(0)

# Print the head of the DataFrame df2
print(df2.head())


# Parse the first sheet and rename the columns: df1
df1 = xl.parse(0, skiprows=[0], parse_cols=[2, 9], names=["AAM due to War (2002)", "Country"])

# Print the head of the DataFrame df1
print(df1.head())

# Parse the tenth column of the first sheet and rename the column: df2
df2 = xl.parse(0, parse_cols=[9], skiprows=[0], names=["Country"])

# Print the head of the DataFrame df2
print(df2.head())


# DO NOT HAVE THIS FILE EITHER
# Import sas7bdat package
from sas7bdat import SAS7BDAT

# Save file to a DataFrame: df_sas
# with SAS7BDAT('sales.sas7bdat') as file:
#     df_sas = file.to_data_frame()

# Print head of DataFrame
# print(df_sas.head())

import matplotlib.pyplot as plt

# Plot histogram of DataFrame features (pandas and pyplot already imported)
# pd.DataFrame.hist(df_sas[['P']])
# plt.ylabel('count')
# plt.show()


# DO NOT HAVE THIS FILE EITHER
# Import pandas

# Load Stata file into a pandas DataFrame: df
# df = pd.read_stata("disarea.dta")

# Print the head of the DataFrame df
# print(df.head())

# Plot histogram of one column of the DataFrame
# pd.DataFrame.hist(df[['disa10']])
# plt.xlabel('Extent of disease')
# plt.ylabel('Number of coutries')
# plt.show()


# DO NOT HAVE THIS FILE EITHER
# Import packages
import numpy as np
import h5py

# Assign filename: file
# file = 'LIGO_data.hdf5'

# Load file: data
# data = h5py.File(file, "r")

# Print the datatype of the loaded file
# print(type(data))

# Print the keys of the file
# for key in data.keys():
#     print(key)


# Get the HDF5 group: group
# group = data["strain"]

# Check out keys of group
# for key in group.keys():
#     print(key)

# Set variable equal to time series data: strain
# strain = data['strain']['Strain'].value

# Set number of time points to sample: num_samples
# num_samples = 10000

# Set time vector
# time = np.arange(0, 1, 1/num_samples)

# Plot data
# plt.plot(time, strain[:num_samples])
# plt.xlabel('GPS Time (s)')
# plt.ylabel('strain')
# plt.show()


# DO NOT HAVE THIS FILE EITHER - see https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm
# Import package (cannot get to download)
# import scipy.io

# Load MATLAB file: mat
# mat = scipy.io.loadmat('albeck_gene_expression.mat')

# Print the datatype type of mat
# print(type(mat))


# Print the keys of the MATLAB dictionary
# print(mat.keys())

# Print the type of the value corresponding to the key 'CYratioCyt'
# print(type(mat["CYratioCyt"]))

# Print the shape of the value corresponding to the key 'CYratioCyt'
# print(np.shape(mat["CYratioCyt"]))

# Subset the array and plot it
# data = mat['CYratioCyt'][25, 5:]
# fig = plt.figure()
# plt.plot(data)
# plt.xlabel('time (min.)')
# plt.ylabel('normalized fluorescence (measure of expression)')
# plt.show()

```
  
  
***
  
Chapter 3 - Relational databases  
  
Introduction to relational databases - standard discussion of how a relational database (system of tables) works:  
  
* Each of the tables is a data frame, keyed by a primary key (unique identifier for the row in question)  
* The tables are all linked by way of the primary keys, and the existence of these keys as columns in some of the other tables  
* The relational linking process saves a great deal of space  
* Many systems exist, such as PostgreSQL, MySQL, SQLite, and the like  
* SQL is an acronym for "Structured Query Language" which is a standard way for interacting with the relational databases  
  
Creating a database engine in Python - goal is to get data out of the relational database using SQL:  
  
* SQLite is nice since it is fast and simple, though other databases may have additional valuable features  
* The package "SQLAlchemy" works with many other RDBMS (relational database management systems)  
	* from sqlalchemy import create_engine  
    * engine = create_engine("mySQLDatabase.sqlite")  # may have different extensions if a different type of database  
    * engine.table_names()  # provides the names of all the tables in engine  
  
Querying relational databases in Python - connecting to the engine and then querying (getting data out from) the database:  
  
* SELECT * FROM myTable will bring over all columns of all rows  
* General workflow for SQL in Python include: 1) import packages, 2) create the DB engine, 3) connect to the engine, 4) query the database, 5) save query results to a DataFrame, and 6) close the connection  
	* Step 3: con = engine.connect()  
    * Step 4: rs = con.execute("valid SQL queries")  
    * Step 5: df = pd.DataFrame(rs.fetchall()) ; df.columns = rs.keys() # if wanting to bring over meaningful column names  
    * Step 6: con.close()  
* A context manager (with engine.connect() as con) can save the hassle of con.close(), or worse forgetting to close the connection  
* Note that rs.fetchmany(size=5) is an option for bringing over just 5 lines from the query (can use numbers other than 5 also)  
  
Querying relational databases directly with pandas - shortcut to the above process:  
  
* df = pd.read_sql_query("valid SQL code", engine)  # where import pandas as pd and engine = create_engine("mySQLConnection") have previously been run  
  
Advanced querying - exploiting table relationships (combining mutliple tables):  
  
* The SQL join to bring 2+ tables together  
* SELECT myVars FROM Table1 INNER JOIN Table2 ON joinCriteria  
    * Note that the format for variables is Table.Variable, so Orders.CustomerID = Customers.CustomerID  
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

# NEED FILE - may be able to get at http://chinookdatabase.codeplex.com/
# Downloaded the ZIP, extracted the SQLite, and renamed to Chinook.sqlite
# Import necessary module
from sqlalchemy import create_engine

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')  # The sqlite:/// is called the 'connection string'


# Save the table names to a list: table_names
table_names = engine.table_names()

# Print the table names to the shell
print(table_names)


# Import packages
from sqlalchemy import create_engine
import pandas as pd

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Open engine connection: con
con = engine.connect()

# Perform query: rs
rs = con.execute("SELECT * FROM Album")

# Save results of the query to DataFrame: df
df = pd.DataFrame(rs.fetchall())

# Close connection
con.close()

# Print head of DataFrame df
print(df.head())


# Open engine in context manager
# Perform query and save results to DataFrame: df
with engine.connect() as con:
    rs = con.execute("SELECT LastName, Title FROM Employee")
    df = pd.DataFrame(rs.fetchmany(size=3))
    df.columns = rs.keys()

# Print the length of the DataFrame df
print(len(df))

# Print the head of the DataFrame df
print(df.head())


# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Open engine in context manager
# Perform query and save results to DataFrame: df
with engine.connect() as con:
    rs = con.execute("SELECT * FROM Employee WHERE EmployeeID >= 6")
    df = pd.DataFrame(rs.fetchall())
    df.columns = rs.keys()

# Print the head of the DataFrame df
print(df.head())


# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Open engine in context manager
with engine.connect() as con:
    rs = con.execute("SELECT * FROM Employee ORDER BY BirthDate")
    df = pd.DataFrame(rs.fetchall())
    
    # Set the DataFrame's column names
    df.columns = rs.keys()

# Print head of DataFrame
print(df.head())


# Import packages
from sqlalchemy import create_engine
import pandas as pd

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Execute query and store records in DataFrame: df
df = pd.read_sql_query("SELECT * FROM Album", engine)

# Print head of DataFrame
print(df.head())

# Open engine in context manager
# Perform query and save results to DataFrame: df1
with engine.connect() as con:
    rs = con.execute("SELECT * FROM Album")
    df1 = pd.DataFrame(rs.fetchall())
    df1.columns = rs.keys()

# Confirm that both methods yield the same result: does df = df1 ?
print(df.equals(df1))


# Import packages
from sqlalchemy import create_engine
import pandas as pd

# Create engine: engine
engine = create_engine('sqlite:///' + myPath + 'Chinook.sqlite')

# Execute query and store records in DataFrame: df
df = pd.read_sql_query("SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate", engine)

# Print head of DataFrame
print(df.head())


# Open engine in context manager
# Perform query and save results to DataFrame: df
with engine.connect() as con:
    rs = con.execute("SELECT Title, Name FROM Album INNER JOIN Artist ON Album.ArtistID = Artist.ArtistID")
    df = pd.DataFrame(rs.fetchall())
    df.columns = rs.keys()

# Print head of DataFrame df
print(df.head())


# Execute query and store records in DataFrame: df
df = pd.read_sql_query("SELECT * FROM PlaylistTrack INNER JOIN Track ON PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000", engine)

# Print head of DataFrame
print(df.head())


```
  
  
###_Importing Data in Python (Part II)_#
  
Chapter 1 - Importing Data from the Internet  
  
Importing flat files from the web - non-local files:  
  
* Clicking on URL and downloading files creates reproducibility problems and is non-scalable  
* Course covers 1) import and locally save from the web, 2) load datasets in to pandas DataFrames, 3) make HTTP requests, 4) scrape HTML (BeustifulSoup)  
* This course will particularly focus on "urllib" and "requests" packages  
* The "urllib" package has an interface for fetching data from across the web  
	* urllib.urlopen("myURL")  # Very similar to open() but takes an URL rather than a local file name  
    * from urllib.request import urlretrieve ; url = "myQuotedURL" ; urlretrieve(url, "myLocalFileName")  
  
HTTP requests to import files from the web - unpacking the urlretrieve from urllib.request:  
  
* URL is an acronym for Uniform/Universal Resource Locator (reference to web resources such as web addresses, FTP, and the like)  
* Ingredients for an URL include 1) protocol identifier (such as "http:") and a resource name (such as "datacamp.com")  
* HTTP is an acronym for Hyper-Text Transfer Protocol which is the foundation for data communication on the web  
	* Going to a website is the process of sending a GET request through HTTP ; the urlretrieve does this automatically  
* HTML is an acronym for HyperText Markup Language, which is the standard mark-up language used on the internet  
* Example process for GET requests using urllib  
	* from urllib.request import urlopen, Request  
    * url = "https://www.wikipedia.org/" ; request = Request(url) ; response = urlopen(request) ; html = response.read() ; response.close()  
* Can also send GET requests using "requests"", a commonly used package that simplifies the process  
	* import requests  
    * url = "https://www.wikipedia.org/" ; r = requests.get(url) ; text = r.text  
  
Scraping the web in Python using BeautifulSoup - make sense of the jumbled, unstructured HTML data:  
  
* Structured data has either 1) a pre-defined data model, or 2) organization in a defined manner  
* HTML is unstructured data, possessing neither of these properties  
* BeautifulSoup parses and extracts structured data from HTML  
* General usage would include  
	* from bs4 import BeautifulSoup ; import requests  
    * url = "https://www.crummy.com/software/BeautifulSoup/"  
    * r = requests.get(url) ; html_doc = r.text  
    * soup = BeautifulSoup(html_doc)  
    * print(soup.prettify()) # printes properly indented html code, easier for human parsing  
  
Example code includes:  
```{r engine='python'}

# Import package
from urllib.request import urlretrieve
import pandas as pd

# Assign url of file: url (ran once - no need to re-run)
# url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
# urlretrieve(url, 'winequality-red.csv')

# Read file into a DataFrame and print its head
df = pd.read_csv('winequality-red.csv', sep=';')
print(df.head())


# Import packages
import matplotlib.pyplot as plt
import pandas as pd

# Assign url of file: url (ran once - no need to re-run)
# url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Read file into a DataFrame: df
# df = pd.read_csv(url, sep=";")

# Print the head of the DataFrame
# print(df.head())

# Plot first column of df
pd.DataFrame.hist(df.ix[:, 0:1])
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
# plt.show()
plt.savefig("_dummyPy044.png", bbox_inches="tight")
plt.clf()


# Assign url of file: url
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file: xl
xl = pd.read_excel(url, sheetname=None)

# Print the sheetnames to the shell
print(xl.keys())

# Print the head of the first sheet (using its name, NOT its index)
print(xl["1700"].head())


# Import packages
from urllib.request import urlopen, Request

# Specify the url
url = "http://www.datacamp.com/teach/documentation"

# This packages the request: request
request = Request(url)

# Sends the request and catches the response: response
response = urlopen(request)

# Print the datatype of response
print(type(response))

# Be polite and close the response!
response.close()


# Specify the url
url = "http://docs.datacamp.com/teach/"

# This packages the request
request = Request(url)

# Sends the request and catches the response: response
response = urlopen(request)

# Extract the response: html
html = response.read()

# Print the html
print(html)

# Be polite and close the response!
response.close()


import requests

# Specify the url: url
url = "http://docs.datacamp.com/teach/"

# Packages the request, send the request and catch the response: r
r = requests.get(url)

# Extract the response: text
text = r.text

# Print the html
print(text)


# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url: url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extracts the response as html: html_doc
html_doc = r.text

# Create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Prettify the BeautifulSoup object: pretty_soup
pretty_soup = soup.prettify()

# Print the response
print(pretty_soup)


# Get the title of Guido's webpage: guido_title
guido_title = soup.title

# Print the title of Guido's webpage to the shell
print(guido_title)

# Get Guido's text: guido_text
guido_text = soup.get_text()

# Print Guido's text to the shell
print(guido_text)


# Find all 'a' tags (which define hyperlinks): a_tags
a_tags = soup.find_all("a")

# Print the URLs to the shell
for link in a_tags:
    print(link.get("href"))


```
  
  
**Acidity of Red Wine**:  
![](_dummyPy044.png)
  
***

Chapter 2 - Interacting with APIs  
  
Introduction to APIs (Application Programming Interface) and JSON (JavaScript Object Notation):  
  
* API is a protocol and routine for building and interacting with software applications  
* JSON helps with rel-time browser to server communication, developed by Douglas Crockford  
* JSON has name-value pairs, very similar to a Python dictionary  
* General process might include  
	* import json  
    * with open("snakes.json", "r") as json_file: json_data = json.load(json_file)  # json_data will be imported as a dictionary  
  
APIs and interacting with the world-wide web - what APIs are and why they are important:  
  
* The API is a set of protocols and routines for interacting with software programs  
* The "Open Movies Database" (OMDB) has an API, as do most websites that might be data sources  
* Example usage might include  
	* import requests  
    * url = "http://www.omdbapi.com/?t=hackers"  # the ? Represents a "query string", in this case asking for "t" (title) equals "hackers" (the movie "Hackers")  
    * r = requests.get(url)  
    * json_data = r.json()  
* Can get the OMDB API webpage for how they allow their data to be queried/used and how to fomat the relevant "query strings"  
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

# DO NOT HAVE FILE a_movie.json, which appears to be JSON for the movie Social Network (2010)
# Created and saved file
import json

# Load JSON: json_data
with open(myPath + "a_movie.json") as json_file:
    json_data = json.load(json_file)

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])


# PROBABLY DO NOT RUN; NEED API KEY
# Import requests package
import requests

# Assign URL to variable: url
url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Print the text of the response
print(r.text)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])


# Assign URL to variable: url
url = "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza"

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print the Wikipedia page extract
pizza_extract = json_data['query']['pages']['24768']['extract']
print(pizza_extract)

```
  
  
***
  
Chapter 3 - Diving deeper in to the Twitter API  
  
Twitter API and Authentication - 1) Twitter API, 2) filtering tweets, 3) API Authentication and Oauth, 4) Python package "tweepy":  
  
* The Twitter API requires a Twitter account, then creating a new Twitter App, then copying over the Token and Token Secret  
* Twitter has many API including the REST API (Representational State API), which allows for reading and writing Twitter data  
* The Twitter Streaming API includes a "Public streams" for low-latency access to tweets  
* The Twitter Firehose API is not publicly avaiable, requires special permission, and would likely be very expensive  
* Tweets are generally returned as JSON  
* The "tweepy" package has a nice balance between functionality and usability  
	* auth = tweepy.OAuthHandler(consumer_key, consumer_secret)  
    * auth.set_access(access_token, access_token_secret)  
  
Example code includes:  
```{r engine='python', eval=FALSE}

# DO NOT RUN THIS - NO IDEA WHOSE KEYS THESE ARE (DataCamp???)
# Import package
import tweepy

# Store OAuth authentication credentials in relevant variables
access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"
consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"

# Pass OAuth details to tweepy's OAuth handler
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)


# The class MyStreamListener is available at https://gist.github.com/hugobowne/18f1c0c0709ed1a52dc5bcd462ac69f4
# Initialize Stream listener
l = MyStreamListener()

# Create you Stream object with authentication
stream = tweepy.Stream(auth, l)

# Filter Twitter Streams to capture data by the keywords:
stream.filter(track=['clinton', 'trump', 'sanders', 'cruz'])


# Import package
import json

# String of path to file: tweets_data_path
tweets_data_path = "tweets.txt"

# Initialize empty list to store tweets: tweets_data
tweets_data = []

# Open connection to file
tweets_file = open(tweets_data_path, "r")

# Read in tweets and store in list: tweets_data
for line in tweets_file:
    tweet = json.loads(line)
    tweets_data.append(tweet)

# Close connection to file
tweets_file.close()

# Print the keys of the first tweet dict
print(tweets_data[0].keys())


# Import package
import pandas as pd

# Build DataFrame of tweet texts and languages
df = pd.DataFrame(tweets_data, columns=["text", "lang"])

# Print head of DataFrame
print(df.head())


def word_in_text(word, tweet):
    word = word.lower()
    text = tweet.lower()
    match = re.search(word, tweet)

    if match:
        return True
    return False


# Initialize list to store tweet counts
[clinton, trump, sanders, cruz] = [0, 0, 0, 0]

# Iterate through df, counting the number of tweets in which
# each candidate is mentioned
for index, row in df.iterrows():
    clinton += word_in_text('clinton', row['text'])
    trump += word_in_text('trump', row['text'])
    sanders += word_in_text('sanders', row['text'])
    cruz += word_in_text('cruz', row['text'])


# Import packages
import matplotlib.pyplot as plt
import seaborn as sns

# Set seaborn style
sns.set(color_codes=True)

# Create a list of labels:cd
cd = ['clinton', 'trump', 'sanders', 'cruz']

# Plot histogram
ax = sns.barplot(cd, [clinton, trump, sanders, cruz])
ax.set(ylabel="count")
plt.show()

```
  
  
###_Cleaning Data in Python_#
  
Chapter 1 - Exploring Your Data  
  
Diagnose data for cleaning - column names, missing data, outliers, duplicate rows, un-tidy data, unexpected data values, etc.:  
  
* Pandas can be identified/filtered using row/column names or row/column indices  
* Missing data are typically NaN in Python  
* For a pandas DataFrame df, df.head() and df.tail() will show the first/last 5 rows  
	* df.columns returns an index of column names, which can reveal leading/trailing spaces  
    * df.shape is analogous to dim() in R  
    * df.info() will give a summary of the frame, as well as the associated columns (data types, non-missing values, and the like) - note that type "object" means it is non-numeric  
  
Exploratory data analysis - suppose that a pandas DataFrame, df, has already been created:  
  
* To get frequency counts, use df.continent.value_counts(dropna=False)  # in this case "continent" is the column name (can also subset using bracket notation, which is required if name has any "problems"). . .   
	* Frequency counts will be in descending order  
    * df["continent"].value_counts(dropna=False) will return the same thing  
* To get summaries of numeric data, use df.describe()  # will only be run for numeric columns  
  
Visual exploratory data analysis - easy way to spot outliers and obvious errors - assume again that a pandas DataFrame, df, has already been explained:  
  
* Bar plots for discrete data  
* Histograms for contiuous data - df["myColumn"].plot("hist") will create the histogram, and plt.show() will then show the histogram  
* Data can be subset similar to R - df[df["myVar"] condition] will pull only the rows where the specified condition is met  
* Box plots can be handy ways to summarize the numerical data - df.boxplot(column="myColumn", by="myByVariable")  
* Scatter plots can be handy ways to look at relationships between two numeric columns  
  
Example code includes:  
```{r engine='python'}

# Downloaded small portion to myPath from https://data.cityofnewyork.us/Housing-Development/DOB-Job-Application-Filings/ic3t-wcy2/data

# tempData = pd.read_csv(myPath + "DOB_JOB_Application_Filings.csv")

# keyCols = ["Borough", "State", "Site Fill", "Existing Zoning Sqft", "Initial Cost", "Total Est. Fee"]
# useData = tempData[keyCols]
# useData.loc[:, "initial_cost"] = [float(d[1:]) for d in useData["Initial Cost"]]
# useData.loc[:, "total_est_fee"] = [float(d[1:]) for d in useData["Total Est. Fee"]]
# useData.to_csv(myPath + "dob_job_application_filings_subset.csv")

# MAY NEED TO GET DATA FROM https://opendata.cityofnewyork.us/
# Import pandas
import pandas as pd

myPath = "./PythonInputFiles/"


# Read the file into a DataFrame: df
df = pd.read_csv(myPath + 'dob_job_application_filings_subset.csv')

# Print the head of df
print(df.head())

# Print the tail of df
print(df.tail())

# Print the shape of df
print(df.shape)

# Print the columns of df
print(df.columns)

# Print the info of df
print(df.info())


# Print the value counts for 'Borough'
print(df['Borough'].value_counts(dropna=False))

# Print the value_counts for 'State'
print(df['State'].value_counts(dropna=False))

# Print the value counts for 'Site Fill'
print(df['Site Fill'].value_counts(dropna=False))


# Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Plot the histogram
df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)

# Display the histogram
# plt.show()
plt.savefig("_dummyPy045.png", bbox_inches="tight")
plt.clf()

# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt

# Create the boxplot
df.boxplot(column="initial_cost", by="Borough", rot=90)

# Display the plot
# plt.show()
plt.savefig("_dummyPy046.png", bbox_inches="tight")
plt.clf()

# Import necessary modules
import pandas as pd
import matplotlib.pyplot as plt

# Create and display the first scatter plot
df.plot(kind="scatter", x="initial_cost", y="total_est_fee", rot=70)
# plt.show()
plt.savefig("_dummyPy047.png", bbox_inches="tight")
plt.clf()

```
  
  
**NYC Open Data Sub-sample (Building Permits - Existing Zoning Sq Ft)**:  
![](_dummyPy045.png)

**NYC Open Data Sub-sample (Building Permits - Initial Cost by Borough)**:  
![](_dummyPy046.png)

**NYC Open Data Sub-sample (Building Permits)**:  
![](_dummyPy047.png)


  
***
  
Chapter 2 - Tidying data for analysis  
  
Tidy data per the Hadley Wickham paper - "standard way to organize data within a dataset":  
  
* Columns should represent separate variables - if the values are in the column names, then the data need to be melted  
	* pd.melt(frame=myFrame, id_vars=["myID"], value_vars=["myValues"])  
    * id_vars will be held fixed; these are the columns that will not be changed during the melting process  
    * If the value_vars columns are not specified, Python will assume you want to melt all columns other than the ID variables  
    * The default outputs new columns "variable" and "value", though these can be over-ridden using var_name="myVarName" and value_name="myValName" inside melt  
* Rows should represent individual observations  
* Observational units should form tables  
* There are some trade-offs in reporting vs. data analysis, and tidying the data is primarily for making it easier to analyze  
  
Pivoting data is the opposite of melting; turn unique values in to separate columns (assuming again that the DataFrame, df, already exists):  
  
* To pivot the data, use df.pivot(index="myIndex", columns="myColumns", values="myValues")  
	* index is the columns to be fixed  
    * columns is what is to be pivoted in the new columns  
    * values is what is to be placed in to the new columns  
* If there is a duplicate value, the "pivot table" is required, specifying to Python how the duplicated value should be managed  
	* df.pivot_table(index="myIndex", columns="myColumns", values="myValues", aggfunc=myFunc) # same as .pivot, but specifying something like np.mean for how to handle duplicates  
  
Beyond melt and pivot - example from the Wickham data of having a single variable that combines sex and age-group (TB data) - common shape for reporting, but less than ideal for analysis:  
  
* First, melt the data so that all these columns become a single "variable" that contains the associated "value"  
* Second, create new variables for sex and age from the current variable named "variable"  
	* tb_melt["sex"] = tb_melt["variable"].str[0]  # will extract the first character, which is the sex in this case  
    * tb_melt["age"] = tb_melt["variable"].str[1:]  # will extract all but the first character  
  
Example code includes:  
```{r engine='python'}

# THIS SEEMS TO BE THE STANARD R datasets file as a pandas
# Saved airquality.csv to the ./PythonInputFiles

myPath = "./PythonInputFiles/"



import pandas as pd
import numpy as np

airquality = pd.read_csv(myPath + "airquality.csv")


# Print the head of airquality
print(airquality.head())

# Melt airquality: airquality_melt
airquality_melt = pd.melt(airquality, id_vars=["Month", "Day"])

# Print the head of airquality_melt
print(airquality_melt.head())


# Print the head of airquality
print(airquality.head())

# Melt airquality: airquality_melt
airquality_melt = pd.melt(airquality, id_vars=["Month", "Day"], var_name="measurement", value_name="reading")

# Print the head of airquality_melt
print(airquality_melt.head())


# Print the head of airquality_melt
print(airquality_melt.head())

# airquality_melt.pivot() would bomb out on this; not sure why . . . (may be due to having 2+ variables in the index
# Pivot airquality_melt: airquality_pivot
airquality_pivot = airquality_melt.pivot_table(index=["Month", "Day"], columns="measurement", values="reading")

# Print the head of airquality_pivot
print(airquality_pivot.head())


# Print the index of airquality_pivot
print(airquality_pivot.index)

# Reset the index of airquality_pivot: airquality_pivot
airquality_pivot = airquality_pivot.reset_index()

# Print the new index of airquality_pivot
print(airquality_pivot.index)

# Print the head of airquality_pivot
print(airquality_pivot.head())


# Pivot airquality_dup: airquality_pivot
# keyRows = [x for x in range(len(airquality.index))] + [2, 4, 6, 8, 10]
# airquality_dup = airquality.iloc[keyRows, :]
airquality_pivot = airquality_melt.pivot_table(index=["Month", "Day"], columns="measurement", values="reading", aggfunc=np.mean)

# Reset the index of airquality_pivot
airquality_pivot = airquality_pivot.reset_index()

# Print the head of airquality_pivot
print(airquality_pivot.head())

# Print the head of airquality
print(airquality.head())


# tb is 201x18 with variables ['country', 'year', 'm014', 'm1524', 'm2534', 'm3544', 'm4554', 'm5564', 'm65', 'mu', 'f014', 'f1524', 'f2534', 'f3544', 'f4554', 'f5564', 'f65', 'fu']
# year is set to be always 2000 with fu and mu always NaN
# Create dummy data for tb (just use 3 countries and the 014 and 1524 columns)
tb = pd.DataFrame( { "country":["USA", "CAN", "MEX"] , "year":2000 , "fu":np.nan , "mu":np.nan , "f014":[2, 3, 4] , "m014":[5, 6, 7] , "f1524": [8, 9, 0] , "m1524":[1, 2, 3] } )


# Melt tb: tb_melt
tb_melt = pd.melt(tb, id_vars=["country", "year"])

# Create the 'gender' column
tb_melt['gender'] = tb_melt.variable.str[0]

# Create the 'age_group' column
tb_melt['age_group'] = tb_melt.variable.str[1:]

# Print the head of tb_melt
print(tb_melt.head())  # Is now 3,216 x 6 ['country', 'year', 'variable', 'value', 'gender', 'age_group']


# Ebola dataset is available at https://data.humdata.org/dataset/ebola-cases-2014
# Variables are split by an underscore 'Date', 'Day', 'Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone', 'Cases_Nigeria', 'Cases_Senegal', 'Cases_UnitedStates', 'Cases_Spain', 'Cases_Mali', 'Deaths_Guinea', 'Deaths_Liberia', 'Deaths_SierraLeone', 'Deaths_Nigeria', 'Deaths_Senegal', 'Deaths_UnitedStates', 'Deaths_Spain', 'Deaths_Mali'

# Downloaded file, then manipulated to be like the above as follows:
# ebola_test = pd.read_csv(myPath + "ebola_data_db_format.csv")
# ebola_test["UseCountry"] = ebola_test["Country"].str.replace(" ", "")
# ebola_test["UseCountry"] = ebola_test["UseCountry"].str.replace("2", "")
# keyIndic = ["Cumulative number of confirmed Ebola deaths", "Cumulative number of confirmed Ebola cases"]
# keyBool = [x in keyIndic for x in ebola_test["Indicator"]]
# ebola_test = ebola_test.loc[keyBool, :]
# indicMap = {keyIndic[0]:"Deaths", keyIndic[1]:"Cases"}
# ebola_test["UseIndicator"] = ebola_test["Indicator"].map(indicMap)
# ebolaPre = ebola_test[["Date", "UseCountry", "UseIndicator", "value"]]
# ebolaPre["CI"] = ebolaPre["UseIndicator"] + "_" + ebolaPre["UseCountry"]
# ebolaSave = ebolaPre.pivot_table(index="Date", columns="CI", values="value", aggfunc="max").fillna(method="ffill").fillna(0)
# ebolaSave.to_csv(myPath + "ebola.csv")

ebola = pd.read_csv(myPath + "ebola.csv", parse_dates=["Date"])


# Melt ebola: ebola_melt
ebola_melt = pd.melt(ebola, id_vars=["Date"], var_name="type_country", value_name="counts")

# Create the 'str_split' column
ebola_melt['str_split'] = ebola_melt["type_country"].str.split("_")

# Create the 'type' column
ebola_melt['type'] = ebola_melt['str_split'].str.get(0)

# Create the 'country' column
ebola_melt['country'] = ebola_melt['str_split'].str.get(1)

# Print the head of ebola_melt
print(ebola_melt.head())


# ebola_melt.to_csv(myPath + "ebola_melt.csv", index=False)
# Run outside of this shell so that the file is accessible later

```
  
  
***
  
Chapter 3 - Combining data for analysis  
  
Concatenating data - data may be in separate files (too many records, time series data by day, etc.), while you want to combine it:  
  
* Concatenating data (similar to rbind in R) in Python leaves the original row indices untouched, which can induce duplicate indices  
* The pd.concat([myFileList]) will place the frames together in a single frame # requires that import pandas as pd was called previously  
	* Using ignore_index=True inside pd.concat() will re-cast the row indices from 0 to n-1  
    * To instead concatenate columns (similar to cbind in R), declare the axis=1 option inside pd.concat()  
  
Finding and concatenating data - issue of many files needing to be concatenated:  
  
* The glob function from the glob library helps to find files based on a consistent search pattern  
	* The wildcards * and ? Are both available, with * being any number while ? Is a single character  
    * Basic usage would be glob.glob("mySearchString")  
* The basic plan would be to 1) load all the files to pandas, and then 2) make a list of the DataFrame names for concatenation  
  
Merge data - extension on concatenation (which is more piecing something back together that was originally one piece but became split):  
  
* Merging can manage joins of tables that never were one piece, combining disparate data based on common columns  
* The merge syntax is pd.merge(left=leftFrame, right=rightFrame, how=, on=, left_on=, right_on=)  # default is an INNER JOIN and need to specify either on= (common variables) or left_on=/right_on=  
	* The defaults for on, left_on, and right_on are all None  
    * The default is for how="inner" though options for "left", "right", and "outer" can also be declared  
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

import pandas as pd
import numpy as np


# uber datasets are a small subset from within http://data.beta.nyc/dataset/uber-trip-data-foiled-apr-sep-2014
# downloaded file "Uber-Jan-Feb-FOIL.csv" to myPath


uber = pd.read_csv(myPath + "Uber-Jan-Feb-FOIL.csv")

cuts = [round(len(uber.index) / 3), round(2 * len(uber.index) / 3)]

uber1 = uber.iloc[:cuts[0], :]
uber2 = uber.iloc[cuts[0]:cuts[1], :]
uber3 = uber.iloc[cuts[1]:, :]

# Save outside of this routine
# uber1.to_csv(myPath + "uber1.csv", index=False)
# uber2.to_csv(myPath + "uber2.csv", index=False)
# uber3.to_csv(myPath + "uber3.csv", index=False)


# Concatenate uber1, uber2, and uber3: row_concat
row_concat = pd.concat([uber1, uber2, uber3])

# Print the shape of row_concat
print(row_concat.shape)

# Print the head of row_concat
print(row_concat.head())

print(np.sum(row_concat != uber))


# ebola_melt is 1,952x4 of Date-Day-status_country-counts
# status_country is 1,952x2 of status-country (the previous status_country has been string split)
# Create this from the file in the previous exercise
ebola_melt = pd.read_csv(myPath + "ebola_melt.csv", parse_dates=["Date"])
ebola_melt.columns = ["Date", "status_country", "counts", "str_split", "status", "country"]

status_country = ebola_melt[["status", "country"]]
ebola_melt = ebola_melt[["Date", "status_country", "counts"]]

# Concatenate ebola_melt and status_country column-wise: ebola_tidy
ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)

# Print the shape of ebola_tidy
print(ebola_tidy.shape)

# Print the head of ebola_tidy
print(ebola_tidy.head())


# Has files ['uber-raw-data-2014_06.csv', 'uber-raw-data-2014_04.csv', 'uber-raw-data-2014_05.csv'] available
# Date/Time-Lat-Lon-Base
# Import necessary modules
import glob
import pandas as pd

# Write the pattern: pattern
# This is designed to get the uber1.csv, uber2.csv, and uber3.csv files
pattern = myPath + 'uber?.csv'

# Save all file matches: csv_files
csv_files = glob.glob(pattern)

# Print the file names
print(csv_files)

# Load the second file into a DataFrame: csv2
csv2 = pd.read_csv(csv_files[1])

# Print the head of csv2
print(csv2.head())


# Create an empty list: frames
frames = []

#  Iterate over csv_files
for csv in csv_files:
    
    #  Read csv into a DataFrame: df
    df = pd.read_csv(csv)
    
    # Append df to frames
    frames.append(df)

# Concatenate frames into a single DataFrame: uber
uber = pd.concat(frames)

# Print the shape of uber
print(uber.shape)

# Print the head of uber
print(uber.head())


# site is a 3x3 with name-lat-long - name=["DR-1", "DR-3", "MSK-4"], lat=[-50, -47, -48.9], lon=[-129, -127, -123.4]
# visited is a 3x3 with ident-site-dated - ident=[619, 734, 837], site=["DR-1", "DR-3", "MSK-4"], dated=["1927-02", "1939-01", "1932-01"]

site = pd.DataFrame( { "name":["DR-1", "DR-3", "MSK-4"], "lat":[-50, -47, -48.9], "lon":[-129, -127, -123.4] } )
visited = pd.DataFrame( { "ident":[619, 734, 837], "site":["DR-1", "DR-3", "MSK-4"], "dated":["1927-02", "1939-01", "1932-01"] } )

# Merge the DataFrames: o2o
o2o = pd.merge(left=site, right=visited, left_on=["name"], right_on=["site"])

# Print o2o
print(o2o)


# now make visited 8x3 with ident=[619, 622, 734, 735, , 751, 752, 837, 844], site=['DR-1', 'DR-1', 'DR-3', 'DR-3', 'DR-3', 'DR-3', 'MSK-4', 'DR-1'], dated=['1927-02-08', '1927-02-10', '1939-01-07', '1930-01-12', '1930-02-26', nan, '1932-01-14', '1932-03-22']

visited = pd.DataFrame( {"ident":[619, 622, 734, 735, 751, 752, 837, 844], "site":['DR-1', 'DR-1', 'DR-3', 'DR-3', 'DR-3', 'DR-3', 'MSK-4', 'DR-1'], "dated":['1927-02-08', '1927-02-10', '1939-01-07', '1930-01-12', '1930-02-26', np.nan, '1932-01-14', '1932-03-22']} )

# Merge the DataFrames: m2o
m2o = pd.merge(left=site, right=visited, left_on=["name"], right_on=["site"])

# Print m2o
print(m2o)


# add an additional frame surveyed which is 21x4 with taken-person-quant-reading (taken matched ident in file visited)
# Merge site and visited: m2m
# m2m = pd.merge(left=site, right=visited, left_on=["name"], right_on=["site"])

# Merge m2m and survey: m2m
# m2m = pd.merge(left=m2m, right=survey, left_on=["ident"], right_on=["taken"])

# Print the first 20 lines of m2m
# print(m2m.head(20))

```
  
  
***
  
Chapter 4 - Cleaning data for analysis  
  
Data types and conversions - can see the data types using the df.dtypes attribute of a pandas DataFrame df:  
  
* Often helpful to convert strings to numerics or vice versa  
* The .astype() method will allow for type conversions  
	* df["a"] = df["a"].astype(str) will create a string variable  
    * df["a"] = df["a"].astype("category") will create a categorical (factor) variable  
    * df["a"] = pd.to_numeric(df["a"], errors="coerce") will create a numeric variable, with NaN written where the string is not a sensible numeric  
  
Using regular expressions to clean strings - the most common form of data cleaning is string manipulation:  
  
* As an example, monetary values can be represented in many ways  
* The "re" library is used for pattern matching (using regular expressions) within strings  
	* The asterisk (*) means "0 or more times"  
    * The plus sign (+) means "1 or more times"  
    * The \d represents any digit, broadly the same as [0-9], so \d* means zero or more consecutive digits  
    * The \$ means the actual "$" symbol, with the back-slash escaping the symbol from its default meaning as "end-of-string"; so \$\d* will match the dollar sign followed by 0+ digits  
    * The \. Means the actual "." symbol, with the back-slash escaping the default meaning of the period; so \$\d*\.\d* will maktc the dollar sign followed by 0+ digits followed by the period followed by 0+ digits 
    * The {2} means to have exactly two of the items; sp \$\d*\.\d{2} will match "$[0+ digits].[2 digits]"  
    * The carat means "at the start" and the dollar means "at the end", so ^\$\d*.\d{2}$ will not match anything with 3+ digits after the period, nor anything with text before the $  
* Best practices for using an re are to 1) compile the pattern first, then 2) apply the compiled pattern to the pattern  
	* pattern = re.compile("myRegEx") will compile the specified regular expression for use elsewhere  
    * result = pattern.match("myText") will then pull out the relevant matches to the compiled pattern  
    * bool(result) will return a True/False as to whether we made any matches  
  
Using functions to clean data - in particular, the .apply() function:  
  
* df.apply(myFunction, myAxis=)  # axis 0 is columns, axis 1 is rows, etc.  
* Example using a few columns with dollar data - check that valid numbers, remove the dollar sign, cast as numeric (NaN if invalid data), store as new column  
	* from numpy import NaN  
    * myVar.replace("$", "") will replace the "$" with "" (more or less, remove the leading dollar signs  
* Frequently, a function will be passed an entire row of data, so the cleaning can be done for all required variables in the same function  
	* df["myNewVar"] = df.apply(myFunc, axis=1, pattern=pattern)  # will pass the rows as argument 1 and pattern as argument 2 to myFunc, once for each row  
  
Duplicate and missing data - can skew results in undesirable manners:  
  
* The df.drop_duplicates() method will remove any rows that are exact duplicates of each other  
* The df.info() method is a nice way to see how much missing data there is by variable  
* The df.dropna() method will drop any rows that have any NaN included in them (keeps only the complete.cases() in R syntax)  
* The df.fillna() allows for replacing either a user-provided value or a calculated value (such as mean/median for the variable where it exists)  
	* Can run through multiple columns at the same time by encoding a list; df[[myList]] = df[[myList]].fillna(0) will make NaN in to zero in every column specified in myList  
  
Testing with asserts - early detection for problems that may plague the analysis later:  
  
* More or less, assert myExpression does nothing if True and errors out if False  
  
Example code includes:  
```{r engine='python'}

# The tips data is available at https://github.com/mwaskom/seaborn-data/blob/master/tips.csv

myPath = "./PythonInputFiles/"

import pandas as pd
import numpy as np

tips = pd.read_csv(myPath + "tips.csv")


# Convert the sex column to type 'category'
tips.sex = tips["sex"].astype("category")

# Convert the smoker column to type 'category'
tips.smoker = tips["smoker"].astype("category")

# Print the info of tips
print(tips.info())


# Convert 'total_bill' to a numeric dtype
tips['total_bill'] = pd.to_numeric(tips["total_bill"], errors="coerce")

# Convert 'tip' to a numeric dtype
tips['tip'] = pd.to_numeric(tips["tip"], errors="coerce")

# Print the info of tips
print(tips.info())


# Import the regular expression module
import re

# Compile the pattern: prog
prog = re.compile('\d{3}-\d{3}-\d{4}')

# See if the pattern matches
result = prog.match('123-456-7890')
print(bool(result))

# See if the pattern matches
result = prog.match("1123-456-7890")
print(bool(result))


# Import the regular expression module
import re

# Find the numeric values: matches
matches = re.findall('\d+', 'the recipe calls for 10 strawberries and 1 banana')

# Print the matches
print(matches)


# Write the first pattern
pattern1 = bool(re.match(pattern='\d{3}-\d{3}-\d{4}', string='123-456-7890'))
print(pattern1)

# Write the second pattern
pattern2 = bool(re.match(pattern='\$\d*\.\d{2}', string='$123.45'))
print(pattern2)

# Write the third pattern
pattern3 = bool(re.match(pattern='[A-Z]\w*', string='Australia'))
print(pattern3)


import numpy

# Define recode_sex()
def recode_sex(sex_value):
    
    # Return 1 if sex_value is 'Male'
    if sex_value == "Male":
        return 1
    
    # Return 0 if sex_value is 'Female'    
    elif sex_value == "Female":
        return 0
    
    # Return np.nan    
    else:
        return np.nan

# Apply the function to the sex column
tips['sex_recode'] = tips["sex"].apply(recode_sex)


# Create the total_dollar field
tips["total_dollar"] = "$" + tips["total_bill"].astype(str)

# Write the lambda function using replace
tips['total_dollar_replace'] = tips["total_dollar"].apply(lambda x: x.replace('$', ''))

# Write the lambda function using regular expressions
tips['total_dollar_re'] = tips["total_dollar"].apply(lambda x: re.findall('\d+\.\d+', x))

# Print the head of tips
print(tips.head())


# DO NOT HAVE DATASET "tracks"
# Create the new DataFrame: tracks
# tracks = billboard[['year', 'artist', 'track', 'time']]

# Print info of tracks
# print(tracks.info())

# Drop the duplicates: tracks_no_duplicates
# tracks_no_duplicates = tracks.drop_duplicates()

# Print info of tracks
# print(tracks_no_duplicates.info())


# SEEMS TO BE "airquality" as per the R datasets package
# Previously saved as myPath + "airquality.csv"
airquality = pd.read_csv(myPath + "airquality.csv")


# Calculate the mean of the Ozone column: oz_mean
oz_mean = airquality["Ozone"].mean()

# Replace all the missing values in the Ozone column with the mean
airquality['Ozone'] = airquality["Ozone"].fillna(oz_mean)

# Print the info of airquality
print(airquality.info())


# DO NOT HAVE FRAME ebola - 122 x 18 of Date-Day-Cases_[8 countries]-Deaths_[8 countries]
# Use the version saved previously
ebola = pd.read_csv(myPath + "ebola.csv", parse_dates=["Date"])

# Assert that there are no missing values
assert ebola.notnull().all().all()

# Assert that all values are >= 0
assert (ebola >= 0).all().all()

```
  
  
***
  
Chapter 5 - Case Study  
  
Putting it all together - Gapminder data (NPO supporting global sustainable development):  
  
* Dataset will be life expectancy by country and year  
* Goal is to clean and combine all of the data so there is a single file ready for further data analysis  
  
Initial impressions of the data - depending on the analysis needs, can melt (columns to rows) or pivot (new columns from column data) the data:  
  
* Can check the column types by using df.dftypes  
* Can change column types using .to_numeric() or .astype()  
* Can save a CSV using df.to_csv(myFile)  
  
Example code includes:  
```{r engine='python'}

myPath = "./PythonInputFiles/"

# The DataFrame g1800s is a life expectancy table of 260 x 101 - "Life Expectancy" (country) followed by "1800" through "1899"
# Copied data from https://docs.google.com/spreadsheets/d/1H3nzTwbn8z4lJ5gJ_WfDgCeGEXK3PVGcNjQ_U5og8eo/pub as accessed from http://www.gapminder.org/data/ to myPath + "gapminder_lifeExp_1800_1916.xlsx"

import pandas as pd
gapExcel = pd.read_excel(myPath + "gapminder_lifeExp_1800_1916.xlsx")


# Convert column labels to text
gapExcel.columns = gapExcel.columns.astype(str)
assert gapExcel.columns[0] == "Life expectancy"

# Create booleans for 1800s, 1900s, and 2000s, including "Life expectancy" (country columns) as true in all
col1800s = gapExcel.columns.str.startswith("18")
col1900s = gapExcel.columns.str.startswith("19")
col2000s = gapExcel.columns.str.startswith("20")
col1800s[0] = True
col1900s[0] = True
col2000s[0] = True

# Create g1800s, g1900s, g2000s
g1800s = gapExcel.loc[:, col1800s]
g1900s = gapExcel.loc[:, col1900s]
g2000s = gapExcel.loc[:, col2000s]


# Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Create the scatter plot
g1800s.plot(kind="scatter", x="1800", y="1899")

# Specify axis labels
plt.xlabel('Life Expectancy by Country in 1800')
plt.ylabel('Life Expectancy by Country in 1899')

# Specify axis limits
plt.xlim(20, 55)
plt.ylim(20, 55)

# Display the plot
# plt.show()
plt.savefig("_dummyPy048.png", bbox_inches="tight")
plt.clf()


import pandas as pd
import numpy as np


def check_null_or_valid(row_data):
    """Function that takes a row of data,
    drops all missing values,
    and checks if all remaining values are greater than or equal to 0
    """
    no_na = row_data.dropna()[1:-1]
    numeric = pd.to_numeric(no_na)
    ge0 = numeric >= 0
    return ge0

# Check whether the first column is 'Life expectancy'
assert g1800s.columns[0] == "Life expectancy"

# Check whether the values in the row are valid
assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()

# Check that there is only one instance of each country
assert g1800s['Life expectancy'].value_counts()[0] == 1


# Also frames g1900s as 260x101 and g2000s as 260x18
# Concatenate the DataFrames row-wise
gapminder = pd.concat([g1800s, g1900s, g2000s])

# Print the shape of gapminder
print(gapminder.shape)

# Print the head of gapminder
print(gapminder.head())


# Melt gapminder: gapminder_melt
gapminder_melt = pd.melt(gapminder, id_vars="Life expectancy")

# Rename the columns
gapminder_melt.columns = ['country', 'year', 'life_expectancy']

# Print the head of gapminder_melt
print(gapminder_melt.head())


# Exercises used gapminder_melt as gapminder - keep copy before over-writing in case needed later
gapminder_old = gapminder.loc[:, :]
gapminder = gapminder_melt.loc[:, :]


# Convert the year column to numeric
gapminder.year = pd.to_numeric(gapminder.year)

# Test if country is of type object
assert gapminder.country.dtypes == np.object

# Test if year is of type int64
assert gapminder.year.dtypes == np.int64

# Test if life_expectancy is of type float64
assert gapminder.life_expectancy.dtypes == np.float64


# Create the series of countries: countries
countries = gapminder["country"]

# Drop all the duplicates from countries
countries = countries.drop_duplicates()

# Write the regular expression: pattern
pattern = '^[A-Za-z\.\s]*$'

# Create the Boolean vector: mask
mask = countries.str.contains(pattern)

# Invert the mask: mask_inverse
mask_inverse = ~mask  # The ~ is for inversion

# Subset countries using mask_inverse: invalid_countries
invalid_countries = countries.loc[mask_inverse]

# Print invalid_countries
print(invalid_countries)


# Assert that country does not contain any missing values
assert pd.notnull(gapminder.country).all()

# Assert that year does not contain any missing values
assert pd.notnull(gapminder.year).all()

# Print the shape of gapminder (prior to dropping NaN)
print(gapminder.shape)

# Drop the missing values
gapminder = gapminder.dropna()

# Print the shape of gapminder (after dropping NaN)
print(gapminder.shape)


# Add first subplot
plt.subplot(2, 1, 1) 

# Create a histogram of life_expectancy
gapminder["life_expectancy"].plot(kind="hist")

# Group gapminder: gapminder_agg
gapminder_agg = gapminder.groupby(by="year")["life_expectancy"].mean()

# Print the head of gapminder_agg
print(gapminder_agg.head())

# Print the tail of gapminder_agg
print(gapminder_agg.tail())


# Add second subplot
plt.subplot(2, 1, 2)

# Create a line plot of life expectancy per year
gapminder_agg.plot()

# Add title and specify axis labels
plt.title('Life expectancy over the years')
plt.ylabel('Life expectancy')
plt.xlabel('Year')

# Display the plots
plt.tight_layout()
# plt.show()
plt.savefig("_dummyPy049.png", bbox_inches="tight")
plt.clf()

# Save both DataFrames to csv files
gapminder.to_csv(myPath + "gapminder.csv")
gapminder_agg.to_csv(myPath + "gapminder_agg.csv")


```
  
  
**Gapminder Life Expectancy by Country (1899 vs 1800)**:  
![](_dummyPy048.png)

**Gapminder Life Expectancy**:  
![](_dummyPy049.png)


  