---
title: "Data Camp Insights"
author: "davegoblue"
date: "June 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)

```

## Background and Overview  
DataCamp offer interactive courses related to R Programming.  While some is review, it is helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  This document summarizes a few key insights from:  

This document is currently split between _v003 and _v003_a and _v003_b and _v003_c due to the need to keep the number of DLL that it opens below the hard-coded maximum.  This introductory section needs to be re-written, and the contents consolidated, at a future date.
  
* R Programming (Introductory R, Intermediate R, Writing Functions in R, Object Oriented Programming in R, Introduction to Tidyverse)  
* Importing and Cleaning Data (Cleaning Data in R, Importing Data in to R)  
* Data Manipulation (dplyr, data.table, xts/zoo, dplyr joins, cases for EDA/Time Series/Pitch Data)  
* Data Visualization (base, ggplot2 parts I/II/III, ggvis, geospatial)  
* Statistics (8 refresher modules)  
* Machine Learning (3 modules + 1 text mining case)  
* R Studio (2 parts) and R Markdown (1 module)  
  
The original DataCamp_Insights_v001 and DataCamp_Insights_v002 documents have been split for this document:  
  
* This DataCamp_Insights_v003 document contains evolving sections on R Programming, Machine Learning, and RStudio / R Markdown  
* Importing and Cleaning Data components have been moved to DataCamp_ImportClean_v002  
* Data Manipulation components have been moved to DataCamp_DataManipulation_v002  
* Visualization components have been moved to DataCamp_Visualization_v002  
* Statistics components have been moved to DataCamp_Statistics_v002  
  
  
***
  

### _Hierarchical and Mixed Effects Models_  
  
Chapter 1 - Overview and Introduction  
  
What is a hierarchical model?  
  
* Hierarchical data is nested within itself, and can be analyzed using the lme package  
	* Example of students in a classroom - may not all be independent of each other due to teacher quality, building conditions, etc.  
    * Hierarchical models can help with pooling means across small sample sizes  
    * Repeated measurements (test scores each year) are also a common example of data that are not truly independent  
* Hierarchical models can include nested models and multi-level models  
* Regression frameworks can include pool information and random effects (vs. fixed effects) and mixed-effects and linear mixed-effects  
* Repeated sampling can have repeated measures modeling  
  
Parts of a regression:  
  
* Linear regression and linear model can be used interchangeably for this course - epsilon is the error term, assumed to be normal with zero mean and constant variance  
* The linear model in R is closely related to analysis of variance (ANOVA)  
	* lm (y ~ x, myData)  
    * anova( lm (y ~ x, myData) )  
* The most basic regression has an intercept, a slope, a single predictor, and an error term  
	* The concept can be extended to multiple regression with additional predictors  
* There are some limitations to the multiple regression approach  
	* Parameter estimates can be very sensitive to other variables - Simpson's paradox and the like  
    * Need to note that the regression coefficient is "after controlling for . . . " (all the other variables)  
    * Interaction terms can be important as well  
* Regressions in R for an intercept for every group are called as lm(y ~ x - 1)  
* The interaction term x1*x2 is the same as x1 + x2 + x1:x2  
  
Random effects in regression:  
  
* Nested relationships tend to be hierarchical in nature - students are part of classes are part of schools and the like  
	* Mathematically, this is referred to as a mapping among the distributions  
* The algebraic representation is that y ~ B*x + eps, with B ~ N(mu, sigma**2)  
	* library(lme4) is the best packages for this in R  
    * lme4::lmer(y ~ x + (1|randomGroup), data=myData)  
    * lme4::lmer(y ~ x + (randomSlope|randomGroup), data=myData)  
  
School data:  
  
* Appliciation of multi-level models to school data - influence of sex, teacher training, plotting parameter estmates  
  
Example code includes:  
```{r}

rawStudent <- read.csv("./RInputFiles/classroom.csv")

studentData <- rawStudent %>%
    mutate(sex=factor(sex, labels=c("male", "female")), minority=factor(minority, labels=c("no", "yes")))


# Plot the data
ggplot(data = studentData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Fit a linear model
summary( lm(mathgain ~ housepov , data = studentData))


# I have aggregated the data for you into two new datasets at the classroom- and school-levels (As a side note, if you want to learn how to aggregate data, the dplyr or data.table courses teach these skills)
# We will also compare the model outputs across all three outputs
# Note: how we aggregate the data is important
# I aggregated the data by taking the mean across the student data (in pseudo-code: mean(mathgain) by school or mean(mathgain) by classroom), 
# but another reasonable method for aggregating the data would be to aggregate by classroom first and school second

classData <- studentData %>%
    group_by(schoolid, classid) %>%
    summarize_at(vars(mathgain, mathprep, housepov, yearstea), mean, na.rm=TRUE)
str(classData)

schoolData <- studentData %>%
    group_by(schoolid) %>%
    summarize_at(vars(mathgain, mathprep, housepov, yearstea), mean, na.rm=TRUE)
str(schoolData)


# First, plot the hosepov and mathgain at the classroom-level from the classData data.frame
ggplot(data = classData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Second, plot the hosepov and mathgain at the school-level from the schoolData data.frame
ggplot(data = schoolData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Third, compare your liner regression results from the previous expercise to the two new models
summary( lm(mathgain ~ housepov, data = studentData)) ## student-level data
summary( lm(mathgain ~ housepov, data = classData)) ## class-level data
summary( lm(mathgain ~ housepov, data = schoolData)) ## school-level data


# Plot the means of your data, predictor is your x-variable, response is your y-variable, and intDemo is your data.frame
intDemo <- data.frame(predictor=factor(c(rep("a", 5), rep("b", 5), rep("c", 5))), 
                      response=c(-1.207, 0.277, 1.084, -2.346, 0.429, 5.759, 4.138, 4.18, 4.153, 3.665, 9.046, 8.003, 8.447, 10.129, 11.919)
                      )
str(intDemo)


ggIntDemo <- ggplot(intDemo, aes(x = predictor, y = response) ) +
    geom_point() +
    theme_minimal() + stat_summary(fun.y = "mean", color = "red",
                                   size = 3, geom = "point") +
    xlab("Intercept groups")
print(ggIntDemo)

# Fit a linear model to your data where response is "predicted by"(~) predictor
intModel <- lm( response ~ predictor - 1 , data = intDemo)
summary(intModel)


extractAndPlotResults <- function(intModel){
    intCoefPlot <- broom::tidy(intModel)
    intCoefPlot$term <- factor(gsub("predictor", "", intCoefPlot$term))

    plotOut <- ggIntDemo + geom_point(data = intCoefPlot,
                           aes(x = term, y = estimate),
                           position = position_dodge(width = 0.4),
                           color = 'blue', size = 8, alpha = 0.25)
    print(plotOut)
}


# Run the next code that extracts out the model's coeffiecents and plots them 
extractAndPlotResults(intModel)


multIntDemo <- data.frame(group=factor(c(rep("a", 5), rep("b", 5), rep("c", 5))), 
                          x=rep(0:4, times=3), 
                          intercept=c(4.11, -1.69, 1.09, 1.9, 1.21, 4.63, 10.29, 4.67, 12.06, 4.78, 15.22, 19.15, 4.44, 8.88, 9.47), 
                          response=c(4.11, 2.31, 9.09, 13.9, 17.21, 4.63, 14.29, 12.67, 24.06, 20.78, 15.22, 23.15, 12.44, 20.88, 25.47)
                          )
str(multIntDemo)

plot_output1 <- function(out1){
    ggmultIntgDemo <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', fill = NA, color = 'orange', size = 3)
    print(ggmultIntgDemo)
}

plot_output2 <- function(out2){
    out2Tidy <- broom::tidy(out2)
    out2Tidy$term <- gsub("group", "", out2Tidy$term)
    out2Plot <- data.frame(group = pull(out2Tidy[ -1, 1]),
                           slope = pull(out2Tidy[ 1, 2]),
                           intercept = pull(out2Tidy[ -1, 2])
                           )
    ggmultIntgDemo2 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group))+
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        geom_abline(data = out2Plot,
                    aes(intercept = intercept, slope = slope, color = group))
    print(ggmultIntgDemo2)
}

plot_output3 <- function(out3){
    ggmultIntgDemo3 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', aes(color = group), fill = NA)
    print(ggmultIntgDemo3)
}

# First, run a model without considering different intercept for each group
out1 <- lm( response ~ x, data=multIntDemo )
summary(out1)
plot_output1(out1)

# Considering same slope but different intercepts
out2 <- lm( response ~ x + group - 1, data=multIntDemo )
summary(out2)
plot_output2(out2)

# Consdering different slope and intercept for each group (i.e., an interaction)
out3 <- lm( response ~ x + group - 1 + x:group, multIntDemo)
summary(out3)
plot_output3(out3)


multIntDemo$intercept <- c(-0.87, 3.35, 1.25, 0.88, -1.05, 4.55, 1.22, 3.34, 1.26, 3.75, 7.71, 9.59, 2.28, 1.9, 13.35)
multIntDemo$response <- c(-0.87, 6.35, 7.25, 9.88, 10.95, 4.55, 4.22, 9.34, 10.26, 15.75, 7.71, 12.59, 8.28, 10.9, 25.35)

# Run model
outLmer <- lme4::lmer( response ~ x + ( 1 | group), multIntDemo)

# Look at model outputs 
summary( outLmer )
broom::tidy( outLmer )


extractAndPlotOutput <- function(outLmer, slope=3){
    multIntDemo$lmerPredict <- predict(outLmer)
    ggmultIntgDemo2 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group))+
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        geom_abline(data = multIntDemo,
                    aes(intercept = intercept, slope = slope, color = group))
    outPlot <-  ggmultIntgDemo2 +
                geom_line( data =  multIntDemo,
                      aes(x = x, y = lmerPredict, color = group),
                      linetype = 2)
    print(outPlot)
}


# Extract predictor variables and plot
extractAndPlotOutput(outLmer)


# Random effect slopes
multIntDemo$response <- c(-0.72, 1.5, 4.81, 6.61, 13.62, 10.21, 9.64, 11.91, 16.39, 16.97, 8.76, 14.79, 15.83, 15.27, 17.36)
multIntDemo$intercept <- c(-0.72, -1.5, -1.19, -2.39, 1.62, 10.21, 6.64, 5.91, 7.39, 4.97, 8.76, 11.79, 9.83, 6.27, 5.36)

outLmer2 <- lme4::lmer( response ~ ( x|group ), multIntDemo)
summary(outLmer2)
broom::tidy(outLmer2)


plotOutput <- function(outLmer2){
    multIntDemo$lmerPredict2 <- predict(outLmer2)
    ggmultIntgDemo3 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', aes(color = group), fill = NA)
    plotOut <- ggmultIntgDemo3 +
            geom_line( data =  multIntDemo,
                      aes(x = x, y = lmerPredict2, color = group),
                      linetype = 2)
    print(plotOut)
}


# Extract and plot
plotOutput(outLmer2)


# Mixed effect model
lmerModel <- lme4::lmer(mathgain ~ sex + 
                  mathprep + mathknow + (1|classid) +
                  (1|schoolid), data = studentData, na.action = "na.omit",
                  REML = TRUE)
summary(lmerModel)


extractAndPlot <- function(lmerModel){
    modelOutPlot <- broom::tidy(lmerModel, conf.int = TRUE)
    modelOutPlot <- modelOutPlot[ modelOutPlot$group =="fixed" &
                               modelOutPlot$term != "(Intercept)", ]
    plotOut <- ggplot(modelOutPlot, aes(x = term, y = estimate,
                             ymin = conf.low,
                             ymax = conf.high)) +
            theme_minimal() +
            geom_hline(yintercept = 0.0, color = 'red', size = 2.0) +
            geom_point() +
            geom_linerange() + coord_flip()
    print(plotOut)
}


# Extract and plot 
extractAndPlot(lmerModel)

```
  
  
  
***
  
Chapter 2 - Linear Mixed-Effect Models  
  
Linear mixed effect model - Birth rates data:  
  
* Small populations are highly sensitive to stochastic effects - if the mean is 1, a group of 5 might have 0 or 10  
* Questions about how counties may impact birth rates, over and above other demographic factors  
	* Example of plotting birth rate vs. county - will see both the highest and lowest birth rates in the smallest counties  
* Random effect syntax for the lmer model includes  
	* (1 | group) - random intercept with fixed mean  
    * (1 | g1/g2) - intercepts vary among g1 and g2 within g2  
    * (1 | g1) + (1 | g2) - random intercepts for two variables  
    * x + (x | g) - correlated random slope and intercept  
    * x + (x || g) - uncorrelated random slope and intercept  
    * See lme4 documentation for additional details  
  
Understanding and reporting the outputs of lmer:  
  
* The output from lmer is similar to the output from lm, but with some key differences - if using print(), will see  
	* The method used is REML - restricted maximum likelihood - which tends to solve better than maximum likelihood for these problems  
    * There is an REML convergence criteria, which can be a helpful diagnostic  
    * Can see the standard deviations for both the state and the residual, along with the number of observations  
    * Get the fixed effects coefficients in a similar form as lm()  
* The summary() call on lmer produces several additional outputs  
	* Residuals summary  
    * Fixed effects estimates include SE and t-values (but not p-values)  
    * Correlations of fixed effects  
* Can grab only the fixed effects using fixef(myLMERObject)  
	* Can grab only the random effects using ranef(myLMERObject), though these will not have confidence intervals  
    * The random effects confidence intervals could be estimated using bootstrapping or Bayesian methods per the author of lme4 - but actual random effects are just unobserved random variables rather than parameters  
* Can grab only the confidence intervals using confint(myLMERObject)  
* Need to be careful in reporting the results - figures vs. tables vs. in-line descriptions  
  
Statistical inference with Maryland crime data:  
  
* The Maryland crime data is available on data.gov - interesting for many public and private purposes  
* The null hypothsis test can be used with LMER - frequentist approach  
	* By default, lmer does not provide p-values, as there is ongoing debate as to the degrees of freedom and impact on reported results  
    * Can use lmerTest package to calculate and report on the p-values  
* Can use ANOVA to look at the variability explained by one model versus another model, and the associated degrees of freedom needed  
  
Example code includes:  
```{r}

# Read in births data
rawBirths <- read.csv("./RInputFiles/countyBirthsDataUse.csv")
countyBirthsData <- rawBirths
str(countyBirthsData)


# First, build a lmer with state as a random effect. Then look at the model's summary and the plot of residuals. 
birthRateStateModel <- lme4::lmer(BirthRate ~ (1|State), data=countyBirthsData)
summary(birthRateStateModel)
plot(birthRateStateModel)

# Next, plot the predicted values from the model ontop of the plot shown during the video.
countyBirthsData$birthPredictState <- predict(birthRateStateModel, countyBirthsData)
ggplot() + theme_minimal() +
    geom_point(data =countyBirthsData, aes(x = TotalPopulation, y = BirthRate)) + 
    geom_point(data = countyBirthsData, aes(x = TotalPopulation, y = birthPredictState),
               color = 'blue', alpha = 0.5
               )

# Include the AverageAgeofMother as a fixed effect within the lmer and state as a random effect
ageMotherModel <- lme4::lmer( BirthRate ~ AverageAgeofMother + (1|State), data=countyBirthsData)
summary(ageMotherModel)

# Compare the random-effect model to the linear effect model 
summary(lm(BirthRate ~ AverageAgeofMother, data = countyBirthsData))


# Include the AverageAgeofMother as a correlated random-effect slope parameter
ageMotherModelRandomCorrelated <- lme4::lmer(BirthRate ~ AverageAgeofMother + (AverageAgeofMother|State),
                       countyBirthsData)
summary(ageMotherModelRandomCorrelated)


# Include the AverageAgeofMother as a correlated random-effect slope parameter
ageMotherModelRandomUncorrelated <- lme4::lmer(BirthRate ~ AverageAgeofMother + 
                                                    (AverageAgeofMother || State), data=countyBirthsData
                                               )
summary(ageMotherModelRandomUncorrelated)


out <- ageMotherModelRandomUncorrelated

# Extract the fixed-effect coefficients
lme4::fixef(out)

# Extract the random-effect coefficients
lme4::ranef(out)

# Estimate the confidence intervals 
(ciOut <- confint(out))


# Technical note: Extracting out the regression coefficients from lmer is tricky (see discussion between the lmer and broom authors development)
# Extract out the parameter estimates and confidence intervals and manipulate the data
dataPlot <- data.frame(cbind( lme4::fixef(out), ciOut[ 4:5, ]))
rownames(dataPlot)[1] <- "Intercept"
colnames(dataPlot) <- c("mean", "l95", "u95")
dataPlot$parameter <- rownames(dataPlot)

# Print the new dataframe
print(dataPlot)

# Plot the results using ggplot2
ggplot(dataPlot, aes(x = parameter, y = mean,
                     ymin = l95, ymax = u95)) +
    geom_hline( yintercept = 0, color = 'red' ) +
    geom_linerange() + geom_point() + coord_flip() + theme_minimal()



# Read in crime data
rawCrime <- read.csv("./RInputFiles/MDCrime.csv")
MDCrime <- rawCrime
str(MDCrime)


plot1 <- ggplot(data = MDCrime, aes(x = Year, y = Crime, group = County)) +
    geom_line() + theme_minimal() +
    ylab("Major crimes reported per county")
print(plot1)

plot1 + geom_smooth(method = 'lm')


# Null hypothesis testing uses p-values to see if a variable is "significant"
# Recently, the abuse and overuse of null hypothesis testing and p-values has caused the American Statistical Association to issue a statement about the use of p-values
# Because of these criticisms and other numerical challenges, Doug Bates (the creator of the lme4 package) does not include p-values as part of his package
# However, you may still want to estimate p-values, because p-values are sill commonly used. Several packages exist, including the lmerTest package
# https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf

# Load lmerTest
# library(lmerTest)

# Fit the model with Year as both a fixed and random-effect
lme4::lmer(Crime ~ Year + (1 + Year | County) , data = MDCrime)

# Fit the model with Year2 rather than Year
out <- lme4::lmer(Crime ~ Year2 + (1 + Year2 | County) , data = MDCrime)

# Examine the model's output
summary(out)


## Build the Null model with only County as a random-effect
null_model <- lme4::lmer(Crime ~ (1 | County) , data = MDCrime)

## Build the Year2 model with Year2 as a fixed and random slope and County as the random-effect
year_model <- lme4::lmer(Crime ~ Year2 + (1 + Year2 | County) , data = MDCrime)

## Compare the two models using an anova
anova(null_model, year_model)

```
  
  
  
***
  
Chapter 3 - Generalized Linear Mixed-Effect Models  
  
Crash course on GLMs - relaxing the assumptions around normality of the residuals:  
  
* Non-normal data can be transformed using arcsin or the like  
* However, with advances in methodology, it is possible to more directly model the data using binomial and poisson distributions  
* The basic glm call is glm(y ~ x, family="")  # default is family="gaussian", which same as the lm()  
* The Poisson distribution is frequently best for count data, such as website visitors per hour - mean equals variance (generally best for small counts less than 30; can use normals for large counts)  
* For logistic regression, data can be entered in any of three formats  
	* Binary (y=0 or 1) - glm(y ~ x, family="binomial")  
    * Wilkinson-Rogers - glm(cbind(success, failure) ~ x, family="binomial")  
    * Weighted - glm(y ~ x, weights=weights, family="binomial")  
    * These methods differ primarily in the degrees of freedom (and thus deviance)  
  
Binomial data - modeling data with only two outcomes:  
  
* Traditional method for analysis includes looking at proportion of successes  
* The GLM allows for direct looks at the data - logistic regression (logit)  
* Binomial data can be fit using glmer(y ~ x + (1/group), family="error term")  
* The regression coefficients can be difficult to explain, sometimes leading to the use of odds ratios instead  
	* The odds ratio of 2.0 would mean 2:1 odds for that specific group  
  
Count data:  
  
* Examples like number of events per hour (website hits) or counts per area (birds)  
* The count data differs from the binomial in that there is no pre-specified upper boundary  
* While Chi-squared is often used for goodness of fit or test of association for count data, the Poisson GLM can be a nice alternative  
	* glm(y ~ x, family="poisson")  
    * glmer(y ~ x + (1|group), family="poisson")  
  
Example code includes:  
```{r cache=TRUE}

# In this case study, we will be working with simulated dose-response data
# The response is mortality (1) or survival (0) at the end of a study. During this exercise, we will fit a logistic regression using all three methods described in the video
# You have been given two datasets. dfLong has the data in a "long" format with each row corresponding to an observation (i.e., a 0 or 1)
# dfShort has the data in an aggregated format with each row corresponding to a treatment (e.g., 6 successes, 4 failures, number of replicates = 10, proportion = 0.6)

dfLong <- data.frame(dose=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10), 
                     mortality=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1)
                     )
str(dfLong)

dfShort <- dfLong %>% 
    group_by(dose) %>%
    summarize(mortality=sum(mortality), nReps=n()) %>%
    mutate(survival=nReps-mortality, mortalityP=mortality/nReps)
dfShort


# Fit a glm using data in a long format
fitLong <- glm(mortality ~ dose, data = dfLong, family = "binomial")
summary(fitLong)

# Fit a glm using data in a short format with two columns
fitShort <- glm( cbind(mortality , survival ) ~ dose , data = dfShort, family = "binomial")
summary(fitShort)

# Fit a glm using data in a short format with weights
fitShortP <- glm( mortalityP ~ dose , data = dfShort, weights = nReps , family = "binomial")
summary(fitShortP)


y <- c(0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 5, 1, 1)
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)


# Fit the linear model
summary(lm(y ~ x))

# Fit the generalized linear model
summary(glm(y ~ x, family = "poisson"))


# Often, we want to "look" at our data and trends in our data
# ggplot2 allows us to add trend lines to our data
# The defult lines are created using a technique called local regression
# However, we can specify different models, including GLMs
# During this exercise, we'll see how to plot a GLM

# Plot the data using jittered points and the default stat_smooth
ggplot(data = dfLong, aes(x = dose, y = mortality)) + 
    geom_jitter(height = 0.05, width = 0.1) +
    stat_smooth(fill = 'pink', color = 'red') 

# Plot the data using jittered points and the the glm stat_smooth
ggplot(data = dfLong, aes(x = dose, y = mortality)) + 
    geom_jitter(height = 0.05, width = 0.1) +
    stat_smooth(method = 'glm',  method.args = list(family = "binomial"))


# library(lmerTest)

df <- data.frame(dose=rep(rep(c(0, 2, 4, 6, 8, 10), each=20), times=3), 
                 mortality=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1), 
                 replicate=factor(rep(letters[1:3], each=120))
                 )
str(df)


glmerOut <- lme4::glmer(mortality ~ dose + (1|replicate), family = 'binomial', data = df)
summary(glmerOut)


# library(lmerTest)
# Fit the model and look at its summary 
# modelOut <- lme4::glmer( cbind(Purchases, Pass) ~ friend + ranking + (1|city), data = allData, family = 'binomial')
# summary( modelOut) 

# Compare outputs to a lmer model
# summary(lme4::lmer( Purchases/( Purchases + Pass) ~ friend + ranking + (1|city), data = allData))


# Run the code to see how to calculate odds ratios
# summary(modelOut) 
# exp(fixef(modelOut)[2])
# exp(confint(modelOut)[3, ])


# Load lmerTest
# library(lmerTest)


userGroups <- data.frame(group=factor(rep(rep(LETTERS[1:4], each=10), times=2)), 
                         webpage=factor(rep(c("old", "new"), each=40)), 
                         clicks=c(0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 3, 2, 3, 1, 2, 4, 2, 1, 0, 2, 0, 1, 2, 0, 2, 1, 1, 2, 4, 2, 8, 1, 1, 1, 2, 1, 1, 0, 0, 3, 0, 1, 4, 1, 2, 0, 1, 1, 0, 0, 3, 2, 0, 3, 1, 2, 2, 0, 2, 3, 1, 3, 2, 4, 4, 2, 1, 5, 2)
                         )
str(userGroups)


# Fit a Poisson glmer
summary( lme4::glmer(clicks ~ webpage + (1|group), family = 'poisson', data = userGroups))


# library(lmerTest)


rawIL <- read.csv("./RInputFiles/ILData.csv")
ILdata <- rawIL
str(ILdata)

# Age goes before year
modelOut <- lme4::glmer(count ~ age + year + (year|county), family = 'poisson', data = ILdata)
summary(modelOut)


# Extract out fixed effects
lme4::fixef(modelOut)

# Extract out random effects 
lme4::ranef(modelOut)


# Run code to see one method for plotting the data
ggplot(data = ILdata, aes(x = year, y = count, group = county)) +
    geom_line() +
    facet_grid(age ~ . ) +
    stat_smooth( method = 'glm',
                method.args = list( family = "poisson"), se = FALSE,
                alpha = 0.5) +
    theme_minimal()

```
  
  
  
***
  
Chapter 4 - Repeated Measures  
  
An introduction to repeated measures:  
  
* Sampling the same thing over time is a repeated measure, a specific example of a mixed effects model  
	* Follow the same individual through time - cohorts allow for controlling for individuality  
    * The paired t-test is often used for assessing a repeated measures dataset - t.test(x1, x2, paired=TRUE)  # x1 and x2 need to be the same length and each element needs to be the same individual  
* Repeated measures ANOVA is a conceptual extension of the paired t-test - are the means constant over time  
	* anova(lmer(y ~ time + (1|individual)))  
    * Can be used with glmer() also  
    * Note that degrees of freedom is still an open question - different packages calculate this differently  
  
Sleep study:  
  
* Applying LMER to the sleep study dataset - impact of drugs on sleep patterns for 10 patients followed over time  
	* This is the classic "Student" dataset due to Guinness at the time not allowing its employees to publish  
    * Ho will be that the amount of sleep does not vary with the treatments  
    * Modeling will be done using a linear mixed model  
* Modeling approach - iteratively;  
	* EDA  
    * Simple regression  
    * Model of interest  
    * Extract information from model  
    * Visualize final data  
  
Hate in NY state?  
  
* Change in rate of hate crimes over time by county - available from data.gov for 2010-2016  
* Level of technical details in reporting should vary significantly by audience - blend data in to story for wider audiences, while being reporducible/technical for a scientifc audience  
  
Wrap up:  
  
* Hiearchical data, mixed effects models, case studies  
* Start with the LME4 documentation for additional explorations and details  
  
Example code includes:  
```{r}

y <- c(0.23, 2.735, -0.038, 6.327, -0.643, 1.69, -1.378, -1.228, -0.252, 2.014, -0.073, 6.101, 0.213, 3.127, -0.29, 8.395, -0.33, 2.735, 0.223, 1.301)
treat <- rep(c("before", "after"), times=10)
x <- rep(letters[1:10], each=2)

# Run a standard, non-paired t-test
t.test(y[treat == "before"], y[treat == "after"], paired = FALSE)

# Run a standard, paired t-test
t.test(y[treat == "before"], y[treat == "after"], paired = TRUE)


library(lmerTest)
library(lme4)

# Run the paired-test like before
t.test(y[treat == "before"], y[treat == "after"], paired = TRUE)

# Run a repeated-measures ANOVA
anova(lmer( y ~ treat + (1|x)))


data(sleepstudy, package="lme4")
str(sleepstudy)

# Plot the data
ggplot(data = sleepstudy) +
    geom_line(aes(x = Days, y = Reaction, group = Subject)) +
    stat_smooth(aes(x = Days, y = Reaction), method = 'lm', size = 3, se = FALSE)

# Build a lm 
lm( Reaction ~ Days, data = sleepstudy)

# Build a lmer
(lmerOut <- lmer( Reaction ~ Days + (1|Subject), data = sleepstudy))


# The lmer model you built during the previous exercise has been saved as lmerOut
# During this exercise, you will examine the effects of drug type using both an ANOVA framework and a regression framework

# Run an anova
anova(lmerOut)

# Look at the regression coefficients
summary(lmerOut)


# Read in NY hate data
rawHate <- read.csv("./RInputFiles/hateNY.csv")
hate <- rawHate
str(hate)


ggplot( data = hate, aes(x = Year, y = TotalIncidents, group = County)) +
    geom_line() + 
    geom_smooth(method = 'lm', se = FALSE)


# During this exercise, you will build a glmer
# Because most of the incidents are small count values, use a Poisson (R function family poisson) error term
# First, build a model using the actually year (variable Year, such as 2006, 2007, etc) - this model will fail
# Second, build a model using the rescaled year (variable Year2, such as 0, 1, 2, etc)
# This demonstrates the importance of considering where the intercept is located when building regression models
# Recall that a variable x can be both a fixed and random effect in a lmer() or glmer(): for example lmer(y ~ x + (x| group) demonstrates this syntax

# glmer with raw Year
glmer(TotalIncidents ~ Year + (Year|County), data = hate, family = "poisson")

# glmer with scaled Year, Year2
glmerOut <- glmer(TotalIncidents ~ Year2 + (Year2|County), data = hate, family = "poisson")
summary(glmerOut)


# Extract and manipulate data
countyTrend <- ranef(glmerOut)$County
countyTrend$county <- factor(row.names(countyTrend), levels =row.names(countyTrend)[order(countyTrend$Year2)])

# Plot results 
ggplot(data = countyTrend, aes(x = county, y = Year2)) + geom_point() +
    coord_flip() + 
    ylab("Change in hate crimes per year")  +
    xlab("County")

```
  
  
  
***
  
### _Forecasting Product Demand in R_  
  
Chapter 1 - Forecasting Demand with Time Series  
  
Loading data in to an xts object:  
  
* The xts object will be the buidling block for the course - extensible time series (xts) is an extension of the zoo package - basically, a time index attached to the data matrix  
* Can create dates using dates=seq(as.Date("MM-DD-YYYY"), length=, by="weeks")  # to create weekly data  
	* xts(myData, order.by=dates)  # will create an XTS using dates as the index  
  
ARIMA Time Series 101:  
  
* AR - AutoRegressive (lags help to determine today's values - "long memory models")  
* MA - Moving Average (shocks/errors help to determine today's shocks/errors - "short memory models" due to dissipation)  
* I - Integrated (does the data have a dependency across time, and how long does it last) - make the time series stationary  
	* Stationarity is the idea that effects disipate over time - today has more impact on tomorrow than on time periods in the future  
    * Differencing (monthly, seasonal, etc.) the data can be a useful approach for data with stationarity  
* Begin by creating training dataset and valiadation training dataset  
* The auto.arima() function tries to estimate the best ARIMA for a given data series  
	* ARIMA(p, d, q) is ARIMA(AR, Differencing, MA)  
  
Forecasting and Evaluating:  
  
* Can use the ARIMA data to forecast the data forward - extrapolating the signal (forecasting) and estimating the amount of noise (error or CI)  
* The forecast() function in R simplifies the process - forecast(myModel, h=) which will forecast forward h time periods  
* Two common error measurements include MAE (mean average error) and MAPE (mean average percentage error)  
	* MAPE is better at putting things on a common scale  
  
Example code includes:  
```{r}


# Read in beverages data
rawBev <- read.csv("./RInputFiles/Bev.csv")
bev <- rawBev
str(bev)


# Load xts package 
library(xts)
library(forecast)


# Create the dates object as an index for your xts object
dates <- seq(as.Date("2014-01-19"), length = 176, by = "weeks")

# Create an xts object called bev_xts
bev_xts <- xts(bev, order.by = dates)


# Create the individual region sales as their own objects
MET_hi <- bev_xts[,"MET.hi"]
MET_lo <- bev_xts[,"MET.lo"]
MET_sp <- bev_xts[,"MET.sp"]

# Sum the region sales together
MET_t <- MET_hi + MET_lo + MET_sp

# Plot the metropolitan region total sales
plot(MET_t)


# Split the data into training and validation
MET_t_train <- MET_t[index(MET_t) < "2017-01-01"]
MET_t_valid <- MET_t[index(MET_t) >= "2017-01-01"]

# Use auto.arima() function for metropolitan sales
MET_t_model <- auto.arima(MET_t_train)


# Forecast the first 22 weeks of 2017
forecast_MET_t <- forecast(MET_t_model, h = 22)

# Plot this forecast #
plot(forecast_MET_t)


# Convert to numeric for ease
for_MET_t <- as.numeric(forecast_MET_t$mean)
v_MET_t <- as.numeric(MET_t_valid)

# Calculate the MAE
MAE <- mean(abs(for_MET_t - v_MET_t))

# Calculate the MAPE
MAPE <- 100*mean(abs(for_MET_t - v_MET_t)/v_MET_t)

# Print to see how good your forecast is!
print(MAE)
print(MAPE)


# Convert your forecast to an xts object
for_dates <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_t_xts <- xts(forecast_MET_t$mean, order.by = for_dates)

# Plot the validation data set
plot(for_MET_t_xts, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(MET_t_valid, col = "blue")


# Plot the validation data set
plot(MET_t_valid, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(for_MET_t_xts, col = "blue")

# Convert the limits to xts objects
lower <- xts(forecast_MET_t$lower[, 2], order.by = for_dates)
upper <- xts(forecast_MET_t$upper[, 2], order.by = for_dates)

# Adding confidence intervals of forecast to plot
lines(lower, col = "blue", lty = "dashed")
lines(upper, col = "blue", lty = "dashed")

```
  
  
  
***
  
Chapter 2 - Components of Demand  
  
Price elasticity:  
  
* Price is one of the obvious factors that impacts demand, with the relationship called price elasticity (% dDemand / % dPrice)  
	* Elastic goods have elasticity > 1, meaning demand changes more quickly (percentage wise) than price  
    * Inelastic goods have elasticity < 1, for example gasoline  
    * Unit elastic goods have elasticity = 1, meaning that X% increase in price drives X% decrease in demand  
    * Linear regression can be employed to estimate the elasticity for a given product - the log-log transform helps get the % vs % coefficients  
  
Seasonal/holiday/promotional effects:  
  
* Seasonal products are common - can be bought any time of the year, though certain seasons have higher demand (holidays are a common example)  
* Promotions are attempts by companies to influence demand  
* Linear regression can help determine relationships between demand and many other factors  
	* If an xts vector has been created for key dates, can merge(train, holiday, fill=0) and the holiday column will be 0 wherever there is no match to holiday  
  
Forecasting with regression:  
  
* Forecasting with time series is straightforward due to the lag nature of the models - tomorrow forecasts today and today forecasts tomorrow and etc.  
* Forecasting with regression can be more tricky, particularly since we need the future inputs (such as price) in order to predict the future demand  
	* Even when there are contractually fixed prices, promotions can effectively create a de facto price change anyways  
* Need to have the same column names in the test/validation dataset as were used in the modeling  
	* Then, can use predict(myModel, myData)  
    * May need to exponentiate in case the data are currently on the log scale rather than the absolute scale  
  
Example code includes:  
```{r}

bev_xts_train <- bev_xts[index(bev_xts) < "2017-01-01"]
bev_xts_valid <- bev_xts[index(bev_xts) >= "2017-01-01"]

# Save the prices of each product
l_MET_hi_p <- log(as.vector(bev_xts_train[, "MET.hi.p"]))

# Save as a data frame
MET_hi_train <- data.frame(as.vector(log(MET_hi[index(MET_hi) < "2017-01-01"])), l_MET_hi_p)
colnames(MET_hi_train) <- c("log_sales", "log_price")

# Calculate the regression
model_MET_hi <- lm(log_sales ~ log_price, data = MET_hi_train)


# Plot the product's sales
plot(MET_hi)

# Plot the product's price
MET_hi_p <- bev_xts_train[, "MET.hi.p"]
plot(MET_hi_p)


# Create date indices for New Year's week
n.dates <- as.Date(c("2014-12-28", "2015-12-27", "2016-12-25"))

# Create xts objects for New Year's
newyear <- as.xts(rep(1, 3), order.by = n.dates)

# Create sequence of dates for merging
dates_train <- seq(as.Date("2014-01-19"), length = 154, by = "weeks")

# Merge training dates into New Year's object
newyear <- merge(newyear, dates_train, fill = 0)


# Add newyear variable to your data frame
MET_hi_train <- data.frame(MET_hi_train, newyear=as.vector(newyear))

# Build regressions for the product
model_MET_hi_full <- lm(log_sales ~ log_price + newyear, data = MET_hi_train)


# Subset the validation prices #
l_MET_hi_p_valid <- log(as.vector(bev_xts_valid[, "MET.hi.p"]))

# Create a validation data frame #
MET_hi_valid <- data.frame(l_MET_hi_p_valid)
colnames(MET_hi_valid) <- "log_price"


# Predict the log of sales for your high end product
pred_MET_hi <- predict(model_MET_hi, MET_hi_valid)

# Convert predictions out of log scale
pred_MET_hi <- exp(pred_MET_hi)


# Convert to an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
pred_MET_hi_xts <- xts(pred_MET_hi, order.by = dates_valid)

# Plot the forecast
plot(pred_MET_hi_xts)

# Calculate and print the MAPE
MET_hi_v <- bev_xts_valid[,"MET.hi"]

MAPE <- 100*mean(abs((pred_MET_hi_xts - MET_hi_v)/MET_hi_v))
print(MAPE)

```
  
  
  
***
  
Chapter 3 - Blending Regression with Time Series  
  
Residuals from regression model:  
  
* The residuals from the regression models can be used for further modeling - see if the residuals are related over time, and model them with time series if so  
* Need to start by gathering the residuals and then converting them to an XTS object - explore for patterns in this XTS object  
  
Forecasting residuals:  
  
* When the residuals are related across time, we can use time series to model the residuals - basically, patterns to the errors provide an opportunity for further modeling  
* Can use auto.arima() on the residuals data, to see what the best ARIMA model for the residuals is  
	* Can then forecast the residuals in to the future using forecast(myModel, h=) # h being the time periods to predict forward  
  
Transfer functions and ensembling:  
  
* Techniques for combining forecasts - single model (transfer function) or averaging of models (ensembling)  
* Demand can be based on both regression (modeling external factors) and time series (residuals)  
* Ensembling is a combination (blend) of the forecasts, with simple averaging being the simplest approach  
	* Basically, build a stand-alone time series model and a stand-alone regression model  
    * The ensemble forecast can be better or worse than any of the stand-alone models  
  
Example code includes:  
```{r}

# Calculate the residuals from the model
MET_hi_full_res <- resid(model_MET_hi_full)

# Convert the residuals to an xts object
MET_hi_full_res <- xts(MET_hi_full_res, order.by = dates_train)


# Plot the histogram of the residuals
hist(MET_hi_full_res)

# Plot the residuals over time
plot(MET_hi_full_res)


# Build an ARIMA model on the residuals: MET_hi_arima
MET_hi_arima <- auto.arima(MET_hi_full_res)

# Look at a summary of the model
summary(MET_hi_arima)


# Forecast 22 weeks with your model: for_MET_hi_arima
for_MET_hi_arima <- forecast(MET_hi_arima, h=22)

# Print first 10 observations
head(for_MET_hi_arima$mean, n = 10)


# Convert your forecasts into an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_arima <- xts(for_MET_hi_arima$mean, order.by = dates_valid)

# Plot the forecast
plot(for_MET_hi_arima)


# Convert your residual forecast to the exponential version
for_MET_hi_arima <- exp(for_MET_hi_arima)

# Multiply your forecasts together!
for_MET_hi_final <- for_MET_hi_arima * pred_MET_hi_xts


# Plot the final forecast - don't touch the options!
plot(for_MET_hi_final, ylim = c(1000, 4300))

# Overlay the validation data set
lines(MET_hi_v, col = "blue")


# Calculate the MAE
MAE <- mean(abs(for_MET_hi_final - MET_hi_v))
print(MAE)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_hi_final - MET_hi_v)/MET_hi_v)
print(MAPE)


# Build an ARIMA model using the auto.arima function
MET_hi_model_arima <- auto.arima(MET_hi)

# Forecast the ARIMA model
for_MET_hi <- forecast(MET_hi_model_arima, h = length(MET_hi_v))

# Save the forecast as an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_xts <- xts(for_MET_hi$mean, order.by = dates_valid)

# Calculate the MAPE of the forecast
MAPE <- 100 * mean(abs(for_MET_hi_xts - MET_hi_v) / MET_hi_v)
print(MAPE)


# Ensemble the two forecasts together
for_MET_hi_en <- 0.5 * (for_MET_hi_xts + pred_MET_hi_xts)

# Calculate the MAE and MAPE
MAE <- mean(abs(for_MET_hi_en - MET_hi_v))
print(MAE)

MAPE <- 100 * mean(abs(for_MET_hi_en - MET_hi_v) / MET_hi_v)
print(MAPE)

```
  
  
  
***
  
Chapter 4 - Hierarchical Forecasting  
  
Bottom-Up Hierarchical Forecasting:  
  
* The hierarchical data structuring can be an advantage in forecasting, provided that the data has a natural hierarchy  
* The sum of all the lower-level forecasts should equal the higher-level forecasts  
	* Bottom-up: Forecast at the lowest level and aggregate (easiest but requires the most number of forecasts)  
    * Top-down: Forecast at the top level and the apply downwards  
    * Middle-out: Forecast at the middle levels and then apply both upwards and downwards  
  
Top-Down Hierarchical Forecasting:  
  
* The top-down forecasting process is typically quicker but less accurate than the bottom-up forecasting process  
* Two techniques available for top-down reconciliation  
	* Average of historical proportions - mean percentage that each component contributes to the total (calculated by sub-component such as week)  
    * Proportion of historical averages - mean percentage that each component contributes to the total (calculated by aggregate)  
* Reconciled forecasts at lower levels are typically less accurate than the direct forecast of the lower levels  
  
Middle-Out Hierarchical Forecasting:  
  
* Bottom-up forecasting is higher quality but more time-consuming than top-down forecasting  
* The middle-out forecasting method is a sometimes successful blend of the methods, getting decent accuracy at a lesser time commitment  
  
Wrap up:  
  
* Using time series to forecast demand forward  
* Incorporating external factors using linear regression  
* Blending time series and regression approaches  
* Top-down, bottom-up, middle-out approaches to aggregation and forecasting at various levels (hierarchical)  
* Can extend by looking at cross-elasticities (impact of competitor pricing)  
* Can better forecast proportions using time series analysis  
* Additional demand forecasting models include neural networks, exponential smoothing, etc.  
  
Example code includes:  
```{r}

# Build a time series model 
MET_sp_model_arima <- auto.arima(MET_sp)

# Forecast the time series model for 22 periods
for_MET_sp <- forecast(MET_sp_model_arima, h=22)

# Create an xts object
for_MET_sp_xts <- xts(for_MET_sp$mean, order.by=dates_valid)

MET_sp_v <- MET_sp["2017"]

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_sp_xts - MET_sp_v) / MET_sp_v)
print(MAPE)


MET_sp_train <- bev_xts_train %>%
    transform(log_sales = log(MET.sp), log_price=log(MET.sp.p))
MET_sp_train <- MET_sp_train[, c("log_sales", "log_price")]
MET_sp_train$newyear <- 0
MET_sp_train$valentine <- 0
MET_sp_train$christmas <- 0
MET_sp_train$mother <- 0

MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-12-28", "2015-12-27", "2016-12-25")), "newyear"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-02-09", "2015-02-08", "2016-02-07")), "valentine"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-12-21", "2015-12-20", "2016-12-18")), "christmas"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-05-04", "2015-05-03", "2016-05-01")), "mother"] <- 1


# THE BELOW IS TOTAL NONSENSE
# Build a regression model
model_MET_sp <- lm(log_sales ~ log_price + newyear + valentine + christmas + mother, data = MET_sp_train)


MET_sp_valid <- as.data.frame(bev_xts_valid) %>%
    mutate(log_sales = log(MET.sp), log_price=log(MET.sp.p)) %>%
    select("log_sales", "log_price")
MET_sp_valid$newyear <- 0
MET_sp_valid$valentine <- 0
MET_sp_valid$christmas <- 0
MET_sp_valid$mother <- 0  

MET_sp_valid[7, "valentine"] <- 1
MET_sp_valid[19, "mother"] <- 1
MET_sp_valid$log_sales <- NULL


# Forecast the regression model using the predict function 
pred_MET_sp <- predict(model_MET_sp, MET_sp_valid)

# Exponentiate your predictions and create an xts object
pred_MET_sp <- exp(pred_MET_sp)
pred_MET_sp_xts <- xts(pred_MET_sp, order.by = dates_valid)

# Calculate MAPE
MAPE <- 100*mean(abs((pred_MET_sp_xts - MET_sp_v)/MET_sp_v))
print(MAPE)


# Ensemble the two forecasts
for_MET_sp_en <- 0.5 * (for_MET_sp_xts + pred_MET_sp_xts)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_sp_en - MET_sp_v) / MET_sp_v)
print(MAPE)


# Copy over pred_MET_lo_xts
pred_MET_lo_xts <- xts(c(2960.6, 2974.1, 2943.2, 2948.6, 2915.6, 2736.4, 2953.9, 3199.4, 2934, 2898.7, 3027.7, 3165.9, 3073.1, 2842.7, 2928.7, 3070.2, 2982.2, 3018, 3031.9, 2879.4, 2993.2, 2974.1), order.by=dates_valid)


# Calculate the metropolitan regional sales forecast
for_MET_total <- pred_MET_hi_xts + for_MET_sp_en + pred_MET_lo_xts

# Calculate a validation data set 
MET_t_v <- bev_xts_valid[,"MET.hi"] + bev_xts_valid[,"MET.lo"] + bev_xts_valid[,"MET.sp"]

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_total - MET_t_v) / MET_t_v)
print(MAPE)


# Create the MET_total data
MET_total <- xts(data.frame(MET.hi=c(5942, 5600, 5541, 6892, 5586, 5943, 6329, 6693, 6938, 6138, 6361, 6378, 5423, 5097, 4937, 5496, 6870, 6626, 6356, 5657, 6577, 7202, 7381, 7404, 7204, 6667, 6153, 6035, 5633, 5283, 5178, 4758, 5058, 5254, 5954, 6166, 6247, 6304, 7202, 6662, 6814, 6174, 5412, 5380, 5674, 6472, 6912, 7404, 8614, 8849, 7174, 6489, 7174, 6555, 6402, 7671, 5012, 4790, 5075, 5238, 5615, 6113, 7706, 7811, 7898, 7232, 6585, 5870, 7084, 5125, 5330, 5553, 6349, 6195, 6271, 5851, 5333, 5854, 5609, 5649, 6051, 6409, 5786, 5190, 5085, 4949, 5151, 5147, 5426, 5509, 6956, 7870, 8224, 6685, 6153, 5802, 5244, 5162, 5036, 5025, 8378, 8944, 7109, 7605, 7846, 7598, 8012, 9551, 6102, 5366, 4932, 4962, 5392, 6194, 7239, 7621, 7460, 7097, 6596, 5848, 8306, 5344, 5848, 6341, 7364, 7269, 7053, 6682, 6971, 7521, 7063, 6298, 6003, 5227, 5047, 4877, 4851, 4628, 4516, 4442, 4935, 5181, 5431, 5866, 5919, 5704, 5957, 6019, 5962, 6021, 5880, 5674, 7439, 7415)),
                 order.by=dates_train
                 )

# Build a regional time series model
MET_t_model_arima <- auto.arima(MET_total)

# Calculate a 2017 forecast for 22 periods
for_MET_t <- forecast(MET_t_model_arima, h=22)

# Make an xts object from your forecast
for_MET_t_xts <- xts(for_MET_t$mean, order.by=dates_valid)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_t_xts - MET_t_v) / MET_t_v)
print(MAPE)


# Calculate the average historical proportions
prop_hi <- mean(MET_hi/MET_total)
prop_lo <- mean(MET_lo/MET_total)
prop_sp <- mean(MET_sp/MET_total)

# Distribute out your forecast to each product
for_prop_hi <- prop_hi*for_MET_t_xts
for_prop_lo <- prop_lo*for_MET_t_xts
for_prop_sp <- prop_sp*for_MET_t_xts

# Calculate the MAPE's for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi - MET_hi_v) / MET_hi_v)
print(MAPE_hi)

MET_lo_v <- bev_xts_valid[,"MET.lo"]
MAPE_lo <- 100 * mean(abs(for_prop_lo - MET_lo_v) / MET_lo_v)
print(MAPE_lo)

MAPE_sp <- 100 * mean(abs(for_prop_sp - MET_sp_v) / MET_sp_v)
print(MAPE_sp)


# Calculate the average historical proportions
prop_hi_2 <- mean(MET_hi) / mean(MET_total)
prop_lo_2 <- mean(MET_lo) / mean(MET_total)
prop_sp_2 <- mean(MET_sp) / mean(MET_total)

# Distribute out your forecast to each product
for_prop_hi_2 <- prop_hi_2 * for_MET_t_xts
for_prop_lo_2 <- prop_lo_2 * for_MET_t_xts
for_prop_sp_2 <- prop_sp_2 * for_MET_t_xts

# Calculate the MAPE's for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi_2 - MET_hi_v) / MET_hi_v)
print(MAPE_hi)
MAPE_lo <- 100 * mean(abs(for_prop_lo_2 - MET_lo_v) / MET_lo_v)
print(MAPE_lo)
MAPE_sp <- 100 * mean(abs(for_prop_sp_2 - MET_sp_v) / MET_sp_v)
print(MAPE_sp)


SEC_total <- xts(data.frame(SEC.hi=c(700, 775, 789, 863, 765, 759, 757, 747, 746, 709, 749, 786, 796, 726, 727, 723, 778, 755, 739, 740, 723, 695, 727, 707, 725, 684, 667, 698, 727, 722, 748, 695, 742, 739, 715, 724, 686, 671, 688, 682, 710, 700, 672, 680, 695, 780, 751, 693, 809, 881, 703, 712, 768, 796, 808, 904, 641, 662, 693, 725, 719, 736, 715, 722, 732, 745, 689, 705, 811, 739, 744, 700, 745, 735, 732, 722, 721, 732, 750, 714, 752, 677, 731, 674, 720, 675, 741, 722, 715, 719, 649, 697, 743, 733, 772, 698, 690, 734, 713, 644, 788, 833, 749, 731, 670, 675, 675, 993, 773, 751, 697, 677, 750, 723, 780, 763, 721, 701, 704, 684, 985, 791, 731, 714, 704, 694, 685, 652, 708, 754, 747, 705, 711, 699, 712, 745, 706, 665, 666, 692, 676, 696, 689, 697, 689, 717, 697, 708, 660, 707, 715, 680, 922, 888)), order.by=dates_train
                 )

# Build a time series model for the region
SEC_t_model_arima <- auto.arima(SEC_total)

# Forecast the time series model
for_SEC_t <- forecast(SEC_t_model_arima, h=22)

# Make into an xts object
for_SEC_t_xts <- xts(for_SEC_t$mean, order.by=dates_valid)

SEC_t_v <- bev_xts_valid$SEC.hi + bev_xts_valid$SEC.lo
# Calculate the MAPE
MAPE <- 100 * mean(abs(for_SEC_t_xts - SEC_t_v) / SEC_t_v)
print(MAPE)


SEC_hi <- bev_xts_train[, "SEC.hi"]
SEC_lo <- bev_xts_train[, "SEC.lo"]
SEC_hi_v <- bev_xts_valid[, "SEC.hi"]
SEC_lo_v <- bev_xts_valid[, "SEC.lo"]

# Calculate the average of historical proportions
prop_hi <- mean(SEC_hi / SEC_total)
prop_lo <- mean(SEC_lo / SEC_total)

# Distribute the forecast
for_prop_hi <- prop_hi * for_SEC_t_xts
for_prop_lo <- prop_lo * for_SEC_t_xts

# Calculate a MAPE for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi - SEC_hi_v) / SEC_hi_v)
print(MAPE_hi)

MAPE_lo <- 100 * mean(abs(for_prop_lo - SEC_lo_v) / SEC_lo_v)
print(MAPE_lo)


# Copy over for_M_t_xts data
for_M_t_xts <- xts(c(2207, 2021, 2010, 2052, 2075, 2074, 2065, 2058, 2056, 2055, 2053, 2052, 2050, 2049, 2048, 2047, 2046, 2045, 2044, 2043, 2043, 2042), order.by=dates_valid)

# Calculate the state sales forecast: for_state
for_state = for_SEC_t_xts + for_MET_t_xts + for_M_t_xts

# See the forecasts
for_state

```
  
  
  
***
  
### _HR Analytics in R: Exploring Employee Data_  
  
Chapter 1 - Identifying the Best Recruiting Source  
  
Introduction - Ben Teusch, HR Analytics Consultant:  
  
* HR analytics has many other names - people analytics, workforce analytics, etc.  
* Identify groups for comparison - high vs. low performers, groups with high vs. low turnover, etc.  
	* Exploratory analysis and statistics for each group, including plots of key differences  
* Course is outlines as a series of case studies, with one case per chapter  
  
Recruiting and quality of hire:  
  
* Where are the best hires coming from, and how can you get more of them  
	* Defining quality of hire is challenging - some mix of productivity, satisfaction, retention, performance reviews, etc.  
    * Attrition can be defined as the mean of a 1, 0 vector of "did the person leave in the time period T"  
  
Visualizing recruiting data:  
  
* Helpful for communicating findings to decision makers  
* The geom_col() in ggplot will make a bar chart, with the y aestehtic being the bar height  
  
Example code includes:  
```{r}

# Import the recruitment data
recruitment <- readr::read_csv("./RInputFiles/recruitment_data.csv")

# Look at the first few rows of the dataset
head(recruitment)

# Get an overview of the recruitment data
summary(recruitment)

# See which recruiting sources the company has been using
recruitment %>% 
  count(recruiting_source)


# Find the average sales quota attainment for each recruiting source
avg_sales <- recruitment %>% 
  group_by(recruiting_source) %>% 
  summarize(avg_sales_quota_pct=mean(sales_quota_pct))

# Display the result
avg_sales


# Find the average attrition for the sales team, by recruiting source, sorted from lowest attrition rate to highest
avg_attrition <- recruitment %>%
  group_by(recruiting_source) %>% 
  summarize(attrition_rate=mean(attrition)) %>%
  arrange(attrition_rate)

# Display the result
avg_attrition

# Plot the bar chart
avg_sales %>% ggplot(aes(x=recruiting_source, y=avg_sales_quota_pct)) + geom_col()

# Plot the bar chart
avg_attrition %>% ggplot(aes(x=recruiting_source, y=attrition_rate)) + geom_col()

```
  
  
  
***
  
Chapter 2 - What is driving low employee engagement  
  
Analyzing employee engagement:  
  
* Gallup defines engaged employees as those who are involved in, enthusiastic about, and committed to their workplace  
* Survey data are available in the example case study  
	* Will use both mutate() and ifelse()  
    * The ifelse() is needed for vectors of length > 1 since it can work in a vectorized manner (and is thus OK inside the mutate call)  
  
Visualizing the engagement data:  
  
* Multiple attributes in a single place can make for a more compelling report  
* The tidyr package is part of the tidyverse, and hslps arrange the data properly for plotting  
	* tidyr::gather(columns, key="key", value="value") will be the package used in this example - pull the data from the columns down to the rows  
    * ggplot(survey_gathered, aes(x = key, y = value, fill = department)) + geom_col(position = "dodge")  
    * ggplot(survey_gathered, aes(x = key, y = value, fill = department)) + geom_col(position = "dodge") + facet_wrap(~ key, scales = "free")  
  
Are differences meaningful?  
  
* Can use significance testing to assess likelhood (p-value) that the second sample could have come from the same population as the first sample  
	* This course will use t-test (continuous variables) and chi-squared test (categorical variables)  
    * t.test(tenure ~ is_manager, data = survey)  
    * chisq.test(survey$left_company, survey$is_manager)  # no data= argument is available in the function  
  
Example code includes:  
```{r}

# Import the data
survey <- readr::read_csv("./RInputFiles/survey_data.csv")

# Get an overview of the data
summary(survey)

# Examine the counts of the department variable
survey %>% count(department)


# Output the average engagement score for each department, sorted
survey %>%
  group_by(department) %>%
  summarize(avg_engagement=mean(engagement)) %>%
  arrange(avg_engagement)


# Create the disengaged variable and assign the result to survey
survey_disengaged <- survey %>% 
  mutate(disengaged = ifelse(engagement <= 2, 1, 0)) 

survey_disengaged

# Summarize the three variables by department
survey_summary <- survey_disengaged %>%
  group_by(department) %>%
  summarize(pct_disengaged=mean(disengaged), 
            avg_salary=mean(salary), 
            avg_vacation_taken=mean(vacation_days_taken)
            )

survey_summary


# Gather data for plotting
survey_gathered <- survey_summary %>% 
  gather(key = "measure", value = "value",
         pct_disengaged, avg_salary, avg_vacation_taken)

# Create three bar charts
ggplot(survey_gathered, aes(x=measure, y=value, fill=department)) +
  geom_col(position="dodge") + 
  facet_wrap(~ measure, scales="free")


# Add the in_sales variable
survey_sales <- survey %>%
  mutate(in_sales = ifelse(department == "Sales", "Sales", "Other"), 
         disengaged = ifelse(engagement < 3, 1L, 0L)
         )

# Test the hypothesis using survey_sales
chisq.test(survey_sales$disengaged, survey_sales$in_sales)
t.test(disengaged ~ in_sales, data=survey_sales)


# Test the hypothesis using the survey_sales data
t.test(vacation_days_taken ~ in_sales, data = survey_sales)

```
  
  
  
***
  
Chapter 3 - Are new hires getting paid too much?  
  
Paying new hires fairly:  
  
* Sometimes, current employees get paid less than new employees, which can drive low engagement and turnover  
* Case study will have a simulated pay dataset available for analysis  
* Can use broom::tidy() to return the outputs in a nicely formatted data frame  
	* chisq.test(survey$in_sales, survey$disengaged) %>% tidy()  
  
Omitted variable bias:  
  
* Key assumption of the tests is that the groups are the same, with the exception of the variables being tested  
* Omitted variable bias occurs when both 1) the omitted variable is correlated with the dependent variable, and 2) the omitted variable is correlated with an explanatory variable  
	* Omitted variables are often known as confounders  
    * Plotting can help to identify the issue, particularly with a stacked (to 100%) bar chart  
    * pay %>% ggplot(aes(x = new_hire, fill = department)) + geom_bar(position = "fill")  
    * The geom_bar() object has height that is fully dependent on x, in contrast to geom_col() which has a y-aestehtic  
  
Linear regression helps to test the multivariate impacts of variables:  
  
* lm(salary ~ new_hire, data = pay) %>% tidy()  # single dependent variable  
* lm(salary ~ new_hire + department, data = pay) %>% tidy()  # multiple dependent variables  
* lm(salary ~ new_hire + department, data = pay) %>% summary()  # more detailed summary of the linear regression  
  
Example code includes:  
```{r}

# Import the data
pay <- readr::read_csv("./RInputFiles/fair_pay_data.csv")

# Get an overview of the data
summary(pay)

# Check average salary of new hires and non-new hires
pay %>% 
  group_by(new_hire) %>%
  summarize(avg_salary=mean(salary))


# Perform the correct statistical test
t.test(salary ~ new_hire, data = pay)
t.test(salary ~ new_hire, data = pay) %>%
  broom::tidy()


# Create a stacked bar chart
pay %>%
  ggplot(aes(x=new_hire, fill=job_level)) + 
  geom_bar(position="fill")

# Calculate the average salary for each group of interest
pay_grouped <- pay %>% 
  group_by(new_hire, job_level) %>% 
  summarize(avg_salary = mean(salary))
  
# Graph the results using facet_wrap()  
pay_grouped %>%
  ggplot(aes(x=new_hire, y=avg_salary)) + 
  geom_col() + 
  facet_wrap(~ job_level)


# Filter the data to include only hourly employees
pay_filter <- pay %>%
  filter(job_level == "Hourly")

# Test the difference in pay
t.test(salary ~ new_hire, data=pay_filter) %>%
  broom::tidy()


# Run the simple regression
model_simple <- lm(salary ~ new_hire, data = pay)

# Display the summary of model_simple
model_simple %>% 
  summary()

# Display a tidy summary
model_simple %>% 
  broom::tidy()


# Run the multiple regression
model_multiple <- lm(salary ~ new_hire + job_level, data = pay)

# Display the summary of model_multiple
model_multiple %>% 
  summary()

# Display a tidy summary
model_multiple %>% 
  broom::tidy()

```
  
  
  
***
  
Chapter 4 - Are performance ratings being given consistently?  
  
Joining HR data:  
  
* Employee data tend to be stored in different locations, requiring joins (merges) prior to running analyses  
	* dplyr::left_join(hr_data, bonus_pay_data, by = "employee_id")  
    * All employees in hr_data will be kept, even if there is no matching record in bonus_pay_data  
    * Employee ID (or similar) is by far the best way to join data - names tend to be non-unique and can differ in different systems  
  
Performance ratings and fairness:  
  
* Performance ratings are inherently subjective and thus prone to bias  
* Unconscious bias is based on the brain's heuristics, and may include preferences for members of various groups (biases, as reflected in hiring, promotion, etc.)  
  
Logistic regression is especially helpful for modeling binary response variables:  
  
* glm(high_performer ~ salary, data = hr, family = "binomial") %>% tidy()  
* glm(high_performer ~ salary + department, data = hr, family = "binomial") %>% tidy()  
  
Example code includes:  
```{r}

# Import the data
hr_data <- readr::read_csv("./RInputFiles/hr_data.csv")
performance_data <- readr::read_csv("./RInputFiles/performance_data.csv")

# Examine the datasets
summary(hr_data)
summary(performance_data)


# Join the two tables
joined_data <- left_join(hr_data, performance_data, by = "employee_id")

# Examine the result
summary(joined_data)

# Check whether the average performance rating differs by gender 
joined_data %>%
  group_by(gender) %>%
  summarize(avg_rating = mean(rating))


# Add the high_performer column
performance <- joined_data %>%  
  mutate(high_performer = ifelse(rating >= 4, 1, 0))

# Test whether one gender is more likely to be a high performer
chisq.test(performance$gender, performance$high_performer)   
 
# Do the same test, and tidy the output
chisq.test(performance$gender, performance$high_performer) %>% broom::tidy()


# Visualize the distribution of high_performer by gender
performance %>%
  ggplot(aes(x=gender, fill=factor(high_performer))) + 
  geom_bar(position="fill")

# Visualize the distribution of all ratings by gender
performance %>%
  ggplot(aes(x=gender, fill=factor(rating))) + 
  geom_bar(position="fill")

# Visualize the distribution of job_level by gender
performance %>%
  ggplot(aes(x = gender, fill = job_level)) +
  geom_bar(position = "fill")
 
# Test whether men and women have different job level distributions
chisq.test(performance$gender, performance$job_level) 


# Visualize the distribution of high_performer by gender, faceted by job level
performance %>%
  ggplot(aes(x = gender, fill = factor(high_performer))) +
  geom_bar(position = "fill") + 
  facet_wrap(~ job_level)


# Run a simple logistic regression
logistic_simple <- glm(high_performer ~ gender, family = "binomial", data = performance) 

# View the result with summary()
logistic_simple %>%
  summary()

# View a tidy version of the result
logistic_simple %>%
  broom::tidy()


# Run a multiple logistic regression
logistic_multiple <- glm(high_performer ~ gender + job_level, family = "binomial", data = performance)

# View the result with summary() or tidy()
logistic_multiple %>% broom::tidy()

```
  
  
  
***
  
Chapter 5 - Improving employee safety with data  
  
Employee safety - looking at accident rates and drivers:  
  
* Requires joining data on multiple variables  
	* joined_data <- left_join(hr_data, safety_data, by = c("year", "employee_id"))  
    * joined_data %>% filter(is.na(accident_time)) # use is.na() instead  
  
Focusing on the location of interest:  
  
* May want to run comparisons of the same location over time  
* May want to assess differences by locations to see if they may be explanatory variables  
  
Explaining the increase in accidents:  
  
* Can use multiple regression to help test for explanatory variables that impact the accident rate  
  
Wrap up:  
  
* Key tools from the Tidyverse (ggplot2, broom, dplyr, etc.) to assess HR data  
* Analytics usage within HR, including differences in HR and other data  
* Can apply additional data science techniques on HR data  
  
Example code includes:  
```{r}

# Import the data 
hr_data <- readr::read_csv("./RInputFiles/hr_data_2.csv")
accident_data <- readr::read_csv("./RInputFiles/accident_data.csv")

# Create hr_joined with left_join() and mutate()
hr_joined <- left_join(hr_data, accident_data, by=c("year", "employee_id")) %>% 
  mutate(had_accident=ifelse(is.na(accident_type), 0, 1))
  
hr_joined


# Find accident rate for each year
hr_joined %>% 
  group_by(year) %>% 
  summarize(accident_rate = mean(had_accident))

# Test difference in accident rate between years
chisq.test(hr_joined$year, hr_joined$had_accident)

# Which location had the highest acccident rate?
hr_joined %>%
  group_by(location) %>%
  summarize(accident_rate=mean(had_accident)) %>%
  arrange(-accident_rate)


# Compare annual accident rates by location
accident_rates <- hr_joined %>% 
  group_by(location, year) %>% 
  summarize(accident_rate = mean(had_accident))
  
accident_rates

# Graph it
accident_rates %>% 
  ggplot(aes(factor(year), accident_rate)) +
  geom_col() +
  facet_wrap(~location)


# Filter out the other locations
southfield <- hr_joined %>% 
  filter(location == "Southfield")

# Find the average overtime hours worked by year
southfield %>%
  group_by(year) %>% 
  summarize(average_overtime_hours = mean(overtime_hours))

# Test difference in Southfield's overtime hours between years
t.test(overtime_hours ~ year, data=southfield) 


# Import the survey data
survey_data <- readr::read_csv("./RInputFiles/survey_data_2.csv")

# Create the safety dataset
safety <- left_join(hr_joined, survey_data, by=c("employee_id", "year")) %>%
  mutate(disengaged=ifelse(engagement <= 2, 1, 0), year=factor(year))


# Visualize the difference in % disengaged by year in Southfield
safety %>% 
    filter(location=="Southfield") %>%
    ggplot(aes(x = year, fill = factor(disengaged))) +
    geom_bar(position = "fill")
 
# Test whether one year had significantly more disengaged employees
southSafety <- safety %>% 
    filter(location=="Southfield")
chisq.test(southSafety$disengaged, southSafety$year)


# Filter out Southfield
other_locs <- safety %>% 
  filter(location != "Southfield")

# Test whether one year had significantly more overtime hours worked
t.test(overtime_hours ~ year, data = other_locs) 

# Test whether one year had significantly more disengaged employees
chisq.test(other_locs$year, other_locs$disengaged)


# Use multiple regression to test the impact of year and disengaged on accident rate in Southfield
regression <- glm(had_accident ~ year + disengaged, family = "binomial", data = southSafety)

# Examine the output
regression %>% broom::tidy()

```
  
  
  
***
  
### _Supervised Learning in R: Case Studies_  
  
Chapter 1 - Cars Data  
  
Making predictions using machine learning:  
  
* Course focuses on applied skills from predictive learning, using regression and classification as well as EDA  
	* Regression tends to be for predicting continuous, numeric variables  
    * Classification tends to be for predicting categorical variables  
* Case studies include 1) fuel efficiency, 2) Stack Overflow developer survey, 3) voter turnout, and 4) ages of nuns  
* The fuel efficiency data is stored in cars2018 and is based on data from the US Department of Energy  
	* Variables names with spaces can be handled by surrounding them with backticks  
    * Tidyverse includes tibble, readr, ggplot2, dplyr, tidyr, purrr, etc. - can be loaded as a package using library(tidyverse)  
  
Getting started with caret:  
  
* The caret package is useful for predictive modeling - full process including the test/train split for the raw dataset  
	* in_train <- createDataPartition(cars_vars$Aspiration, p = 0.8, list = FALSE)  # will stratify on 'Aspiration' variable  
    * training <- cars_vars[in_train,]  
    * testing <- cars_vars[-in_train,]  
* Can then train the model using only the training dataset  
	* fit_lm <- train(log(MPG) ~ ., method = "lm", data=training, trControl=trainControl(method = "none"))  
    * Can then use the yardstick package to assess the quality of the model  
  
Sampling data:  
  
* Bootstrap resampling means sampling with replacement, and then fitting on the resampled dataset (run multiple times)  
	* cars_rf_bt <- train(log(MPG) ~ ., method = "rf", data = training, trControl = trainControl(method = "boot"))  # default 25 resamples  
    * Can both visualize the models and assess the model statistically  
  
Example code includes:  
```{r cache=TRUE}

cars2018 <- readr::read_csv("./RInputFiles/cars2018.csv")
str(cars2018, give.attr = FALSE)
summary(cars2018)

# Print the cars2018 object
cars2018

# Plot the histogram
ggplot(cars2018, aes(x = MPG)) +
    geom_histogram(bins = 25) +
    labs(y = "Number of cars",
         x = "Fuel efficiency (mpg)")


# Deselect the 2 columns to create cars_vars
cars_vars <- cars2018 %>%
    select(-Model, -`Model Index`)

# Fit a linear model
fit_all <- lm(MPG ~ ., data = cars_vars)

# Print the summary of the model
summary(fit_all)


# Load caret
library(caret)

# Split the data into training and test sets
set.seed(1234)
in_train <- createDataPartition(cars_vars$Transmission, p = 0.8, list = FALSE)
training <- cars_vars[in_train, ]
testing <- cars_vars[-in_train, ]

# Train a linear regression model
fit_lm <- train(log(MPG) ~ ., method = "lm", data = training,
                trControl = trainControl(method = "none"))

# Print the model object
fit_lm


# Train a random forest model
fit_rf <- train(log(MPG) ~ ., method = "rf", data = training,
                trControl = trainControl(method = "none"))

# Print the model object
fit_rf


# Create the new columns
results <- training %>%
    mutate(`Linear regression` = predict(fit_lm, training),
           `Random forest` = predict(fit_rf, training))

# Evaluate the performance
yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)


# Create the new columns
results <- testing %>%
    mutate(`Linear regression` = predict(fit_lm, testing),
           `Random forest` = predict(fit_rf, testing))

# Evaluate the performance
yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)


# Fit the models with bootstrap resampling
cars_lm_bt <- train(log(MPG) ~ ., method = "lm", data = training,
                   trControl = trainControl(method = "boot"))
cars_rf_bt <- train(log(MPG) ~ ., method = "rf", data = training,
                   trControl = trainControl(method = "boot"))
                   
# Quick look at the models
cars_lm_bt
cars_rf_bt


results <- testing %>%
    mutate(`Linear regression` = predict(cars_lm_bt, testing),
           `Random forest` = predict(cars_rf_bt, testing))

yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)

results %>%
    gather(Method, Result, `Linear regression`:`Random forest`) %>%
    ggplot(aes(log(MPG), Result, color = Method)) +
    geom_point(size = 1.5, alpha = 0.5) +
    facet_wrap(~Method) +
    geom_abline(lty = 2, color = "gray50") +
    geom_smooth(method = "lm")

```
  
  
  
***
  
Chapter 2 - Stack Overflow Developer Data  
  
Essential copying and pasting from Stack Overflow (largest and most trusted developer community):  
  
* Annual survey of developer perspectives on Stack Overflow - can be used for predictive modeling  
* Data is made available publicly at insights.stackoverflow.com/survey  
* Key question is "what makes a developer more likely to work remotely" (size of company, geography of employee, etc.)  
	* Data are calss imbalanced, with many more Non-Remote employees than Remote employees  
    * Best first step is the simplest model - logit, without any tricks  
    * simple_glm <- stackoverflow %>% select(-Respondent) %>% glm(Remote ~ ., family = "binomial", + data = .)  # Remote ~ . Means "all variables" while data=. Means from the piped dataset  
  
Dealing with imbalanced data:  
  
* Class imbalance is a common problem that can negatively impact model performance  
	* This dataset has 10x the number of non-remote, which can influence models to just start predicting non-remote in all cases  
* One approach to class imbalance is upsampling, basically running resampling with replacement on the small class until it is the same size as the large class  
	* Simple to implement, but with the risk of over-fitting  
    * up_train <- upSample(x = select(training, -Remote), y = training$Remote, yname = "Remote") %>% as_tibble()  
    * stack_glm <- train(Remote ~ ., method = "glm", family = "binomial", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
  
Predicting remote status:  
  
* Classification models can include logistic regression and random forests  
	* stack_glm <- train(Remote ~ ., method = "glm", family = "binomial", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
    * stack_rf <- train(Remote ~ ., method = "rf", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
* Classification models can be evaluated using the confusion matrix  
	* confusionMatrix(predict(stack_glm, testing), testing$Remote)  
    * yardstick::accuracy(testing_results, truth = Remote, estimate = `Logistic regression`)  
    * yardstick::ppv(testing_results, truth = Remote, estimate = `Logistic regression`)  
    * yardstick::npv(testing_results, truth = Remote, estimate = `Logistic regression`)  
  
Example code includes:  
```{r cache=TRUE}

stackoverflow <- readr::read_csv("./RInputFiles/stackoverflow.csv")
stackoverflow$Remote <- factor(stackoverflow$Remote, levels=c("Not remote", "Remote"))
str(stackoverflow, give.attr = FALSE)


# Print stackoverflow
stackoverflow

# First count for Remote
stackoverflow %>% 
    count(Remote, sort = TRUE)

# then count for Country
stackoverflow %>% 
    count(Country, sort = TRUE)


ggplot(stackoverflow, aes(x=Remote, y=YearsCodedJob)) +
    geom_boxplot() +
    labs(x = NULL,
         y = "Years of professional coding experience") 


# Build a simple logistic regression model
simple_glm <- stackoverflow %>%
        select(-Respondent) %>%
        glm(Remote ~ .,
            family = "binomial",
            data = .)

# Print the summary of the model
summary(simple_glm)


stack_select <- stackoverflow %>%
    select(-Respondent)

# Split the data into training and testing sets
set.seed(1234)
in_train <- caret::createDataPartition(stack_select$Remote, p=0.8, list = FALSE)
training <- stack_select[in_train,]
testing <- stack_select[-in_train,]


up_train <- caret::upSample(x = select(training, -Remote), y = training$Remote, yname = "Remote") %>%
    as_tibble()

up_train %>%
    count(Remote)


# Sub-sample to 5% of original
inUse <- sample(1:nrow(training), round(0.05*nrow(training)), replace=FALSE)
useTrain <- training[sort(inUse), ]

# Build a logistic regression model
stack_glm <- caret::train(Remote ~ ., method="glm", family="binomial", data = training, 
                          trControl = trainControl(method = "boot", sampling = "up")
                          )

# Print the model object 
stack_glm


# Build a random forest model
stack_rf <- caret::train(Remote ~ ., method="rf", data = useTrain, 
                         trControl = trainControl(method = "boot", sampling="up")
                         )

# Print the model object
stack_rf


# Confusion matrix for logistic regression model
caret::confusionMatrix(predict(stack_glm, testing), testing$Remote)

# Confusion matrix for random forest model
caret::confusionMatrix(predict(stack_rf, testing), testing$Remote)


# Predict values
testing_results <- testing %>%
    mutate(`Logistic regression` = predict(stack_glm, testing), `Random forest` = predict(stack_rf, testing))

## Calculate accuracy
yardstick::accuracy(testing_results, truth = Remote, estimate = `Logistic regression`)
yardstick::accuracy(testing_results, truth = Remote, estimate = `Random forest`)

## Calculate positive predict value
yardstick::ppv(testing_results, truth = Remote, estimate = `Logistic regression`)
yardstick::ppv(testing_results, truth = Remote, estimate = `Random forest`)

```
  
  
  
***
  
Chapter 3 - Voting  
  
Predicting voter turnout from survey data:  
  
* Survey data available from https://www.voterstudygroup.org/publications/2016-elections/data  
	* Opinions about political and economic topics  
    * Includes whether the voter turned out (voted), based on self-reporting, in the 2016 election  
    * Data are coded as integers, requiring a data dictionary to map the questions and responses to what they mean  
  
Vote 2016:  
  
* Exploratory data analysis will help with learning about the underlying dataset  
	* There are differences on many of the individual dimensions between voters and non-voters  
    * A good first step can be to start with the very simplest model, Dependent ~ .  
  
Cross-validation is the process of sub-dividing the data into folds, with each fold used once as the validation set:  
  
* Allows for more accurate estimates of model performance on out-of-sample error  
* Each process of CV will work through the data k times (assuming there are k folds)  
	* Repeated CV is the process of running CV multiple times (this is particularly well suited to parallel processing)  
  
Comparing model performance:  
  
* Random forest models tend to be more powerful and capable of classifying the training data (and thus subject to risk of overfits and associated poor quality of test set predictions)  
  
Example code includes:  
```{r cache=TRUE}

voters <- readr::read_csv("./RInputFiles/voters.csv")
voters$turnout16_2016 <- factor(voters$turnout16_2016, levels=c("Did not vote", "Voted"))
str(voters, give.attr = FALSE)

# Print voters
voters

# How many people voted?
voters %>%
    count(turnout16_2016)


# How do the reponses on the survey vary with voting behavior?
voters %>%
    group_by(turnout16_2016) %>%
    summarize(`Elections don't matter` = mean(RIGGED_SYSTEM_1_2016 <= 2),
              `Economy is getting better` = mean(econtrend_2016 == 1),
              `Crime is very important` = mean(imiss_a_2016 == 2))


## Visualize difference by voter turnout
voters %>%
    ggplot(aes(econtrend_2016, ..density.., fill = turnout16_2016)) +
    geom_histogram(alpha = 0.5, position = "identity", binwidth = 1) +
    labs(title = "Overall, is the economy getting better or worse?")


# Remove the case_indetifier column
voters_select <- voters %>%
        select(-case_identifier)

# Build a simple logistic regression model
simple_glm <- glm(turnout16_2016 ~ .,  family = "binomial", 
                  data = voters_select)

# Print the summary                  
summary(simple_glm)


# Split data into training and testing sets
set.seed(1234)
in_train <- caret::createDataPartition(voters_select$turnout16_2016, p = 0.8, list = FALSE)
training <- voters_select[in_train, ]
testing <- voters_select[-in_train, ]


# Perform logistic regression with upsampling and no resampling
vote_glm_1 <- caret::train(turnout16_2016 ~ ., method = "glm", family = "binomial", data = training,
                           trControl = trainControl(method = "none", sampling = "up")
                           )

# Print vote_glm
vote_glm_1


useSmall <- sort(sample(1:nrow(training), round(0.1*nrow(training)), replace=FALSE))
trainSmall <- training[useSmall, ]

# Logistic regression
vote_glm <- caret::train(turnout16_2016 ~ ., method = "glm", family = "binomial", data = trainSmall,
                         trControl = trainControl(method = "repeatedcv", repeats = 2, sampling = "up")
                         )

# Print vote_glm
vote_glm


# Random forest
vote_rf <- caret::train(turnout16_2016 ~ ., method = "rf", data = trainSmall,
                        trControl = trainControl(method="repeatedcv", repeats=2, sampling = "up")
                        )

# Print vote_rf
vote_rf


# Confusion matrix for logistic regression model on training data
caret::confusionMatrix(predict(vote_glm, trainSmall), trainSmall$turnout16_2016)

# Confusion matrix for random forest model on training data
caret::confusionMatrix(predict(vote_rf, trainSmall), trainSmall$turnout16_2016)

# Confusion matrix for logistic regression model on testing data
caret::confusionMatrix(predict(vote_glm, testing), testing$turnout16_2016)

# Confusion matrix for random forest model on testing data
caret::confusionMatrix(predict(vote_rf, testing), testing$turnout16_2016)

```
  
  
  
***
  
Chapter 4 - Nuns  
  
Catholic sisters survey from 1967 - https://curate.nd.edu/show/0r967368551 with codebook at https://curate.nd.edu/downloads/0v838051f6x	 
	
* Responses from 130,000 sisters in ~400 congergations  
* There was significant change occuring during this time period, both in society at large and within the community of nuns  
* Age has been binned in groups of 10 years (has been recoded as a numeric at the top of the range, so 20 will mean 11-20 and 30 will mean 21-30 and the like)  
* Historical dataset, centered in the context of nuns in 1967  
* Good first step is to tidy the data, so that it is easier for exploratory data analysis  
	* sisters67 %>% select(-sister) %>% gather(key, value, -age)  
  
Exploratory data analysis with tidy data:  
  
* Easy to see levels of agreement (overall) using dplyr::count()  
* Agreement with specific questions by age  
	* tidy_sisters %>% filter(key %in% paste0("v", 153:170)) %>% group_by(key, value) %>% summarise(age = mean(age)) %>% ggplot(aes(value, age, color = key)) + geom_line(alpha = 0.5, size = 1.5) + geom_point(size = 2) + facet_wrap(~key)  
	* Can use the mix of responses to make estimates about the ages of the nuns  
* Data will be split in to training, validation, and test sets  
	* The validation set will be used for model selection  
  
Predicting age with supervised learning:  
  
* "rpart" - building a tree-based (CART) model  
* "xgbLinear" - extreme gradient boosting  
* "gbm" - gradient boosted ensembles  
* Validation datasets are useful for assessing hyper-parameters and model choices, leaving the test dataset pure for a final out-of-sample error estimate  
  
Wrap up:  
  
* Train-Validation-Test to select the best models, tune the parameters, and estimate the out-of-sample error rates  
* Dealing with class imbalances; improving performance with resamples (bootstraps, cross-validation, etc.)  
* Hyper-parameter tuning can be valuable, but the time investment in other areas can often generate a greater return  
* Gradient boosting and random forests tend to perform very well, but there is always value in trying out multiple models  
	* Start with EDA and begin with a very simple model  
  
Example code includes:  
```{r cache=TRUE}

sisters67 <- readr::read_csv("./RInputFiles/sisters.csv")
str(sisters67, give.attr = FALSE)


# View sisters67
glimpse(sisters67)

# Plot the histogram
ggplot(sisters67, aes(x = age)) +
    geom_histogram(binwidth = 10)


# Tidy the data set
tidy_sisters <- sisters67 %>%
    select(-sister) %>%
    gather(key, value, -age)

# Print the structure of tidy_sisters
glimpse(tidy_sisters)


# Overall agreement with all questions varied by age
tidy_sisters %>%
    group_by(age) %>%
    summarize(value = mean(value, na.rm = TRUE))

# Number of respondents agreed or disagreed overall
tidy_sisters %>%
    count(value)


# Visualize agreement with age
tidy_sisters %>%
    filter(key %in% paste0("v", 153:170)) %>%
    group_by(key, value) %>%
    summarize(age = mean(age, na.rm = TRUE)) %>%
    ggplot(aes(value, age, color = key)) +
    geom_line(show.legend = FALSE) +
    facet_wrap(~key, nrow = 3)


# Remove the sister column
sisters_select <- sisters67 %>% 
    select(-sister)

# Build a simple linear regression model
simple_lm <- lm(age ~ ., 
                data = sisters_select)

# Print the summary of the model
summary(simple_lm)


# Split the data into training and validation/test sets
set.seed(1234)
in_train <- caret::createDataPartition(sisters_select$age, p = 0.6, list = FALSE)
training <- sisters_select[in_train, ]
validation_test <- sisters_select[-in_train, ]

# Split the validation and test sets
set.seed(1234)
in_test <- caret::createDataPartition(validation_test$age, p = 0.5, list = FALSE)
testing <- validation_test[in_test, ]
validation <- validation_test[-in_test, ]


# Fit a CART model
sisters_cart <- caret::train(age ~ ., method = "rpart", data = training)

# Print the CART model
sisters_cart


inSmall <- sample(1:nrow(training), 500, replace=FALSE)
smallSisters <- training[sort(inSmall), ]

sisters_xgb <- caret::train(age ~ ., method = "xgbTree", data = smallSisters)
sisters_gbm <- caret::train(age ~ ., method = "gbm", data = smallSisters, verbose=FALSE)

# Make predictions on the three models
modeling_results <- validation %>%
    mutate(CART = predict(sisters_cart, validation),
           XGB = predict(sisters_xgb, validation),
           GBM = predict(sisters_gbm, validation))

# View the predictions
modeling_results %>% 
    select(CART, XGB, GBM)


# Compare performace
yardstick::metrics(modeling_results, truth = age, estimate = CART)
yardstick::metrics(modeling_results, truth = age, estimate = XGB)
yardstick::metrics(modeling_results, truth = age, estimate = GBM)


# Calculate RMSE
testing %>%
    mutate(prediction = predict(sisters_gbm, testing)) %>%
    yardstick::rmse(truth = age, estimate = prediction)

```
  
  
  
***
  
### _Business Process Analytics in R_  
  
Chapter 1 - Introduction to Process Analysis  
  
Introduction and overview:  
  
* Efficient processes are core to many businesses, and improved data makes further analysis possible  
* The "internet of things" has created significant amounts of event data - why, what, and who  
	* Why is the purpose  
    * What is the steps in the process  
    * Who is the person responsible for the activity (can be machines or IS or the like; referred to as "resources")  
* Process workflow is iterative across Extraction-Processing-Analysis  
  
Activities as cornerstones of processes:  
  
* Data from an online learning platform; activities are captured and can be used for further analysis  
* Activities describe the flow of the process, and are one of the most important components of the process  
	* bupaR::activities_labels() is like names() for activities data  
    * bupaR::activities() is like summary() for activities data  
* Each case is described by the sequence of activities, known as its "trace"  
	* bupaR::traces() will create a frequency table of the traces  
    * bupaR::trace_explorer() will visualize the cases  
  
Components of process data:  
  
* Cases are the objects flowing through the process, while activities are the actions performed on them  
	* An activity instance is the occurrence of an activity (which can be a series of events) - specific action, case, time, etc.  
    * The "lifecycle status" is an area like Scheduled, Started, Completed, and the like  
    * The "event log" is the journal of the events  
    * The "resources" are the actors in the process  
* Can create an event log using the eventlog() function  
	* event_data %>% eventlog(case_id = "patient", activity_id = "handling", activity_instance_id = "handling_id", timestamp = "time", lifecycle_id = "registration_type", resource = "employee")  
  
Example code includes:  
```{r}

# Load the processmapR package using library
library(processmapR)
library(bupaR)


handling <- c('Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'X-Ray', 'X-Ray', 'X-Ray', 'X-Ray', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'X-Ray', 'X-Ray', 'X-Ray', 'X-Ray', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out')
patient <- c('43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '156', '170', '172', '184', '278', '348', '420', '43', '156', '170', '172', '184', '278', '348', '420', '155', '221', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '156', '170', '172', '184', '278', '348', '420', '43', '156', '170', '172', '184', '278', '348', '420', '155', '221', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493')
employee <- c('r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r5', 'r5', 'r5', 'r5', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r5', 'r5', 'r5', 'r5', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7')
handling_id <- c('43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '543', '655', '656', '670', '672', '684', '721', '778', '848', '920', '955', '993', '1020', '1072', '1081', '1082', '1088', '1127', '1163', '1199', '1257', '1309', '1318', '1319', '1325', '1364', '1400', '1436', '1557', '1587', '1710', '1730', '1777', '1889', '1890', '1904', '1906', '1918', '1955', '2012', '2082', '2154', '2189', '2227', '2272', '2384', '2385', '2399', '2401', '2413', '2450', '2507', '2577', '2649', '2684', '2720', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '543', '655', '656', '670', '672', '684', '721', '778', '848', '920', '955', '993', '1020', '1072', '1081', '1082', '1088', '1127', '1163', '1199', '1257', '1309', '1318', '1319', '1325', '1364', '1400', '1436', '1557', '1587', '1710', '1730', '1777', '1889', '1890', '1904', '1906', '1918', '1955', '2012', '2082', '2154', '2189', '2227', '2272', '2384', '2385', '2399', '2401', '2413', '2450', '2507', '2577', '2649', '2684', '2720')
registration_type <- c('start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete')
rTime <- c('2017-02-19 04:38:51', '2017-06-03 10:05:28', '2017-06-03 10:05:28', '2017-06-17 15:10:30', '2017-06-17 23:00:33', '2017-06-27 07:48:22', '2017-08-03 17:05:27', '2017-09-26 20:22:49', '2017-11-24 08:28:44', '2018-02-08 03:39:21', '2018-03-14 21:04:28', '2018-04-29 04:55:10', '2017-02-19 07:28:53', '2017-06-04 06:27:00', '2017-06-03 13:23:14', '2017-06-17 16:31:58', '2017-06-18 18:29:13', '2017-06-28 00:14:50', '2017-08-04 07:22:06', '2017-09-27 22:57:03', '2017-11-24 10:33:00', '2018-02-08 17:33:12', '2018-03-15 15:12:41', '2018-04-30 19:40:22', '2017-02-20 19:59:18', '2017-06-04 15:18:50', '2017-06-18 22:51:07', '2017-06-21 02:43:27', '2017-07-01 23:55:10', '2017-09-28 22:58:23', '2017-11-25 12:06:18', '2018-02-12 09:01:38', '2017-02-21 06:49:49', '2017-06-04 23:23:28', '2017-06-19 06:44:30', '2017-06-21 11:16:30', '2017-07-02 11:16:08', '2017-09-29 07:28:10', '2017-11-25 21:54:56', '2018-02-12 19:43:42', '2017-06-05 00:12:24', '2017-08-05 08:25:17', '2018-03-17 10:30:24', '2018-05-02 07:32:45', '2017-02-21 14:50:43', '2017-06-05 14:03:19', '2017-06-05 10:26:16', '2017-06-19 22:46:10', '2017-06-22 04:39:35', '2017-07-03 01:28:49', '2017-08-05 22:06:23', '2017-09-29 19:13:51', '2017-11-26 06:52:23', '2018-02-17 02:44:58', '2018-03-18 00:20:51', '2018-05-02 18:14:11', '2017-02-24 14:58:43', '2017-06-05 15:58:53', '2017-06-05 15:58:53', '2017-06-20 03:48:37', '2017-06-22 08:40:55', '2017-07-03 03:39:51', '2017-08-08 23:17:45', '2017-09-29 21:16:01', '2017-11-27 04:56:53', '2018-02-20 09:49:29', '2018-03-18 08:12:07', '2018-05-03 00:11:10', '2017-02-19 07:28:53', '2017-06-03 14:19:00', '2017-06-03 13:23:14', '2017-06-17 16:31:58', '2017-06-18 01:07:42', '2017-06-27 12:22:51', '2017-08-03 19:25:12', '2017-09-26 22:17:18', '2017-11-24 10:33:00', '2018-02-08 06:01:38', '2018-03-15 00:34:01', '2018-04-29 07:39:14', '2017-02-19 21:58:08', '2017-06-04 14:23:26', '2017-06-04 06:27:00', '2017-06-18 04:14:55', '2017-06-19 00:40:19', '2017-06-28 12:48:20', '2017-08-04 21:09:17', '2017-09-28 12:00:12', '2017-11-25 00:44:30', '2018-02-09 07:05:52', '2018-03-16 04:08:03', '2018-05-01 10:37:51', '2017-02-21 03:12:26', '2017-06-04 19:35:51', '2017-06-19 03:01:11', '2017-06-21 08:02:20', '2017-07-02 07:43:48', '2017-09-29 04:58:49', '2017-11-25 18:30:43', '2018-02-12 13:57:13', '2017-02-21 09:57:05', '2017-06-05 02:46:59', '2017-06-19 11:40:53', '2017-06-21 16:09:26', '2017-07-02 16:03:16', '2017-09-29 12:44:39', '2017-11-26 02:40:30', '2018-02-12 23:53:46', '2017-06-05 04:39:38', '2017-08-05 13:56:39', '2018-03-17 14:09:40', '2018-05-02 12:24:41', '2017-02-21 17:57:58', '2017-06-05 15:58:53', '2017-06-05 14:03:19', '2017-06-20 01:44:29', '2017-06-22 08:40:55', '2017-07-03 03:39:51', '2017-08-05 23:53:27', '2017-09-29 21:16:01', '2017-11-26 09:44:37', '2018-02-17 06:17:57', '2018-03-18 03:22:17', '2018-05-02 21:17:12', '2017-02-24 16:03:49', '2017-06-05 17:22:16', '2017-06-05 17:15:30', '2017-06-20 05:36:40', '2017-06-22 10:59:58', '2017-07-03 05:00:48', '2017-08-09 00:13:39', '2017-09-29 23:42:48', '2017-11-27 06:53:23', '2018-02-20 12:04:00', '2018-03-18 10:48:34', '2018-05-03 02:11:42')
rOrder <- c(43, 155, 156, 170, 172, 184, 221, 278, 348, 420, 455, 493, 543, 655, 656, 670, 672, 684, 721, 778, 848, 920, 955, 993, 1020, 1072, 1081, 1082, 1088, 1127, 1163, 1199, 1257, 1309, 1318, 1319, 1325, 1364, 1400, 1436, 1557, 1587, 1710, 1730, 1777, 1889, 1890, 1904, 1906, 1918, 1955, 2012, 2082, 2154, 2189, 2227, 2272, 2384, 2385, 2399, 2401, 2413, 2450, 2507, 2577, 2649, 2684, 2720, 2764, 2876, 2877, 2891, 2893, 2905, 2942, 2999, 3069, 3141, 3176, 3214, 3264, 3376, 3377, 3391, 3393, 3405, 3442, 3499, 3569, 3641, 3676, 3714, 3741, 3793, 3802, 3803, 3809, 3848, 3884, 3920, 3978, 4030, 4039, 4040, 4046, 4085, 4121, 4157, 4278, 4308, 4431, 4451, 4498, 4610, 4611, 4625, 4627, 4639, 4676, 4733, 4803, 4875, 4910, 4948, 4993, 5105, 5106, 5120, 5122, 5134, 5171, 5228, 5298, 5370, 5405, 5441)

pFrame <- tibble(handling=factor(handling, levels=c('Blood test', 'Check-out', 'Discuss Results', 'MRI SCAN', 'Registration', 'Triage and Assessment', 'X-Ray')), 
                 patient=patient, 
                 employee=factor(employee, levels=c('r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7')), 
                 handling_id=handling_id, 
                 registration_type=factor(registration_type, levels=c("complete", "start")), 
                 time=as.POSIXct(rTime), 
                 .order=rOrder
                 )

patients <- eventlog(pFrame,
    case_id = "patient",
    activity_id = "handling",
    activity_instance_id = "handling_id",
    lifecycle_id = "registration_type",
    timestamp = "time",
    resource_id = "employee")


# The function slice can be used to take a slice of cases out of the eventdata. slice(1:10) will select the first ten cases in the event log, where first is defined by the current ordering of the data.

# How many patients are there?
n_cases(patients)

# Print the summary of the data
summary(patients)

# Show the journey of the first patient
slice(patients, 1)


# How many distinct activities are there?
n_activities(patients)

# What are the names of the activities?
activity_labels(patients)

# Create a list of activities
activities(patients)


# Have a look at the different traces
traces(patients)

# How many are there?
n_traces(patients)

# Visualize the traces using trace_explorer
trace_explorer(patients, coverage=1)

# Draw process map
process_map(patients)


claims <- tibble(id=c("claim1", "claim1", "claim2", "claim2", "claim2"), 
                 action=c(10002L, 10011L, 10015L, 10024L, 10024L), 
                 action_type=c("Check Contract", "Pay Back Decision", "Check Contract", "Pay Back Decision", "Pay Back Decision"), 
                 date=as.Date(c("2008-01-12", "2008-03-22", "2008-01-13", "2008-03-23", "2008-04-14")), 
                 originator=c("Assistant 1", "Manager 2", "Assistant 6", "Manager 2", "Manager 2"), 
                 status=as.factor(c("start", "start", "start", "start", "complete"))
                 )
claims


#create eventlog claims_log 
claims_log <- eventlog(claims,
    case_id = "id",
    activity_id = "action_type",
    activity_instance_id = "action",
    lifecycle_id = "status",
    timestamp = "date",
    resource_id = "originator")

# Print summary
summary(claims_log)

# Check activity labels
activity_labels(claims_log)

# Once you have an eventlog, you can access its complete metadata using the function mapping or the functions case_id, activity_id etc., to inspect individual identifiers.

```
  
  
  
***
  
Chapter 2 - Analysis Techniques  
  
Organizational analysis:  
  
* Processes are always dependent on resources, even if automated (machines and algorithms can be resources)  
	* Who executes the task, how specialized is the knowledge, etc.  
    * resource_labels(log_hospital)  # will pull out the resources  
    * resources(log_hospital)  # will pull out frequencies by resource  
* Can create a resource-activity matrix  
	* A person who performs only a few activities is considered to be specialized in that activity  
    * If only one person ever performs a specific activity, then there is a high risk of "brain drain"  
    * The plot() function, applied to an event_log, will create the resource-activity matrix  
    * resource_map(log_hospital)  # shows arrows between the work flows  
  
Structuredness:  
  
* Control-flow refers to the succession of activities  
	* Each unique flow is referred to as a trace  
    * Metrics include entry/exit points, length of cases, presence of activities, rework, etc.  
    * log_healthcare %>% start_activities("activity") %>% plot()  
    * log_healthcare %>% end_activities("activity") %>% plot()  
* Rework is when the same activity is done multiple times for the same case  
	* Repetitions are when the activity is repeated after some intervening steps  
    * Sel-loops are when the activity is repeated immediately after itself  
* The precedence matrix shows the relationships between the activities in a more structured manner  
	* eventlog %>% precedence_matrix(type = "absolute") %>% plot  # can be type="relative" also  
  
Performance analysis:  
  
* Visuals can include performance process maps and dotted charts; metrics can include throughput time, processing time, idle time  
	* eventlog %>% process_map(type = frequency())  # normal process map  
    * eventlog %>% process_map(type = performance())  # performance process map  
* The dotted chart shows the freqency of activities over time; basically, a form of scatter plot  
	* throughput_time is total time, processing_time is the sum of activity time, idle_time is the sume of when nothing is happening  
  
Linking perspectives:  
  
* Granularity can help give the statistics at the desired levels  
	* <process_metric>(level = "log", "trace", "case", "activity", "resource", "resource-activity")  
* Categorical data can be leveraged using the group_by() functionality - each group will then be calculated separately  
	* eventlog %>% group_by(priority) %>% number_of_repetitions(level = "resource") %>% plot()  
  
Example code includes:  
```{r}


data(sepsis, package="eventdataR")
str(sepsis)


# Print list of resources
resource_frequency(sepsis, level="resource")

# Number of resources per activity
resource_frequency(sepsis, level = "activity")

# Plot Number of executions per resource-activity (not working in R 3.5.3)
# resource_frequency(sepsis, level = "resource-activity") %>% plot


# Calculate resource involvement
resource_involvement(sepsis, level="resource")

# Show graphically 
sepsis %>% resource_involvement(level = "resource") %>% plot

# Compare with resource frequency
resource_frequency(sepsis, level="resource")


# Min, max and average number of repetitions
sepsis %>% number_of_repetitions(level = "log")

# Plot repetitions per activity
sepsis %>% number_of_repetitions(level = "activity") %>% plot

# Number of repetitions per resources
sepsis %>% number_of_repetitions(level = "resource")


eci <- c('21', '21', '21', '21', '21', '21', '21', '21', '21', '31', '31', '31', '31', '31', '31', '31', '31', '31', '31', '41', '41', '41', '41', '41', '41', '41', '51', '51', '51', '51', '51', '51', '51', '61', '61', '61', '61', '61', '61', '91', '91', '91', '91', '91', '91', '101', '101', '101', '101', '101', '101', '111', '111', '111', '111', '121', '121', '121', '121', '121', '121', '121', '121', '121', '131', '131', '131', '131', '131', '131', '131', '131', '161', '161', '171', '171', '171', '171', '181', '181', '181', '181', '181', '181', '201', '201', '201', '201', '201', '201', '201', '12', '12', '12', '12', '12', '22', '22', '22', '22', '22', '22', '32', '32', '32', '32', '32', '32', '42', '42', '42', '42', '52', '52', '52', '52', '52', '82', '82', '82', '82', '82', '92', '92', '92', '92', '92', '102', '102', '102', '102', '102', '112', '112', '122', '122', '21', '21', '21', '21', '21', '21', '21', '21', '21', '31', '31', '31', '31', '31', '31', '31', '31', '31', '31', '41', '41', '41', '41', '41', '41', '41', '51', '51', '51', '51', '51', '51', '51', '61', '61', '61', '61', '61', '61', '91', '91', '91', '91', '91', '91', '101', '101', '101', '101', '101', '101', '111', '111', '111', '111', '121', '121', '121', '121', '121', '121', '121', '121', '121', '131', '131', '131', '131', '131', '131', '131', '131', '161', '161', '171', '171', '171', '171', '181', '181', '181', '181', '181', '181', '201', '201', '201', '201', '201', '201', '201', '12', '12', '12', '12', '12', '22', '22', '22', '22', '22', '22', '32', '32', '32', '32', '32', '32', '42', '42', '42', '42', '52', '52', '52', '52', '52', '82', '82', '82', '82', '82', '92', '92', '92', '92', '92', '102', '102', '102', '102', '102', '112', '112', '122', '122')
ea1 <- c('prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'prepareBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'snack', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'eatingLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast')
ea2 <- c('eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'prepareBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'snack', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'eatingLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast')
eaii <- c('9', '10', '19', '23', '24', '26', '36', '40', '41', '51', '52', '58', '60', '62', '63', '67', '69', '72', '73', '86', '87', '89', '90', '104', '105', '107', '119', '120', '128', '132', '133', '138', '139', '149', '150', '156', '159', '160', '164', '174', '175', '192', '194', '195', '198', '205', '206', '208', '211', '213', '214', '229', '236', '237', '239', '245', '251', '252', '253', '255', '259', '260', '262', '264', '271', '276', '281', '287', '292', '293', '297', '299', '310', '312', '331', '332', '336', '347', '363', '364', '374', '376', '387', '389', '434', '435', '447', '448', '450', '453', '454', '462', '463', '471', '472', '475', '483', '484', '487', '491', '492', '496', '508', '509', '512', '517', '518', '522', '536', '540', '541', '543', '562', '563', '565', '566', '572', '584', '585', '589', '590', '598', '615', '616', '618', '619', '627', '639', '640', '642', '643', '653', '665', '666', '682', '683', '9', '10', '19', '23', '24', '26', '36', '40', '41', '51', '52', '58', '60', '62', '63', '67', '69', '72', '73', '86', '87', '89', '90', '104', '105', '107', '119', '120', '128', '132', '133', '138', '139', '149', '150', '156', '159', '160', '164', '174', '175', '192', '194', '195', '198', '205', '206', '208', '211', '213', '214', '229', '236', '237', '239', '245', '251', '252', '253', '255', '259', '260', '262', '264', '271', '276', '281', '287', '292', '293', '297', '299', '310', '312', '331', '332', '336', '347', '363', '364', '374', '376', '387', '389', '434', '435', '447', '448', '450', '453', '454', '462', '463', '471', '472', '475', '483', '484', '487', '491', '492', '496', '508', '509', '512', '517', '518', '522', '536', '540', '541', '543', '562', '563', '565', '566', '572', '584', '585', '589', '590', '598', '615', '616', '618', '619', '627', '639', '640', '642', '643', '653', '665', '666', '682', '683')
elci <- c('start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete')
ets1 <- c('2012-11-12 09:42:02', '2012-11-12 09:52:33', '2012-11-12 11:05:44', '2012-11-12 13:45:49', '2012-11-12 13:48:49', '2012-11-12 15:23:00', '2012-11-12 18:47:29', '2012-11-12 22:35:21', '2012-11-12 22:35:21', '2012-11-13 08:56:37', '2012-11-13 09:04:54', '2012-11-13 10:14:04', '2012-11-13 13:47:45', '2012-11-13 14:08:24', '2012-11-13 14:19:01', '2012-11-13 17:34:23', '2012-11-13 18:51:51', '2012-11-13 23:05:07', '2012-11-13 23:17:07', '2012-11-14 09:06:08', '2012-11-14 09:17:48', '2012-11-14 10:38:16', '2012-11-14 10:44:16', '2012-11-14 21:30:09', '2012-11-14 21:37:09', '2012-11-14 22:14:23', '2012-11-15 09:37:15', '2012-11-15 09:47:12', '2012-11-15 10:11:08', '2012-11-15 14:35:27', '2012-11-15 14:41:27', '2012-11-15 22:07:26', '2012-11-15 22:26:02', '2012-11-16 10:39:14', '2012-11-16 10:52:56', '2012-11-16 12:09:10', '2012-11-16 14:13:00', '2012-11-16 14:19:00', '2012-11-16 18:11:36', '2012-11-19 10:13:23', '2012-11-19 10:25:00', '2012-11-19 15:55:22', '2012-11-19 21:47:27', '2012-11-19 21:59:27', '2012-11-19 22:31:06', '2012-11-20 10:20:00', '2012-11-20 10:21:02', '2012-11-20 11:00:16', '2012-11-20 13:03:28', '2012-11-20 14:25:11', '2012-11-20 14:41:22', '2012-11-21 10:01:00', '2012-11-21 15:02:08', '2012-11-21 15:15:08', '2012-11-21 17:50:29', '2012-11-22 01:40:42', '2012-11-22 10:19:15', '2012-11-22 10:26:15', '2012-11-22 11:02:27', '2012-11-22 11:56:06', '2012-11-22 15:05:51', '2012-11-22 15:12:55', '2012-11-22 16:43:08', '2012-11-22 18:15:32', '2012-11-23 00:36:00', '2012-11-23 01:03:00', '2012-11-23 09:49:00', '2012-11-23 12:53:06', '2012-11-23 14:01:08', '2012-11-23 14:23:08', '2012-11-23 16:57:24', '2012-11-23 17:58:00', '2012-11-26 09:06:12', '2012-11-26 09:57:12', '2012-11-27 10:20:26', '2012-11-27 10:30:50')
ets2 <- c('2012-11-27 11:54:15', '2012-11-27 19:46:15', '2012-11-28 09:27:15', '2012-11-28 09:34:15', '2012-11-28 12:28:02', '2012-11-28 13:16:33', '2012-11-28 19:30:08', '2012-11-28 22:15:02', '2012-11-30 10:43:19', '2012-11-30 10:46:19', '2012-11-30 14:51:36', '2012-11-30 15:08:36', '2012-11-30 17:30:40', '2012-11-30 22:12:05', '2012-11-30 22:16:07', '2011-11-28 10:38:00', '2011-11-28 10:43:00', '2011-11-28 14:31:06', '2011-11-28 14:42:00', '2011-11-28 20:20:55', '2011-11-29 12:09:09', '2011-11-29 12:11:01', '2011-11-29 13:25:29', '2011-11-29 15:15:14', '2011-11-29 15:23:00', '2011-11-29 16:32:20', '2011-11-30 10:23:46', '2011-11-30 10:28:46', '2011-11-30 13:05:27', '2011-11-30 14:39:42', '2011-11-30 14:56:00', '2011-11-30 16:41:05', '2011-11-30 14:37:00', '2011-12-01 11:17:05', '2011-12-01 11:20:05', '2011-12-01 14:29:37', '2011-12-02 12:29:08', '2011-12-02 12:32:08', '2011-12-02 14:47:18', '2011-12-02 14:51:00', '2011-12-02 19:40:44', '2011-12-05 12:15:45', '2011-12-05 12:18:05', '2011-12-05 15:00:55', '2011-12-05 15:14:00', '2011-12-05 19:24:11', '2011-12-06 11:30:19', '2011-12-06 11:33:02', '2011-12-06 14:41:16', '2011-12-06 14:56:00', '2011-12-06 19:22:50', '2011-12-07 11:12:17', '2011-12-07 11:17:22', '2011-12-07 14:04:32', '2011-12-07 14:14:00', '2011-12-07 19:23:55', '2011-12-08 11:25:12', '2011-12-08 11:29:01', '2011-12-09 11:00:13', '2011-12-09 11:03:33', '2012-11-12 09:50:02', '2012-11-12 09:55:29', '2012-11-12 12:39:42', '2012-11-12 14:48:14', '2012-11-12 14:53:14', '2012-11-12 15:31:53', '2012-11-12 19:00:56', '2012-11-12 22:37:55', '2012-11-12 22:40:55', '2012-11-13 09:00:26', '2012-11-13 09:10:12', '2012-11-13 10:51:55', '2012-11-13 14:03:31', '2012-11-13 14:18:36', '2012-11-13 14:42:36', '2012-11-13 17:36:34', '2012-11-13 19:45:03', '2012-11-13 23:15:33', '2012-11-13 23:37:33', '2012-11-14 09:09:41', '2012-11-14 09:21:43', '2012-11-14 11:43:23', '2012-11-14 11:06:23', '2012-11-14 21:35:17', '2012-11-14 21:47:18', '2012-11-14 22:17:47', '2012-11-15 09:44:06', '2012-11-15 09:48:08', '2012-11-15 10:23:49', '2012-11-15 15:40:32', '2012-11-15 15:46:32', '2012-11-15 22:22:44', '2012-11-15 22:31:00', '2012-11-16 10:42:13') 
ets3 <- c('2012-11-16 10:52:58', '2012-11-16 12:09:57', '2012-11-16 14:58:55', '2012-11-16 14:55:55', '2012-11-16 18:14:49', '2012-11-19 10:17:12', '2012-11-19 10:33:59', '2012-11-19 16:07:49', '2012-11-19 21:59:01', '2012-11-19 22:24:58', '2012-11-19 22:31:59', '2012-11-20 10:21:02', '2012-11-20 10:37:51', '2012-11-20 11:14:44', '2012-11-20 13:28:35', '2012-11-20 14:40:16', '2012-11-20 15:10:16', '2012-11-21 10:06:50', '2012-11-21 15:14:47', '2012-11-21 15:30:55', '2012-11-21 17:55:48', '2012-11-22 01:45:42', '2012-11-22 10:25:45', '2012-11-22 10:59:45', '2012-11-22 11:10:30', '2012-11-22 12:09:07', '2012-11-22 15:12:19', '2012-11-22 15:26:18', '2012-11-22 16:51:54', '2012-11-22 18:17:25', '2012-11-23 00:41:13', '2012-11-23 10:28:57', '2012-11-23 10:01:57', '2012-11-23 12:57:33', '2012-11-23 14:20:47', '2012-11-23 14:38:47', '2012-11-23 16:57:43', '2012-11-23 18:06:38', '2012-11-26 10:37:28', '2012-11-26 10:05:28', '2012-11-27 10:30:43', '2012-11-27 10:44:43', '2012-11-27 11:54:59', '2012-11-27 19:46:56', '2012-11-28 09:33:52', '2012-11-28 09:44:52', '2012-11-28 12:57:42', '2012-11-28 13:38:45', '2012-11-28 19:45:20', '2012-11-28 22:18:43', '2012-11-30 11:45:40', '2012-11-30 11:51:40', '2012-11-30 15:05:54', '2012-11-30 15:20:00', '2012-11-30 17:42:59', '2012-11-30 22:15:48', '2012-11-30 22:39:48', '2011-11-28 10:42:55', '2011-11-28 10:49:00', '2011-11-28 14:41:54', '2011-11-28 15:04:00', '2011-11-28 20:20:59', '2011-11-29 12:10:37', '2011-11-29 12:19:00', '2011-11-29 13:25:32', '2011-11-29 15:22:57', '2011-11-29 15:49:00', '2011-11-29 16:32:23', '2011-11-30 10:27:58', '2011-11-30 10:38:58', '2011-11-30 13:05:31', '2011-11-30 14:55:24', '2011-11-30 15:11:00', '2011-11-30 16:41:09', '2011-11-30 15:08:00', '2011-12-01 11:19:43', '2011-12-01 11:29:43', '2011-12-01 14:36:38', '2011-12-02 12:31:10', '2011-12-02 12:37:10', '2011-12-02 14:50:19', '2011-12-02 15:24:00', '2011-12-02 19:40:50', '2011-12-05 12:17:58', '2011-12-05 12:26:02', '2011-12-05 15:13:55', '2011-12-05 15:42:00', '2011-12-05 19:24:16', '2011-12-06 11:32:49', '2011-12-06 11:38:51', '2011-12-06 14:55:18', '2011-12-06 15:18:18', '2011-12-06 19:22:55', '2011-12-07 11:17:14', '2011-12-07 11:22:35', '2011-12-07 14:13:34', '2011-12-07 14:41:00', '2011-12-07 20:38:18', '2011-12-08 11:28:24', '2011-12-08 11:35:55', '2011-12-09 11:03:09', '2011-12-09 11:09:08')
etsF <- c(ets1, ets2, ets3)

eatData <- tibble(case_id=eci, 
                  activity=factor(c(ea1, ea2)), 
                  activity_instance_id=eaii, 
                  lifecycle_id=factor(elci), 
                  resource=factor("UNDEFINED"), 
                  timestamp=as.POSIXct(etsF)
                  )

eat_patterns <- eventlog(eatData,
    case_id = "case_id",
    activity_id = "activity",
    activity_instance_id = "activity_instance_id",
    lifecycle_id = "lifecycle_id",
    timestamp = "timestamp",
    resource_id = "resource")


# Create performance map
eat_patterns %>% process_map(type = performance(FUN = median, units = "hours"))

# Inspect variation in activity durations graphically
eat_patterns %>% processing_time(level = "activity") %>% plot()

# Draw dotted chart
eat_patterns %>% dotted_chart(x = "relative_day", sort = "start_day", units = "secs")


# Time per activity
# daily_activities %>% processing_time(level = "activity") %>% plot

# Average duration of recordings
# daily_activities %>% throughput_time(level="log", units = "hours")

# Missing activities
# daily_activities %>% idle_time(level="log", units = "hours")


# Distribution throughput time
# vacancies %>% throughput_time(units="days")

# Distribution throughput time per department
# vacancies %>% group_by(vacancy_department) %>% throughput_time(units="days") %>% plot()

# Repetitions of activities
# vacancies %>% number_of_repetitions(level = "activity") %>% arrange(-relative)

```
  
  
  
***
  
Chapter 3 - Event Data Processing  
  
Filtering cases:  
  
* Sometimes there are too many cases, too many activities, missing data, and the like  
	* Can filter by either cases or events (time periods or specific activity types)  
    * Three levels of cases - performance, control-flow, and time frame  
* Look at long cases for what went wrong, and short cases for what to mimic  
	* filter_throughput_time(log, interval = c(5,10))  # absolute case length is 5-10 days  
    * filter_throughput_time(log, percentage = 0.5)  # shortest 50% of the cases  
    * filter_throughput_time(log, interval = c(5,10), units = "days", reverse =TRUE)  # cases that are NOT 5-10 days  
    * filter_throughput_time(log, interval = c(5,NA), units = "days") # cases longer than 5 days  
* Control-flow filters can be based on activity presence/absence, timing, and the like  
  
Filtering events - trim, frequency, label, general attribute:  
  
* Can trim to a time period based on start or end  
	* filter_time_period(log, interval = ymd(c("20180110","20180122")), filter_method = "trim")  # discards everything else  
* Can trim based on a specific start and end activities  
	* filter_trim(start_activities = "blues")  # traces that have no blues will be discarded  
    * filter_trim(start_activities = "blues", end_activities = "greens")  # traces that do not have blues followed by greens will be discarded  
    * Can set reverse=TRUE to get the opposites of these  
* Can filter by frequencies by either activity or resource  
	* filter_activity_frequency(log, interval = c(50,100))  
    * filter_activity_frequency(log, percentage = 0.8)  
    * filter_resource_frequency(log, interval = c(60,900))  
    * filter_resource_frequency(log, percentage = 0.6)  
* Can filter by labels  
	* filter_activity(log, activities = c("reds","oranges","purples")))  
    * dplyr::filter(log, cost > 1000, priority == "High", ...)  
  
Aggregating events - Is-A and Part-of:  
  
* The Is-A is when there are many subtypes of activity that are really all part of a main activity  
	* act_unite(log, "New name" = c("Old Variant 1","Old Variant 2","Old Variant 3"), ...)  # same number of activity instances, just fewer names  
* The Part-of is when there are clearly distinct activities that can also be considered components of a higher-level activity  
	* act_collapse(log, "Sub process" = c("Part 1","Part 2","Part 3"), ...)  # fewer number of activity instances, as they are collapsed to a single activity  
  
Enriching events - mutation (adding calculated variables):  
  
* The dplyr::mutate() can be used to directly add variables such as the cost  
	* log %>% group_by_case() %>% mutate(total_cost = sum(cost, na.rm = TRUE)  # group_by_case() is a function applied to event logs  
    * log %>% group_by_case() %>% mutate(total_cost = sum(cost, na.rm = TRUE) %>% mutate(impact = case_when(cost <= 1000 ~ "Low", cost <= 5000 ~ "Medium", TRUE ~ "High"))  
    * log %>% group_by_case() %>% mutate(refund_made = any(str_detect(activity, "Pay Claim")))  
* Metric functions can be used directly, with apped=TRUE, to both calculate the metric and add to the event log  
	* log %>% througput_time(level = "case", units = "days", append = TRUE) %>% mutate(on_time = processing_time_case <= 7)  
  
Example code includes:  
```{r eval=FALSE}

# Select top 20% of cases according to trace frequency
happy_path <- filter_trace_frequency(vacancies, percentage = 0.2)

# Visualize using process map
happy_path %>% process_map(type=requency(value = "absolute_case"))

# Compute throughput time
happy_path %>% throughput_time(units="days")


# Find no_declines
no_declines <- filter_activity_presence(vacancies, activities = "Decline Candidate", reverse=TRUE)

# What is the average number of  
first_hit <- filter_activity_presence(vacancies, activities = c("Send Offer", "Offer Accepted"), method="all")

# Create a performance map
first_hit %>% process_map(type=performance())

# Compute throughput time
first_hit %>% throughput_time()


# Create not_refused
not_refused <- vacancies %>% filter_precedence(antecedents = "Receive Response", consequents = "Review Non Acceptance", precedence_type = "directly_follows", filter_method = "none") 

# Select longest_cases
worst_cases <- not_refused %>% filter_throughput_time(interval=c(300, NA))

# Show the different traces
worst_cases %>% trace_explorer(coverage=1)


# Select activities
disapprovals <- vacancies %>% filter_activity(activities=c("Construct Offer", "Disapprove Offer", "Revise Offer","Disapprove Revision", "Restart Procedure"))

# Explore traces
disapprovals %>% trace_explorer(coverage=0.8)

# Performance map
disapprovals %>% process_map(type = performance(FUN = sum, units = "weeks"))


# Select cases
high_paid <- vacancies %>% filter(vacancy_department=="R&D", vacancy_salary_range==">100000")

# Most active resources
high_paid %>% resource_frequency(level="resource")

# Create a dotted chart
high_paid %>% dotted_chart(x="absolute", sort="start")

# Filtered dotted chart
library(lubridate)
high_paid %>% filter_time_period(interval = ymd(c("20180321","20180620")), filter_method = "trim") %>% dotted_chart(x="absolute", sort="start")


# Count activities and instances
n_activities(vacancies)
n_activity_instances(vacancies)

# Combine activities
united_vacancies <- vacancies %>% 
    act_unite("Disapprove Contract Offer" = c("Disapprove Offer","Disapprove Revision"),
              "Approve Contract Offer" = c("Approve Offer","Approve Revision"), 
              "Construct Contract Offer" = c("Construct Offer","Revise Offer")
              )
              
# Count activities and instances
n_activities(united_vacancies)
n_activity_instances(united_vacancies)


# Aggregate sub processes
aggregated_vacancies <- act_collapse(united_vacancies, 
                            "Interviews" = c("First Interview","Second Interview","Third Interview"),
                            "Prepare Recruitment" = c("Publish Position","File Applications","Check References"),
                            "Create Offer" = c("Construct Contract Offer", "Disapprove Contract Offer", "Approve Contract Offer")
                            )

# Calculated number of activities and activity instances
n_activities(aggregated_vacancies)
n_activity_instances(aggregated_vacancies)

# Create performance map
aggregated_vacancies %>% process_map(type=performance())


# Add total_cost
vacancies_cost <- vacancies %>% 
    group_by_case() %>% 
    mutate(total_cost = sum(activity_cost, na.rm = TRUE))

# Add cost_impact
vacancies_impact <- vacancies_cost %>%




# Compute throughput time per impact
vacancies_impact %>% group_by(cost_impact) %>% throughput_time(units = "weeks") %>% plot()


# Create cost_profile
vacancies_profile <- vacancies_impact %>%
    mutate(cost_profile = case_when(cost_impact == "High" & urgency < 7 ~ "Disproportionate",
                                    cost_impact == "Medium" & urgency < 5 ~ "Excessive",
                                    cost_impact == "Low" & urgency > 6 ~ "Lacking",
                                    TRUE ~ "Appropriate")) 

# Compare number of cases 
vacancies_profile %>% 
    group_by(cost_profile) %>%
    n_cases()
    
# Explore lacking traces
vacancies_profile %>%
  filter(cost_profile == "Lacking") %>%
  process_map()

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Preparing the event data - example includes data from Sales, Purchasing, Manufacturing, Packaging & Delivery, Accounting:  
  
* While all departments need to work together, it is common for each department to have different data, business rules, relational data, etc.  
* Need to create event data first prior to running anything in the bupar package  
* Various field names (ends in _at or _by) may indicate the timing and resource levels  
* The tidyverse tools are helpful for creating the initial data  
  
Getting to know the process:  
  
* Identify data sources, transform so that each row is an event, harmonize them, create an eventlog  
* Start with high-level understanding of the process - summary(otc)  
	* activity_presence(otc) %>% plot()  
    * trace_length(otc) %>% plot()  
    * start_activities(otc, "activity") %>% plot()  
    * end_activities(otc, "activity") %>% plot()  
  
Roles and rules:  
  
* Parallel activities can be run in any order, which can cause an explosion in the number of traces - collapsing can help with abstraction  
* Research questions may be related to performance, compliance, etc.  
* The "4-eye" pricniple says that certain activities should not be performed by the same person  
  
Fast production, fast delivery:  
  
* Dotted charts can show the progression of the cases - request for quotation may be declined, or the offer may only be sent (no response)  
* May want to look at the performance by stages (sub-groups of activities), for more fair comparisons  
  
Course recap:  
  
* Process maps  
* Process analytics  
* Data preprocessing  
* Analysis and use cases  
  
Example code includes:  
```{r cache=TRUE}

quotations <- readRDS("./RInputFiles/otc_quotations.RDS")

# Inspect quotations
str(quotations)

# Create offer_history
offer_history <- quotations %>%
    gather(key, value, -quotation_id) %>%
    separate(key, into = c("activity", "info"))

# Recode the key variable
offer_history <- offer_history %>%
    mutate(info = fct_recode(info,  "timestamp" = 'at',  "resource" = 'by'))

# Spread the info variable
offer_history <- offer_history %>%
    spread(info, value)


validations <- readRDS("./RInputFiles/otc_validations.RDS")

# Inspect validations
str(validations)

# Create validate_history
validate_history <- validations %>%
    mutate(
        activity = "Validate",
        action = paste(quotation_id, "validate",  sep = "-"))

# Gather the timestamp columns
validate_history <- validate_history  %>%
    gather(lifecycle, timestamp, started, completed)


# Recode the lifecycle column of validate_history
validate_history <- validate_history %>%
    mutate(lifecycle = fct_recode(lifecycle,
                "start" = "started",
                "complete" = "completed"))


# Add lifecycle and action column to offer_history
offer_history <- offer_history %>%
    mutate(
        lifecycle = "complete",
        action = paste(quotation_id, 1:n(), sep = "-"))

# Create sales_history
sales_history <- bind_rows(validate_history, offer_history)


sales_history <- readRDS("./RInputFiles/otc_sales_history.RDS")
order_history <- readRDS("./RInputFiles/otc_order_history.RDS")
# sales_quotations <- readRDS("./RInputFiles/otc_sales_quotation.RDS")

str(sales_history)
str(order_history)
# str(sales_quotations)

order_history <- order_history %>% 
    rename(timestamp=time, lifecycle=status) %>%
    select(-activity_cost) %>%
    mutate(activity=as.character(activity), 
           resource=as.character(activity), 
           lifecycle=as.character(lifecycle)
           )
sales_history <- sales_history %>%
    mutate(timestamp=lubridate::as_datetime(timestamp))

# sales_history <- sales_history %>% left_join(sales_quotations)
otc <- bind_rows(sales_history, order_history)


# Create the eventlog object 
otc <- otc %>%
    mutate(case_id = paste(quotation_id, sales_order_id, sep = "-")) %>%
    eventlog(
        case_id = "case_id",
        activity_id = "activity",
        activity_instance_id = "action",
        timestamp = "timestamp",
        resource_id = "resource",
        lifecycle_id = "lifecycle"
        )

# Create trace coverage graph
trace_coverage(otc, level="trace") %>% plot()

# Explore traces
otc %>%
    trace_explorer(coverage = 0.25)


# Collapse activities
otc_high_level <- act_collapse(otc, "Delivery" = c(
  "Handover To Deliverer",
  "Order Delivered",
  "Present For Collection",
  "Order Fetched")
  )

# Draw a process map
process_map(otc_high_level)

# Redraw the trace coverage graph
otc_high_level %>% trace_coverage(level="trace") %>% plot()

# Compute activity wise processing time
otc_high_level %>% processing_time(level="activity", units="days")

# Plot a resource activity matrix of otc (does not work in R 3.5.3)
# otc %>% resource_frequency(level = "resource-activity") %>% plot()


# Create otc_selection
otc_selection <- otc %>% filter_activity(activities = c("Send Quotation","Send Invoice"))

# Explore traces
otc %>% trace_explorer(coverage=1)

# Draw a resource map
otc_selection %>% resource_map()


# Create otc_returned
otc_returned <- otc %>% filter_activity_presence("Return Goods")

# Compute percentage of returned orders
n_cases(otc_returned)/n_cases(otc)

# Trim cases and visualize
otc_returned %>% filter_trim(start_activities="Return Goods") %>% process_map()


# Time from order to delivery
# otc %>% filter_trim(start_activities="Receive Sales Order", end_activities="Order Delivered") %>% 
#     processing_time(units="days")


# Plot processing time by type
# otc %>%
#     group_by(type) %>%
#     throughput_time() %>%
#     plot()

```
  
  
  
***
  
### _Network Science in R - A Tidy Approach_  
  
Chapter 1 - Hubs of the Network  
  
Network science - include social networks, neural networks, etc.:  
  
* Nodes and edges (connections between nodes, aka "ties") make up a network  
	* In a directed network, ties have a direction (for example, followers and follwing)  
    * In an undirected network, ties do not have a direction (for example, mutual friendship)  
    * In a weighted network, the ties have an associated weight (such as bandwidth, duration of friendship, etc.)  
* Chapter will focus on the terrorism network associated with the Madrid train bombing of 2004  
	* Ties include friendhsip, training camps, previous attacks, and other terrorists  
* The network is reflected in tidy fashion, using one data frame for nodes and another for ties  
    * g <- igraph::graph_from_data_frame(d = ties, directed = FALSE, vertices = nodes)  
    * V(g); vcount(g)  
    * E(g); ecount(g)  
* And, then working with attributes of the network  
    * g$name <- "Madrid network"; g$name  
    * V(g)$id <- 1:vcount(g)  
    * E(g)$weight  
  
Visualizing networks:  
  
* The ggraph package can help with visualizing networks  
	* ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = weight)) + geom_node_point()  
    * Much like the language of ggplot2  
  
Centrality measures:  
  
* Objective is to find the most important nodes - connections among members of the networks  
* Network science is a spinoff of data science, with the goal of measuring networks  
* The agree of "degree" measures the number of ties (edges) that a node has  
	* degree(g) # gives the number of edges per node  
    * strength(g) # sumes the weights of the edges per node  
  
Example code includes:  
```{r eval=FALSE}

# read the nodes file into the variable nodes
nodes <- readr::read_csv("./RInputFiles/nodes.csv")
nodes

# read the ties file into the variable ties
ties <- readr::read_csv("./RInputFiles/ties.csv")
ties


library(igraph)
library(ggraph)


# make the network from the data frame ties and print it
g <- graph_from_data_frame(ties, directed = FALSE, vertices = nodes)
g

# explore the set of nodes
V(g)

# print the number of nodes
vcount(g)

# explore the set of ties
E(g)

# print the number of ties
ecount(g)


# give the name "Madrid network" to the network and print the network `name` attribute
g$name <- "Madrid network"
g$name

# add node attribute id and print the node `id` attribute
V(g)$id <- 1:vcount(g)
V(g)$id

# print the tie `weight` attribute
E(g)$weight

# print the network and spot the attributes
g


# visualize the network with layout Kamada-Kawai
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# add an id label to nodes
ggraph(g, layout = "with_kk") +
  geom_edge_link(aes(alpha = weight)) +
  geom_node_point()  + 
  geom_node_text(aes(label = id), repel=TRUE)


# visualize the network with circular layout. Set tie transparency proportional to its weight
ggraph(g, layout = "in_circle") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# visualize the network with grid layout. Set tie transparency proportional to its weight
ggraph(g, layout = "grid") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# compute the degrees of the nodes
dgr <- degree(g)

# add the degrees to the data frame object
nodes <- mutate(nodes, degree = dgr)

# add the degrees to the network object
V(g)$degree <- dgr

# arrange the terrorists in decreasing order of degree
arrange(nodes, -degree)


# compute node strengths
stg <- strength(g)

# add strength to the data frame object using mutate
nodes <- mutate(nodes, strength = stg)

# add the variable stg to the network object as strength
V(g)$strength <- stg

# arrange terrorists in decreasing order of strength and then in decreasing order of degree
arrange(nodes, -degree)
arrange(nodes, -strength)

```
  
  
  
***
  
Chapter 2 - Weakness and strength  
  
Tie betweenness:  
  
* Betweeness is the number of shortest paths that go through a specific tie (edge) - these removals would be the most disruptive  
* In a weighted network, the shortest path is defined as the lowest sum of weights, rather than the fewest edges  
	* Often need to inverse the weights prior to running, since a "high" weight usually means a close connection and thus an easy path  
    * dist_weight = 1 / E(g)$weight  
    * edge_betweenness(g, weights = dist_weight)  
	
Visualizing centrality measures:  
  
* Visualizing betweenness can be done within the igraph package  
    * ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = betweenness)) + geom_node_point()  
    * ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = weight)) + geom_node_point(aes(size = degree))  
  
The strength of weak ties:  
  
* "The strength of weak ties" is a research paper written about network strengths  
	* Argument is that the "weak ties" in a network are often the most important - relationships between diverse communities, leading to diverse ideas  
    * The "strong ties" are the relationships between people who are frequently together - can lead to group-think and stasis  
    * Noted that the Madrid group (and similar) tended to be highly dispersed and thus having many weak ties  
    * ties %>% group_by(weight) %>% summarise(n = n(), p = n / nrow(ties)) %>% arrange(-n)  
  
Example code includes:  
```{r eval=FALSE}

# save the inverse of tie weights as dist_weight
dist_weight <- 1 / E(g)$weight

# compute weighted tie betweenness
btw <- edge_betweenness(g, weights = dist_weight)

# mutate the data frame ties adding a variable betweenness using btw
ties <- mutate(ties, betweenness=btw)

# add the tie attribute betweenness to the network
E(g)$betweenness <- btw


# join ties with nodes
ties_joined <- ties %>% 
  left_join(nodes, c("from" = "id")) %>% 
  left_join(nodes, c("to" = "id")) 

# select only relevant variables and save to ties
ties_selected <- ties_joined %>% 
  select(from, to, name_from = name.x, name_to = name.y, betweenness)

# arrange named ties in decreasing order of betweenness
arrange(ties_selected, -betweenness)


# set (alpha) proportional to weight and node size proportional to degree
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha=weight)) + 
  geom_node_point(aes(size=degree))

# produce the same visualization but set node size proportional to strength
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point(aes(size = strength))


# visualize the network with tie transparency proportional to betweenness
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness)) + 
  geom_node_point()

# add node size proportional to degree
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness)) + 
  geom_node_point(aes(size = degree))


# find median betweenness
q = median(E(g)$betweenness)

# filter ties with betweenness larger than the median
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness, filter = (betweenness > q))) + 
  geom_node_point() + 
  theme(legend.position="none")


# find number and percentage of weak ties
ties %>%
  group_by(weight) %>%
  summarise(number = n(), percentage=n()/nrow(.)) %>%
  arrange(-number)


# build vector weakness containing TRUE for weak ties
weakness <- ifelse(ties$weight == 1, TRUE, FALSE)

# check that weakness contains the correct number of weak ties
sum(weakness)


# visualize the network by coloring the weak and strong ties
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(color = weakness)) + 
  geom_node_point()


# visualize the network with only weak ties using the filter aesthetic
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(filter=weakness), alpha = 0.5) + 
  geom_node_point()

```
  
  
  
***
  
Chapter 3 - Connection patterns  
  
Connection patterns:  
  
* The adjacency matrix can be calculated using as_adjacency_matrix(g)  
	* For each match of row/column, there will be a 1 for adjacency and a 0 for non-adjacency  
    * Alternately, can have the weight of the tie as the entry for each row/column (with 0 as before meaning non-adjacency)  
    * A = as_adjacency_matrix(g, attr = "weight")  
    * diag(A)  
* Can use the adjacency matrix to assess similarity of nodes in the matrix  
	* The Pearson similarity measures the correlation between the columns in the matrix  
  
Pearson correlation coefficient:  
  
* Can visualize the correlations using scatterplots  
* Can compute the correlations analytically as well  
	* cor(nodes$degree, nodes$strength)  
  
Most similar and most dissimilar terrorists:  
  
* Can use named graphs with weighted ties for a graphical representation of nodes and paths  
* Can use the adjacency matrix to reprsent the ties in a manner simplified for algebra  
* Can use the data frame format (one for nodes, and one for ties) for use with dplur and ggplot2  
	* as_data_frame(g, what = "both")  
* Can easily switch back and forth between the representations of the network  
	* as_adjacency_matrix(g)  
    * graph_from_adjacency_matrix(A)  
    * as_data_frame(g, what = "both")  
    * graph_from_data_frame(df$ties, vertices = df$nodes)  
    * as_data_frame(graph_from_adjacency_matrix(A), what = "both")  
    * as_adjacency_matrix(graph_from_data_frame(df$ties, vertices = df$nodes))  
  
Example code includes:  
```{r eval=FALSE}

# mutate ties data frame by swapping variables from and to 
ties_mutated <- mutate(ties, temp = to, to = from, from = temp) %>% select(-temp)

# append ties_mutated data frame to ties data frame
ties <- rbind(ties, ties_mutated)

# use a scatter plot to visualize node connection patterns in ties setting color aesthetic to weight
ggplot(ties, aes(x = from, y = to, color = factor(weight))) +
  geom_point() +
  labs(color = "weight")


# get the weighted adjacency matrix
A <- as_adjacency_matrix(g, attr = "weight", sparse = FALSE, names = FALSE)

# print the first row and first column of A
A[1, ]
A[, 1]

# print submatrix of the first 6 rows and columns
A[1:6, 1:6]


# obtain a vector of node strengths
rowSums(A)

# build a Boolean (0/1) matrix from the weighted matrix A
B <- ifelse(A > 0, 1, 0)

# obtain a vector of node degrees using the Boolean matrix
rowSums(B)


# compute the Pearson correlation on columns of A
S <- cor(A)

# set the diagonal of S to 0
diag(S) = 0

# print a summary of the similarities in matrix S
summary(c(S))

# plot a histogram of similarities in matrix S
hist(c(S), xlab = "Similarity", main = "Histogram of similarity")


# Scatter plot of degree and strength with regression line
ggplot(nodes, aes(x = degree, y = strength)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# Pearson correlation coefficient 
cor(nodes$degree, nodes$strength)


# build weighted similarity network and save to h
h <- graph_from_adjacency_matrix(S, mode = "undirected", weighted = TRUE)

# convert the similarity network h into a similarity data frame sim_df
sim_df <- as_data_frame(h, what = "edges")

# map the similarity data frame to a tibble and save it as sim_tib
sim_tib <- as_tibble(sim_df)

# print sim_tib
sim_tib


# left join similarity and nodes data frames and then select and rename relevant variables
sim2 <- sim_tib %>% 
  left_join(nodes, c("from" = "id")) %>% 
  left_join(nodes, c("to" = "id")) %>%
  select(from, to, name_from = name.x, name_to = name.y, similarity = weight, 
         degree_from = degree.x, degree_to = degree.y, strength_from = strength.x, strength_to = strength.y)
  
# print sim2
sim2


# arrange sim2 in decreasing order of similarity. 
sim2 %>% arrange(-similarity)

# filter sim2, allowing only pairs with a degree of least 10, arrange the result in decreasing order of similarity
sim2 %>%
  filter(degree_from >= 10, degree_to >= 10) %>%
  arrange(-similarity)

# Repeat the previous steps, but in increasing order of similarity
sim2 %>%
  filter(degree_from >= 10, degree_to >= 10) %>%
  arrange(similarity)


# filter the similarity data frame to similarities larger than or equal to 0.60
sim3 <- filter(sim2, similarity >= 0.6)

# build a similarity network called h2 from the filtered similarity data frame
h2 <- graph_from_data_frame(sim3, directed = FALSE)

# visualize the similarity network h2
ggraph(h2, layout = "with_kk") + 
  geom_edge_link(aes(alpha = similarity)) + 
  geom_node_point()

```
  
  
  
***
  
Chapter 4 - Similarity Clusters  
  
Hierarchical clustering - find clusters of similar people:  
  
* Basic idea is to define a measure of similarity, then match the most similar entities to groups, proceeding until there is a single cluster containing everyone  
* The dendrogram (tree diagram) is helpful for viewing this data  
* The similarity measure between individual nodes (person similarity) exists, and needs to be extended to groups  
	* Single-linkage - similarity is the maximum of the similarities of anyone in the groups  
    * Complete-linkage - similarity is the minimum of the similarities of anyone in the groups  
    * Average-linkage - similarity is the average of the simlarities of everyone in the groups  
* The clustering algorithm works as follows  
	* Evaluate simlarity for all node pairs  
    * Assign each node to its own group  
    * Find the pair of groups with the highest simlarity, and join them  
    * Calculate simlarity of this newly formed group to all previously existing entities (groups or individuals)  
    * Repeat until there is just a single cluster remaining  
* The R implementation is hclust()  
    * D <- 1-S  
    * d <- as.dist(D)  
    * cc <- hclust(d, method = "average")  
    * cls <- cutree(cc, k = 4)  
  
Interactive visualizations with visNetwork:  
  
* visNetwork is an interactive package for viewing networks  
	* Many different layouts are available, and you can interact with the nodes and the ties  
    * Can select nodes and see their neighborhoods (nodes within a certain distance)  
    * Can select nodes by name  
    * Can partition nodes in to groups and color, highlight, etc.  
  
Wrap up:  
  
* Analysis of networks with measures of centrality and similarity  
* Visualization of networks, including interactivity  
  
Example code includes:  
```{r eval=FALSE}

# compute a distance matrix
D <- 1 - S

# obtain a distance object 
d <- as.dist(D)

# run average-linkage clustering method and plot the dendrogram 
cc <- hclust(d, method = "average")
plot(cc)

# find the similarity of the first pair of nodes that have been merged 
S[40, 45]


# cut the dendrogram at 4 clusters
cls <- cutree(cc, k = 4)

# add cluster information to the nodes data frame
nodes <- mutate(nodes, cluster = cls)

# print the nodes data frame
nodes


# output the names of terrorists in the first cluster
filter(nodes, cluster == 1) %>% 
    select(name)

# for each cluster select the size of the cluster, the average node degree, and the average node strength and sorts by cluster size
group_by(nodes, cluster) %>%
  summarise(size = n(), 
            avg_degree = mean(degree),
            avg_strength = mean(strength)
            ) %>%
  arrange(-size)


# add cluster information to the network 
V(g)$cluster <- nodes$cluster

# visualize the original network with colored clusters
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight), show.legend=FALSE) + 
  geom_node_point(aes(color = factor(cluster))) +
  labs(color = "cluster")

# facet the network with respect to cluster attribute
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight), show.legend=FALSE) + 
  geom_node_point(aes(color = factor(cluster))) +
  facet_nodes(~cluster, scales="free")  +
  labs(color = "cluster")


# convert igraph to visNetwork
data <- visNetwork::toVisNetworkData(g)

# print head of nodes and ties
head(data$nodes)
head(data$edges)

# visualize the network
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300)


# use the circle layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk")

# use the circle layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_in_circle")

# use the grid layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_on_grid")


# highlight nearest nodes and ties of the selected node
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(highlightNearest = TRUE) 


# select nodes by id 
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(nodesIdSelection = TRUE)

# set color to cluster and generate network data
V(g)$color = V(g)$cluster
data <- visNetwork::toVisNetworkData(g)

# select by group (cluster)
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(selectedBy = "group")

```
  
  
  
***
  
### _Data Privacy and Anaonymization in R_  
  
Chapter 1 - Introduction to Data Privacy  
  
Intro to Anonymization - Part I:  
  
* Need to implement better data privacy techniques - e.g., census data, healthcare data, etc.  
* Need to have data such as individualized health, but not in a manner that identifies specific individuals  
* Topics covered in this course will include  
	* Remove identifiers, synthesize data  
    * Laplace mechnaism for removing names  
    * Differential privacy and post-processing  
    * Release of data using the above techniques  
* Data sets will include White House salaries and male infertility data  
	* One basic technique is removing identifiers, such as replacing names with numbers  
    * Another basic technique is to round continuous values (such as to the nearest 1000)  
  
Intro to Anonymization - Part II:  
  
* Additional approaches include generalization and top/bottom coding  
	* Generalization creates larger buckets of data  
    * Top/bottom is about setting outliers back to a pre-defined top and bottom of the range  
* Additional dplyr functions of interest  
	* count() is used to find the number of observations for each distinct group  
    * whitehouse %>% count(Status)  
    * whitehouse %>% count(Status, Title, sort = TRUE)  # sort=TRUE sorts by descending n  
    * summarize_at() lets you get summary statistics for a key variable  
    * whitehouse %>% summarise_at(vars(Salary), sum)  # vars() holds the bare variables, while sum is the requested function  
    * whitehouse %>% summarise_at(vars(Salary), funs(mean, sd))  # funs() holds the list of functions that you want to apply  
  
Data Synthesis:  
  
* Fake datasets created based on sampling from a probability distribution  
* Goal is a fake dataset (by definition anaonymized) that is statistically similar to the real dataset  
	* For 1/0 data, sampling from the binomial distribution can work well  
    * For bell-shaped data, the normal or log-normal can often work well (though there can be issues with bounding)  
    * Hard-bounding is setting values to a proper max/min, while another approach is to discard the record and sample again  
  
Example code includes:  
```{r}

load("./RInputFiles/dataPriv.RData")


# Preview data
whitehouse

# Set seed
set.seed(42)

# Replace names with random numbers from 1 to 1000
whitehouse_no_names <- whitehouse %>%
    mutate(Name = sample(1:1000, nrow(.), replace=FALSE))

whitehouse_no_names


# Rounding Salary to the nearest ten thousand
whitehouse_no_identifiers <- whitehouse_no_names %>%
    mutate(Salary = round(Salary, -4))

whitehouse_no_identifiers


# Convert the salaries into three categories
whitehouse.gen <- whitehouse %>%
    mutate(Salary = ifelse(Salary < 50000, 0, 
                           ifelse(Salary >= 50000 & Salary < 100000, 1, 2)))

whitehouse.gen


# Bottom Coding
whitehouse.bottom <- whitehouse %>%
    mutate(Salary = pmax(Salary, 45000))

# Filter Results
whitehouse.bottom %>%
    filter(Salary <= 45000)


# View fertility data
fertility

# Number of participants with Surgical_Intervention and Diagnosis
fertility %>%
    summarise_at(vars(Surgical_Intervention, Diagnosis), sum)

# Mean and Standard Deviation of Age
fertility %>%
    summarise_at(vars(Age), funs(mean, sd))

# Counts of the Groups in High_Fevers
fertility %>%
    count(High_Fevers)

# Counts of the Groups in Child_Disease
fertility %>%
    count(Child_Disease, Accident_Trauma)

# Find proportions
fertility %>%
    summarise_at(vars(Accident_Trauma, Surgical_Intervention), mean)


# Set seed
set.seed(42)

# Generate Synthetic data
accident <- rbinom(100, 1, prob=0.440)
surgical <- rbinom(100, 1, prob=0.510)


# Square root Transformation of Salary
whitehouse.salary <- whitehouse %>%
    mutate(Salary = sqrt(Salary))

# Calculate the mean and standard deviation
stats <- whitehouse.salary %>%
    summarize(mean(Salary), sd(Salary))

stats


# Generate Synthetic data
set.seed(42)
salary_transformed <- rnorm(nrow(whitehouse), mean=279, sd=71.8)

# Power transformation
salary_original <- salary_transformed ** 2

# Hard bound
salary <- ifelse(salary_original < 0, 0, salary_original)

```
  
  
  
***
  
Chapter 2 - Introduction to Differential Privacy  
  
Differential Privacy - quantification of privacy loss via a privacy budget:  
  
* The worst-case scenario is that no assumptions are made about data intruders  
	* If an individual is from a small group, their data may be 100% available by looking at statistics in aggregate and statistics for the group that excludes them (everyone else)  
* The privacy budget is defined using epsilon - smaller numbers mean that less information will be made available  
* The general concept is to look at a dataset that includes the segment the individual is in, and a dataset that includes all other segments  
	* The answer sent back to the query will have noise added to it depending on the privacy budget  
* Basically, the differential privacy algorithm finds the most "unique" person in the dataset, and then decides how much noise to add based on how identifiable they are by attribute  
  
Global Sensitivity - usual decision-making factor for differential privacy:  
  
* The global sensitivity of a query is the most a variable could change based on removing one individual  
	* By definition, count queries always have a global sensitivity of 1 (exclude 1 individual)  
    * Therefore, proportion queries always have a global sensitity of 1/n  
    * Mean queries always have a global sensitivity of (max - min) / n  
    * Variance queries always have a global sensitivity of (max - min)^2 / n  
* The global sensitivity and the epsilon work together to determine the amount of noise  
	* Measures like median are not very sensitive to outliers, and thus very little noise needs to be added  
    * Measures like maximum are very sensitive to outliers (e.g., Bill Gates income), and thus very little noise needs to be added  
  
Laplace Mechanism - adds noise based on the Laplace distribution with mean 0 and parameters global sensitivity and privacy budget:  
  
* fertility %>% summarise_at(vars(Child_Disease), sum)  
* library(smoothmest)  # has function rdoublex(draws, mean, shaping) - set draws=1, mean=true_mean, shaping=globalSensitivity / epsilon  
  
Example code includes:  
```{r}

# Number of observations
n <- nrow(fertility)

# Global sensitivity of counts
gs.count <- 1

# Global sensitivity of proportions
gs.prop <- 1/n


# Lower bound of Hours_Sitting
a <- 0

# Upper bound of Hours_Sitting
b <- 1

# Global sensitivity of mean for Hours_Sitting
gs.mean <- (b - a) / n

# Global sensitivity of proportions Hours_Sitting
gs.var <- (b - a)**2 / n


# How many participants had a Surgical_Intervention?
fertility %>%
   summarise_at(vars(Surgical_Intervention), sum)

# Set the seed
set.seed(42)

# Apply the Laplace mechanism
eps <- 0.1
smoothmest::rdoublex(1, 51, 1/eps)


# Proportion of Accident_Trauma
stats <- fertility %>%
   summarise_at(vars(Accident_Trauma), mean)

stats

# Set the seed
set.seed(42)

# Apply the Laplace mechanism
eps <- 0.1
smoothmest::rdoublex(1, 0.440, (1/n)/eps)


# Mean and Variance of Hours Sitting
fertility %>%
    summarise_at(vars(Hours_Sitting), funs(mean, var))

# Setup
set.seed(42)
eps <- 0.1

# Laplace mechanism to mean
smoothmest::rdoublex(1, 0.41, gs.mean/eps)

# Laplace mechanism to variance
smoothmest::rdoublex(1, 0.03, gs.var/eps)

```
  
  
  
***
  
Chapter 3 - Differentially Private Properties  
  
Sequential Composition - method to require that someone cannot find the real answer by just sending multiple queries:  
  
* Idea is that the privacy budget is divided by the number of queries you plan to send  
* For example, if a query will be made for mean and another query will be made for maximum, then epsilon needs to be divided by two  
  
Parallel Composition - method to account for queries to different parts of the database (no adjustment to epsilon needed):  
  
* Deciding between sequential and parallel is whether queries could be answered using completely different (MECE) splits of the dataset  
  
Post-processing:  
  
* When new queries can be answered using data that has already been privatized, it can be synthesized to a noisy answer to this new query  
	* The privacy budget need not be adjusted in this case  
    * For example, if there are three groups, can just add noise to two of the groups and let the third group be total minus these two groups  
  
Impossible and inconsistent answers:  
  
* Bounding can be introduced, such as making all negative numbers zero or anything greater than the total to the total  
	* rdoublex(1, 12, gs.count / eps) %>% round() %>% max(0)  # lower bound is zero  
    * normalized <- (smoking/sum(smoking)) * (nrow(fertility))  # upper bound is the size of the dataset  
  
Example code includes:  
```{r}

# Set Value of Epsilon
eps <- 0.1 / 2

# Number of observations
n <- nrow(fertility)

# Lower bound of Age
a <- 0

# Upper bound of Age
b <- 1

# GS of counts for Diagnosis
gs.count <- 1

# GS of mean for Age
gs.mean <- (b-a)/n


# Number of Participants with abnormal diagnosis
stats1 <- fertility %>% 
    summarize_at(vars(Diagnosis), sum)

stats1

# Mean of age
stats2 <- fertility %>%
    summarize_at(vars(Age), mean)

stats2


# Set seed
set.seed(42)

# Laplace mechanism to the count of abnormal diagnosis
smoothmest::rdoublex(1, 12, gs.count/eps)

# Laplace mechanism to the mean of age
smoothmest::rdoublex(1, 0.67, gs.mean/eps)


# Set Value of Epsilon
eps <- 0.1

# Mean of Age per diagnosis level 
fertility %>%
  group_by(Diagnosis) %>%
  summarise_at(vars(Age), mean)


# Set the seed
set.seed(42)

# Laplace mechanism to the mean age of participants with an abnormal diagnoisis
smoothmest::rdoublex(1, 0.71, gs.mean/eps)

# Laplace mechanism to the mean age of participants with a normal diagnoisis
smoothmest::rdoublex(1, 0.66, gs.mean/eps)


# Set Value of Epsilon
eps <- 0.5/3

# GS of Counts
gs.count <- 1

# Number of participants in each of the four seasons
fertility %>%
    group_by(Diagnosis) %>%
    summarise_at(vars(Age), mean)

# Set the seed
set.seed(42)

# Laplace mechanism to the number of participants who were evaluated in the winter, spring, and summer
winter <- smoothmest::rdoublex(1, 28, gs.count / eps) %>%
    round()

spring <- smoothmest::rdoublex(1, 37, gs.count / eps) %>%
    round()

summer <- smoothmest::rdoublex(1, 4, gs.count / eps) %>%
    round()

# Post-process based on previous queries
fall <- nrow(fertility) - winter - spring - summer


# Set Value of Epsilon
eps <- 0.01

# GS of counts
gs.count <- 1

# Number of Participants with Child_Disease
fertility %>%
    summarise_at(vars(Child_Disease), sum)

# Apply the Laplace mechanism
set.seed(42)
lap_childhood <- smoothmest::rdoublex(1, 87, gs.count / eps) %>%
    round()

# Total number of observations in fertility
max_value <- nrow(fertility)

# Bound the value such that the noisy answer does not exceed the total number of observations
ifelse(lap_childhood > max_value, max_value, lap_childhood)


# Set the seed
set.seed(42)

# Apply the Laplace mechanism
fever1 <- smoothmest::rdoublex(1, 9, gs.count/eps) %>%
    max(0)
fever2 <- smoothmest::rdoublex(1, 63, gs.count/eps) %>%
    max(0)
fever3 <- smoothmest::rdoublex(1, 28, gs.count/eps) %>%
    max(0)

fever <- c(fever1, fever2, fever3)

# Normalize noise 
fever_normalized <- (fever/sum(fever)) * (nrow(fertility))

# Round the values
round(fever_normalized)

```
  
  
  
***
  
Chapter 4 - Differentially Private Data Synthesis  
  
Laplace Sanitizer - basic way to generate "noisy" categorical data:  
  
* Takes advantage of parallel - if the data can be binned or placed in a contingency table, assumes no more need to divide the privacy budget  
	* Since the data is queries as a histogram, it can be considered disjoint (non-overlapping) and thus parallel composition  
* Can generate data using rep() for a single vector  
  
Parametric Approaches:  
  
* Sampling from a binomial distribution (where appropriate), with a known proportion that has been modified by Laplace differential privacy guarantee  
* Sampling from a normal or log-normal distribution (where appropriate), with a known mean and variance that has been modified by Laplace differential privacy guarantee  
  
Wrap up:  
  
* Basics of anonymyzing data, such as removing names  
* Basics of modifying data such as generalizing to categorical data  
* Basics of generating synthetic data using rbinom() and rnorm()  
* Basics of privacy budgets, global sensitivities, and the Laplace mechanism  
* Basics of differential privacy, such as sequential (split epsilon) or parallel (including through binning or continegnecy tables)  
* Basics of the Laplace sanitizer for both categorical data (rbinom) and continuous data (rnorm)  
* Next steps include managing data gaps, incorrect statistics distributions with hard bounding, etc.  
	* Local differential privacy (Apple) and probabilistic differential privacy (US census)  
    * Techniques specific to GPS data or PCA  
  
Example code includes:  
```{r}

# Set Value of Epsilon
eps <- 0.1

# GS of Counts
gs.count <- 1

# Number of participants in each season
fertility %>%
    count(Season)


# Set the seed
set.seed(42)

# Apply the Laplace mechanism 
winter <- smoothmest::rdoublex(1, 28, gs.count/eps) %>% max(0)
spring <- smoothmest::rdoublex(1, 37, gs.count/eps) %>% max(0)
summer <- smoothmest::rdoublex(1, 4, gs.count/eps) %>% max(0)
fall <- smoothmest::rdoublex(1, 31, gs.count/eps) %>% max(0)


# Store noisy results
seasons <- c(winter = winter, spring = spring, summer = summer, fall = fall)

# Normalizing seasons
seasons_normalized <- (seasons/sum(seasons)) * nrow(fertility)

# Round the values
round(seasons_normalized)

# Generate synthetic data for winter
rep(-1, 29)

# Generate synthetic data for spring
rep(-0.33, 38)

# Generate synthetic data for summer
rep(0.33, 0)

# Generate synthetic data for fall
rep(1, 33)


# Calculate proportions
fertility %>%
    summarise_at(vars(Accident_Trauma, Surgical_Intervention), mean)

# Number of Observations
n <- nrow(fertility)

# Set Value of Epsilon
eps <- 0.1

# GS of Proportion
gs.prop <- (1/n)


# Apply the Laplace mechanism
set.seed(42)
smoothmest::rdoublex(1, 0.44, gs.prop/eps)
smoothmest::rdoublex(1, 0.51, gs.prop/eps)

# Generate Synthetic data
set.seed(42)
accident <- rbinom(n, 1, 0.46)
surgical <- rbinom(n, 1, 0.54)


# Set Value of Epsilon
eps <- 0.1 / 2

# Number of observations
n <- nrow(fertility)

# Upper and lower bounds of age
a <- 0
b <- 1

# GS of mean and variance for age
gs.mean <- (b-a) / n
gs.var <- (b-a)**2 / n


# Mean and Variance of Age
fertility %>%
    summarise_at(vars(Age), funs(mean, var))

# Apply the Laplace mechanism
set.seed(42)
smoothmest::rdoublex(1, 0.67, gs.mean/eps)
smoothmest::rdoublex(1, 0.01, gs.var/eps)


# Generate Synthetic data
set.seed(42)
age <- rnorm(n, mean=0.71, sd=sqrt(0.07))

# Hard Bounding the data
age[age < 0] <- 0
age[age > 1] <- 1

```
  
  
  
*** 
  
### _Marketing Analytics in R: Statistical Modeling_  
  
Chapter 1 - Modeling Customer Lifetime Value with Linear Regression  
  
Introduction - Verena from INWT Statistics (consultancy in marketing analytics):  
  
* Customer Lifetime Value (CLV) is the expected value of forecasted customer value to the company  
	* CLV is based on margin, and needs to use current information to predict future margins  
    * Customers predicted to have higher CLV can then be targeted  
* Can inspect the data without seeing attributes using str(clvData1, give.attr = FALSE)  
* Can derive correlations using corrplot  
	* library(corrplot)  
    * clvData1 %>% select(nOrders, nItems, ... ,margin, futureMargin) %>% cor() %>% corrplot()  
  
Simple linear regression - one predictor variable to predict one response variable:  
  
* Can run linear regressions using basic stats modules  
	* simpleLM <- lm(futureMargin ~ margin, data = clvData1)  
    * summary(simpleLM)  
* Can plot previous margin vs. current margin, including a linear regression (smooth)  
	* ggplot(clvData1, aes(margin, futureMargin)) + geom_point() + geom_smooth(method = lm, se = FALSE) + xlab("Margin year 1") + ylab("Margin year 2")  
* Several conditions must apply for linear regression to be the best method  
	* Linear relationship between x and y  
    * No measurement error in x (weak exogeneity)  
    * Independence of errors  
    * Expectation of errors is 0  
    * Constant variance of prediction errors (homoscedasticity)  
    * Normality of errors  
  
Multiple linear regression:  
  
* Omitted variable bias is when a variable not in the regression is correlated with both the predictor and the response variables  
	* Simpson's Paradox is an example - upward sloping becomes downward sloping after properly splitting on the extra variable  
* Multicollinearity is a threat to a linear regression - leads to unstable regression coefficients, with associated under-reporting of standard errors  
	* rms::vif(myLMModel)  # above 5 is concerning, above 10 almost always needs to be addressed  
  
Model validation, fit, and prediction:  
  
* The R-squared is the proportion of variance in the depedent variable that is explained by the regression  
* Can look at the p-value of the F-test to assess the overall statistical significance of the model  
* There is a risk of over-fitting, when the model is overly complex and learns artifacts of the training data rather than genuine patterns  
	* Can use stats::AIC() or MASS::stepAIC(), with the goal being to minimize AIC (needs to be models of the same data)  
    * AIC(multipleLM2)  
* Can predict outputs automatically, such as with  
	* predMargin <- predict(multipleLM2, newdata = clvData2)  
  
Example code includes:  
```{r}

salesData <- readr::read_csv("./RInputFiles/salesData.csv")

# Structure of dataset
str(salesData, give.attr = FALSE)

# Visualization of correlations
salesData %>% select_if(is.numeric) %>%
  select(-id) %>%
  cor() %>%
  corrplot::corrplot()

# Frequent stores
ggplot(salesData) +
    geom_boxplot(aes(x = mostFreqStore, y = salesThisMon))

# Preferred brand
ggplot(salesData) +
    geom_boxplot(aes(x = preferredBrand, y = salesThisMon))


# Model specification using lm
salesSimpleModel <- lm(salesThisMon ~ salesLast3Mon, data = salesData)

# Looking at model summary
summary(salesSimpleModel)


# Estimating the full model
salesModel1 <- lm(salesThisMon ~ . -id, data = salesData)

# Checking variance inflation factors
car::vif(salesModel1)

# Estimating new model by removing information on brand
salesModel2 <- lm(salesThisMon ~ . -id -preferredBrand -nBrands, data = salesData)

# Checking variance inflation factors
car::vif(salesModel2)


salesData2_4 <- readr::read_csv("./RInputFiles/salesDataMon2To4.csv")

# getting an overview of new data
summary(salesData2_4)

# predicting sales
predSales5 <- predict(salesModel2, newdata = salesData2_4)

# calculating mean of future sales
mean(predSales5)

```
  
  
  
***
  
Chapter 2 - Logistic Regression for Churn Prevention  
  
Churn prevention in online marketing:  
  
* Objective is to predict the likelihood of a customer repeating their business, assessed using logistic regression  
	* Model the log-odds (defined as log (P(Y=1) / P(Y=0))) as a linear function of the inputs  
    * Convert the log-odds to odds (defined as P(Y=1) / P(Y=0)) by exponentiation  
    * Convert the odds to a probability of churning, using odds / (1 + odds)  
* Can begin with basic data exploration  
	* ggplot(churnData, aes(x = returnCustomer)) + geom_histogram(stat = "count")  
  
Modeling and model selection:  
  
* The logit model can be run using the GLM provided in R  
	* logitModelFull <- glm(returnCustomer ~ title + newsletter + websiteDesign + ..., family = binomial, churnData)  
* Interpreting the coefficients is not easy - they are related to the log-odds  
	* Can exponentiate the coefficients to get their impact on the odds  
    * Can then interpret that greater than 1 means "more likely, all else equal"  
* Can use MASS::stepAIC() to help refine the modeling  
	* library(MASS)  
    * logitModelNew <- stepAIC(logitModelFull, trace = 0)  
    * summary(logitModelNew)  
    * Produces a model with fewer variables and a lower AIC  
  
In-sample model fit and thresholding:  
  
* There are three types of pseudo-R-squared statistics available for the results of logistical regression  
	* McFadden: R-squared = 1 - L(null) / L(full)  
    * Cox-Snell: R-squared = 1 - (L(null) / L(full)) ** (2/n)  
    * Nagelkerke: R-squared = [1 - (L(null) / L(full)) ** (2/n)] / [1 - L(null) ** (2/n)]  
    * Generally, anything above 0.2 is reasonably good  
    * descr::LogRegR2(logitModelNew)  
    * library(SDMTools)  
    * churnData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)  # get the prediction probabilities  
    * data %>% select(returnCustomer, predNew) %>% tail()  
    * confMatrixNew <- confusion.matrix(churnData$returnCustomer, churnData$predNew, threshold = 0.5)  # this is the version from SDMTools  
* Can give different weights to the different errors (false negatives, false positives, etc.)  
	* Can instead look at a payoff, defined based on scalars for the various quadrants  
  
Out-of-sample validation and cross validation:  
  
* Begin by randomly splitting data in to training (roughly two-thirds) and holding back the remainder for validation (roughly one-third)  
	* set.seed(534381)  
    * churnData$isTrain <- rbinom(nrow(churnData), 1, 0.66)  
    * train <- subset(churnData, churnData$isTrain == 1)  
    * test <- subset(churnData, churnData$isTrain == 0)  
    * test$predNew <- predict(logitTrainNew, type = "response", newdata = test)  # make predictions only on the test dataset  
* Cross-validation is an even more powerful tool for assessing out-of-sample error  
	* Split the data in to k subsets, and run the model k times with k-1 training data and the last subset used as the validation data  
    * Acc03 <- function(r, pi = 0) {  
    *   cm <- confusion.matrix(r, pi, threshold = 0.3)  
    *   acc <- sum(diag(cm)) / sum(cm) return(acc)  
    * }  
    * set.seed(534381)  
    * boot::cv.glm(churnData, logitModelNew, cost = Acc03, K = 6)$delta  
* Can continually tweak the model to see if transforms, variable additions, etc., might tend to improve the out-of-sample error rate  
  
Example code includes:  
```{r}

defaultData <- readr::read_delim("./RInputFiles/defaultData.csv", delim=";")

# Summary of data
summary(defaultData)

# Look at data structure
str(defaultData, give.attr=FALSE)

# Analyze the balancedness of dependent variable
ggplot(defaultData, aes(x = PaymentDefault)) +
  geom_histogram(stat = "count") 


# Build logistic regression model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
                   age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + 
                   billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + 
                   payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, 
                family = "binomial", data = defaultData)

# Take a look at the model
summary(logitModelFull)

# Take a look at the odds
coefsexp <- coef(logitModelFull) %>% exp() %>% round(2)
coefsexp


# The old (full) model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
                   age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + 
                   billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + 
                   payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, 
                 family = binomial, defaultData)

#Build the new model
logitModelNew <- MASS::stepAIC(logitModelFull, trace=0) 

#Look at the model
summary(logitModelNew) 

# Save the formula of the new model (it will be needed for the out-of-sample part) 
formulaLogit <- as.formula(summary(logitModelNew)$call)
formulaLogit


# Make predictions using the full Model
defaultData$predFull <- predict(logitModelFull, type = "response", na.action = na.exclude)

# Construct the in-sample confusion matrix
confMatrixModelFull <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                                  defaultData$predFull, 
                                                  threshold = 0.5
                                                  )
confMatrixModelFull

# Calculate the accuracy for the full Model
accuracyFull <- sum(diag(confMatrixModelFull)) / sum(confMatrixModelFull)
accuracyFull


# Calculate the accuracy for 'logitModelNew'
# Make prediction
defaultData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)

# Construct the in-sample confusion matrix
confMatrixModelNew <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                                 defaultData$predNew, 
                                                 threshold = 0.5
                                                 )
confMatrixModelNew

# Calculate the accuracy...
accuracyNew <- sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew)
accuracyNew

# and compare it to the full model's accuracy
accuracyFull
accuracyNew


# Prepare data frame with threshold values and empty payoff column
payoffMatrix <- data.frame(threshold = seq(from = 0.1, to = 0.5, by = 0.1), payoff = NA) 
payoffMatrix
 
for(i in 1:length(payoffMatrix$threshold)) {
  # Calculate confusion matrix with varying threshold
  confMatrix <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                           defaultData$predNew, 
                                           threshold = payoffMatrix$threshold[i]
                                           )
  # Calculate payoff and save it to the corresponding row
  payoffMatrix$payoff[i] <- confMatrix[1, 1]*250 + confMatrix[1, 2]*(-1000)
}
payoffMatrix


# Split data in train and test set
set.seed(534381) 
defaultData$isTrain <- rbinom(nrow(defaultData), 1, 0.66)
train <- subset(defaultData, isTrain == 1)
test <- subset(defaultData, isTrain  == 0)

logitTrainNew <- glm(formulaLogit, family = binomial, data = train) # Modeling
test$predNew <- predict(logitTrainNew, type = "response", newdata = test) # Predictions

# Out-of-sample confusion matrix and accuracy
confMatrixModelNew <- SDMTools::confusion.matrix(test$PaymentDefault, test$predNew, threshold = 0.3) 
sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew) # Compare this value to the in-sample accuracy


# Accuracy function
costAcc <- function(r, pi = 0) {
  cm <- SDMTools::confusion.matrix(r, pi, threshold = 0.3)
  acc <- sum(diag(cm)) / sum(cm)
  return(acc)
}

# Cross validated accuracy for logitModelNew
set.seed(534381)
boot::cv.glm(defaultData, logitModelNew, cost = costAcc, K = 6)$delta[1]

```
  
  
  
***
  
Chapter 3 - Modeling Time to Reorder with Survival Analysis  
  
Survival Analysis Introduction:  
  
* Often have "censored" data, meaning that the customer journeys are not yet complete  
	* Random Type I Right censoring is the most common - a point can only be observed if it has occurred before time X, and it is otherwise unknowable (but known that they have not yet churned)  
    * Can plot histograms of whether someone has churned depending on the length of time  
    * plotTenure <- dataSurv %>% mutate(churn = churn %>% factor(labels = c("No", "Yes"))) %>%  
    *   ggplot() + geom_histogram(aes(x = tenure, fill = factor(churn))) + facet_grid( ~ churn) +           
    *   theme(legend.position = "none")  
* Survival analysis attempts to estimate when something will happen (churn, second order, renewal, etc.)  
  
Survival curve analysis by Kaplan-Meier:  
  
* Begin by creating a new object containing the survival attribute  
	* cbind(dataSurv %>% select(tenure, churn), surv = Surv(dataSurv$tenure, dataSurv$churn)) %>% head(10)  
* The survival function is the probability of "no event" in cumulative by time t  
	* The hazard function is the cumulative probability of "event" by time t  
    * The "hazard rate" is the probability of the event happening in a small time, provided that it has not yet happened  
* The Kaplan-Meier analysis can be used to estimate survival  
	* fitKM <- survival::survfit(Surv(dataSurv$tenure, dataSurv$churn) ~ 1, type = "kaplan-meier")  
    * print(fitKM)  # gives a few rough summary statistics  
    * plot(fitKM) # survival curve with confidence interval  
    * fitKMstr <- survfit(Surv(tenure, churn) ~ Partner, data = dataSurv)  # add covariates, such as ~ Partner rather than ~1 as in the baseline  
  
Cox PH model with constant covariates:  
  
* Model definition: cannot parse to ISO - see Excel notes  
	* Predictors are lineary and multiplicatively related to the hazard function, lambda  
    * Relative hazard function needs to remain constant over time  
* Fitting a survival model in R  
	* library(rms)  
    * units(dataSurv$tenure) <- "Month"  
    * dd <- datadist(dataSurv)  
    * options(datadist = "dd")  
    * fitCPH1 <- cph(Surv(tenure, churn) ~ gender + SeniorCitizen + Partner + Dependents + StreamMov + PaperlessBilling + PayMeth + MonthlyCharges, data = dataSurv, x = TRUE, y = TRUE, surv = TRUE, time.inc = 1)  
    * Coefficient interpretation is relatively similar to logistic regression - exp(fitCPH1$coefficients) - can simplify the coefficients be making them multiplicative (1.00 is no impact)  
    * survplot(fitCPH1, MonthlyCharges, label.curves = list(keys = 1:5))  # plots the survival probabilities based on varying 1 variable, assuming other variables constant  
    * survplot(fitCPH1, Partner)  # covariate with partner, plotted  
    * plot(summary(fitCPH1), log = TRUE)  # visualizing the hazard ratios  
  
Checking model assumptions and making predictions:  
  
* Can again use the Cox PH function  
	* testCPH1 <- cox.zph(fitCPH1)  
    * print(testCPH1)  # if p < 0.05, can reject the assumption that the predictor meets the proportional hazard assumption  
    * plot(testCPH1, var = "Partner=Yes")  
    * plot(testCPH1, var = "MonthlyCharges")  
    * This test is conservative and sensitive to the number of observations  
* If the PH (proportional hazard) assumptions are violated, can correct for this using  
	* fitCPH2 <- cph(Surv(tenure, churn) ~ MonthlyCharges + SeniorCitizen + Partner + Dependents + StreamMov + Contract, stratum = "gender = Male", data = dataSurv, x = TRUE, y = TRUE, surv = TRUE)  
    * rms::validate(fitCPH1, method = "crossvalidation", B = 10, pr = FALSE)  # pr=FALSE means only print at the end; R2 is the R-squared corrected by cross-validation  
* Can then assess probabilities for the event to occur  
	* oneNewData <- data.frame(gender = "Female", SeniorCitizen = "Yes", Partner = "No", Dependents = "Yes", StreamMov = "Yes", PaperlessBilling = "Yes", PayMeth = "BankTrans(auto)", MonthlyCharges = 37.12)  
    * str(survest(fitCPH1, newdata = oneNewData, times = 3))  
    * plot(survfit(fitCPH1, newdata = oneNewData))  
    * print(survfit(fitCPH1, newdata = oneNewData))  
  
Example code includes:  
```{r}

survData <- readr::read_delim("./RInputFiles/survivalDataExercise.csv", delim=",")


dataNextOrder <- survData %>%
    select(daysSinceFirstPurch, boughtAgain)

# Look at the head of the data
head(dataNextOrder)

# Plot a histogram
ggplot(dataNextOrder) +
  geom_histogram(aes(x = daysSinceFirstPurch, fill = factor(boughtAgain))) +
  facet_grid( ~ boughtAgain) + # Separate plots for boughtAgain = 1 vs. 0
  theme(legend.position = "none") # Don't show legend


# Create survival object
survObj <- survival::Surv(dataNextOrder$daysSinceFirstPurch, dataNextOrder$boughtAgain)

# Look at structure
str(survObj)


# Compute and print fit
fitKMSimple <- survival::survfit(survObj ~ 1)
print(fitKMSimple)

# Plot fit
plot(fitKMSimple, conf.int = FALSE, xlab = "Time since first purchase", 
     ylab = "Survival function", main = "Survival function"
     )


dataNextOrder <- survData %>%
    select(daysSinceFirstPurch, boughtAgain, voucher)

# Compute fit with categorical covariate
fitKMCov <- survival::survfit(survObj ~ voucher, data = dataNextOrder)

# Plot fit with covariate and add labels
plot(fitKMCov, lty = 2:3, xlab = "Time since first purchase", 
     ylab = "Survival function", main = "Survival function"
     )
legend(90, .9, c("No", "Yes"), lty = 2:3)


dataNextOrder <- survData

# Determine distributions of predictor variables
dd <- rms::datadist(dataNextOrder)
options(datadist = "dd")

# Compute Cox PH Model and print results
fitCPH <- rms::cph(survival::Surv(daysSinceFirstPurch, boughtAgain) ~ 
                       shoppingCartValue + voucher + returned + gender, data = dataNextOrder, 
                   x = TRUE, y = TRUE, surv = TRUE
                   )
print(fitCPH)

# Interpret coefficients
exp(fitCPH$coefficients)

# Plot result summary
plot(summary(fitCPH), log = TRUE)


# Check proportional hazard assumption and print result
testCPH <- survival::cox.zph(fitCPH)
print(testCPH)

# Plot time-dependent beta
plot(testCPH, var = "gender")

# Validate model
rms::validate(fitCPH, method = "crossvalidation", B = 10, dxy = TRUE, pr = FALSE)


# Create data with new customer
newCustomer <- data.frame(daysSinceFirstPurch = 21, shoppingCartValue = 99.9, gender = "female", 
                          voucher = 1, returned = 0, stringsAsFactors = FALSE
                          )

# Make predictions
pred <- survival::survfit(fitCPH, newdata = newCustomer)
print(pred)
plot(pred)

# Correct the customer's gender
newCustomer2 <- newCustomer
newCustomer2$gender <- "male"

# Redo prediction
pred2 <- survival::survfit(fitCPH, newdata = newCustomer2)
print(pred2)

```
  
  
  
***
  
Chapter 4 - Reducing Dimensionality with Principal Component Analysis  
  
PCA for CRM Data - address mutlicollinearity and data volume issues in the raw CRM data:  
  
* PCA reduces a large number of correlated variables to a smaller number of uncorrelated (orthogonal) variables  
* PCA can also help with creating an index, such as using the first component of the PCA  
* All variables must be either continuous or binary prior to running the PCA analysis  
	* dataCustomers %>% cor() %>% corrplot()  # plot the initial correlations  
  
PCA Computation:  
  
* Need to manage for variance, otherwise high-variance variables will be over-represented in the PCA  
	* lapply(dataCustomers, var)  
    * dataCustomers <- dataCustomers %>% scale() %>% as.data.frame()  
    * pcaCust <- prcomp(dataCustomers)  
    * pcaCust$sdev %>% round(2)  # standard deviations by component  
    * pcaCust$sdev ^ 2 %>% round(2)  # variances, also known as eigenvalues, by component give a good sense for relative importance (relative ratio is percent of variance explained)  
    * round(pcaCust$rotation[, 1:6], 2)  # correlations between original variables and principal components (can use these to give descriptive names to components)  
* Values of the observations are the weightings for the PC to make up the underlying data  
	* sum(dataCustomers[1,] * pcaCust$rotation[,1])  # Value on 1st component for 1st customer  
    * pcaCust$x[1:5, 1:6]  # first 5 customers and first 6 component loadings (weightings)  
  
PCA Model Specification:  
  
* Need to decide on how many components to keep - balance size of data vs. reconstruction of original data  
	* Can set a minimum requirement for percentage of variance explained (such as 70%)  
    * summary(pcaCust)  # will show cumulatives also  
    * Can use the Kaiser-Guttman criteria, which keeps only components with an eigenvalue of 1 (since 1 is the average)  
    * Can also draw a scree plot to see the variances (eigenvalues) in descending order - look for an elbow  
    * screeplot(pcaCust, type = "lines")  
    * Generally, use a few different techniques, and pick a number that is "in the range"  
* The biplot can help to show how the data map on to the principal components  
	* biplot(pcaCust, choices = 1:2, cex = 0.7)  # will show PC1 and PC2, with arrows for the various features and how they map on them  
  
Principal components in a regression analysis:  
  
* PCA can help to solve the multi-collinearity problem in a regression  
	* dataCustComponents <- cbind(dataCustomers[, "customerSatis"], pcaCust$x[, 1:6]) %>% as.data.frame  
    * mod2 <- lm(customerSatis ~ ., dataCustComponents)  
    * vif(mod2)  # by construction, these will all be 1, since the principal components are orthogonal  
* Factor analysis is another dimension-reduction technique, sometimes confused with PCA  
	* Factor analysis theorizes that latent constructs (e.g., intelligence) which cannot be directly measured are influencing the observed variables  
    * Factor analysis is often used in questionnaires - factor analysis can investigate where multiple questions really just measure one thing  
    * In contrast, with PCA, the features are actually being combined to model the data  
  
Wrap up:  
  
* Logistic regression for churn  
* Survival analysis to prevent churn  
* Principal component analysis (PCA) to reduce multicollinearity  
  
Example code includes:  
```{r cache=TRUE}

load("./RInputFiles/newsData.RData")

rawData <- newsData
newsData <- newsData[, c('n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'num_keywords', 'is_weekend', 'kw_avg_min', 'kw_avg_avg', 'kw_avg_max', 'average_token_length', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words', 'avg_positive_polarity', 'avg_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity')]


# Overview of data structure:
str(newsData, give.attr = FALSE)

# Correlation structure:
newsData %>% cor() %>% corrplot::corrplot()


# Standardize data
newsData <- newsData %>% scale() %>% as.data.frame()

# Compute PCA
pcaNews <- newsData %>% prcomp()

# Eigenvalues
pcaNews$sdev**2


# Screeplot:
screeplot(pcaNews, type = "lines")

# Cumulative explained variance:
summary(pcaNews)

# Kaiser-Guttmann (number of components with eigenvalue larger than 1):
sum(pcaNews$sdev > 1)


# Print loadings of the first six components
pcaNews$rotation[, 1:6] %>% round(2)

pcaNews %>% biplot(choices=1:2, cex = 0.5)


# Predict log shares with all original variables
logShares <- rawData %>%
    select(shares) %>%
    mutate(logShares=log(1+shares)) %>%
    pull(logShares) %>%
    scale()

newsData <- newsData %>%
    cbind(logShares)

mod1 <- lm(logShares ~ ., data = newsData)

# Create dataframe with log shares and first 6 components
dataNewsComponents <- cbind(logShares = newsData[, "logShares"], pcaNews$x[, 1:6]) %>%
  as.data.frame()

# Predict log shares with first six components
mod2 <- lm(logShares ~ ., data = dataNewsComponents)

# Print adjusted R squared for both models
summary(mod1)$adj.r.squared
summary(mod2)$adj.r.squared

```
  
  
  
***
  
### _Interactive Maps with leaflet in R_  
  
Chapter 1 - Setting Up Interactive Web Maps  
  
Introduction to leaflet - open-source JavaScript library that makes interactive, mobile-friendly maps:  
  
* Objective for this course is to build up to an interactive map of 4-year colleges, including incorporation type (public, private, etc.)  
	* Additionally, labels that occur when hovering  
* Leaflet builds maps using tiles, which join many smaller maps together  
	* library(leaflet)  
    * leaflet() %>% addTiles()  # zooming and scrolling lead to new tiles being shown  
* In Chapter 1, will use multiple tile types to create maps of the DataCamp HQ in Belgium and Boston  
	* leaflet() %>% addProviderTiles("CartoDB") %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  
  
Map tiles - over 100 pre-canned maps that are available as bases:  
  
* Selecting a base map - consider the intended purpose of the map, and ensure that the maps selected meet that purpose  
	* Instructor has a preference for gray-scale maps (for ease of seeing other data)  
* The base maps are stored as "providers" - most are available for immediate use, but a few require registration  
	* names(providers)  # get all the available providers  
    * names(providers)[str_detect(names(providers), "OpenStreetMap")]  # all from OpenStreetMap  
    * leaflet() %>% # addTiles() addProviderTiles("OpenStreetMap.BlackAndWhite")  # replace the default with the BW OpenStreetMap  
  
Setting the default map view:  
  
* Can load the map centered on a specific point and with a requested zoom level - coomon to use ggmap::geocode()  
	* ggmap::geocode("350 5th Ave, New York, NY 10118")   # will return the lat-lon where possible (uses google API unless source="dsk" is chosen)  
* Can use either setView() or fitBounds()  
	* leaflet() %>% addTiles() %>% setView(lng = -73.98575, lat = 40.74856, zoom = 13)  # setView picks a lat/lon and zoom  
    * leaflet() %>% addTiles() %>% fitBounds( lng1 = -73.910, lat1 = 40.773, lng2 = -74.060, lat2 = 40.723)  # fitBounds defines a rectangle  
* Can limit user controls such as panning and zooming  
	* leaflet(options = leafletOptions(dragging = FALSE, minZoom = 14, maxZoom = 18)) %>% addProviderTiles("CartoDB") %>% setView(lng = -73.98575, lat = 40.74856, zoom = 18)  
    * dragging=FALSE removes the ability to pan  
    * maxZoom and minZoom limit the options for zooming  
    * leaflet() %>% addTiles() %>% setView(lng = -73.98575, lat = 40.74856, zoom = 18) %>% setMaxBounds(lng1 = -73.98575, lat1 = 40.74856, lng2 = -73.98575, lat2 = 40.74856)  
    * setMaxBounds() limits the user to the boundaries that you pre-specify  
* For more information, can go to  
	* http://leafletjs.com/reference-1.3.0.html  
    * https://rstudio.github.io/leaflet/  
  
Plotting DataCamp HQ:  
  
* Location markers are a common addition, managed using addMarkers()  
	* leaflet() %>% addTiles() %>% addMarkers(lng = -73.98575, lat = 40.74856)  
    * If single vectors are passed to lng and lat, then a single blue pin will be placed and the map will be centered/zoomed there  
    * dc_hq <- tibble( hq = c("DataCamp - NYC", "DataCamp - Belgium"), lon = c(-73.98575, 4.717863), lat = c(40.74856, 50.881363))  
    * leaflet() %>% addTiles() %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat)  
    * When the tibble is passed, then the map will be zoomed/centered such that all the pins can be displayed
dc_hq %>% leaflet() %>% addTiles() %>% addMarkers()   
    * The functions will seek a lat and lon column from the piped in data (dc_hq in this case), and pass along a note that they were used  
* Pop-ups are a common way to provide additional information about a marker  
	* leaflet() %>% addTiles() %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  # markers, with popup enabled on clicking  
    * leaflet() %>% addTiles() %>% addPopups(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  # popups instead of markers  
* Leaflets can be stored as objects (similar to ggplot2), with additions and prints and whatnot called later  
  
Example code includes:  
```{r}

# Load the leaflet library
library(leaflet)

# Create a leaflet map with default map tile using addTiles()
leaflet() %>%
    addTiles()


# Print the providers list included in the leaflet library
providers

# Print only the names of the map tiles in the providers list 
names(providers)

# Use str_detect() to determine if the name of each provider tile contains the string "CartoDB"
str_detect(names(providers), "CartoDB")

# Use str_detect() to print only the provider tile names that include the string "CartoDB"
names(providers)[str_detect(names(providers), "CartoDB")]


# Change addTiles() to addProviderTiles() and set the provider argument to "CartoDB"
leaflet() %>% 
    addProviderTiles("CartoDB")

# Create a leaflet map that uses the Esri provider tile 
leaflet() %>% 
    addProviderTiles("Esri")

# Create a leaflet map that uses the CartoDB.PositronNoLabels provider tile
leaflet() %>% 
    addProviderTiles("CartoDB.PositronNoLabels")


# Map with CartoDB tile centered on DataCamp's NYC office with zoom of 6
leaflet()  %>% 
    addProviderTiles("CartoDB")  %>% 
    setView(lng = -73.98575, lat = 40.74856, zoom = 6)


dc_hq <- tibble::tibble(hq=c("NYC", "Belgium"), lon=c(-73.98575, 4.71786), lat=c(40.7486, 50.8814))
dc_hq

# Map with CartoDB.PositronNoLabels tile centered on DataCamp's Belgium office with zoom of 4
leaflet() %>% 
    addProviderTiles("CartoDB.PositronNoLabels") %>% 
    setView(lng = dc_hq$lon[2], lat = dc_hq$lat[2], zoom = 4)


leaflet(options = leafletOptions(
                    # Set minZoom and dragging 
                    minZoom = 12, dragging = TRUE))  %>% 
  addProviderTiles("CartoDB")  %>% 
  # Set default zoom level 
  setView(lng = dc_hq$lon[2], lat = dc_hq$lat[2], zoom = 14) %>% 
  # Set max bounds of map 
  setMaxBounds(lng1 = dc_hq$lon[2] + 0.05, 
               lat1 = dc_hq$lat[2] + .05, 
               lng2 = dc_hq$lon[2] - 0.05, 
               lat2 = dc_hq$lat[2] - .05) 


# Plot DataCamp's NYC HQ
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = dc_hq$lon[1], lat = dc_hq$lat[1])

# Plot DataCamp's NYC HQ with zoom of 12    
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = -73.98575, lat = 40.74856)  %>% 
    setView(lng = -73.98575, lat = 40.74856, zoom = 12)    

# Plot both DataCamp's NYC and Belgium locations
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = dc_hq$lon, lat = dc_hq$lat)


# Store leaflet hq map in an object called map
map <- leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    # add hq column of dc_hq as popups
    addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, 
               popup = dc_hq$hq
               )

# Center the view of map on the Belgium HQ with a zoom of 5  
map_zoom <- map %>% 
      setView(lat = 50.881363, lng = 4.717863, zoom = 5)

# Print map_zoom
map_zoom

```
  
  
  
***
  
Chapter 2 - Plotting points  
  
Introduction to IPEDS Data:  
  
* Can clear the boundaries of a map, while keeping everything else (data and the like) constant  
	* m %>% clearBounds()  # kills the bounds layers  
    * m %>% clearBounds() %>% clearMarkers()  # kills the markers layers  
* The IPEDS data is the Integrated Post-Secondary Education dataset - this course uses a subset consisting of 4-year colleges  
	* Goal is to create a subset of the IPEDS data consisting of the ~300 colleges in California  
    * Can then plot and color-code the California colleges  
  
Mapping California colleges:  
  
* Clustered markers are poorly shown by pins due to obscuring  
* A nice alternative is to use circle markers, which have much less tendency for overlaps  
	* maine_colleges_map %>% clearMarkers() %>% addCircleMarkers(data = maine, radius = 3)  
    * maine_colleges_map %>% addCircleMarkers( data = maine_colleges, radius = 4, color = "red", popup = ~name)  # custom color and radius while maintaining popups  
  
Labels and pop-ups:  
  
* Can use piping as well as the tilde, which allows for referring to key variables in the piped in data  
	* ipeds %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers( lng = ~lng, lat = ~lat, popup = ~name, color = "#FF0000")  
    * Colors can be specified using hexadecimal, as shown in the example above - can find these using google and color sliders  
* Can build better popups using pipes and tildes  
	* addCircleMarkers(popup = ~paste0(name, "-", sector_label)  
    * addCircleMarkers(popup = ~paste0("<b>",name,"</b>","<br/>",sector_label))  # enhanced with html tags  
* Labels provide similar information as pop-ups, but require only a hover rather than a click  
	* ipeds %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(label = ~name, radius = 2)  
  
Color coding colleges:  
  
* Can include differential colors depending on a variables that has been piped in using colorFactor()  
	* OR <- ipeds %>% filter(state == "OR")  
    * pal <- colorFactor(palette = c("red", "blue", "#9b4a11"), levels = c("Public", "Private", "For-Profit"))  # create the color palette for future use  
    * oregon_colleges <- OR %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(radius = 2, color = ~pal(sector_label), label = ~name)  # apply as pal()  
    * oregon_colleges %>% addLegend(position = "bottomright", pal = pal, values = c("Public", "Private", "For-Profit"))  # add to legend  
* Can instead color based on a numeric value using colorNumeric()  
	* admit <- admit %>% filter(!is.na(rate), rate < 50, rate > 0)  # filer for rates that exist and are between 0 and 50  
    * pal <- colorNumeric(palette = "Reds", domain = c(1:50), reverse = TRUE)  # reverse=TRUE flips the gradient so that lower admit rates are darker red  
    * admit_map <- admit %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(radius = 4, color = ~pal(rate), label = ~name) %>% addLegend(title = "Admit Rate", pal = pal, values = c(1:50), position = "bottomright")  
* Can use RColorBrewer for default color palettes  
	* library(RColorBrewer)  
    * display.brewer.all()  
  
Example code includes:  
```{r}

# Remove markers, reset bounds, and store the updated map in the m object
map <- map %>%
    clearMarkers() %>% 
    clearBounds()

# Print the cleared map
map


ipedsRaw <- readr::read_csv("./RInputFiles/ipeds.csv")


# Remove colleges with missing sector information
ipeds <- 
    ipedsRaw %>% 
    tidyr::drop_na()

# Count the number of four-year colleges in each state
ipeds %>% 
    group_by(state)  %>% 
    count()

# Create a list of US States in descending order by the number of colleges in each state
ipeds  %>% 
    group_by(state)  %>% 
    count()  %>% 
    arrange(desc(n))

# Create a dataframe called `ca` with data on only colleges in California
ca <- ipeds %>%
    filter(state == "CA")

map <- leaflet() %>% 
    addProviderTiles("CartoDB")

# Use `addMarkers` to plot all of the colleges in `ca` on the `m` leaflet map
map %>%
    addMarkers(lng = ca$lng, lat = ca$lat)


la_coords <- data.frame(lat = 34.05223, lon = -118.2437) 

# Center the map on LA 
map %>% 
    addMarkers(data = ca) %>% 
    setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 12)

# Set the zoom level to 8 and store in the m object
map_zoom <-
    map %>%
    addMarkers(data = ca) %>%
    setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 8)

map_zoom


# Clear the markers from the map 
map2 <- map %>% clearMarkers()

# Use addCircleMarkers() to plot each college as a circle
map2 %>%
    addCircleMarkers(lng = ca$lng, lat = ca$lat)

# Change the radius of each circle to be 2 pixels and the color to red
map2 %>% 
    addCircleMarkers(lng = ca$lng, lat = ca$lat, radius = 2, color = "red")


# Add circle markers with popups for college names
map %>%
    addCircleMarkers(data = ca, radius = 2, popup = ~name)

# Change circle color to #2cb42c and store map in map_color object
map_color <- map %>% 
    addCircleMarkers(data = ca, radius = 2, color = "#2cb42c", popup = ~name)

# Print map_color
map_color


# Clear the bounds and markers on the map object and store in map2
map2 <- map %>% 
    clearBounds() %>% 
    clearMarkers()

# Add circle markers with popups that display both the institution name and sector
map2 %>% 
    addCircleMarkers(data = ca, radius = 2, 
                     popup = ~paste0(name, "<br/>", sector_label)
                     )

# Make the institution name in each popup bold
map2 %>% 
    addCircleMarkers(data = ca, radius = 2, 
                     popup = ~paste0("<b>", name, "</b>", "<br/>", sector_label)
                     )


# Add circle markers with labels identifying the name of each college
map %>% 
    addCircleMarkers(data = ca, radius = 2, label = ~name)

# Use paste0 to add sector information to the label inside parentheses 
map %>% 
    addCircleMarkers(data = ca, radius = 2, label = ~paste0(name, " (", sector_label, ")"))


# Make a color palette called pal for the values of `sector_label` using `colorFactor()`  
# Colors should be: "red", "blue", and "#9b4a11" for "Public", "Private", and "For-Profit" colleges, respectively
pal <- colorFactor(palette = c("red", "blue", "#9b4a11"), 
                   levels = c("Public", "Private", "For-Profit")
                   )

# Add circle markers that color colleges using pal() and the values of sector_label
map2 <- map %>% 
        addCircleMarkers(data = ca, radius = 2, 
                         color = ~pal(sector_label), 
                         label = ~paste0(name, " (", sector_label, ")")
                         )

# Print map2
map2


# Add a legend that displays the colors used in pal
map2 %>% 
    addLegend(pal = pal, values = c("Public", "Private", "For-Profit"))

# Customize the legend
map2 %>% 
    addLegend(pal = pal, 
              values = c("Public", "Private", "For-Profit"),
              # opacity of .5, title of Sector, and position of topright
              opacity = 0.5, title = "Sector", position = "topright"
              )

```
  
  
  
***
  
Chapter 3 - Groups, Layers, Extras  
  
Leaflet Extras Package:  
  
* The leaflet.extras package provides some nice extensibility to the baseline leaflet package  
	* leaflet() %>% addTiles() %>% addSearchOSM()  # searching open-source-maps (magnifying glass icon with search box)  
    * leaflet() %>% addTiles() %>% addSearchOSM() %>% addReverseSearchOSM()  # can also use geocode to find a click, as requested by addReverseSearchOSM()  
    * leaflet() %>% addTiles() %>% addSearchOSM() %>% addReverseSearchOSM() %>% addResetMapButton()  # can click "reset" to return to the default view  
  
Overlay Groups - ability to control the segments that are displayed on the map:  
  
* One option is to segment the data in advance, then to add as layers using addCircleMarkers  
	* ca_public <- ipeds %>% filter(sector == "Public", state == "CA")  
    * m %>% addCircleMarkers( data = ca_public, group = "Public")  
* After creating multiple calls for addCircleMarkers(), each with group=, can then activate the grouping  
	* addLayersControl( overlayGroups = c("Public", "Private", "For-Profit"))  
* Since the layers are stacked, the order in which they are added matters (they layer/stack on top of each other)  
  
Base Groups - can provide multiple options for toggling (only one may be selected at a time):  
  
* Need to call addProviderTiles() once for each layer that is an option, then activate using addLayersControl()  
	* a <- leaflet() %>% addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>% addProviderTiles("Esri", group = "Esri")   
    * a %>% addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), position = "topleft")  
* Can be handy to try a few different base groups during exploratory analysis, to find the base that best matches the rest of the analysis  
* Basic four-step process for building up the base groups includes  
	* leaflet() %>% # initialize leaflet map  
    * addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>% addProviderTiles("Esri", group = "Esri") %>% # add basemaps with groups  
    * addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>% addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Private") %>% addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "For-Profit") %>% # add marker layer for each sector with corresponding group name  
    * addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), overlayGroups = c("Public", "Private", "For-Profit")) # add layer controls for base and overlay groups  
  
Pieces of Flair:  
  
* Can customize ths search function using leaflet.extra capability  
	* ca_public <- ipeds %>% filter(sector_label == "Public", state == "CA")  
    * ca_public %>% leaflet() %>% addProviderTiles("Esri") %>% addCircleMarkers(radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>% addSearchFeatures(targetGroups = 'Public', options = searchFeaturesOptions(zoom = 10))  # will filter the search on Public data, with a specified zoom  
* Can cluster the colleges to improve readability of the maps  
	* ipeds %>% leaflet() %>% addTiles() %>% addCircleMarkers(radius = 2, color = ~pal(sector_label), clusterOptions = markerClusterOptions())  # many colleges in one circle  
  
Example code includes:  
```{r}

library(leaflet.extras)
library(htmltools)

leaflet() %>%
  addTiles() %>% 
  addSearchOSM() %>% 
  addReverseSearchOSM() 


m2 <- ipeds %>% 
    leaflet() %>% 
    # use the CartoDB provider tile
    addProviderTiles("CartoDB") %>% 
    # center on the middle of the US with zoom of 3
    setView(lat = 39.8282, lng = -98.5795, zoom=3)

# Map all American colleges 
m2 %>% 
    addCircleMarkers() 


# Create data frame called public with only public colleges
public <- filter(ipeds, sector_label == "Public")  

# Create a leaflet map of public colleges called m3 
m3 <- leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Public"
                     )

m3


# Create data frame called private with only private colleges
private <- filter(ipeds, sector_label == "Private")  

# Add private colleges to `m3` as a new layer
m3 <- m3 %>% 
    addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Private"
                     ) %>% 
    addLayersControl(overlayGroups = c("Public", "Private"))

m3


# Create data frame called profit with only for-profit colleges
profit <- filter(ipeds, sector_label == "For-Profit")  

# Add for-profit colleges to `m3` as a new layer
m3 <- m3 %>% 
    addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label),   group = "For-Profit"
                     )  %>% 
    addLayersControl(overlayGroups = c("Public", "Private", "For-Profit"))  

# Center the map on the middle of the US with a zoom of 4
m4 <- m3 %>%
    setView(lat = 39.8282, lng = -98.5795, zoom = 4) 
        
m4


leaflet() %>% 
  # Add the OSM, CartoDB and Esri tiles
  addTiles(group = "OSM") %>% 
  addProviderTiles("CartoDB", group = "Carto") %>% 
  addProviderTiles("Esri", group = "Esri") %>% 
  # Use addLayersControl to allow users to toggle between basemaps
  addLayersControl(baseGroups = c("OSM", "Carto", "Esri"))


m4 <- leaflet() %>% 
    addTiles(group = "OSM") %>% 
    addProviderTiles("CartoDB", group = "Carto") %>% 
    addProviderTiles("Esri", group = "Esri") %>% 
    addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label),  group = "Public"
                     ) %>% 
    addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Private"
                     )  %>% 
    addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "For-Profit"
                     )  %>% 
    addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), 
                     overlayGroups = c("Public", "Private", "For-Profit")
                     ) %>% 
    setView(lat = 39.8282, lng = -98.5795, zoom = 4) 

m4


ipeds %>% 
    leaflet() %>% 
    addTiles() %>% 
    # Sanitize any html in our labels
    addCircleMarkers(radius = 2, label = ~htmlEscape(name), 
                     # Color code colleges by sector using the `pal` color palette 
                     color = ~pal(sector_label), 
                     # Cluster all colleges using `clusterOptions` 
                     clusterOptions = markerClusterOptions()
                     ) 

```
  
  
  
***
  
Chapter 4 - Plotting Polygons  
  
Spatial Data - ability to plot polygons rather than points:  
  
* Polygons have many points, and are stored in SPDF (Spatial Polygons Data Frame) with 5 slots  
	* data - one observation per polygon  
    * polygons - coordinates to plot each polygon  
    * plotOrder - order for plotting  
    * bbox - rectangle containing all the polygons  
    * proj4string - coordinate reference system (CRS)  
    * All accessed using the @ symbol  
* Can join from the data component of the SPDF, accessed using @  
	* hp@data <- shp@data %>% left_join(nc_income, by = c("GEOID10" = "zipcode"))  
    * shp@polygons[[1]] %>% leaflet() %>% addPolygons()  # can plot a single polygon  
  
Mapping Polygons - can pipe SPDF in to a series of leaflet calls:  
  
* The basic polygon plotting method using leaflet() may produce shape boundaries that are too thick  
	* shp %>% leaflet() %>% addTiles() %>% addPolygons()  
    * weight - thickness of lines  
    * color - color of lines  
    * label - information shown on hover  
    * highlight - options to highlight polygon on hover  
* The refined plotting approach adds customization for better readability  
	* shp %>% leaflet() %>% addTiles() %>% addPolygons(weight = 1, color = "grey", label = ~paste0("Total Income: " dollar(income)), highlight = highlightOptions(weight = 3, color = "red", bringToFront = TRUE))  
* Can color numeric data when plotting polygons  
	* colorNumeric - maps continuous data to interpolated palettes  
    * colorBin - colors based on cut function  
    * colorQuantile - colors based on quantile  
    * nc_pal <- colorNumeric(palette = "Blues", domain = high_inc@data$mean_income)  
    * nc_pal <- colorBin(palette = "YlGn", bins = 5, domain = high_inc@data$mean_income)  
    * nc_pal <- colorQuantile(palette = "YlGn", n = 4, domain = high_inc@data$mean_income)  
* Example of coloring using colorNumeric()  
	* nc_pal <- colorNumeric("Blues", domain = high_inc@data$mean_income)  
    * previewColors(pal = nc_pal, values = c(seq(100000, 600000, by = 100000)))  # explore sample values  
    * shp %>% leaflet() %>% # addTiles() %>% addPolygons(weight = 1, fillOpacity = 1, color = ~nc_pal(mean_income), label = ~paste0("Mean Income: ", dollar(mean_income)), highlight = highlightOptions(weight = 3, color = "red", bringToFront = TRUE))  
* Sometimes need to log-transform skewed data for better displays  
  
Putting Everything Together:  
  
* Leaflet and htmlwidgets for base maps and coloring  
* Base and overlay groups to enhance interactivity  
* Features available in the leaflet.extras function  
* Can piece together a full map that includes both polygons and circle markers  
	* leaflet() %>% addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>%           
	*   addProviderTiles("Esri", group = "Esri") %>%  
    *   addPolygons(data = shp, weight = 1, fillOpacity = .75, color = ~nc_pal(log(mean_income)), label = ~paste0("Mean Income: ", dollar(mean_income)), group = "Mean Income") %>%  
    *   addCircleMarkers(data = nc_public, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>%  
    *   addCircleMarkers(data = nc_private, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Private") %>%  
    *   addCircleMarkers(data = nc_profit, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "For-Profit") %>%  
    * addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), overlayGroups = c("Public", "Private", "For-Profit", "Mean Income"))  
* Can also save a map for future use  
	* m <- leaflet() %>% addTiles() %>% addMarkers( data = ipeds, clusterOptions = markerClusterOptions())%>% addPolygons(data = shp)  
    * library(htmlwidgets)  
    * saveWidget(m, file="myMap.html")  # saves the file as html  
  
Wrap up - additional resources:  
  
* RStudio's leaflet website: https://rstudio.github.io/leaflet/  
* Leaflet extras: https://github.com/bhaskarvk/leaflet.extras  
* JavaScript library: http://leafletjs.com/  
  
Example code includes:  
```{r}


load("./RInputFiles/nc_zips.Rda")
load("./RInputFiles/wealthiest_zips.Rda")
nc_income <- readr::read_csv("./RInputFiles/mean_income_by_zip_nc.csv")
str(nc_income, give.attr = FALSE)


# Print a summary of the `shp` data
summary(shp)

# Print the class of `shp`
class(shp)

# Print the slot names of `shp`
slotNames(shp)


# Glimpse the data slot of shp
glimpse(shp@data)

# Print the class of the data slot of shp
class(shp@data)

# Print GEOID10
shp@data$GEOID10
shp@data$GEOID10 <- as.integer(as.character(shp@data$GEOID10))
str(shp@data$GEOID10)


# Glimpse the nc_income data
glimpse(nc_income)

# Summarise the nc_income data
summary(nc_income)

# Left join nc_income onto shp@data and store in shp_nc_income
shp_nc_income <- shp@data %>% 
                left_join(nc_income, by = c("GEOID10" = "zipcode"))

# Print the number of missing values of each variable in shp_nc_income
shp_nc_income %>%
  summarise_all(funs(sum(is.na(.))))


shp <- merge(shp, shp_nc_income, by=c("GEOID10", "ALAND10"))


# map the polygons in shp
shp %>% 
    leaflet() %>% 
    addTiles() %>% 
    addPolygons()

# which zips were not in the income data?
shp_na <- shp[is.na(shp$mean_income),]

# map the polygons in shp_na
shp_na %>% 
    leaflet() %>% 
    addTiles() %>% 
    addPolygons()


# summarise the mean income variable
summary(shp$mean_income)

# subset shp to include only zip codes in the top quartile of mean income
high_inc <- shp[!is.na(shp$mean_income) & shp$mean_income > 55917,]

# map the boundaries of the zip codes in the top quartile of mean income
high_inc %>%
  leaflet() %>%
  addTiles() %>%
  addPolygons()


dollar <- function (x, negative_parens=TRUE, prefix="$", suffix="") {
    # KLUGE to make this work . . . 
    needs_cents <- function(...) { FALSE }
    if (length(x) == 0) 
        return(character())
    x <- plyr::round_any(x, 0.01)
    if (needs_cents(x, largest_with_cents)) {
        nsmall <- 2L
    }
    else {
        x <- plyr::round_any(x, 1)
        nsmall <- 0L
    }
    negative <- !is.na(x) & x < 0
    if (negative_parens) {
        x <- abs(x)
    }
    amount <- format(abs(x), nsmall = nsmall, trim = TRUE, big.mark = ",", scientific = FALSE, digits = 1L)
    if (negative_parens) {
        paste0(ifelse(negative, "(", ""), prefix, amount, suffix, ifelse(negative, ")", ""))
    }
    else {
        paste0(prefix, ifelse(negative, "-", ""), amount, suffix)
    }
}


# create color palette with colorNumeric()
nc_pal <- colorNumeric("YlGn", domain = high_inc@data$mean_income)

high_inc %>%
  leaflet() %>%
  addTiles() %>%
  # set boundary thickness to 1 and color polygons blue
  addPolygons(weight = 1, color = ~nc_pal(mean_income),
              # add labels that display mean income
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              # highlight polygons on hover
              highlight = highlightOptions(weight = 5, color = "white",
              bringToFront = TRUE))


# Create a logged version of the nc_pal color palette
nc_pal <- colorNumeric("YlGn", domain = log(high_inc@data$mean_income))

# apply the nc_pal
high_inc %>%
  leaflet() %>%
  addProviderTiles("CartoDB") %>%
  addPolygons(weight = 1, color = ~nc_pal(log(mean_income)), fillOpacity = 1,
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlightOptions = highlightOptions(weight = 5, color = "white", bringToFront = TRUE))


# Print the slot names of `wealthy_zips`
slotNames(wealthy_zips)

# Print a summary of the `mean_income` variable
summary(wealthy_zips$mean_income)

# plot zip codes with mean incomes >= $200k
wealthy_zips %>% 
  leaflet() %>% 
  addProviderTiles("CartoDB") %>% 
  addPolygons(weight = 1, fillOpacity = .7, color = "Green",  group = "Wealthy Zipcodes", 
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlightOptions = highlightOptions(weight = 5, color = "white", bringToFront = TRUE))


# Add polygons using wealthy_zips
final_map <- m4 %>% 
   addPolygons(data = wealthy_zips, weight = 1, fillOpacity = .5, color = "Grey",  group = "Wealthy Zip Codes", 
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlight = highlightOptions(weight = 5, color = "white", bringToFront = TRUE)) %>% 
    # Update layer controls including "Wealthy Zip Codes"
    addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), 
                         overlayGroups = c("Public", "Private", "For-Profit", "Wealthy Zip Codes"))

# Print and explore your very last map of the course!
final_map

```
  
  
  
***
  
### _Support Vector Machines in R_  
  
Chapter 1 - Introduction  
  
Sugar content of soft drinks:  
  
* Course covers Support Vector Machines (SVM), including visualization, mechanics, situations where they work best, etc.  
	* Will stick with binary classification for this course  
* For a 1-dimensional dataset, the clusters can be separated by choosing a "separating boundary" (decision boundary)  
* Margins are the distances between the decision boundary and the closest point  
	* The best decision boundary is considered to be the decision boundary that maximizes the margin (more robust to noise)  
    * The SVM tries to find the decision boundary that maximizes the margin in n-dimensions  
  
Generating a linearly separable dataset  
  
* Can use runif to generate random data that is unifotm from 0 to 1  
	* n <- 200  
    * set.seed(42)  
    * df <- data.frame(x1 = runif(n), x2 = runif(n))  
* Can define the points with x1 < x2 as class A and the points with x1 > x2 as class B  
	* Can also create a margin by filtering out points where abs(x1-x2) is below a user-specified threshold  
  
Example code includes:  
```{r}

df <- data.frame(sample=1:25, 
                 sugar_content=c(10.9, 10.9, 10.6, 10, 8, 8.2, 8.6, 10.9, 10.7, 8, 7.7, 7.8, 8.4, 11.5, 11.2, 8.9, 8.7, 7.4, 10.9, 10, 11.4, 10.8, 8.5, 8.2, 10.6)
                 )
str(df)

#print variable names
names(df)

#build plot
plot_ <- ggplot(data = df, aes(x = sugar_content, y = c(0))) + 
    geom_point() + 
    geom_text(label = df$sugar_content, size = 2.5, vjust = 2, hjust = 0.5)

#display plot
plot_


#The maximal margin separator is at the midpoint of the two extreme points in each cluster.
mm_separator <- (8.9 + 10)/2


#create data frame
separator <- data.frame(sep = c(mm_separator))

#add ggplot layer 
plot_ <- plot_ + 
  geom_point(data = separator, x = separator$sep, y = c(0), color = "blue", size = 4)

#display plot
plot_


#set seed
set.seed(42)

#set number of data points. 
n <- 600

#Generate data frame with two uniformly distributed predictors lying between 0 and 1.
df <- data.frame(x1 = runif(n), x2 = runif(n))

#classify data points depending on location
df$y <- factor(ifelse(df$x2 - 1.4*df$x1 < 0, -1, 1), levels = c(-1, 1))


#set margin
delta <- 0.07

# retain only those points that lie outside the margin
df1 <- df[abs(1.4*df$x1 - df$x2) > delta, ]

#build plot
plot_margins <- ggplot(data = df1, aes(x = x1, y = x2, color = y)) + geom_point() + 
    scale_color_manual(values = c("red", "blue")) + 
    geom_abline(slope = 1.4, intercept = 0)+
    geom_abline(slope = 1.4, intercept = delta, linetype = "dashed") +
    geom_abline(slope = 1.4, intercept = -delta, linetype = "dashed")
 
#display plot 
plot_margins

```
  
  
  
***
  
Chapter 2 - Support Vector Classifiers - Linear Kernels  
  
Linear Support Vector Machines:  
  
* Can split the data from the previous chapter (perfectly separable) in to train/test on an 80-20 basis  
	* set.seed() = 1  
    * df[, "train"] <- ifelse(runif(nrow(df))<0.8,1,0)  
    * trainset <- df[df$train==1,]  
    * testset <- df[df$train==0,]  
    * trainColNum <- grep("train", names(trainset))  
    * trainset <- trainset[,-trainColNum]  
    * testset <- testset[,-trainColNum]  
* Decision boundaries have many shapes-types (called kernels) such as lines, polynomials, etc.  
* For this chapter, will use e1071::svm(), a function with many options  
	* formula, data, type ("C-classification" for classification), kernel ("linear" for this chapter), cost/gamma (tuning parameters, which will be left at the defaults for now), scale (boolean telling whether to scale the data in advance - FALSE makes for easier plotting, but typically would be set to TRUE in the real-world)  
* Example of running e1071::svm()  
	* library(e1071)  
    * svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear", scale = FALSE)  
    * svm_model  
    * svm_model$index  # indices of the support vectors  
    * svm_model$SV  # support vector coordinates  
    * svm_model$rho  # negative y-intercept of the decision boundary  
    * svm_model$coefs  # weighting coefficients of support vectors (magnitude is importance, side is which part of boundary)  
    * pred_train <- predict(svm_model,trainset)  
    * pred_test <- predict(svm_model,testset)  
  
Visualizing Linear SVM:  
  
* Can begin by plotting the training data, distinguished by color  
	* p <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + geom_point() + scale_color_manual(values = c("red","blue"))  
    * df_sv <- trainset[svm_model$index,]  
    * p <- p + geom_point(data = df_sv, aes(x = x1, y = x2), color = "purple", size = 4, alpha = 0.5)  
    * p  
* The support vectors tend to be close to the decision boundary - in fact, they are defined as points that "support" the boundary  
* Goal is to extract the slope and coefficients from the model (not stored in the model object)  
	* w <- t(svm_model$coefs) %*% svm_model$SV  
    * slope_1 <- -w[1]/w[2]  
    * intercept_1 <- svm_model$rho/w[2]  
    * p <- p + geom_abline(slope = slope_1, intercept = intercept_1)  
    * p <- p + geom_abline(slope = slope_1, intercept = intercept_1-1/w[2], linetype = "dashed") + geom_abline(slope = slope_1, intercept = intercept_1+1/w[2], linetype = "dashed")  
    * p  
* There are several properties observed in the plot  
	* The boundary is supported by the support vectors  
    * The boundary is "soft", which allows for uncertainty in location/shape of the boundary  
    * Can also use e1071::plot(x=myModel, data=myData) to plot the function  
  
Tuning Linear SVM:  
  
* Can tweak the cost parameter to change the size of the soft boundary for the SVM  
	* Higher costs lead to harder (smaller, narrower) decision boundaries, with fewer support vectors  
    * The implication is that raising the cost can be a good idea if the data are known to be linearly separable  
  
Multi-class problems:  
  
* SVM can manage classification problems with 3+ target types also - using the example iris data  
	* p <- ggplot(data = iris, aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point()  
    * p  
* The SVM at core is a binary classifier, but can be used in a multi-class setting  
	* Have a model for each of the choose(m, 2) possible combinations, and use majority voting on the outputs (ties broken by random)  
    * This method is called the "one against one" classification, and it is automatically included in e1071  
    * svm_model<- svm(Species ~ ., data = trainset, type = "C-classification", kernel = "linear")  # all run automatically  
  
Example code includes:  
```{r}


dfOld <- df
delta <- 0.07
df <- df[abs(1.4*df$x1 - df$x2) > delta, ]


#split train and test data in an 80/20 proportion
df[, "train"] <- ifelse(runif(nrow(df))<0.8, 1, 0)

#assign training rows to data frame trainset
trainset <- df[df$train == 1, ]
#assign test rows to data frame testset
testset <- df[df$train == 0, ]

#find index of "train" column
trainColNum <- grep("train", names(df))

#remove "train" column from train and test dataset
trainset <- trainset[, -trainColNum]
testset <- testset[, -trainColNum]


library(e1071)

#build svm model, setting required parameters
svm_model<- svm(y ~ ., 
                data = trainset, 
                type = "C-classification", 
                kernel = "linear", 
                scale = FALSE)


#list components of model
names(svm_model)

#list values of the SV, index and rho
svm_model$SV
svm_model$index
svm_model$rho

#compute training accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)

#compute test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)


#build scatter plot of training dataset
scatter_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + 
    scale_color_manual(values = c("red", "blue"))
 
#add plot layer marking out the support vectors 
layered_plot <- 
    scatter_plot + geom_point(data = trainset[svm_model$index, ], aes(x = x1, y = x2), color = "purple", size = 4, alpha = 0.5)

#display plot
layered_plot


#calculate slope and intercept of decision boundary from weight vector and svm model
w <- c(x1=6.55241, x2=-4.73278)  # calculated manually outside of this module
slope_1 <- -w[1]/w[2]
intercept_1 <- svm_model$rho/w[2]

#build scatter plot of training dataset
scatter_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + scale_color_manual(values = c("red", "blue"))
#add decision boundary
plot_decision <- scatter_plot + geom_abline(slope = slope_1, intercept = intercept_1) 
#add margin boundaries
plot_margins <- plot_decision + 
 geom_abline(slope = slope_1, intercept = intercept_1 - 1/w[2], linetype = "dashed")+
 geom_abline(slope = slope_1, intercept = intercept_1 + 1/w[2], linetype = "dashed")
#display plot
plot_margins


#build svm model
svm_model<- 
    svm(y ~ ., data = trainset, type = "C-classification", 
        kernel = "linear", scale = FALSE)

#plot decision boundaries and support vectors
plot(x = svm_model, data = trainset)


#build svm model, cost = 1
svm_model_1 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 1,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_1

#build svm model, cost = 100
svm_model_100 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 100,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_100


# Create the base train_plot
train_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + scale_color_manual(values = c("red", "blue"))
w_1 <- c(x1=6.55241, x2=-4.73278)  # calculated manually outside of this module
w_100 <- c(x1=18.3097, x2=-13.09972)  # calculated manually outside of this module
intercept_1 <- -0.005515526  # calculated outside of this module
intercept_100 <- 0.001852543  # calculated outside of this module
slope_1 <- -w_1[1]/w_1[2]
slope_100 <- -w_100[1]/w_100[2]


#add decision boundary and margins for cost = 1 to training data scatter plot
train_plot_with_margins <- train_plot + 
    geom_abline(slope = slope_1, intercept = intercept_1) +
    geom_abline(slope = slope_1, intercept = intercept_1 - 1/w_1[2], linetype = "dashed")+
    geom_abline(slope = slope_1, intercept = intercept_1 + 1/w_1[2], linetype = "dashed")

#display plot
train_plot_with_margins

#add decision boundary and margins for cost = 100 to training data scatter plot
train_plot_with_margins <- train_plot_with_margins + 
    geom_abline(slope = slope_100, intercept = intercept_100, color = "goldenrod") +
    geom_abline(slope = slope_100, intercept = intercept_100 - 1/w_100[2], linetype = "dashed", color = "goldenrod")+
    geom_abline(slope = slope_100, intercept = intercept_100 + 1/w_100[2], linetype = "dashed", color = "goldenrod")

#display plot 
train_plot_with_margins


svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear", scale = FALSE)

#compute training accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)

#compute test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot
plot(svm_model, trainset)


data(iris)
nTrials <- 100
accuracy <- numeric(nTrials)

#calculate accuracy for n distinct 80/20 train/test partitions
for (i in 1:nTrials){ 
    iris[, "train"] <- ifelse(runif(nrow(iris))<0.8, 1, 0)
    trainColNum <- grep("train", names(iris))
    trainset <- iris[iris$train == 1, -trainColNum]
    testset <- iris[iris$train == 0, -trainColNum]
    svm_model <- svm(Species~ ., data = trainset, 
                     type = "C-classification", kernel = "linear")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$Species)
}

#mean accuracy and standard deviation
mean(accuracy) 
sd(accuracy)

```
  
  
  
***
  
Chapter 3 - Polynomial Kernels  
  
Generating radially separable datasets:  
  
* The goal is to generate 2D points (again uniformly distributed on x1 and x2 using runif)  
* Can then define a value for whether the points are within x of the center  
	* radius <- 0.7  
    * radius_squared <- radius^2  
    * df$y <- factor(ifelse(df$x1^2 + df$x2^2 < radius_squared, -1, 1), levels = c(-1,1))  
    * p <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + geom_point() + scale_color_manual(values = c("-1" = "red","1" = "blue"))   
    * p  
* Can add a circular boundary  
	* circle <- function(x1_center, x2_center, r, npoint = 100){ 
	*   #angular spacing of 2*pi/npoint between points  
	*   theta <- seq(0,2*pi,length.out = npoint)  
	*   x1_circ <- x1_center + r * cos(theta)  
	*   x2_circ <- x2_center + r * sin(theta)  
	*   return(data.frame(x1c = x1_circ, x2c = x2_circ))  
	*   }  
    * boundary <- circle(x1_center = 0, x2_center = 0, r = radius)  
    * p <- p + geom_path(data = boundary, aes(x = x1c, y = x2c), inherit.aes = FALSE)  
  
Linear SVM on radially separable datasets:  
  
* The linear SVM will perform poorly on the radially separable dataset  
	* svm_model<- svm(y ~ ., data=trainset, type="C-classification", kernel="linear")  
    * svm_model  
    * pred_test <- predict(svm_model,testset)  
    * plot(svm_model,trainset)  # all points are classified as 1  
  
Kernel trick - devise a mathematical transformation that makes the data linearly separable:  
  
* For a circles could map X1 = x1**2 and X2 = x2**2, where X1 + X2 = 0.49 (which is linearly separable)  
* The polynomial kernel has a degree (e.g., 1 for linear, 2 for quadratic, etc.) and tuning parameters gamma and coef0  
	* The kernel also uses u dot v where u and v are vectors belonging to the dataset  
    * (gamma * (u dot v) + coef0) ** degree  
* Applying the quadratic kernel to the circular data from above  
	* svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)  
    * plot(svm_model, trainset)  
  
Tuning SVM:  
  
* Set a search range for each parameter, typically as a sequence of variable (e.g., in multiples of 10)  
* For each combination of parameters, build an SVM and assess the out-of-sample accuracy - can become computationally intensive, though  
	* tune_out <- tune.svm(x = trainset[,-3], y = trainset[,3], type = "C-classification", kernel = "polynomial", degree = 2, cost = 10^(-1:2), gamma = c(0.1,1,10), coef0 = c(0.1,1,10))  
    * tune_out$best.parameters$cost  
    * tune_out$best.parameters$gamma  
    * tune_out$best.parameters$coef0  
    * svm_model <- svm(y~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2, cost = tune_out$best.parameters$cost, gamma = tune_out$best.parameters$gamma, coef0 = tune_out$best.parameters$coef0)  
  
Example code includes:  
```{r}

#set number of variables and seed
n <- 400
set.seed(1)

#Generate data frame with two uniformly distributed predictors, x1 and x2
df <- data.frame(x1 = runif(n, min = -1, max = 1), x2 = runif(n, min = -1, max = 1))

#We want a circular boundary. Set boundary radius 
radius <- 0.8
radius_squared <- radius^2

#create dependent categorical variable, y, with value -1 or 1 depending on whether point lies
#within or outside the circle.
df$y <- factor(ifelse(df$x1**2 + df$x2**2 < radius_squared, -1, 1), levels = c(-1, 1))


#build scatter plot, distinguish class by color
scatter_plot <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    geom_point() +
    scale_color_manual(values = c("red", "blue"))

#display plot
scatter_plot


inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]


#default cost mode;
svm_model_1 <- svm(y ~ ., data = trainset, type = "C-classification", cost = 1, kernel = "linear")

#training accuracy
pred_train <- predict(svm_model_1, trainset)
mean(pred_train == trainset$y)

#test accuracy
pred_test <- predict(svm_model_1, testset)
mean(pred_test == testset$y)

#cost = 100 model
svm_model_100 <- svm(y ~ ., data = trainset, type = "C-classification", cost = 100, kernel = "linear")

#accuracy
pred_train <- predict(svm_model_100, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model_100, testset)
mean(pred_test == testset$y)


#print average accuracy and standard deviation
accuracy <- rep(NA, 100)
set.seed(2)

#comment
for (i in 1:100){
    df[, "train"] <- ifelse(runif(nrow(df))<0.8, 1, 0)
    trainset <- df[df$train == 1, ]
    testset <- df[df$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


#transform data
df1 <- data.frame(x1sq = df$x1^2, x2sq = df$x2^2, y = df$y)

#plot data points in the transformed space
plot_transformed <- ggplot(data = df1, aes(x = x1sq, y = x2sq, color = y)) + 
    geom_point()+ guides(color = FALSE) + 
    scale_color_manual(values = c("red", "blue"))

#add decision boundary and visualize
plot_decision <- plot_transformed + geom_abline(slope = -1, intercept = 0.64)
plot_decision


# Still want to use the old (non-squared) data
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
df$train <- NULL
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]

svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)

#measure training and test accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot
plot(svm_model, trainset)


#tune model
tune_out <- 
    tune.svm(x = trainset[, -3], y = trainset[, 3], 
             type = "C-classification", 
             kernel = "polynomial", degree = 2, cost = 10^(-1:2), 
             gamma = c(0.1, 1, 10), coef0 = c(0.1, 1, 10))

#list optimal values
tune_out$best.parameters$cost
tune_out$best.parameters$gamma
tune_out$best.parameters$coef0


#Build tuned model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", 
                 kernel = "polynomial", degree = 2, 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma, 
                 coef0 = tune_out$best.parameters$coef0)

#Calculate training and test accuracies
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model
plot(svm_model, trainset)

```
  
  
  
***
  
Chapter 4 - Radial Basis Kernel Functions  
  
Generating complex datasets:  
  
* The RBF kernel is highly flexible, can fit complex boundaries, and is common in the real-world  
* Can generate complex data by using different distributions for x and y  
	* n <- 600  
    * set.seed(42)  
    * df <- data.frame(x1 = rnorm(n, mean = -0.5, sd = 1), x2 = runif(n, min = -1, max = 1))  
* The decision boundary can then be two circles that just barely touch at the origin  
	* radius <- 0.7  
    * radius_squared <- radius^2  
    * center_1 <- c(-0.7,0)  
    * center_2 <- c(0.7,0)  
    * df$y <- factor(ifelse( (df$x1-center_1[1])^2 + (df$x2-center_1[2])^2 < radius_squared| (df$x1-center_2[1])^2 + (df$x2-center_2[2])^2 < radius_squared, -1,1), levels = c(-1,1))  
    * p <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + geom_point() + guides(color = FALSE) + scale_color_manual(values = c("red","blue"))  
    * p  
* Can then build linear, polynomial, and RBF kernels to model the data  
  
Motivating the RBF kernel:  
  
* Neither the linear kernel nor the polynomial kernel will work well for the dataset as described  
* Can use the heuristic that points near each other probably belong to the same class (similar to kNN)  
	* The kernel should have a maximum at (a, b), and should decay as you move away from (a, b)  
    * The rate of decay, all else equal should be the same in all directions, with a tunable gamma  
    * As good fortune has it, the exponential exp(-gamma * r) has all of these properties  
    * rbf <- function(r, gamma) exp(-gamma*r)  
    * ggplot(data.frame(r = c(-0, 10)), aes(r)) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.2), aes(color = "0.2")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.4), aes(color = "0.4")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.6), aes(color = "0.6")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.8), aes(color = "0.8")) +  
    *   stat_function(fun = rbf, args = list(gamma = 1), aes(color = "1")) +  
    *   stat_function(fun = rbf, args = list(gamma = 2), aes(color = "2")) +  
    *   scale_color_manual("gamma", values = c("red","orange","yellow", "green","blue","violet")) +  
    *   ggtitle("Radial basis function (gamma=0.2 to 2)")  
  
The RBF kernel simulates some of the principles of kNN using exponential decay:  
  
* The RBF kernel can be built using pre-set R commands  
	* svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "radial")  
* The predicted decision boundary will no longer be linear, and can be refined through tuning  
	* tune_out <- tune.svm(x = trainset[,-3], y = trainset[,3], gamma = 5*10^(-2:2), cost = c(0.01,0.1,1,10,100), type = "C-classification", kernel = "radial")  
    * tune_out$best.parameters$cost  
    * tune_out$best.parameters$gamma  
    * svm_model <- svm(y~ ., data=trainset, type="C-classification", kernel="radial", cost=tune_out$best.parameters$cost, gamma=tune_out$best.parameters$gamma)  
  
Example code includes:  
```{r}

#number of data points
n <- 1000

#set seed
set.seed(1)

#create dataframe
df <- data.frame(x1 = rnorm(n, mean = -0.5, sd = 1), x2 = runif(n, min = -1, max = 1))


#set radius and centers
radius <- 0.8
center_1 <- c(-0.8, 0)
center_2 <- c(0.8, 0)
radius_squared <- radius^2

#create binary classification variable
df$y <- factor(ifelse((df$x1-center_1[1])^2 + (df$x2-center_1[2])^2 < radius_squared |
                      (df$x1-center_2[1])^2 + (df$x2-center_2[2])^2 < radius_squared, -1, 1),
                      levels = c(-1, 1))


#create scatter plot
scatter_plot<- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    geom_point() + 
    scale_color_manual(values = c("red", "blue"))
 
scatter_plot 


# Create 75/25 split for train/test
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]


#build model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear")

#accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model against testset
plot(svm_model, testset)


#build model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)

#accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model
plot(svm_model, trainset)


#create vector to store accuracies and set random number seed
accuracy <- rep(NA, 100)
set.seed(2)


# Create a dummy frame dfDum for use in the for loop
dfDum <- df

#calculate accuracies for 100 training/test partitions
for (i in 1:100){
    dfDum[, "train"] <- ifelse(runif(nrow(dfDum))<0.8, 1, 0)
    trainset <- dfDum[dfDum$train == 1, ]
    testset <- dfDum[dfDum$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


#create vector to store accuracies and set random number seed
accuracy <- rep(NA, 100)
set.seed(2)

#calculate accuracies for 100 training/test partitions
for (i in 1:100){
    dfDum[, "train"] <- ifelse(runif(nrow(dfDum))<0.8, 1, 0)
    trainset <- dfDum[dfDum$train == 1, ]
    testset <- dfDum[dfDum$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "radial")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


# Re-create original 75/25 split for train/test
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]

#tune model
tune_out <- tune.svm(x = trainset[, -3], y = trainset[, 3], 
                     gamma = 5*10^(-2:2), 
                     cost = c(0.01, 0.1, 1, 10, 100), 
                     type = "C-classification", kernel = "radial")
tune_out

#build tuned model
svm_model <- svm(y~ ., data = trainset, type = "C-classification", kernel = "radial", 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma)

#calculate test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#Plot decision boundary against test data
plot(svm_model, testset)

```
  
  
  
***
  
### _Experimental Design in R_  
  
Chapter 1 - Introduction to Experimental Design  
  
Introduction to experimental design:  
  
* Experiments start with a question in mind, then finding and analyzing data  
* This course will use open data, meaning that we do not know the original experimental design  
* Key conditions of an experiment include randomization, replication, and blocking  
  
Hypothesis testing:  
  
* The null hypothesis changes depending on the question of interest - "no effect" (two-sided) or "no positive effect" (one-sided) or etc.  
* Power is the probability that the test correctly reject the null hypothesis when the alternative hypothesis is true (target >= 80%)  
* The effect size is the standardized measure of the difference that you are trying to detect  
* Sample size is generally chosen so that the effect size can be measured at the required power  
* Example of using the power package for calculating the metrics  
	* library(pwr)  
    * pwr.anova.test(k = 3, n = 20, f = 0.2, sig.level = 0.05, power = NULL)  # one must be entered as NULL (this will be calculated) ; k groups with n per group and f effect size  
  
Example code includes:  
```{r}

# load the ToothGrowth dataset
data("ToothGrowth")

#perform a two-sided t-test
t.test(x = ToothGrowth$len, alternative = "two.sided", mu = 18)

#perform a t-test
ToothGrowth_ttest <- t.test(len ~ supp, data = ToothGrowth)

#tidy the t-test model object
broom::tidy(ToothGrowth_ttest)


#group by supp, dose, then examine how many observations in ToothGrowth there are by those groups
ToothGrowth %>% 
    group_by(supp, dose) %>% 
    summarize(n=n())

#create a boxplot with geom_boxplot()
ggplot(ToothGrowth, aes(x=as.factor(dose), y=len)) + 
    geom_boxplot()

#create the ToothGrowth_aov model object
ToothGrowth_aov <- aov(len ~ dose + supp, data = ToothGrowth)

#examine the model object with summary()
summary(ToothGrowth_aov)


#less than
t.test(x = ToothGrowth$len, alternative = "less", mu = 18)

#greater than
t.test(x = ToothGrowth$len, alternative = "greater", mu = 18)


#calculate power
pwr::pwr.t.test(n = 100, d = 0.35, sig.level = 0.10, type = "two.sample", 
                alternative = "two.sided", power = NULL
                )

#calculate sample size
pwr::pwr.t.test(n = NULL, d = 0.25, sig.level = 0.05, 
                type = "one.sample", alternative = "greater", power = 0.8
                )

```
  
  
  
***
  
Chapter 2 - Basic Experiments  
  
Single and Multiple Factor Experiments:  
  
* The ANOVA (Analysis of Variance) test allows for comparing means across 3-groups; is at least one mean different  
	* model_1 <- lm(y ~ x, data = dataset)  # first option is lm followed by aov  
    * anova(model_1)  # first option is lm followed by anova  
    * aov(y ~ x, data = dataset)  # second option is a straight call to aov  
* The multiple factor experiment includes additional potential explanatory variables  
	* model2 <- lm(y ~ x + r + s + t)  
    * anova(model2)  
* The Lending Club data is 890k x 75, and contains data from a lending company  
  
Model Validation:  
  
* EDA is an important step prior to modeling the data  
* Boxplots can be a helpful way to explore the data  
	* ggplot(data = lendingclub, aes(x = verification_status, y = funded_amnt)) + geom_boxplot()  
* ANOVA and other linear models generally assume that the residuals are normally distributed  
  
A/B Testing:  
  
* A/B tests are a type of controlled experiment with only two variants of something  
* Power and sample size are crucial to A/B testing, allowing for an understanding of the required size for a desired power and expected effect size  
  
Example code includes:  
```{r}

lendingclub <- readr::read_csv("./RInputFiles/lendclub.csv")


#examine the variables with glimpse()
glimpse(lendingclub)

#find median loan_amt, mean int_rate, and mean annual_inc with summarise()
lendingclub %>% summarise(median(loan_amnt), mean(int_rate), mean(annual_inc))

# use ggplot2 to build a bar chart of purpose
ggplot(data=lendingclub, aes(x = purpose)) + geom_bar()

#use recode() to create the new purpose_recode variable.
lendingclub$purpose_recode <- lendingclub$purpose %>% recode( 
        "credit_card" = "debt_related",
        "debt_consolidation" = "debt_related", 
        "medical" = "debt_related",
        "car" = "big_purchase", 
        "major_purchase" = "big_purchase", 
        "vacation" = "big_purchase",
        "moving" = "life_change", 
        "small_business" = "life_change", 
        "wedding" = "life_change",
        "house" = "home_related", 
        "home_improvement" = "home_related"
        )


#build a linear regression model, stored as purpose_recode_model
purpose_recode_model <- lm(funded_amnt ~ purpose_recode, data = lendingclub)

#look at results of purpose_recode_model
summary(purpose_recode_model)

#get anova results and save as purpose_recode_anova
purpose_recode_anova <- anova(purpose_recode_model)

# look at the class of purpose_recode_anova
class(purpose_recode_anova)


#Use aov() to build purpose_recode_aov
purpose_recode_aov <- aov(funded_amnt ~ purpose_recode, data = lendingclub)

#Conduct Tukey's HSD test to create tukey_output
tukey_output <- TukeyHSD(purpose_recode_aov)

#tidy tukey_output to make sense of the results
broom::tidy(tukey_output)


#Use aov() to build purpose_emp_aov
purpose_emp_aov <- aov(funded_amnt ~ purpose_recode + emp_length, data=lendingclub)

#print purpose_emp_aov to the console
purpose_emp_aov

#call summary() to see the p-values
summary(purpose_emp_aov)


#examine the summary of int_rate
summary(lendingclub$int_rate)

#examine int_rate by grade
lendingclub %>% 
    group_by(grade) %>% 
    summarise(mean = mean(int_rate), var = var(int_rate), median = median(int_rate))

#make a boxplot of int_rate by grade
ggplot(lendingclub, aes(x = grade, y = int_rate)) + geom_boxplot()

#use aov() to create grade_aov plus call summary() to print results
grade_aov <- aov(int_rate ~ grade, data = lendingclub)
summary(grade_aov)


#for a 2x2 grid of plots:
par(mfrow=c(2, 2))

#plot grade_aov
plot(grade_aov)

#back to defaults
par(mfrow=c(1, 1))

#Bartlett's test for homogeneity of variance
bartlett.test(int_rate ~ grade, data=lendingclub)


#use the correct function from pwr to find the sample size
pwr::pwr.t.test(n=NULL, d=0.2, sig.level=0.05, 
                type="two.sample", alternative="two.sided", power=0.8
                )


lc_A <- c(11976148, 1203719, 54998739, 5801830, 31587242, 7711391, 54494666, 57663583, 8967787, 21760921, 44765721, 8596988, 5794746, 59501253, 10578432, 36058744, 11727607, 357888, 51936863, 1178593, 57315811, 5705168, 46024211, 12947039, 57345207, 55299831, 28763037, 49763149, 20077511, 60216198, 12295190, 1570287, 61408414, 59121340, 32349527, 5773180, 26899704, 55412161, 2217935, 16462713, 9196065, 27802028, 40949245, 56007625, 56935379, 62187473, 20178048, 604912, 58533358, 652594, 44066849, 38942161, 6414816, 65617953, 51816492, 43489983, 6794967, 42345315, 59532019, 13107597, 63249029, 7371829, 12335467, 8560739, 7337238, 887484, 23493355, 41031080, 60537197, 12816159, 38446687, 51026618, 6374688, 18685270, 296645, 44439325, 4915968, 63449566, 25256236, 63407874, 36753301, 20728660, 7937228, 13058684, 636359, 50527238, 40450502, 1018943, 12438198, 3065732, 1510626, 5764344, 37840363, 27460227, 39751366, 5028066, 43956700, 56109033, 1412622, 44289534, 41770436, 49956562, 44409121, 47168726, 60953428, 52189251, 64281487, 51928150, 1002880, 4537354, 12605849, 477843, 6808167, 38629237, 33311208, 36109419, 58593881, 40362979, 440300, 9848361, 30656060, 15691500, 4375269, 15360849, 7077904, 66076532, 33350264, 4175651, 44006939, 21130605, 54098234, 53192890, 7371114, 12967808, 58061230, 34803392, 5544911, 28843825, 63244663, 38504887, 68565204, 1211255, 63427670, 56472411, 10548622, 43957279, 59313014, 5768723, 66210490, 25507112, 55472659, 61339767, 65684813, 45544639, 43710238, 46833245, 13028661, 13167268, 3064642, 62072249, 27631726, 65825964, 15540990, 64320858, 8605358, 17795606, 9894584, 543619, 2380700, 20959552, 57743104, 63917130, 38480348, 61393540, 19916851)
lc_A <- c(lc_A, 12528162, 7264617, 61480809, 36411752, 20139228, 21290880, 390228, 45584424, 17755019, 23413261, 15490914, 1254285, 875004, 24274579, 51006600, 11458143, 5125832, 37802077, 57327243, 41059894, 64978360, 58683523, 4290736, 40919379, 65029207, 7096004, 42285591, 7388784, 65914238, 46833088, 21221678, 62855006, 10557733, 44915714, 23083224, 67289213, 9746670, 349608, 66610322, 1595886, 3635144, 38419356, 9715410, 9726377, 621152, 23213635, 18685424, 65782663, 57304429, 20770003, 8865120, 58664359, 1454540, 42404539, 60952405, 61339308, 7367648, 11215938, 41207320, 23553299, 1681376, 7617266, 30485630, 10604792, 46044414, 63094909, 59189668, 10106916, 52058386, 17763104, 6396213, 8981232, 48070364, 10615808, 11956507, 38444903, 60216940, 58310439, 10099562, 7504691, 17533228, 62236540, 38626163, 55657128, 7728107, 42415348, 42454693, 4777573, 23834164, 25157042, 1339435, 50587486, 55998961, 32950014, 28422748, 492346, 50607472, 11335041, 4254623, 65058537, 5375256, 5646680, 44430975, 4054992, 55253292, 68375791, 16822421, 64978226, 59859214, 65424555, 10112206, 6908772, 67879649, 4794842, 31227479, 17423361, 64049774, 58624386, 14829134, 50233873, 44389635, 29684724, 452267, 43044890, 55942742, 19516366, 34443897, 57135665, 34392172, 17352839, 12896521, 40451807, 43255228, 40372428, 8568706, 68364520, 3486848, 40991148, 19196658, 8658538, 65885614, 38352455, 65674149, 1029473, 39290483, 47420355, 65364529, 32318884, 13115811, 48484348, 65975356, 56129109, 3378980, 31026386, 55231010, 41113253, 1480114, 51406116, 2445051, 8627441, 60942818, 55453270, 58573102, 25767158, 9655554, 49783137, 42273770, 32038806, 681948, 65059359, 48546050, 20169281, 68546780, 7065575, 46387142, 66180493, 58430918, 1390497, 41950574, 39888056, 11774847, 55308824, 51969105, 7936525, 5960208, 7700566, 14529825, 14688918, 43024566, 21110140, 55797803, 31236439, 6817136, 1467168, 36028128, 60781310, 66595886, 57548184, 3194733, 8589175, 1546517, 17654773, 40572454, 63284984, 5780985, 39660177, 64050493, 55081623, 51346675, 1235123, 65633931, 66390924, 17413278, 57950994, 55911330, 11814853, 31357211, 56038385, 40038565, 64400706, 35034758, 60296238, 6527713, 5685238, 1062701, 63406447, 64008930, 63476297, 5114652, 20060374, 10085133, 61328568, 9435001, 56057656, 49934674, 39661404, 19616499, 34342717, 46653815, 45614269, 59290211, 31296803, 50605437, 46928301, 58562582, 63879452, 65733359, 51086476, 40601201, 9845217, 29213549, 41227222, 7337659, 46517072, 38610653, 9694813, 21350102, 46716202, 50535150, 39729407, 22263578, 25987787, 64913590, 19636684, 59311687, 4295372, 571012, 20588847, 63424767, 1099384, 3810242, 5604591, 39760687, 43739869, 56019939, 51526987, 45494853, 4302122, 21009984, 66210827, 67255219, 46613149, 63345017, 43570211, 62002161, 2214708, 4234697, 51055338, 19647002, 28593783, 6804647, 40542044, 42263319, 4784593, 19636686, 44015285, 55697847, 5814660, 15409525, 2307393, 54404433, 15490230, 62245810, 64969544, 48120716, 41040511, 51176224, 6376426, 60386775, 826517, 27601385, 8185587, 28564285, 68613325, 58623041, 60941473, 1635691, 7729270, 46417835, 57285778, 55960993, 66510262, 60285691, 61902329, 68565071)


lc_B <- c(62012715, 49974687, 27570947, 63417796, 61449107, 12906517, 57074291, 21021086, 404854, 15139172, 46774978, 50486061, 4305577, 65783354, 48544529, 31667129, 36980133, 19117791, 3845908, 846821, 40381968, 64018601, 57184860, 49963980, 44142706, 6327771, 20811335, 67336862, 3628833, 31247310, 4764984, 1619549, 56492219, 67959628, 61672211, 1472227, 55268407, 13497237, 57538143, 43096178, 35723158, 226780, 2307012, 1210773, 50273799, 28903599, 50839792, 44916418, 9714937, 51876659, 3919804, 12968154, 54978278, 6938022, 53854432, 63350177, 39692948, 67216234, 22253060, 59099446, 46135199, 11717805, 48596572, 8475061, 61462130, 21480483, 2014943, 41430440, 43196143, 243173, 61543762, 66562164, 67878273, 41100627, 11915326, 28753020, 12617369, 59090559, 55583726, 31256585, 544537, 61430245, 1681767, 7670078, 38506546, 36500594, 31367711, 46694948, 2080069, 38457330, 54524836, 27651989, 63358477, 62002922, 8995111, 45694307, 61470409, 17933815, 27370082, 66612753, 1536521, 54948920, 57548472, 876991, 40127147, 57365210, 1904740, 3195692, 743529, 67408356, 8766184, 23643466, 51336378, 13397002, 3700020, 49935259, 38455198, 63506356, 11386690, 32479126, 6300017, 67427011, 63344398, 51366616, 727247, 59291548, 21551336, 8776003, 16111335, 1051513, 61973285, 60764833, 59190150, 25406927, 10138072, 61361677, 32279884, 63337618, 49933340, 30565592, 3217416, 61883095, 63436296, 58290318, 29884855, 50353289, 14699170, 67625637, 6815821, 2286867, 6274586, 17853756, 55948157, 6995898, 44126015, 66643915, 41338910, 8626219, 67858810, 38597465, 45884338, 565018, 46436141, 15259622, 6594706, 39479497, 5535388, 5855546, 48734782, 2896555, 67296211, 713979, 33110251, 8987918, 1224687, 5637315, 484473, 9814600, 29694710, 60902260, 25897153, 40705483, 1439301, 3055155, 26319992, 6245002, 66441896, 46427698, 36330836, 8915199, 46205024, 62459417, 3497439, 54888931, 30475522, 38998249, 12636103, 60536957)
lc_B <- c(lc_B, 27521279, 2365984, 361549, 43430210, 35843833, 9768308, 12705933, 59179388, 60830121, 67929084, 36138408, 854552, 8865548, 13096420, 23836169, 61502149, 1621627, 11426617, 48274995, 41123011, 7296181, 29635336, 30565882, 8145149, 46116481, 21119590, 43894290, 65866235, 44143687, 873468, 12419378, 26378681, 55140334, 56964922, 61682200, 14338072, 65047247, 57267246, 59581503, 41093708, 48524124, 513842, 1685090, 42723216, 60647576, 55341080, 9735578, 41110083, 30255415, 56010965, 63214550, 67828966, 671468, 38540004, 65107371, 18645038, 26017706, 660734, 573283, 9454644, 64017354, 617449, 7645594, 43286428, 55941273, 8636865, 31226902, 46194753, 6160505, 1412225, 65741544, 24084859, 58532795, 41880754, 45515321, 60585561, 65272380, 7937327, 1489732, 17553239, 7638498, 1473206, 38162164, 3355990, 15610681, 57025137, 6254978, 38162571, 52768311, 5938741, 58101279, 18895673, 30175739, 38222417, 55909312, 65663878, 6607837, 24725076, 61722475, 11895058, 28182084, 185962, 55259655, 16241080, 66602227, 5781939, 60801476, 6996130, 12346893, 65672013, 19076244, 1475379, 9056893, 59492895, 56864322, 60942704, 44015940, 62225220, 39739191, 66435524, 44199929, 59471139, 38547168, 6205030, 38615829, 6698930, 66514563, 1623685, 60545969, 46703319, 39739315, 12636426, 65364691, 16403147, 9204637, 19306532, 66270322, 65653692, 22313524, 59082682, 19796545, 10766253, 50436003, 49363132, 27600713, 44865530, 57763719, 47857115, 48535477, 65986020, 58603818, 42934257, 1167844, 66390187, 58281312, 63888770, 48596526, 67385135, 24775459, 55090096, 12347068, 37317537, 64007908, 1683908, 11976597, 41019342, 6855113, 7964638, 65701227, 44037648, 23133074, 9787718, 61389384, 38418035, 33130454, 13038119, 14639242, 38505864, 65725266, 62904623, 68513661, 36039498, 6538734, 51857455, 59139740, 64341225, 21430833, 55455899, 17795459, 65128493, 46428798, 43216120, 59199242, 50364311, 41079485, 27711293, 63218354, 65492649, 50819365, 40737432, 377507, 65736437, 61488876, 44886450, 31467727, 46651816, 11914779, 65352381, 24726593, 52989922, 43105128, 34322310, 8669148, 12795739, 38485516, 39559934, 4280915, 63437401, 7103037, 44946049, 15400322, 28583975, 59592185, 877645, 56019484, 3372858, 60556772, 19846532, 11658194, 6894823, 61414862, 52708301, 48806212, 12204849, 60863986, 3919883, 37661631, 47210580, 14689912, 23393084, 60961679, 6170889, 55191727, 14690280, 42415518, 65855022, 62156039, 38536464, 44603544, 63527328, 48182146, 25867085, 61952845, 4744682, 20110370, 65854766, 57722242, 11438361, 34111919, 53262232, 12247443, 64210396, 37630339, 41237564, 46722148, 65791211, 16882760, 7719304, 37622016, 3220774, 51906280, 12446784, 50064210, 57733299, 63437152, 38445791, 3730324, 56052115, 57354312, 58010576, 626701, 7224706, 64079786, 62167132, 8396526, 7625377, 12707224, 35084508, 56022111, 52027979, 43215589, 50425264, 59253209, 28312549, 67376619, 30795837, 43869662, 20849433, 55351366, 39549686, 22972745, 1025579)


# The specific member IDs in lc_A and lc_B are not in dataset lendingclub
lendingclub_ab <- lendingclub %>%
    mutate(Group=ifelse(member_id %in% lc_A, "A", ifelse(member_id %in% lc_B, "B", "C")))


# ggplot(lendingclub_ab, aes(x=Group, y=loan_amnt)) + geom_boxplot()

#conduct a two-sided t-test
# t.test(loan_amnt ~ Group, data=lendingclub_ab)


#build lendingclub_multi
# lendingclub_multi <-lm(loan_amnt ~ Group + grade + verification_status, data=lendingclub_ab)

#examine lendingclub_multi results
# broom::tidy(lendingclub_multi)

```
  
  
  
***
  
Chapter 3 - Randomized Complete (and Balanced Incomplete) Block Designs  
  
Intro to NHANES Dataset and Sampling:  
  
* NHANES is the National Health and Nutrition Examination Study, run once every 2 years in the US since the late 1990s (was run on different frequency since the 1960s)  
* NHANES individuals are sampled from a scheme to match the US demographics - upsampling of elderly and minorities for sufficient sample size for statistical conclusions  
* Two key types of sampling  
	* Probability sampling - probability is used to select the sample (will be covered in this course)  
    * Non-probability sampling - voluntary (whoever responds), convenience (whoever the researcher can find)  
* Many types of random sampling can be run in R  
	* Simple Random Sampling - sample()  
    * Stratified Sampling - dataset %>% group_by(strata_variable) %>% sample_n()  # sample a specified number of people inside each segment  
    * Cluster Sampling - cluster(dataset, cluster_var_name, number_to_select, method = "option")  # select everyone in each randomly select cluster  
    * Systematic Sampling - every 5th or 10th or etc. person (implemented by custom functions)  
    * Multi-Stage Sampling - combinations of 2+ of the above approaches in a sensible and structured manner  
  
Randomized Complete Block Designs (RCBD):  
  
* RCBD is run when there is a potential nuisance factor in the data that might otherwise impact the results and conclusions  
	* Randomized - treatment is assigned randomly inside each block  
    * Complete - each treatment is used the same number of times inside each block  
    * Block - experimental groups are blocked to be similar (differences within the group are expected to be lesser than differences across the groups)  
    * Design - the experiment  
    * "Block what you can, randomize what you cannot"  
* The library(agricolae) allows for drawing some of the experimental designs such as an RCBD  
	* library(agricolae)  
    * trt <- letters[1:4]  
    * rep <- 4  
    * design.rcbd <- design.rcbd(trt, r = rep, seed = 42, serie = 0)  # serie has to do with tagging of number blocks  
    * design.rcbd$sketch  
  
Balanced Incomplete Block Designs (BIBD):  
  
* Incomplete blocaks are when you cannot fully fit a treatment inside a block  
	* Balanced - each pair of treatments occur together in a block an equal number of times  
    * Incomplete - not every treatment will appear in every block  
    * Block - experimental groups are blocked to be similar (differences within the group are expected to be lesser than differences across the groups  
    * Design - the experiment  
* Suppose that t is the number of treatments, k is the number of treatments per block, and r is the number of replications  
	* lambda = r * (k - 1) / (t - 1)  
    * If lambda is a whole number, then a BIBD is possible; otherwise, it is not  
  
Example code includes:  
```{r}

nhanes_demo <- readr::read_csv("./RInputFiles/nhanes_demo.csv")
nhanes_medical <- readr::read_csv("./RInputFiles/nhanes_medicalconditions.csv")
nhanes_bodymeasures <- readr::read_csv("./RInputFiles/nhanes_bodymeasures.csv")
dummy_nhanes_final <- readr::read_csv("./RInputFiles/nhanes_final.csv")

#merge the 3 datasets you just created to create nhanes_combined
nhanes_combined <- list(nhanes_demo, nhanes_medical, nhanes_bodymeasures) %>%
  Reduce(function(df1, df2) inner_join(df1, df2, by="seqn"), .)


#fill in the dplyr code
nhanes_combined %>% group_by(mcq365d) %>% summarise(mean = mean(bmxwt, na.rm = TRUE))

#fill in the ggplot2 code
nhanes_combined %>% filter(ridageyr > 16) %>% 
  ggplot(aes(x=as.factor(mcq365d), y=bmxwt)) +
  geom_boxplot()


#filter out anyone less than 16
nhanes_filter <- nhanes_combined %>% filter(ridageyr > 16)

#use simputation & impute bmxwt to fill in missing values
nhanes_final <- simputation::impute_median(nhanes_filter, bmxwt ~ riagendr)

#recode mcq365d with ifelse() & examine with table()
nhanes_final$mcq365d <- ifelse(nhanes_final$mcq365d==9, 2, nhanes_final$mcq365d)
table(nhanes_final$mcq365d)


#use sample() to create nhanes_srs
nhanes_srs <- nhanes_final[sample(nrow(nhanes_final), 2500), ]

#create nhanes_stratified with group_by() and sample_n()
nhanes_stratified <- nhanes_final %>%
  group_by(riagendr) %>%
  sample_n(2000)
table(nhanes_stratified$riagendr)

#load sampling package and create nhanes_cluster with cluster()
nhanes_cluster <- sampling::cluster(nhanes_final, "indhhin2", 6, method = "srswor")


#use str() to view design.rcbd's criteria
str(agricolae::design.rcbd)

#build trt and rep
trt <- LETTERS[1:5]
rep <- 4

#Use trt and rep to build my.design.rcbd and view the sketch part of the object
my_design_rcbd <- agricolae::design.rcbd(trt, r=rep, seed = 42, serie=0)
my_design_rcbd$sketch


#make nhanes_final$riagendr a factor variable
nhanes_final$riagendr <- factor(nhanes_final$riagendr)

#use aov() to create nhanes_rcbd
nhanes_rcbd <- aov(bmxwt ~ mcq365d + riagendr, data=nhanes_final)

#check the results of nhanes_rcbd with summary()
summary(nhanes_rcbd)

#print the difference in weights by mcq365d and riagendr
nhanes_final %>% group_by(mcq365d, riagendr) %>% summarise(mean_wt = mean(bmxwt))


#set up the 2x2 plotting grid and then plot nhanes_rcbd
par(mfrow=c(2, 2))
plot(nhanes_rcbd)
par(mfrow=c(1, 1))

#run the code to view the interaction plots
with(nhanes_final, interaction.plot(mcq365d, riagendr, bmxwt))

#run the code to view the interaction plots
with(nhanes_final, interaction.plot(riagendr, mcq365d, bmxwt))


#create my_design_bibd_1
# my_design_bibd_1 <- design.bib(LETTERS[1:3], k = 4, r = 16, serie = 0, seed = 42)  # will throw an error

#create my_design_bibd_2
# my_design_bibd_2 <- design.bib(letters[1:2], k = 3, r = 5, serie = 0, seed = 42)  # will throw warning

#create my_design_bibd_3
my_design_bibd_3 <- agricolae::design.bib(letters[1:4], k = 4, r = 6, serie = 0, seed = 42)
my_design_bibd_3$sketch


lambda <- function(t, k, r){
  return((r*(k-1)) / (t-1))
}

#calculate lambda
lambda(4, 3, 3)


#build the data.frame
creatinine <- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22)
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"))
color <- as.factor(rep(c("Black", "White", "Orange", "Spotted"), each = 3))
cat_experiment <- as.data.frame(cbind(creatinine, food, color))

#create cat_model & then wrong_cat_model and examine them with summary()
cat_model <- aov(creatinine ~ food + color, data=cat_experiment)
summary(cat_model)


#calculate lambda
lambda(3, 3, 2)

#create weightlift_model & examine results (variable does not exist in dataset)
# weightlift_model <- aov(bmxarmc ~ weightlift_treat + ridreth1, data=nhanes_final)
# summary(weightlift_model)

```
  
  
  
***
  
Chapter 4 - Latin Squares, Graeco-Latin Squares, Factorial Experiments  
  
Latin Squares have two blocking factors, assumed not to interact with each other or the treatment, and each with the same number of levels:  
  
* Latin squares can be analyzed just like an RCBD  
* In a Latin square, both the rows and the columns are the blocking factors  
* Can use nyc_scores dataset containing reading, writing, and math scores from all accredited high schools  
	* Goal is to assess the impact of a (fabricated) tutoring program on the scores by school  
  
Graeco-Latin Squares builds on Latin squares by adding an additional blocking factor:  
  
* Three blocking factors, all with the same number of levels, and assumed not to interact with each other or the treatment  
	* Greek letters added next to the Latin letters indicate the third blocking factors (can use Latin and numbers instead)  
    * All of the combinations occur only once (each letter once per row/column, and each number once per letter)  
  
Factorial Experiments - designs in which 2+ variables are crossed in an experiment, with each combination considered a factor:  
  
* Example of testing all combinations of high/low water and high/low light - each combination is tested, with TukeyHSD() applied after  
* This course will focus on 2^k factor experiments, meaning that each level has only a High/Low (or similar) possibility  
  
Next steps:  
  
* Many other types of factorial designs - do not all need to be 2**k, with many factor levels  
	* Might consider a fractional factorial design to minimize the analytical burden  
* Design should be a valued and integrated part of the process  
* There will always be some unmeasured confounders, but good design can help to reduce that noise  
  
Example code includes:  
```{r}

nyc_scores <- readr::read_csv("./RInputFiles/nyc_scores.csv")
glimpse(nyc_scores)


tEL <- c('PhD', 'BA', 'BA', 'MA', 'MA', 'PhD', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'College Student', 'Grad Student', 'MA', 'MA', 'MA', 'BA', 'MA', 'BA', 'MA', 'College Student', 'PhD', 'MA', 'MA', 'BA', 'MA', 'College Student', 'BA', 'PhD', 'Grad Student', 'MA', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'MA', 'Grad Student', 'BA', 'BA', 'College Student', 'Grad Student', 'College Student', 'BA', 'BA', 'PhD', 'BA', 'Grad Student', 'Grad Student', 'College Student', 'College Student', 'BA', 'PhD', 'College Student', 'PhD', 'PhD', 'PhD', 'College Student', 'Grad Student', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'MA', 'MA', 'College Student', 'Grad Student', 'MA', 'PhD', 'MA', 'College Student', 'MA', 'PhD', 'MA', 'College Student', 'College Student', 'Grad Student', 'PhD', 'MA', 'MA', 'Grad Student', 'MA', 'MA', 'Grad Student', 'PhD', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'PhD', 'BA', 'MA', 'Grad Student', 'BA', 'College Student', 'MA', 'College Student', 'Grad Student', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'Grad Student', 'College Student', 'College Student', 'College Student', 'MA', 'BA', 'Grad Student', 'Grad Student', 'MA', 'College Student', 'BA', 'Grad Student', 'MA', 'Grad Student', 'PhD', 'MA', 'MA', 'College Student', 'MA', 'College Student', 'PhD', 'College Student', 'MA', 'MA', 'MA', 'MA', 'College Student', 'MA', 'BA', 'MA', 'Grad Student', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'MA', 'College Student', 'MA', 'MA', 'BA', 'MA', 'College Student', 'Grad Student', 'College Student', 'MA', 'BA', 'MA', 'BA', 'College Student', 'Grad Student', 'Grad Student', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'BA', 'MA', 'BA', 'College Student', 'MA', 'BA', 'MA', 'Grad Student', 'MA', 'PhD', 'MA', 'BA', 'Grad Student', 'MA', 'BA', 'BA', 'MA', 'BA', 'College Student', 'BA', 'MA', 'MA', 'BA', 'MA', 'College Student', 'BA', 'Grad Student', 'MA', 'BA', 'MA', 'MA', 'MA', 'BA', 'College Student', 'College Student')
tEL <- c(tEL, 'BA', 'Grad Student', 'BA', 'BA', 'MA', 'Grad Student', 'BA', 'MA', 'BA', 'PhD', 'MA', 'MA', 'MA', 'BA', 'College Student', 'PhD', 'BA', 'Grad Student', 'BA', 'College Student', 'BA', 'MA', 'College Student', 'MA', 'College Student', 'Grad Student', 'College Student', 'MA', 'PhD', 'BA', 'PhD', 'Grad Student', 'BA', 'BA', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'MA', 'BA', 'College Student', 'BA', 'MA', 'College Student', 'MA', 'College Student', 'BA', 'MA', 'BA', 'BA', 'MA', 'PhD', 'BA', 'MA', 'Grad Student', 'College Student', 'MA', 'College Student', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'Grad Student', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'Grad Student', 'MA', 'BA', 'College Student', 'MA', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'College Student', 'College Student', 'College Student', 'College Student', 'College Student', 'PhD', 'MA', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'MA', 'MA', 'PhD', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'MA', 'PhD', 'BA', 'BA', 'Grad Student', 'Grad Student', 'PhD', 'BA', 'BA', 'Grad Student', 'College Student', 'BA', 'College Student', 'MA', 'MA', 'MA', 'Grad Student', 'BA', 'BA', 'MA', 'Grad Student', 'PhD', 'BA', 'Grad Student', 'Grad Student', 'Grad Student', 'BA', 'MA', 'BA', 'College Student', 'College Student', 'Grad Student', 'MA', 'Grad Student', 'Grad Student', 'BA', 'BA', 'MA', 'College Student', 'BA', 'Grad Student', 'Grad Student', 'College Student', 'Grad Student', 'College Student', 'PhD', 'BA', 'MA', 'MA', 'BA', 'College Student', 'College Student', 'PhD', 'MA', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'PhD', 'MA', 'MA', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'College Student', 'Grad Student', 'BA', 'MA', 'MA', 'Grad Student', 'BA', 'Grad Student', 'Grad Student', 'MA', 'PhD', 'Grad Student', 'Grad Student', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'MA', 'BA', 'MA', 'College Student', 'MA', 'PhD', 'BA', 'MA', 'College Student', 'PhD', 'PhD', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'MA', 'BA', 'College Student', 'BA', 'BA', 'MA', 'MA', 'College Student', 'College Student', 'Grad Student', 'College Student', 'MA', 'MA', 'MA', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'BA', 'Grad Student', 'BA', 'MA', 'College Student', 'MA')


nyc_scores <- nyc_scores %>%
    mutate(Teacher_Education_Level=tEL)
glimpse(nyc_scores)


#mean, var, and median of Math score
nyc_scores %>%
    group_by(Borough) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))

#mean, var, and median of Math score
nyc_scores %>%
    group_by(Teacher_Education_Level) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))

#mean, var, and median of Math score
nyc_scores %>%
    group_by(Borough, Teacher_Education_Level) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))


# If we want to use SAT scores as our outcome, we need to examine their missingness
# First, look at the pattern of missingness using md.pattern() from the mice package
# There are 60 scores missing in each of the scores
# There are many R packages which help with more advanced forms of imputation, such as MICE, Amelia, mi, and more
# We will use the simputation andimpute_median() as we did previously

#examine missingness with md.pattern()
mice::md.pattern(nyc_scores)

#impute the Math, Writing, and Reading scores by Borough
nyc_scores_2 <- simputation::impute_median(nyc_scores, Average_Score_SAT_Math ~ Borough)

#convert Math score to numeric
nyc_scores_2$Average_Score_SAT_Math <- as.numeric(nyc_scores_2$Average_Score_SAT_Math)

#examine scores by Borough in both datasets, before and after imputation
nyc_scores %>% 
  group_by(Borough) %>% 
  summarise(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE))
nyc_scores_2 %>% 
  group_by(Borough) %>% 
  summarise(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE))


#design a LS with 5 treatments A:E then look at the sketch
my_design_lsd <- agricolae::design.lsd(LETTERS[1:5], serie=0, seed=42)
my_design_lsd$sketch


# To execute a Latin Square design on this data, suppose we want to know the effect of of our tutoring program, which includes one-on-one tutoring, two small groups, and an in and after school SAT prep class
# A new dataset nyc_scores_ls is available that represents this experiment. Feel free to explore the dataset in the console.

# We'll block by Borough and Teacher_Education_Level to reduce their known variance on the score outcome
# Borough is a good blocking factor because schools in America are funded partly based on taxes paid in each city, so it will likely make a difference on quality of education

lsID <- c('11X290', '10X342', '09X260', '09X412', '12X479', '14K478', '32K554', '14K685', '22K405', '17K382', '05M692', '02M427', '02M308', '03M402', '02M282', '30Q501', '26Q495', '24Q455', '29Q326', '25Q670', '31R450', '31R445', '31R080', '31R460', '31R455')
lsTP <- c('One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)')

nyc_scores_ls <- nyc_scores_2 %>%
    filter(School_ID %in% lsID) %>%
    mutate(Tutoring_Program=lsTP)


#build nyc_scores_ls_lm
nyc_scores_ls_lm <- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level,
                       data=nyc_scores_ls
                       )

#tidy the results with broom
nyc_scores_ls_lm %>% broom::tidy()

#examine the results with anova
nyc_scores_ls_lm %>% anova()


#create a boxplot of Math scores by Borough, with a title and x/y axis labels
ggplot(nyc_scores, aes(x=Borough, y=Average_Score_SAT_Math)) + 
  geom_boxplot() + 
  ggtitle("Average SAT Math Scores by Borough, NYC") + 
  xlab("Borough (NYC)") + 
  ylab("Average SAT Math Scores (2014-15)")


#create trt1 and trt2
trt1 <- LETTERS[1:5]
trt2 <- 1:5

#create my_graeco_design
my_graeco_design <- agricolae::design.graeco(trt1, trt2, serie=0, seed=42)

#examine the parameters and sketch
my_graeco_design$parameters
my_graeco_design$sketch


glsID <- c('09X241', '10X565', '09X260', '07X259', '11X455', '18K563', '23K697', '32K403', '22K425', '16K688', '02M135', '06M348', '02M419', '02M489', '04M495', '30Q502', '24Q530', '30Q555', '24Q560', '27Q650', '31R440', '31R064', '31R450', '31R445', '31R460')
glsTP <- c('SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)')
glsHT <- c('Small Group', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Individual')


nyc_scores_gls <- nyc_scores_2 %>%
    filter(School_ID %in% glsID) %>%
    mutate(Tutoring_Program=glsTP, Homework_Type=glsHT)


#build nyc_scores_gls_lm
nyc_scores_gls_lm <- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level + Homework_Type, data=nyc_scores_gls)

#tidy the results with broom
nyc_scores_gls_lm %>% broom::tidy()

#examine the results with anova
nyc_scores_gls_lm %>% anova()


pctTHL <- c(1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2)
pctBHL <- c(2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1)
tP <- c('Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No')

nyc_scores <- nyc_scores %>%
    select(-Teacher_Education_Level) %>%
    mutate(Percent_Tested_HL=factor(pctTHL), Percent_Black_HL=factor(pctBHL), Tutoring_Program=factor(tP))


#build the boxplots for all 3 factor variables: tutoring program, pct black, pct tested
ggplot(nyc_scores, aes(x=Tutoring_Program, y=Average_Score_SAT_Math)) + 
    geom_boxplot()

ggplot(nyc_scores, aes(x=Percent_Black_HL, y=Average_Score_SAT_Math)) + 
    geom_boxplot()

ggplot(nyc_scores, aes(x=Percent_Tested_HL, y=Average_Score_SAT_Math)) + 
    geom_boxplot()


#create nyc_scores_factorial and examine the results
nyc_scores_factorial <- aov(Average_Score_SAT_Math ~ Percent_Tested_HL * Percent_Black_HL * Tutoring_Program, data=nyc_scores)
broom::tidy(nyc_scores_factorial)


#use shapiro.test() to test the outcome
shapiro.test(nyc_scores$Average_Score_SAT_Math)

#plot nyc_scores_factorial to examine residuals
par(mfrow = c(2, 2))
plot(nyc_scores_factorial)
par(mfrow = c(1, 1))

```
  
  
  
***
  
### _Structural Equation Modeling with lavaan in R_  
  
Chapter 1 - One-Factor Models  
  
Model Specification - Structural Equation Models (SEM) - explore relationships between variables:  
  
* Can confirm the structure of a developed model also  
* Two variable types - manifest (directly measured) which are represented by squares, and latent (abstract underlying phenomenon) represented as circles  
	* The manifest variables are assumed to be driven by the latent variables (such as intelligence)  
* Can set up an analysis in R using lavaan based on 1939 intelligence data  
	* library(lavaan)  
    * data(HolzingerSwineford1939)  
    * example model <- 'latent_variable =~ manifest_variable1 + manifest_variable2 + ...'  # latent_variable can have any name not in dataset, =~ is direction of prediction  
    * visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'  # x1, x2, x3, x7, x8, x9 are visual components inside the 1939 dataset  
  
Model Analysis:  
  
* Degrees of freedom are based on df = Possible Values - Estimated Values  
	* Possible Values = Manifest Variables * (Manifest Variables + 1) / 2  
    * Models need to have at least 3 manifest variables and df > 0  
    * Can use scaling and constraints to control degrees of freedom - managed inside lavaan but can modify defaults  
* Can run the models using lavaan in R  
	* visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'  
    * visual.fit <- cfa(model = visual.model, data = HolzingerSwineford1939)  # include the previously defined model and the data frame  
    * summary(visual.fit)  # basic information about the model  
    * The loadings (weightings) for each of the manifest variables will be shown, typically with the first coefficient set to 1 as per the scaling  
    * The variance estimates are also provided for each of the variables - should be positive, but can be negative (needs to be troubleshot)  
  
Model Assessment:  
  
* Standardized loadings measure the strength of the relationships between the manifest variables and the latent variables  
	* Can be measured based on the estimates, relative to the variable that was set as 1.00  
    * Can instead use the standardized solution based on the z-scores  
    * summary(visual.fit, standardized = TRUE)  # to get the standardized solution (Std.all column, with close to 1 being best; Std.lv being scaled like the loading variable)  
* The model fit measures how well the data fit the specified model  
	* Goodness of fit indices like the Comparative Fit Index or the Tucker Lewis Index - goal is closer to 1 and 0.9+  
    * Badness of fit indices like RMSE Approximation or Standardized Root Mean Square Residual (SRMR) - goal is lower and 0.1-  
    * summary(visual.fit, standardized = TRUE, fit.measures = TRUE)  # will show most common fit meaasures  
  
Example code includes:  
```{r}

#Load the lavaan library
library(lavaan)

#Look at the dataset
data(HolzingerSwineford1939, package="lavaan")
head(HolzingerSwineford1939[ , 7:15])

#Define your model specification
text.model <- "textspeed =~ x4 + x5 + x6 + x7 + x8 + x9"

#Analyze the model with cfa()
text.fit <- lavaan::cfa(model=text.model, data=HolzingerSwineford1939)

#Summarize the model
summary(text.fit)
summary(text.fit, standardized=TRUE)
summary(text.fit, fit.measures=TRUE)


#Look at the dataset
data(PoliticalDemocracy, package="lavaan")
head(PoliticalDemocracy)

#Define your model specification
politics.model <- "poldemo60 =~ y1 + y2 + y3 + y4"

#Analyze the model with cfa()
politics.fit <- lavaan::cfa(model = politics.model, data = PoliticalDemocracy)

#Summarize the model
summary(politics.fit, standardized=TRUE, fit.measures=TRUE)

```
  
  
  
***
  
Chapter 2 - Multi-Factor Models  
  
Multifactor Specification - exploring multiple latent relationships, and their relationships to each other:  
  
* Combining manifest variables that represent different latent variables often results in a model with poor fit  
* Can instead convert each of the manifest variables to the appropriate latent variable, for example  
	* visual.model <- 'visual =~ x1 + x2 + x3'  
    * visual.fit <- cfa(model = visual.model, data = HolzingerSwineford1939)   
    * speed.model <- 'speed =~ x7 + x8 + x9'  
    * speed.fit <- cfa(model = speed.model, data = HolzingerSwineford1939)  
* However, having too many models can lead to having zero degrees of freedom; constraints (such as same loading for x2/x3) are used to address this  
	* visual.model <- 'visual =~ x1 + a*x2 + a*x3'  # The a means that x2 and x3 will be set equal to each other, while a number rather than a would use that exact number  
* One larger model can sometimes better capture all the relationships  
	* twofactor.model <- 'visual =~ x1 + x2 + x3 \n speed =~ x7 + x8 + x9'  # adding them all at the same time (must have new line for new model)  
    * twofactor.fit <- cfa(model = twofactor.model, data = HolzingerSwineford1939)  
    * summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)  
  
Model Structure:  
  
* The two-factor model assumes there is a covariant relationship between the latent variables - basically, one latent variable can predict another  
* Can see the correlation between the standardized variables using the summary() function - technically shows R-squared  
	* =~ creates latent variables  
    * ~~ creates covariances between variables  
    * ~ creates direct prediction between variables  
    * if there is a newline followed by 'speed ~~ 0*visual' then speed will be assumed NOT to vary at all with visual  
    * if there is a newline followed by 'speed ~ visual' then there is assumed to be a direct relationship between these variables  
  
Modification Indices:  
  
* If a model has a poor fit, can examine the standardized solutions - desire is to see loading greater than 0.3  
* Model problems can often be identified by variances that are very high relative to the raw data  
* Modification indices can help show the improvement in the model when an additional index is added  
	* modificationindices(twofactor.fit, sort = TRUE)  
    * Output will be lhs op rhs (left-hand side, operator, right-hand-side) followed by mi (modification index, a form of chi-squared)  
    * Parameters should be added one at a time, and only if they "make theoretical sense"  
    * Take the desired path(s) and add them as new lines in the model  
  
Model Comparison:  
  
* Can create and save two models, then analyze both using the same cfa(), then use anova() to compare the models  
	* anova(twofactor.fit, twofactor.fit1)  
    * This is only useful for nested models that otherwise share the same variables  
* Can also compare the fit indices using more detailed criteria  
	* fitmeasures(twofactor.fit)  
    * AIC (lower is better, including more negative better than less negative)  
    * ECVI is the likelihood of replicating the model with the same sample size and population (lower is better)  
    * fitmeasures(twofactor.fit1, c("aic", "ecvi"))  
  
Example code includes:  
```{r eval=FALSE}

#Create your text model specification
text.model <- 'text =~ x4 + x5 + x6'

#Analyze the model
text.fit <- cfa(model=text.model, data=HolzingerSwineford1939)

#Summarize the model
summary(text.fit, standardized = TRUE, fit.measures = TRUE)


#Update the model specification by setting two paths to the label a
text.model <- 'text =~ x4 + a*x5 + a*x6'

#Analyze the model
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)

#Summarize the model
summary(text.fit, standardized = TRUE, fit.measures = TRUE)


#Create a two-factor model of text and speed variables
twofactor.model <- 'text =~ x4 + x5 + x6
speed =~ x7 + x8 + x9'

#Previous one-factor model output
summary(text.fit, standardized = TRUE, fit.measures = TRUE)

#Two-factor model specification
twofactor.model <- 'text =~ x4 + x5 + x6
speed =~ x7 + x8 + x9'

#Use cfa() to analyze the model
twofactor.fit <- cfa(model=twofactor.model, data=HolzingerSwineford1939)

#Use summary() to view the fitted model
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)


#Load the library and data
data(epi, package="psych")

#Specify a three-factor model with one correlation set to zero
epi.model <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
extraversion ~~ 0*neuroticism'

#Run the model
epi.fit <- cfa(model = epi.model, data = epi)

#Examine the output 
summary(epi.fit, standardized = TRUE, fit.measures = TRUE)


#Specify a three-factor model where lying is predicted by neuroticism
epi.model <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
lying ~ neuroticism'


#Run the model
epi.fit <- cfa(model = epi.model, data = epi)

#Examine the output 
summary(epi.fit, standardized = TRUE, fit.measures = TRUE)

#Calculate the variance of V1
var(epi$V1, na.rm=TRUE)

#Examine the modification indices
modificationindices(epi.fit, sort=TRUE)


#Edit the model specification
epi.model1 <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
neuroticism =~ V3'

#Reanalyze the model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Summarize the updated model
summary(epi.fit1, standardized = TRUE, fit.measures = TRUE)


#Analyze the original model
epi.fit <- cfa(model = epi.model, data = epi)

#Analyze the updated model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Compare those models
anova(epi.fit, epi.fit1)


#Analyze the original model
epi.fit <- cfa(model = epi.model, data = epi)

#Find the fit indices for the original model
fitmeasures(epi.fit)[c("aic", "ecvi")]

#Analyze the updated model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Find the fit indices for the updated model
fitmeasures(epi.fit1)[c("aic", "ecvi")]

```
  
  
  
***
  
Chapter 3 - Troubleshooting Model Errors and Diagrams  
  
Heywood Cases on the Latent Variable:  
  
* Heywood cases (defined by Heywood in 1931) are cases where correlations (greater than 1) or variances (negative) are out of bounds  
* The lavaan package will throw a warning that the matrix of latent variables is "not positive definite"  
	* Usually occurs because one of the latent variables is really a combination of the others  
    * Can then identify the highly correlated variables, and collapse them in to a single equation (fewer factors or the like)  
  
Heywood Cases on the Manifest Variable (negative error variances):  
  
* Generally occur dur to a mis-specified (under-specified) model, small sample sizes, manifest variables on vastly different scales, etc.  
* The lavaan package will throw a warning that "model has not converged"  
	* summary(negative.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)  # rsquare can help to identify the issue; variance in each manifest variable should be (0, 1)  
* Can just freeze the variance of one of the wonky variables to its variance in the raw data  
	* negative.model <- 'latent1 =~ V1 + V2 + V3\nlatent2 =~ V4 + V5 + V6\nV2 ~~ 18.83833*V2'   # 18.84 is var(V2)  
  
Create Diagrams with semPaths():  
  
* The semPlot library allows for diagramming the fit models  
	* library(semPlot)  
    * twofactor.model <- 'text =~ x4 + x5 + x6\nspeed =~ x7 + x8 + x9'  
    * twofactor.fit <- cfa(model = twofactor.model, data = HolzingerSwineford1939)  
    * semPaths(object = twofactor.fit)  
* The double-headed arrows on the manifest variables are variances, and the double-headed arrows on the latent variables are covariances  
* There are many options for semPaths, and allow a few will be covered here  
	* semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1)  # std is standardized while par is parameters; edge.label.cex is the font size for the edges  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "circle")  # "tree" is the default for layouts  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "tree", rotation = 2)  # rotation can only be used for trees; 2 means left/right  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "tree", rotation = 2, what = "std", edge.color = "purple")  # what colors arrows by strength  
  
Example code includes:  
```{r}

badlatentdata <- readr::read_csv("./RInputFiles/badlatentdata.csv")
badvardata <- readr::read_csv("./RInputFiles/badvardata.csv")

adoptsurvey <- badlatentdata %>%
    select(-X1) %>%
    rename(pictures=V1, background=V2, loveskids=V3, energy=V4, wagstail=V5, playful=V6)

#Look at the data
str(adoptsurvey, give.attr=FALSE)
head(adoptsurvey)

#Build the model
adopt.model <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model = adopt.model, data = adoptsurvey)
lavInspect(adopt.fit, "cov.lv")
summary(adopt.fit, standardized=TRUE, fit.measures=TRUE)


#Edit the original model 
adopt.model <- 'goodstory =~ pictures + background + loveskids + energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model = adopt.model, data = adoptsurvey)

#Look for Heywood cases
summary(adopt.fit, standardized = TRUE, fit.measures = TRUE)



adoptsurvey <- badvardata %>%
    select(-X1) %>%
    rename(pictures=V1, background=V2, loveskids=V3, energy=V4, wagstail=V5, playful=V6)
str(adoptsurvey, give.attr=FALSE)
summary(adoptsurvey)


#Build the model
adopt.model <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model=adopt.model, data=adoptsurvey)

#Summarize the model to view the negative variances
summary(adopt.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#View the variance of the problem manifest variable
var(adoptsurvey$wagstail)


#Update the model using 5 decimal places
adopt.model2 <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful
wagstail~~23.07446*wagstail'

#Analyze and summarize the updated model
adopt.fit2 <- cfa(model = adopt.model2, data = adoptsurvey)
summary(adopt.fit2, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)


#Create a default picture
semPlot::semPaths(adopt.fit)

#Update the default picture
semPlot::semPaths(object = adopt.fit, layout="tree", rotation=2)

#Update the default picture
semPlot::semPaths(object = adopt.fit, layout = "tree", rotation = 2, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "blue"
                  )

```
  
  
  
***
  
Chapter 4 - Full Example and Extension  
  
Model WAIS-III IQ Scale:  
  
* WAIS-III is a four-factor model of intelligence, including verbal, working memory, perceptual organization, and processing speed  
	* Idea is that Verbal IQ drives verbal and working memory, Performance IQ drives perceptual and processing, and Verbal/Performance drive each other  
    * 4 latent variables, measured by 12 manifest variables, with 2 additional latent variables at a higher layer that drive the initial 4 latent variables  
  
Update WAIS-III Model:  
  
* Once the model is stable, can look for additional areas to further improve the model  
* Variables that are poor on loadings and are also high in variance should be further explored  
* Can also use modification indices to better understand and model the data  
	* modificationindices(wais.fit, sort = TRUE)  
  
Hierarchical Model of IQ:  
  
* One overall IQ that is the latent variable for all of the other latent variable  
	* wais.model3 <- 'verbalcomp =~ vocab + simil + inform + compreh  
    * workingmemory =~ arith + digspan + lnseq  
    * perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch  
    * simil ~~ inform  
    * general =~ verbalcomp + workingmemory + perceptorg'  # the general is a new latent variable, built from other latent variables  
* The updated model will often have the same fit indices (simply shifting parameters from covariances to loadings)  
  
Wrap Up:  
  
* Learned model syntax for lavaan (=~ for latent, ~~ for covariance/correlation, and ~ for prediction)  
* Learned to add constraints and troubleshoot Heywood cases  
* Learned one-factor, multi-factor, and hierarchical models  
  
Example code includes:  
```{r}

IQdata <- readr::read_csv("./RInputFiles/IQdata.csv")
glimpse(IQdata)
IQdata <- IQdata %>%
    select(-X1)
glimpse(IQdata)


#Build a four-factor model
wais.model <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason
processing =~ digsym + symbolsearch'

#Analyze the model
wais.fit <- cfa(model=wais.model, data=IQdata)

#Summarize the model
summary(wais.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)


#Edit the original model
wais.model <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch'

#Analyze the model
wais.fit <- cfa(model=wais.model, data=IQdata)

#Summarize the model
summary(wais.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#Update the default picture
semPlot::semPaths(object = wais.fit, layout = "tree", rotation = 1, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "black"
                  )

#Examine modification indices 
modificationindices(wais.fit, sort = TRUE)


#Update the three-factor model
wais.model2 <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch
simil ~~ inform'

#Analyze the three-factor model where data is IQdata
wais.fit2 <- cfa(model=wais.model2, data=IQdata)

#Summarize the three-factor model 
summary(wais.fit2, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#Compare the models
anova(wais.fit, wais.fit2)


#View the fit indices for the original model
fitmeasures(wais.fit, c("aic", "ecvi"))

#View the fit indices for the updated model
fitmeasures(wais.fit2, c("aic", "ecvi"))


#Update the three-factor model to a hierarchical model
wais.model3 <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch
simil ~~ inform
general =~ verbalcomp + workingmemory + perceptorg'

#Analyze the hierarchical model where data is IQdata
wais.fit3 <- cfa(model = wais.model3, data = IQdata)

#Examine the fit indices for the old model
fitmeasures(wais.fit2, c("rmsea", "srmr"))

#Examine the fit indices for the new model
fitmeasures(wais.fit3, c("rmsea", "srmr"))


#Update the default picture
semPlot::semPaths(object = wais.fit3, layout = "tree", rotation = 1, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "navy"
                  )

```
  
  
  
***
  
### _Working with Data in the Tidyverse_  
  
Chapter 1 - Explore Data  
  
Import data:  
  
* Begging steps of the pipeline include importing, tidying, and transforming (wrangling)  
* Focus of this course will be recatngular data including both columns (variables) and rows (observations)  
	* bakers  # 10x6 tibble  
    * tibbles are a special type of data frame - both store rectangular data in R  
* Can read the data using readr::read_csv()  
	* ?read_csv  
    * bakers <- read_csv("bakers.csv")  
    * bakers  # same 10x6 tibble  
  
Know data:  
  
* The bakeoff data includes three types of challenges - Signature, Technical, Showstopper  
* Tibble printing by default will cut off columns and just show the variables - glimpse from dplyr can help with visualizing  
	* glimpse(bakers_mini)  
    * library(skimr)  
    * skim(bakers_mini)  # skim provides statistics for every column depending on the variable types  
  
Count data - broken video that provides some code snippets:  
  
* bakers %>% distinct(series)  
* bakers %>% count(series)  
* bakers %>% group_by(series) %>% summarize(n = n())  
* bakers %>% count(aired_us, series)  
* bakers %>% count(aired_us, series) %>% mutate(prop_bakers = n/sum(n))  
* bakers %>% group_by(aired_us, series) %>% summarize(n = n()) %>% mutate(prop_bakers = n/sum(n))  
* bakers %>% count(aired_us, series) %>% count(aired_us)  
  
Example code includes:  
```{r}

# Read in "bakeoff.csv" as bakeoff
bakeoff <- readr::read_csv("./RInputFiles/bakeoff.csv")

# Print bakeoff
bakeoff


# Data set above is already OK - UNKNOWN are NA in CSV
# Filter rows where showstopper is UNKNOWN 
bakeoff %>% 
    filter(showstopper == "UNKNOWN")

# Edit to add list of missing values
bakeoff <- read_csv("./RInputFiles/bakeoff.csv", na = c("", "NA", "UNKNOWN"))

# Filter rows where showstopper is NA 
bakeoff %>% 
    filter(is.na(showstopper))


# Edit to filter, group by, and skim
bakeoff %>% 
  filter(!is.na(us_season)) %>% 
  group_by(us_season) %>%
  skimr::skim()
  

bakeoff %>% 
  distinct(result)

# Count rows by distinct results
bakeoff %>% 
  count(result)

# Count whether or not star baker
bakeoff %>% 
  count(result=="SB")


# Count the number of rows by series and episode
bakeoff %>%
  count(series, episode)

# Add second count by series
bakeoff %>% 
  count(series, episode) %>%
  count(series)


# Count the number of rows by series and baker
bakers_by_series <- 
  bakeoff %>%
  count(series, baker)

# Print to view
bakers_by_series

# Count again by series
bakers_by_series %>%
  count(series)

# Count again by baker
bakers_by_series %>%
  count(baker, sort=TRUE)


ggplot(bakeoff, aes(x=episode)) + 
    geom_bar() + 
    facet_wrap(~series)

```
  
  
  
***
  
Chapter 2 - Tame Data  
  
Cast column types:  
  
* Type-casting can be an important step in taming data  
* The readr package has options for col_type within the read_csv() function  
	* By default, all of the column types are guessed from the first 1,000 rows  
    * bakers_raw %>% dplyr::slice(1:4)  # look at the first 4 rows  
* Can convert a character to a number using parse_number()  
	* parse_number("36 years")  # will become 36  
    * bakers_tame <- read_csv(file = "bakers.csv", col_types = cols(age = col_number()) )  # col_number() will wrangle the age column to a numeric  
* Can also use the parse_date capability to manage datetime inputs  
	* parse_date("14 August 2012", format = "%d %B %Y")  
    * bakers <- read_csv("bakers.csv", col_types = cols( last_date_uk = col_date(format = "%d %B %Y") ))  # col_date() will wrangle last_date_uk to a datetime  
* There is always both a parse_* and a col_* for any given data type; can practive with parse_* then use col_* in the read-in  
  
Recode values:  
  
* The recode() function in dplyr can be used to recode values in the data  
	* young_bakers %>% mutate(stu_label = recode(student, `0` = "other", .default = "student"))  # 0 will become other, anything else will become student  
    * young_bakers %>% mutate(stu_label = recode(student, `0` = NA_character_, .default = "student"))  # create NA for a specific string  
    * young_bakers %>% mutate(student = na_if(student, 0))  # na_if will convert to NA if the condition(s) is met  
  
Select variables:  
  
* Can select just a subset of the variables using select  
* The select() function is powerful when you only need to work with a subset of the data  
	* young_bakers2 %>% select(baker, series_winner)  # keep these variables  
    * young_bakers2 %>% select(-technical_winner)  # drop these variables (signalled by the minus sign)  
* Can use helper functions inside the select() call  
	* young_bakers2 %>% select(baker, starts_with("series"))  
    * young_bakers2 %>% select(ends_with("winner"), baker)  
    * young_bakers2 %>% select(contains("bake"))  
* The filter() function works on rows rather than columns  
	* young_bakers2 %>% filter(series_winner == 1 | series_runner_up == 1)  
  
Tame variable names:  
  
* Can rename variables while selecting  
	* young_bakers3 %>% select(baker, tech_1 = tre1)  
    * young_bakers3 %>% select(baker, tech_ = tre1:tre3)  
    * young_bakers3 %>% select(baker, tech_ = starts_with("tr"), result_ = starts_with("rs"))  
* Within the rename call, it is not possible to use the helper functions  
	* young_bakers3 %>% rename(tech_1 = t_first, result_1 = r_first)  # new = old  
    * young_bakers3 %>% select(everything(), tech_ = starts_with("tr"), result_ = starts_with("rs"))  # everything first keeps all the column orders the same  
* Can also use the janitor package to help with cleaning variables  
	* young_bakers3 %>% janitor::clean_names()  
    * Converts to snake case (lower case with underscores)  
  
Example code includes:  
```{r}

# NOTE THAT THIS WILL THROW WARNINGS
# Try to cast technical as a number
desserts <- readr::read_csv("./RInputFiles/desserts.csv",
                      col_types = cols(
                        technical = col_number())
                     )

# View parsing problems
readr::problems(desserts)

# NOTE THAT THIS WILL FIX THE ERRORS
# Edit code to fix the parsing error 
desserts <- readr::read_csv("./RInputFiles/desserts.csv",
                      col_types = cols(
                        technical = col_number()),
                      na = c("", "NA", "N/A") 
                     )

# View parsing problems
readr::problems(desserts)


# Find format to parse uk_airdate 
readr::parse_date("17 August 2010", format = "%d %B %Y")

# Edit to cast uk_airdate
desserts <- readr::read_csv("./RInputFiles/desserts.csv", 
                     na = c("", "NA", "N/A"),
                     col_types = cols(
                       technical = col_number(),
                       uk_airdate = col_date("%d %B %Y")
                     ))

# Print by descending uk_airdate
desserts %>%
  arrange(desc(uk_airdate))


# Cast result a factor
desserts <- readr::read_csv("./RInputFiles/desserts.csv", 
                     na = c("", "NA", "N/A"),
                     col_types = cols(
                       technical = col_number(),
                       uk_airdate = col_date(format = "%d %B %Y"),
                       result = col_factor(levels=NULL)
                     ))
                    
# Glimpse to view
glimpse(desserts)


oldDesserts <- desserts
tempDesserts <- desserts %>%
    gather(key="type_ing", value="status", starts_with(c("showstopper")), starts_with(c("signature"))) %>%
    separate(type_ing, into=c("challenge", "ingredient"), sep="_") %>%
    spread(ingredient, status)
glimpse(tempDesserts)
desserts <- tempDesserts


# Count rows grouping by nut variable
desserts %>%
  count(nut, sort=TRUE)

# Recode filberts as hazelnuts
desserts <- desserts %>% 
  mutate(nut = recode(nut, "filbert" = "hazelnut"))

# Count rows again 
desserts %>% 
    count(nut, sort = TRUE)

# Edit code to recode "no nut" as missing
desserts <- desserts %>% 
  mutate(nut = recode(nut, "filbert" = "hazelnut", 
                           "no nut" = NA_character_))

# Count rows again 
desserts %>% 
    count(nut, sort = TRUE)


# Edit to recode tech_win as factor
desserts <- desserts %>% 
  mutate(tech_win = recode_factor(technical, `1` = 1,
                           .default = 0))

# Count to compare values                      
desserts %>% 
  count(technical == 1, tech_win)


ratings0 <- readr::read_csv("./RInputFiles/02.03_messy_ratings.csv")
str(ratings0, give.attr=FALSE)

ratings <- ratings0 %>%
    filter(series >= 3) %>%
    rename(day=day_of_week) %>%
    mutate(series=factor(series), 
           season_premiere=lubridate::mdy(season_premiere), 
           season_finale=lubridate::mdy(season_finale), 
           viewer_growth = (e10_viewers_7day - e1_viewers_7day)
           ) %>%
    select(-contains("uk_airdate"))


# Recode channel as dummy: bbc (1) or not (0)
ratings <- ratings %>% 
  mutate(bbc = recode_factor(channel, "Channel 4"=0, .default=1))

# Look at the variables to plot next
ratings %>% select(series, channel, bbc, viewer_growth)

# Make a filled bar chart
ggplot(ratings, aes(x = series, y = viewer_growth, fill = bbc)) +
  geom_col()


# Move channel to first column
ratings %>% 
  select(channel, everything())

# Edit to drop 7- and 28-day episode viewer data
ratings %>% 
  select(-ends_with("day"))

# Edit to move channel to first and drop episode viewer data
ratings %>% 
  select(-ends_with("day")) %>%
  select(channel, everything())


# Glimpse messy names
# glimpse(messy_ratings)

# Reformat to lower camelcase
# ratings <- messy_ratings %>%
#   clean_names(case="lower_camel")
    
# Glimpse cleaned names
# glimpse(ratings)

# Reformat to snake case
# ratings <- messy_ratings %>% 
#     clean_names("snake")

# Glimpse cleaned names
# glimpse(ratings)


# Select 7-day viewer data by series
viewers_7day <- ratings %>%
  select(series, contains("7day"))

# Glimpse
glimpse(viewers_7day)

# Adapt code to also rename 7-day viewer data
viewers_7day <- ratings %>% 
    select(series, viewers_7day_ = ends_with("7day"))

# Glimpse
glimpse(viewers_7day)


# Adapt code to drop 28-day columns; move 7-day to front
viewers_7day <- ratings %>% 
    select(viewers_7day_ = ends_with("7day"), everything(), -contains("28day"))

# Glimpse
glimpse(viewers_7day)


# Adapt code to keep original order
viewers_7day <- ratings %>% 
    select(everything(), -ends_with("28day"), viewers_7day_ = ends_with("7day"))

# Glimpse
glimpse(viewers_7day)

```
  
  
  
***
  
Chapter 3 - Tidy Your Data  
  
Introduction to Tidy Data:  
  
* Tidy data helps with producing good plots - allows for faceting and the like  
* Data can be tidy but not tame, and can be tame but not tidy  
	* In general, tidy data is long rather than wide  
    * As a result, tidy data tends to take up more space, but with the advantage of being easier to plot or analyze  
* Can automatically get counts summed to a specific level  
	* juniors_tidy %>% count(baker, wt = correct)  # variable wt will be the sum of correct  
    * ggplot(juniors_tidy, aes(baker, correct)) + geom_col()  # roughly the equivalent if plotting the data  
  
Gather:  
  
* Gathering is the process of converting data from wide to long  
	* gather(data, key, value, …)  
    * key is the new column containing the variable  
    * value is the new column contining the value  
    * The … are the columns to be gathered, with column name going to the key column and associated values going to the value column  
    * The key and value need to be quoted while the … can be passed bare (unquoted)  
  
Separate:  
  
* Sometimes, a column really contains two variable, for example when there is spice_trail or the like  
* The separate function requires at least three arguments  
	* data - the data frame  
    * col - the column that you want to separate (can be a bare variable name since it already exists in the data frame)  
    * into - quoted variables to be created, inside the c() function  
    * By default, the existing column col is replaced  
    * There is also an option for convert=TRUE where it will try to pick the best variable type (especially helpful when creating numbers)  
    * There is also the option for sep, where the defaults for separators can be over-ridden to better match the data  
  
Spread:  
  
* The spread function is designed to convert long data to wide data  
	* Spread can be considered a tool to tidy messy rows, where gather is a tool to tidy messy columns  
    * data - the data frame  
    * key - the key is the column that currently contains what should become the new columns  
    * value - value is the column that currently contains what should become the values in the new columns  
    * convert=TRUE will help with re-casting variable types (particularly helpful when numbers are being pulled out of a mixed character-number column (likely what drove the need to spread)  
  
Tidy multiple sets of data:  
  
* Sometimes, there are multiple data components to tidy, where the columns need to be fixed in several ways  
	* For example, score_1, guess_1, score_2, guess_2  
    * Ideal target would be to have trials (1, 2, 3) in one column, and with columns score and guess containing the variables  
* Example code for converting multiple columns simultaneously  
	* juniors_multi %>% gather(key = "key", value = "value", score_1:guess_3) %>% separate(key, into = c("var", "order"), convert = TRUE)  
    * juniors_multi %>% gather(key = "key", value = "value", score_1:guess_3) %>% separate(key, into = c("var", "order"), convert = TRUE) %>% spread(var, value)  
  
Example code includes:  
```{r}

ratings1 <- readr::read_csv("./RInputFiles/messy_ratings.csv")
oldRatings <- ratings
ratings <- ratings1
ratings1

# Plot of episode 1 viewers by series
ratings %>%
  ggplot(aes(x=series, y=e1)) + 
  geom_bar(stat="identity")
  
# Adapt code to plot episode 2 viewers by series
ggplot(ratings, aes(x = series, y = e2)) +
    geom_col() 


# Gather and count episodes
tidy_ratings <- ratings %>%
    gather(key = "episode", value = "viewers_7day", -series, 
           factor_key = TRUE, na.rm = TRUE) %>% 
    arrange(series, episode) %>% 
    mutate(episode_count = row_number())

# Plot viewers by episode and series
ggplot(tidy_ratings, aes(x = episode_count, y = viewers_7day, fill = as.factor(series))) +
    geom_col()


ratings2 <- readr::read_csv("./RInputFiles/messy_ratings2.csv")
ratings2$series <- as.factor(ratings2$series)
ratings2

# Gather 7-day viewers by episode (ratings2 already loaded)
week_ratings <- ratings2  %>% 
    select(series, ends_with("7day")) %>% 
    gather(episode, viewers_7day, ends_with("7day"), na.rm = TRUE, factor_key = TRUE)
    
# Plot 7-day viewers by episode and series
ggplot(week_ratings, aes(x = episode, y = viewers_7day, group = series)) +
    geom_line() +
    facet_wrap(~series)


# Edit to parse episode number
week_ratings <- ratings2 %>% 
    select(series, ends_with("7day")) %>% 
    gather(episode, viewers_7day, ends_with("7day"), na.rm = TRUE) %>% 
    separate(episode, into = "episode", extra = "drop") %>% 
    mutate(episode = parse_number(episode))
    
# Edit your code to color by series and add a theme
ggplot(week_ratings, aes(x = episode, y = viewers_7day, 
                         group = series, color = series)) +
    geom_line() +
    facet_wrap(~series) +
    guides(color = FALSE) +
    theme_minimal() 


week_ratings_dec <- week_ratings %>%
    mutate(viewers_7day=as.character(viewers_7day)) %>%
    separate(viewers_7day, into=c("viewers_millions", "viewers_decimal"), sep="\\.") %>%
    mutate(viewers_decimal=ifelse(is.na(viewers_decimal), ".", paste0(".", viewers_decimal))) %>%
    dplyr::arrange(series, episode)

# Unite series and episode
ratings3 <- week_ratings_dec %>% 
    unite("viewers_7day", viewers_millions, viewers_decimal)

# Print to view
ratings3


# Adapt to change the separator
ratings3 <- week_ratings_dec  %>% 
    unite(viewers_7day, viewers_millions, viewers_decimal, sep="")

# Print to view
ratings3


# Adapt to cast viewers as a number
ratings3 <- week_ratings_dec  %>% 
    unite(viewers_7day, viewers_millions, viewers_decimal, sep="") %>%
    mutate(viewers_7day = parse_number(viewers_7day))

# Print to view
ratings3


# Create tidy data with 7- and 28-day viewers
tidy_ratings_all <- ratings2 %>%
    gather(episode, viewers, ends_with("day"), na.rm = TRUE) %>% 
    separate(episode, into = c("episode", "days")) %>%  
    mutate(episode = parse_number(episode),
           days = parse_number(days)) 

# Adapt to spread counted values
tidy_ratings_all %>% 
    count(series, days, wt = viewers) %>%
    spread(key=days, value=n, sep="_")

# Fill in blanks to get premiere/finale data
tidy_ratings <- ratings %>%
    gather(episode, viewers, -series, na.rm = TRUE) %>%
    mutate(episode = parse_number(episode)) %>% 
    group_by(series) %>% 
    filter(episode == 1 | episode == max(episode)) %>% 
    ungroup()


# Recode first/last episodes
first_last <- tidy_ratings %>% 
  mutate(episode = recode(episode, `1` = "first", .default = "last")) 

# Fill in to make slope chart
ggplot(first_last, aes(x = episode, y = viewers, color = as.factor(series))) +
  geom_point() +
  geom_line(aes(group = series))

# Switch the variables mapping x-axis and color
ggplot(first_last, aes(x = series, y = viewers, color = episode)) +
  geom_point() + # keep
  geom_line(aes(group = series)) + # keep
  coord_flip() # keep

# Calculate relative increase in viewers
bump_by_series <- first_last %>% 
  spread(episode, viewers) %>%   
  mutate(bump = (last - first) / first)
  
# Fill in to make bar chart of bumps by series
ggplot(bump_by_series, aes(x = series, y = bump)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) # converts to %

```
  
  
  
***
  
Chapter 4 - Transform Your Data  
  
Complex recoding with case_when:  
  
* The case_when function allow for vectoizing multiple if-else-then statements  
	* The LHS must give (or be) a boolean  
    * The default value for else is NA  
* Example using ages of the baker data  
	* bakers %>% mutate(gen = if_else(between(birth_year, 1981, 1996), "millenial", "not millenial"))  # simple if statement (boundaries of between are inclusive)  
    * bakers %>% mutate(gen = case_when( between(birth_year, 1965, 1980) ~ "gen_x", between(birth_year, 1981, 1996) ~ "millenial" ))  # logical ~ result  
    * bakers %>% mutate(gen = case_when( between(birth_year, 1928, 1945) ~ "silent", between(birth_year, 1946, 1964) ~ "boomer", between(birth_year, 1965, 1980) ~ "gen_x", between(birth_year, 1981, 1996) ~ "millenial", TRUE ~ "gen_z" ))  
    * bakers %>% count(gen, sort = TRUE) %>% mutate(prop = n / sum(n))  
  
Factors:  
  
* The forcats package is made specifically for working with factors - all functions start with fct_  
* Converting to factors helps ensure the proper ordering of the data  
	* ggplot(bakers, aes(x = fct_rev(fct_infreq(gen)))) + geom_bar()  # reverse by infrequency (build from small to large)  # on-the-fly conversions inside ggplot  
    * bakers <- bakers %>% mutate(gen = fct_relevel(gen, "silent", "boomer", "gen_x", "millenial", "gen_z"))  # conversions of the raw dataset  
    * bakers %>% dplyr::pull(gen) %>% levels()  # check that this worked  
    * ggplot(bakers, aes(x = gen)) + geom_bar()  # will now be plotted in the desired order  
* Need to be careful of the proper treatment of factors  
	* ggplot(bakers, aes(x = gen, fill = series_winner)) + geom_bar()  # FAIL  
    * bakers <- bakers %>% mutate(series_winner = as.factor(series_winner))  
    * ggplot(bakers, aes(x = gen, fill = series_winner)) + geom_bar()  # WORKS  
    * ggplot(bakers, aes(x = gen, fill = as.factor(series_winner))) + geom_bar()  # ALSO WORKS  
  
Dates:  
  
* Can use lubridate for convenience functions such as ymd() or dmy(), with the output being ISO (YYYY-MM-DD)  
	* Can also include a vector of suspected dates  
    * dmy("17 August 2010")  # will work  
    * hosts <- tibble::tribble( ~host, ~bday, ~premiere, "Mary", "24 March 1935", "August 17th, 2010", "Paul", "1 March 1966", "August 17th, 2010")  
    * hosts <- hosts %>% mutate(bday = dmy(bday), premiere = mdy(premiere))  
* There are three aspects of timespans  
	* interval - time span bound by two real dates  
    * duration - exact number of seconds in an interval  
    * period - change in clock time of an interval  
    * hosts <- hosts %>% mutate(age_int = interval(bday, premiere))  # new variable age_int will be of type interval  
    * hosts %>% mutate(years_decimal = age_int / years(1), years_whole = age_int %/% years(1))  # years(1) is one year, so this is fractional and whole (floored) years  
  
Strings:  
  
* The separate function splits one column in to 2+ columns (for example "age, job" could become "age" and "job")  
	* series5 <- series5 %>% separate(about, into = c("age", "occupation"), sep = ", ")  
    * series5 <- series5 %>% separate(about, into = c("age", "occupation"), sep = ", ") %>% mutate(age = parse_number(age))  # numeric age. Dropping years  
* The stringr package makes working with strings in R easier (typically used within a mutate) - all functions start with str_  
	* series5 <- series5 %>% mutate(baker = str_to_upper(baker), showstopper = str_to_lower(showstopper))  
    * series5 %>% mutate(pie = str_detect(showstopper, "pie"))  # returns a boolean  
    * series5 %>% mutate(showstopper = str_replace(showstopper, "pie", "tart"))  # find and replace for strings  
    * series5 %>% mutate(showstopper = str_remove(showstopper, "pie"))  # remove "pie", though there may be trailing whitespace  
    * series5 %>% mutate(showstopper = str_remove(showstopper, "pie"), showstopper = str_trim(showstopper))  # trim whitespace at the beginning or end  
  
Final thoughts:  
  
* R using the tidyverse for analysis and presentation  
* Reading data using readr and analyzing using dplyr and ggplot2  
* Taming variable types, names, and values  
* Transforming data using stringr and lubridate  
* The "here" package can make working with file paths much easier  
  
Example code includes:  
```{r}

baker_results <- readr::read_csv("./RInputFiles/baker_results.csv")
messy_baker_results <- readr::read_csv("./RInputFiles/messy_baker_results.csv")
bakers <- baker_results
glimpse(bakers)


# Create skill variable with 3 levels
bakers <- bakers %>% 
  mutate(skill = case_when(
    star_baker > technical_winner ~ "super_star",
    star_baker < technical_winner ~ "high_tech",
    TRUE ~ "well_rounded"
  ))
  
# Filter zeroes to examine skill variable
bakers %>% 
  filter(star_baker==0 & technical_winner==0) %>% 
  count(skill)


# Add pipe to drop skill = NA
bakers_skill <- bakers %>% 
  mutate(skill = case_when(
    star_baker > technical_winner ~ "super_star",
    star_baker < technical_winner ~ "high_tech",
    star_baker == 0 & technical_winner == 0 ~ NA_character_,
    star_baker == technical_winner  ~ "well_rounded"
  )) %>% 
  drop_na(skill)
  
# Count bakers by skill
bakers_skill %>%
  count(skill)


# Cast skill as a factor
bakers <- bakers %>% 
  mutate(skill = as.factor(skill))

# Examine levels
bakers %>%
  pull(skill) %>%
  levels()


baker_dates <- bakers %>%
    select(series, baker, contains("date")) %>%
    mutate(last_date_appeared_us=as.character(last_date_us), 
           first_date_appeared_us=as.character(first_date_us)
           ) %>%
    rename(first_date_appeared_uk=first_date_appeared, last_date_appeared_uk=last_date_appeared) %>%
    select(-last_date_us, -first_date_us)
glimpse(baker_dates)


# Add a line to extract labeled month
baker_dates <- baker_dates %>% 
  mutate(last_date_appeared_us=lubridate::ymd(last_date_appeared_us), 
         last_month_us=lubridate::month(last_date_appeared_us, label=TRUE)
         )
         
ggplot(baker_dates, aes(x=last_month_us)) + geom_bar()


baker_time <- baker_dates %>%
    mutate(first_date_appeared_us=lubridate::ymd(first_date_appeared_us)) %>%
    select(-last_month_us)
glimpse(baker_time)

           
# Add a line to create whole months on air variable
baker_time <- baker_time  %>% 
  mutate(time_on_air = lubridate::interval(first_date_appeared_uk, last_date_appeared_uk),
         weeks_on_air = time_on_air / lubridate::weeks(1), 
         months_on_air = time_on_air %/% months(1)
         )

# Count rows
messy_baker_results %>% 
  count(position_reached)


# Add another mutate to replace "THIRD PLACE" with "RUNNER UP"and count
messy_baker_results <- messy_baker_results %>% 
  mutate(position_reached = str_to_upper(position_reached),
         position_reached = str_replace(position_reached, "-", " "), 
         position_reached = str_replace(position_reached, "THIRD PLACE", "RUNNER UP"))

# Count rows
messy_baker_results %>% 
  count(position_reached)


# Add a line to create new variable called student
bakers <- bakers %>% 
    mutate(occupation = str_to_lower(occupation), 
           student=str_detect(occupation, "student")
           )

# Find all students and examine occupations
bakers %>% 
  filter(student) %>%
  select(baker, occupation, student)

```
  
  
  
***
  
### _Modeling Data in the Tidyverse_  
  
Chapter 1 - Introduction to Modeling  
  
Background on modeling for explanation:  
  
* Generally, the model has y as a function of x plus epsilon, where y is the outcome of interest and x is a set of explanatory variables and epsilon is irreducible error  
	* The x can be either explanatory or predictive - depends on the purpose of the analysis  
* Example of explanation - can differences in teacher evaluation scores be explained by teacher attributes  
	* library(dplyr)  
    * library(moderndive)  
    * glimpse(evals)  # evals data is available in the moderndivw package (From the moderndive package for ModernDive.com:)  
    * ggplot(evals, aes(x = score)) + geom_histogram(binwidth = 0.25) + labs(x = "teaching score", y = "count")  # EDA on scores using histogram  
    * evals %>% summarize(mean_score = mean(score), median_score = median(score), sd_score = sd(score))  # summary statistics using dplyr::summarize  
  
Background on modeling for prediction:  
  
* House sales in King County USA in 2014-2015 (from Kaggle) based on features such as size, bedrooms, etc.  
	* glimpse(house_prices)  
    * ggplot(house_prices, aes(x = price)) + geom_histogram() + labs(x = "house price", y = "count")  
    * house_prices <- house_prices %>% mutate(log10_price = log10(price))  
    * house_prices %>% select(price, log10_price)  
    * ggplot(house_prices, aes(x = log10_price)) + geom_histogram() + labs(x = "log10 house price", y = "count")  # after transformation  
  
Modeling problem for explanation:  
  
* Typically, both the function that relates x and y and the function that generates the errors is unknwon  
	* Goal is to create a model that can generate y-hat by separating signal from noise  
* Can start by considering linear models, assessed as a starting point by examining a scatter plot  
	* ggplot(evals, aes(x = age, y = score)) + geom_point() + labs(x = "age", y = "score", title = "Teaching score over age")  
    * ggplot(evals, aes(x = age, y = score)) + geom_jitter() + labs(x = "age", y = "score", title = "Teaching score over age (jittered)")  
* Can further explore the data by looking at correlations among some or all of the potential explanatory variables  
	* evals %>% summarize(correlation = cor(score, age))  
  
Modeling problem for prediction:  
  
* For explanation, we care about the form of the function  
* For prediction, we care mainly that the function makes good predictions (even if it may not be easy to explain)  
	* house_prices %>% select(log10_price, condition) %>% glimpse()  # condition is a categorical variable saved as a factor  
    * ggplot(house_prices, aes(x = condition, y = log10_price)) + geom_boxplot() + labs(x = "house condition", y = "log10 price", title = "log10 house price over condition")  
* Means tend to be at the center of the linear modeling process  
	* house_prices %>% group_by(condition) %>% summarize(mean = mean(log10_price), sd = sd(log10_price), n = n())  
  
Example code includes:  
```{r}

data(evals, package="moderndive")
glimpse(evals)


# Plot the histogram
ggplot(evals, aes(x = age)) +
  geom_histogram(binwidth = 5) +
  labs(x = "age", y = "count")

# Compute summary stats
evals %>%
  summarize(mean_age = mean(age),
            median_age = median(age),
            sd_age = sd(age))


data(house_prices, package="moderndive")
glimpse(house_prices)


# Plot the histogram
ggplot(house_prices, aes(x = sqft_living)) +
  geom_histogram() +
  labs(x="Size (sq.feet)", y="count")

# Add log10_sqft_living
house_prices_2 <- house_prices %>%
  mutate(log10_sqft_living = log10(sqft_living))

# Plot the histogram  
ggplot(house_prices_2, aes(x = log10_sqft_living)) +
  geom_histogram() +
  labs(x = "log10 size", y = "count")


# Plot the histogram
ggplot(evals, aes(x=bty_avg)) +
  geom_histogram(binwidth=0.5) +
  labs(x = "Beauty score", y = "count")

# Scatterplot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "teaching score")

# Jitter plot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter() +
  labs(x = "beauty score", y = "teaching score")


# Compute correlation
evals %>%
  summarize(correlation = cor(score, bty_avg))


house_prices <- house_prices %>%
    mutate(log10_price=log10(price))

# View the structure of log10_price and waterfront
house_prices %>%
  select(log10_price, waterfront) %>%
  glimpse()

# Plot 
ggplot(house_prices, aes(x = waterfront, y = log10_price)) +
  geom_boxplot() +
  labs(x = "waterfront", y = "log10 price")


# Calculate stats
house_prices %>%
  group_by(waterfront) %>%
  summarize(mean_log10_price = mean(log10_price), n = n())
  
# Prediction of price for houses with view
10^(6.12)

# Prediction of price for houses without view
10^(5.66)

```
  
  
  
***
  
Chapter 2 - Modeling with Regression  
  
Explaining teaching score with age:  
  
* Can overlay a regression line to the scatter plot for a bivariate relationship  
	* ggplot(evals, aes(x = age, y = score)) + geom_point() + labs(x = "age", y = "score", title = "Teaching score over age") + geom_smooth(method = "lm", se = FALSE)  
* In simple linear regression, the assumption is that f(x) is B0 + B1*x  
    * The fitted model f-hat does not have an error term, since it is just the model prediction for a given value of x  
    * model_score_1 <- lm(score ~ age, data = evals)  
    * moderndive::get_regression_table(model_score_1)  
  
Predicting teaching score using age:  
  
* Can make predictions based on the existing regression line - f-hat can be used for both explanatory and predictive purposes  
* The residuals are the errors (predictive vs. actual values), and correspond to the epsilon of the general modeling framework  
    * On average, for linear regression, the residuals should average out to zero  
    * get_regression_points(model_score_1)  # gives y, x, y-hat, and residuals  
  
Explaining teaching score with gender:  
  
* Can extend the models to include categorical data, such as gender  
	* ggplot(evals, aes(x = score)) + geom_histogram(binwidth = 0.25) + facet_wrap(~gender) + labs(x = "score", y = "count")  
    * model_score_3 <- lm(score ~ gender, data = evals)  # will just give an overall mean and a change in mean vs. the first-level factor  
* Can also look at multi-level factors, such as rank (teacher type)  
	* evals %>% group_by(rank) %>% summarize(n = n())  
  
Predicting teaching score with gender:  
  
* Can use group means as part of the predictive approach - if only factor are used in the regression, there will be the same prediction for everyone who is in the same class(es)  
	* model_score_3_points <- get_regression_points(model_score_3)  
  
Example code includes:  
```{r}

# Plot 
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "score") +
  geom_smooth(method = "lm", se = FALSE)

# Fit model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Output content
model_score_2

# Output regression table
moderndive::get_regression_table(model_score_2)

# Use fitted intercept and slope to get a prediction
y_hat <- 3.88 + 0.067 * 5
y_hat

# Compute residual y - y_hat
4.7 - y_hat


# Get regression table
moderndive::get_regression_table(model_score_2, digits = 5)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2) %>% 
  mutate(score_hat_2 = 3.88 + 0.0666 * bty_avg)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2) %>% 
  mutate(residual_2 = score - score_hat)


ggplot(evals, aes(x=rank, y=score)) +
  geom_boxplot() +
  labs(x = "rank", y = "score")

evals %>%
  group_by(rank) %>%
  summarize(n = n(), mean_score = mean(score), sd_score = sd(score))


# Fit regression model
model_score_4 <- lm(score ~ rank, data = evals)

# Get regression table
moderndive::get_regression_table(model_score_4, digits = 5)

# teaching mean
teaching_mean <- 4.28

# tenure track mean
tenure_track_mean <- 4.28-0.13 

# tenure mean
tenure_mean <- 4.28-0.145


# Calculate predictions and residuals
model_score_4_points <- moderndive::get_regression_points(model_score_4)
model_score_4_points

# Plot residuals
ggplot(model_score_4_points, aes(x=residual)) +
  geom_histogram() +
  labs(x = "residuals", title = "Residuals from score ~ rank model")

```
  
  
  
***
  
Chapter 3 - Modeling with Multiple Regression  
  
Explaining house price with year and size:  
  
* Can incorporate 2+ explanatory / predictive variable using multiple regression  
	* house_prices %>% select(price, sqft_living, condition, waterfront) %>% glimpse()  
    * The log-10 transformation is helpful for this specific dataset (assume the code below for future examples in this course)  
    * house_prices <- house_prices %>% mutate( log10_price = log10(price), log10_sqft_living = log10(sqft_living) )  
* Exploring the relationship between mutliple variables - EDA and regression  
	* Can create a 3D plot with associated regression plane using plotly  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * get_regression_table(model_price_1, digits = 5))  
  
Predicting house price using year and size:  
  
* Can get the fitted values and exponentiate as needed, assessing the overall fit or lack thereof (sum-squared residuals) of the model  
	* get_regression_points(model_price_1, digits = 5)  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(squared_residuals))  # SSR  
  
Explaining house price with size and condition:  
  
* The EDA from previous chapters is repeated  
	* house_prices <- house_prices %>% mutate( log10_price = log10(price), log10_sqft_living = log10(sqft_living) )  
    * house_prices %>% group_by(condition) %>% summarize(mean = mean(log10_price), sd = sd(log10_price), n = n())  
* The parallel slopes model is lines where the slopes are the same but they have a different intercept (likely, coefficients of a categorical variable)  
	* model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_table(model_price_3, digits = 5)  
  
Predicting house price using size and condition:  
  
* Objective is to predict on new data (as opposed to checking our predictions on data where we already had the answer)  
	* model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_table(model_price_3)  
* Automating the housing price prediction process  
	* new_houses <- read_csv("new_houses.csv")  
    * new_houses  
    * get_regression_points(model_price_3, newdata = new_houses)  # moderndata form of predict() function  
    * get_regression_points(model_price_3, newdata = new_houses) %>% mutate(price_hat = 10^log10_price_hat)  
  
Example code includes:  
```{r}

# Create scatterplot with regression line
ggplot(house_prices, aes(x=bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)

# Remove outlier
house_prices_transform <- house_prices %>%
    filter(bedrooms < 33) %>%
    mutate(log10_sqft_living=log10(sqft_living))

# Create scatterplot with regression line
ggplot(house_prices_transform, aes(x = bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)


# Fit model
model_price_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data = house_prices_transform)

# Get regression table
moderndive::get_regression_table(model_price_2)

# Make prediction in log10 dollars
2.69 + 0.941 * log10(1000) - 0.033 * 3

# Make prediction dollars
10**(2.69 + 0.941 * log10(1000) - 0.033 * 3)

# Automate prediction and residual computation
moderndive::get_regression_points(model_price_2) %>%
    mutate(squared_residuals = residual**2) %>%
    summarize(sum_squared_residuals = sum(squared_residuals))


# Fit model
model_price_4 <- lm(log10_price ~ log10_sqft_living + waterfront, data = house_prices_transform)

# Get regression table
moderndive::get_regression_table(model_price_4)

# Prediction for House A
10**(2.96 + 0.825*2.9 + 0.322)

# Prediction for House B
10**(2.96 + 0.825*3.1 + 0)


# View the "new" houses
new_houses_2 <- tibble(log10_sqft_living=c(2.9, 3.1), waterfront=c(TRUE, FALSE))
new_houses_2

# Get predictions price_hat in dollars on "new" houses
moderndive::get_regression_points(model_price_4, newdata = new_houses_2) %>% 
  mutate(price_hat = 10**log10_price_hat)

```
  
  
  
***
  
Chapter 4 - Model Selection and Assessment  
  
Model selection and assessment:  
  
* Can use multiple models for the same data and compare  
	* model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
    * get_regression_points(model_price_3) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
  
Assessing model fit with R-squared:  
  
* The R-squared is a reasonable measure of model fit - R-squared = 1 - Var(Residuals) / Var(Y)  
	* Larger R-squared is suggestive of better fit, with values (typically) constrained between 0 and 1  
    * R-squared is the proportion of variation in the outcome model that can be explained using the model  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * get_regression_points(model_price_1) %>% summarize(r_squared = 1 - var(residual) / var(log10_price))  
    * model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_points(model_price_3) %>% summarize(r_squared = 1 - var(residual) / var(log10_price))  
  
Assessing predictions with RMSE:  
  
* RMSE (Root Mean Squared Error) is a slight variation on RSS  
	* Where RSS is the sum-squared of the residuals, RMSE is the square root of the average of the residuals-squared  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)   
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals))  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals)) %>% mutate(rmse = sqrt(mse))  
* Cannot calculate RMSE on new data - predictions means that we do not know the actual values  
	* get_regression_points(model_price_3, newdata = new_houses) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals)) %>% mutate(rmse = sqrt(mse))  
    * The above code will crash out, since the residuals do not exist  
  
Validation set prediction framework:  
  
* Use two different datasets for modeling; a training set used for modeling, and a test set used for assessing likely out-of-sample errors  
	* house_prices_shuffled <- house_prices %>% sample_frac(size = 1, replace = FALSE)  # Randomly shuffle order of rows  
    * train <- house_prices_shuffled %>% slice(1:10000)  
    * test <- house_prices_shuffled %>% slice(10001:21613)  
    * train_model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = train)  
* After having trained the model on the train data, can assess the fit using the test data  
	* get_regression_points(train_model_price_1, newdata = test)  
    * get_regression_points(train_model_price_1, newdata = test) %>% mutate(sq_residuals = residual^2) %>% summarize(rmse = sqrt(mean(sq_residuals)))  
  
Next steps:  
  
* Tidyverse ties together many of the packages that help with data wrangling and analysis  
* Can extend regressions to areas like polynomials and trees  
* "ModernDive" is a textbook on the tidyverse tools  
  
Example code includes:  
```{r}

# Model 2
model_price_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data = house_prices_transform)

# Calculate squared residuals
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals=residual**2) %>% 
    summarize(sum_sq_residuals=sum(sq_residuals))

# Model 4
model_price_4 <- lm(log10_price ~ log10_sqft_living + waterfront, data = house_prices_transform)

# Calculate squared residuals
moderndive::get_regression_points(model_price_4) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(sum_sq_residuals=sum(sq_residuals))


# Get fitted/values & residuals, compute R^2 using residuals
moderndive::get_regression_points(model_price_2) %>% 
    summarize(r_squared = 1 - var(residual) / var(log10_price))

# Get fitted/values & residuals, compute R^2 using residuals
moderndive::get_regression_points(model_price_4) %>% 
    summarize(r_squared = 1 - var(residual) / var(log10_price))


# Get all residuals, square them, take the mean and square root
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals)) %>% 
    mutate(rmse = sqrt(mse))

# MSE and RMSE for model_price_2
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))

# MSE and RMSE for model_price_4
moderndive::get_regression_points(model_price_4) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))


# Set random number generator seed value for reproducibility
set.seed(76)

# Randomly reorder the rows
house_prices_shuffled <- house_prices_transform %>% 
    sample_frac(size = 1, replace = FALSE)

# Train/test split
train <- house_prices_shuffled %>% 
    slice(1:10000)
test <- house_prices_shuffled %>% 
    slice(10001:nrow(.))

# Fit model to training set
train_model_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data=train)


# Compute RMSE (train)
moderndive::get_regression_points(train_model_2) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(rmse = sqrt(mean(sq_residuals)))

# Compute RMSE (test)
moderndive::get_regression_points(train_model_2, newdata = test) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(rmse = sqrt(mean(sq_residuals)))

```
  
  
  
***
  
### _Analyzing Survey Data in R_  
  
Chapter 1 - Introduction to Survey Data  

What are survey weights?  
  
* Survey weights sometimes appear inside a dataset, to reflect potential over/under sampling  
	* Survey weights result from a complex survey design - number of points in the population represented by each entry in the sampling frame  
    * For example, average income would be the sum-product of weights and incomes divided by the sum of weights  
  
Specifying elements of the design in R:  
  
* Simple random sampling is when every member of the population is known and had an equal chance of being selected  
	* library(survey)  
    * srs_design <- svydesign(data = paSample, weights = ~wts, fpc=~N, id=~1)  # the ~ means that these are column names  
* Stratified sampling is when a simple random sample is taken from each of the strata (sub-units)  
	* For example, taking 100 people from every county in a state, so that county-level averages can be gathered  
    * stratified_design <- svydesign(data = paSample, id = ~1, weights = ~wts, strata = ~county, fpc = ~N)  
* Cluster sampling is when the population are grouped in to clusters, with a simple random sample of clusters selected, and with simple random samples taken within each selected cluster  
	* cluster_design <- svydesign(data = paSample, id = ~county + personid, fpc = ~N1 + N2, weights = ~wts) 
  
Visualizing impact of survey weights:  
  
* NHANES data - assessment of health of persons in the US, derived by a health check in a mobile doctor's office  
	* Stage 0 - stratified by geography and proportion minority  
    * Stage 1 - within strata, counties randomly selected (selection likelihood proportional to population)  
    * Stage 2 - within counties, city blocks randomly selected (selection likelihood proportional to population)  
    * Stage 3 - within city blocks, households randomly selected (based on demographics)  
    * Stage 4 - within households, people randomly selected  
* NHANES data are availabl through a package in R  
	* library(NHANES)  
    * dim(NHANESraw)  
    * summarize(NHANESraw, N_hat = sum(WTMEC2YR))  # sums to double the US population, due to having 4 years of data when desiring only 2 years of data  
    * NHANESraw <- mutate(NHANESraw, WTMEC4YR = WTMEC2YR/2)  # fix the double population issue  
    * NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR)  # id is the cluster (first-level), nest=TRUE is due to id being nested within strata (???)  
    * distinct(NHANESraw, SDMVPSU)  # only takes 3 values, since only 1-3 counties are selected  
  
Example code includes:  
```{r}

colTypes <- "FINLWT21 numeric _ FINCBTAX integer _ BLS_URBN integer _ POPSIZE integer _ EDUC_REF character _ EDUCA2 character _ AGE_REF integer _ AGE2 character _ SEX_REF integer _ SEX2 integer _ REF_RACE integer _ RACE2 integer _ HISP_REF integer _ HISP2 integer _ FAM_TYPE integer _ MARITAL1 integer _ REGION integer _ SMSASTAT integer _ HIGH_EDU character _ EHOUSNGC numeric _ TOTEXPCQ numeric _ FOODCQ numeric _ TRANSCQ numeric _ HEALTHCQ numeric _ ENTERTCQ numeric _ EDUCACQ integer _ TOBACCCQ numeric _ STUDFINX character _ IRAX character _ CUTENURE integer _ FAM_SIZE integer _ VEHQ integer _ ROOMSQ character _ INC_HRS1 character _ INC_HRS2 character _ EARNCOMP integer _ NO_EARNR integer _ OCCUCOD1 character _ OCCUCOD2 character _ STATE character _ DIVISION integer _ TOTXEST integer _ CREDFINX character _ CREDITB integer _ CREDITX character _ BUILDING character _ ST_HOUS integer _ INT_PHON character _ INT_HOME character _ "

ce <- readr::read_csv("./RInputFiles/ce.csv")
glimpse(ce)
ceColTypes <- ""
for (x in names(ce)) { ceColTypes <- paste0(ceColTypes, x, " ", class(ce[, x, drop=TRUE]), " _ ") }
all.equal(colTypes, ceColTypes)

# Construct a histogram of the weights
ggplot(data = ce, mapping = aes(x = FINLWT21)) +
    geom_histogram()

# In the next few exercises we will practice specifying sampling designs using different samples from the api dataset, located in the survey package
# The api dataset contains the Academic Performance Index and demographic information for schools in California
# The apisrs dataset is a simple random sample of schools from the api dataset
# Notice that pw contains the survey weights and fpc contains the total number of schools in the population

data(api, package="survey")
library(survey)

# Look at the apisrs dataset
glimpse(apisrs)

# Specify a simple random sampling for apisrs
apisrs_design <- svydesign(data = apisrs, weights = ~pw, fpc = ~fpc, id = ~1)

# Print a summary of the design
summary(apisrs_design)


# Now let's practice specifying a stratified sampling design, using the dataset apistrat
# The schools are stratified based on the school type stype where E = Elementary, M = Middle, and H = High School
# For each school type, a simple random sample of schools was taken

# Glimpse the data
glimpse(apistrat)

# Summarize strata sample sizes
apistrat %>%
  count(stype)

# Specify the design
strat_design <- svydesign(data = apistrat, weights = ~pw, fpc = ~fpc, id = ~1, strata = ~stype)

# Look at the summary information for the stratified design
summary(strat_design)


# Now let's practice specifying a cluster sampling design, using the dataset apiclus2
# The schools were clustered based on school districts, dnum
# Within a sampled school district, 5 schools were randomly selected for the sample
# The schools are denoted by snum
# The number of districts is given by fpc1 and the number of schools in the sampled districts is given by fpc2

# Glimpse the data
glimpse(apiclus2)

# Specify the design
apiclus_design <- svydesign(id = ~dnum + snum, data = apiclus2, weights = ~pw, fpc = ~fpc1 + fpc2)

#Look at the summary information stored for both designs
summary(apiclus_design)


# Construct histogram of pw
ggplot(data = apisrs, mapping = aes(x = pw)) + 
    geom_histogram()

# Construct histogram of pw
ggplot(data = apistrat, mapping = aes(x = pw)) + 
    geom_histogram()

# Construct histogram of pw
ggplot(data = apiclus2, mapping = aes(x = pw)) + 
    geom_histogram()



NHANESraw <- read.csv("./RInputFiles/NHANESraw.txt")
NHANESraw <- NHANESraw %>%
    mutate(WTMEC4YR=WTMEC2YR / 2)
names(NHANESraw)[1] <- "SurveyYr"
glimpse(NHANESraw)

#Create table of average survey weights by race
tab_weights <- NHANESraw %>%
  group_by(Race1) %>%
  summarize(avg_wt = mean(WTMEC4YR))

#Print the table
tab_weights


# The two important design variables in NHANESraw are SDMVSTRA, which contains the strata assignment for each unit, and SDMVPSU, which contains the cluster id within a given stratum
# Specify the NHANES design
NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, 
                           nest = TRUE, weights = ~WTMEC4YR
                           )

# Print summary of design
summary(NHANES_design)

# Number of clusters
NHANESraw %>%
  summarize(n_clusters = n_distinct(SDMVSTRA, SDMVPSU))

# Sample sizes in clusters
NHANESraw %>%
  count(SDMVSTRA, SDMVPSU) 

```
  
  
  
***
  
Chapter 2 - Exploring categorical data  
  
Visualizing categorical variables:  
  
* Can estimate distributions of race, including both the weighted and unweighted distributions  
	* tab_unw <- NHANESraw %>% group_by(Race1) %>% summarize(Freq = n()) %>% mutate(Prop = Freq/sum(Freq)) %>% arrange(desc(Prop))  
    * ggplot(data = tab_unw, mapping = aes(x = Race1, y = Prop)) + geom_col() + coord_flip() + scale_x_discrete(limits = tab_unw$Race1) # Labels layer omitted  
* Can convert back to the weighted frequencies  
	* tab_w <- svytable(~Race1, design = NHANES_design) %>% as.data.frame() %>% mutate(Prop = Freq/sum(Freq)) %>% arrange(desc(Prop))  
    * ggplot(data = tab_w, mapping = aes(x = Race1, y = Prop)) + geom_col() + coord_flip() + scale_x_discrete(limits = tab_w$Race1) # Labels layer omitted  
  
Exploring two categorical variables:  
  
* Can look at diabetes withing the NHANES data, using the syvtable() function  
	* svytable(~Diabetes, design = NHANES_design)  
    * tab_w <- svytable(~Race1 + Diabetes, design = NHANES_design)  # Race and Diabetes  
    * tab_w <- as.data.frame(tab_w)  # converts contingency table to frame  
    * ggplot(data = tab_w, mapping = aes(x = Race1, fill = Diabetes, y = Freq)) + geom_col() + coord_flip()  
    * ggplot(data = tab_w, mapping = aes(x = Race1, y = Freq, fill = Diabetes)) + geom_col(position = "fill") + coord_flip()  # stacked bars to 100%  
  
Inference for categorical variables:  
  
* Formal statistical tests for associations among categorical variables using chi-squared tests for association  
	* svychisq(~Race1 + Diabetes, design = NHANES_design, statistic = "Chisq")  
  
Example code includes:  
```{r}

# Specify the survey design
NHANESraw <- mutate(NHANESraw, WTMEC4YR = .5 * WTMEC2YR)
NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR)

# Determine the levels of Depressed
levels(NHANESraw$Depressed)

# Construct a frequency table of Depressed
tab_w <- svytable(~Depressed, design = NHANES_design)

# Determine class of tab_w
class(tab_w)

# Display tab_w
tab_w


# Add proportions to table
tab_w <- tab_w %>%
  as.data.frame() %>%
  mutate(Prop = Freq/sum(Freq))

# Create a barplot
ggplot(data = tab_w, mapping = aes(x = Depressed, y = Prop)) + 
  geom_col()


# Construct and print a frequency table
tab_D <- svytable(~Depressed, design = NHANES_design)
tab_D

# Construct and print a frequency table
tab_H <- svytable(~HealthGen, design = NHANES_design)
tab_H

# Construct and print a frequency table
tab_DH <- svytable(~Depressed + HealthGen, design = NHANES_design)
tab_DH


# Add conditional proportions to tab_DH
tab_DH_cond <- tab_DH %>%
    as.data.frame() %>%
    group_by(HealthGen) %>%
    mutate(n_HealthGen = sum(Freq), Prop_Depressed = Freq/n_HealthGen) %>%
    ungroup()

# Print tab_DH_cond
tab_DH_cond

# Create a segmented bar graph of the conditional proportions in tab_DH_cond
ggplot(data = tab_DH_cond, mapping = aes(x = HealthGen, y = Prop_Depressed, fill = Depressed)) + 
  geom_col() + 
  coord_flip() 


# We can also estimate counts with svytotal(). The syntax is given by:
# svytotal(x = ~interaction(Var1, Var2), design = design, na.rm = TRUE)
# For each combination of the two variables, we get an estimate of the total and the standard error


# Estimate the totals for combos of Depressed and HealthGen
tab_totals <- svytotal(x = ~interaction(Depressed, HealthGen), design = NHANES_design, na.rm = TRUE)

# Print table of totals
tab_totals

# Estimate the means for combos of Depressed and HealthGen
tab_means <- svymean(x = ~interaction(Depressed, HealthGen), design = NHANES_design, na.rm = TRUE)

# Print table of means
tab_means


# Run a chi square test between Depressed and HealthGen
svychisq(~Depressed + HealthGen, design = NHANES_design, statistic = "Chisq")

# Construct a contingency table
tab <- svytable(~Education + HomeOwn, design=NHANES_design)

# Add conditional proportion of levels of HomeOwn for each educational level
tab_df <- as.data.frame(tab) %>%
  group_by(Education) %>%
  mutate(n_Education = sum(Freq), Prop_HomeOwn = Freq/n_Education) %>%
  ungroup()

# Create a segmented bar graph
ggplot(data = tab_df, mapping = aes(x=Education, y=Prop_HomeOwn, fill=HomeOwn)) + 
  geom_col() + 
  coord_flip()

# Run a chi square test
svychisq(~Education + HomeOwn, 
    design = NHANES_design, 
    statistic = "Chisq")

```
  
  
  
***
  
Chapter 3 - Exploring quantitative data  
  
Summarizing quantitative data:  
  
* Can look at the physician health bad variable and summarize  
	* NHANESraw %>% filter(Age >= 12) %>% select(DaysPhysHlthBad)  # just the data  
    * svymean(x = ~DaysPhysHlthBad, design = NHANES_design, na.rm = TRUE)  # means of number of days feeling in bad heatlh  
    * svyquantile(x = ~DaysPhysHlthBad, design = NHANES_design, na.rm = TRUE, quantiles = 0.5)  # get the median (quantile 0.5) of the data  
* Can grab summaries by group using svyby with a function FUN provided  
	* svyby(formula = ~DaysPhysHlthBad, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, row.names = FALSE)  
    * svyby(formula = ~Age, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE)  
  
Visualizing quantitative data:  
  
* Can create bar graphs of the means  
	* out <- svyby(formula = ~DaysPhysHlthBad, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE)  
    * ggplot(data = out, mapping = aes(x = SmokeNow, y = DaysPhysHlthBad)) + geom_col() + labs(y = "Monthly Average Number\n of Bad Health Days", x = "Smoker?")  
    * out <- mutate(out, lower = DaysPhysHlthBad - se, upper = DaysPhysHlthBad + se)  
* Create histograms of the data  
	* ggplot(data = out, mapping = aes(x = SmokeNow, y = DaysPhysHlthBad, ymin = lower, ymax = upper)) + geom_col(fill = "lightblue") + geom_errorbar(width = .5) + labs(y = "Monthly Average Number\n of Bad Health Days", x = "Smoker?")  
    * ggplot(data = NHANESraw, mapping = aes(x = DaysPhysHlthBad, weight = WTMEC4YR)) + geom_histogram(binwidth = 1, color = "white") + labs(x = "Number of Bad Health Days in a Month")  
* Create density plots of the data  
	* NHANESraw %>% filter(!is.na(DaysPhysHlthBad)) %>% mutate(WTMEC4YR_std = WTMEC4YR/sum(WTMEC4YR)) %>%  
    *     ggplot(mapping = aes(x = DaysPhysHlthBad, weight = WTMEC4YR_std)) + geom_density(bw = .6, fill = "lightblue") + labs(x = "Number of Bad Health Days in a Month")  
  
Inference for quantitative data:  
  
* May want to compare means across two groups in the data using a weighted 2-sample t-test  
	* The test statistic is the difference in means divided by the SE (standard error)  
    * svyttest(formula = DaysPhysHlthBad ~ SmokeNow, design = NHANES_design)  
  
Example code includes:  
```{r}

# Compute the survey-weighted mean
svymean(x = ~SleepHrsNight, design = NHANES_design, na.rm = TRUE)

# Compute the survey-weighted mean by Gender
svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, 
      FUN = svymean, na.rm = TRUE, keep.names = FALSE
      )

# Compute the survey-weighted quantiles
svyquantile(x = ~SleepHrsNight, design = NHANES_design, na.rm = TRUE, 
            quantiles = c(0.01, 0.25, 0.5, 0.75, .99)
            )

# Compute the survey-weighted quantiles by Gender
svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, FUN = svyquantile, 
      na.rm = TRUE, quantiles = c(0.5), keep.rows = FALSE, keep.var = FALSE
      )

# Compute the survey-weighted mean by Gender
out <- svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, 
             FUN = svymean, na.rm = TRUE, keep.names = FALSE
             )
             
# Construct a bar plot of average sleep by gender
ggplot(data = out, mapping = aes(x=as.factor(Gender), y=SleepHrsNight)) + 
    geom_col() + 
    labs(y="Average Nightly Sleep")

# Add lower and upper columns to out
out_col <- mutate(out, lower = SleepHrsNight - 2*se, upper = SleepHrsNight + 2*se)

# Construct a bar plot of average sleep by gender with error bars
ggplot(data = out_col, mapping = aes(x = Gender, y = SleepHrsNight, ymin = lower, ymax = upper)) + 
    geom_col(fill = "gold") + 
    labs(y = "Average Nightly Sleep") + 
    geom_errorbar(width = 0.7)  


# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 1, color = "white") + 
    labs(x = "Hours of Sleep")

# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 0.5, color = "white") + 
    labs(x = "Hours of Sleep")

# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 2, color = "white") + 
    labs(x = "Hours of Sleep")


# Density plot of sleep faceted by gender
NHANESraw %>% 
    filter(!is.na(SleepHrsNight), !is.na(Gender)) %>%
    group_by(Gender) %>%
    mutate(WTMEC4YR_std = WTMEC4YR/sum(WTMEC4YR)) %>%
    ggplot(mapping = aes(x = SleepHrsNight, weight = WTMEC4YR_std)) + 
        geom_density(bw = 0.6,  fill = "gold") +
        labs(x = "Hours of Sleep") + 
        facet_wrap(~Gender, labeller = "label_both")


# Run a survey-weighted t-test
svyttest(formula = SleepHrsNight ~ Gender, design = NHANES_design)

# Find means of total cholesterol by whether or not active 
out <- svyby(formula = ~TotChol, by = ~PhysActive, design = NHANES_design, 
             FUN = svymean, na.rm = TRUE, keep.names = FALSE
             )

# Construct a bar plot of means of total cholesterol by whether or not active 
ggplot(data = out, mapping = aes(x=PhysActive, y=TotChol)) + 
    geom_col()

# Run t test for difference in means of total cholesterol by whether or not active
svyttest(formula = TotChol ~ PhysActive, design = NHANES_design)

```
  
  
  
***
  
Chapter 4 - Modeling quantitative data  
  
Visualization with scatter plots:  
  
* Can look at head circumference compared to age (only captured for babies) using a scatterplot  
	* babies <- filter(NHANESraw, AgeMonths <= 6) %>% select(AgeMonths, HeadCirc)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc)) + geom_point()  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc)) + geom_jitter(width = 0.3, height = 0)  # width jitter but no height jitter  
* Can use weighting to extrapolate the scatter plot to the entire population  
	* ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, size = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(size = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, size = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0, alpha = 0.3) + guides(size = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, color = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(color = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE)  
  
Visualizing trends:  
  
* Survey-weighted lines of best fit can be added using the geom_smooth() in ggplot2, with the weight= provided as an aestehtic to the geom_smooth()  
	* ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE) + geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR))  
* Can also graph the best fit trendlines split by a categorical variable  
	* babies <- filter(NHANESraw, AgeMonths <= 6) %>% select(AgeMonths, HeadCirc, WTMEC4YR, Gender)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR, color = Gender)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE) + geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR))  
  
Modeling survey data:  
  
* Can use the regression equations directly to predict values for a new data point  
	* mod <- svyglm(HeadCirc ~ AgeMonths, design = NHANES_design)  
    * summary(mod)  
* The standard errors are an assessment of the likely errors between the estimated regression line and the true regression line  
  
More complex modeling:  
  
* Can extend the simple regression to a multiple regression in a parallel slopes model  
	* mod <- svyglm(HeadCirc ~ AgeMonths + Gender, design = NHANES_design)  
* Can also extend the simple regression to a multiple regression with different slopes  
  
Wrap up:  
  
* Packages survey, dplyr, and ggplot2  
* Survey fundamentals - clusters, strata, weights, svydesign(), etc.  
* Categorical data, svytable(), svychisq()  
* Quantiative data, svymean(), svytotal(), svyby(), svyquantile(), svyttest()  
* Modeling trends, svyglm()  
  
Example code includes:  
```{r}

# Create dataset with only 20 year olds
NHANES20 <- filter(NHANESraw, Age == 20)

# Construct scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight)) + 
    geom_point(alpha = 0.3) + 
    guides(size = FALSE)

# Construct bubble plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, size=WTMEC4YR)) + 
    geom_point(alpha = 0.3) + 
    guides(size = FALSE)

# Construct a scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, color=WTMEC4YR)) + 
    geom_point() + 
    guides(color = FALSE)

# Construct a scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, alpha=WTMEC4YR)) + 
    geom_point() + 
    guides(alpha = FALSE)


# Add gender to plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, size=WTMEC4YR, color=Gender)) + 
    geom_point(alpha=0.3) + 
    guides(size = FALSE)

# Add gender to plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, alpha=WTMEC4YR, color=Gender)) + 
    geom_point() + 
    guides(alpha = FALSE)


# Bubble plot with linear of best fit
ggplot(data = NHANESraw, mapping = aes(x = Height, y = Weight, size=WTMEC4YR)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight=WTMEC4YR))

# Add quadratic curve and cubic curve
ggplot(data = NHANESraw, mapping = aes(x = Height, y = Weight, size = WTMEC4YR)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR)) +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), formula = y ~ poly(x, 2), color = "orange") +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), formula = y ~ poly(x, 3), color = "red")


# Add survey-weighted trend lines to bubble plot
ggplot(data = NHANES20, mapping = aes(x = Height, y = Weight, size = WTMEC4YR, color = Gender)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 2)

# Add non-survey-weighted trend lines
ggplot(data = NHANES20, mapping = aes(x = Height, y = Weight, size = WTMEC4YR, color = Gender)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 2) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight=WTMEC4YR))


# Subset survey design object to only include 20 year olds
NHANES20_design <- subset(NHANES_design, Age == 20)

# Build a linear regression model
mod <- svyglm(Weight ~ Height, design = NHANES20_design)

# Print summary of the model
summary(mod)


# Build a linear regression model same slope
mod1 <- svyglm(Weight ~ Height + Gender, design = NHANES20_design)

# Print summary of the same slope model
summary(mod1)

# Build a linear regression model different slopes
mod2 <- svyglm(Weight ~ Height*Gender, design = NHANES20_design)

# Print summary of the different slopes model
summary(mod2)


# Plot BPDiaAve and BPSysAve by Diabetes and include trend lines
drop_na(NHANESraw, Diabetes) %>% 
    ggplot(mapping = aes(x=BPDiaAve, y=BPSysAve, size=WTMEC4YR, color=Diabetes)) + 
    geom_point(alpha = 0.2) +  
    guides(size = FALSE) + 
    geom_smooth(method="lm", se = FALSE, mapping = aes(weight=WTMEC4YR))

# Build simple linear regression model
mod1 <- svyglm(BPSysAve ~ BPDiaAve, design = NHANES_design)

# Build model with different slopes
mod2 <- svyglm(BPSysAve ~ BPDiaAve*Diabetes, design = NHANES_design)

# Summarize models
summary(mod1)
summary(mod2)

```
  
  
  
***
  
### _Inference for Catgeorical Data_  
  
Chapter 1 - Inference for a Single Parameter  
  
General Social Survey:  
  
* Categorical data are where the data are categories rather than numbers, which is prevalent in the General Social Survey (GSS)  
	* Several thousand people are surveyed, with a goal of drawing inferences about the population from the sample  
    * Can grab the "gss" dataframe from the tidyverse package  
* Can generate an approximate error by using mean +/- 2*SE  
* The bootstrap can be a valuable way to assess the standard errors - calculate the sample statistic within each replicate, and calculate its distribution  
	* library(infer)  
    * boot <- gss2016 %>% specify(response=happy, success="HAPPY") %>% generate(reps=500, type="bootstrap") %>% calculate(stat="prop")  
  
CI interpretations:  
  
* In classicial statistical inference, there is assumed to be a fix but unknown population parameter that is being estimated by way of sampling  
* A 95% CI means that 95% of the intervals formed from random samples would include the true population parameter  
  
Approximation shortcut:  
  
* Standard errors tend to increase when the sample size is small or the probability is close to 50%  
* The normal distribution (bell curve) can be a useful approximation for a large sample size - the normal becomes the sampling distribution  
	* SE = sqrt( p * (1-p) / n )  
    * n * p and n * (1-p) should both be greater than or equal to 10  
  
Example code includes:  
```{r}

load("./RInputFiles/gss.RData")
glimpse(gss)


# Subset data from 2016
gss2016 <- gss %>%
  filter(year == 2016)

gss2016 %>% count(consci)
gss2016 <- gss2016 %>%
    mutate(old_consci=consci, 
           consci=fct_other(fct_recode(old_consci, "High"="A GREAT DEAL"), keep="High", other_level="Low")
           )
gss2016 %>% count(consci)

# Plot distribution of consci
ggplot(gss2016, aes(x = consci)) +
  geom_bar()

# Compute proportion of high conf
p_hat <- gss2016 %>%
  summarize(p = mean(consci == "High", na.rm = TRUE)) %>%
  pull()


# Load the infer package
library(infer)

# Create single bootstrap data set
b1 <- gss2016 %>%
    specify(response = consci, success = "High") %>%
    generate(reps = 1, type = "bootstrap")

# Plot distribution of consci
ggplot(b1, aes(x = consci)) +
  geom_bar()

# Compute proportion with high conf
b1 %>%
  summarize(p = mean(consci == "High")) %>%
  pull()


# Create bootstrap distribution for proportion that favor
boot_dist <- gss2016 %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500) %>%
  calculate(stat = "prop", success = "High", na.rm = TRUE)

# Plot distribution
ggplot(boot_dist, aes(x=stat)) +
  geom_density()

# Compute estimate of SE
SE <- boot_dist %>%
  summarize(se = sd(stat)) %>%
  pull()

# Create CI
c(p_hat - 2*SE, p_hat + 2*SE)


# Two new smaller data sets have been created for you from gss2016: gss2016_small, which contains 50 observations, and gss2016_smaller which contains just 10 observations

id50 <- c(6, 98, 2673, 1435, 1535, 525, 2784, 1765, 163, 1859, 2497, 1780, 184, 575, 2781, 2310, 1677, 2478, 1226, 2350, 1139, 1635, 1350, 1809, 1842, 1501, 1502, 2610, 2456, 49, 56, 2167, 2401, 2002, 2343, 2012, 860, 2557, 1147, 1119, 2449, 695, 1511, 666, 1595, 1094, 2643, 769, 1263, 2426)
id10 <- c(1609, 1342, 2066, 2710, 1809, 503, 1889, 486, 1469, 6)

gss2016_small <- gss2016 %>%
    filter(id %in% id50)
gss2016_smaller <- gss2016 %>%
    filter(id %in% id10)

# Create bootstrap distribution for proportion
boot_dist_small <- gss2016_small %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "prop")

# Compute estimate of SE
SE_small_n <- boot_dist_small %>%
  summarize(se = sd(stat)) %>%
  pull()

# Create bootstrap distribution for proportion
boot_dist_smaller <- gss2016_smaller %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "prop")

# Compute estimate of SE
SE_smaller_n <- boot_dist_smaller %>%
  summarize(se = sd(stat)) %>%
  pull()

c(SE_small_n, SE_smaller_n)


# Create bootstrap distribution for proportion that have hardy any
boot_dist <- gss2016 %>%
  specify(response=consci,  success = "Low") %>%
  generate(reps=500, type="bootstrap") %>%
  calculate(stat = "prop", na.rm = TRUE)

# Compute estimate of SE
SE_low_p <- boot_dist %>%
    summarize(se = sd(stat)) %>%
    pull()


# Compute p-hat and n
p_hat <- gss2016_small %>% 
    summarize(p = mean(consci == "High", na.rm=TRUE)) %>%
    pull()
n <- nrow(gss2016_small)

# Check conditions
p_hat * n >= 10
(1 - p_hat) * n >= 10

# Calculate SE
SE_approx <- sqrt(p_hat * (1 - p_hat) / n)

# Form 95% CI
c(p_hat - 2 * SE_approx, p_hat + 2 * SE_approx)

```
  
  
  
***
  
Chapter 2 - Proportions (Testing and Power)  
  
Hypothesis test for a proportion:  
  
* The hypothesis test for a proportion looks at what sort of p-hat would be observed if p held a specific value  
	* The hypothesize() function prior to generate() sets out the hypothesis in question  
* Suppose that analysis is being run on whether people favor capital punishment  
	* null <- gss2016 %>% specify(response=cappun, success="FAVOR") %>% hypothesize(null="point", p=0.5) %>% generate(reps=500, type="simulate") %>% calculate(stat="prop")  
    * null %>% summarize(mean(stat > p_hat)) %>% pull() * 2  # The times 2 is for a two-sided test  
  
Intervals for differences:  
  
* Can also look at differences in proportions, for example men vs. women belief in afterlife  
* Can generate null data by rewording the null hypothesis to "there is no association between belief in the afterlife and gender" - enables test by permutation  
	* gss2016 %>% specify(response=postlife, explanatory=sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=1, type="permute")  
    * gss2016 %>% specify(postlife ~ sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=1, type="permute")  # can use formula notation; same command as above, but simplified  
    * null <- gss2016 %>% specify(postlife ~ sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=500, type="permute")  %>% calculate(stat="diff in props", order=c("FEMALE", "MALE")) # Full command  
    * null %>% summarize(mean(stat > d_hat)) %>% pull() * 2  
  
Statistical errors:  
  
* Type I errors - probability of rejecting a true null hypothesis - will happen with probability alpha  
* Type II errors - probability of not rejecting a false null hypothesis  - will happen with probability beta, meaning the test has power 1-beta  
  
Example code includes:  
```{r}

# Construct plot
ggplot(gss2016, aes(x = postlife)) + 
    geom_bar()

# Compute and save proportion that believe
p_hat <- gss2016 %>%
    summarize(mean(postlife == "YES", na.rm = TRUE)) %>%
    pull()

# Generate one data set under H0
sim1 <- gss2016 %>%
    specify(response = postlife, success = "YES") %>%
    hypothesize(null = "point", p = 0.75) %>%
    generate(reps = 1, type = "simulate")

# Construct plot
ggplot(sim1, aes(x=postlife)) +
    geom_bar()

# Compute proportion that believe
sim1 %>%
    summarize(mean(postlife == "YES")) %>%
    pull()


# Generate null distribution
null <- gss2016 %>%
    specify(response = postlife, success = "YES") %>%
    hypothesize(null = "point", p = .75) %>%
    generate(reps = 100, type = "simulate") %>%
    calculate(stat = "prop")

# Visualize null distribution
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = p_hat, color = "red")

# Compute the two-tailed p-value
null %>%
    summarize(mean(stat > p_hat)) %>%
    pull() * 2


# Plot distribution
ggplot(gss2016, aes(x = sex, fill = cappun)) +
    geom_bar(position = "fill")
  
# Compute two proportions
p_hats <- gss2016 %>%
    group_by(sex) %>%
    summarize(mean(cappun == "FAVOR", na.rm = TRUE)) %>%
    pull()

# Compute difference in proportions
d_hat <- diff(p_hats)


# Create null distribution
null <- gss2016 %>%
    specify(cappun ~ sex, success = "FAVOR") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "diff in props", order = c("FEMALE", "MALE"))
  
# Visualize null
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, col = "red")
  
# Compute two-tailed p-value
null %>%
    summarize(mean(stat < d_hat)) %>%
    pull() * 2


# Create the bootstrap distribution
boot <- gss2016 %>%
    specify(cappun ~ sex, success="FAVOR") %>%
    generate(reps=500, type="bootstrap") %>%
    calculate(stat = "diff in props", order = c("FEMALE", "MALE"))
  
# Compute the standard error
SE <- boot %>%
    summarize(sd(stat)) %>%
    pull()
  
# Form the CI (lower, upper)
c( d_hat - 2*SE, d_hat + 2*SE )


gssmod <- gss2016 %>%
    mutate(coinflip=sample(c("heads", "tails"), size=nrow(.), replace=TRUE))
table(gssmod$coinflip)

# Find difference in props
p_hats <- gssmod %>%
    group_by(coinflip) %>%
    summarize(mean(cappun == "FAVOR", na.rm = TRUE)) %>%
    pull()

# Compute difference in proportions
d_hat <- diff(p_hats)

# Form null distribution
null <- gssmod %>%
    specify(cappun ~ coinflip, success = "FAVOR") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "diff in props", order = c("heads", "tails"))

ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, color = "red")


# Set alpha
alpha <- 0.05

# Find cutoffs
upper <- null %>%
    summarize(quantile(stat, probs = c(1-alpha/2))) %>%
    pull()
lower <- null %>%
    summarize(quantile(stat, probs = alpha/2)) %>%
    pull()
  
# Visualize cutoffs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, color = "red") +
    geom_vline(xintercept = lower, color = "blue") +
    geom_vline(xintercept = upper, color = "blue")

# check if inside cutoffs
d_hat %>%
    between(lower, upper)

```
  
  
  
***
  
Chapter 3 - Comparing Many Parameters (Independence)  
  
Contingency tables:  
  
* Can look at bivariate relationships, such as political party affiliation vs. opinions on military spending  
	* The broom package can help in movements to/from contingency tables, by keeping things cleaner  
    * tab <- gss2016 %>% select(natarms, party) %>% table()  
    * tab %>% broom::tidy() %>% uncount(Freq)  
  
Chi-squared test statistic:  
  
* Can use Chi-squared to look at dependence of variables  
* Can create a contingency table O of the observations and a contingency table E of the expected distribution if there is pure independence  
	* Can then look at (O-E)**2 / E, and sum up to get the overall Chi-squared distribution  
    * Hypothesis tests can then assess how extreme a given Chi-squared may be  
  
Alternative method - chi-squared test statistic:  
  
* The Chi-squared statistic is derived from the Chi-squared distribution, which is specified solely by the number of degrees of freedom  
	* The degrees of freedom are (nRows - 1) * (nCols - 1)  
    * pchisq(chi_obs_spac, df=4)  # gives the likelihood of actual being less than, can use 1-pchisq() for the amount that is greater (the p-value of interest  
* Generally, need to have 5+ counts per cell, and to only use chi-squared for df=2+ (for df=1, can just compare proportions using the normal distribution)  
  
Intervals for chi-squared:  
  
* Can remove the hypothesize() call and use bootstrap() instead, but there is no real meaning to a Chi-squared in the absence of a null hypothesis  
* It is very unlikely that you would ever see a confidence interval attached to a Chi-squared interval  
  
Example code includes:  
```{r}

# Exclude "other" party
gss_party <- gss2016 %>%
    mutate(party=fct_collapse(partyid, 
                              "D"=c("STRONG DEMOCRAT", "NOT STR DEMOCRAT"), 
                              "R"=c("NOT STR REPUBLICAN", "STRONG REPUBLICAN"),
                              "I"=c("IND,NEAR DEM", "INDEPENDENT", "IND,NEAR REP"),
                              "O"="OTHER PARTY"
                              )
           ) %>%
    filter(!is.na(party), party != "O") %>%
    droplevels()

# Bar plot of proportions
gss_party %>%
    ggplot(aes(x = party, fill = natspac)) +
    geom_bar(position = "fill")
  
# Bar plot of counts
gss_party %>%
    ggplot(aes(x=party, fill = natspac)) +
    geom_bar()


# Create table of natspac and party
O <- gss_party %>%
    select(natspac, party) %>%
    table()

# Convert table back to tidy df
O %>%
    broom::tidy() %>%
    uncount(n)


# Create one permuted data set
perm_1 <- gss_party %>%
    specify(natarms ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1, type = "permute")
  
# Visualize permuted data
ggplot(perm_1, aes(x = party, fill = natarms)) +
    geom_bar()

# Make contingency table
tab <- perm_1 %>%
    ungroup() %>%
    select(natarms, party) %>%
    table()
  
# Compute chi-squared stat
(chi_obs_arms <- chisq.test(tab)$statistic)

(chi_obs_spac <- chisq.test(gss_party$natspac, gss_party$party)$statistic)

# Create null
null <- gss_party %>%
    specify(natspac ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 100, type = "permute") %>%
    calculate(stat = "Chisq")

# Visualize H_0 and obs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_spac, color = "red")

# Create null
null <- gss_party %>%
    specify(natarms ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 100, type = "permute") %>%
    calculate(stat = "Chisq")

# Visualize H_0 and obs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_arms, color = "red")


# create bar plot
gss2016 %>%
    ggplot(aes(x = region, fill = happy)) +
    geom_bar(position = "fill") +
    coord_flip()

# create table
tab <- gss2016 %>%
    select(happy, region) %>%
    table()
  
# compute observed statistic
(chi_obs_stat <- chisq.test(tab)$statistic)


# generate null distribution
null <- gss2016 %>%
    mutate(happy=fct_other(happy, keep=c("VERY HAPPY"))) %>%
    specify(happy ~ region, success = "VERY HAPPY") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "Chisq")

# plot null(s)
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_stat) +
    stat_function(fun = dchisq, args = list(df = (9-1)*(2-1)), color = "blue")

# permutation p-value
null %>% 
    summarize(mean(stat > chi_obs_stat)) %>% 
    pull()

# approximation p-value
1 - pchisq(chi_obs_stat, df = (9-1)*(2-1))

```
  
  
  
***
  
Chapter 4 - Comparing Many Parameters (Goodness of Fit)  
  
Case Study: Election Fraud:  
  
* Election fraud has many meanings; this course will focus on altering vote totals  
* Benford's Law applies when looking at broad collections of data, and considering only the first digit  
	* The law proposed that 30.1% of the first digits should be 1, with decreases as the numbers increase  
    * The basic idea is that the 1's always happen first (get to the 100s before any other x00s)  
* Can look at the 2009 Iranian election, and assess in comparison to Benford's Law  
  
Goodness of Fit:  
  
* Desire to assess whether the voter data is well aligned with Benford's law - Chi-squared is a good statistic for this  
	* chisq.test(myTab, p=myProbNull)  
* Can simulate the null hypothesis, for example by using  
	* gss2016 %>% specify(response=party) %>% hypothesize(null="point", p=p_uniform) %>% generate(reps=1, type="simulate")  
  
Now to the US:  
  
* Comparison to the US election in Iowa in 2016  
* Can look at county-level data  
  
Wrap-Up:  
  
* Could have rejected the null hypothesis even when it is true - typically 5%  
* More fundamental errors could be at play, such as assuming the first digit should follow Benford's Law  
	* Population of world cities tend to fit Benford's Law criteria (uniform distribution, consistency of logs, etc.)  
* Techniques for carrying out inference on categorical data - confidence intervals, hypothesis tests, Chi-squared tests for independence, goodness of fit of distributions  
* All tests follow specify-hypohteize-generate-calculate  
  
Example code includes:  
```{r}

iran <- readr::read_csv("./RInputFiles/iran.csv")
glimpse(iran)


# Compute candidate totals
totals <- iran %>%
  summarize(ahmadinejad = sum(ahmadinejad),
            rezai = sum(rezai),
            karrubi = sum(karrubi),
            mousavi = sum(mousavi))

# Plot totals
totals %>%
  gather(key = "candidate", value = "votes") %>%
  ggplot(aes(x = candidate, y = votes)) +
  geom_bar(stat = "identity")
  
# Cities won by #2
iran %>%
  group_by(province) %>%
  summarize(ahmadinejad = sum(ahmadinejad),
            mousavi = sum(mousavi)) %>%
  mutate(mousavi_win = mousavi > ahmadinejad) %>%
  filter(mousavi_win)


# Print get_first
get_first <- function(x) {
    substr(as.character(x), 1, 1) %>%
      as.numeric() %>%
      as.factor()
}

# Create first_digit
iran2 <- iran %>%
  mutate(first_digit = get_first(total_votes_cast))
  
# Construct barchart
iran2 %>%
  ggplot(aes(x=first_digit)) +
  geom_bar()


# Tabulate the counts of each digit
tab <- iran2 %>%
  select(first_digit) %>%
  table()

# Compute observed stat
p_benford <- c(0.301029995663981, 0.176091259055681, 0.1249387366083, 0.0969100130080564, 0.0791812460476248, 0.0669467896306132, 0.0579919469776867, 0.0511525224473813, 0.0457574905606751)
names(p_benford) <- 1:9
p_benford[9] <- 1 - sum(p_benford[-9])
sum(p_benford)
chi_obs_stat <- chisq.test(tab, p = p_benford)$stat

# Form null distribution
null <- iran2 %>%
  specify(response=first_digit) %>%
  hypothesize(null = "point", p = p_benford) %>%
  generate(reps=500, type = "simulate") %>%
  calculate(stat = "Chisq")


# plot both nulls
ggplot(null, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = chi_obs_stat) + 
  stat_function(fun = dchisq, args = list(df = 9-1), color = "blue")

# permutation p-value
null %>%
  summarize(mean(stat > chi_obs_stat)) %>%
  pull()

# approximation p-value
pchisq(chi_obs_stat, df=9-1, lower.tail=FALSE)


iowa <- readr::read_csv("./RInputFiles/iowa.csv")
glimpse(iowa)

# Get R+D county totals
iowa2 <- iowa %>%
  filter(candidate == "Hillary Clinton / Tim Kaine" | candidate == "Donald Trump / Mike Pence") %>%
  group_by(county) %>%
  summarize(dem_rep_votes = sum(votes, na.rm = TRUE)) 

# Add first_digit
iowa3 <- iowa2 %>%
  mutate(first_digit = get_first(dem_rep_votes))

# Construct bar plot
iowa3 %>%
  ggplot(aes(x=first_digit)) + 
  geom_bar()


# Tabulate the counts of each digit
tab <- iowa3 %>%
  select(first_digit) %>%
  table()

# Compute observed stat
chi_obs_stat <- chisq.test(tab, p = p_benford)$stat

# Form null distribution
null <- iowa3 %>%
  specify(response = first_digit) %>%
  hypothesize(null = "point", p = p_benford) %>%
  generate(reps = 500, type = "simulate") %>%
  calculate(stat = "Chisq")
  
# Visualize null
ggplot(null, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = chi_obs_stat)

```
  
  
  
***
  
### _Building Dashboards with flexdashboard_  
  
Chapter 1 - Dashboard Layouts  
  
Introduction:  
  
* Dashboards are a collection of components in a single display - graphs, text, tables, widgets, etc.  
* The flexdashboard is an R package that allows for using R Markdown to create a dashboard  
	* Can include all the power of R  
    * Can combine with Shiny for reactive elements  
* Course will include capabilities of flexdashboard, decision as to whether to incorporate Shiny, and potential extensions  
  
Anatomy of flexdashboard:  
  
* Within R Markdown, the header controls the type of document created during knitting  
	* output: flexdashboard::flex_dashboard (will create the flesdashboard output)  
* The flexdashboard is made up of charts, with each chart denoted by ### ChartName  
	* The succeeding lines can then be R code, similar to other R Markdown processes  
* By default, all of the charts will stack in a single column, though multiple columns can also be declared  
	* Columns are created using 14+ dashes, with everything underneath contained in that column  
    * Can give a specific name and specify options for each of the columns  
* Can start in Rstudio using File - New File - R Markdown - From Template - flexdashboard  
* Course data will include bicycle sharing data from San Francisco  
  
Layout basics:  
  
* Columns can be of variable width by using data-width= such that they add up to 1000  
* Can create by rows rather than columns using orientation:rows underneath flexdashboard::flex_dashboard:  
	* Can use data-height to vary the row heights  
    * Can use certical_layout: scroll as an option to allow for scrolling rather than forcing everything on to one page (this is considered poor dashboard design, though)  
  
Advanced layouts:  
  
* Options for extending the dashboard include tabsets  
	* Column {.tabset} - will apply the tabset to every chart in that column  
* Can also extend by using pages, where columns and rows are children of their respective pages  
	* Sixteen (16) or more equal signs under a Page title specify a call to the page  
    * Can add Page xxx {data-navmenu=yyy} to specify that the page xxx should belong to the navmenu yyy  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 2 - Data Visualization for Dashboards  
  
Graphs:  
  
* The easiest way to add a graph is to include it as part of a code snippet in flexdashboard  
* Graphs will flex and resize to stay in the container  
* Sometimes, may want to set the figure width and height to match the aspect ratio of the target device as closely as possible  
	* {r, fig.width=10, fig.height=5}  
    * Downside #1 - trial and error needed  
    * Downside #2 - need to adjust every time charts are added  
    * Downside #3 - graphs are no longer responsive to user inputs  
  
Web-Friendly Visualizations:  
  
* Web-friendly packages include plotly, highcharter, dygraphs, rbokeh, ggvis  
* The plotly calls are helpful since they are closely linked to ggplot2  
	* library(plotly)  
    * ggplotly(my_ggplot)  # my_ggplot is a ggplot2 object  
  
htmlwidgets:  
  
* htmlwidgets are a framework that connects R with Javascript (web-friendly and well-suited to dashboards)  
	* Learn more at: http://htmlwidgets.org  
* The leaflet package allows for adding interactive maps  
	* library(leaflet)  
    * leaflet() %>% addTiles() %>% addMarkers(lng = data_df$longitude, lat = data_df$latitude)  
    * leaflet(data_df) %>% addTiles() %>% addMarkers()  # leaflet called on a data frame  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 3 - Dashboard Components  
  
Highlighting Single Values:  
  
* Gauges can be helpful for values in a defined range, such as 0%-100%  
	* gauge(value = pct_subscriber_trips, min = 0, max = 100)  # basics for creating a gauge include the value, the min, and the max  
    * gauge(value = pct_subscriber_trips, min = 0, max = 100, sectors = gaugeSectors( success = c(90, 100), warning = c(70, 89), danger = c(0, 69) ), symbol = '%')  # additional features  
* Value boxes can be helpful for values that do not fall in a pre-defined range  
	* valueBox(prettyNum(num_trips, big.mark = ","), caption = "Total Daily Trips", icon = "fa-bicycle")  # font-awesome bicycles  
* Both gauges and value boxes can be linked  
	* valueBox(prettyNum(num_trips, big.mark = ','), caption = 'Total Daily Trips', icon = 'fa-bicycle', href = '#trip-raw-data')  # href makes the caption linked and clickable  
  
Dashboard Tables:  
  
* The kable function from knitr is one of the easiest ways to create a table - but, not very well tuned to html  
	* library(knitr)  
    * kable(my_data_df)  
* The DT package is better suited to making responsive tables  
	* library(DT)  
    * datatable(my_data_df)  
    * datatable(my_data_df, rownames = FALSE)  # eliminate row numbering  
    * datatable(my_data_df, rownames = FALSE, options = list(pageLength = 15))  # most options are set by way of a list; note the contrast to rownames  
    * datatable( my_data_df, rownames = FALSE, extensions = 'Buttons', options = list( dom = 'Bfrtip', buttons = c('copy', 'csv', 'excel', 'pdf', 'print') ) )  # buttons for extract  
  
Text for Dashboards:  
  
* Captions (notes) are a common way to add text to a chart, and are added using a greater than sign with text; there must be an empty line between the end of the chunk and the caption  
* Another way to provide more context is with the storyboard format  
	* Presents one chart at a time in a specified order, where the user controls the navigation speed between the charts  
    * Good format for content that runs in order  
    * Requires storyboard: true in the yaml header in the flexdashboard::flex_dashboard items  
    * Within the story, the ### signal the next page of the story, and should have descriptive text  
    * Can add commentary using the triple asterisk (***) which needs to be AFTER the R chunk and separated by at least one space  
* Can also mix in storyboard on some pages but not on others (requires leaving this out of the yaml header)  
	* Add {.storyboard} to the end of the page description  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 4 - Adding Interactivity with Shiny  
  
Incorporating Shiny into Dashboards:  
  
* Incorporating Shiny is optional but can mak the dashboards even more interactive  
	* Shiny is interactive and lightweight, though at the expense of greater complication and hosting challenges  
* Even after incorporating Shiny, the flexdashboard document is still an interactive R Markdown document  
* runtime:shiny in the yaml header will make the flexdashboard in to a Shiny App  
  
Reactive Dataframe Pattern:  
  
* Creating a narrow sidebar using  
	* Column {data-width=200 .sidebar}  
* Widgets can be use inside of an R chunk, like any other dashboard component  
	* sliderInput("duration_slider", label="Select maximum trip duration to display (in minutes): ", min=0, max=120, value=15, step=5, dragRange=TRUE)  
    * show_trips_df <- reactive({ trips_df %>% filter(duration_sec <= input$duration_slider * 60) })  
    * To call the reactice data frame later, use show_trips_df()  
* Output from the reactive needs to be encloses in the appropriate render*() function  
	* renderLeaflet({ show_trips_df() %>% … %>% leaflet() %>% … })  # no need for the output call like there would be in a typical Shiny document  
* Five key steps for the reactive data frame pattern  
	* Create a sidebar column  
    * Add user inputs to the sidebar - *Input() Shiny widgets  
    * Make a data frame that uses the inputs, called later using ()  
    * Replace the dataframe in the dashboard component code with the reactive version  
    * Wrap with the appropriate rendering function render*()  
  
Customized Inputs for Charts:  
  
* Can have a reactive component impact everything, as per the example worked through in the previous section  
* May also want to have sliders that only impact a single object  
	* Putting these together in the same loaction can cause headaches due to the need to work with layouts  
* Example code to implement includes  
	* fillCol(height=600, flex=c(NA, 1), inputPanel(sliderInput("my_input", …)), plotOutput("my_plot", height="100%"))  # flex is the flexible height for the components  
    * output$my_plot <- renderPlot({ … })  
* Can use a global shortcut  
	* Can put all of the charts that are driven by the same slider on to the same page  
    * Can put the sidebar as a class with its own page, followed by all the other pages, to have the same sidebar drive all of the pages  
  
Wrap-up:  
  
* Additional resources available through Rstudio and htmlwidgets.org (information about all html widgets available in R)  
	* The highcharter can be helpful - high quality charts with some interactivity  
* Can use shinydashboard for just Shiny if R Markdown and flexdasboard are not needed  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
### _Network Analysis in R: Case Studies_  
  
Chapter 1 - Exploring Graphs Through Time  
  
Exploring Data Set:  
  
* Daily snapshots of items purchased together (co-purchases) from Amazon in 2003  
	* There is the to and from that will make up the graph, plus associated metadata  
    * Desire to look only at a single data, and only the from-to data (assuming a directional graph)  
* Can look at a smaller subset of the graph, for example  
	* sg <- induced_subgraph(amzn_g, 1:500)  
    * sg <- delete.vertices(sg, degree(sg) == 0)  
    * plot(sg, vertex.label = NA, edge.arrow.width = 0, edge.arrow.size = 0, margin = 0, vertex.size = 2)  
* Can count the number of diad (2-connect) and triad (3-connect) in the data  
	* null, asymmetric, and mutual are the potential results for diads  
    * Three-digit codes are used to reflect the 16 potential triad states - #Bi/#Assym/#Uncon - plus a letter D, U, and C  
  
Exploring Temporal Structure:  
  
* Dataset has 4 days worth of data - can build from the earliest date to the latest date  
* Can create a list to hold the igraphs at each time period, loop over the times, and then plot them  
	* A handful of vertices may be important and interesting across time  
  
Example code includes:  
```{r}

library(igraph)
amzn_g <- read.graph("./RInputFiles/amzn_g.gml", format=c("gml"))
amzn_g


# Perform dyad census
dc <- dyad_census(amzn_g)

# Perform triad census
tc <- triad_census(amzn_g)

# Find the edge density
ed <- edge_density(amzn_g)

# Output values
print(dc)
print(tc)
print(ed)


# Calculate transitivity
transitivity(amzn_g)

# Calculate reciprocity
amzn_rp <- reciprocity(amzn_g)

# Simulate our outputs
nv <- gorder(amzn_g)
ed <- edge_density(amzn_g)
rep_sim <- rep(NA, 1000)

# Simulate 
for(i in 1:1000){
  rep_sim[i] <- reciprocity(erdos.renyi.game(nv, ed, "gnp", directed = TRUE))
}

# Compare
quantile(rep_sim, c(0.25, .5, 0.975))
print(amzn_rp)


# Get the distribution of in and out degrees
table(degree(amzn_g, mode = "in"))
table(degree(amzn_g, mode = "out"))

# Find important products based on the ratio of out to in and look for extremes
imp_prod <- V(amzn_g)[degree(amzn_g, mode = "out") > 3 & degree(amzn_g, mode = "in") < 3]

## Output the vertices
print(imp_prod)


ipFrom <- c(1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 32129, 32129, 32129, 32129, 32129, 32129, 32129, 38131, 38131, 38131, 38131, 38131, 38131, 45282, 45282, 45282, 45282, 52831, 52831, 52831, 52831, 52831, 52831, 52831, 52831, 53591, 53591, 53591, 53591, 53591, 53591, 53591, 53591, 56427, 56427, 56427, 56427, 59706, 59706, 59706, 59706, 59706, 59706, 59706, 59706, 62482, 62482, 62482, 62482, 62482, 62482, 67038, 67038, 67038, 67038, 71192, 71192, 71192, 71192, 71192, 77957, 77957, 77957, 77957, 77957, 77957, 103733, 103733, 103733, 103733, 103733, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 144749, 144749, 144749, 144749, 144749, 144749, 144749, 170830, 170830, 170830, 170830, 170830, 170830, 177282, 177282, 177282, 177282, 177282, 177282, 177432, 177432, 177432, 177432, 177432, 177432, 177432, 184526, 184526, 184526, 184526, 184526, 191825, 191825, 191825, 191825, 191825, 215668, 215668, 215668, 221085, 221085, 221085, 221085, 221085, 231604, 231604, 231604, 231604, 231604, 231604, 239014, 239014, 239014, 239014, 239014, 242693, 242693, 242693, 242693, 242693, 257621, 257621, 257621, 257621, 261587, 261587, 261587, 261587, 261587, 261587, 261657, 261657, 261657, 261657, 261657, 261657)
ipTo <- c(190, 1366, 2679, 4023, 1625, 1627, 7529, 1272, 1628, 1630, 1631, 11124, 15360, 20175, 10626, 20970, 10776, 11164, 11166, 5955, 8719, 11164, 23842, 23843, 24115, 15312, 23329, 32127, 80473, 44848, 44849, 44850, 38133, 31084, 33711, 10920, 20178, 20179, 87093, 2134, 2136, 4119, 9995, 36524, 64698, 64700, 52833, 120083, 120085, 120086, 36689, 12340, 113789, 32094, 51015, 1898, 10076, 15800, 61488, 63836, 63837, 63838, 8882, 59708, 59711, 26982, 59708, 69497, 69498, 69499, 69500, 23349, 62480, 58926, 58928, 64118, 52271, 71190, 71380, 75384, 9762, 57876, 43543, 43546, 98488, 77951, 77953, 116842, 103732, 103734, 103735, 103728, 124733, 117842, 117843, 117845, 117842, 117843, 117845, 117842, 117842, 117843, 117845, 59267, 89503, 89506, 156, 190, 105428, 184973, 195785, 195787, 132753, 132754, 132755, 52563, 132755, 132756, 132759, 132762, 126757, 132754, 132755, 132756, 189269, 265886, 43155, 80519, 159667, 82479, 152760, 136747, 65216, 114684, 114686, 114687, 117132, 132667, 81755, 109198, 109199, 109202, 144124, 75023, 216449, 139527, 149146, 152038, 177428, 177430, 177428, 177430, 56930, 61658, 207112, 250755, 250756, 56930, 141148, 191036, 147084, 245110, 175959, 177376, 177377, 88463, 103641, 115111, 165118, 228427, 43553, 76706, 78278, 131353, 75725, 119146, 12615, 15740, 229533, 151325, 237568, 239545, 239546, 239547, 110872, 215593, 60310, 60312, 133398, 44502, 261582, 261590, 261599, 271593, 261584, 261588, 261649, 261653, 261654, 261658, 261662, 105814)
ipGroupFrom <- factor(c('DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD'), levels=c("DVD", "Video"))
ipSRFrom <- c(30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 16, 16, 16, 16, 16, 16, 16, 37, 37, 37, 37, 37, 37, 26, 26, 26, 26, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 10, 10, 10, 10, 1, 1, 1, 1, 1, 1, 1, 1, 19, 19, 19, 19, 19, 19, 10, 10, 10, 10, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 27, 27, 27, 27, 27, 27, 27, 10, 10, 10, 10, 10, 10, 6, 6, 6, 6, 6, 6, 19, 19, 19, 19, 19, 19, 19, 25, 25, 25, 25, 25, 3, 3, 3, 3, 3, 8, 8, 8, 27, 27, 27, 27, 27, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 26, 26, 26, 26, 26, 15, 15, 15, 15, 8, 8, 8, 8, 8, 8, 26, 26, 26, 26, 26, 26)
ipSRTo <- c(5, 2, 18, 20, 12, 6, 8, 14, 16, 4, 18, 20, 3, 6, 14, 5, 3, 3, 4, 3, 13, 3, 5, 9, 18, 17, 8, 2, 8, 9, 16, 9, 24, 11, 25, 6, 9, 3, 21, 1, 5, 2, 24, 2, 6, 6, 8, 18, 7, 4, 20, 6, 22, 13, 10, 19, 4, 22, 7, 7, 9, 7, 11, 21, 12, 17, 21, 5, 7, 2, 1, 26, 6, 14, 2, 17, 4, 13, 12, 6, 8, 13, 4, 7, 1, 7, 9, 15, 19, 6, 20, 0, 19, 14, 18, 11, 14, 18, 11, 14, 14, 18, 11, 16, 1, 5, 3, 5, 6, 22, 5, 20, 10, 29, 9, 22, 9, 12, 10, 9, 12, 29, 9, 12, 13, 6, 23, 6, 18, 10, 18, 6, 9, 11, 8, 8, 19, 12, 10, 9, 8, 14, 1, 7, 10, 13, 18, 6, 6, 4, 6, 4, 4, 22, 5, 8, 4, 4, 13, 11, 3, 4, 21, 22, 8, 18, 1, 6, 5, 5, 4, 8, 6, 12, 6, 3, 13, 8, 10, 1, 1, 22, 12, 18, 19, 5, 18, 31, 8, 13, 10, 14, 25, 4, 19, 17, 5, 21, 3, 1, 19, 10)
ipTRFrom <- c(290, 290, 290, 290, 290, 290, 290, 290, 290, 290, 290, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 73, 73, 73, 73, 73, 73, 73, 294, 294, 294, 294, 294, 294, 43, 43, 43, 43, 5, 5, 5, 5, 5, 5, 5, 5, 13, 13, 13, 13, 13, 13, 13, 13, 28, 28, 28, 28, 1, 1, 1, 1, 1, 1, 1, 1, 110, 110, 110, 110, 110, 110, 7, 7, 7, 7, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 25, 25, 25, 25, 25, 25, 25, 2, 2, 2, 2, 2, 2, 12, 12, 12, 12, 12, 12, 111, 111, 111, 111, 111, 111, 111, 294, 294, 294, 294, 294, 0, 0, 0, 0, 0, 0, 0, 0, 243, 243, 243, 243, 243, 43, 43, 43, 43, 43, 43, 15, 15, 15, 15, 15, 483, 483, 483, 483, 483, 1, 1, 1, 1, 12, 12, 12, 12, 12, 12, 2, 2, 2, 2, 2, 2)
ipTRTo <- c(19, 2, 22, 105, 22, 1, 6, 55, 40, 21, 47, 13, 0, 42, 14, 51, 2, 4, 0, 2, 41, 4, 0, 19, 21, 63, 5, 0, 2, 4, 63, 63, 7, 1, 8, 11, 134, 134, 12, 5, 10, 3, 58, 1, 6, 2, 27, 39, 2, 18, 87, 12, 218, 2, 30, 17, 0, 41, 13, 9, 3, 2, 13, 8, 10, 1, 8, 1, 0, 7, 1, 167, 63, 28, 0, 6, 1, 10, 4, 0, 2, 0, 5, 2, 3, 2, 2, 12, 24, 45, 21, 0, 8, 2, 21, 20, 2, 21, 20, 2, 2, 21, 20, 14, 6, 6, 3, 19, 13, 88, 4, 9, 6, 0, 19, 54, 19, 6, 9, 1, 2, 0, 19, 6, 3, 13, 46, 29, 6, 1, 15, 1, 4, 18, 28, 5, 15, 21, 10, 12, 3, 5, 4, 3, 8, 5, 0, 0, 5, 0, 5, 0, 1, 221, 1, 13, 3, 1, 7, 40, 5, 0, 8, 37, 67, 48, 0, 6, 1, 25, 1, 69, 0, 55, 3, 0, 5, 5, 2, 13, 0, 44, 53, 9, 4, 5, 13, 212, 3, 3, 1, 3, 8, 0, 3, 12, 11, 10, 5, 0, 49, 42)
ipTitleFrom <- c(16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 13, 13, 13, 13, 13, 13, 13, 30, 30, 30, 30, 30, 30, 11, 11, 11, 11, 26, 26, 26, 26, 26, 26, 26, 26, 5, 5, 5, 5, 5, 5, 5, 5, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 14, 14, 14, 14, 14, 12, 12, 12, 12, 12, 12, 21, 21, 21, 21, 21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 27, 27, 27, 27, 27, 27, 27, 27, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 29, 29, 29, 29, 29, 29, 29, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 15, 15, 15, 15, 15, 15, 15, 30, 30, 30, 30, 30, 7, 7, 7, 7, 7, 6, 6, 6, 9, 9, 9, 9, 9, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 24, 24, 24, 24, 24, 19, 19, 19, 19, 3, 3, 3, 3, 3, 3, 28, 28, 28, 28, 28, 28)
ipNames <- c('Attraction', 'Barbara The Fair With The Silken Hair', 'Cannibal Apocalypse', "DJ Qbert's Wave Twisters", 'David and Lisa', 'Def Comedy Jam  Vol. 13', 'Detroit Lions 2001 NFL Team Video', 'Donnie McClurkin: Live in London and More', 'El Hombre Sin Sombra (Hollow Man)', 'Gladiator', 'Kindergarten Cop', "Kingsley's Meadow - Wise Guy", "Lady & The Tramp II - Scamp's Adventure", 'Lojong - Transforming the Mind (Boxed Set)', 'Menace II Society', 'Merlin', 'Modern Times', 'Murder by Numbers (Full Screen Edition)', 'Nancy Drew: A Haunting We Will Go', 'Princess Nine - Triple Play (Vol. 3)', 'Secret Agent AKA Danger Man  Set 2', 'Seguire Tus Pasos', 'Selena Remembered', 'Seven (New Line Platinum Series)', 'Sheba  Baby', 'Slaughter', 'The Complete Guide to Medicine Ball Training', 'The Gambler', 'The Getaway', 'The Sum of All Fears')
ip_df <- data.frame(X=1:202, 
                    from=ipFrom, 
                    to=ipTo, 
                    salesrank.from=ipSRFrom, 
                    salesrank.to=ipSRTo, 
                    totalreviews.from=ipTRFrom, 
                    totalreviews.to=ipTRTo, 
                    group.from=ipGroupFrom, 
                    title.from=factor(ipNames[ipTitleFrom], levels=ipNames)
                    )


# Create a new graph
ip_g <- graph_from_data_frame(ip_df %>% select(from, to), directed = TRUE)

# Add color to the edges based on sales rank, blue is higer to lower, red is lower to higher
E(ip_g)$rank_flag <- ifelse(ip_df$salesrank.from <= ip_df$salesrank.to, "blue", "red")

# Plot and add a legend
plot(ip_g, vertex.label = NA, edge.arrow.width = 1, edge.arrow.size = 0, 
    edge.width = 4, margin = 0, vertex.size = 4, 
    edge.color = E(ip_g)$rank_flag, vertex.color = "black" )
legend("bottomleft", legend = c("Lower to Higher Rank", "Higher to Lower Rank"), 
       fill = unique(E(ip_g)$rank_flag ), cex = .7)


# Get a count of out degrees for all vertices
# deg_ct <- lapply(time_graph, function(x){return(degree(x, mode = "out") )})

# Create a dataframe starting by adding the degree count
# deg_df <- data.frame(ct = unlist(deg_ct))

# Add a column with the vertex names 
# deg_df$vertex_name <- names(unlist(deg_ct))

# Add a time stamp 
# deg_df$date <- ymd(rep(d, unlist(lapply(time_graph, function(x){length(V(x))}))))

# See all the vertices that have more than three out degrees
# lapply(time_graph, function(x){return(V(x)[degree(x, mode = "out") > 3])})

# Create a dataframe to plot of three important vertices
# vert_df <- deg_df %>% filter(vertex_name %in% c(1629, 132757, 117841))

# Draw the plot to see how they change through time
# ggplot(vert_df, aes(x = date, y = ct, group = vertex_name, colour = vertex_name)) + geom_path()


# Calculate clustering and reciprocity metrics
# trans <- unlist(lapply(all_graphs, FUN=transitivity))
# rp <- unlist(lapply(all_graphs, FUN=reciprocity))

# Create daaframe for plotting
# met_df <- data.frame("metric" = c(trans, rp))

# Repeat the data
# met_df$date <- rep(ymd(d), 2)

# Sort and then Repeat the metric labels
# met_df$name <- sort(rep(c("clustering", "reciprocity"), 4))

# Plot
# ggplot(met_df, aes(x= date, y= metric, group = name, colour = name)) + geom_path()

```
  
  
  
***
  
Chapter 2 - Talk About R on Twitter  
  
Creating retweet graphs:  
  
* Data is several days of tweets from #rstats - want to use retweets (starts with RT) to form the network  
	* raw_tweets <- read.csv("datasets/rstatstweets.csv", stringsAsFactors = F)  
    * all_sn <- unique(raw_tweets$screen_name)  
    * rt_g <- graph.empty()  
    * rt_g <- rt_g + vertices(all_sn)  
    * for(i in 1:dim(raw_tweets)[1]){  
    *     rt_name <- find_rt(raw_tweets$tweet_text[i])  
    *     if(!is.null(rt_name)){  
    *         if(!rt_name %in% all_sn){   
    *             rt_g <- rt_g + vertices(rt_name)  
    *         }  
    *     rt_g <- rt_g + edges(c(raw_tweets$screen_name[i], rt_name))  
    *     }  
    * }  
    * sum(degree(rt_g) == 0)  
    * rt_g <- simplify(rt_g)  
    * rt_g <- delete.vertices(rt_g, degree(rt_g) == 0)  
  
Building mentions graphs:  
  
* Tweets that mention someone can be a reply or a callout  
* There is more complexity than with the retweets, since there is no common format to a mention such as "starts with RT"  
    * ment_g <- graph.empty()  
    * ment_g <- ment_g + vertices(all_sn)  
    * for(i in 1:dim(raw_tweets)[1]) {  
    *     ment_name <- mention_ext(raw_tweets$tweet_text[i])  
    *     if(length(ment_name) > 0 ) {  
    *         for(j in ment_name) {  
    *             if(!j %in% all_sn) {  
    *                 ment_g <- ment_g + vertices(j)  
    *             }  
    *         ment_g <- ment_g + edges(c(raw_tweets$screen_name[i], j))  
    *         }  
    *     }  
    * }  
* The mentions graph is significantly different, with many more small conversations shown by way of a sub-graph  
  
Finding communities:  
  
* Communities are natural way to think of graphs - people who talk much more to each other than to the full network  
	* ment_edg <- cluster_edge_betweenness(as.undirected(ment_g))  
    * ment_eigen <- cluster_leading_eigen(as.undirected(ment_g))  
    * ment_lp <- cluster_label_prop(as.undirected(ment_g))  
    * length(ment_edg)  
    * table(sizes(ment_edg))  
* Can compare similarities within community structures  
	* compare(ment_edg, ment_eigen, method = 'vi')  # "vi" is "variance information"  
    * compare(ment_eigen, ment_lp, method = 'vi')  
    * compare(ment_lp, ment_edg, method = 'vi')  
* Can also plot the community structures  
	* lrg_eigen <- as.numeric(names(ment_eigen[which(sizes(ment_eigen) >45)]))  
    * eigen_sg <- induced.subgraph(ment_g, V(ment_g)[ eigen %in% lrg_eigen])  
* plot(eigen_sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2,  
*      coords = layout_with_fr(ment_sg), margin = 0, vertex.size = 6,  
*      vertex.color = as.numeric(as.factor(V(eigen_sg)$eigen))  
*      )  
  
Example code includes:  
```{r cache=TRUE}

rt_g <- read.graph("./RInputFiles/rt_g.gml", format=c("gml"))
rt_g


# Calculate the number of nodes
gsize(rt_g)

# Calculate the number of edges
gorder(rt_g)

# Calculate the density
graph.density(rt_g)

# Create the plot
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.4, vertex.size = 3)


# Set the default color to black
V(rt_g)$color <- "black"

# Set the color of nodes that were retweeted just once to blue
V(rt_g)[degree(rt_g, mode = "in") == 1]$color <- "blue"

# Set the color of nodes that were retweeters just once to green 
V(rt_g)[degree(rt_g, mode = "out") == 1 ]$color <- "green"

# Plot the network
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, 
    edge.arrow.size = 0.25, 
    vertex.size = 4, vertex.color = V(rt_g)$color)


# Set the default color to black
V(rt_g)$color <- "black"

# Set the color of nodes that were retweeted just once to blue
V(rt_g)[degree(rt_g, mode = "in") == 1 & degree(rt_g, mode = "out") == 0]$color <- "blue"

# Set the color of nodes that were retweeters just once to green 
V(rt_g)[degree(rt_g, mode = "in") == 0 & degree(rt_g, mode = "out") == 1 ]$color <- "green"

# Plot the network
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, 
    edge.arrow.size = 0.25, 
    vertex.size = 4, vertex.color = V(rt_g)$color)


# Calculate betweenness
rt_btw <- igraph::betweenness(rt_g, directed = TRUE)

# Plot histogram
hist(rt_btw, breaks = 2000, xlim = c(0, 1000), main = "Betweenness")

# Calculate eigen centrality
rt_ec <- eigen_centrality(rt_g, directed = TRUE)

# Plot histogram
hist(rt_ec$vector, breaks = 100, xlim = c(0, .2), main = "Eigen Centrality")


# Get top 1% of vertices by eigen centrality
top_ec <- rt_ec$vector[rt_ec$vector > quantile(rt_ec$vector, .99)]

# Get top 1% of vertices by betweenness
top_btw <- rt_btw[rt_btw > quantile(rt_btw, .99)]

# Make a nice data frame to print, with three columns, Rank, Betweenness, and Eigencentrality
most_central <- as.data.frame(cbind(1:length(top_ec), names(sort(top_btw, decreasing = T)), 
                                    names(sort(top_ec, decreasing = T))
                                    )
                              )

# Set column names
colnames(most_central) <- c("Rank", "Betweenness", "Eigen Centrality")

# Print out the data frame
print(most_central)


# Transform rt_btw and add as centrality
V(rt_g)$cent <-  log(rt_btw+2)

# Visualize
plot(rt_g, vertex.label = NA, edge.arrow.width = .2,
     edge.arrow.size = 0.0,
     vertex.size = unlist(V(rt_g)$cent), vertex.color = "red")

# Create subgraph 
rt_sub <-induced_subgraph(rt_g, V(rt_g)[V(rt_g)$cent >= quantile(V(rt_g)$cent, 0.99 )])

# Plot subgraph
plot(rt_sub, vertex.label = NA, edge.arrow.width = .2,
     edge.arrow.size = 0.0,
     vertex.size = unlist(V(rt_sub)$cent), vertex.color = "red")


ment_g <- read.graph("./RInputFiles/ment_g.gml", format=c("gml"))
ment_g


rt_ratio <- degree(rt_g, mode="in") / (degree(rt_g, mode="out"))
ment_ratio <- degree(ment_g, mode="in") / (degree(ment_g, mode="out"))

# Create a dataframe to plot with ggplot
ratio_df <- data.frame(io_ratio = c(ment_ratio, rt_ratio))
ratio_df["graph_type"] <- c(rep("Mention", length(ment_ratio)), rep("Retweet", length(rt_ratio)) )
ratio_df_filtered <- ratio_df %>% filter(!is.infinite(io_ratio) & io_ratio > 0)

# Plot the graph
ggplot(ratio_df, aes(x = io_ratio , fill= graph_type, group = graph_type)) +
  geom_density(alpha = .5) +
  xlim(0, 10)
 
# Check the mean and median of each ratio
ratio_df %>% group_by(graph_type) %>% summarise(m_ratio = mean(io_ratio))
ratio_df %>% group_by(graph_type) %>% summarise(med = median(io_ratio))
ratio_df %>% filter(io_ratio != +Inf) %>% group_by(graph_type) %>% summarise(m_ratio = mean(io_ratio))
ratio_df %>% filter(io_ratio != +Inf) %>% group_by(graph_type) %>% summarise(med = median(io_ratio))


# Plot mention graph 
plot(ment_g, vertex.label = NA, edge.arrow.width = .8,
     edge.arrow.size = 0.2,
     margin = 0,
     vertex.size = 3)

# Find the assortivity of each graph
assortativity_degree(rt_g, directed = TRUE)
assortativity_degree(ment_g, directed = TRUE)

# Find the reciprocity of each graph
reciprocity(rt_g) 
reciprocity(ment_g)


# Get size 3 cliques
clq_list <- cliques(ment_g, min = 3, max = 3)

# Convert to a dataframe and filter down to just revodavid cliques
clq_df <- data.frame(matrix(names(unlist(clq_list)), nrow = length(clq_list), byrow = T))
rev_d <- clq_df %>% filter(X1 == "revodavid" | X2 == "revodavid" | X3 == "revodavid") %>% droplevels()

# Create empty graph and build it up
clq_g_empty <- graph.empty()
clq_g <- clq_g_empty + vertices(unique(unlist(rev_d)))
for(i in 1:dim(rev_d)[1]){
  clq_g <- clq_g + edges(rev_d[i, 1], rev_d[i, 2])
  clq_g <- clq_g + edges(rev_d[i, 2], rev_d[i, 3])
  clq_g <- clq_g + edges(rev_d[i, 1], rev_d[i, 3])}

# Trim graph and plot using `simplify()`
clq_g_trimmed <- as.undirected(simplify(clq_g))
plot(clq_g_trimmed)


# Find the communities
rt_fgc <-  cluster_fast_greedy(as.undirected(rt_g))
rt_info <- cluster_infomap(as.undirected(rt_g))
rt_clust <- cluster_louvain(as.undirected(rt_g))

# Compare all the communities
compare(rt_fgc, rt_clust, method = 'vi')
compare(rt_info, rt_clust, method = 'vi')
compare(rt_fgc, rt_info, method = 'vi')

# Test membership of the same users
fgc_test <- which(names(membership(rt_fgc)) %in% c("bass_analytics", "big_data_flow"))
membership(rt_fgc)[fgc_test]

info_test <- which(names(membership(rt_info)) %in% c("bass_analytics", "big_data_flow"))
membership(rt_info)[info_test]


# The crossing() function in igraph will return true if a particular edge crosses communities
# This is useful when we want to see certain vertices that are bridges between communities

# Assign cluster membership to each vertex in rt_g using membership()
V(rt_g)$clust <- membership(rt_clust)

# Assign crossing value to each edge
E(rt_g)$cross <- crossing(rt_clust, rt_g)

# Plot the whole graph (this is probably a mess)
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2, 
     coords = layout_with_fr(rt_g), margin = 0, vertex.size = 3, 
     vertex.color = V(rt_g)$clust, edge.color = E(rt_g)$cross+1)

# Create a subgraph with just a few communities greater than 50 but less than 90 in size
mid_comm <- as.numeric(names(sizes(rt_clust)[sizes(rt_clust) > 50 & sizes(rt_clust) < 90 ]))
rt_sg <- induced.subgraph(rt_g, V(rt_g)[ clust %in% mid_comm ])

# Plot the subgraph
plot(rt_sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2, 
     coords = layout_with_fr(rt_sg), margin = 0, vertex.size = 3, 
     vertex.color = V(rt_sg)$clust, edge.color = E(rt_sg)$cross+1)

```
  
  
  
***
  
Chapter 3 - Bike Sharing in Chicago  
  
Creating our graph from raw data:  
  
* Dataset based on Chicago Divvy bike sharing, from freely available data  
	* bike_dat <- read.csv("/Users/edmundhart/wkspace/courses-case-studies-network-r/datasets/bike2_test3.csv", stringsAsFactors = F)  
    * trip_df <- bike_dat %>% group_by(from_station_id, to_station_id) %>% summarise(weights = n())  
    * head(trip_df)  
    * trip_g <- graph_from_data_frame(trip_df[, 1:2])  
    * E(trip_g)$weight <- trip_df$weights  
    * gsize(trip_g)  
    * gorder(trip_g)  
* Can create a sub-graph and run some initial explorations on the network - will notice that there are many loops (trips where to/from is the same)  
	* sg <- induced_subgraph(trip_g, 1:12)  
    * plot(sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.6, margin = 0, vertex.size = 6, edge.width = log(E(sg)$weight+2))  
  
Compare Graph Distance vs. Geographic Distance:  
  
* Graphs do not always reflect the geography well; graph distance may or may not be related to geographic distance  
* Can get graph distances using built in functions  
	* farthest_vertices(trip_g_simp)  
    * get_diameter(trip_g_simp)  
* Can use geographic coding (lat/lon) to find the geographic distances  
	* library(geosphere)  
    * st_to <- bike_dat %>% filter(from_station_id == 336 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    * st_from <- bike_dat %>% filter(from_station_id == 340 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    * farthest_dist <- distm(st_from, st_to, fun = distHaversine)  
    * bike_dist <- function(station_1, station_2, divy_bike_df){  
    *     st1 <- divy_bike_df %>% filter(from_station_id == station_1 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    *     st2 <- divy_bike_df %>% filter(from_station_id == station_2 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    *     farthest_dist <- distm(st1, st2, fun = distHaversine)  
    *     return(farthest_dist)  
    * }
  
Connectivity:  
  
* Can be measured either for vertex or edges - how many need to be removed to create 2 distinct graphs  
	* rand_g <- erdos.renyi.game(10, .4, "gnp", directed = F)  
    * plot(rand_g)  
    * vertex_connectivity(rand_g)  
    * edge_connectivity(rand_g)  
    * min_cut(rand_g, value.only = F)  # more information about the connectivity  
* Can then run comparisons between random graphs and bike-sharing graphs  
	* nv <- gorder(trip_g_ud)  
    * ed <- edge_density(trip_g_ud)  
    * graph_vec <- rep(NA, 1000)  
    * for(i in 1:1000){ w1 <- erdos.renyi.game(nv, ed, "gnp", directed = T) ; graph_vec[i]<- edge_connectivity(w1) }  
  
Example code includes:  
```{r}

bike_dat <- readr::read_csv("./RInputFiles/divvy_bike_sample.csv")
glimpse(bike_dat)

# Create trip_df_subs
trip_df_subs <- bike_dat %>% 
  filter(usertype == "Subscriber") %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_subs <- graph_from_data_frame(trip_df_subs[, 1:2])

# Add edge weights
E(trip_g_subs)$weights <- trip_df_subs$weights / sum(trip_df_subs$weights)

# Now work the same code and filter it down to non-subs
trip_df_non_subs <- bike_dat %>% 
  filter(usertype == "Customer") %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_non_subs <- graph_from_data_frame(trip_df_non_subs[, 1:2])

# Add edge weights
E(trip_g_non_subs)$weights <- trip_df_non_subs$weights / sum(trip_df_non_subs$weights)

# Now let's compare these graphs
gsize(trip_g_subs)
gsize(trip_g_non_subs)


# Create the subgraphs
sg_sub <- induced_subgraph(trip_g_subs, 1:12)
sg_non_sub <- induced_subgraph(trip_g_non_subs, 1:12)

# Plot sg_sub
plot(sg_sub, vertex.size = 20, edge.arrow.width = .8, edge.arrow.size = 0.4, 
     margin = 0, edge.width = E(sg_sub)$weights*10000, main = "Subscribers")

# Plot sg_non_sub
plot(sg_non_sub, vertex.size = 20, edge.arrow.width = .8, edge.arrow.size = 0.4, 
     margin = 0, vertex.size = 10, edge.width = E(sg_non_sub)$weights*10000, 
     main = "Customers")


bike_dist <- function(station_1, station_2, divy_bike_df){ 
   st1 <- divy_bike_df %>% filter(from_station_id == station_1 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)
   st2 <- divy_bike_df %>% filter(from_station_id == station_2 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)
   farthest_dist <- geosphere::distm(st1, st2, fun = geosphere::distHaversine)
   return(farthest_dist)
}


# See the diameter of each graph
get_diameter(trip_g_subs)
get_diameter(trip_g_non_subs)

# Find the farthest vertices
farthest_vertices(trip_g_subs)
farthest_vertices(trip_g_non_subs)

# See how far apart each one is and compare the distances
bike_dist(200, 298, bike_dat)
bike_dist(116, 281, bike_dat)


# Create trip_df
trip_df <- bike_dat %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_df <- graph_from_data_frame(trip_df[, 1:2])

# Add edge weights
E(trip_g_df)$weights <- trip_df$weights / sum(trip_df$weights)


trip_g_simp <- simplify(trip_g_df, remove.multiple=FALSE)
trip_g_simp


# Find the degree distribution
trip_out <- degree(trip_g_simp, mode = "out")
trip_in <- degree(trip_g_simp, mode = "in")

# Create a data frame for easier filtering 
trip_deg <- data.frame(cbind(trip_out, trip_in))
trip_deg$station_id <- names(trip_out)
trip_deg_adj <- trip_deg %>% mutate(ratio = trip_out / trip_in)

# Filter out rarely traveled to stations
trip_deg_filter <- trip_deg_adj %>% filter(trip_out > 10) %>% filter(trip_in > 10) 

# Plot histogram
hist(trip_deg_filter$ratio)

# See which stations were the most skewed using which.min() and which.max()
trip_deg_filter %>% slice(which.min(ratio))
trip_deg_filter %>% slice(which.max(ratio))


# If the weights are the same across all stations, then an unweighted degree ratio would work
# But if we want to know how many bikes are actually flowing, we need to consider weights
# The weighted analog to degree distribution is strength
# We can calculate this with the strength() function, which presents a weighted degree distribution based on the weight attribute of a graph's edges

# Calculate the weighted in and out degrees
trip_out_w <- strength(trip_g_simp, mode = "out")
trip_in_w <- strength(trip_g_simp, mode = "in")

# Create a data frame for easier filtering 
trip_deg_w <- data.frame(cbind(trip_out_w, trip_in_w))
trip_deg_w$station_id <- names(trip_out_w)
trip_deg_w_adj <- trip_deg_w %>% mutate(ratio = trip_out_w / trip_in_w)

# Filter out rarely traveled to stations
trip_deg_w_filter <- trip_deg_w_adj %>% filter(trip_out_w > 10) %>% filter(trip_in_w > 10) 

# Plot histogram of ratio
hist(trip_deg_w_filter$ratio)

# See which stations were the most skewed using which.min() and which.max()
trip_deg_w_filter %>% slice(which.min(ratio))
trip_deg_w_filter %>% slice(which.max(ratio))


latlong <- data.frame(from_longitude=c(-87.656495, -87.660996, -87.6554864, -87.642746, -87.67328, -87.661535, -87.623727, -87.668745, -87.65103, -87.666507, -87.666611), 
                      from_latitude=c(41.858166, 41.869417, 41.8694821, 41.880422, 41.87501, 41.857556, 41.864059, 41.857901, 41.871737, 41.865234, 41.891072)
                      )

# Create a sub graph of the least traveled graph 275
g275 <- make_ego_graph(trip_g_simp,  1, nodes = "275", mode= "out")[[1]]

# Plot graph with geographic coordinates
plot(g275, layout = as.matrix(latlong), vertex.label.color = "blue", vertex.label.cex = .6,
     edge.color = 'black', vertex.size = 15, edge.arrow.size = .1,
     edge.width = E(g275)$weight, main = "Lat/Lon Layout")

# Plot graph without geographic coordinates
plot(g275, vertex.label.color = "blue", vertex.label.cex = .6,
     edge.color = 'black', vertex.size = 15, edge.arrow.size = .1,
     edge.width = E(g275)$weight,
     main = "Default Layout")


# Eigen centrality weighted
ec_weight <- eigen_centrality(trip_g_simp, directed = T, weights = NULL)

# Eigen centrality unweighted
ec_unweight <- eigen_centrality(trip_g_simp, directed = T, weights = NA)

# Closeness weighted
close_weight <- closeness(trip_g_simp, weights = NULL)

# Closeness unweighted
close_unweight <- closeness(trip_g_simp, weights = NA)

# Output nicely with cbind()
cbind(c(
  names(V(trip_g_simp))[which.min(ec_weight$vector)],
  names(V(trip_g_simp))[which.min(close_weight)],
  names(V(trip_g_simp))[which.min(ec_unweight$vector)],
  names(V(trip_g_simp))[which.min(close_unweight)]
  ), c("Weighted Eigen Centrality", "Weighted Closeness", "Unweighted Eigen Centrality", "Unweighted Closeness")
)


trip_g_ud <- as.undirected(trip_g_simp)
trip_g_ud


# Find the minimum number of cuts using min_cut()
ud_cut <- min_cut(trip_g_ud, value.only = FALSE)

# Print the vertex with the minimum number of cuts
print(ud_cut$partition1)

# Make an ego graph
g<- make_ego_graph(trip_g_ud, 1, nodes = "281")[[1]]
plot(g, edge.color = 'black', edge.arrow.size = .1)

# Print the value
print(ud_cut$value)

# Print cut object
print(ud_cut$cut)

far_stations <- c("231", "321")
close_stations <- c("231", "213")

# Compare the output of close and far vertices
stMincuts(trip_g_simp, far_stations[1], far_stations[2])$value
stMincuts(trip_g_simp, close_stations[1], close_stations[2])$value


# Find the actual value
clust_coef <- transitivity(trip_g_simp, type = "global")

# Get randomization parameters using gorder() and edge_density()
nv <- gorder(trip_g_simp)
ed <- edge_density(trip_g_simp)

# Create an empty vector to hold output of 300 simulations
graph_vec <- rep(NA, 300)

# Calculate clustering for random graphs
for(i in 1:300){
  graph_vec[i]<- transitivity(erdos.renyi.game(nv, ed, "gnp", directed = T), type = "global")
}

# Plot a histogram of the simulated values
hist(graph_vec, xlim = c(.35, .6), main = "Unweighted clustering randomization")

# Add a line with the true value
abline(v = clust_coef, col = "red")


# Find the mean local weighted clustering coeffecient
m_clust <- mean(transitivity(trip_g_simp, type = "weighted"))
nv <- gorder(trip_g_simp)
ed <- edge_density(trip_g_simp)
graph_vec <- rep(NA, 100)

for(i in 1:100){
  g_temp <- erdos.renyi.game(nv, ed, "gnp", directed = T)
  # Sample existing weights and add them to the random graph
  E(g_temp)$weight <- sample(x = E(trip_g_simp)$weights, size = gsize(g_temp), replace = TRUE)
  graph_vec[i]<- mean(transitivity(g_temp, type = "weighted"))
}

# Plot a histogram of the simulated values
hist(graph_vec, xlim = c(.35, .7), main = "Unweighted clustering randomization")

# Add a line with the true value
abline(v = m_clust ,col = "red")

```
  
  
  
***
  
Chapter 4 - Other Ways to Visualize Graph Data  
  
Other packages for plotting graphs:  
  
* Base plotting in igraph is good for quick visualizations, but other libraries can make great plots simply  
* Can use the ggplot syntax with ggnet, for example  
	* library(ggnetwork)  
    * library(igraph)  
    * library(GGally)  
    * library(intergraph)  
* The basic igraph plotting is as follows  
	* rand_g <- erdos.renyi.game(30, .15, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * plot(rand_g)  
* The basic ggnet plotting requires use of asNetwork on the core graph  
	* net <- asNetwork(rand_g)  
    * ggnet2(net)  
* The basic ggnetwork plotting is more similar to ggplot2  
	* gn <- ggnetwork(rand_g)  
    * g <- ggplot(gn, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges() + geom_nodes() + theme_blank()  
    * plot(g)  
* Where an igraph might require voluminous code for extensions, the ggnet or ggnetwork can be extended in a much simpler manner  
	* ggnet2(net, node.size = "cent", node.color = "comm", edge.size = .8, color.legend = "Community Membership", color.palette = "Spectral", edge.color = c("color", "gray88"), size.cut = T, size.legend = "Centrality")  
    * g <- ggplot(gn, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(aes(color = as.factor(comm))) + geom_nodes(aes(color = as.factor(comm), size = cent)) + theme_blank() + guides(color = guide_legend(title = "Community"), size = guide_legend(title = "Centrality"))  
    * plot(g)  
  
Interactive visualizations:  
  
* Can use R to create interactive javascript plots for outputs to html or similar  
* Examples can be built on a simple random graph  
	* library(ggiraph)  
    * library(htmlwidgets)  
    * library(networkD3)  
    * rand_g <- erdos.renyi.game(30, .12, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * V(rand_g)$cent <- betweenness(rand_g)  
* Can plot using ggplot2 and ggiraph  
	* g <- ggplot(ggnetwork(rand_g), aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(color = "black") + geom_nodes(aes(size = cent)) + theme_blank() + guides(size = guide_legend(title = "Centrality"))  
    * my_gg <- g + geom_point_interactive(aes(tooltip = round(cent, 2)), size = 2)  # Create ggiraph object  
    * ggiraph(code = print(my_gg))  # Display ggiraph object  
* Can also customize the ggiraph call using valid CSS, for example using  
	* hover_css = "cursor:pointer;fill:red;stroke:red;r:5pt"  
    * data_id = round(cent, 2)), size = 2)  
    * ggiraph(code = print(my_gg), hover_css = hover_css, tooltip_offx = 10, tooltip_offy = -10)  
* Can also plot with networkD3 which is easy to use but does not allow for much customization  
	* nd3 <- igraph_to_networkD3(rand_g)  
    * simpleNetwork(nd3$links)  
* Can add complexity to the D3 plot by specifying information about the nodes and edges to be plotted insinde forceNetwork()  
	* nd3$nodes$group = V(rand_g)$comm  
    * nd3$nodes$cent = V(rand_g)$cent  
    * forceNetwork(Links = nd3$links, Nodes = nd3$nodes, Source = 'source', Target = 'target', NodeID = 'name', Group = 'group', Nodesize = 'cent', legend = T, fontSize = 20)  
  
Alternative visualizations:  
  
* Hairball plots are designed to maximize spacing between vertices, but as plots get large the information is hard if not impossible to interpret  
* One potential solution is the hive plot  
	* library(HiveR)  
    * rand_g <- erdos.renyi.game(18, .3, "gnp", directed = T)  
    * plot(rand_g, vertex.size = 7)  # standard igraph plot  
    * rand_g_df <- as.data.frame(get.edgelist(rand_g))  
    * rand_g_df$weight <- 1  
    * rand_hive <- edge2HPD(edge_df = rand_g_df)  
    * rand_hive$nodes$axis <- sort(rep(1:3, 6))  
    * rand_hive$nodes$radius <- as.double(rep(1:6, 3))  
    * plotHive(rand_hive, method="abs", bkgnd="white")  
* Can modify hive plots by way of either nodes or edges  
	* # Setting location of each node  
    * rand_hive$nodes$axis <- sort(rep(1:3, 6))  
    * rand_hive$nodes$radius <- as.double(rep(1:6, 3))  
    * # Add weights to each edge  
    * rand_hive$edges$weight <- as.double(rpois(length(rand_hive$edges$weight), 5))  
    * # Add color based on edge origination  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 1:6] <- 'red'  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 7:12] <- 'blue'  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 13:18] <- 'green'  
    * # Plot  
    * plotHive(rand_hive, method = "abs", bkgnd = "white")  
* Another alternative is the biofabric plot  
	* # Create random graph  
    * rand_g <- erdos.renyi.game(10, .3, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * # Add names to vertices  
    * V(rand_g)$name <- LETTERS[1:length(V(rand_g))]  
    * # Create biofabric plot  
    * biofbc <- bioFabric(rand_g)  
    * bioFabric_htmlwidget(biofbc)  
  
Example code includes:  
```{r cache=TRUE}

verts <- c(1185, 3246, 1684, 3634, 3870, 188, 2172, 3669, 2267, 1877, 3931, 1862, 2783, 2351, 423, 3692, 1010, 173, 1345, 3913, 3646, 2839, 2624, 4072, 2685, 2901, 2227, 2431, 1183, 602, 3937, 3688, 2823, 3250, 101, 1951, 3097, 884, 1299, 945, 583, 1691, 1687, 1504, 622, 566, 949, 1897, 1083, 3491, 187, 1799, 3249, 496, 2280, 840, 519, 3060, 4115, 1520, 2700, 385, 1558, 1113, 3303, 1818, 3283, 3291, 3218, 1781, 3055, 2547, 2874, 3, 1923, 890, 1536, 2477, 1422, 449, 984, 2697, 1686, 3181, 415, 1754, 3972, 3600, 3573, 706, 527, 2631, 1383, 2644, 1290, 756, 3147, 377, 4109, 2056, 2411, 1337, 1963, 3833, 1939, 4030, 4111, 2442, 1647, 590, 3749, 1208, 244, 3796, 2886, 570, 2199, 3818, 2342, 1618, 2591, 1279, 1230, 878, 1476, 3930, 616, 364, 567, 2753, 2470, 3554, 2683, 2938, 2077, 2629, 3273, 3131, 3900, 1749, 1240, 1629, 42, 731, 3350, 919, 950, 305, 976, 2906, 3363, 1974, 1539, 978, 441, 1546, 4110, 860, 1762, 864, 1989, 1401, 2572, 1482, 1406, 2110, 2926, 874, 1631, 1050, 2488, 726, 3408, 2946, 2636, 2437, 1468, 2089, 3447, 2292, 3308, 1231, 2788, 1043, 2339, 1893, 3935, 2220, 3589, 3544, 1077, 1263, 4114, 2434, 3679, 1831, 1596, 2585, 598, 2246, 936, 3770, 2355, 2017, 1576, 3445, 1425, 1128, 668, 674, 1884, 989, 845, 2634, 4068, 2736, 1374, 3922, 3202, 3583, 1102, 3746, 2838, 2674, 206, 3966, 1860, 2180, 2717, 3562, 2405, 1666, 2107, 228, 1014, 1543, 768, 3229, 594, 3117, 2121, 2568, 666, 2454, 1209, 2807, 1545, 3753, 3744, 2812, 995, 858, 2293, 1034, 2053, 3034, 650, 1562, 1821, 3351, 3572, 3402, 2600, 3663, 1991, 2222, 1296, 1338, 78, 1936, 3352, 25, 278, 632, 2962, 2826, 3734, 1792, 286, 2491, 2912, 4028, 1522, 863, 223, 1518, 249, 866, 210, 2567, 1140, 386, 276, 3368, 2885, 3122, 3754, 396, 379, 3051, 2996, 36, 2973, 4106, 2404, 1834, 3920, 32, 1724, 1876, 1484, 1769, 2715, 211, 1350, 3054, 3178, 904, 1346, 3256, 3243, 1124, 559, 2672, 394, 128, 3790, 133, 1283, 3468, 3934, 1085, 2794, 3157, 1190, 1864, 2638, 2426, 2435, 3696, 1567, 451, 1987, 850, 1836, 1397, 3710, 1465, 865, 2350, 515, 3645, 1940, 614, 2341, 3711, 2516, 3914, 1216, 3140, 541, 725, 3369, 1157, 1364, 2943, 3947, 67, 1525, 1812, 1582, 1285, 4117, 1705, 1999, 3608, 2899, 782, 1155, 3632, 2187, 2844, 1393, 2873, 2008, 3412, 692, 1053, 355, 785, 3643, 1105, 2706, 2927, 393, 893, 1007, 4021, 439, 3687, 3667, 510, 3365, 2141, 1469, 1671, 2623, 307, 1259, 2526, 1176, 3083, 798, 1845, 1023, 712, 3520, 1191, 1771, 104, 2025, 2382, 2204, 3784, 3292, 2313, 1119, 1433, 593, 3182, 3516, 2079, 1215, 3673, 3831, 2257, 399, 1793, 366, 3690, 1041, 2147, 2690, 609, 3184, 2603, 2793, 540, 1315, 2471, 1922, 3792, 882, 214, 867, 3261, 3816, 2737, 3990, 457, 3566, 1595, 1697, 605, 2138, 990, 841, 2524, 1033, 2958, 343, 2998, 1559, 2756, 2414, 1620, 2285, 2, 791, 2566, 783, 2961, 1120, 2500, 3390, 421, 464, 2463, 4056, 3029, 3525, 256, 1668, 2544, 316, 3598, 917, 180, 2485, 2848, 1280, 1326, 1039, 290, 1321, 644)
verts <- c(verts, 1937, 1820, 3733, 1232, 1677, 298, 3102, 1427, 2653, 619, 1639, 2774, 226, 2934, 1084, 1312, 1123, 135, 1865, 2440, 3245, 92, 3551, 1088, 3370, 2467, 1604, 2928, 142, 2648, 1250, 2970, 1918, 983, 2866, 328, 2976, 3653, 2692, 4099, 291, 3819, 2864, 1375, 1169, 732, 2031, 3166, 1888, 2092, 2372, 1887, 1816, 58, 170, 3306, 3903, 715, 2312, 2323, 1404, 3824, 1942, 3142, 1964, 3214, 2084, 1502, 3366, 2513, 1464, 66, 2007, 1735, 3109, 2876, 3021, 1301, 3089, 535, 996, 3916, 3451, 2057, 1858, 215, 3417, 424, 312, 3103, 1791, 1189, 3149, 113, 835, 2415, 794, 3636, 612, 2816, 514, 2889, 1162, 1313, 2210, 339, 3850, 3481, 2047, 2739, 3124, 2643, 3428, 155, 3161, 3027, 2711, 1317, 148, 1273, 956, 2969, 1265, 1063, 3899, 3945, 1597, 2543, 363, 767, 3322, 2618, 2850, 1454, 2066, 2778, 3534, 1339, 314, 2174, 2589, 297, 3932, 2132, 2612, 3180, 1649, 1966, 2552, 3581, 3148, 196, 1741, 1213, 2924, 3936, 406, 3631, 813, 259, 3230, 543, 2233, 599, 70, 1797, 3607, 975, 1448, 2022, 2777, 696, 1581, 1542, 2523, 2457, 2857, 3046, 3272, 1891, 3681, 586, 1644, 871, 137, 2176, 1849, 480, 972, 1996, 565, 330, 1466, 1217, 2888, 889, 80, 3487, 1143, 2157, 3594, 3747, 634, 1463, 2150, 1775, 2247, 2484, 1658, 1309, 24, 13, 3383, 367, 1423, 2439, 2522, 3637, 2064, 3639, 4046, 2078, 3676, 3506, 1413, 2964, 2192, 3130, 4078, 1069, 2720, 3344, 1090, 5, 3848, 501, 167, 3915, 3787, 4049, 3986, 233, 2343, 3196, 3918, 4063, 537, 242, 3809, 1648, 1662, 2986, 124, 685, 1726, 4087, 1932, 3999, 1910, 484, 489, 1382, 2289, 2189, 3067, 2722, 2262, 2702, 429, 839, 1109, 1361, 2123, 4058, 3959, 2735, 52, 2183, 2707, 1538, 678, 63, 943, 3047, 3108, 1806, 730, 1628, 2664, 1355, 345, 932, 1201, 861, 3861, 1214, 403, 156, 3429, 3210, 3355, 1583, 2479, 3508, 164, 2299, 3320, 2923, 2562, 460, 4013, 417, 1947, 1853, 2272, 1027, 1997, 3266, 2449, 250, 1486, 177, 1118, 3644, 14, 2538, 3836, 2368, 3349, 1879, 2310, 3413, 4032, 319, 3155, 2413, 3842, 3724, 1802, 3319, 2940, 31, 773, 426, 1067, 2374, 3240, 2335, 4010, 3398, 3096, 392, 245, 2898, 4026, 138, 2109, 1526, 2011, 881, 512, 372, 1650, 3373, 3659, 552, 2474, 1712, 3786, 2185, 43, 3406, 2890, 3504, 348, 2982, 2186, 481, 4018, 3048, 1360, 962, 838, 720, 1826, 4011, 2161, 1763, 2617, 2447, 65, 1227, 3938, 2569, 3662, 1746, 2742, 4020, 2148, 1643, 2450, 4093, 3905, 230, 3401, 168, 2779, 1847, 1006, 3074, 1894, 1702, 1229, 3704, 2586, 3595, 1163, 3661, 2230, 3236, 1111, 1770, 438, 2504, 2828, 651, 2456, 1900, 3050, 506, 1674, 3477, 2766, 76, 3606, 3630, 1237, 3617, 295, 3512, 1286, 3623, 3495, 964, 3407, 494, 3629, 140, 1178, 3045, 2041, 194, 3852, 3800, 1605, 1420, 1968, 442, 3570, 1796, 1729, 369, 2401, 1507, 2462, 145, 2580, 848, 4043, 3443, 2979, 22, 3727, 1316, 1437, 3450, 3590, 3465, 3188, 2373, 432, 3425, 3449, 1356, 273, 700, 1789, 1251, 1767, 3998, 2005, 1222, 2214, 340)

# Create subgraph of rt_g
rt_samp <- induced_subgraph(rt_g, verts)

# Convert from igraph using asNetwork()
net <- intergraph::asNetwork(rt_samp)

# Plot using igraph
plot(rt_samp, vertex.label = NA, edge.arrow.size = 0.2, edge.size = 0.5, 
     vertex.color = "black", vertex.size = 1
     )

# Plot using ggnet2
GGally::ggnet2(net, node.size = 1, node.color = "black", edge.size = .4)


# Raw plot of rt_samp using ggnetwork()
library(ggnetwork)
library(GGally)

ggplot(ggnetwork(rt_samp, arrow.gap = .01) , aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), color = "black") +
    geom_nodes(size = 4) 

# Prettier plot of rt_samp using ggnetwork()
ggplot(ggnetwork(rt_samp, arrow.gap = .01),aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), color = "black", curvature = .2) +
    geom_nodes(size = 4) + theme_blank()

# NEED TO FIX!
rt_keys <- sort(table(vertex_attr(rt_g)$clust), decreasing=TRUE)
# rt_drops <- names(rt_keys)[11:length(rt_keys)]
# vt_drops <- which(vertex_attr(rt_g)$clust %in% rt_drops)
# rt_use <- delete_vertices(rt_g, vt_drops)
rt_use <- induced_subgraph(rt_g, which(V(rt_g)$clust %in% names(rt_keys[1:10])))

# Convert to a network object
net <- intergraph::asNetwork(rt_use)
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral"
       )

# Now remove the centrality legend by setting size to false in the guide() function
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral"
       ) + 
    guides( size = FALSE)

# Add edge colors
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral", 
       edge.color = c("color", "gray88")) +
  guides( size = FALSE)


# NEED TO CREATE rt_g_smaller!
# Basic plot where we set parameters for the plots using geom_edegs() and geom_nodes()
# ggplot(ggnetwork(rt_g_smaller, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
#   geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), curvature = .2, color = "black") + 
#   geom_nodes(size = 4, aes(color = comm)) + 
#   theme_blank()

# Added guide legend, changed line colors, added size 
# ggplot(ggnetwork(rt_g_smaller, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
#   geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), curvature = .2, lwd = .3, aes(color=comm)) +
#   geom_nodes(aes(color = comm, size = cent)) + 
#   theme_blank() +  
#   guides(color = guide_legend(title = "Community"), size = guide_legend(title = "Centrality"))


# NEED TO FIX!
# Add betweenness centrality using betweenness()
V(trip_g_simp)$cent <- igraph::betweenness(trip_g_simp)

# Create a ggplot object with ggnetwork to render using ggiraph
g <- ggplot(ggnetwork(trip_g_simp, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(size = cent)) + 
    theme_blank() 
plot(g)

# Create ggiraph object and assign the tooltip to be interactive
my_gg <- g + ggiraph::geom_point_interactive(aes(tooltip = round(cent, 2), 
                                                 data_id = round(cent, 2)
                                                 ), size = 2
                                             ) 

# Define some hover css so the cursor turns red
hover_css = "cursor:pointer;fill:red;stroke:red;r:3pt"
# ggiraph::ggiraph(code = print(my_gg), hover_css = hover_css, tooltip_offx = 10, tooltip_offy = -10)


# Add community membership as a vertex attribute using the cluster_walktrap algorithm
V(rt_g)$comm <- membership(cluster_walktrap(rt_g))

# Create an induced_subgraph
rt_sub_g <- induced_subgraph(rt_g, which(V(rt_g)$comm %in% 10:13))

# Plot to see what it looks like without an interactive plot using ggnetwork
ggplot(ggnetwork(rt_sub_g, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(color = as.factor(comm))) + 
    theme_blank()  

# Convert to a networkD3 object
# nd3 <- igraph_to_networkD3(rt_sub_g)

# Assign grouping factor as community membership
# nd3$nodes$group = V(rt_sub_g)$comm

# Render your D3.js graph
# forceNetwork(Links = nd3$links, Nodes = nd3$nodes, Source = 'source', 
#              Target = 'target', NodeID = 'name', Group = 'group', legend = T, fontSize = 20
#              )

# Convert  trip_df to hive object using edge2HPD()
# bike_hive <- edge2HPD(edge_df =  as.data.frame(trip_df))

# Assign to trip_df edgecolor using our custom function
# trip_df$edgecolor <- dist_gradient(trip_df$geodist)

# Calculate centrality with betweenness()
# bike_cent <- betweenness(trip_g)

# Add axis and radius based on longitude and radius
# bike_hive$nodes$radius<- ifelse(bike_cent > 0, bike_cent, runif(1000, 0, 3))

# Set axis as integers and axis colors to black
# bike_hive$nodes$axis <- as.integer(dist_stations$axis)
# bike_hive$axis.cols <- rep("black", 3)

# Set the edge colors to a heatmap based on trip_df$edgecolor
# bike_hive$edges$color <- trip_df$edgecolor
# plotHive(bike_hive, method = "norm", bkgnd = "white")


# Add community membership as a vertex attribute
V(rt_g)$comm <- membership(cluster_walktrap(rt_g))

# Create a subgraph
rt_sub_g <- induced_subgraph(rt_g, which(V(rt_g)$comm %in% 10:15))

# Plot to see what it looks like without an interactive plot
ggplot(ggnetwork(rt_sub_g, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(color = as.factor(comm)))+ theme_blank() +
    theme(legend.position = "none")

# Make a Biofabric plot htmlwidget
# rt_bf <- bioFabric(rt_sub_g)
# bioFabric_htmlwidget(rt_bf)


# Create a dataframe of start and end latitude and longitude and add weights
# ll_to_plot <- bike_dat %>% group_by(from_station_id, to_station_id, from_latitude, 
#                                     from_longitude, to_latitude, to_longitude, usertype
#                                     ) %>% 
#     summarise(weight = n())

# Create a base map with station points with ggmap()
# ggmap(chicago) + 
#     geom_segment(data = ll_to_plot, aes(x = from_longitude, y = from_latitude, 
#                                         xend = to_longitude, yend = to_latitude, 
#                                         colour = usertype, size = weight
#                                         ), alpha = .5
#                )

```
  
  
  
***
  
### _Fundamentals of Bayesian Analysis in R:_  
  
Chapter 1 - What is Bayesian Analysis?  
  
Introduction:  
  
* British team spearheaded by Turing found ways to decrypt German communications in 1941  
	* Key to Turing's success was the use of Bayesian methods, which wree not very widely used  
* Bayesian inference is "A method for figuring out unobservable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be"  
	* The unknown was the configuration of the wheels in the encryption machine - British already knew what a given wheel configuration would produce  
    * Turing worked backwards to figure out the probable configuration of the wheels from the messages that he had received  
* Bayesian analysis is flexible and can be problem-specific and customized to a specific dataset and analysis need  
  
Bayesian data analysis - named for Thomas Bayes from the early-mid 1700s:  
  
* Bayesian data analysis is about probabilistic inference for learning from data and drawing conclusions  
* For this specific course, probability will be a statement about the certainty (p=1 means certain yes, p=0 means certain no), though there are other definitions that can be used  
	* Probability need not be only about yes/no statements, and can be associated to distributions such as "amount of rainfall tomorrow"  
    * "The role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has been learned from data."  
* Example of using a Bayesian approach to patients  
	* prop_model(data) is a function that has been created to plot out probabilities vs. p  
    * The data is a vector of successes and failures represented by 1s and 0s  
    * There is an unknown underlying proportion of success  
    * If data point is a success is only affected by this proportion  
    * Prior to seeing any data, any underlying proportion of success is equally likely  
    * The result is a probability distribution that represents what the model knows about the underlying proportion of success  
  
Samples and posterior samples:  
  
* Prior probability distribution is a distribution PRIOR to updating with some data; for example, equally probable that p falls anywhere between 0 and 1  
* Posterior probability distribution is a distribution AFTER updating with what is learned by seeing some data  
* Can be valuable to have a vector of potential outcomes, appropriately weighted by the likelihood of each of the outcomes  
  
Chapter wrap-up:  
  
* Can draw conclusions about the probabilities based on even a small sample of data observed  
* Next chapters will cover mechanics of Bayesian inference in more detail  
  
Example code includes:  
```{r}

prop_model <- function(data = c(), prior_prop = c(1, 1), n_draws = 10000) {
    data <- as.logical(data)
    proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
    data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 20)))

    post_curves <- map_dfr(data_indices, function(i) {
        value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", "Failure"))
        label <- paste0("n=", i)
        probability <- dbeta(proportion_success, prior_prop[1] + sum(data[seq_len(i)]), 
                             prior_prop[2] + sum(!data[seq_len(i)])
                             )
        probability <- probability / max(probability)
        data_frame(value, label, proportion_success, probability)
        }
    )
    post_curves$label <- fct_rev(factor(post_curves$label, levels =  paste0("n=", data_indices )))
    post_curves$value <- factor(post_curves$value, levels = c("Prior", "Success", "Failure"))
  
    p <- ggplot(post_curves, aes(x = proportion_success, y = label, height = probability, fill = value)) +
        ggridges::geom_density_ridges(stat="identity", color = "white", 
                                      alpha = 0.8, panel_scaling = TRUE, size = 1
                                      ) +
        scale_y_discrete("", expand = c(0.01, 0)) +
        scale_x_continuous("Underlying proportion of success") +
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", 
                          drop = FALSE, labels =  c("Prior   ", "Success   ", "Failure   ")
                          ) +
        #ggtitle(paste0("Binomial model - Data: ", sum(data),  " successes, " , sum(!data), " failures"))  +
        theme_light(base_size = 18) +
        theme(legend.position = "top")
    print(p)
  
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data)))
}


# Define data and run prop_model
data = c(1, 0, 0, 1)
prop_model(data)


# Define data and run prop_model
data = c(1, 0, 0, 1)
prop_model(data)


data = c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
posterior <- prop_model(data)
head(posterior)
hist(posterior, breaks = 30, xlim = c(0, 1), col = "palegreen4")

# Get some more information about posterior
median(posterior)
quantile(posterior, c(0.05, 0.95))
sum(posterior > 0.07) / length(posterior)

```
  
  
  
***
  
Chapter 2 - How Does Bayesian Inference Work?  
  
Parts needed for Bayesian inference:  
  
* Bayesian inference requires priors (what is known before seeing data), generative model, and data  
	* The generative model is a formula or computer expression that can generate simulated data based on provided input parameters  
    * For example, could assume that there is a proportion of zombies cured and a number of zombies treated and then simulate data based on these  
  
Using a generative model:  
  
* The binomial distribution function can be very helpful as a generative model for summing probabilities of single 1/0 events  
* Typically in data analysis, we know the outcome and want to figure out the likely parameters in our generative model (that is the Bayesian inference)  
  
Repressing uncertainty with priors:  
  
* The prior reflects our certainty/uncertainty in the parameters prior to running the analysis  
	* Example could be a uniform distribution from (a, b)  
    * proportion_clicks <- runif(n = 6, min = 0.0, max = 1.0)  # sample 6 values that are between 0 and 1 with every number being equally likely  
    * n_clicks <- rbinom(n = 6, size = 100, proportion_clicks)  # rbinom will vectorize over proportion_clicks  
  
Bayesian models and conditioning:  
  
* The Bayesian model is based on the generative model and the prior  
	* prior <- data.frame(proportion_clicks, n_visitors)  # joint PDF over proportion_clicks and n_visitors  
* Can then condition on the observed data and assess the joint PDF for the implications on the distribution of the underlying proportion  
	* "Bayesian inference is conditioning on data, in order to learn about parameter values."  
  
Chapter wrap-up:  
  
* Used the binomial model as an assumed generative function and a uniform distribution as the prior probabilities  
* Calculated joint probability distributions and then found a probability distribution based on a known outcome (data)  
* The posterior can then be used as the prior for future analyses, and can be repeated indefinitely  
* Bayesian machinery from simple cases can be extended to more complex cases  
	* Just need a generative model and an assumption for the prior  
    * Need a computational model that can scale easily  
  
Example code includes:  
```{r}

# Generative zombie drug model
# Parameters
prop_success <- 0.42
n_zombies <- 100
# Simulating data
data <- c()
for(zombie in 1:n_zombies) {
  data[zombie] <- runif(1, min = 0, max = 1) < prop_success
}
data <- as.numeric(data)
data
data_counts <- sum(as.numeric(data))
data_counts


# Try out rbinom
rbinom(n = 1, size = 100, prob = 0.42)

# Try out rbinom
rbinom(n = 200, size = 100, prob = 0.42)


# Fill in the parameters
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = proportion_clicks)

# Visualize the results
hist(n_visitors)


# Update proportion_clicks
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n = n_samples, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)

# Visualize the results
hist(n_visitors)
hist(proportion_clicks)


# Create prior
prior <- data.frame(proportion_clicks, n_visitors)
head(prior)

# Create posterior
posterior <- prior[prior$n_visitors==13, ]
hist(posterior$proportion_clicks)


prior <- posterior
head(prior)
prior$n_visitors <- rbinom(nrow(prior), size=100, prob=prior$proportion_clicks)
hist(prior$n_visitors)
mean(prior$n_visitors >= 5)

```
  
  
  
***
  
Chapter 3 - Why Use Bayesian Data Analysis?  
  
Four good things with Bayes:  
  
* Many good tools exist for Bayesian analysis, and those will be covered in Chapter 5  
* The main reasons for Bayesian analysis are the flexibility and power  
	* You can include information sources such as expertise in addition to the data  
    * You can make any comparisons between groups or data sets  
    * You can use the results of Bayesian analysis for Decision Analysis  
    * You can change the underlying statistical model  
* Background information, common knowledge, or expertise can be incorporated in to the prior  
	* Can also exclude all such information by assuming a uniform distribution for the prior  
    * The beta distribution can be useful for a proportion, and is set based on alpha and beta  
  
Contrasts and comparisons:  
  
* The more data, the more likely that the posterior is informed by the data than by the prior  
* There is often a benefit to comparing groups - for example, two different treatments or two different campaigns  
	* Can compare the posterior distributions from the two groups, which is fairly simple since each distribution is contained in a vector  
    * posterior$prop_diff <- posterior$video_prop - posterior$text_prop  
  
Decision analysis:  
  
* Can calculate median, mean, credible interval, likelihood of more extreme than a certain parameter, etc.  
* The results of a Bayesian analysis can be used for Decision analysis  
* Can add dimensions such as cost and revenue, then compare profitability  
	* video_cost <- 0.25  
    * text_cost <- 0.05  
    * visitor_spend <- 2.53  
    * posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost  
    * posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost  
    * posterior$profit_diff <- posterior$video_profit - posterior$text_profit  
  
Change anything and everything:  
  
* There can be large uncertainty as to the outcomes, particularly if the data sizes are small and the key metrics are close to the critical parameter  
* Can switch out the generating functions and re-run the approach - example of a banner that is pay-per-day rather than pay-per-impression  
	* One option is to split each day in to 1440 minutes and assume a probability of success (1/0), which has drawbacks  
    * In the limiting case of smaller and smaller slices, the Poisson distribution is created - has only a single parameter, the expected value of successes  
    * n_clicks <- rpois(n = 100000, labmda = 20)  
  
Bayes is optimal, kind of . . .   
  
* Bayesian analysis can be useful in many ways as shown in this chapter  
* Bayesian analysis is (kind of) optimal, provided that the generative model is perfectly precise and accurate  
	* In the world defined by the model the Bayesian approach is optimal  
    * However, no statistical model can ever be optimal in the real world  
  
Example code includes:  
```{r}

# Draw from the beta distribution
beta_sample <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)

# Explore the results
hist(beta_sample)

# Draw from the beta distribution
beta_sample <- rbeta(n = 10000, shape1 = 100, shape2 = 100)

# Explore the results
hist(beta_sample)

# Draw from the beta distribution
beta_sample <- rbeta(n = 10000, shape1 = 100, shape2 = 20)

# Explore the results
hist(beta_sample)


n_draws <- 100000
n_ads_shown <- 100

# Update proportion_clicks
proportion_clicks <- rbeta(n_draws, shape1 = 5, shape2 = 95)
n_visitors <- rbinom(n_draws, size = n_ads_shown, prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 13, ]

# Plots the prior and the posterior in the same plot
par(mfcol = c(2, 1))
hist(prior$proportion_clicks, 
     xlim = c(0, 0.25))
hist(posterior$proportion_clicks, 
     xlim = c(0, 0.25))
# Reset mfcol below

# Define parameters
n_draws <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_draws, size = n_ads_shown, prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)

# Create posteriors
posterior_video <- prior[prior$n_visitors == 13, ]
posterior_text <- prior[prior$n_visitors == 6, ]

# Visualize posteriors
hist(posterior_video$proportion_clicks, xlim = c(0, 0.25))
hist(posterior_text$proportion_clicks, xlim = c(0, 0.25))


posterior <- data.frame(video_prop = posterior_video$proportion_clicks[1:4000], 
                        text_prop  = posterior_text$proportion_click[1:4000]
                        )
    
# Create prop_diff
posterior$prop_diff <- posterior$video_prop - posterior$text_prop

# Plot your new column
hist(posterior$prop_diff)

# Explore prop_diff
median(posterior$prop_diff)
mean(posterior$prop_diff > 0)


visitor_spend <- 2.53
video_cost <- 0.25
text_cost <- 0.05

posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost
posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost
head(posterior)
hist(posterior$video_profit)
hist(posterior$text_profit)


posterior$profit_diff <- posterior$video_profit - posterior$text_profit
head(posterior)
hist(posterior$profit_diff)
median(posterior$profit_diff)
mean(posterior$profit_diff < 0)


x <- rpois(n = 10000, lambda = 3)
hist(x)

x <- rpois(n = 10000, lambda = 11.5)
hist(x)

x <- rpois(n = 10000, lambda = 11.5)
mean(x >= 15)


n_draws <- 100000
n_ads_shown <- 100
mean_clicks <- runif(n_draws, min = 0, max = 80)
n_visitors <- rpois(n_draws, lambda=mean_clicks)
                     
prior <- data.frame(mean_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 19, ]

hist(prior$mean_clicks)
hist(posterior$mean_clicks)

# Reset to default
par(mfcol = c(1, 1))

```
  
  
  
***
  
Chapter 4 - Bayesian Inference with Bayes' Theorem  
  
Probability rules:  
  
* The computation method used so far does not scale well, but there are alternatives  
* Bayesian statistics is a hot research error, and there are many methods to get to the same results in a faster method  
* Probability is defined as a statement of certainty/uncertainty between 0 and 1 (may be a distribution or an allocation of probabilities over all possible values)  
* Conditional probabilities are often of interest - P(A | B) is the probability of A given that B has occurred  
	* Can also get a conditional probability distribution  
* Sometimes probabilities can be summed - when they are exclusive and the goal is to get 1 of them  
* Sometimes probabilities can be multiplied - when they are independent and the goal is to get all of them  
  
Calculating likelihoods:  
  
* Can simulate or calculate probabilities - simulate and count with a common generative model and small dataset, or calculate using an optimized formula  
* For common distributions, can use the density functions such as dbinom() to get the key probabilities in many cases  
	* dbinom(13, size = 100, prob = 0.1) + dbinom(14, size = 100, prob = 0.1)  # probability of getting 13 or 14 successes in 100 trials with success 0.1 per trial  
    * n_visitors = seq(0, 100, by = 1)  
    * probability <- dbinom(n_visitors, size = 100, prob = 0.1)  
* For continuous distributions such as the uniform, there is no specific probability of any given value  
	* Probability densities are returned instead, and can be viewed as the relative probabilities  
  
Bayesian calculation:  
  
* Conditioning on the observed data is at the core of Bayesian inference  
* Example of converting previous simulations to precise calculations  
	* n_ads_shown <- 100  
    * n_visitors <- seq(0, 100, by = 1)  # full potential range of visitors based on ads  
    * proportion_clicks <- seq(0, 1, by = 0.01)  # fine grid of values even if it does not fully sample the given space  
    * pars <- expand.grid(proportion_clicks = proportion_clicks, n_visitors = n_visitors)  # all possible combinations of n_visitors and proportion_clicks  
    * pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)  # prior is of uniform density  
    * pars$likelihood <- dbinom(pars$n_visitors, size = n_ads_shown, prob = pars$proportion_clicks)  # likelihood given the rows in pars  
    * pars$probability <- pars$likelihood * pars$prior  # unscaled posterior probability  
    * pars$probability <- pars$probability / sum(pars$probability)  # normalized posterior probability  
    * pars <- pars[pars$n_visitors == 13, ]  # filter on the 13 observed clicks  
    * pars$probability <- pars$probability / sum(pars$probability)  # normalize remaining probs to 1  
  
Bayes theorem:  
  
* An example of Bayes' theorem is provided above by the multiplication of the prior and the probability  
	* pars$probability <- pars$likelihood * pars$prior  
    * pars$probability <- pars$probability / sum(pars$probability)  
    * This is an example of p(Param | Data) = P(Data | Param) * P(Param Before Seeing Data) / sum-of-all-numerators  
* The grid approximation technique was used above - cannot ever get all parameters for a continuous distribution  
* Can use mathematical notations where = is a point parameter and ~ is follows a specific distribution  
  
Example code includes:  
```{r}

prob_to_draw_ace <- 4 / 52
prob_to_draw_four_aces <- (4 / 52) * (3 / 51) * (2 / 50) * (1 / 49)


n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n = 99999, 
    size = n_ads_shown, prob = proportion_clicks)
prob_13_visitors <- sum(n_visitors == 13) / length(n_visitors)
prob_13_visitors

prob_13_visitors <- dbinom(x=13, size=n_ads_shown, prob=proportion_clicks)
prob_13_visitors


n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- 0:n_ads_shown
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
plot(x=n_visitors, y=prob, type="h")

n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 13
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
plot(x=proportion_clicks, y=prob, type="h")


n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- seq(0, 100, by = 1)
pars <- expand.grid(proportion_clicks = proportion_clicks,
                    n_visitors = n_visitors)
pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)
pars$likelihood <- dbinom(pars$n_visitors, 
    size = n_ads_shown, prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability)
pars_conditioned <- pars[pars$n_visitors==6, ]
pars_conditioned$probability <- pars_conditioned$probability / sum(pars_conditioned$probability)
plot(x=pars_conditioned$proportion_clicks, y=pars_conditioned$probability, type="h")


# Simplify slightly for a known result of 6
n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 6
pars <- expand.grid(proportion_clicks = proportion_clicks,
                    n_visitors = n_visitors)
pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)
pars$likelihood <- dbinom(pars$n_visitors, 
    size = n_ads_shown, prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability)
plot(pars$proportion_clicks, pars$probability, type = "h")

```
  
  
  
***
  
Chapter 5 - More Parameters, Data, and Bayes  
  
Temperature in a normal lake:  
  
* Example of having some water data temperature for a given day  
	* temp <- c(19, 23, 20, 17, 23)  
    * The normal distribution could be a good candidate for the generative function in this case - parameters with mu and sigma  
    * rnorm(n = , mean = , sd = )  
    * dnorm(x = , mean = , sd = )  
    * like <- dnorm(x = temp, mean = 20, sd = 2)  # likelihood of our observed temperatures given a mean of 20 and a standard deviation of 2  
    * prod(like)  # joint likelihood of probabilities  
    * log(like)  # addresses the problem of likelihoods being so small that computer precision becomes an issue  
  
Bayesian model of water temperature:  
  
* Can define priors such as sigma ~Uniform(0, 10) and mean ~ N(18, 5)  
* Can then run an additional grid-approximation exercise  
	* temp <- c(19, 23, 20, 17, 23)  
    * mu <- seq(8, 30, by = 0.5)  
    * sigma <- seq(0.1, 10, by = 0.3)  
    * pars <- expand.grid(mu = mu, sigma = sigma))  
    * pars$mu_prior <- dnorm(pars$mu, mean = 18, sd = 5)  
    * pars$sigma_prior <- dunif(pars$sigma, min = 0, max = 10)  
    * pars$prior <- pars$mu_prior * pars$sigma_prior  
    * for(i in 1:nrow(pars)) {  
    *     likelihoods <- dnorm(temp, pars$mu[i], pars$sigma[i])  
    *     pars$likelihood[i] <- prod(likelihoods)  
    * }  
    * pars$probability <- pars$likelihood * pars$prior  
    * pars$probability <- pars$probability / sum(pars$probability)  
  
Beach party implications of water temperatures:  
  
* Suppose that there is aminimum water temperature for holding a beach party and that we want the probability of exceedence of this temperature  
* It is helpful to create a frame for further analysis; random sampling, weighted by probability, can help  
	* sample_indices <- sample( 1:nrow(pars), size = 10000, replace = TRUE, prob = pars$probability)  # draw some random samples proportional to each row's probability  
    * pars_sample <- pars[sample_indices, c("mu", "sigma")]  
    * hist(pars_sample$mu, 30)  
    * pred_temp <- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma)  
    * hist(pred_temp, 30)  
    * sum(pred_temp >= 18) / length(pred_temp )  
  
Practical tool (BEST):  
  
* Models are often available off-the-shelf which can save time; one example is BEST by John Kruschke  
	* BEST assumes that data come from a t-distribution (more or less a normal distribution with the degrees of freedom added)  
    * BEST estimates standard deviation, mean, and degrees of freedom  
    * BEST uses MCMC (Markov chain Monte Carlo)  
* Can fit the data directly using BEST  
	* library(BEST)  
    * iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)  
    * fit <- BESTmcmc(iq)  
    * fit  # note that nu is the degrees of freedom  
    * plot(fit)  
  
Wrap and up and next steps:  
  
* Bayesian inference as a technique for modeling uncertainty - data, generative model, and prior probability distributions  
* Can use sampling and grid approximation to calculate probabilities and distributions  
	* Under-the-hood, used MCMC as implemented by way of BEST  
* Additional areas for exploration include full application of Bayesian approaches to time series, deep learning, and the like  
* Can also add more advanced computational models  
  
Example code includes:  
```{r}

mu <- 3500
sigma <- 600

weight_distr <- rnorm(n = 100000, mean = mu, sd = sigma)
hist(weight_distr, xlim = c(0, 6000), col = "lightgreen")


mu <- 3500
sigma <- 600

weight <- seq(0, 6000, by=100)
likelihood <- dnorm(weight, mean=mu, sd=sigma)

plot(x=weight, y=likelihood, type="h")


# The IQ of a bunch of zombies
iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
# Defining the parameter grid
pars <- expand.grid(mu = seq(0, 150, length.out = 100), 
                    sigma = seq(0.1, 50, length.out = 100))
# Defining and calculating the prior density for each parameter combination
pars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 100)
pars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)
pars$prior <- pars$mu_prior * pars$sigma_prior
# Calculating the likelihood for each parameter combination
for(i in 1:nrow(pars)) {
  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])
  pars$likelihood[i] <- prod(likelihoods)
}
# Calculating the probability of each parameter combination
pars$probability <- pars$likelihood * pars$prior / sum(pars$likelihood * pars$prior)
lattice::levelplot(probability ~ mu * sigma, data = pars)


head(pars)
sample_indices <- sample( nrow(pars), size = 10000,
    replace = TRUE, prob = pars$probability)
head(sample_indices)
pars_sample <- pars[sample_indices, c("mu", "sigma")]
hist(pars_sample$mu)
quantile(pars_sample$mu, c(0.025, 0.5, 0.975))


head(pars_sample)
pred_iq <- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma)
hist(pred_iq)
mean(pred_iq >= 60)


# The IQ of zombies on a regular diet and a brain based diet.
iq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)
iq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
mean(iq_brains) - mean(iq_regular)

# Need to load http://www.sourceforge.net/projects/mcmc-jags/files for rjags (called by BEST)
# library(BEST)
# best_posterior <- BESTmcmc(iq_brains, iq_regular)
# plot(best_posterior)

```
  
  
  
***
  
### _Categorical Data in the Tidyverse_  
  
Chapter 1 - Introduction to Factor Variables  
  
Introduction to qualitative variables:  
  
* Identifying and inspecting categorical data, using the forcats package, effective visualization  
* Qualitative data in this course will include categorical data and ordianl data  
	* Each have a fixed and known set of possible values - ordinal adds that there is an ordering, though no specific meaning such that 1 vs 2 and 2 vs 3 may be different distances  
* Categorical data is generally best converted to factors iff there is a finite set of potential values  
* Can check for factors using is.factor()  
  
Understanding your qualitative variables:  
  
* Data is from the Kaggle 2017 data science survey  
* High-level summaries include category names and levels; converting to factors can be valuable  
	* multipleChoiceResponses %>% mutate_if(is.character, as.factor)  # nice function!  Run the mutate only if the first condition (is.character) holds  
    * nlevels(multipleChoiceResponses$LearningDataScienceTime)  # number of levels  
    * levels(multipleChoiceResponses$LearningDataScienceTime)  # level names  
    * multipleChoiceResponses %>% summarise_if(is.factor, nlevels)  # run summarize only for the factors  
  
Making better plots:  
  
* Can use forcats::fct_reorder() to reorder data for plotting - fct_reorder(factor, orderingCriteria)  
	* ggplot(WorkChallenges) + geom_point(aes(x = fct_reorder(question, perc_problem), y = perc_problem))  
* Can use forcats::fct_infreq() to order based on frequency and reverse the order using forcats::fct_rev() as needed  
	* ggplot(multiple_choice_responses) + geom_bar(aes(x = fct_infreq(CurrentJobTitleSelect))  
    * ggplot(multiple_choice_responses) + geom_bar(aes(x = fct_rev(fct_infreq(CurrentJobTitleSelect))))  
  
Example code includes:  
```{r}

multiple_choice_answers <- readr::read_csv("./RInputFiles/smc_with_js.csv")

# Print out the dataset
glimpse(multiple_choice_answers)

# Check if CurrentJobTitleSelect is a factor
is.factor(multiple_choice_answers$CurrentJobTitleSelect)


# mutate() and summarise() in dplyr both have variants where you can add the suffix if, all, or at to change the operation
# mutate_if() applies a function to all columns where the first argument is true
# mutate_all() applies a function to all columns
# mutate_at() affects columns selected with a character vector or select helpers (e.g. mutate_at(c("height", "weight"), log))

# Change all the character columns to factors
responses_as_factors <- multiple_choice_answers %>%
    mutate_if(is.character, as.factor)

# Make a two column dataset with variable names and number of levels
number_of_levels <- responses_as_factors %>%
    summarise_all(nlevels) %>%
    gather(variable, num_levels)


# dplyr has two other functions that can come in handy when exploring a dataset
# The first is top_n(x, var), which gets us the first x rows of a dataset based on the value of var
# The other is pull(), which allows us to extract a column and take out the name, leaving only the value(s) from the column

# Select the 4 rows with the highest number of levels
number_of_levels %>%
    top_n(4, num_levels)
    
# How many levels does CurrentJobTitleSelect have? 
number_of_levels %>%
    filter(variable=="CurrentJobTitleSelect") %>%
    pull(num_levels)

# Get the names of the levels of CurrentJobTitleSelect
responses_as_factors %>%
    pull(CurrentJobTitleSelect) %>%
    levels()


# Make a bar plot
ggplot(multiple_choice_answers, aes(x=FormalEducation)) + 
    geom_bar() + 
    coord_flip()

# Make a bar plot
ggplot(multiple_choice_answers, aes(x=fct_rev(fct_infreq(FormalEducation)))) + 
    geom_bar() + 
    coord_flip()


multiple_choice_answers %>%
  filter(!is.na(Age) & !is.na(FormalEducation)) %>%
  group_by(FormalEducation) %>%
  summarize(mean_age = mean(Age)) %>%
  ggplot(aes(x = fct_reorder(FormalEducation, mean_age), y = mean_age)) + 
    geom_point() + 
    coord_flip()

```
  
  
  
***
  
Chapter 2 - Manipulating Factor Variables  
  
Reordering factors:  
  
* Can order by frequency or by another variable for pure categorical variables  
* For ordinal variables, typically is best to order by the implied order inside the ordinal variable - can manually enter using fct_relevel()  
	* ggplot(aes(nlp_frequency, x = fct_relevel(response, "Rarely", "Sometimes", "Often", "Most of the time"))) + geom_bar()  
    * nlp_frequency %>% pull(response) %>% levels()  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time")) %>% pull(response) %>% levels()  # This moves Often and Most of the time to the front, leaving others alone  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time", after = 2)) %>% pull(response) %>% levels()  # move these to after 2  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time", after = Inf) %>% pull(response) %>% levels()  # move this to the end  
  
Renaming factor levels:  
  
* Can convert names for factor levels using forcats::fct_recode()  
	* levels(flying_etiquette$middle_arm_rest_three)  # get the initial levels  
    * ggplot(flying_etiquette, aes(x = fct_infreq(middle_arm_rest_three))) + geom_bar() + coord_flip() + labs(x = "Arm rest opinions")  # labels are very wordy, graph is compressed  
    * flying_etiquette %>% mutate(middle_arm_rest_three = fct_recode(middle_arm_rest_three,   
    *     "Other" = "Other (please specify)", "Everyone should share" = "The arm rests should be shared",  
    *     "Aisle and window people" = "The people in the aisle and window seats get both arm rests",   
    *     "Middle person" = "The person in the middle seat gets both arm rests",  
    *     "Fastest person" = "Whoever puts their arm on the arm rest first")  
    * ) %>%   
    * count(middle_arm_rest_three)  
  
Collapsing factor levels:  
  
* Can collapse factor levels using forcats::fct_collapse()  
	* flying_etiquette %>% mutate(height = fct_collapse(height, under_5_3 = c("Under 5 ft.", "5'0\"", "5'1\"", "5'2\""), over_6_1 = c("6'1\"", "6'2\"", "6'3\"", "6'4\"", "6'5\"", "6'6\" and above"))) %>% pull(height) %>% levels()  
* Can collapse factor levels to other using forcats::fct_other()  
	* flying_etiquette %>% mutate(new_height = fct_other(height, keep = c("6'4\"", "5'1\""))) %>% count(new_height)  # will make everything other than keep in to Other  
    * flying_etiquette %>% mutate(new_height = fct_other(height, drop = c("Under 5 ft.", "5'0\"", "5'1\"", "5'2\"", "5'3\""))) %>% pull(new_height) %>% levels()  # will move the drop items to Other  
    * flying_etiquette %>% mutate(new_height = fct_lump(height, prop = .08)) %>% count(new_height)  # anything less than a proportion of 0.08 will be moved to other  
    * flying_etiquette %>% mutate(new_height = fct_lump(height, n = 3)) %>% count(new_height)  # keep the top 3 categories  
  
Example code includes:  
```{r}

multiple_choice_responses <- multiple_choice_answers

# Print the levels of WorkInternalVsExternalTools
levels(multiple_choice_responses$WorkInternalVsExternalTools)

# Reorder the levels from internal to external 
mc_responses_reordered <- multiple_choice_responses %>%
    mutate(WorkInternalVsExternalTools = fct_relevel(WorkInternalVsExternalTools, 
                                                     c('Entirely internal', 'More internal than external',
                                                       'Approximately half internal and half external', 
                                                       'More external than internal', 'Entirely external',
                                                       'Do not know'
                                                       )
                                                     )
           )

# Make a bar plot of the responses
ggplot(mc_responses_reordered, aes(x=WorkInternalVsExternalTools)) + 
    geom_bar() + 
    coord_flip()


multiple_choice_responses %>%
    # Move "I did not complete any formal education past high school" and "Some college/university study without earning a bachelor's degree" to the front
    mutate(FormalEducation = fct_relevel(FormalEducation, c("I did not complete any formal education past high school", "Some college/university study without earning a bachelor's degree"))) %>%
    # Move "Doctoral degree" to be the sixth level
    mutate(FormalEducation = fct_relevel(FormalEducation, after=6, "Doctoral degree")) %>%
    # Move "I prefer not to answer" to be the last level.
    mutate(FormalEducation = fct_relevel(FormalEducation, after=Inf, "I prefer not to answer")) %>%
    # Examine the new level order
    pull(FormalEducation) %>%
    levels()


# make a bar plot of the frequency of FormalEducation
ggplot(multiple_choice_responses, aes(x=FormalEducation)) + 
    geom_bar()


multiple_choice_responses %>%
    # rename levels
    mutate(FormalEducation = fct_recode(FormalEducation, "High school" ="I did not complete any formal education past high school", "Some college" = "Some college/university study without earning a bachelor's degree")) %>%
    # make a bar plot of FormalEducation
    ggplot(aes(x=FormalEducation)) + 
    geom_bar()


multiple_choice_responses %>%
    # Create new variable, grouped_titles, by collapsing levels in CurrentJobTitleSelect
    mutate(grouped_titles = fct_collapse(CurrentJobTitleSelect, 
        "Computer Scientist" = c("Programmer", "Software Developer/Software Engineer"), 
        "Researcher" = "Scientist/Researcher", 
        "Data Analyst/Scientist/Engineer" = c("DBA/Database Engineer", "Data Scientist", 
                                              "Business Analyst", "Data Analyst", 
                                              "Data Miner", "Predictive Modeler"))) %>%
    # Turn every title that isn't now one of the grouped_titles into "Other"
    mutate(grouped_titles = fct_other(grouped_titles, 
                             keep = c("Computer Scientist", 
                                     "Researcher", 
                                     "Data Analyst/Scientist/Engineer"))) %>% 
    # Get a count of the grouped titles
    count(grouped_titles)


multiple_choice_responses %>%
  # remove NAs of MLMethodNextYearSelect
  filter(!is.na(MLMethodNextYearSelect)) %>%
  # create ml_method, which lumps all those with less than 5% of people into "Other"
  mutate(ml_method = fct_lump(MLMethodNextYearSelect, prop=0.05)) %>%
  # print the frequency of your new variable in descending order
  count(ml_method, sort=TRUE)


multiple_choice_responses %>%
  # remove NAs 
  filter(!is.na(MLMethodNextYearSelect)) %>%
  # create ml_method, retaining the 5 most common methods and renaming others "other method" 
  mutate(ml_method = fct_lump(MLMethodNextYearSelect, 5, other_level="other method")) %>%
  # print the frequency of your new variable in descending order
  count(ml_method, sort=TRUE)

```
  
  
  
***
  
Chapter 3 - Creating Factor Variables  
  
Examining common themed variables:  
  
* Tidy data has each row as an observation and each column as a variable (generally, moving from wide to long)  
	* multipleChoiceResponses %>% select(contains("WorkChallengeFrequency")) %>% gather(work_challenge, frequency)  
    * work_challenges <- multipleChoiceResponses %>% select(contains("WorkChallengeFrequency")) %>% gather(work_challenge, frequency) %>%  
    *     mutate(work_challenge = str_remove(work_challenge, "WorkChallengeFrequency"))  # will remove the string "WorkChallengeFrequency" from column work_challenge  
* Can also convert the variables to 0/1 and then use for statistical summaries  
	* work_challenges %>% filter(!is.na(frequency)) %>% mutate(frequency = if_else( frequency %in% c("Most of the time", "Often"), 1, 0) ) %>%  
    * group_by(work_challenge) %>% summarise(perc_problem = mean(frequency))  
  
Tricks of ggplot2:  
  
* Initial plots may not look so good, for example  
	* ggplot(job_titles_by_perc, aes(x = CurrentJobTitleSelect,, y = perc_w_title)) + geom_point()  
* Can angle the tick axes for better readability  
	* ggplot(job_titles_by_perc, aes(x = CurrentJobTitleSelect, y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
* Can reorder by oreder of popularity using fct_reorder  
	* ggplot(job_titles_by_perc, aes(x = fct_reorder(CurrentJobTitleSelect, perc_w_title), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
    * ggplot(job_titles_by_perc, aes(x = fct_rev(fct_reorder(CurrentJobTitleSelect, perc_w_title)), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
* Can add axis labels and titles using labs()  
	* g <- ggplot(job_titles_by_perc, aes(x = fct_rev(fct_reorder(CurrentJobTitleSelect, perc_w_title)), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
    * g <- g + labs(x = "Job Title", y = "Percent with title")  
* Can change the y-axis ticks to be explicit percentages  
	* g + scale_y_continuous(labels = scales::percent_format())  
  
Changing and creating variables with dplyr::case_when():  
  
* Suppose that there is a simple vector to be recoded using simple business rules  
	* x <- 1:20  
    * case_when(x %% 15 == 0 ~ "fizz buzz", x %% 3 == 0 ~ "fizz", x %% 5 == 0 ~ "buzz", TRUE ~ as.character(x) )  # TRUE is the all others  
* Conditions evaluate from first to last, and the first matching condition is acted on for that value  
	* moods %>% mutate(action = case_when( mood == "happy" & status == "know it" ~ "clap your hands", mood == "happy" & status == "do not know it" ~ "stomp your feet", mood == "sad" ~ "look at puppies", TRUE ~ "jump around")  
  
Example code includes:  
```{r}

learning_platform_usefulness <- multiple_choice_responses %>%
  # select columns with LearningPlatformUsefulness in title
  select(contains("LearningPlatformUsefulness")) %>%
  # change data from wide to long
  gather(learning_platform, usefulness) %>%
  # remove rows where usefulness is NA
  filter(!is.na(usefulness)) %>%
  # remove "LearningPlatformUsefulness" from each string in `learning_platform 
  mutate(learning_platform = str_remove(learning_platform, "LearningPlatformUsefulness"))


learning_platform_usefulness %>%
  # change dataset to one row per learning_platform usefulness pair with number of entries for each
  count(learning_platform, usefulness) %>%
  # use add_count to create column with total number of answers for that learning_platform
  add_count(learning_platform, wt=n, name="nn") %>%
  # create a line graph for each question with usefulness on x-axis and percentage of responses on y
  ggplot(aes(x = usefulness, y = n/nn, group = learning_platform)) + 
  geom_line() + 
  facet_wrap(~ learning_platform)


avg_usefulness <- learning_platform_usefulness %>%
    # If usefulness is "Not Useful", make 0, else 1 
    mutate(usefulness = ifelse(usefulness=="Not Useful", 0, 1)) %>%
    # Get the average usefulness by learning platform 
    group_by(learning_platform) %>%
    summarize(avg_usefulness = mean(usefulness))

# Make a scatter plot of average usefulness by learning platform 
ggplot(avg_usefulness, aes(x=learning_platform, y=avg_usefulness)) + 
    geom_point()

ggplot(avg_usefulness, aes(x = learning_platform, y = avg_usefulness)) + 
    geom_point() + 
    # rotate x-axis text by 90 degrees
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    # rename y and x axis labels
    labs(x="Learning Platform", y="Percent finding at least somewhat useful") + 
    # change y axis scale to percentage
    scale_y_continuous(labels = scales::percent)

ggplot(avg_usefulness, 
       aes(x = fct_rev(fct_reorder(learning_platform, avg_usefulness)), y = avg_usefulness)
       ) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    labs(x = "Learning Platform", y = "Percent finding at least somewhat useful") + 
    scale_y_continuous(labels = scales::percent)


# Check the min age
min(multiple_choice_responses$Age, na.rm=TRUE)
# Check the max age
max(multiple_choice_responses$Age, na.rm=TRUE)
sum(is.na(multiple_choice_responses$Age))


multiple_choice_responses %>%
    # Eliminate any ages below 10 and above 90
    filter(between(Age, 10, 90)) %>%
    # Create the generation variable based on age
    mutate(generation=case_when(
      between(Age, 10, 22) ~ "Gen Z", 
      between(Age, 23, 37) ~ "Gen Y", 
      between(Age, 38, 52) ~ "Gen X", 
      between(Age, 53, 71) ~ "Baby Boomer", 
      between(Age, 72, 90) ~ "Silent"
    )) %>%
    # Get a count of how many answers in each generation
    count(generation)


multiple_choice_responses %>%
    # Filter out people who selected Data Scientist as their Job Title
    filter(!is.na(CurrentJobTitleSelect) & CurrentJobTitleSelect != "Data Scientist")  %>%
    # Create a new variable, job_identity
    mutate(job_identity = case_when(
        CurrentJobTitleSelect == "Data Analyst" & DataScienceIdentitySelect == "Yes" ~ "DS analysts", 
        CurrentJobTitleSelect == "Data Analyst" & DataScienceIdentitySelect %in% c("No", "Sort of (Explain more)") ~ "NDS analyst", 
        CurrentJobTitleSelect != "Data Analyst" & DataScienceIdentitySelect == "Yes" ~ "DS non-analysts", 
        TRUE ~ "NDS non analysts")
        ) %>%
    mutate(JobSat=case_when(
        is.na(JobSatisfaction) ~ NA_integer_,
        JobSatisfaction == "I prefer not to share" | JobSatisfaction == "NA" ~ NA_integer_, 
        JobSatisfaction == "1 - Highly Dissatisfied" ~ 1L, 
        JobSatisfaction == "10 - Highly Satisfied" ~ 10L, 
        TRUE ~ as.integer(JobSatisfaction))) %>%
    # Get the average job satisfaction by job_identity, removing NAs
    group_by(job_identity) %>%
    summarize(avg_js = mean(JobSat, na.rm=TRUE))

```
  
  
  
***
  
Chapter 4 - Case Study on Flight Etiquette  
  
Case study introduction:  
  
* Recreation of 538 dataset on flying etiquette  
* Need to begin by converting variable types and tidying the data and selecting key columns  
	* wide_data %>% mutate_if(is.character, as.factor)  
    * wide_data %>% gather(column, value)  
    * wide_data %>% select(contains("favorite"))  
  
Data preparation and regex:  
  
* Names will need to be changed to something more succinct for plotting  
	* gathered_data %>% distinct(response_var)  
* Regular expressions can be used in any computing language to find instances of general patterns  
	* str_detect("happy", ".")  # the . Will match anything  
    * str_detect("happy", "h.")  # TRUE  
    * str_detect("happy", "y.")  # FALSE, since nothing follows the y  
    * str_remove(string, ".*the ")  # will remove everything up to and including the followed by a space  
  
Recreating the plot:  
  
* The labs() command allows for both a caption and a subtitle  
	* ggplot(mtcars, aes(disp, mpg)) + geom_point() + labs(x = "x axis label", y = "y axis label", title = "My title", subtitle = "and a subtitle", caption = "even a caption!")  
* The geom_text() layer allows for adding the specific numbers to the plot  
	* initial_plot + geom_text(aes(label = round(mean_mpg)))  
    * initial_plot + geom_text(aes(label = round(mean_mpg), y = mean_mpg + 2))  # fix the issue where the text is on top of the tip of the bar  
* Can use the theme layer to modify the non-data layers of the plot  
	* initial_plot + geom_text(aes(label = round(mean_mpg), y = mean_mpg + 2)) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())  # get rid of x and y ticks  
  
End of course recap:  
  
* Basic forcats functions  
* Tidyverse functions  
* ggplot2 tricks  
* Recreating the 538 plot for airplane rudeness behaviors  
  
Example code includes:  
```{r}

flying_etiquette <- read.csv("./RInputFiles/flying-etiquette.csv", stringsAsFactors = FALSE)
names(flying_etiquette) <- 
    stringr::str_replace_all(stringr::str_replace_all(names(flying_etiquette), "\\.", " "), "  ", " ")
names(flying_etiquette) <- stringr::str_trim(names(flying_etiquette))
names(flying_etiquette)[2:22] <- paste0(names(flying_etiquette)[2:22], "?")
names(flying_etiquette) <- stringr::str_replace_all(names(flying_etiquette), "itrude", "it rude")
glimpse(flying_etiquette)


gathered_data <- flying_etiquette %>%
    mutate_if(is.character, as.factor) %>%
    filter(`How often do you travel by plane?` != "Never") %>%
    # Select columns containing "rude"
    select(contains("rude")) %>%
    # Change format from wide to long
    gather(response_var, value)


rude_behaviors <- gathered_data %>%
    mutate(response_var = str_replace(response_var, '.*rude to ', '')) %>%
    mutate(response_var = str_replace(response_var, 'on a plane', '')) %>%
    mutate(rude = if_else(value %in% c("No, not rude at all", "No, not at all rude"), 0, 1)) %>%
    # Create perc_rude, the percent considering each behavior rude
    group_by(response_var) %>%
    summarize(perc_rude=mean(rude))

rude_behaviors


# Create an ordered by plot of behavior by percentage considering it rude
initial_plot <- ggplot(rude_behaviors, aes(x=fct_reorder(response_var, perc_rude), y=perc_rude)) +
geom_col()

# View your plot
initial_plot


titled_plot <- initial_plot + 
    # Add the title, subtitle, and caption
    labs(title = "Hell Is Other People In A Pressurized Metal Tube",
         subtitle = "Percentage of 874 air-passenger respondents who said action is very or somewhat rude",
         caption = "Source: SurveyMonkey Audience", 
         # Remove the x- and y-axis labels
         x="",
         y=""
         ) 

titled_plot


flipped_plot <- titled_plot + 
    # Flip the axes
    coord_flip() + 
    # Remove the x-axis ticks and labels
    theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank())

flipped_plot + 
    # Add labels above the bar with the perc value
    geom_text(aes(label = paste0(round(100*perc_rude), "%"), y = perc_rude + .03), 
              position = position_dodge(0.9), vjust = 1)

```
  
  
  
***
  
### _Bayesian Modeling with RJAGS_  
  
Chapter 1 - Introduction to Bayesian Modeling  
  
Prior model:  
  
* Goals include foundational Bayesian models such as Beta-Binomial, Normal-Normal, and Bayesian regression  
	* Define, compile, simulate using RJAGS  
    * Conduct Bayesian posterior inference using RJAGS  
* Example of election poll - there is some uncertainty around the polling figures that have been released  
	* An additional poll might tend to update the prior model that was built using the previous elections data  
    * Bayesian posterior models are a powerful means of combining priors and data  
* The prior model depends on some specific assumptions and notations  
	* Suppose that p is the percentage of people who support you - assumed to be a random variable between 0 and 1  
    * The prior model for p is p ~ Beta(45, 55)  
    * The beta model can be tuned from pessimism Beta(1, 5) to complete uncertainty Beta(1, 1)  
  
Data and likelihood:  
  
* Suppose that a candidate running for election does a small poll and finds 6 of 10 plan to vote for them  
	* Can integrate even a small data sample like this with assumptions and priors  
    * Assumptions might include that voters are independent and p is a global probability that any given voter supports you  
    * Then, X ~ Bin(n, p) which is to say that X can be defined as the number of n polled voters that support you (assuming per above global probability p)  
* There is a dependence between X, n, and p, thus the data can help to assess how likely each of the values of p may be given that you observed X in n  
	* The likelihood function is the likelihood of observing X given that p takes on a specific value  
  
Posterior model:  
  
* The prior and the likelihood can be integrated to form the posterior  
	* The prior is knowledge that exists before data and the likelihood is what exists based on the data  
    * Multiply prior and likelihood and then scale so probabilities add to 1  
* The RJAGS package combines R with JAGS (Just Another Gibbs Sampler) - requires downloading JAGS and then loading rjags  
* Can define the model within RJAGS, for example  
	* vote_model <- "model{  
    *     # Likelihood model for X
    *     X ~ dbin(p, n)  # order of n and p are reversed (known difference for RJAGS)
    *     # Prior model for p
    *     p ~ dbeta(a, b)
    * }"  
    * vote_jags_A <- jags.model(textConnection(vote_model), data = list(a = 45, b = 55, X = 6, n = 10), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))  
    * vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)  # variable.names is the variable of interest, which is p in this case  
    * plot(vote_sim, trace = FALSE)  
  
Example code includes:  
```{r eval=FALSE}

# Make sure you have installed JAGS-4.x.y.exe (for any x >=0, y>=0) from http://www.sourceforge.net/projects/mcmc-jags/files

# Sample 10000 draws from Beta(45,55) prior
prior_A <- rbeta(n = 10000, shape1 = 45, shape2 = 55)

# Store the results in a data frame
prior_sim <- data.frame(prior_A)

# Construct a density plot of the prior sample
ggplot(prior_sim, aes(x = prior_A)) + 
    geom_density()    


# Sample 10000 draws from the Beta(1,1) prior
prior_B <- rbeta(n = 10000, shape1 = 1, shape2 = 1)    

# Sample 10000 draws from the Beta(100,100) prior
prior_C <- rbeta(n = 10000, shape1 = 100, shape2 = 100)

# Combine the results in a single data frame
prior_sim <- data.frame(samples = c(prior_A, prior_B, prior_C),
        priors = rep(c("A","B","C"), each = 10000))

# Plot the 3 priors
ggplot(prior_sim, aes(x = samples, fill = priors)) + 
    geom_density(alpha = 0.5)


# Define a vector of 1000 p values    
p_grid <- seq(0, 1, length.out=1000)

# Simulate 1 poll result for each p in p_grid   
poll_result <- rbinom(1000, 10, prob=p_grid)

# Create likelihood_sim data frame
likelihood_sim <- data.frame(p_grid, poll_result)    

# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result)) + 
    ggridges::geom_density_ridges()


# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result, fill = poll_result==6)) + 
    ggridges::geom_density_ridges()

# Keep the polls with X = 6    
likelihood_sim_6 <- likelihood_sim %>%     
    filter(poll_result==6)    

# Construct a density plot of the remaining p_grid values
ggplot(likelihood_sim_6, aes(x = p_grid)) + 
    geom_density() + 
    lims(x = c(0,1))


# DEFINE the model
vote_model <- "model{
    # Likelihood model for X
    X ~ dbin(p, n)
    
    # Prior model for p
    p ~ dbeta(a, b)
}"

# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE)


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))

```
  
  
  
***
  
Chapter 2 - Bayesian Models and Markov Chains  
  
Normal-Normal Model:  
  
* Example of reaction times in a sleep deprivation study  
	* Y(i) ~ N(m, s**2) meaning change in reaction time for subject I, assumed to be normally distributed as N(m, s**2)  
    * Prior information is that normal reaction time is 250 and is expected to increase by 0-150 after sleep deprivation (scale of the prior)  
    * The prior for the mean might then be defined as 50 ms increase in reaction time with a standard deviation of 25  
    * The prior for the standard deviation might then be uniform on 0-200  
* Overall formulation of the sleep study model includes  
	* Y(i) ~ N(m, s**2)  
    * m ~ N(50, 25**2)  
    * s ~ Unif(0, 200)  
  
Simulating Normal-Normal in RJAGS:  
  
* Posterior insights are based on the product of the prior and the data, which can be simulated using RJAGS  
	* sleep_model <- "model{  
    *     # Likelihood model for Y[i]
    *     for(i in 1:length(Y)) {
    *         Y[i] ~ dnorm(m, s^(-2))  # requires precision which can be defined as the inverse of sigma-squared  
    *     }  
    *     # Prior models for m and s  
    *     m ~ dnorm(50, 25^(-2))  # requires precision which can be defined as the inverse of sigma-squared  
    *     s ~ dunif(0, 200)  
    * }"  
    * sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))  
    * sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)  
  
Markov chains:  
  
* The RJAGS approach is approximating parameters based on Monte Carlo Markov Chains (MCMC)  
	* The goal of RJAGS is to use Markov chains to estimate (approximate) posteriors that would otherwise be too complicated to model  
* Each iteration of a Markov chain depends on the previous iteration (the iterations are not entirely random or independent)  
	* Over time, the Markov chain explores the sample space, but the exploration is often over a smaller range for smaller intervals (there are auto-corelations)  
    * The overall distribution of the Markov chain mimics a random sampling drawn from the posterior distribution  
  
Markov chain diagnostics and reproducibility:  
  
* The trace plots indicate the longitudinal behavior of the chain while the density plots indicate the distribution of the chain  
* Questions about what makes for a good chain (convergence, trials needed, etc.)  
	* Stability is good - long-run trends should be stabilized  
    * Multiple parallel chains should return very similar results with similar features (should not be overly dependent on RNG/seed)  
    * summary(sleep_sim)  # provides key Markov chain diagnostics  
* Generally, problems with Markov chains can be addressed with longer chains (more iterations)  
* Helpful to set the seed and RNG in the model for reproducibility  
	* inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)  # example inside jags.model()  
  
Example code includes:  
```{r eval=FALSE}

# Take 10000 samples from the m prior
prior_m <- rnorm(10000, 50, 25)

# Take 10000 samples from the s prior    
prior_s <- runif(10000, 0, 200)

# Store samples in a data frame
samples <- data.frame(prior_m, prior_s)

# Density plots of the prior_m & prior_s samples    
ggplot(samples, aes(x = prior_m)) + 
    geom_density()
ggplot(samples, aes(x = prior_s)) + 
    geom_density()


# Check out the first 6 rows of sleep_study
head(sleep_study)

# Define diff_3
sleep_study <- sleep_study %>%
  mutate(diff_3=day_3-day_0)

# Histogram of diff_3    
ggplot(sleep_study, aes(x = diff_3)) + 
    geom_histogram(binwidth = 20, color = "white")

# Mean and standard deviation of diff_3    
sleep_study %>%
  summarize(mean(diff_3), sd(diff_3))


# DEFINE the model    
sleep_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m, s^(-2))
    }

    # Prior models for m and s
    m ~ dnorm(50, 25^(-2))
    s ~ dunif(0, 200)
}"    

# COMPILE the model
sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))    

# SIMULATE the posterior    
sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# PLOT the posterior    
plot(sleep_sim, trace = FALSE)    


# Let m be the average change in reaction time after 3 days of sleep deprivation
# In a previous exercise, you obtained an approximate sample of 10,000 draws from the posterior model of m
# You stored the resulting mcmc.list object as sleep_sim which is loaded in your workspace:
# In fact, the sample of m values in sleep_sim is a dependent Markov chain, the distribution of which converges to the posterior
# You will examine the contents of sleep_sim and, to have finer control over your analysis, store the contents in a data frame

# Check out the head of sleep_sim
head(sleep_sim)

# Store the chains in a data frame
sleep_chains <- data.frame(sleep_sim[[1]], iter = 1:10000)

# Check out the head of sleep_chains
head(sleep_chains)


# NOTE: The 10,000 recorded Iterations start after a "burn-in" period in which samples are discarded
# Thus the Iterations count doesn't start at 1!

# Use plot() to construct trace plots of the m and s chains
plot(sleep_sim, density=FALSE)

# Use ggplot() to construct a trace plot of the m chain
ggplot(sleep_chains, aes(x = iter, y = m)) + 
    geom_line()

# Trace plot the first 100 iterations of the m chain
ggplot(dplyr::filter(sleep_chains, iter<=100), aes(x = iter, y = m)) + geom_line()

# Note that the longitudinal behavior of the chain appears quite random and that the trend remains relatively constant
# This is a good thing - it indicates that the Markov chain (likely) converges quickly to the posterior distribution of m


# Use plot() to construct density plots of the m and s chains
plot(sleep_sim, trace=FALSE)

# Use ggplot() to construct a density plot of the m chain
ggplot(sleep_chains, aes(x = m)) + 
    geom_density()

# Density plot of the first 100 values in the m chain
ggplot(dplyr::filter(sleep_chains, iter<=100), aes(x = m)) + 
    geom_density()


# COMPILE the model
sleep_jags_multi <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), n.chains=4)   

# SIMULATE the posterior    
sleep_sim_multi <- coda.samples(model = sleep_jags_multi, variable.names = c("m", "s"), n.iter = 1000)

# Check out the head of sleep_sim_multi
head(sleep_sim_multi)

# Construct trace plots of the m and s chains
plot(sleep_sim_multi, density=FALSE)


# The mean of the m Markov chain provides an estimate of the posterior mean of m
# The naive standard error provides a measure of the estimate's accuracy.

# Suppose your goal is to estimate the posterior mean of m within a standard error of 0.1 ms
# If the observed naive standard error exceeds this target, no problem!
# You can simply run a longer chain


# SIMULATE the posterior    
sleep_sim_1 <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 1000)

# Summarize the m and s chains of sleep_sim_1
summary(sleep_sim_1)

# RE-SIMULATE the posterior    
sleep_sim_2 <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# Summarize the m and s chains of sleep_sim_2
summary(sleep_sim_2)


# COMPILE the model
sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)) 

# SIMULATE the posterior    
sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# Summarize the m and s chains of sleep_sim
summary(sleep_sim)

```
  
  
  
***
  
Chapter 3 - Bayesian Inference and Prediction
  
Simple Bayesian Regression Model:  
  
* The simple Bayesian regression lays the ground work for more complicated modeling built on it  
* Suppose that the goal is to model human weights, and that they are N(m, s**2)  
	* Y(i) ~ N(m(i), s**2)  # m(i) is an average weight that depends on height  
    * m(i) = a + b*X(i) where X(i) is the height of individual i  
* Can specify Bayesian model with priors  
	* a = intercept ~ N(0, 200**2)  
    * b = slope (expected to be positive) ~ N(1, 0.5**2)  
    * s = residual standard deviation ~ Unif(0, 20)  
  
Bayesian Regression in RJAGS:  
  
* The basic lm() regression will give the parameters based on linear regression  
* Can instead define the Bayesian linear regression for RJAGS  
	* Within RJAGS, [i] means that it varies for each subject, i  
    * Within RJAGS, <- means there is an exact mathematical formula that does not need to be estimated  
    * weight_model <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b * X[i]  
    *     }  
    *     # Prior models for a, b, s  
    *     a ~ dnorm(0, 200^(-2))  
    *     b ~ dnorm(1, 0.5^(-2))  
    *     s ~ dunif(0, 20)  
    * }"  
    * weight_jags <- jags.model(textConnection(weight_model), data = list(X = bdims$hgt, Y = bdims$wgt), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 2018))  
    * weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 10000)  
* Options for addressing the Markov chain instability include  
	* Standardize the height predictor  
    * Increase the chain length  
  
Posterior estimation and inference:  
  
* Bayesian regression using RJAGS provided estimates for the slope and intercept parameters  
* The posterior densities can be summarized for better communication - for example, point estimates based on means  
* Can also plot the lines corresponding to each pair of slope, intercepts - can further create "credible intervals"  
	* The 95% credible intervals are the middle 95% of the densities for each of the parameters (slope and intercept)  
* Can also assess frequencies of exceedence for parameters  
	* table(weight_chains$b > 1.1)  
  
Posterior prediction:  
  
* Based on simulations, the posterior mean trend was estimated  
	* Can use the final coefficients to make estimates about the population  
    * Could instead model the regression based on each set of coefficients included in the chain, including calculating the credible interval  
* Rather than using the regression to find means, the goal may be to predict an individual  
	* Plug in the data as per finding the mean  
    * The residual standard deviation (s) can then be used inside chain, using all of the a, b, s, data  
  
Example code includes:  
```{r eval=FALSE}

# Note the 3 parameters in the model of weight by height: intercept a, slope b, & standard deviation s
# In the first step of your Bayesian analysis, you will simulate the following prior models for these parameters: a ~ N(0, 200^2), b ~ N(1, 0.5^2), and s ~ Unif(0, 20)

# Take 10000 samples from the a, b, & s priors
prior_a <- rnorm(10000, 0, 200)
prior_b <- rnorm(10000, 1, 0.5)
prior_s <- runif(10000, 0, 20)

# Store samples in a data frame
samples <- data.frame(prior_a, prior_b, prior_s, set=1:10000)

# Construct density plots of the prior samples    
ggplot(samples, aes(x = prior_a)) + 
    geom_density()
ggplot(samples, aes(x = prior_b)) + 
    geom_density()
ggplot(samples, aes(x = prior_s)) + 
    geom_density()


# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE)) 

# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>% 
    mutate(height = rnorm(600, 170, 10)) %>% 
    mutate(weight = rnorm(600, prior_a + prior_b*height, prior_s))

# Plot the simulated data & regression model for each parameter set
ggplot(prior_simulation, aes(x = height, y = weight)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE, size = 0.75) + 
    facet_wrap(~ set)


# The bdims data set from the openintro package is loaded in your workspace
# bdims contains physical measurements on a sample of 507 individuals, including their weight in kg (wgt) and height in cm (hgt)

# Construct a scatterplot of wgt vs hgt
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point()

# Add a model smooth
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE)
    
# Obtain the sample regression model
wt_model <- lm(wgt ~ hgt, data = bdims)

# Summarize the model
summary(wt_model)


# DEFINE the model    
weight_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m[i], s^(-2))
        m[i] <- a + b * X[i]
    }

    # Prior models for a, b, s
    a ~ dnorm(0, 200^(-2))
    b ~ dnorm(1, 0.5^(-2))
    s ~ dunif(0, 20)
}"

# COMPILE the model
weight_jags <- jags.model(textConnection(weight_model), data = list(X=bdims$hgt, Y=bdims$wgt), 
                  inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

# COMPILE the model
weight_jags <- jags.model(textConnection(weight_model), data = list(Y = bdims$wgt, X = bdims$hgt), 
                          inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

# SIMULATE the posterior    
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 1000)

# PLOT the posterior    
plot(weight_sim)


# A 100,000 iteration RJAGS simulation of the posterior, weight_sim_big, is in your workspace along with a data frame of the Markov chain output:
head(weight_chains, 2)

# The posterior means of the intercept & slope parameters, a & b, reflect the posterior mean trend in the relationship between weight & height
# In contrast, the full posteriors of a & b reflect the range of plausible parameters, thus posterior uncertainty in the trend
# You will examine the trend and uncertainty in this trend below
# The bdims data are in your workspace

# Summarize the posterior Markov chains
summary(weight_sim_big)

# Calculate the estimated posterior mean of b
mean(weight_chains$b)

# Plot the posterior mean regression model
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red")

# Visualize the range of 20 posterior regression models
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = weight_chains$a[1:20], slope = weight_chains$b[1:20], color = "gray", size = 0.25)


# Summarize the posterior Markov chains
summary(weight_sim_big)

# Calculate the 95% posterior credible interval for b
quantile(weight_chains$b, c(0.025, 0.975))

# Calculate the 90% posterior credible interval for b
quantile(weight_chains$b, c(0.05, 0.95))

# Mark the 90% credible interval 
ggplot(weight_chains, aes(x = b)) + 
    geom_density() + 
    geom_vline(xintercept = quantile(weight_chains$b, c(0.05, 0.95)), color = "red")


# Mark 1.1 on a posterior density plot for b
ggplot(weight_chains, aes(x=b)) + 
    geom_density() + 
    geom_vline(xintercept = 1.1, color = "red")

# Summarize the number of b chain values that exceed 1.1
table(weight_chains$b > 1.1)

# Calculate the proportion of b chain values that exceed 1.1 
mean(weight_chains$b > 1.1)


# Calculate the trend under each Markov chain parameter set
weight_chains <- weight_chains %>% 
    mutate(m_180 = a + b*180)

# Construct a posterior density plot of the trend
ggplot(weight_chains, aes(x = m_180)) + 
    geom_density() 

# Calculate the average trend
mean(weight_chains$m_180)

# Construct a posterior credible interval for the trend
quantile(weight_chains$m_180, c(0.025, 0.975))


# Simulate 1 prediction under the first parameter set
rnorm(1, mean=weight_chains$m_180[1], sd=weight_chains$s[1])

# Simulate 1 prediction under the second parameter set
rnorm(1, mean=weight_chains$m_180[2], sd=weight_chains$s[2])

# Simulate & store 1 prediction under each parameter set
weight_chains <- weight_chains  %>% 
    mutate(Y_180=rnorm(nrow(weight_chains), mean=m_180, sd=s))

# Print the first 6 parameter sets & predictions
head(weight_chains)


# Construct a density plot of the posterior predictions
ggplot(weight_chains, aes(x=Y_180)) + 
    geom_density() + 
    geom_vline(xintercept = quantile(weight_chains$Y_180, c(0.025, 0.975)), color = "red")

# Construct a posterior credible interval for the prediction
quantile(weight_chains$Y_180, c(0.025, 0.975))

# Visualize the credible on a scatterplot of the data
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red") + 
    geom_segment(x = 180, xend = 180, y = quantile(weight_chains$Y_180, c(0.025)), yend = quantile(weight_chains$Y_180, c(0.975)), color = "red")

```
  
  
  
***
  
Chapter 4 - Multivariate and Generalized Linear Models  
  
Bayesian regression with categorical predictor:  
  
* Can incorporate categorical predictors in to the Bayesian regressions  
* Example of usage of a rail-trail in MA  
	* Y[i] ~ N(m[i], s**2) where [i] is the day - assumed to have varying means but constant standard deviations  
    * X[i] is a 1/0 variable where 1 is for weekdays and 0 is for weekends  
    * m[i] = a + b*X[i], meaning that a is the typical weekend volume and a+b is the typical weekday volume  
    * The prior will be a ~ N(400, 100**2) and b ~ N(0, 200**2) and s ~ Unif(0, 200)  
* Can then define the model using RJAGS  
	* rail_model_1 <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b[X[i]]  
    *     }  
    *     # Prior models for a, b, s  
    *     a ~ dnorm(400, 100^(-2))  
    *     s ~ dunif(0, 200)  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 200^(-2))  
    * }"  
    * Note that b[1] <- 0 is because we want m[i] = a for the reference level; b[2], the second level, is what we want to model  
  
Multivariate Bayesian regression:  
  
* Bayesian models can be generalized to multivariate models, for example  
	* Y[i] ~ N(m[i], s**2) where [i] is the day - assumed to have varying means but constant standard deviations  
    * X[i] is a 1/0 variable where 1 is for weekdays and 0 is for weekends  
    * Z[i] is the high temperatue on day [i] in degrees F  
    * m[i] = a + b*X[i] + c*Z[i]  
    * a ~ N(0, 200**2), b ~ N(0, 200**2), c ~ N(0, 20**2), s ~ Unif(0, 200)  
* Can then define and simulate this model using RJAGS  
	* rail_model_2 <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b[X[i]] + c * Z[i]  
    *     }  
    *     # Prior models for a, b, c, s  
    *     a ~ dnorm(0, 200^(-2))  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 200^(-2))  
    *     c ~ dnorm(0, 20^(-2))  
    *     s ~ dunif(0, 200)  
    * }"  
  
Bayesian Poisson regression:  
  
* Can generalize regression techniques to non-normalized settings, such as Poisson regressions  
* Bicycle riders per day might better be modeled as a Poisson - should be discrete and non-negative, for example  
	* Y ~ Pois(lambda[i])  
    * log(lambda[i]) = a + b * X[i] + c * Z[i]  
    * a ~ N(0, 200**2)  
    * b ~ N(0, 2**2)  
    * c ~ N(0, 2**2)  
* Can then define the model within RJAGS  
	* poisson_model <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dpois(l[i])  
    *         log(l[i]) <- a + b[X[i]] + c*Z[i]  
    *     }  
    *     # Prior models for a, b, c  
    *     a ~ dnorm(0, 200^(-2))  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 2^(-2))  
    *     c ~ dnorm(0, 2^(-2))  
    * }"  
* Caveat for the Poisson is the mean and variance should be roughly equal; might accept some imperfections of dispersions  
  
Wrap up:  
  
* Bayesian modeling has grown in popularity along with computing resources  
* RJAGS allows for defining, compiling, and simulating Bayesian models  
* Intutive posterior inference, including credible intervals  
* Generalizing from normal models to Poisson models  
  
Example code includes:  
```{r eval=FALSE}

# Confirm that weekday is a factor variable
is.factor(RailTrail$weekday)

# Construct a density plot of volume by weekday
ggplot(RailTrail, aes(x = volume, fill = weekday)) + 
    geom_density(alpha = 0.5)

# Calculate the mean volume on weekdays vs weekends
RailTrail %>%
  group_by(weekday) %>%
  summarize(mean(volume))


# DEFINE the model    
rail_model_1 <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
      Y[i] ~ dnorm(m[i], s^(-2))
      m[i] <- a + b[X[i]]
    }
    
    # Prior models for a, b, s
    a ~ dnorm(400, 100^(-2))
    b[1] <- 0
    b[2] ~ dnorm(0, 200^(-2))
    s ~ dunif(0, 200)
}"

# COMPILE the model
rail_jags_1 <- jags.model(textConnection(rail_model_1), 
    data = list(Y=RailTrail$volume, X=RailTrail$weekday),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
    )  

# COMPILE the model
rail_jags_1 <- jags.model(textConnection(rail_model_1), data = list(Y = RailTrail$volume, X = RailTrail$weekday), 
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10))

# SIMULATE the posterior    
rail_sim_1 <- coda.samples(model = rail_jags_1, variable.names = c("a", "b", "s"), n.iter = 10000)

# Store the chains in a data frame
rail_chains_1 <- data.frame(rail_sim_1[[1]])

# PLOT the posterior    
plot(rail_sim_1)


# Posterior probability that typical volume is lower on weekdays
mean(rail_chains_1$'b.2.' < 0)

# Construct a chain of values for the typical weekday volume
rail_chains_1 <- rail_chains_1 %>% 
    mutate(weekday_mean = a + b.2.)

# Construct a density plot of the weekday chain
ggplot(rail_chains_1, aes(x=weekday_mean)) +
  geom_density()

# 95% credible interval for typical weekday volume
quantile(rail_chains_1$weekday_mean, c(0.025, 0.975))


# Construct a plot of volume by hightemp & weekday
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point()

# Construct a sample model
rail_lm <- lm(volume ~ weekday + hightemp, data=RailTrail)

# Summarize the model
summary(rail_lm)

# Superimpose sample estimates of the model lines
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point() + 
    geom_abline(intercept = coef(rail_lm)["(Intercept)"], slope = coef(rail_lm)["hightemp"], color = "red") +
    geom_abline(intercept = sum(coef(rail_lm)[c("(Intercept)", "weekdayTRUE")]), slope = coef(rail_lm)["hightemp"], color = "turquoise3")


# DEFINE the model    
rail_model_2 <- "model{
  # Likelihood model for Y[i]
  for(i in 1:length(Y)){
    Y[i] ~ dnorm(m[i], s^(-2))
    m[i] <- a + b[X[i]] + c * Z[i]
  }
    
  # Prior models for a, b, c, s
  a ~ dnorm(0, 200^(-2))
  b[1] <- 0
  b[2] ~ dnorm(0, 200^(-2))
  c ~ dnorm(0, 20^(-2))
  s ~ dunif(0, 200)
}"

# COMPILE the model
rail_jags_2 <- jags.model(textConnection(rail_model_2), 
                          data = list(Y=RailTrail$volume, X=RailTrail$weekday, Z=RailTrail$hightemp),
                          inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
                          )

# SIMULATE the posterior    
rail_sim_2 <- coda.samples(model = rail_jags_2, variable.names = c("a", "b", "c", "s"), n.iter = 10000)

# Store the chains in a data frame
rail_chains_2 <- data.frame(rail_sim_2[[1]])

# PLOT the posterior    
plot(rail_sim_2)


# Summarize the posterior Markov chains
summary(rail_sim_2)

# Plot the posterior mean regression models
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point() + 
    geom_abline(intercept = mean(rail_chains_2[, "a"]), slope = mean(rail_chains_2[, "c"]), color = "red") + 
    geom_abline(intercept = mean(rail_chains_2[, "a"]) + mean(rail_chains_2[, "b.2."]), slope = mean(rail_chains_2[, "c"]), color = "turquoise3")
  
# Posterior probability that typical volume is lower on weekdays
mean(rail_chains_2$'b.2.' < 0)


# DEFINE the model    
poisson_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dpois(l[i])
        log(l[i]) <- a + b[X[i]] + c * Z[i]
    }

    # Prior models for a, b, c
    a ~ dnorm(0, 200^(-2))
    b[1] <- 0
    b[2] ~ dnorm(0, 2^(-2))
    c ~ dnorm(0, 2^(-2))
}" 

# COMPILE the model
poisson_jags <- jags.model(textConnection(poisson_model), 
                           data = list(Y=RailTrail$volume, X=RailTrail$weekday, Z=RailTrail$hightemp),
                           inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
                           )

# SIMULATE the posterior    
poisson_sim <- coda.samples(model = poisson_jags, variable.names = c("a", "b", "c"), n.iter = 10000)

# Store the chains in a data frame
poisson_chains <- data.frame(poisson_sim[[1]])

# PLOT the posterior    
plot(poisson_sim)


# Summarize the posterior Markov chains
summary(poisson_sim)

# Plot the posterior mean regression models
ggplot(RailTrail, aes(x = hightemp, y = volume, color = weekday)) + 
    geom_point() + 
    stat_function(fun = function(x){exp(5.01352 + 0.01426 * x)}, color = "red") + 
    stat_function(fun = function(x){exp(5.01352 - 0.12800 + 0.01426 * x)}, color = "turquoise3")


# Calculate the typical volume on 80 degree weekends & 80 degree weekdays
poisson_chains <- poisson_chains %>% 
    mutate(l_weekend=exp(a + c*80)) %>% 
    mutate(l_weekday=exp(a + b.2. + c*80))

# Construct a 95% CI for typical volume on 80 degree weekend
quantile(poisson_chains$l_weekend, c(0.025, 0.975))

# Construct a 95% CI for typical volume on 80 degree weekday
quantile(poisson_chains$l_weekday, c(0.025, 0.975))


# Simulate weekend & weekday predictions under each parameter set
poisson_chains <- poisson_chains %>% 
    mutate(Y_weekend=rpois(nrow(poisson_chains), l_weekend)) %>% 
    mutate(Y_weekday=rpois(nrow(poisson_chains), l_weekday))
    
# Print the first 6 sets of parameter values & predictions
head(poisson_chains)

# Construct a density plot of the posterior weekday predictions
ggplot(poisson_chains, aes(x=Y_weekday)) +
  geom_density()

# Posterior probability that weekday volume is less 400
mean(poisson_chains$Y_weekday < 400)

```
  
  
  
***
  
### _Parallel Programming in R_  
  
Chapter 1 - Can I run my application in parallel?  
  
Partitioning problems in to independent pieces:  
  
* Course contents include  
	* Methods of parallel programming and R packages for support  
    * The parallel package in R  
    * Packages foreach and future.apply  
    * Random numbers and reproducibility  
* Programs can be partitioned either by tasks (e.g., birth model, death model, migration model) or by data (chunks of data passed to a routine, such as rowSums to a matrix)  
	* Many independent tasks are referred to as "embarassingly parallel", and this is common to statistical simulations  
  
Models of parallel computing:  
  
* Available hardware drives the ability to split components - # CPU, Memory (including shared memory or distributed memory)  
	* Message passing software runs on distributed memory and allows for fully independent processes  
    * Shared memory allows for easier passing of data  
* Programming paradigms include master-worker and map-reduce (Hadoop or Scala or the like)  
* This course will cover the master-worker model  
	* The master process creates processes for the workers and then compiles the results that the workers return  
  
R packages for parallel computing:  
  
* The R core package parallel allows for code to be independent of other packages  
* Can instead work with iotools and sparklyr for working with the map-reduce process  
	* Further, pbdR allows for many parallel approaches within R  
* The master-worker paradigms can be implemented using foreach, future.apply, snow, snowFT, snowfall, future (currently under active development, more modern)  
* The parallel package can be used for basic parallel tasks  
	* ncores <- parallel::detectCores(logical = FALSE)  
    * cl <- parallel::makeCluster(ncores)  
    * parallel::clusterApply(cl, x = ncores:1, fun = rnorm)  # x is passed to the workers in order, so worker 1 will get ronorn(ncores)  
    * parallel::stopCluster(cl)  
  
Example code includes:  
```{r}

extract_words <- function(book_name) {
    # extract the text of the book
    text <- subset(austen_books(), book == book_name)$text
    # extract words from the text and convert to lowercase
    str_extract_all(text, boundary("word")) %>% unlist %>% tolower
}

janeausten_words <- function() {
    # Names of the six books contained in janeaustenr
    books <- austen_books()$book %>% unique %>% as.character
    # Vector of words from all six books
    words <- sapply(books, extract_words) %>% unlist
    words
}

austen_books <- function () 
{
    books <- list('Sense & Sensibility' = janeaustenr::sensesensibility, 
                  'Pride & Prejudice' = janeaustenr::prideprejudice, 
                  'Mansfield Park' = janeaustenr::mansfieldpark, 
                  'Emma' = janeaustenr::emma, 
                  'Northanger Abbey' = janeaustenr::northangerabbey, 
                  'Persuasion' = janeaustenr::persuasion
                  )
    ret <- data.frame(text = unlist(books, use.names = FALSE), stringsAsFactors = FALSE)
    ret$book <- factor(rep(names(books), sapply(books, length)))
    ret$book <- factor(ret$book, levels = unique(ret$book))
    structure(ret, class = c("tbl_df", "tbl", "data.frame"))
}

max_frequency <- function(letter, words, min_length = 1) {
    w <- select_words(letter, words = words, min_length = min_length)
    frequency <- table(w)    
    frequency[which.max(frequency)]
}

select_words <- function(letter, words, min_length = 1) {
    min_length_words <- words[nchar(words) >= min_length]
    grep(paste0("^", letter), min_length_words, value = TRUE)
}

# Vector of words from all six books
words <- janeausten_words()

# Most frequent "a"-word that is at least 5 chars long
max_frequency(letter = "a", words = words, min_length = 5)

# Partitioning
result <- lapply(letters, FUN=max_frequency,
                 words = words, min_length = 5) %>% unlist()

# barplot of result
barplot(result, las = 2)


replicates <- 50
sample_size <- 10000

# Function that computes mean of normal random numbers
myfunc <- function(n, ...) mean(rnorm(n, ...))

# Init result, set seed & repeat the task sequentially
result <- rep(NA, replicates)
set.seed(123)
for(iter in 1:replicates) result[iter] <- myfunc(sample_size)

# View result
hist(result)

# Use sapply() with different distribution parameters
hist(sapply(rep(sample_size, replicates), FUN=myfunc, mean = 10, sd = 5))


# We'll now introduce a demographic model to be used throughout the course. It projects net migration rates via an AR(1) model, rate(t+1) - µ = ?(rate(t) -µ) + error with variance s2
# An MCMC estimation for the USA resulted in 1000 samples of parameters µ, ? and s
# The task is to project the future distribution of migration rates

ar1_trajectory <- function(est, rate0, len = 15) {
    trajectory <- rep(NA, len)
    rate <- rate0
    for (time in seq_len(len)) {
        trajectory[time] <- ar1(est, r = rate)
        rate <- trajectory[time]
    }
    trajectory
}

ar1 <- function(est, r) {
    est['mu'] + est['phi'] * (r - est['mu']) + 
        rnorm(1, sd = est['sigma'])
}

ar1_block <- function(id, rate0 = 0.015, traj_len = 15, block_size = 10) {
    trajectories <- matrix(NA, nrow = block_size, ncol = traj_len)
    for (i in seq_len(block_size)) 
        trajectories[i,] <- ar1_trajectory(unlist(ar1est[id, ]), rate0 = rate0, len = traj_len)
    trajectories
}

show_migration <- function(trajs) {
    df <- data.frame(time = seq(2020, by = 5, len = ncol(trajs)),
                     migration_rate = apply(trajs, 2, median),
                     lower = apply(trajs, 2, quantile, 0.1),
                     upper = apply(trajs, 2, quantile, 0.9)
                    )
    g <- ggplot(df, aes(x = time, y = migration_rate)) + 
        geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey70") + 
        geom_line()
    print(g)
}


# Simulate from multiple rows of the estimation dataset
ar1_multblocks <- function(ids, ...) {
    trajectories <- NULL
    for (i in seq_along(ids)) {
        trajectories <- rbind(trajectories, ar1_block(ids[i], ...))
    }
    trajectories
}

ar1est <- data.frame(mu=c(0.0105, 0.0185, 0.022, 0.0113, 0.0144, 0.0175, -9e-04, 0.0093, 0.0111, -9e-04, -0.0024, 0.0086, 0.012, 0.0161, 0.0043, 0.0175, 0.0118, 0.0019, 0.0116, 0.0048, 0.0154, 0.0137, 0.0168, 0.0191, 0.0108, -0.0037, 0.0135, 0.0203, -0.0042, 0.0097, 0.0209, 0.0034, 0.0113, 0.0102, 0.0094, -0.0012, 0.008, 0.0082, 0.0123, 0.0175, 0.0054, -0.0087, 0.0161, 0.0155, 0.0126, 0.0181, 0.014, -0.0135, -0.0095, 0.0142, 0.011, 0.0194, 0.0149, 0.0115, 0.0129, -0.0124, 0.0116, 0.0136, 0.0161, 0.005, 0.0165, -0.0079, 0.0129, -0.0016, -7e-04, 0.0243, 0.0193, -0.004, 0.0145, 0.0078, 0.0156, 0.001, 0.0032, 0.0069, 0.0146, 0.0164, 0.0113, 0.0116, 0.0182, 0.0167, -0.0031, 0.0168, 0.0137, 0.012, -0.0212, -0.0092, 0.019, 0.0167, -0.0021, 0.0156, 0.0173, 0.0148, -0.0036, 0.0168, 0.0179, 0.0086, 0.0131, 0.015, 0.0106, 0.0132, 0.0119, 0.0156, 0.0159, 0.0256, 0.0071, 0.0163, 0.0107, 0.0139, 0.0228, 0.0139, 0.0117, 0.0133, 0.0127, -0.0162, 0.0115, 0.0095, 0.0183, 0.0183, -6e-04, 0.0177, 0.0145, 0.0041, 0.0143, 0.0135, -0.0078, 0.0036, 0.015, 0.018, 0.0158, 0.0054, -0.0204, 0.0193, 0.0051, 0.0144, 0.0129, 0.0134, 0.0116, 0.0102, 0.0203, 0.0154, 0.0106, 0.0184, 0.0096, -0.0032, 0.0143, 0.0158, 0.0093, 0.0159, 0.0112, 0.0106, 0.0075, 0.0133, 0.0171, 0.0133, 0.0139, 0.0167, 0.0131, -0.0078, 0.0135, 0.0145, 0.0104, 8e-04, 0.0205, 0.0046, 0.011, 0.0148, 0.0202, 8e-04, 0.0211, 0.0135, -8e-04, -0.0104, -0.0027, 0.0094, 0.0179, -0.0101, 0.0156, 0.0155, 0.014, 0.0149, 0.0165, 0.0168, 0.0155, 0.0136, 0.0156, 0.0149, 0.0191, 0.0176, 0.0094, -0.0076, 0.0162, 0.0143, 0.0182, 0.0102, 0.015, -0.0292, 0.0063, -0.0028, 0.0163, 0.015), 
                     sigma=c(0.0081, 0.0053, 0.0069, 0.0075, 0.0082, 0.006, 0.0101, 0.011, 0.0064, 0.0066, 0.0095, 0.0057, 0.0078, 0.005, 0.0076, 0.0064, 0.0067, 0.0049, 0.0086, 0.0067, 0.0063, 0.0054, 0.0063, 0.0077, 0.0072, 0.0074, 0.0067, 0.0047, 0.0125, 0.0069, 0.0052, 0.0073, 0.0063, 0.0072, 0.0086, 0.0079, 0.009, 0.006, 0.0077, 0.0061, 0.0082, 0.0072, 0.0054, 0.0056, 0.0072, 0.0085, 0.0064, 0.0058, 0.0064, 0.0084, 0.0075, 0.006, 0.0048, 0.0068, 0.0065, 0.0082, 0.0072, 0.0056, 0.0056, 0.0055, 0.0054, 0.0059, 0.0064, 0.0069, 0.0073, 0.0071, 0.0057, 0.0062, 0.0086, 0.0062, 0.0054, 0.0052, 0.0066, 0.0076, 0.0046, 0.0056, 0.0066, 0.0077, 0.0074, 0.0061, 0.0056, 0.0065, 0.0069, 0.0084, 0.0058, 0.007, 0.0074, 0.0077, 0.0081, 0.0083, 0.0054, 0.0057, 0.0076, 0.0119, 0.0056, 0.0078, 0.005, 0.0073, 0.0075, 0.0054, 0.0085, 0.011, 0.0063, 0.0056, 0.009, 0.0069, 0.008, 0.0063, 0.007, 0.0059, 0.0064, 0.006, 0.0103, 0.0085, 0.006, 0.0076, 0.0054, 0.0066, 0.0056, 0.0071, 0.0079, 0.007, 0.0085, 0.0075, 0.007, 0.0085, 0.006, 0.0067, 0.006, 0.0074, 0.0098, 0.0066, 0.0058, 0.0075, 0.0064, 0.0059, 0.0103, 0.0055, 0.0053, 0.0068, 0.0057, 0.009, 0.0118, 0.0096, 0.0085, 0.0075, 0.0078, 0.0041, 0.0056, 0.008, 0.0071, 0.006, 0.0046, 0.0061, 0.007, 0.0061, 0.0066, 0.0075, 0.0094, 0.0072, 0.008, 0.0064, 0.0079, 0.0068, 0.0069, 0.0058, 0.0056, 0.0057, 0.0065, 0.006, 0.0073, 0.0067, 0.0068, 0.0071, 0.0048, 0.0071, 0.0063, 0.0051, 0.0079, 0.0042, 0.0048, 0.0066, 0.0072, 0.0058, 0.0057, 0.0083, 0.0063, 0.0057, 0.0103, 0.0096, 0.0067, 0.0051, 0.0075, 0.0064, 0.0069, 0.007, 0.007, 0.0074, 0.0056, 0.006), 
                     phi=c(0.42, 0.3509, 0.8197, 0.5304, 0.1491, 0.3675, 0.9687, 0.7877, 0.7114, 0.9435, 0.9634, 0.9189, 0.4758, 0.5738, 0.8016, 0.0509, 0.8281, 0.8168, 0.7442, 0.9347, 0.1699, 0.3566, 0.8388, 0.7724, 0.7474, 0.7834, 0.6661, 0.5162, 0.9025, 0.5306, 0.6912, 0.7625, 0.8289, 0.6985, 0.9188, 0.9639, 0.3178, 0.7288, 0.4129, 0.2196, 0.9304, 0.9697, 0.193, 0.1474, 0.3111, 0.8844, 0.7386, 0.9674, 0.9983, 0.4863, 0.9338, 0.7999, 0.4696, 0.5078, 0.5141, 0.9958, 0.6404, 0.2886, 0.4171, 0.9856, 0.3261, 0.9713, 0.682, 0.7686, 0.8577, 0.9481, 0.6057, 0.934, 0.3161, 0.9414, 0.8349, 0.8325, 0.8913, 0.7726, 0.7327, 0.1403, 0.8144, 0.7506, 0.225, 0.4884, 0.9052, 0.2891, 0.1652, 0.7612, 0.9403, 0.9865, 0.4107, 0.6518, 0.893, 0.4981, 0.72, 0.3366, 0.8437, 0.2551, 0.7753, 0.5, 0.7857, 0.7107, 0.5643, 0.2887, 0.9621, 0.2384, 0.414, 0.86, 0.6917, 0.4946, 0.2325, 0.3419, 0.9219, 0.2706, 0.717, 0.2327, 0.7541, 0.9692, 0.5838, 0.9346, 0.4739, 0.3219, 0.9634, 0.3046, 0.9913, 0.8485, 0.3071, 0.0373, 0.9183, 0.7935, 0.0039, 0.5968, 0.3654, 0.595, 0.9712, 0.2745, 0.6027, 0.7441, 0.7641, 0.3582, 0.3397, 0.7748, 0.8188, 0.0604, 0.5076, 0.2856, 0.6859, 0.6705, 0.0326, 0.8749, 0.2596, 0.1138, 0.6072, 0.4, 0.9241, 0.612, 0.2375, 0.2495, 0.0661, 0.3234, 0.7651, 0.8581, 0.4818, 0.7303, 0.7458, 0.8925, 0.2861, 0.982, 0.0791, 0.2474, 0.4326, 0.8757, 0.5288, 0.6476, 0.8473, 0.9098, 0.9562, 0.8464, 0.5444, 0.9738, 0.706, 0.0795, 0.391, 0.3167, 0.3311, 0.5681, 0.27, 0.9046, 0.2299, 0.2299, 0.085, 0.4002, 0.7443, 0.9865, 0.7028, 0.9016, 0.6092, 0.2367, 0.5402, 0.9401, 0.8013, 0.993, 0.2473, 0.6414)
                     )
str(ar1est)


# Generate trajectories for all rows of the estimation dataset
trajs <- ar1_multblocks(seq_along(nrow(ar1est)), rate0 = 0.015,  block_size = 10, traj_len = 15)

# Show results
show_migration(trajs)


# Load package
library(parallel)

# How many physical cores are available?
ncores <- detectCores(logical = FALSE)

# Create a cluster
cl <- makeCluster(ncores)

# Process rnorm in parallel
clusterApply(cl, 1:ncores, fun = rnorm, mean = 10, sd = 2)

# Evaluate partial sums in parallel
part_sums <- clusterApply(cl, x = c(1, 51), fun = function(x) sum(x:(x + 49)))

# Total sum
total <- sum(unlist(part_sums))

# Check for correctness
total == sum(1:100)

# Stop the cluster
stopCluster(cl)


# Create a cluster and set parameters
cl <- makeCluster(2)
replicates <- 50
sample_size <- 10000

# Parallel evaluation
means <- clusterApply(cl, x = rep(sample_size, replicates), fun = myfunc)
                
# View results as histogram
hist(unlist(means))

```
  
  
  
***
  
Chapter 2 - The parallel package  
  
Cluster basics:  
  
* The parallel package consists of two parts - snow (Tuerney) and multicore (Urbanek)  
	* The snow can work on any operating system  
    * The multicore works on most systems but not on Windows  
* Supported backends for snow are managed automatically by the parallel package  
	* cl <- makeCluster(ncores, type = "PSOCK")  # default socket communication, works on all OS, all clusters start with a completely empty environment  
    * cl <- makeCluster(ncores, type = "FORK")  # all OS except Windows, all workers are complete copies of the master environment  
    * cl <- makeCluster(ncores, type = "MPI")  # interface provided by Rmpi and may be more efficient on machines where MPI is enabled  
  
Core of parallel:  
  
* The main processing functions are clusterApply and clusterApplyLB ("load balanced")  
* The wrapper functions include parApply, parLapply, parSapply, parRapply (rows of a matrix), parCapply (columns of a matrix)  
	* Further, parLapplyLB, parSapplyLB are wrappers on the clusterApplyLB data  
* Example of using clusterApply for work on a pre-defined cluster cl  
	* clusterApply(cl, x = arg.sequence, fun = myfunc)  # each element of x is passed to myfunc (length of x is the total number of tasks)  
* There are several overheads involved in creating parallel processing - starting/stopping clusters, messages sent between nodes/masters, size of messages  
	* Communications between master and workers is expensive, so long worker times are preferred in general  
    * The overheads may sometimes be so significant as to make parallel processing more time-consuming than serial processing  
  
Initialization of nodes:  
  
* Cluster nodes typically start with a clean, empty environment (default for sockets)  
* Repeated communications with the workers is expensive  
	* clusterApply(cl, rep(1000, n), rnorm, sd = 1:1000)  # master needs to send the vector to all the clusters (big overhead)  
* Good practice is to initialize workers at the beginning with everything that stays constant and/or is time consuming  
	* Sending static data or datasets, loading libraries, evaluating global functions, etc.  
* The clusterCall() will call the same function with the same arguments on all the nodes  
	* cl <- makeCluster(2)  
    * clusterCall(cl, function() library(janeaustenr))  # will be loaded in all the clusters  
    * clusterCall(cl, function(i) emma[i], 20)  # will call the 20th element of emma from janeausten  
* The clusterEvalQ() will evaluate a literal expression on all nodes  
	* cl <- makeCluster(2)  
    * clusterEvalQ(cl, { library(janeaustenr) ; library(stringr) ; get_books <- function() austen_books()$book %>% unique %>% as.character })  # all books in the package  
    * clusterCall(cl, function(i) get_books()[i], 1:3)  # function get_books is available in the environment due to the above  
* The clusterExport() will export objects from master to the workers  
	* books <- get_books()  
    * cl <- makeCluster(2)  
    * clusterExport(cl, "books")  # The books object is passed quoted  
    * clusterCall(cl, function() print(books))  # books will be available since it was passed by clusterExport()  
  
Subsetting data:  
  
* Each task is applied to a different data chunk; these can be made available to the worker in various ways  
	* Random numbers on the fly  
    * Arguments  
    * Chunking on the workers side  
* Example of random numbers being created on the fly by the workers (reproducibility covered in later chapters)  
	* myfunc <- function(n, ...) mean(rnorm(n, ...))  
    * clusterApply(cl, rep(1000, 20), myfunc, sd = 6)  
* Example of chunking the data on the master side and then passing the data to workers as an argument  
	* Incorporated in to parApply() by default  
    * cl <- makeCluster(4)  
    * mat <- matrix(rnorm(12), ncol=4)  
    * parCapply(cl, mat, sum)  # splits the matrix by column and passes to worker  
    * unlist(clusterApply(cl, as.data.frame(mat), sum))  # needs to be converted to data.frame first for clusterApply()  
* Example of chunking data on the worker side (each pre-populated with the full data, chunking on the worker side) - saves communication time  
	* n <- 100  
    * M <- matrix(rnorm(n * n), ncol = n)  
    * clusterExport(cl, "M")  
    * mult_row <- function(id) apply(M, 2, function(col) sum(M[id,] * col))  
    * clusterApply(cl, 1:n, mult_row) %>% do.call(rbind, .)  
  
Example code includes:  
```{r}

# Load parallel and create a cluster
library(parallel)
cl <- makeCluster(4)

# Investigate the cl object and its elements
typeof(cl)
length(cl)
typeof(cl[[3]])
cl[[3]]$rank

# What is the process ID of the workers
clusterCall(cl, Sys.getpid)

# Stop the cluster
stopCluster(cl)


# Define ncores and a print function
ncores <- 2
print_ncores <- function() print(ncores)

# Create a socket and a fork clusters
# cl_sock <- makeCluster(ncores, type = "PSOCK")
# cl_fork <- makeCluster(ncores, type = "FORK")  # this is possible only on OS other than Windows

# Evaluate the print function on each cluster
# clusterCall(cl_sock, print_ncores)  # this will fail since the socket has no knowledge of the main environment
# clusterCall(cl_fork, print_ncores)

# Change ncores and evaluate again
# ncores <- 4
# clusterCall(cl_fork, print_ncores)  # the fork is only of the original environment, so these clusters will still think the answer is 2


# In this exercise, you will take the simple embarrassingly parallel application for computing mean of random numbers (myfunc()) from the first chapter, and implement two functions:
# One that runs the application sequentially, mean_seq(), and one that runs it in parallel, mean_par()
# Both functions have three arguments, n (sample size), repl (number of replicates) and ... (passed to myfunc())
# Function mean_par() assumes a cluster object cl to be present in the global environment

# Function to run repeatedly
myfunc <- function(n, ...) mean(rnorm(n, ...))

# Sequential solution
mean_seq <- function(n, repl, ...) {
    res <- rep(NA, repl)
    for (it in 1:repl) res[it] <- myfunc(n, ...)
    res
}

# Parallel solution
mean_par <- function(n, repl, ...) {
    res <- clusterApply(cl, x = rep(n, repl), fun = myfunc, ...)
    unlist(res)
}


# Load packages 
library(parallel)
library(microbenchmark)

# Create a cluster
cl <- makeCluster(2)

# Compare run times
microbenchmark(mean_seq(3000000, repl = 4), 
               mean_par(3000000, repl = 4),
               mean_seq(100, repl = 100), 
               mean_par(100, repl = 100),
               times = 1, unit = "s")
# Stop cluster               
stopCluster(cl)


# Load extraDistr on master
library(extraDistr)

# Define myrdnorm 
myrdnorm <- function(n, mean = 0, sd = 1) 
    rdnorm(n, mean = mean, sd = sd)

# Run myrdnorm in parallel - should fail
# res <- clusterApply(cl, rep(1000, 20), myrdnorm, sd = 6)   # will error out


# Load extraDistr on all workers
cl <- makeCluster(2)
clusterEvalQ(cl, library(extraDistr))

# Run myrdnorm in parallel again and show results
res <- clusterApply(cl, rep(1000, 20), myrdnorm, sd = 6)
hist(unlist(res))


# myrdnorm that uses global variables
myrdnorm <- function(n) rdnorm(n, mean = mean, sd = sd)

# Initialize workers 
clusterEvalQ(cl, {
    library(extraDistr)
    mean=10
    sd=5
    })
    
# Run myrdnorm in parallel and show results
res <- clusterApply(cl, rep(1000, 100), myrdnorm)

# View results
hist(unlist(res))


# Set global objects on master
mean <- 20
sd <- 10

# Export global objects to workers
clusterExport(cl, c("mean", "sd"))

# Load extraDistr on workers
clusterEvalQ(cl, library(extraDistr))

# Run myrdnorm in parallel and show results
res <- clusterApply(cl, rep(1000, 100), myrdnorm)
hist(unlist(res))


select_words <- function(letter, words, min_length = 1) {
    min_length_words <- words[nchar(words) >= min_length]
    grep(paste0("^", letter), min_length_words, value = TRUE)
}

# Export "select_words" to workers
clusterExport(cl, "select_words")

# Split indices for two chunks
ind <- splitIndices(length(words), 2)

# Find unique words in parallel
result <- clusterApply(cl, x = list(words[ind[[1]]], words[ind[[2]]]),  
            function(w, ...) unique(select_words("v", w, ...)), 
            min_length = 10)
            
# Show vectorized unique results
unique(unlist(result))


# Earlier you defined a function ar1_multblocks() that takes a vector of row identifiers as argument and generates migration trajectories using the corresponding rows of the parameter set ar1est
# ar1_multblocks() depends on ar1_block() which in turns depends on ar1_trajectory()
# These functions along with the cluster object cl of size 4, function show_migration(), the dataset ar1est (reduced to 200 rows) and packages parallel and ggplot2 are available in your workspace

ar1_block <- function(id, rate0 = 0.015, traj_len = 15, block_size = 10) {
    trajectories <- matrix(NA, nrow = block_size, ncol=traj_len)
    for (i in seq_len(block_size)) 
        trajectories[i,] <- ar1_trajectory(unlist(ar1est[id, ]), rate0 = rate0, len = traj_len)
    trajectories
}

ar1_trajectory <- function(est, rate0, len = 15) {
    ar1 <- function(est, r) {
        # simulate one AR(1) value
        est['mu'] + est['phi'] * (r - est['mu']) + 
        rnorm(1, sd = est['sigma'])
    }
    trajectory <- rep(NA, len)
    rate <- rate0
    for (time in seq_len(len)) {
        trajectory[time] <- ar1(est, r = rate)
        rate <- trajectory[time]
    }
    trajectory
}

ar1_multblocks <- function(ids, ...) {
    trajectories <- NULL
    for (i in seq_along(ids))
        trajectories <- rbind(trajectories, ar1_block(ids[i], ...))
    trajectories
}

# Export data and functions
clusterExport(cl, c("ar1est", "ar1_block", "ar1_trajectory"))

# Process ar1_multblocks in parallel
res <- clusterApply(cl, 1:nrow(ar1est), ar1_multblocks)

# Combine results into a matrix and show results
trajs <- do.call(rbind, res)
show_migration(trajs)


# The object res returned by clusterApply() in the previous exercise is also in your workspace, now called res_prev
res_prev <- res

# Split task into 5 chunks
ind <- splitIndices(nrow(ar1est), 5)

# Process ar1_multblocks in parallel
res <- clusterApply(cl, ind, ar1_multblocks)

# Dimensions of results 
(res_dim <- c(length(res), nrow(res[[1]])))
(res_prev_dim <- c(length(res_prev), nrow(res_prev[[1]])))

stopCluster(cl)

```
  
  
  
***
  
Chapter 3 - foreach, future.apply, and Load Balancing  
  
foreach:  
  
* The looping construct can be applied using the foreach package (Calaway and Weston), similar to previous examples  
* The foreach makes it possible to run parallel processing for loops - code can be written the same way for both sequential and parallel applications  
* The basic syntax is foreach(…) %do% …  
	* library(foreach)  
    * foreach(n = rep(5, 3)) %do% rnorm(n)  
    * foreach(n = rep(5, 3), m = 10^(0:2)) %do% rnorm(n, mean = m)  # can pass arguments  
    * The foreach() call will return a value - in the example above, this would be a list  
* Can combine results using post-processing arguments - the .combine argument  
	* foreach(n = rep(5, 3), .combine = rbind) %do% rnorm(n)  # rbind of the three lists  
    * foreach(n = rep(5, 3), .combine = '+') %do% rnorm(n)  # sum across the three lists  
* Can also use list comprehensions with the foreach() function - using the %:% operator  
	* foreach(x = sample(1:1000, 10), .combine = c) %:% when(x %% 3 == 0 || x %% 5 == 0) %do% x  
  
foreach and parallel backends:  
  
* The most popular backend is the doParallel() call for parallel foreach() processing  
	* Other backends include doFuture() using the future package and doSEQ() which allows switching between parallel and sequential  
* The doParallel package by Calaway et al is an interface between foreach and parallel, and requires initialization with registerDoParallel()  
	* library(doParallel)  
    * registerDoParallel(cores = 3)  # uses multicore for Unix and snow for Windows  
    * cl <- makeCluster(3)  # can make your own clusters and pass them also  
    * registerDoParallel(cl)  # passing the cl object rather than cores (will default to snow since that is makeCluster() default  
* Examples of converting a sequential loop to a parallel loop  
	* library(foreach)  
    * foreach(n = rep(5, 3)) %do% rnorm(n)  
    * library(doParallel)  
    * cl <- makeCluster(3)  
    * registerDoParallel(cl)  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  # note the conversion to %dopar% which is what engages the parallel processing  
* The doFuture package (Bengtsson) is built on top of the future package  
	* The central idea is that there is a future plan for how foreach should work behind the scenes - sequential, cluster, multicore, multiprocess, etc.  
    * Other packages are available from future.batchtools  
    * library(doFuture)  
    * registerDoFuture()  
    * plan(cluster, workers = 3)  # using the cluster plan  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  # same use of foreach, with the cluster plan applied  
    * plan(multicore)  # using the multicore plan  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  
  
future and future.apply - packages that are continually under development:  
  
* The future package intends to have a uniform API for sequential and parallel processing  
* The future construct is for an expression that may be used in the future  
	* Example in plain R - x <- mean(rnorm(n, 0, 1)) ; y <- mean(rnorm(n, 10, 5)) ; print(c(x, y))  
    * Example in implicit futures - x %<-% mean(rnorm(n, 0, 1)) ; y %<-% mean(rnorm(n, 10, 5)) ; print(c(x, y))  
    * Example in explicit futures - x <- future(mean(rnorm(n, 0, 1))) ; y <- future(mean(rnorm(n, 10, 5))) ; print(c(value(x), value(y)))  
    * Values can be managed asynchronously, meaning that the next line can start running while the current line is still in process  
* The same code can then be run either in parallel or sequentially - sequential example  
	* plan(sequential)  
    * x %<-% mean(rnorm(n, 0, 1))  
    * y %<-% mean(rnorm(n, 10, 5))  
    * print(c(x, y))  
* The same code can then be run either in parallel or sequentially - parallel example  
	* plan(multicore)  
    * x %<-% mean(rnorm(n, 0, 1))  
    * y %<-% mean(rnorm(n, 10, 5))  
    * print(c(x, y))  
* The package future.apply is a higher level API for all the apply packages in R using futures  
	* Sibling to foreach  
    * functions include future_lapply(), future_sapply(), future_apply()  
* Example of using future.apply()  
	* lapply(1:10, rnorm)  # base R  
    * plan(sequential)  
    * future_lapply(1:10, rnorm)  # future.apply implementation of base R above  
    * plan(cluster, workers=4)  
    * future_lapply(1:10, rnorm)  # future.apply implementation of base R above, now run in parallel  
* Separating the plan from the processing allows for processing across many systems  
	* Single CPU uses sequential, cluster plan for many cores, etc.  
  
Load balancing and scheduling:  
  
* There can be significant node waiting times if the master waits for all workers to finish before assigning new tasks  
	* The clusterApplyLB() is designed to speed up processing by sending new tasks to workers as soon as they finish their old tasks  
* Communication overhead can also be a big problem when the tasks are small  
	* Can instead give the workers many tasks at once, and have them communicate with the master only at the start and the finish  
    * Drawback is that idle workers will not be available to help busy workers  
* Can chunk in parallel using the splitIndices() function  
	* splitIndices(10, 2)  
    * clusterApply(cl, x = splitIndices(10, 2), fun = sapply, "*", 100)  # multiply all numbers by 100  
    * Can also use parApply with the chunk.size option in R 3.5+  
    * foreach(s = isplitVector(1:10, chunks = 2)) %dopar% sapply(s, "*", 100)  # itertools functions to implement in foreach  
    * future_sapply(1:10, `*`, 100, future.scheduling = 1)  # 1 chunk per worker  
    * future_sapply(1:10, `*`, 100, future.scheduling = FALSE)  # 1 chunk per task  
  
Example code includes:  
```{r}

# Recall the first chapter where you found the most frequent words from the janeaustenr package that are of certain minimum length
result <- lapply(letters, max_frequency, words = words, min_length = 5) %>% 
    unlist
# In this exercise, you will implement the foreach construct to solve the same problem
# The janeaustenr package, a vector of all words from the included books, words, and a function max_frequency() for finding the results based on a given starting letter are all available in your workspace

# Load the package
library(foreach)

# foreach construct
result <- foreach(l = letters, .combine=c) %do% max_frequency(l, words=words, min_length=5)
                
# Plot results 
barplot(result, las = 2)


# Specifically, your job is to modify the code so that the maximum frequency for the first half of the alphabet is obtained for words that are two and more characters long, while the frequency corresponding to the second half of the alphabet is derived from words that are six and more characters long
# Note that we are using an alphabet of 26 characters

# foreach construct and combine into vector
result <- foreach(l = letters, n = rep(c(2, 6), each=13), .combine = c) %do% 
    max_frequency(l, words=words, min_length=n)
          
# Plot results
barplot(result, las = 2)


# Register doParallel with 2 cores
doParallel::registerDoParallel(cores=2)

# Parallel foreach loop
res <- foreach(r = rep(1000, 100), .combine = rbind, 
            .packages = "extraDistr") %dopar% myrdnorm(r)
            
# Dimensions of res
dim_res <- dim(res)


# So far you learned how to search for the most frequent word in a text sequentially using foreach()
# In the course of the next two exercises, you will implement the same task using doParallel and doFuture for parallel processing and benchmark it against the sequential version
# The sequential solution is implemented in function freq_seq() (type freq_seq in your console to see it)
# It iterates over a global character vector chars and calls the function max_frequency() which searches within a vector of words, while filtering for minimum word length
# All these objects are preloaded, as is the doParallel package
# Your job now is to write a function freq_doPar() that runs the same code in parallel via doParallel

freq_seq <- function(min_length = 5)
    foreach(l = letters, .combine = c) %do% 
        max_frequency(l, words = words, min_length = min_length)

# Function for doParallel foreach
freq_doPar <- function(cores, min_length = 5) {
    # Register a cluster of size cores
    doParallel::registerDoParallel(cores=cores)
    
    # foreach loop
    foreach(l=letters, .combine=c, 
            .export = c("max_frequency", "select_words", "words"),
            .packages = c("janeaustenr", "stringr")) %dopar%
        max_frequency(l, words=words, min_length=min_length)
}

# Run on 2 cores
freq_doPar(cores=2)


# Now your job is to create a function freq_doFut() that accomplishes the same task as freq_doPar() but with the doFuture backend
# Note that when using doFuture, arguments .packages and .export in foreach() are not necessary, as the package deals with the exports automatically
# You will then benchmark these two functions, together with the sequential freq_seq()
# All the functions from the last exercise are available in your workspace
# In addition, the packages doFuture and microbenchmark are also preloaded
# To keep the computation time low, the global chars vector is set to the first six letters of the alphabet only

cores <- 2
min_length <- 5

# Error in tweak.function(strategy, ..., penvir = penvir) : 
# Trying to use non-future function 'survival::cluster': function (x)  { ... }
# For solution see https://github.com/HenrikBengtsson/future/issues/152

# Function for doFuture foreach
freq_doFut <- function(cores, min_length = 5) {
    # Register and set plan
    doFuture::registerDoFuture()
    future::plan(future::cluster, workers=cores)

    # foreach loop
    foreach(l = letters, .combine = c) %dopar%
        max_frequency(l, words = words, min_length = min_length)
}

# Benchmark
microbenchmark(freq_seq(min_length),
               freq_doPar(cores, min_length),
               freq_doFut(cores, min_length),
               times = 1)

# It is straight forward to swap parallel backends with foreach
# In this small example, you might not see any time advantage in running it in parallel
# In addition, doFuture is usually somewhat slower than doParallel
# This is because doFuture has a higher computation overhead
# We encourage you to test these frameworks on more time-consuming applications where an overhead become negligible relative to the overall processing time

extract_words_from_text <- function(text) {
    str_extract_all(text, boundary("word")) %>% 
        unlist %>% 
        tolower
}

# Main function
freq_fapply <- function(words, chars=letters, min_length=5) {
    unlist(future.apply::future_lapply(chars, FUN=max_frequency, words = words, min_length = min_length))
}

obama <- readLines("./RInputFiles/obama.txt")
obama_speech <- paste(obama[obama != ""], collapse=" ")

# Extract words and call freq_fapply
words <- extract_words_from_text(obama_speech)
res <- freq_fapply(words)

# Plot results
barplot(res, las = 2)


# Now imagine you are a user of the fictional package from the previous exercise
# At home you have a two-CPU Mac computer, and at work you use a Linux cluster with two 16-CPU computers, called "oisin" and "oscar"
# Your job is to write a function for each of the hardware that calls freq_fapply() while taking advantage of all available CPUs
# For the cluster, you set workers to a vector of computer names corresponding to the number of CPUs, i.e. 16 x "oisin" and 16 x "oscar"
# For a one-CPU environment, we have created a function fapply_seq()

# fapply_seq <- function(...) {
#     future::plan(strategy="sequential") 
#     freq_fapply(words, letters, ...)
# }

# multicore function
# fapply_mc <- function(cores=2, ...) {
#     plan(strategy="multicore", workers=cores)
#     freq_fapply(words, letters, ...)
# }

# cluster function
# fapply_cl <- function(cores=NULL, ...) {
#     # set default value for cores
#     if(is.null(cores))
#         cores <- rep(c("oisin", "oscar"), each = 16)
#         
#     # parallel processing
#     plan(strategy="cluster", workers=cores)
#     freq_fapply(words, letters, ...)
# }


# Note: Multicore does not work on Windows. We recommend using the 'multiprocess' or 'cluster' plan on Windows computers.

# Microbenchmark
# microbenchmark(fapply_seq = fapply_seq(),
#                fapply_mc_2 = fapply_mc(cores=2), 
#                fapply_mc_10 = fapply_mc(cores=10),
#                fapply_cl = fapply_cl(cores=2), 
#                times = 1)

# Which is the slowest?
# slowest1 <- "fapply_cl"


# This is because for a small number of tasks a sequential code can run faster than a parallel version due to the parallel overhead
# The cluster plan has usually the largest overhead and thus, should be used only for larger number of tasks
# The multicore may be more efficient when the number of workers is equal to the number of cores
# It uses shared memory, and thus is faster than cluster


# In your workspace there is a vector tasktime containing simulated processing times of 30 tasks (generated using runif())
# There is also a cluster object cl with two nodes
# Your job is to apply the function Sys.sleep() to tasktime in parallel using clusterApply() and clusterApplyLB() and benchmark them
# The parallel and microbenchmark packages are loaded
# We also provided functions for plotting cluster usage plots called plot_cluster_apply() and plot_cluster_applyLB()
# Both functions use functionality from the snow package

tasktime <- c(0.1328, 0.1861, 0.2865, 0.4541, 0.1009, 0.4492, 0.4723, 0.3304, 0.3146, 0.031, 0.1031, 0.0884, 0.3435, 0.1921, 0.3849, 0.2489, 0.3588, 0.496, 0.1901, 0.3887, 0.4674, 0.1062, 0.3259, 0.0629, 0.1337, 0.1931, 0.0068, 0.1913, 0.4349, 0.1702)

# plot_cluster_apply <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::clusterApply(cl, x, fun)),
#             title = "Cluster usage of clusterApply")

# plot_cluster_applyLB <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::clusterApplyLB(cl, x, fun)),
#             title = "Cluster usage of clusterApplyLB")

# Benchmark clusterApply and clusterApplyLB
# microbenchmark(
#     clusterApply(cl, tasktime, Sys.sleep),
#     clusterApplyLB(cl, tasktime, Sys.sleep),
#     times = 1
# )

# Plot cluster usage
# plot_cluster_apply(cl, tasktime, Sys.sleep)
# plot_cluster_applyLB(cl, tasktime, Sys.sleep)


# Now we compare the results from the previous exercise with ones generated using parSapply(), which represents here an implementation that groups tasks into as many chunks as there are workers available
# You first explore its cluster usage plot, using the function plot_parSapply() we defined for you
# We generated a version of the tasktime vector, called bias_tasktime that generates very uneven load
# Your job is to compare the run times of parSapply() with clusterApplyLB() applied to bias_tasktime

# plot_parSapply <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::parSapply(cl, x, fun)),
#             title = "Cluster usage of parSapply")

# bias_tasktime <- c(1, 1, 1, 0.1, 0.1, 0.1, 1e-04, 1e-04, 1e-04, 0.001, 1)

# Plot cluster usage for parSapply
# plot_parSapply(cl, tasktime, Sys.sleep)

# Microbenchmark
# microbenchmark(
#     clusterApplyLB(cl, bias_tasktime, Sys.sleep),
#     parSapply(cl, bias_tasktime, Sys.sleep),
#     times = 1
# )

# Plot cluster usage for parSapply and clusterApplyLB
# plot_cluster_applyLB(cl, bias_tasktime, Sys.sleep)
# plot_parSapply(cl, bias_tasktime, Sys.sleep)

```
  
  
  
***
  
Chapter 4 - Random Numbers and Reproducibility  
  
Are results reproducible?  
  
* Many statistical numbers require the use of random numbers - MCMC, boot, simulation, sample, rnorm, etc.  
	* Typically, would use a set.seed() to initialize the RNG to a known state  
* Setting the seed typically does not guarantee reproducibility of parallel processing  
	* library(parallel)  
    * cl <- makeCluster(2)  
    * set.seed(1234)  
    * clusterApply(cl, rep(3, 2), rnorm)  
    * set.seed(1234)  # only sets the RNG in the master node; thus not sent to the workers  
    * clusterApply(cl, rep(3, 2), rnorm)  # will get different sets of results 
* Can instead set the RNG for each of the workers  
	* clusterEvalQ(cl, set.seed(1234))  
    * clusterApply(cl, rep(3, 2), rnorm)  # both clusters will give the identicla results  
* There is another common and not recommended method of generating random numbers in parallel code - gives statistical properties that are not desirable  
	* for (i in 1:2) {  
    *     set.seed(1234)  
    *     clusterApply(cl, sample(1:10000000, 2), set.seed)  
    *     print(clusterApply(cl, rep(3, 2), rnorm))  
    * }  
  
Parallel random number generators:  
  
* A good RNG should comply with certain parameters - long period > 2**100, good structural properties in high dimensions, reproducible  
	* These parameters should hold in a distributed environment also  
* RNG streams with period 2**291 and seeds 2**127 steps apart is available based on research by L'Ecuyer  
	* Allows for each part of a parallel process to have reproducible streams with the proper properties  
    * Available in R through rlecuyer and rstream  
    * Also available in R Code using RNGkind("L'Ecuyer-CMRG")  
* The L'Ecuyer generator is the default when using parallel but needs to be initialized with a seed for cluster cl  
	* clusterSetRNGStream(cl, iseed = 1234)  # initializes a reproducible and independent seed for each of the workers  
* Reproducibility in parallel depends on the task flow - certain conditions need to apply  
	* Runs on clusters of the same size (different number of workers means different random numbers)  
    * Cannot use load balancing (balancing loads means non-deterministic scheduling and assignment)  
  
Reproducibility in foreach and future.apply:  
  
* Can achieve reproducibility in foreach using doRNG - one stream per task  
	* library(doRNG)  
    * library(doParallel)  
    * registerDoParallel(cores = 3)  
    * set.seed(1)  
    * res1 <- foreach(n = rep(2, 5), .combine = rbind) %dorng% rnorm(n)  
    * set.seed(1)  
    * res2 <- foreach(n = rep(2, 5), .combine = rbind) %dorng% rnorm(n)  
    * identical(res1, res2)  # TRUE  
* Can also use doRNG by way of %dopar%  
	* library(doRNG)  
    * library(doParallel)  
    * registerDoParallel(cores = 3)  
    * registerDoRNG(1)  # 1 is the seed  
    * res3 <- foreach(n = rep(2, 5), .combine = rbind) %dopar% rnorm(n)  
    * set.seed(1)  
    * res4 <- foreach(n = rep(2, 5), .combine = rbind) %dopar% rnorm(n)  
    * c(identical(res1, res3), identical(res2, res4))  # TRUE TRUE  
* Can also use independent streams in future.apply  
	* library(future.apply)  
    * plan(sequential)  
    * res5 <- future_lapply(1:5, FUN = rnorm, future.seed = 1234)  
    * plan(multiprocess)  
    * res6 <- future_lapply(1:5, FUN = rnorm, future.seed = 1234)  
    * identical(res5, res6)  # TRUE  
  
Next steps:  
  
* The parallel package is the baseline for many other packages  
	* Often not reproducible  
* The foreach package with doParallel and doFuture can be reproducible using doRNG  
* The future.apply package has an inutitive apply-like syntax and is always reproducible if future.seed is set  
* General best practices include  
	* Minimize overhead and master-worker communication frequencies  
    * Use scheduling and load balancing through effective use of chunking  
    * Be careful that parallel overhead can actually make tasks longer if the tasks are all small and the communication takes more time than the calculations  
  
Example code includes:  
```{r}

# In addition to the code in the previous exercise, we also created a FORK cluster for you.

# cl.fork <- makeCluster(2, type = "FORK")

# Your job is to register the two cluster objects with the preloaded doParallel package and compare results obtained with parallel foreach
# How do the results differ in terms of reproducibility?

library(doParallel)
cl.sock <- makeCluster(2, type = "PSOCK")
registerDoParallel(cl.sock)
set.seed(100)
foreach (i = 1:2) %dopar% rnorm(3)


# Register and use cl.sock
registerDoParallel(cl.sock)
replicate(2, {
    set.seed(100)
    foreach(i = 1:2, .combine = rbind) %dopar% rnorm(3)
    }, simplify = FALSE
)

# Register and use cl.fork
# registerDoParallel(cl.fork)
# replicate(2, {
#     set.seed(100)
#     foreach(i = 1:2, .combine = rbind) %dopar% rnorm(3)
#     }, simplify = FALSE
# )


# Create a cluster
cl <- makeCluster(2)

# Check RNGkind on workers
clusterCall(cl, RNGkind)

# Set the RNG seed on workers
clusterSetRNGStream(cl, iseed=100)

# Check RNGkind on workers
clusterCall(cl, RNGkind)


# Now you are ready to make your results reproducible
# You will use the simple embarrassingly parallel application for computing a mean of random numbers (myfunc) which we parallelized in the second chapter using clusterApply()
# The parallel package, myfunc() , n (sample size, set to 1000) and repl (number of replicates, set to 5) are available in your workspace
# You will now call clusterApply() repeatedly to check if results can be reproduced, without and with initializing the RNG

n <- 1000
repl <- 5

# Create a cluster of size 2
cl <- makeCluster(2)

# Call clusterApply three times
for(i in 1:3)
    print(unlist(clusterApply(cl, rep(n, repl), myfunc)))

# Create a seed object
seed <- 1234

# Repeatedly set the cluster seed and call clusterApply()
for(i in 1:3) {
    clusterSetRNGStream(cl, iseed = seed)
    print(unlist(clusterApply(cl, rep(n, repl), myfunc)))
}


# Create two cluster objects, of size 2 and 4
cl2 <- makeCluster(2)
cl4 <- makeCluster(4)

# Set seed on cl2 and call clusterApply
clusterSetRNGStream(cl2, iseed = seed)
unlist(clusterApply(cl2, rep(n, repl), myfunc))

# Set seed on cl4 and call clusterApply
clusterSetRNGStream(cl4, iseed = seed)
unlist(clusterApply(cl4, rep(n, repl), myfunc))


# Register doParallel and doRNG
library(doRNG)
registerDoParallel(cores = 2)
doRNG::registerDoRNG(seed)

# Call ar1_block via foreach
mpar <- foreach(r=1:5) %dopar% ar1_block(r)

# Register sequential backend, set seed and run foreach
registerDoSEQ()
set.seed(seed)
mseq <- foreach(r=1:5) %dorng% ar1_block(r)

# Check if results identical
identical(mpar, mseq)


# You are able to reproduce sequential and parallel applications! Remember to always use %dorng% if you use the doSEQ backend
# Also note that by default on the Linux DataCamp server, registerDoParallel() creates a FORK cluster if a number of cores is passed to it
# As a result, there was no need to export any functions to workers, as they were copied from the master
# On a different platform, the .export option may be needed


# Set multiprocess plan 
future::plan(strategy="multiprocess", workers = 2)

# Call ar1_block via future_lapply
mfpar <- future.apply::future_lapply(1:5, FUN=ar1_block, future.seed=seed)

# Set sequential plan and repeat future_lapply
future::plan(strategy="sequential")
mfseq <- future.apply::future_lapply(1:5, FUN=ar1_block, future.seed=seed)

# Check if results are identical
identical(mfpar, mfseq)


rm(mean)
rm(sd)

```
  
  
  
***
  
### _Marketing Analytics in R: Choice Modeling_  
  
Chapter 1 - Quickstart Guide  
  
Why choice?  
  
* Choice modeling (and conjoint) is a common and popular tool used in marketing  
	* Linear regression is about predicting a number based on features  
    * Frequently, though, we want to make a choose from a selection of objects (picking a show, purchasing a car, etc.)  
* Multinomial logit (logistic regressions) work well with choice data  
* Choice models can be helpful for feature selection, pricing, trade-offs between quality/cost, etc.  
  
Inspecting choice data:  
  
* Choice data does not fit in to normal predictive modeling formats  
	* For regression, data are typically one row per observation  
    * For choice datasets, data are typically stacked in to a few rows, where each row describes an alternative (rather than an observation), with a flag for which option was chosen  
* May want to count up the number of choices made (total or proportions)  
	* xtabs(choice ~ price, data=sportscar)  
  
Fitting and interpreting a choice model:  
  
* Fitting choice models is similar to fitting linear models  
	* my_model <- lm(y ~ x1 + x2 + x3, data=lm_data)  
    * library(mlogit)  # multinomial logit is needed rather than GLM  
    * mymodel <- mlogit(choice ~ feature1 + feature2 + feature3, data = choice_data)  # data must be choice data, including both ques, alt, and choice columns  
    * summary(mymodel)  
* Coefficients of magnitude greater than 1 are of very high impact in the decisions (rule of thumb)  
  
Using choice models to make decisions:  
  
* Can use the choice models to predict market shares  
    * predict_mnl(model, products)  # for mlogit models - written by instructor  
  
Example code includes:  
```{r eval=FALSE}

# Unload conflicting namespaces
unloadNamespace("rms")
unloadNamespace("quantreg")
unloadNamespace("MatrixModels")

unloadNamespace("lmerTest")
unloadNamespace("semPlot")
unloadNamespace("rockchalk")
unloadNamespace("qgraph")
unloadNamespace("sem")
unloadNamespace("mi")
unloadNamespace("arm")
unloadNamespace("mice")
unloadNamespace("mitml")
unloadNamespace("jomo")
unloadNamespace("arm")
unloadNamespace("jomo")
unloadNamespace("lme4")


# load the mlogit library 
library(mlogit)


scLong <- read.csv("./RInputFiles/sportscar_choice_long.csv")
scWide <- read.csv("./RInputFiles/sportscar_choice_wide.csv")
sportscar <- scLong
sportscar$alt <- as.factor(sportscar$alt)
sportscar$seat <- as.factor(sportscar$seat)
sportscar$price <- as.factor(sportscar$price)
sportscar$choice <- as.logical(sportscar$choice)
sportscar <- sportscar %>% rename(resp.id=resp_id)
sportscar$key <- rep(1:2000, each=3)
row.names(sportscar) <- paste(sportscar$key, sportscar$alt, sep=".")
sportscar <- mlogit.data(sportscar, shape="long", choice="choice", alt.var="alt")
str(sportscar)

# Create a table of chosen sportscars by transmission type
chosen_by_trans <- xtabs(choice ~ trans, data = sportscar)

# Print the chosen_by_trans table to the console
chosen_by_trans

# Plot the chosen_by_price object
barplot(chosen_by_trans)


# Crashes out due to issue with class "family" in MatrixModels and lme4
m1 <- mlogit(choice ~ seat + trans + convert + price, data=sportscar, seed=10)

# fit a choice model using mlogit() and assign the output to m1
# m1 <- mlogit::mlogit(choice ~ seat + trans + convert + price, 
#                      data=sportscar, 
#                      chid.var="key", 
#                      alt.var="alt", 
#                      choice="choice", 
#                      seed=10
#                      )

# summarize the m1 object to see the output of the choice model
summary(m1)


predict_mnl <- function(model, products) {
  # model: mlogit object returned by mlogit()
  # data: a data frame containing the set of designs for which you want to 
  #       predict shares.  Same format at the data used to estimate model. 
  data.model <- model.matrix(update(model$formula, 0 ~ .), data = products)[,-1]
  utility <- data.model%*%model$coef
  share <- exp(utility)/sum(exp(utility))
  cbind(share, products)
}

# inspect products
products <- data.frame(seat=factor("2", levels=c("2", "4", "5")), 
                       trans=factor(rep(c("manual", "auto"), each=2), levels=c("auto", "manual")), 
                       convert=factor(rep(c("no", "yes"), times=2), levels=c("no", "yes")), 
                       price=factor("35", levels=c("30", "35", "40"))
                       )
str(products)

# use predict_mnl to predict share for products
shares <- predict_mnl(m1, products)

# print the shares to the console
shares


barplot(shares$share, ylab="Predicted Market Share", 
        names.arg=c("Our Car", "Comp 1", "Comp 2", "Comp 3"))

```
  
  
  
***
  
Chapter 2 - Managing and Summarizing Choice Data  
  
Assembling choice data:  
  
* Choices made in the wild (revealed preference data) can be analyzed using the transaction record and product set available  
* Can instead run a survey with hypothetical decision making (conjoint)  
* Sometimes, data are provided in wide format, with sets of columns describing the choices  
  
Converting from wide to long:  
  
* Often helpful to convert the wide format data to long format instead  
	* sportscar <- reshape(sportscar_wide, direction="long", varying = list(seat=5:7, trans=8:10, convert=11:13, price=14:16), v.names = c("seat", "trans", "convert", "price"), timevar="alt")  # column labels are given in v.names; timevar="alt" means make an alt column  
    * new_order <- order(sportscar$resp_id, sportscar$ques, sportscar$alt)  
    * sportscar <- sportscar[new_order,]  # ordered by question  
    * sportscar$choice <- sportscar$choice == sportscar$alt  # make a boolean rather than an integer  
  
Choice data in two files:  
  
* Can receive choice data from two separate files - alternatives and choices  
	* sportscar <- merge(sportscar_choices, sportscar_alts, by=c("resp_id", "ques"))  
  
Visualizing choce data:  
  
* Data in long format can be summarized and visualized  
	* xtabs(~ trans, data = sportscar)  # just get the totals by transmission  
    * xtabs(~ trans + choice, data = sportscar)  # COUNT of transmission by choice  
    * xtabs(choice ~ trans, data=sportscar)  # SUM of choice by trans  
    * plot(xtabs(~ trans + choice, data=sportscar))  # mosaic plot  
    * plot(xtabs(~ trans + segment + choice, data=sportscar))  # mosaic plot split primarily by trans, then with segment and choice  
  
Designing a conjoint survey:  
  
* Conjoint surveys are popular for product design - can include any number of features  
	* Begin picking attributes of interest (commonly 8-10) and levels of interest (commonly 2-5 per level)  
* Need to decide which product profiles to show to which users and which questions  
* Can create a random design in R  
	* attribs <- list(Type=c("Milk", "Dark", "White"), Brand=c("Cadbury", "Toblerone", "Kinder"), Price=5:30/10)  
    * all_comb <- expand.grid(attribs)  
    * for (i in 1:100) {  
    *     rand_rows <- sample(1:nrow(all_comb), size=12*3)  
    *     rand_alts <- all_comb[rand_rows, ]  
    *     choc_survey[choc_survey$Subject==i, 4:6] <- rand_alts  
    * }  
* Can code the survey in html or use a platform like Google or Survey Monkey  
	* Can also use a firm like Qualtrics  
  
Example code includes:  
```{r}

chLong <- read.csv("./RInputFiles/chocolate_choice_long.csv")
chWide <- read.csv("./RInputFiles/chocolate_choice_wide.csv")
chocolate_wide <- chWide

# Look at the head() of chocolate_wide
head(chocolate_wide)

# Use summary() to see which brands and types are in chocolate_wide
summary(chocolate_wide)


# use reshape() to change the data from long to wide 
chocolate <- reshape(data= chocolate_wide , direction="long", 
                     varying = list(Brand=3:5, Price=6:8, Type=9:11), 
                     v.names=c("Brand", "Price", "Type"), timevar="Alt")
                     
# use head() to confirm that the data has been properly transformed
head(chocolate)


# Create the new order for the chocolate data frame
new_order <- order(chocolate$Subject, chocolate$Trial, chocolate$Alt)

# Reorder the chocolate data frame to the new_order
chocolate <- chocolate[new_order,]

# Look at the head() of chocolate to see how it has been reordered
head(chocolate)


# Use head(chocolate) and look at the Selection variable. 
head(chocolate)

# Transform the Selection variable to a logical indicator
chocolate$Selection <- chocolate$Alt == chocolate$Selection

# Use head(chocolate) to see how the Selection variable has changed. Now it is logical.
head(chocolate)


choc_choice <- chocolate %>%
    filter(Selection==TRUE) %>%
    mutate(Selection=Alt) %>%
    select(Subject, Trial, Response_Time, Selection)
choc_alts <- chocolate %>%
    select(Subject, Trial, Alt, Brand, Price, Type)

str(choc_choice)
str(choc_alts)


# Merge choc_choice and choc_alts
choc_merge <- merge(choc_choice, choc_alts, by=c("Subject", "Trial"))

# Convert Selection to a logical variable
choc_merge$Selection <- choc_merge$Selection == choc_merge$Alt

# Inspect chocolate_merge using head
head(choc_merge)


# Use xtabs to count up how often each Type is chosen
counts <- xtabs(~ Type + Selection, data=chocolate)

# Plot the counts
plot(counts, cex = 1.5)


# Modify this code to count up how many times each **Brand** is chosen
counts <- xtabs(~ Brand + Selection, data=chocolate)

# Plot the counts
plot(counts, cex = 1.5)


# Use xtabs to count up how often each Price is chosen
counts <- xtabs(~ Price + Selection, data=chocolate)

# Plot the counts
plot(counts, cex=0.6)

```
  
  
  
***
  
Chapter 3 - Building Choice Models  
  
Choice models - under the hood:  
  
* The multimonial logit model begins with a linear equation for utility which drives probabilities  
	* v1 <- alpha * 4 + beta * 100  # value  
    * v2 <- alpha * 5 + beta * 150  
    * v2 <- alpha * 2 + beta * 175  
    * u1 <- v1 + error1  # utility  
    * u2 <- v2 + error2  
    * u3 <- v3 + error3  
    * choice <- which.max(c(u1, u2, u3))  
    * p1 <- exp(v1) / ( exp(v1) + exp(v2) + exp(v3) )  
    * p2 <- exp(v2) / ( exp(v1) + exp(v2) + exp(v3) )  
    * p3 <- exp(v3) / ( exp(v1) + exp(v2) + exp(v3) )  
* We want to estimate the parameters that best calculate the v  
	* m1 <- mlogit(choice ~ 0 + seat + price, data=sportscar, print.level=3)  # find parameters that maximize likelihood, print all iterations as per print.level=3  
    * summary(m1)  
* The mlogit requires a specific data format  
	* sportscar <- mlogit.data(sportscar.df, shape="long", choice="choice", varying=5:8, alt.var="alt")  # what was chose, what attributes vary, and what is the alternative number column  
  
Interpreting choice model parameters:  
  
* Coefficients for each attribute are multiplied by the level of that attribute to create the v1/v2/v3  
* Factor variables are converted to numbers in the model.matrix process - need to understand what is the zero level  
	* head(model.matrix(m2))  
    * head(sportscar)  
* May want to convert to factors even if the data are numeric  
	* sportscar$seat <- as.factor(sportscar$seat)  
    * m3 <- mlogit(choice ~ 0 + seat + trans + convert + price, data=sportscar)  
    * summary(m3)  
* Can make price response non-linear (factor) or keep linear and assume Willingness to Pay  
	* coef(m3)  
    * coef(m3)/-coef(m3)[5]  # assumes that price is the 5th element of the coef vector  
  
Intercepts and interactions:  
  
* There is no intercept in the v1, v2, v3, since adding the same constant to each of them cancels out once the exponentials are taken  
	* The intercept is not identified, and so fixing the intercept using ~ 0 + is preferred (model will just pick a random one otherwise)  
* An interaction term is based on the multiplication of several terms from the original dataset  
	* m4 <- mlogit(choice ~ 0 + seat + trans + convert + price + trans:price, data=sportscar)  # interaction between transmission and price  
    * m4 <- mlogit(choice ~ 0 + seat + trans*convert + price, data=sportscar)  # same command as above, less typing  
* Interpreting the standard errors along with the z-values and p-values is similar to any other regression  
* Can also add segments, as long as they interact with at least one of the attributes  
	* m5 <- mlogit(choice ~ 0 + seat + convert + trans + price:segment, data=sportscar)  
  
Predicting shares:  
  
* Share predictions can be a good way to communicate preferences  
* Begin by creating vectors of attributes of interest  
	* price <- c(35, 30)  
    * seat <- factor(c(2, 2), levels=c(2,4,5))  
    * trans <- factor(c("manual", "auto"), levels=c("auto", "manual"))  
    * convert <- factor(c("no", "no"), levels=c("no", "yes"))  
    * segment <- factor(c("basic", "basic"), levels=c("basic", "fun", "racer"))  
    * prod <- data.frame(seat, trans, convert, price, segment)  
* Use a model prediction to make the share predictions  
	* m5 <- mlogit(choice ~ 0 + seat + convert + trans + price:segment, data=sportscar)  
    * prod.coded <- model.matrix(update(m5$formula, 0 ~ .), data = prod)[,-1]  
    * v <- prod.coded %*% m5$coef  
    * p <- exp(u) / sum(exp(u))  
    * cbind(p, prod)  
    * predict_mnl <- function(model, products) {  
    *     data.model <- model.matrix(update(model$formula, 0 ~ .), data = products)[,-1]  
    *     utility <- data.model%*%model$coef  
    *     share <- exp(utility)/sum(exp(utility))  
    *     cbind(share, products)  
    * }  
    * shares <- predict_mnl(m5, products)  
    * barplot(shares$share, horiz = TRUE, col="tomato2", xlab = "Predicted Market Share", names.arg = c("Our Sportscar", "Competitor 1"))  
  
Example code includes:  
```{r eval=FALSE}

# use mlogit.data() to convert chocolate to mlogit.data
chocolate_df <- mlogit.data(chocolate, shape = "long",
                            choice = "Selection", alt.var = "Alt", 
                            varying = 6:8)
                         
# use str() to confirm that chocolate is an mlogit.data object
str(chocolate_df)


# Fit a model with mlogit() and assign it to choc_m1
choc_m1 <- mlogit(Selection ~ Brand + Type + Price, data=chocolate_df, print.level=3)

# Summarize choc_m1 with summary()
summary(choc_m1)


# modify the call to mlogit to exclude the intercept
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data = chocolate_df, print.level=3)

# summarize the choc_m2 model
summary(choc_m2)


# compute the wtp by dividing the coefficient vector by the negative of the price coefficient
coef(choc_m2) / -coef(choc_m2)["Price"]


# change the Price variable to a factor in the chocolate data
chocolate$Price <- as.factor(chocolate$Price)

# fit a model with mlogit and assign it to choc_m3
choc_m3 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)

# inspect the coefficients
summary(choc_m3)


# likelihood ratio test comparing two models
lrtest(choc_m2, choc_m3)


# add the formula for mlogit
choc_m4 <- mlogit(Selection ~ 0 + Brand + Type + Price + Brand:Type, data=chocolate)

# use summary to see the coefficients
summary(choc_m4)


# add the formula for mlogit
choc_m5 <- mlogit(Selection ~ 0 + Brand + Type + Price + Price:Trial, data=chocolate)

# use summary to see the outputs
summary(choc_m5)


# add the formula for mlogit
choc_m5 <- mlogit(Selection ~ 0 + Brand + Type + Price + Price:Trial, data=chocolate)

# use summary to see the outputs
summary(choc_m5)


predict_mnl <- function(model, products) {
  data.model <- model.matrix(update(model$formula, 0 ~ .), 
                             data = products)[,-1]
  utility <- data.model%*%model$coef
  share <- exp(utility)/sum(exp(utility))
  cbind(share, products)
}

# modify the code below so that the segement is set to "racer" for both alternatives
price <- c(35, 30)
seat <- factor(c(2, 2), levels=c(2,4,5))
trans <- factor(c("manual", "auto"), levels=c("auto", "manual"))
convert <- factor(c("no", "no"), levels=c("no", "yes"))
segment <- factor(c("racer", "racer"), levels=c("basic", "fun", "racer"))
prod <- data.frame(seat, trans, convert, price, segment)

# predict shares for the "racer" segment
predict_mnl(model=m5, products=prod)


# fit the choc_m2 model
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)

# create a data frame with the Ghiradelli products
Brand <- factor(rep("Ghirardelli", 5), level = levels(chocolate$Brand))
Type <- levels(chocolate$Type)
Price <- 3   # treated as a number in choc_m2
ghir_choc <- data.frame(Brand, Type, Price)

# predict shares
predict_mnl(model=choc_m2, products=ghir_choc)


# compute and save the share prediction 
shares <- predict_mnl(choc_m2, ghir_choc)

# make a barplot of the shares
barplot(shares$share, 
        horiz = TRUE, col="tomato2",
        xlab = "Predicted Market Share", 
        main = "Shares for Ghiradelli chocolate bars at $3 each", 
        names.arg = levels(chocolate$Type)
        )

```
  
  
  
***
  
Chapter 4 - Hierarchical Choice Models  
  
What is a hierarchical choice model?  
  
* Hierarchical choice models (random coefficient models) account for differences in preferences across entities (heterogeneity)  
* An assumption is made that each individual is pulled from a distribution  
	* for (i in 1:n_resp) {  
    *     beta[i] <- mvrnorm(1, beta_0, Sigma)  # random normal vector  
    *     for (j in 1:n_task[i]) {  
    *         X <- X[X$resp == i & X$task == j, ]  
    *         u <- X %*% beta[i]  
    *         p[i,] <- exp(u) / sum(exp(u))  
    *     }  
    * }  
* Can fit the hierarchical model using the mlogit() function  
	* sportscar <- mlogit.data(sportscar, choice="choice", shape="long", varying=5:8, alt.var="alt", id.var = "resp_id")  
    * m7 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, rpar = c(price = "n"), panel = TRUE)  # rpar=c(price="n") will nornmally distribute price parameter across people  
    * summary(m7)  
    * plot(m7)  # plotting function for mlogit objects  
  
Heterogeneity in preferences for other features:  
  
* Might have heterogeneity in choices on many other attributes, including factor data  
* A different manner of coding factors can work better for hierarchical models  
	* Effects coding has -1, 0, 1 so that the factors are relative to average rather than relative to the first factor  
    * contrasts(sportscar$seat) <- contr.sum(levels(sportscar$seat))  # stores effects coding with the data frame  
    * dimnames(contrasts(sportscar$seat))[[2]] <- levels(sportscar$seat)[1:2]  # improve readability  
* Can make all of the coefficients heterogeneous  
	* my_rpar <- c("n", "n", "n", "n", "n")  # make them all normal  
    * m3 <- mlogit(choice ~ 0 + seat + trans + convert + price, data=sportscar)  # get coefficient names  
    * names(my_rpar) <- names(m3$coefficients)  # assign them to my_rpar  
    * m8 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, panel = TRUE, rpar = my_rpar)  # fit the model  
    * plot(m8, par=c("seat4", "seat5"))  
    * -sum(m8$coef[1:2])  # can get the 2-seat coefficient since it no longer needs to be 0  
  
Predicting shares with hierarchical models:  
  
* Can predict shares with a hierarchical model, including those where decision-making preferences may be correlated  
	* m10 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, rpar = myrpar, panel=TRUE, correlation = TRUE)  
    * cor.mlogit(m10)  
    * mean <- m10$coef[1:5] # hard coded  
    * Sigma <- cov.mlogit(m10)  
    * share <- matrix(NA, nrow=1000, ncol=nrow(prod.coded))  
    * for (i in 1:1000) {  
    *     coef <- mvrnorm(1, mu=mean, Sigma=Sigma)  
    *     utility <- prod.coded %*% coef  
    *     share[i,] <- exp(utility) / sum(exp(utility))  
    * }  
    * cbind(colMeans(share), prod)  
* Niche product shares tend to increase when heterogeneity is included - each element of the niche may be a small preference, but correlated with preference for other elements of the niche  
  
Wrap up:  
  
* Decisions need to be made in building a choice model - attributes, numeric vs. factors, hierarchical, distributions, nesting, etc.  
* Can start with basic models and build out as needed  
	* Inspect the data - investigate a few choices to confirm understanding of the data  
    * Run the model and inspect the standard errors - if too high, simplify  
    * Heterogeneity is a good idea whenever the decisions are being made by humans  
  
Example code includes:  
```{r eval=FALSE}

# Determine the number of subjects in chocolate$Subjects
length(levels(chocolate$Subject))


# add id.var input to mlogit.data call
chocolate <- mlogit.data(chocolate, choice = "Selection", shape="long", 
                         varying=6:8, alt.var = "Alt", id.var = "Subject"
                         )
                         
# add rpar and panel inputs to mlogit call
choc_m6 <- mlogit(Selection ~ 0 + Brand + Type + Price, data = chocolate, 
                  rpar = c(Price="n"), panel=TRUE)

# plot the model
plot(choc_m6)


# set the contrasts for Brand to effects code
contrasts(chocolate$Brand) <- contr.sum(levels(chocolate$Brand))
dimnames(contrasts(chocolate$Brand))[[2]] <- levels(chocolate$Brand)[1:4]
contrasts(chocolate$Brand)

# set the contrasts for Type to effects code
contrasts(chocolate$Type) <- contr.sum(levels(chocolate$Type))
dimnames(contrasts(chocolate$Type))[[2]] <- levels(chocolate$Type)[1:4]
contrasts(chocolate$Type)


# create my_rpar vector
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)
my_rpar <- rep("n", length(choc_m2$coef))
names(my_rpar) <- names(choc_m2$coef)
my_rpar

# fit model with random coefficients
choc_m7 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate, rpar=my_rpar, panel=TRUE)


# print the coefficients 
choc_m7$coef[5:8]

# compute the negative sum of those coefficients
-sum(choc_m7$coef[5:8])


# Extract the mean parameters and assign to mean
mean <- choc_m8$coef[1:9]   

# Extract the covariance parameters and assign to Sigma
Sigma <- cov.mlogit(choc_m8) 

# Create storage for individual draws of share
share <- matrix(NA, nrow=1000, ncol=nrow(choc_line_coded))

# For each draw (person)
for (i in 1:1000) { 
  # Draw a coefficient vector
  coef <- mvrnorm(1, mu=mean, Sigma=Sigma)
  # Compute utilities for those coef
  utility <- choc_line_coded %*% coef
  # Compute probabilites according to logit formuila
  share[i,] <- exp(utility) / sum(exp(utility))
}  

# Average the draws of the shares
cbind(colMeans(share), choc_line)

```
  
  
  
***
  
### _Single-Cell RNA-Seq Workflows in R_  
  
Chapter 1 - What Is RNA Single-Cell RNA-Seq?  
  
Background and utility:  
  
* Can get gene expression data at the cellular level - allows for better resolution of gene expressions  
	* Previous methods would get a mix of gener expressions across many cells - better for averages and distributions, but cannot identify a specific observation or cell type  
    * Implications are significant for personalized medicine and related areas  
* The data structure from the lab is a matrix - geners are rows and cells are columns  
	* ATCG counts by intersection  
    * Gene-level covariates  
    * Cell-level covariates  
* There are many zeroes - gene not expressed in cell, or dropouts (technical errors)  
  
Typical workflow:  
  
* There has been an exponential scaling in the ability to extract RNA data from 2009 - 2018  
	* Full-range technologies try to capture the full RNA sequence  
    * Tide-based (?) technologies try to capture the two ends of an RNA technology  
* Quality control is the process of removing problematic cells - library size and cell coverage metrics  
	* Library size is the total number of reads assigned to each cell  
    * Coverage is the total number of cells with reads assigned for that gene  
* Workflows then include normalization, dimensionality reduction, clustering, DE analysis (biomarkers with differential expression)  
  
Load, create, and access data:  
  
* The SingleCellExperiment (SCE) class is an S4 class for storing data from single-cell experiments  
	* source("https://bioconductor.org/biocLite.R")  
    * biocLite("SingleCellExperiment")  
    * library(SingleCellExperiment)  
    * counts <- matrix(rpois(8, lambda = 10), ncol = 2, nrow = 4)  
    * rownames(counts) <- c("Lamp5", "Fam19a1", "Cnr1", "Rorb")  
    * colnames(counts) <- c("SRR2140028", "SRR2140022")  
    * counts  
* Can create SCE data using the constructor  
	* sce <- SingleCellExperiment(assays = list(counts = counts), rowData = data.frame(gene = rownames(counts)), colData = data.frame(cell = colnames(counts)))  
* Can create SCE data using coercion  
	* se <- SummarizedExperiment(assays = list(counts = counts))  
    * sce <- as(se, "SingleCellExperiment")  
* Can apply process to a real dataset  
	* library(scRNAseq); data(allen)  # subset of mouse visual coretex data  
  
Example code includes:  
```{r eval=FALSE}

# head of count matrix
counts[1:3, 1:3]

# count of specific gene and cell
alignedReads <- counts['Cnr1', "SRR2140055"]

# overall percentage of zero counts 
pZero <- mean(counts == 0)

# cell library size
libsize <- colSums(counts)


# find cell coverage
coverage <- colMeans(counts > 0)
cell_info$coverage <- coverage

# load ggplot2
library(ggplot2)

# plot cell coverage
ggplot(cell_info, aes(x = names, y = coverage)) + 
  geom_point() +
  ggtitle('Cell Coverage') + 
  xlab('Cell Name') + 
  ylab('Coverage')


# mean of GC content
gc_mean <- mean(gene_info$gc)

# standard deviation of GC content
gc_sd <- sd(gene_info$gc)

# boxplot of GC content 
boxplot(gene_info$gc, main = 'Boxplot - GC content', ylab = 'GC content')


# batch
batch <- cell_info$batch

# patient
patient <- cell_info$patient

# nesting of batch within patient
batch_patient <- table(batch = batch, patient = patient)

# explore batch_patient
batch_patient


# load SingleCellExperiment
library(SingleCellExperiment)

# create a SingleCellExperiment object
sce <- SingleCellExperiment(assays = list(counts = counts ),
                            rowData = data.frame(gene_names = rownames(counts)),
                            colData = data.frame(cell_names = colnames(counts)))


# create a SummarizedExperiment object from counts
se <- SummarizedExperiment(assays = list(counts = counts))

# create a SingleCellExpression object from se
sce <- as(se, "SingleCellExperiment")


# create SingleCellExperiment object
sce <- as(allen, "SingleCellExperiment")

# cell information
cell_info <- colData(sce)

# size factors
sizeFactors(sce) <- colSums(assay(sce))

```
  
  
  
***
  
Chapter 2 - Quality Control and Normalization  
  
Quality Control:  
  
* Need to remove low quality cells and genes - identify first  
* Tung dataset includes three replicates  
	* sce  
    * library(scater)  
    * sce <- calculateQCMetrics(sce, feature_controls = list(ERCC = isSpike(sce, "ERCC"))  
* ERCC spiking genes are used for quality control - filter out cells with improper ratios (usually too much due to dead or stressed or the like) of "spiking"  
* Key functions used in the exercises include  
	* calculateQCMetrics()  
    * counts()  
    * rowSums()  
    * grepl()  
    * isSpike()  
    * plot(density(x))  
    * abline()  
  
Quality Control (continued):  
  
* Can filter based on library sizes using  
	* threshold <- 20000  
    * plot(density(sce$total_counts), main = "Density - total_counts")  
    * abline(v = threshold)  
    * keep <- (sce$total_counts > threshold)  
    * table(keep)  
* Can look at plots of the data  
	* scater::plotPhenoData(sce, aes_string(x = "total_counts", y = "total_counts_ERCC", colour = "batch"))  
* Can then filter based on data that do not meet key criteria in the plot  
	* keep <- (sce$batch != "NA19098.r2")  
    * table(keep)  
    * filter_genes <- apply(counts(sce), 1, function(x) length(x[x >= 2] >= 2)  # keep genes with counts of 2+ in at least 2+ cells  
    * table(filter_genes)  
  
Normalization:  
  
* Want to group cells based on their gene expression profiles; target with different drugs, for example  
* Technical artifacts can be introduced in the data  
	* Batch effect is a common problem with the data - cells may cluster by batch, even if they are from the same entity  
    * Can normalize based on dividing by library size (multiplied if needed to get count per million - CPM)  
* Exercise will use the following functions  
	* plotPCA()  
    * reducedDim(sce, “PCA”)[, 1:2]  
    * computeSumFactors()  
    * sizeFactors()  
    * assays()  
    * normalize()  
    * plotRLE()  
  
Example code includes:  
```{r eval=FALSE}

# remove genes with only zeros
nonZero <- counts(sce) > 0
keep <- rowSums(nonZero) > 0
sce_2 <- sce[keep, ]

# spike-ins ERCC
isSpike(sce_2, "ERCC") <- grepl("^ERCC-", rownames(sce_2))


# load scater
library(scater)

# calculate QCs
sce <- calculateQCMetrics(sce, feature_controls = list(ERCC = isSpike(sce, "ERCC")))

# explore coldata of sce
colData(sce)


# set threshold
threshold <- 20000

# plot density
plot(density(sce@colData$total_counts), main = 'Density - total_counts')
abline(v = threshold)

# keep cells
keep <- sce@colData$total_counts > threshold

# tabulate kept cells
table(keep)


# set threshold
threshold <- 6000

# plot density
plot(density(sce$total_features), main = 'Density - total_features')
abline(v=threshold)

# keep cells
keep <- sce$total_features > threshold

# tabulate kept cells
table(keep)


#extract cell data into a data frame
cDataFrame <- as.data.frame(colData(sce))

# plot cell data
ggplot(cDataFrame, aes(x = total_counts, y = total_counts_ERCC, col = batch)) + 
  geom_point()

# keep cells
keep <- sce$batch != "NA19098.r2"

# tabulate kept cells
table(keep)


# load SingleCellExperiment
library(SingleCellExperiment)

# filter genes
filter_genes <- apply(counts(sce), 1, function(x){
  length(x[x > 1]) > 1
})

# tabulate the results of filter_genes
table(filter_genes)


# PCA raw counts
plotPCA(sce, exprs_values = "counts",
    colour_by = "batch", shape_by = "individual")

# PCA log counts
plotPCA(sce, exprs_values = "logcounts_raw",
        colour_by = "batch", shape_by = "individual")


#find first 2 PCs
pca <- reducedDim(sce, "PCA")[, 1:2]

#create cdata
cdata <- data.frame(PC1 = pca[, 1],
                    libsize = sce$total_counts,
                    batch = sce$batch)

#plot pc1 versus libsize
ggplot(cdata, aes(x = PC1, y = libsize, col = batch)) +
  geom_point()


# load scran
library(scran)

# find size factors
sce <- computeSumFactors(sce)

# display size factor histogram
hist(sizeFactors(sce))


# view assays
assays(sce)

# normalize sce
normalized_sce <- normalize(sce)

# view new assay for normalized logcounts
assays(normalized_sce)

```
  
  
  
***
  
Chapter 3 - Visualization and Dimensionality Reduction  
  
Mouse Epithelium Dataset:  
  
* Goal is to reduce the number of dimensions (from number of genes to something much smaller)  
* Mouse olfactory cell dataset - epithelium stem cell differentiation  
	* Dimensionlaity reduction makes for smaller data with preservation of signal much more so than noise  
  
Visualization:  
  
* Can visualize datasets using dimensionality reduction through any of several methods - PCA, tSNE, ZIFA, ZINB-WaVE  
	* plotPCA(sce, exprs_values = "logcounts", shape_by = "Batch", colour_by = "publishedClusters")  # los help reduce bias towards highly expressed genes  
    * plotTSNE(sce, exprs_values = "logcounts", shape_by = "Batch", colour_by = "publishedClusters", perplexity = 5)  # perplexity is a guess about the kNN parameter  
  
Dimensionality Reduction:  
  
* Can find the most variable genes using magrittr  
	* library(magrittr)  
    * vars <- assay(sce) %>% log1p %>% rowVars  
    * names(vars) <- rownames(sce)  
    * vars <- sort(vars, decreasing = TRUE)  
    * head(vars)  
    * sce_sub <- sce[names(vars[1:50]),]  
    * sce_sub  
* Can run dimensionality reduction using PCA  
	* logcounts <- log1p(assay(sce_sub))  
    * pca <- prcomp(t(logcounts))  
    * reducedDims(sce_sub) <- SimpleList(PCA = pca$x)  
    * sce_sub  
    * head(reducedDim(sce_sub, "PCA")[, 1:2])  
* Can then plot the PCA components  
	* pca <- reducedDim(sce_sub, "PCA")[, 1:2]  
    * col <- colData(sce)[, c("publishedClusters", "batch")]  
    * df <- cbind(pca, col)  
    * ggplot(df, aes(x = PC1, y = PC2, col = publishedClusters, shape = batch)) +  
    *     geom_point()  
  
Example code includes:  
```{r eval=FALSE}

# find dimensions
mydims <- dim(sce)

# extract cell and gene names
cellNames <- colnames(sce)
geneNames <- rownames(sce)


# cell data
cData <- colData(sce)

#print column names
colnames(cData)

# table batch & clusters
cData <- cData[, c('Batch', 'publishedClusters')]

#tabulate cData
table(cData)


# load scater
library(scater)

# plot pc1 and pc2 counts
plotPCA(
    object = sce,
    exprs_values = "counts",
    shape_by = "Batch",
    colour_by = "publishedClusters"
)


# explore initial assays
assays(sce)

# create log counts
logcounts <- log1p(assays(sce)$counts)

# add log counts
assay(sce, 'logcounts') <- logcounts
assays(sce)

# pca log counts
plotPCA(object = sce, exprs_values = "logcounts",
    shape_by = "Batch", colour_by = "publishedClusters")


# default tSNE
plotTSNE(
    sce,
    exprs_values = "counts",
    shape_by = "publishedClusters",
    colour_by = "Batch",
    perplexity = 5
)


# gene variance 
vars <- assay(sce) %>% log1p() %>% rowVars() 

#rename vars
names(vars) <- rownames(sce)

#sort vars
vars_2 <- sort(vars, decreasing = TRUE)
head(vars_2)

# subset sce 
sce_sub <- sce[names(vars[1:50]), ]
sce_sub


# log counts
logcounts <- log1p(assays(sce_sub)$counts)

# transpose
tlogcounts <- t(logcounts)

# perform pca
pca <- prcomp(tlogcounts)

# store pca matrix in sce
reducedDims(sce_sub) <- SimpleList(PCA = pca$x)
head(reducedDim(sce_sub, "PCA")[, 1:2])


# Extract PC1 and PC2 and create a data frame
pca <- reducedDim(sce_sub, "PCA")[, 1:2]
col_shape <- data.frame(publishedClusters = colData(sce)$publishedClusters, Batch = factor(colData(sce)$Batch))
df <- cbind(pca, col_shape)

# plot PC1, PC2
ggplot(df, aes(x = PC1, y = PC2, 
            colour = publishedClusters, 
            shape = Batch)) + 
  geom_point()

```
  
  
  
***
  
Chapter 4 - Cell Clustering and Differential Expression Analysis  
  
Clustering methods for scRNA-Seq:  
  
* Continuing to use the mouse epithelium dataset - cells color coded by cluster as per previous chapters  
* One of the goals of clustering is to group cells with similar gene expression, allowing for finding patterns in gene expression  
	* Hierarchical clustering  
    * k-means clustering  
* Challenges include setting the number of clusters, scalability to large datasets, etc.  
* Can begin by creating the Seurat object  
	* library(Seurat)  
    * library(SingleCellExperiment)  
    * seuset <- CreateSeuratObject(  
    *     raw.data = assay(sce),  
    *     normalization.method = "LogNormalize",  
    *     scale.factor = 10000,  
    *     meta.data = as.data.frame(colData(sce))  
    * )  
    * seuset <- ScaleData(object = seuset)  
    * seuset  
* Can then perform clustering on the seuset object  
	* seuset <- FindClusters( object = seuset, reduction.type = "pca", dims.use = 1:10, resolution = 1.8, print.output = FALSE )  
    * PCAPlot( object = seuset, group.by = "ident", pt.shape = "publishedClusters" )  
  
Differential expression analysis:  
  
* Differential expression (DE) analysis is to find differential expression of genes in various cells  
	* Methods include SCDE, MAST, edgeR, DESeq2, etc.  
* Can fit a MAST model using function zlm  
	* library(MAST)  
    * zlm <- zlm(~ celltype + cngeneson, sce)   
    * summary <- summary(zlm, doLRT = "celltype9")  
    * summary  
    * fit <- summary$datatable  
    * fit <- merge(fit[contrast=='celltype9' & component=='H', .(primerid, `Pr(>Chisq)`)],      fit[contrast=='celltype9' & component=='logFC', .(primerid, coef)], by='primerid')  
    * fit[, padjusted:=p.adjust(`Pr(>Chisq)`, 'fdr')]  
    * res = data.frame(gene = fit$primerid, pvalue = fit[,'Pr(>Chisq)'], padjusted = fit$padj, logFC = fit$coef)  
    * head(res)  
  
Visualization of DE genes:  
  
* Visualization is typically the final step of single-cell analysis  
* The volcano plot looks at fold-change and p-values simultaneously  
	* ggplot(res, aes(x=logFC, y=-log10(padjusted), color=mostDE)) + geom_point() +  
    *     ggtitle("Volcano") + xlab("log2 FC") + ylab("-log10 adjusted p-value")  
* Can also look at results of DE using a heatmap  
	* library(NMF)  
    * norm <- assay(sce[mostDE, ], "logcounts")  
    * norm <- as.matrix(norm)  
    * aheatmap(norm, annCol = colData(sce)$publishedClusters)  
* Course covered the typical workflow for analysis of single-cell RNA sequencing data  
	* Normalization  
    * Dimensionality reduction  
    * Clustering  
    * Differential expression analysis  
  
Example code includes:  
```{r eval=FALSE}

# load Seurat
library(Seurat)

#create seurat object
seuset <- CreateSeuratObject(
    raw.data = assay(sce),
    normalization.method = "LogNormalize", 
    scale.factor = 10000,
    meta.data = as.data.frame(colData(sce))
)

# scale seuset object
scaled_seuset <- ScaleData(object = seuset)


# perform pca
seuset <- RunPCA(
    object = seuset, 
    pc.genes = rownames(seuset@raw.data), 
    do.print = FALSE
)
# plot pca
PCAPlot(object = seuset,
        pt.shape = 'Batch',
        group.by = 'publishedClusters')


# load MAST
library(MAST)

# SingleCellAssay object 
sca

# fit zero-inflated regression 
zlm <- zlm(~ celltype + cngeneson, sca) 

# summary with likelihood test ratio
summary <- summary(zlm, doLRT = "celltype9")


# get summary table
fit <- summary$datatable

# pvalue df
pvalue <- fit[contrast == 'celltype9' & component == 'H', .(primerid, `Pr(>Chisq)`)]
  
# logFC df
logFC <- fit[contrast == 'celltype9' & component == 'logFC', .(primerid, coef)]

# pvalues and logFC
fit <- merge(pvalue, logFC, by = 'primerid')


# adjusted pvalues
fit[, padjusted:=p.adjust(`Pr(>Chisq)`, 'fdr')]

# result table
res <- data.frame(gene = fit$primerid,
                 pvalue = fit[,'Pr(>Chisq)'],
                 padjusted = fit$padj,
                 logFC = fit$coef)


# most DE 
res <- res[order(res$padjusted), ]
mostDE <- res$gene[1:20]
res$mostDE <- res$gene %in% mostDE

# volcano plot
ggplot(res, aes(x=logFC, y=-log10(padjusted), color=mostDE)) +
  geom_point() +
  ggtitle("Volcano plot") +
  xlab("log2 fold change") + 
  ylab("-log10 adjusted p-value")


# load NMF
library(NMF)

# normalize log counts
norm <- assay(sce[mostDE, ], "logcounts")
mat <- as.matrix(norm)

# heatmap
aheatmap(mat, annCol = colData(sce)$publishedClusters)

```
  
  
  
***
  
### _Differential Expression Analysis in R with limma_  
  
Chapter 1 - Differential Expression Analysis  
  
Differential expression analysis:  
  
* Analysis of data from functional genomics experiments  
* Example of having treated cells with phenotypes A and B  
	* The features can be genes or proteins or other molecular features of the cell - proxy for relative abundance (such as RNA)  
    * Upregulated - higher expression level  
    * Downregulated - lower expression level  
* Objectives are to look for novelty (genes that play an unexpected role), context (interpreting relevance of gene behaviors), systems level understanding (simultaneous allows for looking at pathways)  
* Many caveats to the analysis - study design is very important  
  
Differential expression data:  
  
* Data will be from the breast cancer data and CLL data - testing for differences in two groups of people within each  
	* x - Expression matrix  
    * f - Feature data - genes or proteins  
    * p - Phenotype data - description of each of the samples  
* Can begin by looking at the boxplot for a single gene  
	* boxplot(x[1, ] ~ p[, "er"], main = f[1, "symbol"])  
  
ExpressionSet class:  
  
* Data management can become precarious, especially when filtering and subsetting  
* Object-oriented programming can help - the class can hold the data, and has methods/functions that work specially on objectes of that class  
	* Accessors (getters) get the data  
    * Setters modify the stored data  
    * source("https://bioconductor.org/biocLite.R")  
    * biocLite("Biobase")  
    * library(Biobase)  
    * eset <- ExpressionSet(assayData = x, phenoData = AnnotatedDataFrame(p), featureData = AnnotatedDataFrame(f))  
* Can access data from an ExpressionSet object  
	* x <- exprs(eset)  
    * f <- fData(eset)  
    * p <- pData(eset)  
    * eset_sub <- eset[1000, 1:10]  
    * boxplot(exprs(eset)[1, ] ~ pData(eset)[, "er"], main = fData(eset)[1, "symbol"])  
  
The limma package:  
  
* Advantages of the limma package include replacing boiler-plate code and improved inference by sharing across genes (do not assume full independene - all from same experiment)  
	* Method used is empirical Bayes - convenience and better statistics (especially for smaller data sets)  
    * Good functions for pre-processing and post-processing  
* Specifying a linear model - Y = B0 + B1 * X1 + epsilon where Y is the expression level of the gene, B0 is the mean in ER- tumors, and B1 is the mean difference in expression in ER+ tumors  
	* model.matrix(~<explanatory>, data = <data frame>)  
    * design <- model.matrix(~er, data = pData(eset))  
    * colSums(design)  
* Can then test the design matrix using the standard limma pipeline  
	* library(limma)  
    * fit <- lmFit(eset, design)  
    * fit <- eBayes(fit)  
    * results <- decideTests(fit[, "er"])  
    * summary(results)  # -1 will be downregulated and +1 will be upregulated  
  
Example code includes:  
```{r eval=FALSE}

# Create a boxplot of the first gene in the expression matrix
boxplot(x[1, ] ~ p[, "Disease"], main = f[1, "symbol"])


# Load package
library(Biobase)

# Create ExpressionSet object
eset <- ExpressionSet(assayData = x,
                      phenoData = AnnotatedDataFrame(p),
                      featureData = AnnotatedDataFrame(f))

# View the number of features (rows) and samples (columns)
dim(eset)


# Subset to only include the 1000th gene (row) and the first 10 samples
eset_sub <- eset[1000, 1:10]

# Check the dimensions of the subset 
dim(eset_sub)

# Create a boxplot of the first gene in eset_sub
boxplot(exprs(eset_sub)[1, ] ~ pData(eset_sub)[, "Disease"],
        main = fData(eset_sub)[1, "symbol"])


# Create design matrix for leukemia study
design <- model.matrix(~Disease, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Calculate the t-statistics
fit <- eBayes(fit)

# Summarize results
results <- decideTests(fit[, "Diseasestable"])
summary(results)

```
  
  
  
***
  
Chapter 2 - Flexible Models for Common Study Designs  
  
Flexible linear models:  
  
* The models can be extended, for example to Y = B0 + B1*X1 + B2*X2 + eps  
	* This model is known as a treatment-contrast model  
* Can instead use a group-means parameterization - Y = B1*X1 + B2*X2 + eps  
	* Can then test whether B1-B2 == 0 since the intercept was exluded  
    * design <- model.matrix(~0 + er, data = pData(eset))  
    * cm <- limma::makeContrasts(status = erpositive - ernegative, levels = design)  # erpositive-ernegative means multiply erpositive by 1 and multiply ernegative by -1  
    * fit <- lmFit(eset, design)  # per above  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  # for the contrasts method  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
    * summary(results)  
  
Studies with more than two groups:  
  
* Dataset for this example has groups with leukemia types ALL, AML, CML - 20172x36 (12 leukemias of each type)  
* Desire to build a group-means model - Y = B1*X1 + B2*X2 + B3*X3 + eps  
	* design <- model.matrix(~0 + type, data = pData(eset))  
    * cm <- limma::makeContrasts(AMLvALL = typeAML - typeALL, CMLvALL = typeCML - typeALL, CMLvAML = typeCML - typeAML, levels = design)  
    * fit <- lmFit(eset, design)  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
* Exercises will look at gene expressions of stem cells grown in states of hypoxia  
  
Factorial experimental design:  
  
* Factorial designs look at every combination of experimental variables - for example, if there is a 2x2, then there would be 4 combinations examined  
* Example of 2x2 study in plants of types col, vte2 for temperatures of high, low - 11871 x 12  
* Can run the group-means model with the zero intercept; need to first create the type-temperature variable using paste  
	* group <- with(pData(eset), paste(type, temp, sep = "."))  
    * group <- factor(group)  # records the unique levels  
    * design <- model.matrix(~0 + group)  
    * colnames(design) <- levels(group)  
* May want to assess the interaction effect (difference in impact of temperature by type) as well as the direct effects (impact of temperature on a specific type, impact for same temperature across types)  
	* cm <- makeContrasts(type_normal = vte2.normal - col.normal, type_low = vte2.low - col.low, temp_vte2 = vte2.low - vte2.normal, temp_col = col.low - col.normal, interaction = (vte2.low - vte2.normal) - (col.low - col.normal), levels = design)  
    * fit <- lmFit(eset, design)  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
  
Example code includes:  
```{r eval=FALSE}

# Create design matrix with no intercept
design <- model.matrix(~0 + Disease, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(status = Diseaseprogres. - Diseasestable, levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)


# Create design matrix with no intercept
design <- model.matrix(~0 + oxygen, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(ox05vox01 = oxygenox05 - oxygenox01,
                    ox21vox01 = oxygenox21 - oxygenox01,
                    ox21vox05 = oxygenox21 - oxygenox05,
                    levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)


# Create single variable
group <- with(pData(eset), paste(type, water, sep = "."))
group <- factor(group)

# Create design matrix with no intercept
design <- model.matrix(~0 + group)
colnames(design) <- levels(group)

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(type_normal = nm6.normal - dn34.normal,
                    type_drought = nm6.drought - dn34.drought,
                    water_nm6 = nm6.drought - nm6.normal,
                    water_dn34 = dn34.drought - dn34.normal,
                    interaction = (nm6.drought - nm6.normal) - (dn34.drought - dn34.normal),
                    levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)

```
  
  
  
***
  
Chapter 3 - Pre-processing and post-processing  
  
Normalizing and filtering:  
  
* Need to convert raw data to analysis-ready data - generic approach for first-pass to new dataset  
	* Log transformation  
    * Quantile normalization  
    * Filtering  
* Can start with visualization of densities using the limma package  
	* limma::plotDensities(eset, legend = FALSE)  
    * exprs(eset) <- log(exprs(eset))  # log transform  
    * limma::plotDensities(eset, legend = FALSE)  
* Want to remove technical artifacts using quantile normalization  
	* exprs(eset) <- normalizeBetweenArrays(exprs(eset))  
    * limma::plotDensities(eset, legend = FALSE)  
    * abline(v = 5)  # from visualization, 5 may be a cutoff where the data should be kept  
    * keep <- rowMeans(exprs(eset)) > 5  
    * eset <- eset[keep, ]  
    * plotDensities(eset, legend = FALSE)  
  
Accounting for technical batch effects:  
  
* Technical batch effects are artifacts arising from differences in experiments - true for all experiment types, including functional genomics  
	* Need to balance variables of interest across batches - cannot just take type a from batch a and type b from type b and see if there are differences  
    * PCA and other dimension reduction techniques can help identify technical batch effects  
    * limma::plotMDS(eset, labels = pData(eset)[, "time"], gene.selection = "common")  
* Removing batch effects is also possible in limma  
	* exprs(eset) <- limma::removeBatchEffect(eset, batch = pData(eset)[, "batch"], covariates = pData(eset)[, "rin"])  
    * limma::plotMDS(eset, labels = pData(eset)[, "time"], gene.selection = "common")  
* For statistical analysis, it is better to include batch as a coefficient for analysis rather than to run the remove batch effect process  
  
Visualizing results:  
  
* Can inspect the results and visualize using limma  
	* results <- decideTests(fit2)  
    * topTable(fit2, number = 3)  
    * stats <- topTable(fit2, number = nrow(fit2), sort.by = "none")  
* Under the null hypothesis of no impact, the p-values should be uniformly distributed  
	* hist(runif(10000))  
    * hist(stats[, "P.Value"])  # should have many values near zero if there is an actual impact  
* Can examine results using a Volcano plot  
	* volcanoplot(fit2, highlight = 5, names = fit2$genes[, "symbol"])  
  
Enrichment testing:  
  
* Can use curated biological databases as a reference point  
* Can use the Fisher's exact test  
	* fisher.test(matrix(c(10, 100, 90, 900), nrow = 2))  
* Can also test for KEGG (reference set) enrichment, which requires a common ID  
	* entrez <- fit2$genes[, "entrez"]  
    * enrich_kegg <- kegga(fit2, geneid = entrez, species = "Hs")  # Hs is homo sapiens  
    * topKEGG(enrich_kegg, number = 3)  
* Can also test for GO (reference set) enrichment  
	* enrich_go <- goana(fit2, geneid = entrez, species = "Hs")  
    * topGO(enrich_go, ontology = "BP", number = 3)  
  
Example code includes:  
```{r eval=FALSE}

# Load package
library(limma)

# View the distribution of the raw data
plotDensities(eset, legend = FALSE)

# Log tranform
exprs(eset) <- log(exprs(eset))
plotDensities(eset, legend = FALSE)

# Quantile normalize
exprs(eset) <- normalizeBetweenArrays(exprs(eset))
plotDensities(eset, legend = FALSE)


# Load package
library(limma)

# View the normalized gene expression levels
plotDensities(eset, legend = FALSE); abline(v = 5)

# Determine the genes with mean expression level greater than 5
keep <- rowMeans(exprs(eset)) > 5
sum(keep)

# Filter the genes
eset <- eset[keep, ]
plotDensities(eset, legend = FALSE)


# Load package
library(limma)

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")

# Plot principal components labeled by batch
plotMDS(eset, labels = pData(eset)[, "batch"], gene.selection = "common")


# Load package
library(limma)

# Remove the batch effect
exprs(eset) <- removeBatchEffect(eset, batch = pData(eset)[, "batch"])

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")

# Plot principal components labeled by batch
plotMDS(eset, labels = pData(eset)[, "batch"], gene.selection = "common")


# Obtain the summary statistics for every gene
stats <- topTable(fit2, number = nrow(fit2), sort.by = "none")

# Plot a histogram of the p-values
hist(stats[, "P.Value"])


# Create a volcano plot. Highlight the top 5 genes
volcanoplot(fit2, highlight = 5, names = fit2$genes$symbol)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched KEGG Pathways
enrich_kegg <- kegga(fit2, geneid = entrez, species = "Hs")

# View the top 20 enriched KEGG pathways
topKEGG(enrich_kegg, number=20)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched GO categories
enrich_go <- goana(fit2, geneid = entrez, species = "Hs")

# View the top 20 enriched GO Biological Processes
topGO(enrich_go, ontology = "BP", number=20)

```
  
  
  
***
  
Chapter 4 - Case Study: Effect of Doxorubicin Treatment  
  
Pre-process data:  
  
* Doxorubicin is a commonly-prescribed cancer drug, with a strong side effect of cariotoxicity  
* Hypothesis for the MOA is that top2B is involved, and was tested in mice  
	* Wild mice tested against top2b null mice, each given DOX and a placebo  
    * The eset data is 29532 x 12 (3 replicates per combination of 2x2 factors)  
* Begin by inspecting and then pre-processing the data  
	* plotDensities(eset, group = pData(eset)[, "genotype"], legend = "topright")  
* Prep-processing steps include log-transform, quantile transform, and filter  
  
Model the data:  
  
* Can look at the main clusters of mice vs. treatments (top2b null seem to cluster together)  
	* plotMDS(eset, labels = pData(eset)[, "genotype"], gene.selection = "common")  
    * plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")  
* Follow the model.matrix process to run a differential expression analysis  
	* Contrasts will include wild to dox, top2b to dox, interaction effect of wild vs. top2b to dox  
    * Can also run hypothesis tests using the limma pipeline and a Venn diagram  
  
Inspect the results:  
  
* Initial results seem to support the hypothesis that top2b is the main vector for cardiotoxicity  
* The limma function topTable can be run, with a sepcified contrast  
	* coef = "dox_wt"  
    * coef = "dox_top2b"  
    * coef = "interaction"  
* Can use the volcanoplot for x-axis of log-fold change and y-axis for the log-odds of differential expression  
	* kegga and topKEGG functions  
    * species = "Mm"  # common house mouse  
  
Wrap up:  
  
* Pre-processing and visualization of gene data  
* Principal components analysis and plotMDS()  
* Fitting a group-means model for more interpretable contrasts  
* Investigation of p-values for uniformity vs. skew for low p-values  
* Use of volcanoplots  
* Testing for enrichment of differentially expressed genes  
  
Example code includes:  
```{r eval=FALSE}

# Log transform
exprs(eset) <- log(exprs(eset))
plotDensities(eset,  group = pData(eset)[, "genotype"], legend = "topright")

# Quantile normalize
exprs(eset) <- normalizeBetweenArrays(exprs(eset))
plotDensities(eset,  group = pData(eset)[, "genotype"], legend = "topright")

# Determine the genes with mean expression level greater than 0
keep <- rowMeans(exprs(eset)) > 0
sum(keep)

# Filter the genes
eset <- eset[keep, ]
plotDensities(eset, group = pData(eset)[, "genotype"], legend = "topright")


# Find the row which contains Top2b expression data
top2b <- which(fData(eset)["symbol"] == "Top2b")

# Plot Top2b expression versus genotype
boxplot(exprs(eset)[top2b, ] ~ pData(eset)[, "genotype"], main = fData(eset)[top2b, ])


# Plot principal components labeled by genotype
plotMDS(eset, labels = pData(eset)[, "genotype"], gene.selection = "common")

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")


# Create single variable
group <- with(pData(eset), paste(genotype, treatment, sep = "."))
group <- factor(group)

# Create design matrix with no intercept
design <- model.matrix(~0 + group)
colnames(design) <- levels(group)

# Count the number of samples modeled by each coefficient
colSums(design)


# Create a contrasts matrix
cm <- makeContrasts(dox_wt = wt.dox - wt.pbs,
                    dox_top2b = top2b.dox - top2b.pbs,
                    interaction = (top2b.dox - top2b.pbs) - (wt.dox - wt.pbs),
                    levels = design)

# View the contrasts matrix
cm


# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)

# Create a Venn diagram
vennDiagram(results)


# Obtain the summary statistics for the contrast dox_wt
stats_dox_wt <- topTable(fit2, coef = "dox_wt", number = nrow(fit2), sort.by = "none")
# Obtain the summary statistics for the contrast dox_top2b
stats_dox_top2b <- topTable(fit2, coef = "dox_top2b", number = nrow(fit2), sort.by = "none")
# Obtain the summary statistics for the contrast interaction
stats_interaction <- topTable(fit2, coef = "interaction", number = nrow(fit2), sort.by = "none")

# Create histograms of the p-values for each contrast
hist(stats_dox_wt[, "P.Value"])
hist(stats_dox_top2b[, "P.Value"])
hist(stats_interaction[, "P.Value"])


# Extract the gene symbols
gene_symbols <- fit2$genes[, "symbol"]

# Create a volcano plot for the contrast dox_wt
volcanoplot(fit2, coef = "dox_wt", highlight = 5, names = gene_symbols)

# Create a volcano plot for the contrast dox_top2b
volcanoplot(fit2, coef = "dox_top2b", highlight = 5, names = gene_symbols)

# Create a volcano plot for the contrast interaction
volcanoplot(fit2, coef = "interaction", highlight = 5, names = gene_symbols)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched KEGG Pathways for contrast dox_wt
enrich_dox_wt <- kegga(fit2, coef = "dox_wt", geneid = entrez, species = "Mm")

# View the top 5 enriched KEGG pathways
topKEGG(enrich_dox_wt, number = 5)

# Test for enriched KEGG Pathways for contrast interaction
enrich_interaction <- kegga(fit2, coef = "interaction", geneid = entrez, species = "Mm")

# View the top 5 enriched KEGG pathways
topKEGG(enrich_interaction, number = 5)

```
  
  
  
***
  
### _Interactive Data Visualization with bokeh_  
  
Chapter 1 - rbokeh Introduction  
  
Getting started with rbokeh:  
  
* rbokeh is the R interface to the Python Bokeh plot package - interactive and informative for end users  
* Data manipulation and pre-processing are needed, with tidyverse being integral for this course  
* Can run an example using the gapminder dataset  
	* library(gapminder)  
    * data_2002 <- gapminder %>% filter(year == 2002)  
    * gapminder_mod <- gapminder %>% mutate(pop_millions = pop/10^6)  
  
Layers for rbokeh:  
  
* The rbokeh plot is initialized using figure() with layers added using the pipe operator (note the contrast with the plus used in ggplot2)  
	* data_rwanda <- gapminder %>% filter(country == "Rwanda")  
    * figure() %>% ly_lines(x = year, y = lifeExp, data = data_rwanda)  
    * figure() %>% ly_lines(x = data_rwanda$year, y = data_rwanda$lifeExp)  # same output, but with different axis labels  
* Can look at the ggplot2::economics dataset  
	* plot_pop <- figure() %>% ly_lines(x = date, y = pop, data = economics)  
    * plot_pop  
  
Layers for rbokeh (continued):  
  
* There are many layers in rbokeh, and they all begin with ly_...()  
* Example for creating a one-layer plot  
	* dat_1982 <- gapminder %>% filter(year == 1982)  
    * figure() %>% ly_points(x = gdpPercap, y = lifeExp, data = dat_1982)  
* Example for creating a multi-layer plot - note that the data argument can be added to figure, and will inherit to the succeeding ly_() ommands  
	* data_oceania <- gapminder %>% filter(continent == "Oceania")  
    * figure(data = data_oceania, legend_location = "bottom_right") %>% ly_lines(x = year, y = gdpPercap , color = country) %>% ly_points(x = year, y = gdpPercap, color = country)  
  
Example code includes:  
```{r eval=FALSE}

## load rbokeh, gapminder and dplyr libraries
library(rbokeh)
library(gapminder)
library(dplyr)


## explore gapminder dataset 
str(gapminder)

## filter gapminder data by year 1982
dat_1982 <- gapminder %>% filter(year == 1982)


## plot life expectancy Vs GDP per Capita using data_1982
figure(legend_location = "bottom_right", title = "Life Expectancy Vs. GDP per Capita in 1982") %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_1982, 
              color = continent, hover = c(continent, country, pop)
              )


## filter the dataset for the continent Africa and and year 1967
data_africa <- gapminder %>% 
  filter(year==1967, continent=="Africa")
  
## view data_africa
data_africa


## plot life expectancy Vs GDP per Capita using data_africa   
figure(legend_location = "bottom_right",
       title = "Life Expectancy Vs. GDP per Capita in Africa - 1967"
       ) %>% 
       ly_points(x = gdpPercap, y = lifeExp, data = data_africa, hover = c(country, pop))


## add a new column with gdp in millions
gapminder_mill <- gapminder %>% 
  mutate(gdp_millions = gdpPercap * pop / 10^6)
  
## view the first 6 entries in gapminder after adding  gdp_millions
head(gapminder_mill)

## extract the entries for "Rwanda"
data_rwanda <- gapminder_mill %>% 
  filter(country=="Rwanda")

## explore data_rwanda
data_rwanda


## plot gdp over time
figure(data = data_rwanda) %>% 
    ly_lines(x = year, y = gdp_millions, width = 2)


## explore the economics dataset
data(economics)
str(economics)

## pass vectors to x & y
figure() %>%
  ly_lines(x = economics$date, y = economics$pce)

## pass columns names and dataframe
figure() %>%
  ly_lines(x = date, y = pce, data = economics)


## plot unemployment rate  versus time and change the default `ylab`
figure(ylab = "unemployment %") %>%
  ly_lines(x=date, y=100*unemploy/pop, data=economics)


dat_1992 <- gapminder %>%
    filter(year==1992)
str(dat_1992)

## plot lifeExp Vs. gdpPercap using rbokeh
plot_1992<- figure(legend_location = "bottom_right") %>%
  ly_points(x=gdpPercap, y=lifeExp, color=continent, data=dat_1992) 

## show the plot            
plot_1992


data_countries <- gapminder %>%
    filter(country %in% c("United Kingdom", "Australia", "Canada", "United States", "New Zealand"))
str(data_countries)

figure(data = data_countries, legend="top_left") %>% 
  ly_lines(x = year, y = gdpPercap , color = country) %>% 
  ly_points(x=year, y=gdpPercap, color=country)


data_countries <- gapminder %>% 
    filter(country %in% c("China", "India"))

## create a line plot with lifeExp vs. year 
fig_countries <- figure(legend="top_left") %>% 
  ly_lines(x=year, y=lifeExp, color=country, data=data_countries)


## View fig_countries
fig_countries

## modify fig_countries by adding a points layer with gdpPercap vs. year 
fig_countries %>% 
  ly_points(x=year, y=lifeExp, color=country, data=data_countries)

```
  
  
  
***
  
Chapter 2 - rbokeh Aesthetic Attributes and Figure Options  
  
Plot and Managed Attributes (Part I):  
  
* Can use aestehtic to modify areas like color, transparency, line type, shape, and the like  
	* figure(legend_location = "bottom_right", title = "Life Expectancy Vs. GDP per Capita in 1992" ) %>% ly_points(x = gdpPercap, y = lifeExp, data = dat_1992, color = continent)  
* Can use the Human Development Index (HDI) data from UNDP  
	* hdi_countries <- hdi_data %>% filter(country %in% c("Hungary", "Bulgaria", "Poland"))  
    * fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% ly_lines(x = year, y = human_development_index, color = country) %>% ly_points(x = year, y = human_development_index, color = country)  
* Can have varying color attributes by function - ly_points() has both fill_color() and line_color() which will both inherit from color  
	* Can make the fill_color explicit and set its alpha (default is 0.5) explicitly also  
    * fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% ly_points(x = year, y = human_development_index, fill_color = country, fill_alpha = 1) %>% ly_lines(x = year, y = human_development_index, color = country)  
* Can add a custom color palette also  
	* fig_col %>% set_palette(discrete_color = pal_color(c("#3182bd", "#31a354", "#de2d26")))  
  
Plot and Managed Attributes (Part II):  
  
* Bechdel dataset - movie data on finances and exclusion of women  
	* Bechdel criteria is a movie where two+ women have a discussion that is not about a male character  
    * figure() %>% ly_points(x = budget_2013, y = intgross_2013, data = dat_90_13)  # has an over-plotting problem and needs a log-transform  
    * figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13)  # log transform helps with a lot (but not all) of the over-plotting  
    * figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13, alpha = 0.4, size = 5)  # improves readability  
* May want to change the line widths for many countries  
	* hdi_countries <- hdi_data %>% filter(country %in% c("Rwanda", "Kenya", "Botswana"))  
    * figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country)  
    * Can use the width parameter to control the line width in ly_lines()  
    * (WRONG FROM VIDEO) figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country, size = 3)  
    * figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country, width = 3)  
  
Hover Info and Figure Options:  
  
* Can combine the HDI and the CPI (corruption perception index)  
	* The hover() argument added to the ly_points() can allow for hovering - see below  
    * figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = c(country, cpi_rank))  
* Can customize the hover commands using hover= where the @ means to place a variable from the frame at that point  
	* figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = "CPI Rank: @cpi_rank")  
    * Can also use basic html such as <b></b> and <br></br>  
    * figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = "<b>@country</b><br><b>CPI Rank</b>: @cpi_rank")  
* Can further add axis limits to the bokeh plots  
	* hdi_cpi_scatter <- figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015", ylim = c(0, 1), xlab = "CPI", ylab = "HDI", theme = bk_ggplot_theme()) %>% ly_points(x = corruption_perception_index_score, y = human_development_index, data = hdi_cpi_data, color = continent, size = 7)  
  
Example code includes:  
```{r eval=FALSE}

hdiRaw <- read.csv("./RInputFiles/Human Development Index (HDI).csv", skip=1)
str(hdiRaw)
hdi_data <- hdiRaw %>% 
    gather(key="year", value="human_development_index", -Country, -`HDI.Rank..2017.`) %>%
    mutate(country=str_trim(as.character(Country)), year=as.integer(str_sub(year, 2))) %>%
    filter(year %in% 1990:2105) %>%
    select(country, year, human_development_index)
str(hdi_data)

## extract "Namibia" and "Botswana" entries from hdi_data
hdi_countries <- hdi_data %>% 
    filter(country %in% c("Namibia", "Botswana"))
  
## plot human_development_index versus year
fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% 
    ly_lines(x = year, y = human_development_index, color = country) %>% 
    ly_points(x = year, y = human_development_index, 
              fill_color = "white", fill_alpha = 1,
              line_color = country, line_alpha = 1,
              size = 4
              )

## view plot 
fig_col


## use a custom palette with colors "green", "red"
fig_col %>% 
  set_palette(discrete_color = pal_color(c("green", "red")))

## define custom palette   
custom_pal <- pal_color(c("#c51b8a", "#31a354"))

## use custom_pal yp modify fig_col
fig_col %>% 
    set_palette(discrete_color=custom_pal)


## explore bechdel dataset using str
data(bechdel, package="fivethirtyeight")
str(bechdel)

## extract entries between 1980 - 2013
dat_80_13 <- bechdel %>% 
  filter(between(year, 1980, 2013))

dat_80_13 <- dat_80_13 %>% 
  mutate(roi_total = intgross_2013 / budget_2013) 
  
## plot
figure() %>% 
  ly_points(x=log(budget_2013), y=log(roi_total), data=dat_80_13)

## plot log(roi_total) versus log(budget_2013)
figure() %>% 
  ly_points(x=log(budget_2013), y=log(roi_total), size=5, line_alpha=0, fill_alpha=0.3, data=dat_80_13)


## filter the data by country = Syrian Arab Republic
hdi_countries <- hdi_data %>% 
  filter(country %in% c("Syrian Arab Republic", "Morocco"))

## change the color and line width
figure(title = "Human Development Index over Time", legend = "bottom_right") %>% 
    ly_lines(x=year, y=human_development_index, color=country, width=3, data=hdi_countries)


# explore hdi_cpi_data dataset
# str(hdi_cpi_2015)

## add multiple values as hover info (country, cpi_rank)
# figure(legend_location = "bottom_right") %>% 
#     ly_points(x=corruption_perception_index, y=human_development_index, color=continent, hover=c(country, cpi_rank), size=6, data=hdi_cpi_2015)


## modify the figure theme 
# figure(title = "Corruption Perception Index Vs. Human Development Index 2015",
#        legend_location = "bottom_right", xgrid = FALSE, ygrid = FALSE, 
#        xlab = "CPI", ylab = "HDI", theme=bk_ggplot_theme()) %>% 
#     ly_points(x = corruption_perception_index, y = human_development_index, 
#               data = hdi_cpi_2015, color = continent, size = 6, hover = c(country, cpi_rank)
#               )

```
  
  
  
***
  
Chapter 3 - Data Manipulation for Visualization and More rbokeh Layers  
  
Data Formats:  
  
* The proper data format for plotting can make rbokeh much easier  
* Frequently need to transform data from long format to wide format for easier plotting - tidyr will help (inverse of the gather function)  
	* hdi_cpi_wide <- hdi_cpi_long %>% spread(key = index, value = value)  
* May also want to transform data from wide format to long format, for example if time is a column  
	* hdi_data_long <- hdi_data_wide %>% gather(key = year, value = human_development_index, - country)  # year will become a new column, -country means leave country as its own column  
  
More rbokeh Layers:  
  
* Can create a scatter plot with a regression line as an added layer  
	* dat_90_13 <- bechdel %>% filter(between(year, 1990, 2013))  
    * p_scatter <- figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13, size = 5, alpha = 0.4)  
    * lin_reg <- lm(log(intgross_2013) ~ log(budget_2013), data = dat_90_13)  
    * summary(lin_reg)  
    * p_scatter %>% ly_abline(lin_reg)  # plots with the abline of the regression (the regression line) to the figure  
  
Interaction Tools:  
  
* Can use the interaction tool to pan, zoom, reset, and the like  
	* figure(tools=c("pan", "wheel_zoom", "box_zoom", "reset", "save", "help"), toolbar_location="right")  
* Tools can be any of "pan", "wheel_zoom", "box_zoom", "resize", "crosshair", "box_select", "lasso_select", "reset", "save", "help"  
	* Location can be any of 'above', 'below', 'left', 'right', NULL (remove the toolbar)  
* Example of customizing the available tools  
	* figure(tools = c("pan", "wheel_zoom", "box_zoom"), toolbar_location = "above", legend_location = "bottom_right", ylim = c(0, 100)) %>% ly_points(x = gdpPercap, y = lifeExp, data = gapminder_2002, color = continent, size = 6, alpha = 0.7)  
* Can create a plot and then use the widget2png tool to convert to PNG  
	* plot_scatter <- figure(title = "Life Expectancy Vs. GDP per Capita in 2002", legend_location = "bottom_right") %>% ly_points(x = gdpPercap, y = lifeExp, data = gapminder_2002)  
    * widget2png(p = plot_scatter, file = "plot_scatter.png")  
* Can also save as html  
	* rbokeh2html(fig = plot_scatter, file = "plot_scatter_interactive.html")  
    * browseURL("plot_scatter_interactive.html")  
  
Example code includes:  
```{r eval=FALSE}

ctry <- c('Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia', 'Austria', 'Bahrain', 'Bangladesh', 'Belgium', 'Benin', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Eritrea', 'Ethiopia', 'Finland', 'France', 'Gabon', 'Gambia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kenya', 'Kuwait', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Malaysia', 'Mali', 'Mauritania', 'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Rwanda', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Sierra Leone', 'Singapore', 'Slovenia', 'South Africa', 'Spain', 'Sri Lanka', 'Sudan', 'Sweden', 'Switzerland', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'United Kingdom', 'United States', 'Uruguay', 'Zambia', 'Zimbabwe', 'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia', 'Austria', 'Bahrain', 'Bangladesh', 'Belgium', 'Benin', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Eritrea', 'Ethiopia', 'Finland', 'France', 'Gabon', 'Gambia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kenya', 'Kuwait', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Malaysia', 'Mali', 'Mauritania', 'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Rwanda', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Sierra Leone', 'Singapore', 'Slovenia', 'South Africa', 'Spain', 'Sri Lanka', 'Sudan', 'Sweden', 'Switzerland', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'United Kingdom', 'United States', 'Uruguay', 'Zambia', 'Zimbabwe')
ctryCode <- c('AFG', 'ALB', 'DZA', 'AGO', 'ARG', 'AUS', 'AUT', 'BHR', 'BGD', 'BEL', 'BEN', 'BIH', 'BWA', 'BRA', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR', 'CAN', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 'CRI', 'HRV', 'CUB', 'CZE', 'DNK', 'DJI', 'DOM', 'ECU', 'EGY', 'SLV', 'ERI', 'ETH', 'FIN', 'FRA', 'GAB', 'GMB', 'DEU', 'GHA', 'GRC', 'GTM', 'GIN', 'GNB', 'HTI', 'HND', 'HUN', 'ISL', 'IND', 'IDN', 'IRQ', 'IRL', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KEN', 'KWT', 'LBN', 'LSO', 'LBR', 'LBY', 'MDG', 'MWI', 'MYS', 'MLI', 'MRT', 'MUS', 'MEX', 'MNG', 'MON', 'MAR', 'MOZ', 'MMR', 'NAM', 'NPL', 'NLD', 'NZL', 'NIC', 'NER', 'NGA', 'NOR', 'OMN', 'PAK', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'ROM', 'RWA', 'STP', 'SAU', 'SEN', 'SCG', 'SLE', 'SGP', 'SVN', 'ZAF', 'ESP', 'LKA', 'SDN', 'SWE', 'CHE', 'THA', 'TGO', 'TTO', 'TUN', 'TUR', 'UGA', 'GBR', 'USA', 'URY', 'ZMB', 'ZWE', 'AFG', 'ALB', 'DZA', 'AGO', 'ARG', 'AUS', 'AUT', 'BHR', 'BGD', 'BEL', 'BEN', 'BIH', 'BWA', 'BRA', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR', 'CAN', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 'CRI', 'HRV', 'CUB', 'CZE', 'DNK', 'DJI', 'DOM', 'ECU', 'EGY', 'SLV', 'ERI', 'ETH', 'FIN', 'FRA', 'GAB', 'GMB', 'DEU', 'GHA', 'GRC', 'GTM', 'GIN', 'GNB', 'HTI', 'HND', 'HUN', 'ISL', 'IND', 'IDN', 'IRQ', 'IRL', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KEN', 'KWT', 'LBN', 'LSO', 'LBR', 'LBY', 'MDG', 'MWI', 'MYS', 'MLI', 'MRT', 'MUS', 'MEX', 'MNG', 'MON', 'MAR', 'MOZ', 'MMR', 'NAM', 'NPL', 'NLD', 'NZL', 'NIC', 'NER', 'NGA', 'NOR', 'OMN', 'PAK', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'ROM', 'RWA', 'STP', 'SAU', 'SEN', 'SCG', 'SLE', 'SGP', 'SVN', 'ZAF', 'ESP', 'LKA', 'SDN', 'SWE', 'CHE', 'THA', 'TGO', 'TTO', 'TUN', 'TUR', 'UGA', 'GBR', 'USA', 'URY', 'ZMB', 'ZWE')
regn <- c('AP', 'ECA', 'MENA', 'SSA', 'AME', 'AP', 'WE/EU', 'MENA', 'AP', 'WE/EU', 'SSA', 'ECA', 'SSA', 'AME', 'WE/EU', 'SSA', 'SSA', 'AP', 'SSA', 'AME', 'SSA', 'SSA', 'AME', 'AP', 'AME', 'SSA', 'AME', 'WE/EU', 'AME', 'WE/EU', 'WE/EU', 'SSA', 'AME', 'AME', 'MENA', 'AME', 'SSA', 'SSA', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'WE/EU', 'SSA', 'WE/EU', 'AME', 'SSA', 'SSA', 'AME', 'AME', 'WE/EU', 'WE/EU', 'AP', 'AP', 'MENA', 'WE/EU', 'MENA', 'WE/EU', 'AME', 'AP', 'MENA', 'SSA', 'MENA', 'MENA', 'SSA', 'SSA', 'MENA', 'SSA', 'SSA', 'AP', 'SSA', 'SSA', 'SSA', 'AME', 'AP', 'ECA', 'MENA', 'SSA', 'AP', 'SSA', 'AP', 'WE/EU', 'AP', 'AME', 'SSA', 'SSA', 'WE/EU', 'MENA', 'AP', 'AME', 'AME', 'AME', 'AP', 'WE/EU', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'MENA', 'SSA', 'ECA', 'SSA', 'AP', 'WE/EU', 'SSA', 'WE/EU', 'AP', 'MENA', 'WE/EU', 'WE/EU', 'AP', 'SSA', 'AME', 'MENA', 'ECA', 'SSA', 'WE/EU', 'AME', 'AME', 'SSA', 'SSA', 'AP', 'ECA', 'MENA', 'SSA', 'AME', 'AP', 'WE/EU', 'MENA', 'AP', 'WE/EU', 'SSA', 'ECA', 'SSA', 'AME', 'WE/EU', 'SSA', 'SSA', 'AP', 'SSA', 'AME', 'SSA', 'SSA', 'AME', 'AP', 'AME', 'SSA', 'AME', 'WE/EU', 'AME', 'WE/EU', 'WE/EU', 'SSA', 'AME', 'AME', 'MENA', 'AME', 'SSA', 'SSA', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'WE/EU', 'SSA', 'WE/EU', 'AME', 'SSA', 'SSA', 'AME', 'AME', 'WE/EU', 'WE/EU', 'AP', 'AP', 'MENA', 'WE/EU', 'MENA', 'WE/EU', 'AME', 'AP', 'MENA', 'SSA', 'MENA', 'MENA', 'SSA', 'SSA', 'MENA', 'SSA', 'SSA', 'AP', 'SSA', 'SSA', 'SSA', 'AME', 'AP', 'ECA', 'MENA', 'SSA', 'AP', 'SSA', 'AP', 'WE/EU', 'AP', 'AME', 'SSA', 'SSA', 'WE/EU', 'MENA', 'AP', 'AME', 'AME', 'AME', 'AP', 'WE/EU', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'MENA', 'SSA', 'ECA', 'SSA', 'AP', 'WE/EU', 'SSA', 'WE/EU', 'AP', 'MENA', 'WE/EU', 'WE/EU', 'AP', 'SSA', 'AME', 'MENA', 'ECA', 'SSA', 'WE/EU', 'AME', 'AME', 'SSA', 'SSA')
cnt <- c('Asia', 'Europe', 'Africa', 'Africa', 'Americas', 'Oceania', 'Europe', 'Asia', 'Asia', 'Europe', 'Africa', 'Europe', 'Africa', 'Americas', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Americas', 'Africa', 'Africa', 'Americas', 'Asia', 'Americas', 'Africa', 'Americas', 'Europe', 'Americas', 'Europe', 'Europe', 'Africa', 'Americas', 'Americas', 'Africa', 'Americas', 'Africa', 'Africa', 'Europe', 'Europe', 'Africa', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Africa', 'Africa', 'Americas', 'Americas', 'Europe', 'Europe', 'Asia', 'Asia', 'Asia', 'Europe', 'Asia', 'Europe', 'Americas', 'Asia', 'Asia', 'Africa', 'Asia', 'Asia', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Asia', 'Africa', 'Africa', 'Africa', 'Americas', 'Asia', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Asia', 'Europe', 'Oceania', 'Americas', 'Africa', 'Africa', 'Europe', 'Asia', 'Asia', 'Americas', 'Americas', 'Americas', 'Asia', 'Europe', 'Europe', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Europe', 'Africa', 'Asia', 'Europe', 'Africa', 'Europe', 'Asia', 'Africa', 'Europe', 'Europe', 'Asia', 'Africa', 'Americas', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Americas', 'Africa', 'Africa', 'Asia', 'Europe', 'Africa', 'Africa', 'Americas', 'Oceania', 'Europe', 'Asia', 'Asia', 'Europe', 'Africa', 'Europe', 'Africa', 'Americas', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Americas', 'Africa', 'Africa', 'Americas', 'Asia', 'Americas', 'Africa', 'Americas', 'Europe', 'Americas', 'Europe', 'Europe', 'Africa', 'Americas', 'Americas', 'Africa', 'Americas', 'Africa', 'Africa', 'Europe', 'Europe', 'Africa', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Africa', 'Africa', 'Americas', 'Americas', 'Europe', 'Europe', 'Asia', 'Asia', 'Asia', 'Europe', 'Asia', 'Europe', 'Americas', 'Asia', 'Asia', 'Africa', 'Asia', 'Asia', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Asia', 'Africa', 'Africa', 'Africa', 'Americas', 'Asia', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Asia', 'Europe', 'Oceania', 'Americas', 'Africa', 'Africa', 'Europe', 'Asia', 'Asia', 'Americas', 'Americas', 'Americas', 'Asia', 'Europe', 'Europe', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Europe', 'Africa', 'Asia', 'Europe', 'Africa', 'Europe', 'Asia', 'Africa', 'Europe', 'Europe', 'Asia', 'Africa', 'Americas', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Americas', 'Africa', 'Africa')
idx <- rep(c("corruption_perception_index", "human_development_index"), each=121)
cpiRk <- c(166, 88, 88, 163, 106, 13, 16, 50, 139, 15, 83, 76, 29, 76, 69, 76, 150, 150, 130, 10, 145, 147, 23, 83, 83, 136, 40, 50, 56, 38, 1, 98, 102, 106, 88, 72, 154, 102, 3, 23, 98, 123, 11, 56, 58, 123, 139, 158, 158, 111, 50, 13, 76, 88, 161, 18, 32, 61, 69, 18, 45, 139, 55, 123, 61, 83, 161, 123, 111, 54, 95, 111, 45, 111, 72, 61, 88, 111, 147, 45, 130, 9, 1, 130, 98, 136, 5, 60, 117, 72, 130, 88, 95, 29, 28, 58, 43, 66, 48, 61, 71, 119, 7, 34, 61, 37, 83, 165, 4, 6, 76, 106, 72, 76, 66, 139, 11, 16, 21, 76, 150, 166, 88, 88, 163, 106, 13, 16, 50, 139, 15, 83, 76, 29, 76, 69, 76, 150, 150, 130, 10, 145, 147, 23, 83, 83, 136, 40, 50, 56, 38, 1, 98, 102, 106, 88, 72, 154, 102, 3, 23, 98, 123, 11, 56, 58, 123, 139, 158, 158, 111, 50, 13, 76, 88, 161, 18, 32, 61, 69, 18, 45, 139, 55, 123, 61, 83, 161, 123, 111, 54, 95, 111, 45, 111, 72, 61, 88, 111, 147, 45, 130, 9, 1, 130, 98, 136, 5, 60, 117, 72, 130, 88, 95, 29, 28, 58, 43, 66, 48, 61, 71, 119, 7, 34, 61, 37, 83, 165, 4, 6, 76, 106, 72, 76, 66, 139, 11, 16, 21, 76, 150)
vl <- c(0.479, 0.764, 0.745, 0.533, 0.827, 0.939, 0.893, 0.824, 0.579, 0.896, 0.485, 0.75, 0.698, 0.754, 0.794, 0.402, 0.404, 0.563, 0.518, 0.92, 0.352, 0.396, 0.847, 0.738, 0.727, 0.498, 0.776, 0.827, 0.775, 0.878, 0.925, 0.473, 0.722, 0.739, 0.691, 0.68, 0.42, 0.448, 0.895, 0.897, 0.697, 0.452, 0.926, 0.579, 0.866, 0.64, 0.414, 0.424, 0.493, 0.625, 0.836, 0.921, 0.624, 0.689, 0.649, 0.923, 0.899, 0.887, 0.73, 0.903, 0.742, 0.555, 0.8, 0.763, 0.497, 0.427, 0.716, 0.512, 0.476, 0.789, 0.442, 0.513, 0.781, 0.762, 0.735, 0.807, 0.647, 0.418, 0.556, 0.64, 0.558, 0.924, 0.915, 0.645, 0.353, 0.527, 0.949, 0.796, 0.55, 0.788, 0.693, 0.74, 0.682, 0.855, 0.843, 0.802, 0.498, 0.574, 0.847, 0.494, 0.776, 0.42, 0.925, 0.89, 0.666, 0.884, 0.766, 0.49, 0.913, 0.939, 0.74, 0.487, 0.78, 0.725, 0.767, 0.493, 0.91, 0.92, 0.795, 0.579, 0.516, 11, 36, 36, 15, 32, 79, 76, 51, 25, 77, 37, 38, 63, 38, 41, 38, 21, 21, 27, 83, 24, 22, 70, 37, 37, 26, 55, 51, 47, 56, 91, 34, 33, 32, 36, 39, 18, 33, 90, 70, 34, 28, 81, 47, 46, 28, 25, 17, 17, 31, 51, 79, 38, 36, 16, 75, 61, 44, 41, 75, 53, 25, 49, 28, 44, 37, 16, 28, 31, 50, 35, 31, 53, 31, 39, 44, 36, 31, 22, 53, 27, 84, 91, 27, 34, 26, 88, 45, 30, 39, 27, 36, 35, 63, 64, 46, 54, 42, 52, 44, 40, 29, 85, 60, 44, 58, 37, 12, 89, 86, 38, 32, 39, 38, 42, 25, 81, 76, 74, 38, 21)

hdi_cpi_data_long <- data.frame(country=ctry, year=2015L, country_code=ctryCode, cpi_rank=cpiRk, 
                                region=regn, continent=cnt, index=idx, value=vl,
                                stringsAsFactors = FALSE
                                )

## explore hdi_cpi_data_long
str(hdi_cpi_data_long)

## How many unique values are there in the index column?
unique(hdi_cpi_data_long$index)


## convert from long to wide
hdi_cpi_data_wide <- hdi_cpi_data_long %>% 
  spread(key=index, value=value)
  
## display the first 5 rows from hdi_cpi_data_wide
head(hdi_cpi_data_wide, 5)


## plot corruption_perception_index  versus human_development_index
figure(legend_location = "top_left") %>% 
    ly_points(x=human_development_index, y=corruption_perception_index, color=continent, alpha=0.7,
              hover=c(country, cpi_rank,corruption_perception_index, human_development_index), 
              data=hdi_cpi_data_wide
              )


## convert from wide to long
hdi_cpi_remake_long <- hdi_cpi_data_wide %>%
    gather(key="index", value="value", corruption_perception_index, human_development_index)
  
## display the first 5 rows of hdi_data_long
head(hdi_cpi_remake_long, 5)
all.equal(hdi_cpi_data_long, hdi_cpi_remake_long)


## explore the unique values in the movie_budget column
# unique(dat_90_13_long$movie_budget)

## spread the values in the `movie_budget` in two columns
# dat_90_13_wide <- dat_90_13_long %>% 
#   spread(key=movie_budget, value=value)
  
## View column names of dat_90_13_wide
# names(dat_90_13_wide)

## create a scatter plot with log(budget_2013) Vs log(intgross_2013) 
# p_scatter <- figure() %>%
#   ly_points(y=log(intgross_2013), x=log(budget_2013), size=4, alpha=0.5, data=dat_90_13_wide)
  
## View plot
# p_scatter

## fit a linear reg model
# lin_reg <- lm(log(intgross_2013) ~ log(budget_2013), data = dat_90_13)

## add the linear regression line layer to p_scatter
# p_scatter %>% 
#   ly_abline(lin_reg)


## extract entries for year 2007
dat_2007 <- gapminder %>% 
  filter(year == 2007)
dat_2002 <- gapminder %>% 
  filter(year == 2002)

## create scatter plot
figure(toolbar_location="above", legend_location="bottom_right") %>%
    ly_points(x=gdpPercap, y=lifeExp, color=continent, size=6, alpha=0.7, 
              data=dat_2007, hover=c(country, lifeExp, gdpPercap)
              )

figure(legend_location = "bottom_right", tools=c("resize", "save")) %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_2002, color = continent)

figure(legend_location = "bottom_right", tools=c("resize", "save"), toolbar_location=NULL) %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_2002, color = continent)

```
  
  
  
***
  
Chapter 4 - Grid Plots and Maps  
  
Intro to Grid Plots:  
  
* Example of dataset for TB by year by age group in the US - combine multiple figures in the same area  
	* figure() %>% ly_bar(x = year, y = count, data = tb_2534, color = gender, position = "stack")  # x should be a factor, default is stacked bars if position is missing (dodge or fill also available)  
* Can use the grid plot basics for multi-plotting  
	* fig_list <- list(bar_2534 = bar_2534, bar_3544 = bar_3544)  
    * grid_plot(fig_list, width = 1000, height = 500)  # will have different axis limits by default  
    * grid_plot(fig_list, width = 1000, height = 500, same_axes = TRUE)  # forces the axes to be on the same scale  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  # overrides the defaults of all in one row  
    * fig_list <- list(list(bar_1524 = bar_1524, bar_2534 = bar_2534), list(bar_3544 = bar_3544))  
    * grid_plot(fig_list, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  # list of plots plots one list per row, and NULL will place an empty plot in that column  
  
Facets with Grid Plots:  
  
* Facets can be helpful for slicing data, placing small batches of the data (segmented by factor) in a larger plot  
* Can start by creating a plot for each of the groups, though this is inefficient  
	* fig_list <- list(bar_1524 = bar_1524, bar_2534 = bar_2534, bar_3544 = bar_3544, bar_4554 = bar_4554, bar_5564 = bar_5564, bar_65 = bar_65)  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  
* Can instead use the split() function and a function for plotting to create the relevant lists  
	* tb_split_age <- split(tb, tb$age)  
    * plot_bar <- function(x){ figure() %>% ly_bar(y = count, year, data = x, color = gender, position = "dodge")}  
* Can instead use the lapply() functionality  
	* fig_list <- lapply(tb_split_age, plot_bar)  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  
  
rbokeh maps:  
  
* Can create interactive maps using rbokeh - transportation, density, population, and other areas where geography is key to understanding  
* NYC bike data available from the bikedata package  
* Can begin by initializing a map (appears to source Google Maps)  
	* ny_map <- gmap(lat = 40.73306, lng = -73.97351, zoom = 11)   
* Can use the map type argument to change the type of map - hybrid, satellite, road, terrain  
* Can also customize maps using gamp_style(), such as making the water blue  
	* ny_map <- gmap(lat = 40.73306, lng = -73.97351, zoom = 11, map_style = gmap_style("blue_water"))   
* Can then add points layers to the map and arrange in grids using grid_plot() as per previous chapters  
	* ny_map %>% ly_points(x = station_longitude, y = station_latitude, data = ny_bikedata_20170427, fill_color = start_count, line_alpha = 0, size = 8, hover = c(station_name, start_count))  
    * grid_plot(list(weekeend_April23 = map_weekend_20170423, weekday_April25 = map_weekday_20170425), width = 860, height = 420)  
  
Example code includes:  
```{r eval=FALSE}

tb <- data.frame(iso2="US", 
                 gender=rep(c("m", "f"), each=84),
                 year=factor(c(1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008)), 
                 age=c(1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65), 
                 count=c(355, 333, 330, 321, 331, 365, 320, 343, 365, 362, 383, 388, 414, 375, 876, 815, 701, 663, 616, 602, 613, 562, 526, 547, 535, 568, 490, 513, 1417, 1219, 1127, 1009, 1011, 906, 824, 813, 754, 728, 666, 659, 572, 495, 1121, 1073, 979, 1007, 930, 904, 876, 795, 828, 829, 767, 759, 744, 725, 742, 678, 679, 628, 601, 577, 524, 490, 487, 504, 499, 531, 533, 526, 1099, 1007, 944, 914, 801, 738, 649, 592, 650, 582, 624, 596, 562, 561, 280, 289, 269, 269, 232, 246, 239, 233, 277, 265, 241, 257, 257, 220, 579, 487, 449, 425, 391, 376, 410, 423, 353, 339, 348, 384, 338, 329, 499, 478, 447, 424, 394, 349, 346, 362, 310, 302, 276, 263, 260, 269, 285, 279, 254, 267, 245, 253, 247, 255, 269, 252, 242, 212, 225, 224, 202, 217, 201, 179, 244, 152, 176, 167, 169, 166, 161, 146, 135, 172, 591, 541, 514, 492, 444, 396, 389, 370, 354, 344, 322, 303, 308, 300), 
                 stringsAsFactors = FALSE
                 )
str(tb)
tb_2534 <- tb %>% filter(age==2534)
str(tb_2534)


## create a bar plot for age group tb_2534
bar_2534 <- figure() %>%
  ly_bar(x=year, y=count, color=gender, data=tb_2534, hover=TRUE)

## View figure
bar_2534


## create a bar plot for age group tb_2534 with % on the y-axis
bar_2534_percent <- figure(ylab = "share") %>% 
  ly_bar(x = year, y =  count,  tb_2534, color = gender, hover = TRUE,  position = "fill")
         
## View figure
bar_2534_percent


## create a list with bar_2534 and bar_2534_percent figures
fig_list <- list(bar_2534 = bar_2534, bar_2534_percent = bar_2534_percent)

## create a grid plot 
grid_plot(fig_list, width=1000, height=400)

## create a grid plot with same axes limits
grid_plot(figs = fig_list, width = 1000, height = 400, same_axes=TRUE)


plot_line <- function(x){
    figure() %>% 
        ly_lines(y =  count, year, data = x, color = age,  alpha = 1, width = 2)
}

## create two dataframes for female/male data           
tb_female <- tb %>% filter(gender=="f")
tb_male <- tb %>% filter(gender=="m")


## create two plots using plot_line
fig_female <- plot_line(tb_female)
fig_male <- plot_line(tb_male)

## create figure list
fig_list <- list(female = fig_female, male = fig_male)

## plot the two figures in a grid
grid_plot(fig_list, width=1000, height=600, same_axes=TRUE)


## split tb data by gender 
tb_split_gender <- split(tb, tb$gender)

## create a list of figures using lapply
fig_list <- lapply(tb_split_gender, FUN=plot_line)

## create a grid plot 
grid_plot(fig_list, width=1000, height=600, same_axes=TRUE)


## define a function to create a bar plot with the number of tb cases over time
plot_bar <- function(x){ 
    figure() %>% 
        ly_bar(y=count, x=year, data=x, color = gender, position = "dodge", hover=TRUE)
}

## split tb data by age
tb_split_age <- split(tb, tb$age)

## apply the function to the groups in tb_split_age
fig_list <- fig_list <- lapply(tb_split_age, plot_bar)

## create a grid plot 
grid_plot(fig_list, width=600, height=900, nrow=3, same_axes=TRUE) %>% 
    theme_axis("x", major_label_orientation = 90)


## initialize a map for NY center
# ny_map <- gmap(lat=40.73306, lng=-73.97351, zoom=11, map_style=gmap_style("blue_water"))
# ny_map


## filter ny_bikedata to get the entries for day "2017-04-25"
# ny_bikedata_20170425 <- ny_bikedata %>% filter(trip_date==as.Date("2017-04-25"))

## add a points layer to ny_map
# ny_map %>%
#     ly_points(y=station_latitude, x=station_longitude, 
#               size=8, fill_color=start_count, line_alpha=0, 
#               data=ny_bikedata_20170425, hover=c(station_name, start_count, end_count)
#               )

## create a names list with the two figures
# fig_list <- list(map_weekend=map_weekend_20170423, map_weekday=map_weekday_20170425)

## create a grid plot with the 2 maps
# grid_plot(fig_list, width=860, height=420)

```
  
  
  
***
  
### _A/B Testing in R_  
  
Chapter 1 - Mini Case Study in A/B Testing  
  
Introduction:  
  
* A/B testing is a powerful way to experiment with potential changes before implementing them  
	* Framework for testing new ideas to improve an existing design (often a website)  
* Hypothetical example - cat adoption website - could a change in home page improve conversion rate (clicks divided by views)?  
	* Question - does changing the photo improve conversion rate?  
    * Hypothesis - cat in hat will improve conversion rate  
    * Dependent variable - clicks  
    * Independent variable - homepage photo  
* Need to begin by assessing conversion rates in the current website  
	* click_data <- read_csv("click_data.csv")  
  
Baseline conversion rates:  
  
* Contnuning the previous example - hypothesis of cats in hats having "more" conversions  
	* Need to define "more" relative to some baseline - recent past, control group at same time, etc.  
    * click_data %>% summarize(conversion_rate = mean(clicked_adopt_today))  # mean from historical data  
    * click_data_sum <- click_data %>% group_by(lubridate::month(visit_date)) %>% summarize(conversion_rate = mean(clicked_adopt_today))  # mean by month from the historical data  
    * ggplot(click_data_sum, aes(x = `month(visit_date)`, y=conversion_rate)) + geom_point() + geom_line()  
  
Experimental design and power analysis:  
  
* Power analysis helps determine how many samples are needed (thus how long the experiment needs to run)  
	* Ideal is to run both conditions simultaneously - mitigate seasonality and other potential confounders  
    * Should know the planned test, baseline (control) value, and desired (test) value, as well as proportion of the data (typically 0.5), significance/alpha (typically 0.05), and power (typically 0.8)  
* Can use the powerMediation package to assess the power - note the function returns the total sample size, so each group is divided by 2  
	* library(powerMediation)  
    * total_sample_size <- SSizeLogisticBin(p1 = 0.2, p2 = 0.3, B = 0.5, alpha = 0.05, power = 0.8)  # note the function returns the total sample size, so each group is divided by 2  
  
Example code includes:  
```{r}


# Read in data
click_data <- readr::read_csv("./RInputFiles/click_data.csv")
click_data


# Find oldest and most recent date
min(click_data$visit_date)
max(click_data$visit_date)

# Calculate the mean conversion rate by day of the week
click_data %>%
  group_by(weekdays(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Calculate the mean conversion rate by week of the year
click_data %>%
  group_by(lubridate::week(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))


# Compute conversion rate by week of the year
click_data_sum <- click_data %>%
    mutate(weekOfYear = lubridate::week(visit_date)) %>%
    group_by(weekOfYear) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Build plot
ggplot(click_data_sum, aes(x = `weekOfYear`, y = conversion_rate)) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = scales::percent)


# Compute and look at sample size for experiment in August
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.54, p2 = 0.64, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# Compute and look at sample size for experiment in August with 5% increase
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.54, p2 = 0.59, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size

```
  
  
  
***
  
Chapter 2 - Mini Case Study in A/B Testing - Part II  
  
Analyzing Results:  
  
* Can analyze the experiment data from the previous design - available in a new dataset  
	* experiment_data <- read_csv("experiment_data.csv")  
    * experiment_data %>% group_by(condition) %>% summarize(conversion_rate = mean(clicked_adopt_today))  
    * experiment_data_sum <- experiment_data %>% group_by(visit_date, condition) %>% summarize(conversion_rate = mean(clicked_adopt_today))  
    * ggplot(experiment_data_sum, aes(x = visit_date, y = conversion_rate, color = condition, group = condition)) + geom_point() + geom_line()  
* Can further assess the statistical significance of the outcomes  
	* glm(clicked_adopt_today ~ condition, family = "binomial", data = experiment_data) %>% broom::tidy()  
  
Designing follow-up experiments:  
  
* Can continue to refine and test new hypotheses, but typically still with one step at a time  
	* Experiments need to be unique, and each with their own control group  
    * Attempt to avoid confounding variables; hard to explain real-world outcomes if there were many changes at the same time  
  
Pre-follow-up-experiment assumptions:  
  
* Control conditions for seasonal products can be especially challenging - careful not to choose times that already have very extreme conversion rates  
  
Follow-up experiment assumptions:  
  
* May want to look at the differences in conversion rate by month  
	* eight_month_checkin_data_sum <- eight_month_checkin_data %>%  
    *     mutate(month_text = month(visit_date, label = TRUE)) %>% group_by(month_text, condition) %>%  
    *     summarize(conversion_rate = mean(clicked_adopt_today))  
    * eight_month_checkin_data_diff <- eight_month_checkin_data_sum %>%  
    *     spread(condition, conversion_rate) %>%  
    *     mutate(condition_diff = cat_hat - no_hat)  
    * mean(eight_month_checkin_data_diff$condition_diff)  
    * sd(eight_month_checkin_data_diff$condition_diff)  
  
Example code includes:  
```{r}

experiment_data <- read_csv("./RInputFiles/experiment_data.csv")
experiment_data
followup_experiment_data <- read_csv("./RInputFiles/eight_month_checkin_data.csv")
followup_experiment_data


# Group and summarize data
experiment_data_clean_sum <- experiment_data %>%
    group_by(condition, visit_date) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Make plot of conversion rates over time
ggplot(experiment_data_clean_sum, aes(x = visit_date, y = conversion_rate, 
                                      color = condition, group = condition
                                      )
       ) + 
    geom_point() +
    geom_line()


# View summary of results
experiment_data %>% 
    group_by(condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
experiment_results <- glm(clicked_adopt_today ~ condition, family = "binomial", 
                          data = experiment_data
                          ) %>%
    broom::tidy()
experiment_results


# Run logistic regression power analysis
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.39, p2 = 0.59, B = 0.5, 
                                                      alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# View conversion rates by condition
followup_experiment_data %>%
    group_by(condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
followup_experiment_results <- glm(clicked_adopt_today ~ condition, family = "binomial",
                                   data = followup_experiment_data
                                   ) %>%
    broom::tidy()
followup_experiment_results


# Compute monthly summary
eight_month_checkin_data_sum <- followup_experiment_data %>%
    mutate(month_text = lubridate::month(visit_date, label = TRUE)) %>%
    group_by(month_text, condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Plot month-over-month results
ggplot(eight_month_checkin_data_sum, aes(x = month_text, y = conversion_rate, 
                                         color = condition, group = condition
                                         )
       ) +
    geom_point() +
    geom_line()


# Plot monthly summary
ggplot(eight_month_checkin_data_sum, aes(x = month_text, y = conversion_rate,
                                         color = condition, group = condition
                                         )
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
    labs(x = "Month", y = "Conversion Rate")


# Compute difference over time
# no_hat_data_diff <- no_hat_data_sum %>% 
#     spread(year, conversion_rate) %>% 
#     mutate(year_diff = `2018` - `2017`)
# no_hat_data_diff

# Compute summary statistics
# mean(no_hat_data_diff$year_diff, na.rm = TRUE)
# sd(no_hat_data_diff$year_diff, na.rm = TRUE)


# Run power analysis for logistic regression
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.49, p2 = 0.64, B = 0.5, 
                                                      alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# View summary of data
# followup_experiment_data_sep %>% 
#     group_by(condition) %>% 
#     summarize(conversion_rate=mean(clicked_adopt_today))

# Run logistic regression
# followup_experiment_sep_results <- glm(clicked_adopt_today ~ condition,
#                                        family = "binomial",
#                                        data = followup_experiment_data_sep
#                                        ) %>%
#     broom::tidy()
# followup_experiment_sep_results

```
  
  
  
***
  
Chapter 3 - Experimental Design in A/B Testing  
  
A/B Testing Research Questions:  
  
* A/B testing combined experimental design and statistics - core building block basic principles  
* Any experimental design that compares two ideas can be run as an A/B test - conversion rates, engagement with a web site, drop-off rates, total amount of time or money spent  
* Example of looking at time spent on a websit  
	* str(viz_website_2017)  
    * viz_website_2017 %>% summarize(mean(time_spent_homepage_sec))  
    * viz_website_2017 %>% group_by(month(visit_date)) %>% summarize(mean(time_spent_homepage_sec))  
  
Assumptions and types of A/B testing:  
  
* Example of changing text in a website title, then checking the implications  
* Can look at within-group experiments (everyone sees both) or between-group experiments (everyone sees one or the other)  
	* The within experiment will often have better power, while the between experiment is easier to run when people may only interact once  
    * The between experiment needs to be appropriately random, so that whether the person sees A/B is not linked to other attributes of the person  
* There are several types of A/B testing  
	* A/B testing is test and control  
    * A/A testing is to verify that the control process is working well - should be no significant effects  
    * A/B/N testing is a control conditions with any number of test conditions - seems exciting and fast, but has more challenging statistics and requires more data points  
  
Confounding variables?  
  
* Confounding variables are elements of the environment that can confound your ability to find the real effect of A/B  
	* Sometimes the confounder is internal to the experiment - examples of word length/novelty being the real driver rather than the specific work chosen  
    * Sometimes the confounder is external to the experiment - examples of differing demographics by month, with the demographics having been a key driver of outcomes  
  
Side effects:  
  
* Side effects are unintended effects of a change that you made  
    * Example of changing from tools to tips if it changed the page loading times  
* Side effects can include load times and information "above the fold" (what a person sees without doing any scrolling)  
  
Example code includes:  
```{r eval=FALSE}

# Compute summary by month
viz_website_2017 %>%
    group_by(month(visit_date)) %>%
    summarize(article_conversion_rate = mean(clicked_article))


# Compute 'like' click summary by month
viz_website_2017_like_sum <- viz_website_2017 %>%
    mutate(month = month(visit_date, label = TRUE)) %>%
    group_by(month) %>%
    summarize(like_conversion_rate = mean(clicked_like))

# Plot 'like' click summary by month
ggplot(viz_website_2017_like_sum,
       aes(x = month, y = like_conversion_rate, group = 1)
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Plot comparison of 'like'ing and 'sharing'ing an article
ggplot(viz_website_2017_like_share_sum,
       aes(x = month, y = conversion_rate, color = action, group = action)
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Compute conversion rates for A/A experiment
viz_website_2018_01_sum <- viz_website_2018_01 %>%
    group_by(condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))
viz_website_2018_01_sum

# Plot conversion rates for two conditions
ggplot(viz_website_2018_01_sum, aes(x = condition, y = like_conversion_rate)) +
    geom_bar(stat = "identity") +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Run logistic regression
aa_experiment_results <- glm(clicked_like ~ condition, family = "binomial", data = viz_website_2018_01) %>%
    broom::tidy()
aa_experiment_results


# Compute 'like' conversion rate by week and condition
viz_website_2018_02 %>%
    mutate(week = week(visit_date)) %>%
    group_by(week, condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))

# Compute 'like' conversion rate by if article published and condition
viz_website_2018_02 %>%
    group_by(article_published, condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))


# Plot 'like' conversion rates by date for experiment
ggplot(viz_website_2018_02_sum,
       aes(x = visit_date, y = like_conversion_rate, color = condition,
           linetype = article_published, group = interaction(condition, article_published)
           )
       ) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = as.numeric(as.Date("2018-02-15"))) +
    scale_y_continuous(limits = c(0, 0.3), labels = percent)


# Compute 'like' conversion rate and mean pageload time by day
viz_website_2018_03_sum <- viz_website_2018_03 %>%
    group_by(visit_date, condition) %>%
    summarize(mean_pageload_time = mean(pageload_time), like_conversion_rate = mean(clicked_like))

# Plot effect of 'like' conversion rate by pageload time
ggplot(viz_website_2018_03_sum, aes(x = mean_pageload_time, y = like_conversion_rate, color = condition)) +
    geom_point()


# Plot 'like' conversion rate by day
ggplot(viz_website_2018_03_sum, aes(x = visit_date, y = like_conversion_rate, color = condition,
                                    linetype = pageload_delay_added, 
                                    group = interaction(condition, pageload_delay_added)
                                    )
       ) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = as.numeric(as.Date("2018-03-15"))) +
    scale_y_continuous(limits = c(0, 0.3), labels = percent)

```
  
  
  
***
  
Chapter 4 - Statistical Analyses in A/B Testing  
  
Power analyses:  
  
* Generally, the goal of a power analysis is to determine the sample size - dependent on alpha, power (1 minus beta), and effect size  
	* Effect size is often defined as the difference in the two groups divided by the standard deviation of the groups  
* The t-test is often used for significance, and can be planned using the library(pwr)  
	* pwr.t.test(power = 0.8, sig.level = 0.05, d = 0.6)  # will return the number of data points needed for power 0.8, alpha 0.05, effect size 0.6  
    * pwr.t.test(power = 0.8, sig.level = 0.05, d = 0.2)  # effect size change of ~3x drives sample size change of ~9x (delta-effect-size-squared)  
  
Statistical tests:  
  
* Logistic regression and t-tests are both common statistical methods used for A/B testing  
	* viz_website_2018_01 <- read_csv("viz_website_2018_01.csv")  
    * aa_experiment_results <- t.test(time_spent_homepage_sec ~ condition, data = viz_website_2018_01)  
* Linear regression can be thought of as an extension of t-tests with more than 2 levels per variable  
	* However, for an A/B test with only 2-levels, you will get the same results  
  
Stopping rules and sequential analysis:  
  
* Stopping rules are procedures that allow for interim analysis (peaks in to the data) - also known as "sequential analysis"  
	* Can stop because the experiment worked, stop because the experiment failed, or continue experiment  
    * The p-value needs to be adjusted lower to account for the multiple peaks at the data  
    * Need to be very careful to prevent p-hacking by creating the stopping rules and points in advance  
* The library(gsDesign) can help with running sequrntial analysis in R  
	* library(gsDesign)  
    * seq_analysis <- gsDesign(k = 4, test.type = 1, alpha = 0.05, beta = 0.2, sfu = "Pocock")  # k=4 looks, test.type=1 is similar to one-sided test, alpha is significance, beta is 1-power so beta=0.2 is power=0.8, sfu is the spending function  
* Can then figure out the sample sizes using resource-based approaches  
	* max_n <- 1000  
    * max_n_per_group <- max_n / 2  
    * stopping_points <- max_n_per_group * seq_analysis$timing  
  
Multivariate testing:  
  
* Sometmes want to make comparisons that account for multiple changes  
	* multivar_results <- lm(time_spent_homepage_sec ~ word_one data = viz_website_2018_05) %>% tidy()  # single variable  
    * multivar_results <- lm(time_spent_homepage_sec ~ word_one * word_two, data = viz_website_2018_05) %>% tidy()  # full interaction effects  
* The default R order for regressions is to use the lowest alphanumeric as the baseline level - can modify this pre-regression though  
	* multivar_results <- viz_website_2018_05 %>% mutate(word_one = factor(word_one, levels = c("tips", "tools"))) %>% mutate(word_two = factor(word_two, levels = c("better", "amazing"))) %>% lm(time_spent_homepage_sec ~ word_one * word_two, data = .) %>% tidy()  
  
A/B Testing Recap:  
  
* Introduction to the basic concepts of A/B testing  
* New Ideas -> Experiments -> Statistical Analysis -> Implement Winners -> Repeat  
  
Example code includes:  
```{r eval=FALSE}

# Run power analysis for logistic regression
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.17, p2 = 0.27, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# Run power analysis for t-test
sample_size <- pwr::pwr.t.test(d = 0.3, sig.level = 0.05, power = 0.8)
sample_size


# Run logistic regression
ab_experiment_results <- glm(clicked_like ~ condition, family = "binomial", data = viz_website_2018_04) %>%
    broom::tidy()
ab_experiment_results


# Run t-test
ab_experiment_results <- t.test(time_spent_homepage_sec ~ condition, data = viz_website_2018_04)
ab_experiment_results


# Run sequential analysis
seq_analysis_3looks <- gsDesign::gsDesign(k = 3, test.type = 1, 
                                          alpha = 0.05, beta = 0.2, sfu = "Pocock"
                                          )
seq_analysis_3looks


# Fill in max number of points and compute points per group and find stopping points
max_n <- 3000
max_n_per_group <- max_n / 2
stopping_points <- max_n_per_group * seq_analysis_3looks$timing
stopping_points


# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>% 
    group_by(word_one, word_two) %>% 
    summarize(mean_time_spent_homepage_sec = mean(time_spent_homepage_sec))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum, aes(x = word_one, y = mean_time_spent_homepage_sec, fill = word_two)) + 
    geom_bar(stat = "identity", position = "dodge")


# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>% 
    group_by(word_one, word_two) %>% 
    summarize(like_conversion_rate = mean(clicked_like))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum, aes(x = word_one, y = like_conversion_rate, fill = word_two)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Organize variables and run logistic regression
viz_website_2018_05_like_results <- viz_website_2018_05 %>%
    mutate(word_one = factor(word_one, levels = c("tips", "tools"))) %>%
    mutate(word_two = factor(word_two, levels = c("better", "amazing"))) %>%
    glm(clicked_like ~ word_one * word_two, family = "binomial", data = .) %>%
    broom::tidy()
viz_website_2018_05_like_results

```
  
  
  
***
  
### _Mixture Models in R_  
  
Chapter 1 - Introduction to Mixture Models  
  
Introduction to Model-Based Clustering:  
  
* Mixture models are a tool for model-based clustering (partitioning and segmentation)  
	* Objective for clusters to be homogenous within and heterogenous across  
* Common techniques include k-means (assign to nearest centers) and hierarchical (connect based on sililarity) and probabilistic model-based approaches (mixture models)  
	* gender <- read.csv("gender.csv")  
    * ggplot(gender, aes(x = Weight, y = BMI)) + geom_points()  
* May be helpful to have probabilities by gender rather than a hard cutoff for male vs. female  
	* This is the core of the mixture model - assumes an underlying probability distribution, with the outcome being the combination of these distributions  
  
Gaussian Distribution:  
  
* There are packages for fitting mixture models in R - mixtools (no Poisson), bayesmix (Bayesian), EMCluster (Gaussian only), flexmix (covered in this course)  
* The Gaussain distribution frequently plays a role in the mixture model  
	* Defined by the mean and standard deviation  
    * rnorm(n, mean, sd)  # sample from the Gaussian of mean and sd, taking n samples  
    * The mean is typically estimated as the sample mean, and the sd is typically estimated as root-mean-squared-delta-from-mean - sd()  
    * ggplot(data = population_sample) + geom_histogram(aes(x = x, y = ..density..)) + stat_function(geom = "line", fun = dnorm, args = list(mean = mean_estimate, sd = standard_deviation_estimate))  
  
Gaussian Mixture Models (GMM):  
  
* Can imagine two Gaussian distributions, and pick from each randomly with 50/50 probability  
	* number_of_obs <- 500  
    * coin <- sample(c(0,1), size = number_of_obs, replace = TRUE, prob = c(0.5, 0.5))  # can change the coin to be non-50/50 for other mixture simulations  
    * gauss_1 <- rnorm(n = number_of_obs, mean = 5, sd = 2)  
    * gauss_2 <- rnorm(n = number_of_obs)  
    * mixture_simulation <- ifelse(coin, gauss_1, gauss_2)  
    * head(cbind(coin, gauss_1, gauss_2, mixture_simulation))  
    * mixture_simulation <- data.frame(x = mixture_simulation)  
    * ggplot(mixture_simulation) + geom_histogram(aes(x = x, ..density..), bins = 40)  
* Can also create mixtures of 3+ underlying Gaussian  
	* proportions <- sample(c(0, 1, 2), number_of_obs, replace = TRUE, prob = c(1/3, 1/3, 1/3))  
    * gauss_3 <- rnorm(n = number_of_obs, mean = 10, sd = 1)  
    * mixture_simulation <- data.frame(x = ifelse(proportions == 0, gauss_1, ifelse(proportions == 1, gauss_2, gauss_3)))  
    * ggplot(mixture_simulation) + geom_histogram(aes(x = x, ..density..), bins = 40)  
  
Example code includes:  
```{r}

gender <- readr::read_csv("./RInputFiles/gender.csv")
glimpse(gender)

# Have a look to gender (before clustering)
head(gender)

# Scatterplot with probabilities
gender %>% 
  ggplot(aes(x = Weight, y = BMI, col = probability))+
  geom_point(alpha = 0.5)


# Set seed
set.seed(1313)

# Simulate a Gaussian distribution
simulation <- rnorm(n = 500, mean = 5, sd = 4)

# Check first six values
head(simulation)

# Estimation of the mean
mean_estimate <- mean(simulation)
mean_estimate

# Estimation of the standard deviation
standard_deviation_estimate <- sd(simulation)
standard_deviation_estimate

# Transform the results to a data frame
simulation <- data.frame(x = simulation)

# Plot the sample with the estimated curve
ggplot(simulation) + 
  geom_histogram(aes(x = x, y = ..density..)) + 
  stat_function(geom = "line", fun = dnorm,
                args = list(mean = mean_estimate, 
                sd = standard_deviation_estimate))


# Estimation of the mean
mean_estimate <- gender %>% 
  pull(Weight) %>% 
  mean()
mean_estimate

# Estimation of the standard deviation
sd_estimate <- gender %>% 
  pull(Weight) %>% 
  sd()
sd_estimate

# Plot the sample with the estimated curve
gender %>% 
  ggplot() + 
  geom_histogram(aes(x = Weight, y = ..density..), bins = 100) + 
  stat_function(geom = "line", fun = dnorm,
                args = list(mean = mean_estimate, sd = sd_estimate))


# Create coin object
coin <- sample(c(0, 1), size = 500, replace = TRUE, prob = c(0.2, 0.8))

# Sample from two different Gaussian distributions
mixture <- ifelse(coin == 1, rnorm(n = 500, mean = 5, sd = 2), rnorm(n = 500))

# Check the first elements
head(mixture)


# Transform into a data frame
mixture <- data.frame(x = mixture)

# Create histogram especifiying that is a density plot
mixture %>% ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 50)


number_observations <- 1000

# Create the assignment object
assignments <- sample(c(0, 1 , 2), size = number_observations, replace = TRUE, prob = c(0.3, 0.4, 0.3))

# Simulate the GMM with 3 distributions
mixture <- data.frame(
    x = ifelse(assignments == 1, rnorm(n = number_observations, mean = 5, sd = 2), 
               ifelse(assignments == 2, 
                      rnorm(n = number_observations, mean = 10, sd = 1), 
                      rnorm(n = number_observations)
                      )
               )
    )

# Plot the mixture
mixture %>% 
    ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 50)

```
  
  
  
***
  
Chapter 2 - Structure of Mixture Models and Parameter Estimation  
  
Structure of Mixture Models:  
  
* Three questions to be answered for clustering with mixture models  
	* Suitable probability distribution - depends on domain expertise  
    * Number of clusters - domain expertise or testing to see what best satisfies criteria  
    * Parameters and estimates - based on Expectation Maximization (EM) Algorithm  
* Example of the gender dataset - bivariate Gaussian, with two clusters, and mu/sigma/proportion for each  
* Example of handwritten digits (3 vs. 6) - Bernoulli distributions with two clusters, mean probability of being 1 for every dot  
* Example of crime types in Chicago - Poisson distribution with six clusters (crime types), average and proportion of crimes  
  
Parameter Estimation:  
  
* Suppose that we have an assumption of 2 clusters each from a Gaussian distribution and with each distribution having the same sigma  
	* If the probabilities are known, try to estimate the means  
    * If the means are known, try to estimate the probabilities  
    * means_estimates <- data_with_probs %>% summarise(mean_red = sum(x * prob_red) / sum(prob_red), mean_blue = sum(x * prob_blue) / sum(prob_blue))  
    * proportions_estimates <- data_with_probs %>% summarise(proportion_red = mean(prob_red), proportion_blue = 1 - proportion_red)  
    * data %>% mutate(prob_from_red = 0.3 * dnorm(x, mean = 3), prob_from_blue = 0.7 * dnorm(x,mean = 5), prob_red = prob_from_red/(prob_from_red + prob_from_blue), prob_blue = prob_from_blue/(prob_from_red + prob_from_blue)) %>%  
    *     select(x, prob_red, prob_blue) %>% head()  
  
EM Algorithm:  
  
* Can begin by making naïve assumptions about the distributions, for later refinement  
	* means_init <- c(1, 2)  
    * props_init <- c(0.5, 0.5)  
* Can then run a first iteration of the probabilities - the expectations  
	* means_estimates <- data_with_probs %>% summarise(mean_red = sum(x * prob_red) / sum(prob_red), mean_blue = sum(x * prob_blue) / sum(prob_blue)) %>% as.numeric()  
    * props_estimates <- data_with_probs %>% summarise(proportion_red = mean(prob_red), proportion_blue = 1- proportion_red) %>% as.numeric()  
* Basically, the iterations continue  
	* Iteration 0: Initial Parameters -> Estimate Probabilities (1)  
    * Iteration 1: Estimated Probabilities (1) -> Estimated Parameters (2) -> Estimated Probabilities (2)  
    * Iteration 2: Estimated Probabilities (2) -> Estimated Parameters (3) -> Estimated Probabilities (3)  
    * Etc.  
* Can translate the iterative process in to a function that is called by way of a for loop  
	* expectation <- function(data, means, proportions){  
    *     data <- data %>%  
    *         mutate(prob_from_red = proportions[1] * dnorm(x, mean = means[1]),  
    *                prob_from_blue = proportions[2] * dnorm(x, mean = means[2]),  
    *                prob_red = prob_from_red/(prob_from_red + prob_from_blue),  
    *                prob_blue = prob_from_blue/(prob_from_red + prob_from_blue)
    *                ) %>%  
    *         select(x, prob_red, prob_blue)  
    *     return(data)  
    * }  
    * for(i in 1:10){  
    *     new_values <- maximization(expectation(data, means_init, props_init))  
    *     means_init <- new_values[[1]]  
    *     props_init <- new_values[[2]]  
    *     cat(c(i, means_init, proportions_init),"\n")  
    * }  
  
Example code includes:  
```{r}

digits <- readr::read_csv("./RInputFiles/digits.csv")
dim(digits)

digitData <- digits[, 1:256]
digitKey <- digits[, 257:266]

# keep a subset of 4 and 8
digitUse <- rowSums(digitKey[, c(5, 9)]==1)
digData <- digitData[digitUse, ]
digKey <- digitKey[digitUse, ]

show_digit <- function(arr256, col=gray(4:1/4), ...) {
    arr256 <- as.numeric(arr256)
    image(matrix(arr256, nrow=16)[,16:1],col=col,...)
}

# Dimension
# broom::glance(digits)

# Apply `glimpse` to the data
glimpse(digitData)

# Digit in row 50
show_digit(digitData[50, ])

# Digit in row 100
show_digit(digitData[100, ])


gaussian_sample_with_probs <- data.frame(
    x = c(54.5, 7.7, 55.9, 27.9, 4.6, 59.9, 6.4, 60.5, 32.6, 21.3, 0.5, 8.9, 70.7, 49.3, 40.1, 43, 8.1, 62.9, 56, 54.4, 42.5, 46.1, 58.3, 61.7, -11.6, 10.8, 27.5, 12.2, 67.7, -5.6, 13.3, 62.7, 37.2, 41.4, 47.4, 54.2, 31, 60.2, 69.9, 33.8, 25.4, 21.9, 17.9, 61.5, 49.8, 37.9, 55.8, 14.1, 53.3, 45.6, 44.7, 14.2, -5.7, 10.9, 63.7, -6.5, 50.3, 61.4, 35.1, -3.7, 68.4, -6.2, 64, 24.4, 65.7, 59.7, 52.7, 27.2, 17.5, 22.6, 14.7, 22.1, 61.5, 55.6, 62.6, 5.6, 52.3, 8, 25.4, 48.8, 58.4, 6.2, 52.3, 6.6, 64, 43, 60.6, 33.5, 45.8, 2.5, 63, 58.2, 50.9, 22.1, 36.5, 27.1, 61.4, 56.3, 63.5, 55.6, 53.8, 31.9, 30.7, 15.6, 14.8, 44.4, 51.9, 61.4, 11.8, 51.3, 58.6, 45.4, 8.3, 41.5, 52.7, 9.1, 60.8, 40.2, 20.5, 40.2, 59.2, 36.7, 47.5, 12.2, 7.7, 56.2, -13.2, 6, 58.7, 43.7, 67.3, 53.6, 37.6, 54.3, 37.7, 51.9, 10.5, 42, 24, -0.7, 53.1, 27.4, 57.2, 37.3, 28.6, 13.5, 35.2, 22.7, 35.8, 66.9, 45.9, 45.9, 56.7, 55.6, 58.3, 3.2, 45.9, 59.5, 50.8, 43.7, 42.8, 4.7, 29.5, 50.9, 7.8, 44.3, 53.6, 57, 57.8, 47.3, 56.8, 51.1, 27.7, 44.9, 33, 44, 42.1, 38, 52.3, 44, 28.1, 52.7, 53.6, 4.7, 42.1, 40.8, 5, 8, 49.1, 67.5, 16.2, 11.2, 14.6, 32.8, 61.3, 49.8, 51.5, 54.5, 51.6, 45.8, 55.9, 7.4, -10.2, 41.9, 27.4, 45.1, 17.7, 37.5, 53.5, 25.7, 18.1, 13.4, 40.5, 13.3, 2, 49.8, 66.7, 34.7, 11.4, 42.1, 54.4, 48.3, 38.3, 17.4, 48.2, 48.4, 57.4, 54.5, 13.6, 52.3, -0.1, 12.8, 29.3, 45.6, 62.3, 49.2, 32.6, 38.4, 15, 6.1, 12.2, 5.8, 17.7, 20.7, 43.6, 52.3, 42.4, 64.6, 34.3, 9.5, 3.6, 37.2, 45.7, 56.9, 67, 48.7, -3.1, 50.1, 45.4, 54.4, 38.1, 10.8, 7.4, 50.5, 24.7, 11.4, 59.5, 43.9, 4.4, 53.7, 41.9, 60.2, 49.5, 11.6, 51.1, 69.1, 46.2, 35.5, 15, -6.4, 59.9, 57.3, 49.1, 55.5, 55.6, 43.9, 52.5, 46.4, 5.8, 55.3, 22.2, 42.7, 51.3, 40.1, 62.1, 62.2, 48.8, 6.1, 0.6, 19.6, 36.8, 48, 33.8, 52.8, 66.6, 30.2, 45.9, 5.9, 52.7, 49.7, 37.7, 10.4, 60.1, 35.8, 62.1, 35, 38.7, 13.3, -4.9, 30.6, 55.9, 23.7, 12.6, 45.7, 38.1, 9.9, 39.6, 46.3, -3.5, 31.2, 8.3, -8.1, 31.4, 65.7, 10.7, 5.5, 54.4, 51.8, 59.8, 50.3, 45.1, 8.5, 15.3, 3.2, 19.3, 40.8, 48.4, 30.1, 32.7, 12.7, 59.2, 51.4, 55.3, 58.9, -19, 61.9, 30.3, 77.2, 39.8, 31.3, 23.1, 56, 41.9, 0.5, 33.4, 36.6, 54.4, 12.4, 16.4, 24.4, -2.4, 30.9, 56.4, 12.5, 65.2, 10, -1.7, 45.7, 49.5, 45.3, 17.5, 29, -8.7, 51.7, 17.3, 20.2, 14.6, 47.6, 55.3, 50.2, 4.1, 47.5, 71, 13.2, 75.4, 6.2, 53, 54.2, 40.6, 55.1, 67.4, 45, 47.3, 44.2, 8.4, 46.1, 48.7, 8.3, 40.4, 63, 49, 2.8, 50.4, 17.7, 40.4, 41.1, 56.6, 37.3, -0.1, 62.5, 47.7, 62.1, 16.6, 33.3, 4.1, 61, 49.4, 44.1, 18.7, -1.3, 42.1, -11.8, 40.6, 45.6, 14.9, 51.9, 57.4, 41.3, 59.2, 58.6, 50.5, -3.9, -0.6, 11.5, 54.5, 57.1, 46.2, 51.9, 58.2, 51.6, 50.3, 64.2, 8.3, 49, 42, 43.7, 53.4, 6.5, 36.6, -18.2, 41.8, -6.8, 35, 46.8, 43.8, 60.6, -11.3, 18.5, 0.3, 40.2, 73.3, 58.2, 43.9, 22.2, 12.8, 6.7, 36.3, 51.8, 33.6, 71, 56.8, 26, 43.3, 37.4, 60, 17.2, -10.3, 43.9, 69, 38.7, 57.9, 40.2, 48.6, 57.7, 45.8, 56.2, 7.3, 32.1, 41.2, 39.1), 
    prob_cluster1=c(0, 1, 0, 0.552, 1, 0, 1, 0, 0.158, 0.947, 1, 1, 0, 0, 0.01, 0.003, 1, 0, 0, 0, 0.004, 0.001, 0, 0, 1, 0.999, 0.591, 0.999, 0, 1, 0.998, 0, 0.03, 0.006, 0.001, 0, 0.268, 0, 0, 0.107, 0.773, 0.933, 0.985, 0, 0, 0.023, 0, 0.997, 0, 0.001, 0.002, 0.997, 1, 0.999, 0, 1, 0, 0, 0.065, 1, 0, 1, 0, 0.834, 0, 0, 0, 0.626, 0.988, 0.912, 0.996, 0.928, 0, 0, 0, 1, 0, 1, 0.773, 0, 0, 1, 0, 1, 0, 0.003, 0, 0.118, 0.001, 1, 0, 0, 0, 0.926, 0.038, 0.631, 0, 0, 0, 0, 0, 0.201, 0.286, 0.994, 0.996, 0.002, 0, 0, 0.999, 0, 0, 0.001, 1, 0.005, 0, 1, 0, 0.009, 0.961, 0.009, 0, 0.036, 0, 0.999, 1, 0, 1, 1, 0, 0.002, 0, 0, 0.025, 0, 0.024, 0, 0.999, 0.004, 0.855, 1, 0, 0.604, 0, 0.028, 0.484, 0.997, 0.062, 0.909, 0.05, 0, 0.001, 0.001, 0, 0, 0, 1, 0.001, 0, 0, 0.002, 0.003, 1, 0.398, 0, 1, 0.002, 0, 0, 0, 0.001, 0, 0, 0.576, 0.001, 0.138, 0.002, 0.004, 0.022, 0, 0.002, 0.533, 0, 0, 1, 0.004, 0.007, 1, 1, 0, 0, 0.993, 0.999, 0.996, 0.147, 0, 0, 0, 0, 0, 0.001, 0, 1, 1, 0.005, 0.606, 0.001, 0.987, 0.026, 0, 0.754, 0.985, 0.998, 0.008, 0.998, 1, 0, 0, 0.076, 0.999, 0.004, 0, 0, 0.019, 0.988, 0, 0, 0, 0, 0.997, 0, 1, 0.998, 0.417, 0.001, 0, 0, 0.162, 0.018, 0.995, 1, 0.998, 1, 0.987, 0.956, 0.002, 0, 0.004, 0, 0.087, 0.999, 1, 0.03, 0.001, 0, 0, 0, 1, 0, 0.001, 0, 0.021, 0.999, 1, 0, 0.82, 0.999, 0, 0.002, 1, 0, 0.005, 0, 0, 0.999, 0, 0, 0.001, 0.057, 0.995, 1, 0, 0, 0, 0, 0, 0.002, 0, 0.001, 1, 0, 0.925, 0.003, 0, 0.009, 0, 0, 0, 1, 1, 0.971, 0.035, 0, 0.104, 0, 0, 0.329, 0.001, 1, 0, 0, 0.024, 0.999, 0, 0.05, 0, 0.067, 0.016, 0.998, 1, 0.298, 0, 0.87, 0.998, 0.001, 0.021, 0.999, 0.012, 0.001, 1, 0.247, 1, 1, 0.233, 0, 0.999, 1, 0, 0, 0, 0, 0.001, 1, 0.995, 1, 0.975, 0.007, 0, 0.34, 0.157, 0.998, 0, 0, 0, 0, 1, 0, 0.325, 0, 0.011, 0.246, 0.895, 0, 0.005, 1, 0.12, 0.037, 0, 0.998, 0.992, 0.834, 1, 0.273, 0, 0.998, 0, 0.999, 1, 0.001, 0, 0.001, 0.987, 0.446, 1, 0, 0.989, 0.964, 0.996, 0, 0, 0, 1, 0, 0, 0.998, 0, 1, 0, 0, 0.008, 0, 0, 0.001, 0.001, 0.002, 1, 0.001, 0, 1, 0.008, 0, 0, 1, 0, 0.986, 0.008, 0.006, 0, 0.028, 1, 0, 0, 0, 0.991, 0.128, 1, 0, 0, 0.002, 0.98, 1, 0.004, 1, 0.008, 0.001, 0.996, 0, 0, 0.006, 0, 0, 0, 1, 1, 0.999, 0, 0, 0.001, 0, 0, 0, 0, 0, 1, 0, 0.004, 0.002, 0, 1, 0.037, 1, 0.005, 1, 0.068, 0.001, 0.002, 0, 1, 0.981, 1, 0.009, 0, 0, 0.002, 0.925, 0.998, 1, 0.042, 0, 0.114, 0, 0, 0.725, 0.003, 0.027, 0, 0.989, 1, 0.002, 0, 0.016, 0, 0.009, 0, 0, 0.001, 0, 1, 0.189, 0.006, 0.014)
)

gaussian_sample_with_probs <- gaussian_sample_with_probs %>%
    mutate(prob_cluster2 = 1-prob_cluster1)
glimpse(gaussian_sample_with_probs)


# Estimation of the means
means_estimates <- gaussian_sample_with_probs %>% 
    summarise(mean_cluster1= sum(x*prob_cluster1)/sum(prob_cluster1),
              mean_cluster2 = sum(x*prob_cluster2)/sum(prob_cluster2)
              )
means_estimates

# Estimation of the proportions
props_estimates <- gaussian_sample_with_probs %>% 
    summarise(props_cluster1 = mean(prob_cluster1),
              props_cluster2 = mean(prob_cluster2)
              )
props_estimates

# Transform to a vector
means_estimates <- as.numeric(means_estimates)

# Plot histogram with means estimates
ggplot(gaussian_sample_with_probs) + geom_histogram(aes(x = x), bins = 100) +
    geom_vline(xintercept = means_estimates)


gaussian_sample <- data.frame(
    x=c(6.4, 5.9, 57.8, 52.6, 54.3, 52.3, 4.4, 49.1, -4, 12.7, 19.8, 51.8, 35.4, 17.1, 38.8, 44.1, 45.6, 7.9, 57.7, 51.1, 14.1, 36.6, 51.6, 4.1, -1.8, 55.1, 52.4, 54.4, 47.9, 36.6, 53.9, 15, 68.8, 8.3, 40.8, 39.3, 37.1, 12.7, 54.6, 34.1, 24.9, 58.5, 50.8, 48.6, 60, 52.1, 61.5, 6.9, 63, 63.5, 54.1, 37.7, 52.6, 49.1, 53.7, 13.4, 23.6, 45.5, 33.4, 46.4, 46.6, 56.1, 37.8, 44.1, 62.4, 12, 54.4, 31.6, -1, 9.4, 16, 53.4, 71.1, 8.9, 64.4, 55.9, 50.5, 57.2, 45.9, 18.5, 53.9, 12.5, 12.2, 1.5, 0.3, 40.1, 13.9, 53.2, 12.1, 57.2, 2.3, -2.6, 2.7, 59.6, 3, 10.3, 66.9, 57.3, 57.6, 9.1, 43.8, 51.1, 7.7, 13.4, 46.3, 57.5, 0.2, 1.9, 43.8, 53.9, 9.3, 45.5, 15.4, -3.2, -1.2, 40.5, 1.9, 14.5, -2, 3.4, 54.1, 2.9, 58.2, 49.5, 49.1, 60.2, 45.3, 59.7, 38, 22.4, 42.6, 53.6, 7.3, 43.9, 2.8, 66.5, 56.5, 44.4, 53.5, 40.6, 57.1, 43.8, -3.1, 47.3, 42.5, 50.8, -12, -12, 15.2, 43.8, 57.3, 32.2, 61.1, 15.1, 5.8, 24.7, 51.5, 7.7, -5.1, 63.1, 50.1, 39.9, 38.7, -5.2, 50.3, 49.1, 58.1, 31.3, 54.6, 39.1, 4.4, 60.5, 45.6, 59.7, 39.5, 60.6, 42.8, 49.5, 12.9, 47.2, 50, 11.4, 50.9, 57.3, 46.7, 35.6, 38.8, 56, -5.7, 50.5, 21.2, 45.9, 60.7, 22.1, 46.7, 12.5, 55.2, 48.4, 36.6, 54, 47, 50.3, 51.7, 11, 56, 42.4, 61.8, 45.6, 60.5, 40.6, 8.8, 21, 5.6, 68.2, 21.3, 11.5, 47.2, 26.4, 35.8, 25.4, 19.6, 56, 9.1, 63.4, 48.5, 3.2, 57.1, 52.7, 11.3, 16.3, 49, 46.5, 12.4, 9.6, 45.5, 55.3, 72.9, 8.1, -3.8, 53.8, 34.1, 45.7, 56.3, 44, 23.4, 57.2, 0.5, 33.2, 63.4, 37.3, 57.3, 52.7, 9.7, 51.9, 39.4, 63.7, 23.3, 39.9, -0.5, 41.6, 11.3, 48, 38.2, 54.2, 41.3, 30.6, 55.2, 48.9, 34.4, 16.2, 45.7, 10.1, 42.7, 12.2, 39.5, 14.1, 64.9, 53.1, 50.4, 47, 58.5, 50.8, 43.9, 56.8, 12.6, 44.5, 54.6, 8.9, 15.5, 50.2, 4.8, 52.8, 14.4, 33.7, 5.4, -0.2, 19.8, 51, 59.4, -8.2, 10.4, 47.8, 31.2, 41.4, 9.4, -3.2, 21.1, 44.7, 22.9, 11.5, 49.6, 26.7, 11.5, 35.2, 9.4, 44.8, 63.1, 8.5, 21, 30.9, 16.1, 54.4, 53.4, 9.7, 49.8, 45.6, -3, 53, 43.4, 43.4, 43.9, 56.6, 33.5, 55.1, 54.4, 62.8, 37.9, 35.1, 8.6, 7.1, 46.1, 6.1, 27, -9.9, 6.4, 44.6, 49, 46, 42.4, 9.5, 47.1, 51.3, -4.7, 14, 64.8, 38, 33.6, -0.4, 53.5, 40.3, 47.2, 58.5, 45.4, 2.5, 52.9, 47.4, 56.1, 17.7, 3.9, 30.7, 44.6, 42.4, 55.4, 47.1, 11.5, 50.7, 47.6, 11.3, 45.1, 44.2, 46.6, 36.9, 47.4, 54.6, -2, 50.7, 63.6, 58.9, 7.6, -3.1, 31.1, 44.9, 55.7, 16.6, 64.3, 27.1, 23, 48.7, -0.8, 23.6, 72.8, 11.9, 57.3, 25.4, 47.1, 9.4, 57.6, 39.6, 25.3, 31.2, 52.4, 51.1, 1.6, 76.5, 50.7, 34.2, 7.6, 25.4, 11.7, 53.5, 17.5, 53.7, 61.2, 49.9, 48.8, 40.8, 61.2, 16.4, 48.6, 7.5, -2, 64.2, 26.2, 11.2, 3.2, -4.3, 37.9, 47.7, 26.3, 58, 66.9, 59.1, 35.8, 14.2, 53, 60.3, 63.3, 53.6, 47.6, 57.1, 37, 47.6, 61.6, 52.7, 0.8, 50.5, 48.1, -3.4, 53.6, 35.7, 49.8, 2.7, 59.9, 36.5, 63.6, 53.3, 3.8, 20.2, 19.7, 20.7, 45.6, 39.8, 37.2, 38.6, 12.4, 56.3, 59.6, 10.5, 11, -6.8, 58.8, 49.5, -3.6, 51.1, 53.1, 46, 57.9, 15.2, -2.3, 22.9, 32.8, 37.6, 52, 77.5, 2.2, 9.5, 40.4, 48.5, 27.2, 37.4)
)
str(gaussian_sample)


# Create data frame with probabilities
gaussian_sample_with_probs <- gaussian_sample %>% 
    mutate(prob_from_cluster1 = 0.35 * dnorm(x, mean = 10, sd = 10),
           prob_from_cluster2 = 0.65 * dnorm(x, mean = 50, sd = 10),
           prob_cluster1 = prob_from_cluster1/(prob_from_cluster1 + prob_from_cluster2),
           prob_cluster2 = prob_from_cluster2/(prob_from_cluster1 + prob_from_cluster2)) %>%
    select(x, prob_cluster1, prob_cluster2) 
head(gaussian_sample_with_probs)


expectation <- function(data, means, proportions, sds){
  # Estimate the probabilities
  exp_data <- data %>% 
      mutate(prob_from_cluster1 = proportions[1] * dnorm(x, mean = means[1], sd = sds[1]),
             prob_from_cluster2 = proportions[2] * dnorm(x, mean = means[2], sd = sds[2]),
             prob_cluster1 = prob_from_cluster1/(prob_from_cluster1 + prob_from_cluster2),
             prob_cluster2 = prob_from_cluster2/(prob_from_cluster1 + prob_from_cluster2)) %>%
      select(x, prob_cluster1, prob_cluster2)
    # Return data with probabilities
  return(exp_data)
}

maximization <- function(data_with_probs){
    means_estimates <- data_with_probs %>%
        summarise(mean_1 = sum(x * prob_cluster1) / sum(prob_cluster1),
                  mean_2 = sum(x * prob_cluster2) / sum(prob_cluster2)
                  ) %>% 
        as.numeric()
    props_estimates <- data_with_probs %>% 
        summarise(proportion_1 = mean(prob_cluster1), proportion_2 = 1 - proportion_1) %>% 
        as.numeric()
    list(means_estimates, props_estimates)   
}


means_init <- c(0, 100)
props_init <- c(0.5, 0.5)

# Iterative process
for(i in 1:10){
    new_values <- maximization(expectation(gaussian_sample, means_init, props_init, c(10, 10)))
    means_init <- new_values[[1]]
    props_init <- new_values[[2]]
    cat(c(i, means_init, props_init), "\n")
}


fun_gaussian <- function(x, mean, proportion){
    proportion * dnorm(x, mean, sd = 10)
}

means_iter10 <- means_init
props_iter10 <- props_init

gaussian_sample %>% ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 200) +
    stat_function(geom = "line", fun = fun_gaussian, 
                  args = list(mean = means_iter10[1], proportion = props_iter10[1])
                  ) +
    stat_function(geom = "line", fun = fun_gaussian,
                  args = list(mean = means_iter10[2], proportion = props_iter10[2])
                  )

```
  
  
  
***
  
Chapter 3 - Mixture of Gaussians with flexmix  
  
Univariate Gaussian Mixture Models:  
  
* Gaussian mixture models are formed by Gaussian distributions, which can have many potential parameters  
	* Simplest version is the univariate Gaussian, such as the BMI dataset with no labels for gender  
* Example of using the gender dataset for segmenting assuming that the data are not labeled by gender  
	* Weight should be a Gaussian - continuous, not linked to integer values, etc.  
    * Histogram looks like two Gaussians, so begin with the assumption of 2 univariate Gaussians with a resulting 2 segments  
    * Parameters can initially be estimated with a baseline mean, sd, and proportion (6 total parameters)  
    * The EM algorithm, implemented in flexmix, can then help to simplify the calculations  
  
Univariate Gaussian Mixture Models with flexmix:  
  
* Can begin by checking what the most suitable distributions might be  
	* gender %>% ggplot(aes(x = Weight)) + geom_histogram(bins = 100)  
    * Univariate Gaussian with 2 clusters  
* Can use the flexmix::flexmix(formula, data, k, models, control, ...) to make the estimates  
	* formula - describes the model to be fit (often variable ~ 1)  
    * data - data frame  
    * k - number of clusters  
    * models - distribution to be considered, such as FLXMCnorm1 for the univariate Gaussian  
    * control - maximum iterations and tolerance  
* Example of fitting to a dataset using flexmix  
	* fit_mixture <- flexmix(Weight ~ 1, data = gender, k = 2, model = FLXMCnorm1(), control = list(tol = 1e-15, verbose = 1, iter = 1e4))  
    * proportions <- prior(fit_mixture)  # proportions from the model  
    * parameters(fit_mixture)  # all the parameters  
    * comp_1 <- parameters(fit_mixture, component = 1)  # just the parameters for component 1  
    * posterior(fit_mixture) %>% head()  # probability of belonging to each cluster by observation  
    * posterior(fit_mixture) %>% head()  # assignment to the cluster with maximum probability  
    * table(gender$Gender, clusters(fit_mixture))  
  
Bivariate Gaussian Mixture Models with flexmix:  
  
* May want to use additional variables to improve the clustering - for example, using both height and weight from the gender dataset  
	* Bivariate Gaussian with 2 clusters, and needing to estimate additional means and standard deviations (which need to also be between the variables)  
* The flexmix library implements the Bivariate Gaussian distribution, which is conceptually like  
	* There would be two means - variable 1 and variable 2  
    * There would be a 2x2 covariance matrix - sd1, sd2, cov(1, 2), cov(1, 2) - really, three terms since the off-diagonals will be equal  
  
Bivariate Gaussian Mixture Models with flexmix:  
  
* Example of a covariance matrix without cross-terms  
	* fit_without_cov <- flexmix(cbind(Weight, BMI) ~ 1, k = 2, data = gender, model = FLXMCmvnorm(diag = TRUE), control = list(tolerance = 1e-15, iter.max = 1000))  # cbind() is because there are two variables; diag=TRUE means no covariance  
    * proportions <- prior(fit_without_cov)  
    * parameters(fit_without_cov)  
    * comp_1 <- parameters(fit_without_corr, component=1)  
    * comp_2 <- parameters(fit_without_corr, component=2)  
    * mean_comp_1 <- comp_1[1:2]  
    * mean_comp_2 <- comp_2[1:2]  
* Can then visualize the resulting clusters  
	* library(ellipse)  
    * ellipse_comp_1 <- ellipse(x = covariance_comp_1, centre = mean_comp_1, npoints = nrow(gender))  
    * ellipse_comp_2 <- ellipse(x = covariance_comp_2, centre = mean_comp_2, npoints = nrow(gender))  
    * gender %>%  
    *     ggplot(aes(x = Weight, y = BMI)) + geom_point() +  
    *         geom_path(data = data.frame(ellipse_comp_1), aes(x=x,y=y), col = "red") +  
    *         geom_path(data = data.frame(ellipse_comp_2), aes(x=x,y=y), col = "blue"  
* Need to include joint variability to improve the modeling  
	* fit_with_cov <- flexmix(cbind(Weight, BMI) ~ 1, k = 2, data = gender, model = FLXMCmvnorm(diag = FALSE), control = list(tolerance = 1e-15, iter.max = 1000))  # cbind() is because there are two variables; diag=FALSE means covariance  
  
Example code includes:  
```{r}

xExample <- c(7.3, 58.7, 9.7, 16.9, 6.3, 35.1, 33.5, 61.3, 28.3, 24.3, 58.6, 13.1, 58.7, 34, 29.1, 46.4, 54.6, 5.9, 30.6, 27.9, 27.5, -5.3, 37.6, 9.1, 44.5, 57.5, 30.5, 5, 51.9, 33.6, 37.4, 28.8, 47.9, 5.4, 64.1, 45.1, 41, 36.3, 28.2, 33.8, 9.8, 57.4, 48.4, 58.3, 27.7, 38.4, 36.4, 66.9, 30.7, 34.3, 25.9, 48.5, 52, 0.3, 45.3, 31.9, 21.6, 36.6, 29, 13.2, 41.5, 8.2, 46.6, 30.6, 48.6, 5.6, 39.3, 30.5, 34.2, 61.5, 4.2, 71.3, 42.5, 32.7, 54.4, 19.2, 13.3, 40.3, 72, 21.8, 49.5, 38.7, 9.6, 49.6, 32, 30.9, 28.6, 30.1, 29.8, 67.9, 60.8, 55, 34.6, 32.8, 11.9, 50.5, 32.1, 13.7, 48.6, 32.6, 9.1, 27.6, 35.6, 28.3, 15.1, 54.7, 30.8, 22.2, 27.5, 49.3, 56, 26.1, 57.2, 46.4, 50.3, 43.6, 51.8, 47.5, 15.5, 60.2, 63.6, 45.3, 14.1, 42.1, 31.4, 42.4, 61.7, 60.1, 27.7, 55.9, 3.3, 18.7, 58.1, 46, 14, 41.7, 28.9, 29.1, 56.9, 32.3, -0.8, 29.4, 27.3, 33.5, 39.1, 13.9, 28.7, 29.4, 10.3, 44.3, 57.1, 76, 49.4, 44.9, 23.2, 53.9, 33.6, 32.7, 30, 57, 63.6, 32.9, 8.6, 26.5, 26, 53.3, 40.8, 30.1, 10.5, 47.2, 30.2, 49.3, 52.4, 48.8, 51.4, 40.7, 33.8, 45.7, 28.1, 13.2, 28.4, 31.7, 30, 29.6, 49.5, 35, 62, 51.9, 39, 15.4, 59.1, 54.8, 9.2, 9.7, 35.4, 32.9, 31.3, 30.4, 64.4, 63.4, 32.9, 40.6, 37.5, 52.3, 35.3, 8.1, 6.4, 26.2, 29.2, 29.7, 27.8, 35.2, 34.1, 29.8, 49, 65.6, -1.1, 28.6, 33.7, 48.1, 45.7, 30.3, 32.7, 64.5, 29.8, 52.5, 48.4, 48.8, 26.4, 37.4, 33.2, 46.1, 29.5, -0.9, 49.8, 34.1, 48.9, 12.5, 36.6, 22.1, 57.3, 9.5, 9.4, 58.5, 50.2, 45.3, 25.3, 27.4, 4.5, 58.5, 63.4, 48.7, 42.6, 33, 47.9, 30.3, 54.9, 7.9, 50.2, 11.2, 59.7, 46.5, 57.5, 26.9, 28.5, 29.7, 52.5, 16.9, 29.8, 28.6, 31.2, 65.3, 1.7, 31.4, 52.5, 5.1, 66.1, 51.5, 9.5, 9.8, 41.6, 0.3, 10.4, 15.5, 34.8, 27.5, 43.6, 31.4, 46.3, 4.6, 45.8, 49.2, 10.7, 48.1, 7.3, 33.4, 10.7, 53.4, 28.9, 51.1, 52.4, 55.9, 56.8, 47.2, 46.8, 30.8, 60.3, 53.6, 30.9, 70.8, 11.2, 7.5, 55.8, 14.3, 25.8, 14.5, 30.9, 60.8, 26.8, 16.5, 31.4, 26.6, 10.6, 53.4, 33.1, 33.1, 46.3, 8.2, 56, 14.1, 25.5, 59.6, 61.9, 58.6, 63.1, 47.7, 30.5, 42.4, 56.2, 17, 13.4, 34.4, 1.1, 18.4, 63.9, 38.6, 15, 30.1, 23.9, 5.9, 53.8, 18.2, 22.7, 45.7, 29.2, 8.4, 52.5, 42, 28.7, 61.7, 35.4, 32.5, 5.5, 6.8, 60.1, 29.4, 31.5, 2.3, 28.3, 29.6, 34.9, 33.2, 28.9, 33.9, 51, 35.4, 52.3, 60, 27.1, 24.7, 57.7, 32.7, 52.5, 66.3, 37.8, 46.3, 38.1, 30.6, 55.6, 44.9, 28.4, 28.9, 19, 7.7, 9.4, 36, 49.9, 42.2, 28.2, 11.5, 52.4, 46.3, 52.4, 27.4, 15.6, 62.3, 51.7, 41.6, 6.2, 10.5, 14.7, 30.4, 23.9, 58.7, 36.1, 47.6, 31.2, 29.1, 60.1, 18, 30, 56.5, 42.7, 27.1, 45.5, 36.6, 46.4, 25.9, 15.4, 31.6, 3.3, 33.6, 63.3, 57.1, 32.3, 11.8, 32.9, 47.2, 31.2, 49.3, 61.7, 11.5, 9.7, 49.6, 45.7, 16.1, 27.4, 22.8, 8.5, 56.2, 26, 45.7, 29, 34.6, 29.4, 3.9, 45.7, 31.7, 52.6, 40.2, 35.5, 5.8, 56.4, 49.5, 30.6, 40.2, 20.8, 43.9, 32.1, 40.8, 45.6, 32.8, 7.4, 27.5, 29.4, 50.8, 43.9, 36.8, 5.5, 61.5, 41.5, 47.5, 13.9, 30.1, 67.3, 27.1, 50.8, 37.4, 28, 25, 37.1, 49.3, 25.3, 26.9, 34.9, 51.8, 33.9, 34.7, 44.2, 10.1, 71.3, 47.5, 23.4, 45.7, 49.4, 32.6, 6.9, 67.8, 56.8, 41.9, 50.7, 31.5, 55, 14.2, 34.8, 26.2, 25.8, 64, 63.8, 56.4, 42.1, 29.5, 49.4, 30.2, 16.2, 30, -0.2, 30.7, 29.6, 57, 41.5, 6.4, 9.7, 47.1, 19.4, 39.8)
xExample <- c(xExample, 32.9, 53.6, 8.4, 32.8, 63.1, 58.4, 7.5, 26, 41.8, 29, 36.9, 41.5, 39.5, 14.1, 27.4, 14.9, 48.4, 34.8, 72.8, 36.9, 27.8, 27.6, 6.1, 43.8, 36.9, 58.5, 55.1, 45.2, 2.6, 20.4, 59, 60.6, 57.7, 29.8, 60.2, 36.9, 29, 28, 46.5, 55, 29.6, 52.6, 38, 45.3, 5.7, 44.8, 35.3, 56.1, 30.3, 32.4, 56.9, 30.8, 44.8, 62.8, 46.1, 57.2, 50.5, 46.4, 37.6, 29.9, 8.6, 35.5, 47.4, 27.2, 36.4, 33.1, 29.4, 25.8, 46, 27.6, 45.7, 32.3, 12.8, 49.8, 13.7, 65.3, 48.5, 39.6, 4, 32.1, 49.6, 44, 74.5, 31, 52.6, 33.3, 56.8, 11.4, 33.7, 34.3, 25.8, 39.8, 7.3, 33.6, 7.9, 49.6, 52.6, 36.5, 43, 14.7, 43.5, 37, 50.8, 46.5, 46.9, 25.4, 32.7, 48.4, 40.3, 45.9, 51.3, 24, 48.3, 39.5, 21.2, 48.1, 56.9, 32.3, 10.2, 9.3, 40.3, 52.8, 34.5, 32.4, 30.1, 10.8, -3.8, 30.4, 58.2, 57.3, 48.9, 36.1, 46.2, 69, 67.8, 58.5, 41.9, 29.6, 51.7, 39.4, 50.8, 29.2, 56.1, 54.4, 17.2, 57.5, 54.1, 48.6, -0.9, 56.3, 27.7, 58.8, 57, 44.1, 6.3, 4.1, 35.9, 60.2, 44.1, 53.9, 33.3, 35.4, 32.1, 56, 56.8, 30.1, 43.1, 64.6, 27.7, 30.7, 53, 66, 29.1, 45, 12.3, 41.3, 54.7, 45.3, 13.3, 9.7, -2, 29.1, 29.5, 31.3, 29.2, 13.8, 26.7, 7.4, 36.8, 42.6, 54.7, 51.3, 42.6, 18, 34, 44.1, 53.6, 44.7, 28.9, 64.9, 60, 66.6, 32.9, 15.5, 37.6, 8.3, 28.5, 16.2, 39.7, 25.9, 8.8, 30.9, 9.9, 39.3, 66.4, 62.4, 53.8, 9.3, 44.7, 50.4, 57.8, 29, 50.1, 28.5, 62.9, 16.3, 54, 45.4, 60.6, 9, 7.7, 64.2, 54.4, 53.3, 45.5, 38, 5.2, 61.7, 10.8, 4.3, 24.8, 26.5, 32.2, 4.5, 49.3, 3.9, 39.6, 26.8, 36.3, 65.1, 59.6, 61.3, 30.1, 65.5, 55.8, 48.2, 49.8, 11.2, 64.2, 29, 44.6, 59.9, 12.6, 51.8, 14.5, 28.8, 49.8, 30.4, 42.7, 2.8, 31.1, 29.2, 27.4, 49.9, 28.2, 59.5, 28.7, 9.4, 30.2, 33.3, 30, 26, 65.1, 55.9, 30.5, 61.1, 50.3, 31.3, 58.2, 41.3, 33.4, 14.8, 51.2, 40.8, 34.1, 33.7, 29.4, 56, 26.4, 30.7, 55.1, 49.7, 37.7, 56.9, 38.5, 28.8, 50.3, 45.7, 13.2, 32.8, 30.5, 30.6, 61.5, 57.7, 33.6, 24.6, 53.9, 36.1, 37.4, 55.5, 27.4, 44.2, 15.4, 56.3, 28.1, 28.8, 67.6, 17.7, 48.5, 57.5, 33.7, 12.9, 19.5, 30.6, 56.8, 75.4, 26, 32.3, 28.3, 10.7, 9, 66.5, 51.6, 30.2, 46, 44.1, 53, 33.9, 28.4, 53.1, 42.3, 55.2, 42.4, 9.4, 36.3, 26.6, 41.2, 33, 42.1, 27, 25.4, 53.8, 56.7, 22.2, 29.5, 30.9, 9.3, 30.4, 48.1, 30.9, 28.4, 38.6, 28.8, 52, 16.5, 64.3, 56.1, 51.4, 50.2, 30.1, 67.3, 62.3, 12.9, 27.9, 38.9, 29.3, 17.4, 30, 62.5, 40.5, 48, 31.9, 54.7, 27.4, 28.2, 46.6, 14, 61.9, 59.4, 65.4, 30.2, 28.9, 35.4, 55.8, 51.4, 47.8, 34, 56.2, 26.5, 30.2, 8.4, 10.9, 63.9, 41.9, 31.3, 52.8, 36, 45.4, -2, 57.3, 80.3, 41, 13.8, 31.9, 33.8, 48.5, 16.7, 29.5, 6.7, 42.1, 32.2, 45.7, 18.9, 30.5, 30.9, 40.2, 14.6, 41.2, 27, 6.1, 34.9, 57.5, 30.1, 56.6, 62.4, 11.5, 25.7, 14.8, 28.2, 43.5, 37.7, 32.1, 44.4, 56.2, 7.6, 29.4, 63.4, 53, 14.6, 50.1, 62.6, 29.3, 33.5, 52.7)
mix_assign <- c(1, 2, 1, 1, 1, 2, 2, 2, 3, 3, 2, 1, 2, 2, 3, 2, 2, 1, 3, 3, 3, 1, 2, 1, 2, 2, 3, 1, 2, 3, 2, 3, 2, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 1, 2, 3, 1, 3, 3, 1, 2, 1, 2, 3, 2, 1, 2, 3, 3, 2, 1, 2, 2, 3, 2, 1, 1, 2, 2, 3, 2, 2, 1, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 1, 2, 3, 1, 2, 2, 1, 3, 2, 3, 1, 2, 3, 1, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 3, 2, 2, 2, 3, 2, 1, 1, 2, 2, 1, 2, 3, 3, 2, 3, 1, 3, 3, 2, 2, 1, 3, 3, 1, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 2, 2, 3, 1, 3, 3, 2, 2, 3, 1, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 1, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 1, 1, 3, 3, 3, 3, 3, 3, 2, 2, 2, 1, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 1, 2, 3, 2, 1, 3, 3, 2, 1, 1, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 1, 2, 2, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 1, 3, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 3, 3, 2, 3, 2, 1, 2, 2, 1, 2, 1, 3, 1, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 1, 1, 2, 1, 3, 1, 3, 2, 3, 1, 3, 3, 1, 2, 3, 2, 2, 1, 2, 1, 3, 2, 2, 2, 2, 2, 3, 2, 2, 1, 1, 3, 1, 1, 2, 3, 1, 3, 3, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 2, 3, 1, 1, 2, 3, 2, 1, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 1, 1, 1, 2, 2, 2, 3, 1, 2, 2, 2, 3, 1, 2, 2, 2, 1, 1, 1, 3, 3, 2, 3, 2, 3, 2, 2, 1, 3, 2, 2, 3, 2, 3, 2, 3, 1, 3, 1, 3, 2, 2, 3, 1, 3, 2, 3, 2, 2, 1, 1, 2, 2, 1, 3, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 2, 2, 2, 3, 1, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 2, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 1, 2, 2, 3, 2, 2, 3, 1, 2, 2, 2, 2, 3, 2, 1, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 1, 3, 1, 3, 3, 2, 2, 1, 1, 2, 2, 2, 3, 2, 1, 3, 2, 2, 1, 3, 2, 3, 2, 2, 2, 1)
mix_assign <- c(mix_assign, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 1, 2, 1, 2, 2, 2, 1, 3, 2, 2, 2, 3, 2, 3, 2, 1, 3, 2, 3, 2, 1, 2, 1, 2, 2, 3, 2, 1, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 3, 1, 1, 2, 2, 3, 3, 3, 1, 1, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 1, 2, 3, 2, 2, 2, 1, 1, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 1, 2, 2, 2, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 3, 2, 2, 2, 3, 1, 2, 1, 3, 1, 2, 3, 1, 3, 1, 3, 2, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 3, 3, 3, 1, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 3, 2, 2, 1, 2, 1, 3, 2, 3, 2, 1, 3, 3, 3, 2, 3, 2, 3, 1, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 1, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 1, 2, 3, 3, 2, 1, 2, 2, 2, 1, 1, 3, 2, 2, 3, 3, 3, 1, 1, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 2, 1, 3, 2, 2, 1, 3, 2, 2, 2, 3, 2, 3, 3, 2, 1, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 1, 1, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 1, 3, 3, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 1, 3, 2, 3, 2, 2, 1, 3, 1, 3, 2, 2, 3, 2, 2, 1, 3, 2, 2, 2, 2, 2, 3, 3, 2)

mix_example <- data.frame(x=xExample, assignment=mix_assign)
str(mix_example)

library(flexmix)
set.seed(1515)

# If wanting verbose output
# control = list(tolerance = 1e-15, verbose = 1, iter = 1e4)
fit_mix_example <- flexmix(x ~ 1, data = mix_example, k = 3, model = FLXMCnorm1(), 
                           control = list(tolerance = 1e-15, iter = 1e4)
                           )

proportions <- prior(fit_mix_example)
comp_1 <- parameters(fit_mix_example, component = 1)
comp_2 <- parameters(fit_mix_example, component = 2)
comp_3 <- parameters(fit_mix_example, component = 3)


fun_prop <- function(x, mean, sd, proportion){
    proportion * dnorm(x = x, mean = mean, sd = sd)
}

ggplot(mix_example) + 
    geom_histogram(aes(x = x, y = ..density..)) + 
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_1[1], sd = comp_1[2], proportion = proportions[1])
                  ) +
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_2[1], sd = comp_2[2], proportion = proportions[2])
                  ) +
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_3[1], sd = comp_3[2], proportion = proportions[3])
                  )


# Explore the first assignments
head(clusters(fit_mix_example))

# Explore the first real labels
head(mix_example$assignment)

# Create frequency table
table(mix_example$assignment, clusters(fit_mix_example))


genderData <- readr::read_csv("./RInputFiles/gender.csv")
str(genderData)

set.seed(1313)
fit_with_covariance <- flexmix(cbind(Weight, BMI) ~ 1, data = genderData, k = 2, 
                               model = FLXMCmvnorm(diag = FALSE), 
                               control = list(tolerance = 1e-15, iter.max = 1000)
                               )

# Get the parameters
comp_1 <- parameters(fit_with_covariance, component = 1)
comp_2 <- parameters(fit_with_covariance, component = 2)

# The means
mean_comp_1 <- comp_1[1:2]
mean_comp_1
mean_comp_2 <- comp_2[1:2]
mean_comp_2

# The covariance matrices
covariance_comp_1 <- matrix(comp_1[3:6], nrow = 2)
covariance_comp_1
covariance_comp_2 <- matrix(comp_2[3:6], nrow = 2)
covariance_comp_2


# Create ellipse curve 1
ellipse_comp_1 <- ellipse::ellipse(x = covariance_comp_1, centre = mean_comp_1, npoints = nrow(genderData))
head(ellipse_comp_1)

# Create ellipse curve 2
ellipse_comp_2 <- ellipse::ellipse(x = covariance_comp_2, centre = mean_comp_2, npoints = nrow(genderData))
head(ellipse_comp_2)


# Plot the ellipses
genderData %>% 
    ggplot(aes(x = Weight, y = BMI)) + geom_point()+
    geom_path(data = data.frame(ellipse_comp_1), aes(x=x,y=y), col = "red") +
    geom_path(data = data.frame(ellipse_comp_2), aes(x=x,y=y), col = "blue")

# Check the assignments
table(genderData$Gender, clusters(fit_with_covariance))

```
  
  
  
***
  
Chapter 4 - Mixture Models Beyond Gaussians  
  
Bernoulli Mixture Models:  
  
* Example of the handwritten images dataset, with images of 3 and 6  
	* The Bernoulli distribution is appropriate for a binary outcome when you do not want a continuous prediction but rather a probability of success  
    * p <- 0.7  
    * bernoulli <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p, p))  
* Every pixel in an image can be represented by a Bernoulli with outcomes being either black (filled) or white (empty), with a 16x16 considered to be a 256-length vector; example for a 3-pixel image  
	* p1 <- 0.7; p2 <- 0.5; p3 <- 0.4  
    * bernoulli_1 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p1, p1))  
    * bernoulli_2 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p2, p2))  
    * bernoulli_3 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p3, p3))  
    * multi_bernoulli <- cbind(bernoulli_1, bernoulli_2, bernoulli_3)  
    * p_vector <- c(p1, p2, p3)  
  
Bernoulli Mixture Models with flexmix:  
  
* Example of using flexmix to predict whether a digit is a 3 or a 6 in the 16x16 data that is stored in digits  
	* digits_sample <- as.matrix(digits)  # 320 x 256 (flattened array of pixels per image)  
    * show_digit(digits_sample[320,])  
* For fitting a Bernoulli mixture model with two segments, can proceed similar to before  
	* bernoulli_mix_model <- flexmix(digits_sample~1, k=2, model=FLXMCmvbinary(), control = list(tolerance = 1e-15, iter.max = 1000))  # digits_sample is the matrix, with each row as an image  
    * prior(bernoulli_mix_model)  # proportions in each cluster  
    * param_comp1 <- parameters(bernoulli_mix_model, component = 1)  
    * param_comp2 <- parameters(bernoulli_mix_model, component = 2)  
    * show_digit(param_comp1)  # will show probabilities of pixelation given segment is a 3  
  
Poisson Mixture Models:  
  
* Can use Chicago crimes data with a Poisson model, since the goal is a count of crimes by type by community, with segments by level of crime danger in the community  
	* lambda_1 <- 100; lambda_2 <- 200; lambda_3 <- 300  
    * poisson_1 <- rpois(n = 100, lambda = lambda_1)  
    * poisson_2 <- rpois(n = 100, lambda = lambda_2)  
    * poisson_3 <- rpois(n = 100, lambda = lambda_3)  
    * multi_poisson <- cbind(poisson_1, poisson_2, poisson_3)  
* Can extend the general concept to a crime dataset with 13 columns  
	* Multi-Poisson distribution  
    * Try from 1-15 clusters, and minimize BIC criteria  
    * Parameters include each lambda for each multi-Poisson distribution  
  
Poisson Mixture Models with flexmix:  
  
* Example of solving the Poisson mixture model using flexmix, with indeterminate clusters (try 1-15 and minimize BIC), and with parameters of lambdas by cluster and proportions by cluster  
	* crimes_matrix <- as.matrix(crimes[,-1])  # do not include the community names  
    * poisson_mix_model <- stepFlexmix(crimes_matrix ~ 1, k = 1:15, nrep = 5, model = FLXMCmvpois(), control = list(tolerance = 1e-15, iter = 1000))  # stepFlexMix is how to run 1:15  
    * best_fit <- getModel(poisson_mix_model, which = "BIC")  # can also use AIC or ICL  
    * prior(best_fit)  # proportions  
    * param_pmm <- data.frame(parameters(best_fit))  # parameters by cluster, converted to data frame  
    * param_pmm <- param_pmm %>% mutate(Type = colnames(crimes_matrix))  
    * head(param_pmm)  
    * param_pmm %>% gather(Components, Lambda, -Type) %>% ggplot(aes(x = Type, y = Lambda, fill = Type)) + geom_bar(stat = "identity") + facet_wrap(~ Components) + theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")  
    * crimes_c <- crimes %>% mutate(CLUSTER = factor(clusters(best_fit)))  
    * crimes_c %>% group_by(CLUSTER) %>% mutate(NUMBER = row_number()) %>% ggplot(aes(x = CLUSTER, y = NUMBER, col = CLUSTER)) + geom_text(aes(label = COMMUNITY), size = 2.3)+ theme(legend.position="none")  
  
Example code includes:  
```{r}

# Create the vector of probabilities
p_cluster_1 <- c(0.8, 0.8, 0.2, 0.9)

# Create the sample for each pixel
set.seed(18102308)
pixel_1 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[1], p_cluster_1[1]))
pixel_2 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[2], p_cluster_1[2]))
pixel_3 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[3], p_cluster_1[3]))
pixel_4 <- sample(c(0, 1), 100, replace = TRUE, prob = c(1-p_cluster_1[4], p_cluster_1[4]))

# Combine the samples
sample_cluster_1 <- cbind(pixel_1, pixel_2, pixel_3, pixel_4)

# Have a look to the sample
head(sample_cluster_1)


digitUse2 <- rowSums(digitKey[, c(1, 3, 10)]) == 1
digits_sample_2 <- digitData[digitUse2, ]
dim(digits_sample_2)


# transform into matrix
digits_sample_2 <- as.matrix(digits_sample_2)

# dimension
dim(digits_sample_2)

# look to the first observation
show_digit(digits_sample_2[1, ])

# look to the last observation
show_digit(digits_sample_2[nrow(digits_sample_2), ])


set.seed(1513)
# Fit Bernoulli mixture model
bernoulli_mix_model <- flexmix(digits_sample_2 ~ 1, k = 3, model = FLXMCmvbinary(), 
                               control = list(tolerance = 1e-15, iter.max = 1000)
                               )
prior(bernoulli_mix_model)

# Extract the parameters for each cluster
param_comp_1 <- parameters(bernoulli_mix_model, component = 1)
param_comp_2 <- parameters(bernoulli_mix_model, component = 2)
param_comp_3 <- parameters(bernoulli_mix_model, component = 3)

# Visualize the clusters
show_digit(param_comp_1)
show_digit(param_comp_2)
show_digit(param_comp_3)


set.seed(1541)

# Create the vector of lambdas
lambda_1 <- c(150, 300, 50)

# Create the sample of each crime
assault_1 <- rpois(n = 10, lambda = lambda_1[1])
robbery_1 <- rpois(n = 10, lambda = lambda_1[2])
battery_1 <- rpois(n = 10, lambda = lambda_1[3])

# Combine the results
cities_1 <- cbind(assault_1, robbery_1, battery_1)

# Check the sample
cities_1


crimes <- readr::read_csv("./RInputFiles/CoC_crimes.csv")
dim(crimes)
names(crimes) <- stringr::str_replace_all(stringr::str_to_lower(names(crimes)), " ", ".")

# Check with glimpse
glimpse(crimes)

# Transform into a matrix, without `community`
matrix_crimes <- crimes %>%
  select(-community) %>%  
  as.matrix()

# Check the first values
head(matrix_crimes)


set.seed(2017)
# Fit the Poisson mixture model
poisson_mm <- stepFlexmix(matrix_crimes ~ 1, k = 1:15, nrep = 5, model = FLXMCmvpois(), 
                          control = list(tolerance = 1e-15, iter.max = 1000)
                          )

# Select the model that minimize the BIC
best_poisson_mm <- getModel(poisson_mm, which = "BIC")

# Get the parameters into a data frame
params_lambdas <- data.frame(parameters(best_poisson_mm))

# Add the column with the type of crime
params_lambdas_crime <- params_lambdas %>% 
    mutate(crime = colnames(matrix_crimes))

# Plot the clusters with their lambdas
params_lambdas_crime %>% 
    gather(cluster, lambdas, -crime) %>% 
    ggplot(aes(x = crime, y = lambdas, fill = crime)) + 
    geom_bar(stat = "identity") +
    facet_wrap(~ cluster) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none")

# Add the cluster assignments
crimes_with_clusters <- crimes %>% 
    mutate(cluster = factor(clusters(best_poisson_mm)))

# Plot the clusters with the communities
crimes_with_clusters %>% 
    group_by(cluster) %>% 
    mutate(number = row_number()) %>% 
    ggplot(aes(x = cluster, y = number, col = cluster)) + 
    geom_text(aes(label = community), size = 2.3) +
    theme(legend.position="none")

```
  
  
  
***
  
### _Developing R Packages_  
  
Chapter 1 - The R Package Structure  
  
Introduction to Package Building:  
  
* R packages can include functions, data, documentation, vignettes, tests, and the like  
* At a minimum, the package must have 1) R Directory, 2) man Directory, 3) NAMESPACE file, and 4) DESCRIPTION file  
* Can use devtools to help with package building, as well as roxygen2  
	* create()  
    * document()  
    * check()  
    * build()  
    * test()  
* The devtools functions have descriptive names closely related to their assigned tasks  
	* devtools::create("simutils")  # avoid names that are already on CRAN  
  
Description and Namespace Files:  
  
* The DESCRIPTION contains background information such as author, package name, version, license, and the like  
* The NAMESPACE file will be edited based on entries made in other locations  
	* import() will bring in from other packages  
    * export() will export to the calling environment  
  
Optional Directories:  
  
* The most common optional directories include data, vignettes, tests, compiled code, and translations  
* Example for adding data to the package  
	* sim_dat <- data.frame( ID = 1:10, Value = sample(1:11, 10), Apples = sample(c(TRUE, FALSE), 10, replace = TRUE) )  
    * devtools::use_data(sim_dat, pkg = "simutils")  
* Example for adding vignettes to the package  
	* use_vignette("my_first_vignette", pkg = "simutils")  
* Best practices for structuring code - may NOT include subdirectories  
	* Group similar functions together in a file  
    * Do not create a small file for every function  
  
Example code includes:  
```{r eval=FALSE}

# Use the create function to set up your first package
devtools::create("./RPackages/datasummary")

# Take a look at the files and folders in your package
dir("./RPackages/datasummary")


# Create numeric_summary() function
numeric_summary <- function(x, na.rm) {
    # Include an error if x is not numeric
    if(!is.numeric(x)){
        stop("Data must be numeric")
    }
    
    # Create data frame
    data.frame( min = min(x, na.rm = na.rm),
                median = median(x, na.rm = na.rm),
                sd = sd(x, na.rm = na.rm),
                max = max(x, na.rm = na.rm))
}

data(airquality)

# Test numeric_summary() function
numeric_summary(airquality$Ozone, TRUE)


# What is in the R directory before adding a function?
dir("./RPackages/datasummary/R")

# Use the dump() function to write the numeric_summary function
dump("numeric_summary", file = "./RPackages/datasummary/R/numeric_summary.R")

# Verify that the file is in the correct directory
dir("./RPackages/datasummary/R")


# a package should not have the same name as an existing package and its name must only contain letters, numbers, or dots.


# What is in the package at the moment?
dir("./RPackages/datasummary")

# Add the weather data
data(Weather, package="mosaicData")
devtools::use_data(Weather, pkg = "./RPackages/datasummary")

# Add a vignette called "Generating Summaries with Data Summary"
devtools::use_vignette("Generating_Summaries_with_Data_Summary", pkg = "./RPackages/datasummary")

# What directories do you now have in your package now?
dir("./RPackages/datasummary")


data_summary <- function(x, na.rm = TRUE){
  num_data <- select_if(x, .predicate = is.numeric) 
  map_df(num_data, .f = numeric_summary, na.rm = TRUE, .id = "ID")
}

# Write the function to the R directory
dump("data_summary", file = "./RPackages/datasummary/R/data_summary.R")
dir("./RPackages/datasummary")

```
  
  
  
***
  
Chapter 2 - Documenting Packages  
  
Introduction to roxygen2:  
  
* Good documentation is key to the package - all functions, usages, outputs, etc.  
* The roxygen2 package allows for creating this type of documentation  
	* Included above the function, with each line starting with #'  
    * Paragraph 1 is the title (should be short)  
    * Paragraph 2 is the brief description (should be a single sentence)  
    * Paragraph 3 is the longer description (an actual paragraph of a few sentences)  
    * Paragraphs 4+ need to all start with @tag where tag might be param or author or import or return or export or examples or etc  
* Packages need to import other packages rather than calling them by way of library(package)  
  
How to export functions:  
  
* Exported functions become visible to the end-user and are core to package functionality (documented)  
	* Flagged with a tag of #' @export  
* Non-exported functions are not visible to the end-user but instead serve as utility functions within the package  
	* Flagged by having no tag of #' @export  
* Only the exported functions get loaded, so calling a non-exported function requires a triple colon from the namespace  
	* simutils:::sum_na(airquality$Ozone)  
  
Documenting other elements:  
  
* Can include examples using the @examples tag with the example included on the line below  
	* If you do not want an example to be run during checking, then the \dontrun{} tag can be used with the example being inside {} and allowed to be multiple lines  
* The return from the function is specified using the @return tag, and with the \code{} tag being   available to signal that this refers to one of the arguments/parameters  
* Can use the @author and @seealso tags as needed  
  
Documenting a package:  
  
* All roxygen headers need to be followed by some form of R code  
	* Can use a single line "_PACKAGE" for this  
* The minimum level of documentation is title, description, arguments, and exported (for exported functionsl only)  
	* Other components may be useful to the end-user  
* Can put a data frame from the current environment in to the package  
	* use_data(sim_dat, pkg = "simutils")  
* Can also use the roxygen formatting and then conclude with "sim_dat" to put this in with headers and descriptors  
* Can create the manual files (create and/or update) using  
	* document("simutils")  
  
Example code includes:  
```{r eval=FALSE}

#' Summary of Numeric Columns
#'
#' Generate specific summaries of numeric columns in a data frame
#' 
#' @param x A data frame. Non-numeric columns will be removed
#' @param na.rm A logical indicating whether missing values should be removed
#' @import purrr
#' @import dplyr
#' @importFrom tidyr gather
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Summary of Numeric Columns
#'
#' Generate specific summaries of numeric columns in a data frame
#' 
#' @param x A data frame. Non-numeric columns will be removed
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Data Summary for Numeric Columns
#'
#' Custom summaries of numeric data in a provided data frame
#'
#' @param x A data.frame containing at least one numeric column
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
#' @examples
#' data_summary(iris)
#' data_summary(airquality, na.rm = FALSE)
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}



# For code you use \code{text to format}
# To link to other functions you use \link[packageName]{functioName}, although note the package name is only required if the function is not in your package
# To include an unordered list you use \itemize{}. Inside the brakets you mark new items with \item followed by the item text.

#' Data Summary for Numeric Columns
#'
#' Custom summaries of numeric data in a provided data frame
#'
#' @param x A data.frame containing at least one numeric column
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
#' @examples
#' data_summary(iris)
#' data_summary(airquality, na.rm = FALSE)
#'
## Update the details for the return value
#' @return 
#' This function returns a \code{data.frame} including columns:
#' \itemize{
#'  \item ID
#'  \item min
#'  \item median
#'  \item sd
#'  \item max
#' }
#'
#' @export
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Summary of Numeric Columns
#' Generate specific summaries of numeric columns in a data frame
#'
#' @param x A data frame. Non-numeric columns will be removed
#' @param na.rm A logical indicating whether missing values should be removed
#' @import dplyr
#' @import purrr
#' @importFrom tidyr gather
#' @export
#' @examples
#' data_summary(iris)
#' data_summary(airquality, na.rm = FALSE)
#' 
#' @return This function returns a \code{data.frame} including columns: 
#' \itemize{
#'  \item ID
#'  \item min
#'  \item median
#'  \item sd
#'  \item max
#' }
#'
## Add in the author of the `data_summary()` function. 
#' @author My Name <myemail@example.com>
## Update the header to link to the `summary()` function (in the `base` package).
#' @seealso \link[base]{summary}
data_summary <- function(x, na.rm = TRUE){
  
  num_data <- select_if(x, .predicate = is.numeric) 
  
  map_df(num_data, .f = numeric_summary, na.rm = na.rm, .id = "ID")
  
}


#' Custom Data Summaries
#' 
#' Easily generate custom data frame summaries
#' 
#' @docType package
#' @name datasummary
_PACKAGE 


#' Random Weather Data
#'
#' A dataset containing randomly generated weather data.
#'
#' @format A data frame of 7 rows and 3 columns
#' \describe{
#'  \item{Day}{Numeric values giving day of the week, 1 = Monday, 7 = Sunday}
#'  \item{Temp}{Integer values giving temperature in degrees Celsius}
#'  \item{Weather}{Character values giving precipitation type, Sun if none}
#' }
#' @source Randomly generated data
weather


# Generate package documentation
document("datasummary")

# Examine the contents of the man directory
dir("datasummary/man")

# View the documentation for the data_summary function
help("data_summary")

# View the documentation for the weather dataset
help("weather")

```
  
  
  
***
  
Chapter 3 - Checking and Building R Packages  
  
Why check an R package?  
  
* There are many checks for a typical R package, including many that are mandatory for submission to CRAN  
	* Package can be installed  
    * Description information is correct  
    * Dependencies make sense  
    * No code syntax errors  
    * Documentation is complete  
    * Unit tests can be run  
    * Vignettes can be built  
* A system-level R tool can check the package for you  
	* check("simutils")  # default setting is the same as for CRAN  
    * Errors need to be fixed, while warnings are less problematic (still should be investigated)  
  
Errors, warnings, and notes:  
  
* Package dependencies will be checked; will error out if these are not available  
* Often have documentation issues; need to update function and argument names to match what has been updated in code  
* The examples are all run, and need to return with an OK status  
* If LaTEX is not installed, there will be documentation build errors  
  
Differences in package dependencies:  
  
* The dependencies from "Depends:" are loaded in to the search() path, though this is not always the recommended approach due to masking  
* The dependencies from "Imports:" are loaded by a namespace  
	* use_package("dplyr") ## adds to imports  
* The "Suggests:" are not required for running the package but may be helpful (for example, for running the vignettes)  
	* use_package("ggplot2", "suggests") ## adds to suggests  
  
Building packages with continuous integration:  
  
* Can create package as either a source file or as a binary  
	* build("simutils")  # default builds the source file  
    * build("simutils", binary = TRUE)  # builds the binary version, needed for compiled code  
* Continuous integration can help with package maintenance  
	* Automatically checks packages whenever code is changed  
    * Useful with version control  
    * use_travis("simutils")  
  
Example code includes:  
```{r eval=FALSE}

# Check your package
check("datasummary")


#' Numeric Summaries
#' Summarises numeric data and returns a data frame containing the minimum value, median, standard deviation, and maximum value.
#'
#' @param x a numeric vector containing the values to summarize.
#' @param na.rm a logical value indicating whether NA values should be stripped before the computation proceeds.
numeric_summary <- function(x, na.rm){

  if(!is.numeric(x)){
    stop("data must be numeric")
  }

  data.frame( min = min(x, na.rm = na.rm),
              median = median(x, na.rm = na.rm),
              sd = sd(x, na.rm = na.rm),
              max = max(x, na.rm = na.rm))
}


# The way in which you define variables in tidyverse package functions can cause confusion for the R CMD check, which sees column names and the name of your dataset, and flags them as "undefined global variables".
# To get around this, you can manually specify the data and its columns as a vector to utils::globalVariables(), by including a line of code similar to the following in your package-level documentation:
# utils::globalVariables(c("dataset_name", "col_name_1", "col_name_2"))
# This defines dataset_name, col_name_1, and col_name_2 as global variables, and now you shouldn't get the undefined global variables error.

#' datasummary: Custom Data Summaries
#'
#' Easily generate custom data frame summaries
#'
#' @docType package
#' @name datasummary
_PACKAGE

# Update this function call
utils::globalVariables(c("weather", "Temp"))


# Add dplyr as an imported dependency to the DESCRIPTION file
use_package("dplyr", pkg = "datasummary")

# Add purrr as an imported dependency to the DESCRIPTION file
use_package("purrr", pkg = "datasummary")

# Add tidyr as an imported dependency to the DESCRIPTION file
use_package("tidyr", pkg = "datasummary")


# Build the package
build("datasummary")

# Examine the contents of the current directory
dir("datasummary")

```
  
  
  
***
  
Chapter 4 - Adding Unit Tests to R Packages  
  
What are unit tests and why write them?  
  
* Unit tests can help with checking that future function behavior remains as expected  
	* Changes in functionality of supporting code  
    * Later versions of R  
    * Different operating systems  
    * Different underlying data  
* Can add unit tests to the package using roxygen2  
	* Call use_testthat to set up the test framework  
    * This creates a test directory in the package root directory  
    * Within the test directory, there is a script testthat.R which contains code to run the tests  
    * Within the test directory is a directory testthat where you save all of your test scripts  
* Can then create individual tests that begin with expect_*()  
	* library(testthat)  
    * my_vector <- c("First" = 1, "Second" = 2)  
    * expect_identical(my_vector, c("First" = 1, "Second" = 2))  # will pass since types and values are the same  
    * expect_identical(myvector, c(1, 2))  # Error: `vec1` not identical to c(1, 2). names for target but not for current  
    * expect_equal(my_vector, c("First" = 1L, "Second" = 2L))  # checks only the values and attributes, so this will pass even if vector is c(1, 2)  
    * expect_equal(my_vector, c(First = 1.1, Second = 2.1), tolerance = 0.1)  # include a tolerance for differences  
    * expect_equivalent(my_vector, c(1, 2))  # compare only the values, not the types or the attributes  
  
Testing errors and warnings:  
  
* Functions can give warnings or errors for many reasons, such as sqrt(-1)  
* The testthat contains functions for expecting warnings and errors  
	* expect_warning(sqrt(-1)) # passes  
    * expect_error(sqrt("foo")) # passes  
    * expect_error(sqrt(-1))  # fails, since this returns a warning rather than an error  
    * expect_error(sqrt("foo"), "non-numeric argument to mathematical function")  # requires that the error thrown be exactly the string in the second argument  
    * expect_error(sqrt("foo"), "NaNs produced")  # fails, since that is not the message  
  
Testing specific output and non-exported functions:  
  
* Printed messages and plots are side effects, which can be tested using expect_output  
	* expect_output(str(airquality), "41 36 12 18 NA 28 23 19 8 NA")  # passes, since this is part of the output  
    * expect_output(str(airquality), "air")  # fails, since the word "air" is not in the output  
    * expect_output_file(str(airquality), "airq.txt", update = TRUE)  # creates the file, which will throw an error the first time that it is run  
    * expect_output_file(str(airquality), "airq.txt")  # next time run, it will compare the output to the file  
* Can use the library for headers from the roxygen headers  
	* expect_equivalent(na_counter(airquality), c(37, 7, 0, 0, 0, 0))  
    * expect_equal(simutils:::sum_na(airquality$Ozone), 37)  # three colons signal that this is a non-exported function  
  
Grouping and running tests:  
  
* Can organize and group tests for easier future use - can add many tests in a single test_that() call  
	* test_that("na_counter correctly counts NA values", { test_matrix = matrix(c(NA, 1, 4, NA, 5, 6), nrow = 2) ; air_expected = c(Ozone = 37, Solar.R = 7, Wind = 0, Temp = 0, Month = 0, Day = 0) ; mat_expected = c(V1 = 1, V2 = 1, V3 = 0) ;  expect_equal(na_counter(airquality), air_expected) ; expect_equal(na_counter(test_matrix), mat_expected) })  
    * context("na_counter checks")  # will then give the appropriate information about what failed in the checks  
* Need to fix any failed tests, either by fixing the code or by fixing the test  
  
Wrap up:  
  
* Sturcture of R packages, including NAMESPACE and DESCRIPTION  
* Documenting R packages and including roxygen2 for examples/documentation  
* Building integrated packages with checks and tests  
* Unit tests to ensure that packages run as expected  
  
Example code includes:  
```{r eval=FALSE}

# Set up the test framework
use_testthat("datasummary")

# Look at the contents of the package root directory
dir("datasummary")

# Look at the contents of the new folder which has been created 
dir("datasummary/tests")


# Create a summary of the iris dataset using your data_summary() function
iris_summary <- data_summary(iris)

# Count how many rows are returned
summary_rows <- nrow(iris_summary) 

# Use expect_equal to test that calling data_summary() on iris returns 4 rows
expect_equal(summary_rows, 4)


result <- data_summary(weather)

# Update this test so it passes
expect_equal(result$sd, c(2.1, 3.6), tolerance = 0.1)

expected_result <- list(
    ID = c("Day", "Temp"),
    min = c(1L, 14L),
    median = c(4L, 19L),
    sd = c(2.16024689946929, 3.65148371670111),
    max = c(7L, 24L)
)

# Write a passing test that compares expected_result to result
expect_equivalent(result, expected_result)


# Create a vector containing the numbers 1 through 10
my_vector <- 1:10

# Look at what happens when we apply this vector as an argument to data_summary()
data_summary(my_vector)

# Test if running data_summary() on this vector returns an error
expect_error(data_summary(my_vector))


# Run data_summary on the airquality dataset with na.rm set to FALSE
data_summary(airquality, na.rm=FALSE)

# Use expect_warning to formally test this
expect_warning(data_summary(airquality, na.rm = FALSE))


# Expected result
expected <- data.frame(min = 14L, median = 19L, sd = 3.65148371670111, max = 24L)

# Create variable result by calling numeric summary on the temp column of the weather dataset
result <- datasummary:::numeric_summary(weather$Temp, na.rm = TRUE)

# Test that the value returned matches the expected value
expect_equal(result, expected)


# Use context() and test_that() to group the tests below together
context("Test data_summary()")

test_that("data_summary() handles errors correctly", {

  # Create a vector
  my_vector <- 1:10

  # Use expect_error()
  expect_error(data_summary(my_vector))

  # Use expect_warning()
  expect_warning(data_summary(airquality, na.rm = FALSE))

})


# Run the tests on the datasummary package
test("datasummary")

```
  
  
  
***
  
### _Factor Analysis in R_  
  
Chapter 1 - Evaluating Your Measure with Factor Analysis  
  
Introduction to Exploratory Factor Analysis:  
  
* Psychometrics is the study of unobservable ("of the mind") variables  
* Factor analysis is a valuable tool in psychometric analysis  
* Factor analysis is mid-way between SEM (structural equation modeling) and Classical Test Theory  
	* Exploratory Factor Analysis (EFA) is used during measure development  
    * Confirmatory Factor Analysis (CFA) is used to validate a measure after development  
* This course will use library(psych) and the gcbs dataset on conspiracies  
	* EFA_model <- fa(gcbs)  
    * fa.diagram(EFA_model)  
    * EFA_model$loadings  
  
Overview of the Measure Development Process:  
  
* The development process includes  
	* Develop items foir your measure - start with a larger list than you need  
    * Collect pilot data from a representative sample  
    * Check what the dataset looks like - psych::describe()  
    * Consider whether to run EFA or CFA or both  
    * If running both EFA and CFA, split the dataset in to two samples  
    * Compare the two samples to make sure they are similar  
* Can use the psych package to view a dataset by the grouping variable  
	* gcbs_grouped <- cbind(gcbs, group_var)  
    * describeBy(gcbs_grouped, group = group_var)  # group= can be a vector  
    * statsBy(gcbs_grouped, group = "group_var")  # group= needs to be a name of a column  
  
Measure Features: Correlations and Reliability:  
  
* Can grab the lower-diagonal correlations using lowerCor(gcbs)  
* Can also get the p-values using corr.test(gcbs, use = "pairwise.complete.obs")$p  # gets the p-values for each correlation  
* Can also get the 95% CI for the correlations using corr.test(gcbs, use = "pairwise.complete.obs")$ci  
* Can also get the alpha using alpha(gcbs) # measure of the internal consistency or 'reliability' of the measure, with a target of 0.8+  
* Can look at the split-half reliability using splitHalf(gcbs)  
  
Example code includes:  
```{r}

# Load the psych package
library(psych)


gcbs <- readRDS("./RInputFiles/GCBS_data.rds")
glimpse(gcbs)


# Conduct a single-factor EFA
EFA_model <- fa(gcbs)

# View the results
EFA_model


# Set up the single-factor EFA
EFA_model <- fa(gcbs)

# View the factor loadings
EFA_model$loadings

# Create a path diagram of the items' factor loadings
fa.diagram(EFA_model)


# Take a look at the first few lines of the response data and their corresponding sum scores
head(gcbs)
rowSums(gcbs[1:6, ])

# Then look at the first few lines of individuals' factor scores
head(EFA_model$scores)

# To get a feel for how the factor scores are distributed, look at their summary statistics and density plot.
summary(EFA_model$scores)

plot(density(EFA_model$scores, na.rm = TRUE), main = "Factor Scores")


# Basic descriptive statistics
describe(gcbs)

# Graphical representation of error
error.dots(gcbs)

# Graphical representation of error
error.bars(gcbs)


# Establish two sets of indices to split the dataset
N <- nrow(gcbs)
indices <- seq(1, N)
indices_EFA <- sample(indices, floor((.5*N)))
indices_CFA <- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
gcbs_EFA <- gcbs[indices_EFA, ]
gcbs_CFA <- gcbs[indices_CFA, ]


# Use the indices from the previous exercise to create a grouping variable
group_var <- vector("numeric", nrow(gcbs))
group_var[indices_EFA] <- 1
group_var[indices_CFA] <- 2

# Bind that grouping variable onto the gcbs dataset
gcbs_grouped <- cbind(gcbs, group_var)

# Compare stats across groups
describeBy(gcbs_grouped, group = group_var)
statsBy(gcbs_grouped, group = "group_var")


# Take a look at some correlation data
lowerCor(gcbs, use = "pairwise.complete.obs")

# Take a look at some correlation data
corr.test(gcbs, use = "pairwise.complete.obs")$p

# Take a look at some correlation data
corr.test(gcbs, use = "pairwise.complete.obs")$ci


# Estimate coefficient alpha
alpha(gcbs)

# Calculate split-half reliability
splitHalf(gcbs)

```
  
  
  
***
  
Chapter 2 - Multidimensional EFA  
  
Determining dimensionality:  
  
* Can use factor analysis to find the "true" number of dimensions being reflected in the data  
* Can use the bfi dataset ("Big Five" personality trait dataset)  
	* Six point scale with 1 being very inaccurate and 6 being very accurate  
* Suppose that you do not have a theory underlying the data and instead want to use an empirical approach with eigenvalues  
	* bfi_EFA_cor <- cor(bfi_EFA, use = "pairwise.complete.obs")  
    * eigenvals <- eigen(bfi_EFA_cor)  
    * eigenvals$values  
    * scree(bfi_EFA_cor, factors = FALSE)  # eigenvalues greater than 1 are typically the best to use  
  
Understanding multidimensional data:  
  
* Theory and empirical data may lead to different outcomes - constructs for psychology are an example  
* Factors are the mathematical counterpart of a theoretical construct  
	* How well does the hypothesis fit with the data?  
* Can instead run exploratory analysis to try to come up with factors  
	* Lack of theoretical grounding can make interpretation of the results complicated  
* Can run the multidimensional analysis using any number of factors  
	* EFA_model <- fa(bfi_EFA, nfactors = 6)  
    * EFA_model$loadings  
    * head(EFA_model$scores)  
  
Investigating model fit:  
  
* Can look at absolute fit (adequate fit with typical ranges and cutoff values) and relative fit (no set ranges, used mainly for nested models from the same dataset)  
* Goal is to have a non-significant chi-squared test, though that is rare for large datasets  
* The TLI (Tucker-Lewis) should be 0.90+  
* The RMSEA (RMSE approximation) should be 0.05-  
* The BIC is a relative-fit statistic, so it is meaningful only to compare across models (lower BIC is better)  
	* bfi_theory <- fa(bfi_EFA, nfactors = 5)  
    * bfi_eigen <- fa(bfi_EFA, nfactors = 6)  
    * bfi_theory$BIC  
    * bfi_eigen$BIC  
  
Example code includes:  
```{r}

data(bfi, package="psych")
glimpse(bfi)


# Establish two sets of indices to split the dataset
N <- nrow(bfi)
indices <- seq(1, N)
indices_EFA <- sample(indices, floor((.5*N)))
indices_CFA <- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
bfi_EFA <- bfi[indices_EFA, ]
bfi_CFA <- bfi[indices_CFA, ]


# Calculate the correlation matrix first
bfi_EFA_cor <- cor(bfi_EFA, use = "pairwise.complete.obs")

# Then use that correlation matrix to calculate eigenvalues
eigenvals <- eigen(bfi_EFA_cor)

# Look at the eigenvalues returned
eigenvals$values

# Then use that correlation matrix to create the scree plot
scree(bfi_EFA_cor, factors = FALSE)


# Run the EFA with six factors (as indicated by your scree plot)
EFA_model <- fa(bfi_EFA, nfactors=6)

# View results from the model object
EFA_model


# Run the EFA with six factors (as indicated by your scree plot)
EFA_model <- fa(bfi_EFA, nfactors=6)

# View items' factor loadings
EFA_model$loadings

# View the first few lines of examinees' factor scores
head(EFA_model$scores)


# Run each theorized EFA on your dataset
bfi_theory <- fa(bfi_EFA, nfactors = 5)
bfi_eigen <- fa(bfi_EFA, nfactors = 6)

# Compare the BIC values
bfi_theory$BIC
bfi_eigen$BIC

```
  
  
  
***
  
Chapter 3 - Confirmatory Factor Analysis  
  
Setting up CFA:  
  
* Confirmatory analysis is based on explicitly defined factor relationships, to confirm a previously developed theory  
* Can use the results from an EFA as the baseline for a CFA; for example, reversing the wording of items with a negative loading  
	* EFA_syn <- structure.sem(EFA_model)  
    * EFA_syn  # Path goes from factor to item, while an NA for Value means that the starting parameter will be chosen randomly  
* Can also create CFA syntax from theory explicitly, for example  
	* theory_syn_eq <- "  
    * AGE: A1, A2, A3, A4, A5     #Agreeableness  
    * CON: C1, C2, C3, C4, C5     #Conscientiousness  
    * EXT: E1, E2, E3, E4, E5     #Extraversion  
    * NEU: N1, N2, N3, N4, N5     #Neuroticism  
    * OPE: O1, O2, O3, O4, O5     #Openness  
    * "  
    * theory_syn <- cfa(text = theory_syn_eq, reference.indicators = FALSE)  # sets the factor variances to 1 rather than estimating them  
  
Understanding the sem() syntax:  
  
* Syntax will show the Path, Parameter, and Starting Value  
	* Factor variances are shown using <-> arrows, with Start Value of 1 since they have been fixed as 1  
    * Factor covariances also use <-> arrows  
    * Item-level variances are also shown using <-> arrows  
* Can run the model using the sem() function with the syntax object  
	* theory_CFA <- sem(theory_syn, data = bfi_CFA)  
    * summary(theory_CFA)  
  
Investigating model fit:  
  
* The chi-squared test (log-likelihood test) is the only test printed by default, though it will usually be significant for a large dataset size  
* Can change the global options so that additional tests are run  
	* options(fit.indices = c("CFI", "GFI", "RMSEA", "BIC"))  
    * summary(theory_CFA)  
    * summary(theory_CFA)$BIC  # lower BIC is preferred, but is only useful for nested models on the same dataset  
  
Example code includes:  
```{r}

# Conduct a five-factor EFA on the EFA half of the dataset
EFA_model <- fa(bfi_EFA, nfactors = 5)

# Use the wrapper function to create syntax for use with the sem() function
EFA_syn <- structure.sem(EFA_model)


# Set up syntax specifying which items load onto each factor
theory_syn_eq <- "
AGE: A1, A2, A3, A4, A5
CON: C1, C2, C3, C4, C5
EXT: E1, E2, E3, E4, E5
NEU: N1, N2, N3, N4, N5
OPE: O1, O2, O3, O4, O5
"

library(sem)

# Feed the syntax in to have variances and covariances automatically added
theory_syn <- cfa(text = theory_syn_eq, reference.indicators = FALSE)

# Use the sem() function to run a CFA
theory_CFA <- sem(theory_syn, data = bfi_CFA)

# Use the summary function to view fit information and parameter estimates
summary(theory_CFA)


# CAUTION THAT THIS WILL SET GLOBAL OPTIONS
# Set the options to include various fit indices so they will print
origFit <- getOption("fit.indices")
options(fit.indices = c("CFI", "GFI", "RMSEA", "BIC"))

# Use the summary function to view fit information and parameter estimates
summary(theory_CFA)


# Run a CFA using the EFA syntax you created earlier
EFA_CFA <- sem(EFA_syn, data = bfi_CFA)

# Locate the BIC in the fit statistics of the summary output
summary(EFA_CFA)$BIC

# Compare EFA_CFA BIC to the BIC from the CFA based on theory
summary(theory_CFA)$BIC


# Reset to baseline
options(fit.indices = origFit)

```
  
  
  
***
  
Chapter 4 - Refining Your Measure and Model
  
EFA vs CFA Revisited:  
  
* EFA is exploratory and looks at many possible relationships  
* CFA is confirmatory and based only on the loadings defined by a theoretical relationship; will have different loadings than EFA dur to different number of variables  
* Due to the rotations involved in EFA, the variables may have non-intuitive names  
	* EFA_scores <- EFA_model$scores  
    * CFA_scores <- fscores(EFA_CFA, data = bfi_EFA)  
    * plot(density(EFA_scores[,1], na.rm = TRUE), xlim = c(-3, 3), ylim = c(0, 1), col = "blue")  
    * lines(density(CFA_scores[,1], na.rm = TRUE), xlim = c(-3, 3), ylim = c(0, 1), col = "red")  
  
Adding Loadings to Improve Fit:  
  
* Poor model fits are sometimes due to excluded loadings  
* Can alter the syntax to add the new loadings desired; OK to have some items theorized to load on to multiple factors  
* Can then run the updated model and look at the ANOVA  
	* anova(theory_CFA, theory_CFA_add)  
  
Improving Fit by Removing Loadings:  
  
* Can delete loadings rather than add loadings  
	* theory_syn_del <- "  
    * AGE: A1, A2, A3, A4, A5  
    * CON: C1, C2, C3, C4, C5  
    * EXT: E1, E2, E3, E4, E5  
    * NEU: N1, N2, N3, N4, N5  
    * OPE: O1, O2, O3, O5  
    * "  
    * theory_syn3 <- cfa(text = theory_syn_del, reference.indicators = FALSE)  
    * theory_CFA_del <- sem(model = theory_syn3, data = bfi_CFA)  
  
Wrap-Up:  
  
* Unidimensional and multideimensional models  
* EFA (exploratory) and CFA (confirmatory)  
* The psych and sem packages are available also  
  
Example code includes:  
```{r}

# CAUTION THAT THIS WILL SET GLOBAL OPTIONS
# Set the options to include various fit indices so they will print
origFit <- getOption("fit.indices")
options(fit.indices = c("CFI", "GFI", "RMSEA", "BIC"))

# View the first five rows of the EFA loadings
EFA_model$loadings[1:5, ]

# View the first five loadings from the CFA estimated from the EFA results
summary(EFA_CFA)$coeff[1:5, ]


# Extracting factor scores from the EFA model
EFA_scores <- EFA_model$scores

# Calculating factor scores by applying the CFA parameters to the EFA dataset
CFA_scores <- fscores(EFA_CFA, data = bfi_EFA)

# Comparing factor scores from the EFA and CFA results from the bfi_EFA dataset
plot(density(EFA_scores[,1], na.rm = TRUE), 
    xlim = c(-3, 3), ylim = c(0, 1), col = "blue")
lines(density(CFA_scores[,1], na.rm = TRUE), 
    xlim = c(-3, 3), ylim = c(0, 1), col = "red")


# Add some plausible item/factor loadings to the syntax
theory_syn_add <- "
AGE: A1, A2, A3, A4, A5
CON: C1, C2, C3, C4, C5
EXT: E1, E2, E3, E4, E5, N4
NEU: N1, N2, N3, N4, N5, E3
OPE: O1, O2, O3, O4, O5
"

# Convert your equations to sem-compatible syntax
theory_syn2 <- cfa(text = theory_syn_add, reference.indicators = FALSE)

# Run a CFA with the revised syntax
theory_CFA_add <- sem(model = theory_syn2, data = bfi_CFA)

# Conduct a likelihood ratio test
anova(theory_CFA, theory_CFA_add)

# Compare the comparative fit indices - higher is better!
summary(theory_CFA)$CFI
summary(theory_CFA_add)$CFI

# Compare the RMSEA values - lower is better!
summary(theory_CFA)$RMSEA
summary(theory_CFA_add)$RMSEA

# Compare BIC values
summary(theory_CFA)$BIC
summary(theory_CFA_add)$BIC


# Remove the weakest factor loading from the syntax
theory_syn_del <- "
AGE: A1, A2, A3, A4, A5
CON: C1, C2, C3, C4, C5
EXT: E1, E2, E3, E4, E5
NEU: N1, N2, N3, N4, N5
OPE: O1, O2, O3, O5
"

# Convert your equations to sem-compatible syntax
theory_syn3 <- cfa(text = theory_syn_del, reference.indicators = FALSE)

# Run a CFA with the revised syntax
theory_CFA_del <- sem(model = theory_syn3, data = bfi_CFA)


# Compare the comparative fit indices - higher is better!
summary(theory_CFA)$CFI
summary(theory_CFA_del)$CFI

# Compare the RMSEA values - lower is better!
summary(theory_CFA)$RMSEA
summary(theory_CFA_del)$RMSEA

# Compare BIC values
summary(theory_CFA)$BIC
summary(theory_CFA_del)$BIC


# Reset to baseline
options(fit.indices = origFit)

```
  
  
  
***
  
### _Generalized Linear Models in R_  
  
Chapter 1 - GLM - Extension of Regression Toolbox  

Limitations of linear models:  
  
* Linear models are workhorses of data science - explaining variability with linear combinations of variables  
	* lm(y ~ x, data = dat)  
* Linear models assume linear relationships, and normally distributed residuals  
	* lm(formula = weight ~ Diet, data = ChickWeightEnd)  # ChickWeightEnd is the FINAL endpoint  
* Sometimes want to model counts or survival or the like, where the basic linear model is not appropriate  
	* Poisson family for count data  
    * Binomial family for survival data  
    * Link functions to convert the linear model to the relevant family  
* Can run GLM in R  
	* glm( y ~ x, data = data, family = "gaussian")  # this is the same as lm()  
  
Poisson regression:  
  
* The Poisson model is good for modeling count data - scores, visitors, cells, etc.  
* The Poisson distribution is always a non-negative integer, and with the same mean and variance  
	* glm(goal ~ player, data = scores, family = "poisson")  # global intercept and then delta goals for player vs. reference level  
    * glm(goal ~ player -1, data = scores, family = "poisson")  # average goals per player  
  
Basic lm() functions with glm():  
  
* R gives some useful shortcuts when working with lm() and glm(); for example, automatic output printing  
* The summary() call on a regression model provides additional details about the regression  
	* Can further use broom::tidy() to extract key data in a tidy format  
    * Can use coef() and confint() to get the coefficients and the confidence intervals  
* Can also make predictions based on an existing model  
	* predict(model, newData)  
  
Example code includes:  
```{r}

data(ChickWeight, package="datasets")
ChickWeightEnd <- ChickWeight %>% 
    mutate(Chick=as.factor(as.integer(Chick))) %>%
    group_by(Chick) %>% 
    filter(Time==max(Time), !(Chick %in% c(1, 2, 3, 8, 41))) %>%
    ungroup()
glimpse(ChickWeightEnd)


# Fit a lm()
lm(formula = weight ~ Diet, data = ChickWeightEnd)

# Fit a glm()
glm( formula = weight ~ Diet , data = ChickWeightEnd, family = 'gaussian')


dat <- data.frame(time=1:30, 
                  count=c(0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 2, 2, 1, 1, 4, 1, 1, 1, 1, 0, 0)
                  )
dat


# fit y predicted by x with data.frame dat using the poisson family
poissonOut <- glm(count ~ time, data=dat, family="poisson")

# print the output
print(poissonOut)


# Fit a glm with count predicted by time using data.frame dat and gaussian family
lmOut <- glm(count ~ time, data=dat, family="gaussian")

summary(lmOut)
summary(poissonOut)


scores <- data.frame(player=rep(c("Sam", "Lou"), each=5), 
                     goal=c(1, 2, 0, 4, 3, 0, 0, 1, 0, 0)
                     )
scores


# Fit a glm() that estimates the difference between players
summary(glm(goal ~ player, data=scores, family="poisson"))

# Fit a glm() that estimates an intercept for each player 
summary(glm(goal ~ player - 1, data=scores, family="poisson"))


dat2 <- data.frame(Date=as.Date("2005-01-09")+1:4368, Number=0L) %>%
    mutate(Month=as.factor(lubridate::month(Date)))

eq1 <- c(1, 2, 6, 22, 42, 47, 48, 86, 96, 109, 113, 119, 190, 192, 208, 248, 264, 278, 306, 333, 334, 336, 368, 375, 392, 393, 408, 417, 424, 429, 439, 449, 455, 456, 500, 523, 536, 544, 545, 548, 550, 551, 586, 590, 597, 598, 673, 678, 700, 717, 740, 750, 755, 756, 767, 775, 793, 831, 859, 865, 866, 877, 885, 887, 895, 937, 1086, 1101, 1107, 1111, 1112, 1154, 1157, 1183, 1213, 1235, 1247, 1251, 1269, 1272, 1288, 1295, 1300, 1320, 1342, 1350, 1424, 1454, 1457, 1460, 1476, 1522, 1589, 1598, 1608, 1627, 1642, 1665, 1697, 1709, 1733, 1746, 1749, 1766, 1799, 1830, 1866, 1895, 1914, 1920, 1934, 1942, 1953, 1960, 1961, 1966, 1969, 1989, 2007, 2041, 2051, 2087, 2092, 2096, 2106, 2122, 2129, 2138, 2156, 2159, 2174, 2176, 2177, 2180, 2191, 2214, 2217, 2218, 2251, 2276, 2286, 2302, 2308, 2340, 2352, 2361, 2382, 2416, 2419, 2421, 2464, 2468, 2492, 2522, 2526, 2548, 2550, 2573, 2620, 2625, 2627, 2629, 2698, 2706, 2721, 2726, 2760, 2768, 2787, 2796, 2813, 2854, 2858, 2890, 2900, 2909, 2932, 2933, 2955, 2960, 2966, 2997, 3032, 3057, 3063, 3080, 3090, 3095, 3098, 3122, 3130, 3154, 3160, 3199, 3205, 3215, 3227, 3229, 3243, 3244, 3254, 3302, 3340, 3350, 3469, 3506, 3519, 3525, 3535, 3542, 3584, 3604, 3653, 3660, 3673, 3692, 3694, 3706, 3763, 3792, 3801, 3808, 3812, 3814, 3822, 3884, 3892, 4001, 4084, 4194, 4210, 4220, 4229, 4242, 4265, 4267, 4296, 4302, 4325, 4334, 4338, 4341, 4353, 4354, 4357, 4368)
eq2 <- c(21, 195, 308, 505, 522, 560, 913, 1202, 1353, 1439, 1473, 1484, 1614, 1717, 1808, 1940, 2110, 2391, 2407, 2535, 2716, 2748, 2949, 3313, 3421, 3671, 3967, 3991, 4281)
eq3 <- c(624, 776, 1364, 1585, 2063, 2109, 2196, 2569, 2576, 2607, 3399, 3533, 3607)
eq4 <- c(463, 1918, 2417, 3064, 3606)
eq5 <- c(13, 3826)
eq6 <- c(701, 2097)
eq7 <- c(2509, 4276)
eq9 <- c(1637)

dat2[eq1, "Number"] <- 1L
dat2[eq2, "Number"] <- 2L
dat2[eq3, "Number"] <- 3L
dat2[eq4, "Number"] <- 4L
dat2[eq5, "Number"] <- 5L
dat2[eq6, "Number"] <- 6L
dat2[eq7, "Number"] <- 7L
dat2[eq9, "Number"] <- 9L

str(dat2)
table(dat2$Number)
table(dat2$Month)


# build your models
lmOut <- lm(Number ~ Month, data=dat2) 
poissonOut <- glm(Number ~ Month, data=dat2, family="poisson")

# examine the outputs using print
print(lmOut)
print(poissonOut)

# examine the outputs using summary
summary(lmOut)
summary(poissonOut)

# examine the outputs using tidy
broom::tidy(lmOut)
broom::tidy(poissonOut)


# Extract the regression coefficients
coef(poissonOut)

# Extract the confidence intervals
confint(poissonOut)


# use the model to predict with new data 
newDat <- data.frame(Month=as.factor(6:8))
predOut <- predict(object = poissonOut, newdata = newDat, type = "response")

# print the predictions
print(predOut)

```
  
  
  
***
  
Chapter 2 - Logistic Regression  
  
Overview of logistic regression:  
  
* Commonly used for making win/loss or survive/die predictions - binary data such as 0/1, Coke/Pepsi, W/L, etc.  
* The logistic regression is the default for GLM with family "binomial"  
	* The logit link transforms probabilities to log-odds, while the inverse logit transforms log-odds to probabilities  
* Can fit the logistic regression in R using the default link in the glm()  
	* glm(y ~ x, data = dat, family = 'binomial')  
  
Bernoulli vs. Binomial Distribution:  
  
* Binomial and Bernoulli distributions are the foundation of logistic regression  
	* Bernoulli models a single event (for example, the likelihood of heads in 1 coin flip)  
	* Binomial models multiple events at the same time (for example, the number of heads in 10 coin flips)  
* Several options for entering data in R for use in logistic regression  
	* Long format (Bernoulli) - vector of outcomes  
    * Wide format (binomial) - proportions of success with weights, such as looking at groups  
* The appropriate input structure depends on the underlying data - groups vs. individuals  
  
Link functions - probit compared with logit:  
  
* Link functions are important for understanding and simulating GLMs  
* The probit link function is another option (rather than logit) for the link function of the binomial  
	* The "probit" is an abbreviation for probability unit, and is computattionally easier than the logit which was important when computers were slower  
    * The probit returns a z-score, but with thinner tails than the probit (so the logit is often better for modeling outliers)  
* Need to convert z-scores to probabilities, then can run the rbinom function  
  
Example code includes:  
```{r}

busData <- readr::read_csv("./RInputFiles/busData.csv")
bus <- busData %>%
    mutate(Bus=factor(Bus, levels=c("No", "Yes")))
glimpse(bus)


# Build a glm that models Bus predicted by CommuteDays
# using data.frame bus. Remember to use a binomial family.
busOut <- glm(Bus ~ CommuteDays, data=bus, family="binomial")

# Print the busOut (be sure to use the print() function)
print(busOut)

# Look at the summary() of busOut
summary(busOut)

# Look at the tidy() output of busOut
broom::tidy(busOut)


# Simulate 1 draw with a sample size of 100
binomialSim <- rbinom(n=1, size=100, prob=0.5)

# Simulate 100 draw with a sample size of 1 
BernoulliSim <- rbinom(n=100, size=1, prob=0.5)

# Print the results from the binomial
print(binomialSim)

# Sum the results from the Bernoulli
sum(BernoulliSim)


dataLong <- data.frame(x=factor(rep(c("a", "b"), each=14), levels=c("a", "b")), 
                       y=factor(c('fail', 'fail', 'fail', 'fail', 'success', 'fail', 'fail', 'fail', 'fail', 'fail', 'fail', 'fail', 'fail', 'success', 'success', 'fail', 'success', 'success', 'success', 'success', 'success', 'success', 'success', 'success', 'success', 'fail', 'success', 'fail'), levels=c("fail", "success"))
                       )
str(dataLong)

# Fit a a long format logistic regression
lr_1 <- glm(y ~ x, data=dataLong, family="binomial")
print(lr_1)


dataWide <- dataLong %>%
    group_by(x) %>%
    summarize(fail=sum(y=="fail"), success=sum(y=="success"), Total=n(), successProportion = success/Total)
dataWide

# Fit a wide form logistic regression
lr_2 <- glm(cbind(fail, success) ~ x, data=dataWide, family="binomial")

# Fit a a weighted form logistic regression
lr_3 <- glm(successProportion ~ x, weights=Total, data=dataWide, family="binomial")

# print your results
print(lr_2)
print(lr_3)


# Fit a GLM with a logit link and save it as busLogit
busLogit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "logit"))

# Fit a GLM with probit link and save it as busProbit
busProbit <- glm(Bus ~ CommuteDays, data = bus, family = binomial(link = "probit"))

# Print model summaries
summary(busLogit)
summary(busProbit)


# Convert from the logit scale to a probability
p <- dlogis(0)

# Simulate a logit 
rbinom(n=10, size=1, prob=p)


# Convert from the probit scale to a probability
p <- pnorm(0)

# Simulate a probit
rbinom(n=10, size=1, prob=p)

```
  
  
  
***
  
Chapter 3 - Interpreting and Visualizing GLMs  
  
Poisson Regression Coefficients:  
  
* Linear models are additive, but we may want to use the linear model with a link to an exponential (multiplicative)  
* The Poisson model is multiplicative while the linear model is additive  
	* poissonOut <- glm(y ~ x, family = 'poisson')  
    * tidy(poissonOut, exponentiate = TRUE)  # exponentiate the coefficients  
* A significant Poisson coefficient should be statistically different from 1 (since this is exp(0))  
  
Plotting Poisson Regression:  
  
* Can use either a geom_smooth() or a boxplot()  
* Example of using simulated does data on cancer cells  
	* ggplot(data = dat, aes(x = dose, y = cells)) + geom_point()  
    * ggplot(data = dat, aes(x = dose, y = cells)) + geom_jitter(width = 0.05, height = 0.05)  
    * ggplot(data=dat, aes(x = dose, y = cells)) + geom_jitter(width = 0.05, height = 0.05) geom_smooth()  
    * ggplot(data = dat, aes(x = dose, y = cells)) + geom_jitter(width = 0.05, height = 0.05) + geom_smooth(method = 'glm', method.args = list(family = 'poisson'))  # Poisson GLM  
  
Understanding output from logistic regression:  
  
* Linear model results are the easiest to communicate  
* Poisson model results are multiplicative rather than additive, and are relatively easy to communicate  
* Logistic model results are in log-odds form, which are harder to communicate  
	* The odds ratio is p(win) / p(loss)  
    * The log-odds are the logit, and are ln( p/(1-p) ), and this is the logit function  
    * The odds are exp(log-odds)  
    * The odds ratio is exp(Beta-1) - if the odds ratio is 1, there is no impact (odds ratios greater than 1 mean a greater chance of something occuring)  
* Can extract confidence intervals and coefficients from the binomial GLM  
	* glmOut <- glm(y ~ x, family = 'binomial')  
    * coef(glmOut)  
    * exp(coef(glmOut))  
    * confint(glmOut)  
    * exp(confint(glmOut))  
    * tidy(glmOut, exponentiate = TRUE, conf.int= TRUE)  # get everything at once from the broom::tidy() package  
  
ggplot2 and binomial regression:  
  
* Can look at plots for the underlying data for a ggplot2  
	* ggplot(bus, aes(x = MilesOneWay, y = Bus)) + geom_point()  
    * ggJitter <- ggplot(bus, aes(x = MilesOneWay, y = Bus)) + geom_jitter(width = 0, height = 0.05)  
    * ggJitter + geom_smooth()  # does not work!  
    * bus$Bus2 <- as.numeric(bus$Bus) - 1  # convert factor to numeric  
    * ggJitter + geom_smooth()  # still not really right  
    * ggJitter + geom_smooth(method = 'glm', method.args = list(family = "binomial"))  # much better!  
* Can use graphs to compare probit and logit  
	* ggJitter + geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'logit')), se = FALSE, color = 'red') + geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'probit')), se = FALSE, color = 'blue')  
  
Example code includes:  
```{r}

# extract the coeffients from lmOut
(lmCoef <- coef(lmOut))

# extract the coefficients from poisosnOut
(poissonCoef <- coef(poissonOut))

# take the exponetial using exp()
(poissonCoefExp <- exp(poissonCoef))


# This is because the Poisson coefficients are multiplicative
# Notice that 0.129 * 0.706 = 0.091 from the Poisson coefficents is the same as 0.129-0.038 = 0.091 from the linear model

cellData <- data.frame(dose=c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10), 
                       cells=c(1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 3, 0, 2, 2, 1, 0, 1, 2, 2, 2, 2, 3, 5, 3, 0, 3, 6, 2, 4, 4, 2, 2, 8, 4, 4, 4, 7, 2, 6, 5, 2, 5, 8, 4, 7, 4, 4, 7, 9, 3, 6, 7, 9, 5, 3, 5, 5, 3, 4, 11, 2, 7, 9, 3, 4, 2, 6, 5, 5, 6, 4, 5, 8, 10, 11, 9, 8, 8, 11, 7, 10, 12, 9, 12, 10, 12, 9, 17, 6, 9, 15, 11, 11, 10, 4, 9, 13, 8, 8, 13)
                       )

# Use geom_smooth to plot a continuous predictor variable
ggplot(data = cellData, aes(x = dose, y = cells)) + 
    geom_jitter(width = 0.05, height = 0.05) + 
    geom_smooth(method = 'glm', method.args = list(family = 'poisson'))


# Extract out the coefficients 
coefOut <- coef(busOut)

# Convert the coefficients to odds-ratios 
exp(coefOut)

# use tidy on busOut and exponentiate the results and extract the confidence interval
broom::tidy(busOut, exponentiate=TRUE, conf.int=TRUE)


str(bus)
bus <- bus %>%
    mutate(Bus2 = as.integer(Bus)-1)
str(bus)

# add in the missing parts of the ggplot
ggJitter <- ggplot(data = bus, aes(x = MilesOneWay, y = Bus2)) + 
    geom_jitter(width = 0, height = 0.05)

# add in geom_smooth()
ggJitter + geom_smooth()

# add in the missing parts of the ggplot
ggJitter + geom_smooth(method =  "glm" , method.args = list(family="binomial"))


# add in the missing parts of the ggplot
ggJitter + 
    geom_smooth(method = 'glm', method.args = list(family = binomial(link="probit")), 
                color = 'red', se = FALSE
                ) +
    geom_smooth(method = 'glm', method.args = list(family = binomial(link="logit")), 
                color = 'blue', se = FALSE
                )

```
  
  
  
***
  
Chapter 4 - Multiple Regression with GLMs  
  
Multiple logistic regression:  
  
* Can use multiple predictors in the logistic regression  
	* Risks of over-fitting as the number of predictor variables increases - typical target of observations >= 10*predictors  
    * glm(Bus ~ CommuteDay + MilesOneWay, data = bus, family = 'binomial')  
* When there is correlation in the predictors, the coefficients may change depending on the order in which they appear in the equation  
  
Formulas in R:  
  
* The model.matrix() is the cornerstone of the regression process; often run behind the scenes  
* With multiple intercepts, the default is to have the global intercept as the first group, and all other groups being the intercept relative to the reference group  
  
Assumptions of multiple logistic regression:  
  
* Simpson's paradox can be a confounder - need to include all the relevant grouping variables  
	* Example of the UC Berkeley admission data - key to include the "by department" variable  
* Assumptions of linear and monotonic responses  
* Predictors and response variables should be independent  
* Over-dispersion can cause issues - too many zeroes, too many ones, changing variances over x, etc.  
  
Wrap up:  
  
* GLM extensions of LM - count data, logits, plotting, etc.  
  
Example code includes:  
```{r}

# Build a logistic regression with Bus predicted by CommuteDays and MilesOneWay
busBoth <- glm(Bus ~ CommuteDays + MilesOneWay, data=bus, family="binomial")

# Look at the summary of the output
summary(busBoth)


# Build a logistic regression with Bus predicted by CommuteDays
busDays <- glm(Bus ~ CommuteDays, data=bus, family="binomial")

# Build a logistic regression with Bus predicted by MilesOneWay
busMiles <- glm(Bus ~ MilesOneWay, data=bus, family="binomial")


# Build a glm with CommuteDays first and MilesOneWay second
busOne <- glm(Bus ~ CommuteDays + MilesOneWay, data=bus, family="binomial")

# Build a glm with MilesOneWay first and CommuteDays second
busTwo <- glm(Bus ~ MilesOneWay + CommuteDays, data=bus, family="binomial")

# Print model summaries
summary(busOne)
summary(busTwo)


size <- c(1.1, 2.2, 3.3)
count <- c(10, 4, 2)

# use model matrix with size
model.matrix(~ size)

# use model matirx with count
model.matrix(~ size + count)


color <- c("red", "blue", "green")

# create a matrix that includes a reference intercept
model.matrix(~ color)

# create a matrix that includes an intercept for each group
model.matrix(~ color - 1)


shape <- c("square", "square", "circle")

# create a matrix that includes color and shape  
model.matrix(~ color + shape - 1)

# create a matrix that includes shape and color 
model.matrix(~ shape + color - 1)


data("UCBAdmissions", package="datasets")
UCBdata <- as.data.frame(UCBAdmissions) %>%
    mutate(Gender=factor(Gender, levels=c("Female", "Male")), Dept=factor(Dept, levels=LETTERS[1:6])) %>%
    tidyr::spread(Admit, Freq) %>%
    arrange(Dept, Gender)

# build a binomial glm where Admitted and Rejected are predicted by Gender
glm1 <- glm(cbind(Admitted, Rejected) ~ Gender, data=UCBdata, family="binomial")
summary(glm1)

# build a binomial glm where Admitted and Rejected are predicted by Gender and Dept
glm2 <- glm(cbind(Admitted, Rejected) ~ Gender + Dept, data=UCBdata, family="binomial")
summary(glm2)


# Add a non-linear equation to a geom_smooth
ggJitter + 
  geom_smooth(method = 'glm', method.args = list(family = 'binomial'), formula = y~I(x^2), color = 'red')

```
  
  
  
***
  
### _Introduction to Bioconductor_  
  
Chapter 1 - What is Bioconductor?  
  
Introduction to the Bioconductor Project:  
  
* Bioconductor is open-source software - datasets and packages for analyzing biological data  
	* Typically for measuring either the structure or the function (or the interactions of these) for biological elements  
* Bioconductor has its own repository and means of installing packages  
	* source("https://bioconductor.org/biocLite.R")  
    * biocLite("packageName")  
* Bioconductor is constantly in development  
	* library(packageName)  
    * BiocInstaller::biocVersion()   
    * sessionInfo()  
    * packageVersion("packageName")  
    * BiocInstaller::biocValid()  
  
Role of S4 in Bioconductor:  
  
* R uses S3 and S4; Bioconductor inherits from S4  
* The S3 system is simple and powerful, and is typically used for CRAN  
	* S3 often uses generic functions such as plot() and print() which behave differently depending on the object type  
* The S4 system implements OOP by defining objects that are generalized to classes  
	* S4 classes have a formal definition and inheritance, making type checking much easier  
    * mydescriptor <- new("GenomeDescription")  
    * The S4 class is more complex than S3; higher setup costs but easier to share and reuse  
* Can check whether an object is from an S4 class  
	* isS4(mydescriptor)  
    * str(mydescriptor)  # will show a formal class with slots  
* The S4 class has a name, slots (methods/fields), and inheritance (often from "contains")  
* There are S4 accessors for getting the slots (methods/fields)  
	* .S4methods(class = "GenomeDescription")  
    * showMethods(classes = "GenomeDescription", where = search())  
    * show(myDescriptor)  # sort of like str() but for objects  
  
Biology of Genomic Datasets:  
  
* Organisms are complex and interconnected and can be unicellular or multicellular  
* All organisms have a genome (complete genetic material, stored mostly in the chromosomes) - the "blueprint"  
	* TAGC are the building blocks  
* The genome can be thought of as a genetic DNA alphabet  
	* Genes contain heredity instructions  
    * Coding genes are expressed through proteins - DNA to RNA (transcription) and RNA to protein (translation)  
    * Non-coding genes are not expressed  
* Yeast is a single-cell organism that is frequently used in food and beverage creation  
	* library(BSgenome.Scerevisiae.UCSC.sacCer3)  
    * yeast <- BSgenome.Scerevisiae.UCSC.sacCer3  
    * available.genomes()  
    * length(yeast)  
    * names(yeast)  
    * seqlengths(yeast)  
    * getSeq(yeast)  
    * getSeq(yeast, "chrM")  # get chromosomes  
    * getSeq(yeast, end = 10)  # first 10 base pairs of each chromosome  
  
Example code includes:  
```{r eval=FALSE}

# Load the BiocInstaller package
library(BiocInstaller)

# Explicit syntax to check the Bioconductor version
BiocInstaller::biocVersion() 

# When BiocInstaller is loaded use biocVersion alone
biocVersion()


# Load the BSgenome package
library(BSgenome)

# Check the version of the BSgenome package
packageVersion("BSgenome")

# Investigate about the a_genome using show()
# show(a_genome)

# Investigate some other accesors
# organism(a_genome)
# provider(a_genome)
# seqinfo(a_genome)


# Load the yeast genome
library(BSgenome.Scerevisiae.UCSC.sacCer3)

# Assign data to the yeastGenome object
yeastGenome <- BSgenome.Scerevisiae.UCSC.sacCer3

# Get the head of seqnames and tail of seqlengths for yeastGenome
head(seqnames(yeastGenome))
tail(seqlengths(yeastGenome))

# Select chromosome M, alias chrM
yeastGenome$chrM

# Count characters of the chrM sequence
nchar(yeastGenome$chrM)


# Assign data to the yeastGenome object
yeastGenome <- BSgenome.Scerevisiae.UCSC.sacCer3

# Get the first 30 bases of each chromosome
getSeq(yeastGenome, start=1, end=30)

```
  
  
  
***
  
Chapter 2 - Biostrings and When to Use Them  
  
Introduction to Biostrings:  
  
* Biostrings has method for quickly processing biological strings  
	* Memory efficient  
    * Conatiners that can be inherited  
    * showClass("XString")  
    * showClass("BString")  
    * showClass("BStringSet")  
* There are bases for DNA and RNA available for use  
	* DNA_BASES # DNA 4 bases  
    * RNA_BASES # RNA 4 bases  
    * AA_STANDARD # 20 Amino acids  
    * DNA_ALPHABET # contains IUPAC_CODE_MAP  
    * RNA_ALPHABET # contains IUPAC_CODE_MAP  
    * AA_ALPHABET # contains AMINO_ACID_CODE  
* The general process for gene expression includes  
	* Double-strand DNA splits, and is RNA transcribed (T becomes A, A becomes U, C -> G, G -> C)  
    * Each three RNA translate to an amino acid  
    * dna_seq <- DNAString("ATGATCTCGTAA")  
    * rna_seq <- RNAString(dna_seq)  
    * rna_seq  # will give seq: AUGAUCUCGUAA  
    * aa_seq <- translate(rna_seq) # Translation RNA to AA  
    * aa_seq  # seq: MIS*  
    * translate(dna_seq)  # translate() also goes directly from DNA to AA (shortcut to the RNA and translate process)  
  
Sequence handling:  
  
* Can use XString to store a single sequence  
* Can use XStrinSet to store multiple sequences, each of varying lengths  
	* zikaVirus <- readDNAStringSet("data/zika.fa")  
    * zikaVirus_seq <- unlist(zikaVirus)  # to collate the sequence use unlist  
    * zikaSet <- DNAStringSet(zikaVirus_seq, start = c(1, 101, 201), end = c(100, 200, 300))  # to create a new set from a single sequence  
    * complement(a_seq)  # the complementary sequence  
* Can use rev to reverse a sequence  
	* rev(zikaShortSet)  # the last list will become first  
    * reverse(zikaShortSet)  # reverse from right to left for each of the sequences in the set  
    * reverseComplement(rna_seq)  # same as reverseComplement(rna_seq) but more memory efficient  
  
Why we are interested in patterns:  
  
* Can learn more about patterns using sequencing - frequency, occurences, etc.  
* Can use Biostring string matching functions  
	* matchPattern(pattern, subject)  # 1 string to 1 string  
    * vmatchPattern(pattern, subject)  # 1 set of strings to 1 string OR 1 string to a set of strings  
* Palindromes can be important in biology - binding sites  
	* findPalindromes() # find palindromic regions in a single sequence  
* There are six possibilities with translation based on the start of the sequence - reverseComplements and amino acids (based on 3 bases) depending on where the window starts  
	* [1] 30 ACATGGGCCTACCATGGGAGCTACGAAGCC  # original sequence  
    * # 6 possible reading frames, DNAStringSet  
    * [1]    30 ACATGGGCCTACCATGGGAGCTACGAAGCC             + 1  
    * [2]    30 GGCTTCGTAGCTCCCATGGTAGGCCCATGT             - 1  
    * [3]    29  CATGGGCCTACCATGGGAGCTACGAAGCC             + 2  
    * [4]    29  GCTTCGTAGCTCCCATGGTAGGCCCATGT             - 2  
    * [5]    28   ATGGGCCTACCATGGGAGCTACGAAGCC             + 3  
    * [6]    28   CTTCGTAGCTCCCATGGTAGGCCCATGT             - 3  
  
Example code includes:  
```{r eval=FALSE}

# Load packages
library(Biostrings)

# Check the alphabet of the zikaVirus
alphabet(zikaVirus)

# Check the alphabetFrequency of the zikaVirus
alphabetFrequency(zikaVirus)

# Check alphabet of the zikaVirus using baseOnly = TRUE
alphabet(zikaVirus, baseOnly = TRUE)


# Unlist the set and select the first 21 letters as dna_seq, then print it
dna_seq <- DNAString(subseq(as.character(zikaVirus), end = 21))
dna_seq

# 1.1 Transcribe dna_seq as rna_seq, then print it
rna_seq <- RNAString(dna_seq) 
rna_seq

# 1.2 Translate rna_seq as aa_seq, then print it
aa_seq <- translate(rna_seq)
aa_seq

# 2.1 Translate dna_seq as aa_seq_2, then print it
aa_seq_2 <- translate(dna_seq)
aa_seq_2


# Create zikv with one collated sequence using `zikaVirus`
zikv <- unlist(zikaVirus)

# Check the length of zikaVirus and zikv
length(zikaVirus)
length(zikv)

# Check the width of zikaVirus
width(zikaVirus)

# Subset zikv to only the first 30 bases
subZikv <- subseq(zikv, end = 30)
subZikv


# The reverse of zikv is
reverse(zikv)

# The complement of zikv is
complement(zikv)

# The reverse complement of zikv is
reverseComplement(zikv)

# The translation of zikv is
translate(zikv)


# Find palindromes in zikv
findPalindromes(zikv)


# print the rnaframesZikaSet 
rnaframesZikaSet

# translate all 6 reading frames 
AAzika6F <- translate(rnaframesZikaSet)
AAzika6F

# Count the matches allowing 15 mistmatches
vcountPattern(pattern = ns5, subject = AAzika6F, max.mismatch = 15)

# Select the frame that contains the match
selectedSet <- AAzika6F[3]

#Convert this frame into a single sequence
selectedSeq <- unlist(selectedSet)


# Use vmatchPattern with the set
vmatchPattern(pattern = ns5, subject = selectedSet, max.mismatch = 15)

# Use matchPattern with the single sequence
matchPattern(pattern = ns5, subject = selectedSeq, max.mismatch = 15)

```
  
  
  
***
  
Chapter 3 - IRanges and GenomicRanges  
  
IRanges and Genomic Structures:  
  
* Can sequence millions of genes for cheap, so there is need for analyzing large sequence data  
* Sequence ranges are a core component of the analysis  
	* library(IRanges)  
    * myIRanges <- IRanges(start = 20, end = 30)  
    * (myIRanges_width <- IRanges(start = c(1, 20), width = c(30, 11)))  
    * (myIRanges_end <- IRanges(start = c(1, 20), end = 30))  
    * Note that width = end - start + 1  
* Can also use RLE - run length encoding  
	* General S4 containers for saving large and repetitive vectors  
    * (some_numbers <- c(3, 2, 2, 2, 3, 3, 4, 2))  
    * (Rle(some_numbers))  # numeric-Rle of length 8 with 5 runs  
* Can also create using logical vectors for keep or skip  
	* IRanges(start = c(FALSE, FALSE, TRUE, TRUE))  # will pull items 3 and 4 for a range of width 2  
    * gi <- c(TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE)  
    * myRle <- Rle(logi)  
* The Irange is hierarchical and can hold metadata  
  
Gene of Interest:  
  
* Genomic sequences can be split over numerous chromosomes; may want to extract with sequential ranges  
	* library(GenomicRanges)  
    * (myGR <- GRanges("chr1:200-300"))  # name:start-end as a character (each range is associated to a chromosome)  
    * methods(class = "GRanges") # to check available accessors  
    * seqnames(gr) # used for chromosome names   
    * ranges(gr) # returns an IRanges object for ranges  
    * mcols(gr) # stores metadata columns  
    * seqinfo(gr) # generic function to store sequence information  
    * genome(gr) # stores the genome name  
* Accessors can be inherited across S4 classes  
* One of the genes of interest is ABCD1 (end of the X chromosome long arm)  
	* library(TxDb.Hsapiens.UCSC.hg38.knownGene)  
    * hg <- TxDb.Hsapiens.UCSC.hg38.knownGene  
    * hg_chrXg <- genes(hg, filter = list(tx_chrom = c("chrX")))  
  
Manipulating collections of GRanges:  
  
* The GRangesList-class is a container for storing a collection of GRanges  
	* as(mylist, "GRangesList")  
    * GRangesList(myGranges1, myGRanges2, ...)  
    * unlist(myGRangesList)  # convert back to Granges  
    * methods(class = "GRangesList")  
* Multiple GRanges objects may be combined into a GRangesList  
	* GRanges in a list will be taken as compound features of a larger object  
    * transcripts by gene, exons by transcripts, read alignments, sliding windows  
* Can break a region in to smaller regions  
	* hg_chrX  
    * slidingWindows(hg_chrX, width = 20000, step = 10000)  # there is overlap of 10000 and the last range will (typically) be shorter  
* Can grab known genomic features  
	* library(TxDb.Hsapiens.UCSC.hg38.knownGene)  
    * (hg <- TxDb.Hsapiens.UCSC.hg38.knownGene)  
* Can then extract gebomic features  
	* seqlevels(hg) <- c("chrX")  
    * transcripts(hg, columns = c("tx_id", "tx_name"), filter = NULL)  
    * exons(hg, columns = c("tx_id", "exon_id"), filter = list(tx_id = "179161"))  
    * exonsBytx <- exonsBy(hg, by = "tx") # exons by transcript  
    * abcd1_179161 <- exonsBytx[["179161"]] # transcript id  
* Can also find genes of interest in the overlaps  
	* countOverlaps(query, subject)  
    * findOverlaps(query, subject)  
    * subsetByOverlaps(query, subject)  
  
Example code includes:  
```{r eval=FALSE}

# load package IRanges
library(IRanges)

# start vector 1 to 5 and end 100 
IRnum1 <- IRanges(start=1:5, end=100)

# end 100 and width 89 and 10
IRnum2 <- IRanges(end=100, width=c(89, 10))

# logical argument start = Rle(c(F, T, T, T, F, T, T, T))
IRlog1 <- IRanges(start = Rle(c(FALSE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE)))

# Printing objects in a list
print(list(IRnum1 = IRnum1, IRnum2 = IRnum2, IRlog1 = IRlog1))


# Load Package Genomic Ranges
library(GenomicRanges)

# Print the GRanges object
myGR

# Check the metadata, if any
mcols(myGR)


# load human reference genome hg38
library(TxDb.Hsapiens.UCSC.hg38.knownGene)

# assign hg38 to hg, then print it
hg <- TxDb.Hsapiens.UCSC.hg38.knownGene
hg

# extract all the genes in chromosome X as hg_chrXg, then print it
hg_chrXg <- genes(hg, filter = list(tx_chrom = c("chrX")))
hg_chrXg

# extract all positive stranded genes in chromosome X as hg_chrXgp, then sort it
hg_chrXgp <- genes(hg, filter = list(tx_chrom = c("chrX"), tx_strand = "+"))
sort(hg_chrXgp)


# load the human transcripts DB to hg
library(TxDb.Hsapiens.UCSC.hg38.knownGene)
hg <- TxDb.Hsapiens.UCSC.hg38.knownGene

# prefilter chromosome X
seqlevels(hg) <- c("chrX")

# get all transcripts by gene
hg_chrXt <- transcriptsBy(hg, by="gene")

# select gene `215` from the transcripts
hg_chrXt[[215]]


# load the human transcripts DB to hg
library(TxDb.Hsapiens.UCSC.hg38.knownGene)
hg <- TxDb.Hsapiens.UCSC.hg38.knownGene

# prefilter chromosome X
seqlevels(hg) <- c("chrX")

# get all transcripts by gene
hg_chrXt <- transcriptsBy(hg, by="gene")

# select gene `215` from the transcripts
hg_chrXt[['215']]


# Store the overlapping range in rangefound
rangefound <- subsetByOverlaps(hg_chrX, ABCD1)

# Check names of rangefound
names(rangefound)

# Check the geneOfInterest 
ABCD1

# Check rangefound
rangefound

```
  
  
  
***
  
Chapter 4 - Introducing ShortRead  
  
Sequence Files:  
  
* Plant genomes tend to be large datasets; one of the plants has 135 million base pairs  
* Can store with text formats fastq (quality encoding per sequence letter) and fasta  
* The fastq format includes  
	* @ unique sequence identifier  
    * raw sequence string  
    * + optional id  
    * quality encoding per sequence letter  
* The fasta (fasta, fa, seq) format is shorter and includes  
	* > unique sequence identifier  
    * raw sequence string  
* Can read in the fasta files using  
	* library(ShortRead)  
    * fasample <- readFasta(dirPath = "data/", pattern = "fasta") # print fasample  
    * methods(class = "ShortRead")  
    * writeFasta(fasample, file = "data/sample.fasta")  
* Can read in the fastq files using  
	* fqsample <- readFastq(dirPath = "data/", pattern = "fastq")  
    * methods(class = "ShortReadQ")  
    * writeFastq(fqsample, file = "data/sample.fastq.gz")  
* Sometimes valuable to set the seed and run a FastqSampler()  
	* set.seed(123)  
    * sampler <- FastqSampler("data/SRR1971253.fastq", 500)  
    * sample_small <- yield(sampler)  
  
Sequence Quality:  
  
* Quality scores are lograithmic somewhat like earthquake magnitudes  
	* 10 = 1 in 10 wrong  
    * 20 = 1 in 100 wrong  
    * 30 = 1 in 1,000 wrong (often considered to be the cutoff for "good")  
    * 50 = 1 in 100,000 wrong  
    * etc.  
    * encoding(ShortRead::quality(fqsample))  
    * ShortRead::quality(fqsample)  
    * sread(fqsample)[1]  
    * quality(fqsample)[1]  # quality encoding values  
    * pq <- PhredQuality(quality(fqsample))  ## PhredQuality instance  
    * qs <- as(pq, "IntegerList")  # transform encoding into scores  
* Can run a quality assessment process on files that have been read in  
	* qaSummary <- qa(fqsample, lane = 1) # optional lane  
    * names(qaSummary)  
    * browseURL(report(qaSummary))  # HTML report  
    * alphabet(sread(fullSample))  
    * abc <- alphabetByCycle(sread(fullSample))  
    * nucByCycle <- t(abc[1:4,])  
    * nucByCycle <- nucByCycle %>%   
    *     as.tibble() %>% # convert to tibble  
    *     mutate(cycle = 1:50) # add cycle numbers  
  
Match and Filter:  
  
* Can run the match and filter process while reading in a large file  
* Duplicate sequences can result from a biological process, PCR amplification, sequencing more than onbce, etc.  
	* Duplicates should generally be marked, and an acceptable threshhold set  
    * table(srduplicated(dfqsample))  
    * cleanReads <- mydReads[srduplicated(mydReads) == FALSE]  
    * table(srduplicated(cleanReads))  
* Can create your own filters using srFilter  
	* readWidthCutOff <- srFilter(function(x) {width(x) >= minWidth}, name = "MinWidth")  
    * minWidth <- 51  
    * fqsample[readWidthCutOff(fqsample)]  
    * myFilter <- nFilter(threshold = 10, .name = "cleanNFilter")  
    * filtered <- readFastq(dirPath = "data", pattern = ".fastq", filter = myFilter)  
* Can also use idFilter and polynFilter  
	* myFilterID <- idFilter(regex = ":3:1")  
    * filtered <- readFastq(dirPath = "data", pattern = ".fastq", filter = myFilterID)  
    * myFilterPolyA <- polynFilter(threshold = 10, nuc = c("A"))  
    * filtered[myFilterPolyA(filtered)]  
  
Multiple Assessment:  
  
* Desire to save time and resources when reading in high volume data - parallel processing  
	* library(Rqc)  # uses many of the basic Bioconductor packages as well as some of the basic CRAN packages  
    * qaRqc <- rqcQA(files)  # pass a file list as the files object  
    * class(qaRqc) # "list"  
    * names(qaRqc) # name of the input files  
    * qaRqc[1]  
    * qaRqc <- rqcQA(files, workers = 4))  # run it in parallel, saving only the quality assessment  
* Can also save a sample of the parallel reads, using a seed for reproducibility  
	* set.seed(1111)  
    * qaRqc_sample <- rqcQA(files, workers = 4, sample = TRUE, n = 500))  
* Can also build paired files using the pair= argument  
	* pfiles <- "data/seq_11.fq" "data/seq1_2.fq" "data/seq2_1.fq" "data/seq2_2.fq"  
    * qaRqc_paired <- rqcQA(pfiles, workers = 4, pair = c(1, 1, 2, 2)))  
    * reportFile <- rqcReport(qaRqc, templateFile = "myReport.Rmd")  
    * browseURL(reportFile)  
    * qaRqc <- rqcQA(files, workers = 4))  
    * perFileInformation(qaRqc)  
  
Introduction to Bioconductor:  
  
* Installing packages from Bioconductor  
* Basic techniques for reading, manipulating, filtering, raw genomic data  
* BSGenome and TxDb built-in datasets  
* Check the quality of sequence files using ShortRead and Rqc  
* Explored variety of organisms  
  
Example code includes:  
```{r eval=FALSE}

# load ShortRead
library(ShortRead)

# print fqsample
fqsample

# class of fqsample
class(fqsample)

# class sread fqsample
class(sread(fqsample))

# id fqsample
id(fqsample)


qaSummary <- qa(fqsample, type = "fastq", lane = 1)

# load ShortRead
library(ShortRead)

# Check quality
quality(fqsample)

# Check encoding
encoding(quality(fqsample))

# Check baseQuality
qaSummary[["baseQuality"]]


# glimpse nucByCycle
glimpse(nucByCycle)

# make an awesome plot!
nucByCycle %>% 
  # gather the nucleotide letters in alphabet and get a new count column
  gather(key = alphabet, value = count , -cycle) %>% 
  ggplot(aes(x = cycle, y =  count, colour = alphabet)) +
  geom_line(size = 0.5 ) +
  labs(y = "Frequency") +
  theme_bw() +
  theme(panel.grid.major.x = element_blank())


myStartFilter <- srFilter(function(x) substr(sread(x), 1, 5) == "ATGCA")

# Load package ShortRead
library(ShortRead)

# Check class of fqsample
class(fqsample)

# filter reads into selectedReads using myStartFilter
selectedReads <- fqsample[myStartFilter(fqsample)]

# Check class of selectedReads
class(selectedReads)

# Check detail of selectedReads
detail(selectedReads)


# Load package Rqc
library(Rqc)

# Average per cycle quality plot
rqcCycleAverageQualityPlot(qa)

# Average per cycle quality plot with white background
rqcCycleAverageQualityPlot(qa) + theme_minimal()

# Read quality plot with white background
rqcReadQualityPlot(qa) + theme_minimal()

```
  
  
  
***
  
### _Non-Linear Modeling in R with GAM_  
  
Chapter 1 - Introduction to Generalized Additive Models  
  
Introduction:  
  
* There are trade-offs between model power (e.g., ML) and parsimony (e.g., linear regression), with GAM being a middle-ground solution  
* GAM allows for flexibly modeling non-linear relationships  
	* linear_mod <- lm(y ~ x, data = my_data)  
    * library(mgcv)  
    * gam_mod <- gam(y ~ s(x), data = my_data)  # s() is the smoothing function  
* The flexible smooth is built up from simpler basis functions  
	* The overall smooth is the sum of the simpler basis functions  
    * coef(gam_mod)  
  
Basis functions and smoothing:  
  
* Because the GAM often has many basis coefficients, there is a meaningful risk of over-fitting  
	* Fit = Likelihood - lambda * Wiggliness (lambda is the smoothing parameter, and is optimized while R fits the data to the GAM)  
* Can fit the smoothing parameter using arguments in the gam() function call  
	* gam(y ~ s(x), data = dat, sp = 0.1)  # global sp argument  
    * gam(y ~ s(x, sp = 0.1), data = dat)  # sp argument specific to a term  
    * gam(y ~ s(x), data = dat, method = "REML")  # select smoothing using Restricted Maximum Likelihood  
* The number of basis functions also drives both fit to the the training data and risk of over-fitting  
	* gam(y ~ s(x, k = 3), data = dat, method = "REML")  
    * gam(y ~ s(x, k = 10), data = dat, method = "REML")  # even if the k is "too high", the REML will help prevent overfits  
    * gam(y ~ s(x), data = dat, method = "REML")  # defaults  
  
Multivariate GAMs:  
  
* Can run the GAM using multiple independent variables - smooths, categoricals, etc.  
* Can start with a simple model  
	* model <- gam(hw.mpg ~ s(weight), data = mpg, method = "REML")  
* Can then extend the simple model to include additional predictors  
	* model2 <- gam(hw.mpg ~ s(weight) + s(length), data = mpg, method = "REML")  
    * model2 <- gam(hw.mpg ~ s(weight) + length, data = mpg, method = "REML")  # combined linear and non-linear terms since length is not enclosed in an s()  
    * model2b <- gam(hw.mpg ~ s(weight) + s(length, sp = 1000), data = mpg, method = "REML")  # will be linear since there is strong smoothing due to sp=1000  
* Linear terms are especially valuable with categorical predictors  
	* model3 <- gam(hw.mpg ~ s(weight) + fuel, data = mpg, method = "REML")  # fuel needs to be a factor; mgcv will not handle characters  
    * model4 <- gam(hw.mpg ~ s(weight, by = fuel), data = mpg, method = "REML")  # smooths will be by categories of fuel  
    * model4b <- gam(hw.mpg ~ s(weight, by = fuel) + fuel, data = mpg, method = "REML")  # add intercept as well as smooth by vategory of fuel  
  
Example code includes:  
```{r}

data(mcycle, package="MASS")

# Examine the mcycle data frame
head(mcycle)
plot(mcycle)


# Fit a linear model
lm_mod <- lm(accel ~ times, data = mcycle)

# Visualize the model
termplot(lm_mod, partial.resid = TRUE, se = TRUE)


# Load mgcv
library(mgcv)

# Fit the model
gam_mod <- gam(accel ~ s(times), data = mcycle)

# Plot the results
plot(gam_mod, residuals = TRUE, pch = 1)

# Extract the model coefficients
coef(gam_mod)


# Fit a GAM with 3 basis functions
gam_mod_k3 <- gam(accel ~ s(times, k = 3), data = mcycle)

# Fit with 20 basis functions
gam_mod_k20 <- gam(accel ~ s(times, k = 20), data = mcycle)

# Visualize the GAMs
par(mfrow = c(1, 2))
plot(gam_mod_k3, residuals = TRUE, pch = 1)
plot(gam_mod_k20, residuals = TRUE, pch = 1)
par(mfrow = c(1, 1))


# Extract the smoothing parameter
gam_mod <- gam(accel ~ s(times), data = mcycle, method = "REML")
gam_mod$sp

# Fix the smoothing paramter at 0.1
gam_mod_s1 <- gam(accel ~ s(times), data = mcycle, sp = 0.1)

# Fix the smoothing paramter at 0.0001
gam_mod_s2 <- gam(accel ~ s(times), data = mcycle, sp = 0.0001)

# Plot both models
par(mfrow = c(2, 1))
plot(gam_mod_s1, residuals = TRUE, pch = 1)
plot(gam_mod_s2, residuals = TRUE, pch = 1)
par(mfrow = c(1, 1))


# Fit the GAM
gam_mod_sk <- gam(accel ~ s(times, k=50), sp=0.0001, data=mcycle)

#Visualize the model
plot(gam_mod_sk, residuals = TRUE, pch = 1)


data(mpg, package="gamair")

# Examine the data
head(mpg)
str(mpg)


# Fit the model
mod_city <- gam(city.mpg ~ s(weight) + s(length) + s(price), data = mpg, method = "REML")

# Plot the model
plot(mod_city, pages = 1)


# Fit the model
mod_city2 <- gam(city.mpg ~ s(weight) + s(length) + s(price) + fuel + drive + style, data = mpg, method = "REML")

# Plot the model
plot(mod_city2, all.terms = TRUE, pages = 1)


# Fit the model
mod_city3 <- gam(city.mpg ~ s(weight, by=drive) + s(length, by=drive) + s(price, by=drive) + drive, 
                 data = mpg, method = "REML"
                 )

# Plot the model
plot(mod_city3, pages = 1)

```
  
  
  
***
  
Chapter 2 - Interpreting and Visualizing GAMs  
  
Interpreting GAM Outputs:  
  
* Can get summaries of model output by way of the summary() function  
	* mod_hwy <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + s(comp.ratio) + s(width) + fuel + cylinders, data = mpg, method = "REML")  
    * summary(mod_hwy)  
    * The first component of the summary shows the model family, link, and formula  
    * The second component shows the parametric coefficients and significances  
    * The third components shows the approximate significance of the smooths  # edf of 1 is equivalent to a straight line, edf of 2 is equivalent to a parabola, etc.  
    * Generally, a significant smooth can be thought of as one where a straight, horizontal line cannot be drawn through the 95% confidence interval  
  
Visualizing GAMs:  
  
* Visualizations are a powerful way to inspect and communicate results  
	* ?plot.gam  
* The mgcv plots are "partial effects" plots - the components that add up to the overall model  
	* plot(gam_model, select = c(2, 3))  # default is that all smoothed terms are selected, can override with select  
    * plot(gam_model, pages = 1)  # default is as many pages as needed  
    * plot(gam_model, pages = 1, all.terms = TRUE)  # The all.terms will show the non-smoothed terms also  
    * plot(gam_model, residuals = TRUE)  # show the partial residuals  
    * plot(gam_model, rug = TRUE)  # show the rug on the x-axis  
    * plot(gam_model, rug = TRUE, residuals = TRUE, pch = 1, cex = 1)  # pch for shape and cex for size  
    * plot(gam_model, shade = TRUE)  # 95% CI is shaded rather than shown in dotted lines  
    * plot(gam_model, shade = TRUE, shade.col = "lightblue")  # color of shading  
    * plot(gam_model, seWithMean = TRUE)  
    * plot(gam_model, seWithMean = TRUE, shift = coef(gam_model)[1])  # brings in the intercept, which is the first coefficient of the model  
  
Model checking with gam.check():  
  
* There are many potential pitfalls to be checked  
	* Inadequate basis number  
* There is an automated call to look at the model results  
	* gam.check(mod)  
    * Convergence - if it has not converged, it is likely wrong  
    * The p-values for the residuals - should not be significant, though this is only an approximate test  
    * Standard regression residuals plots  
  
Checking concurvity:  
  
* There can be collinearity concerns with a linear model, which can result in poor models with large confidence intervals  
* The GAM can have a concurvity concern, where one variable is a smooth of another variable (such as x and x**2)  
	* concurvity(m1, full = TRUE)  
    * Generally, a concurvity worst case of 0.8+ is an area for concern  
    * concurvity(m1, full = FALSE)  # get the pairwise concurvities  
  
Example code includes:  
```{r}

# Fit the model
mod_city4 <- gam(city.mpg ~ s(weight) + s(length) + s(price) + s(rpm) + s(width),
                 data = mpg, method = "REML")

# View the summary
summary(mod_city4)


# Fit the model
mod <- gam(accel ~ s(times), data = mcycle, method = "REML")

# Make the plot with residuals
plot(mod, residuals=TRUE)

# Change shape of residuals
plot(mod, residuals=TRUE, pch=1, cex=1)


# Fit the model
mod <- gam(hw.mpg ~ s(weight) + s(rpm) + s(price) + comp.ratio, 
           data = mpg, method = "REML")

# Plot the price effect
plot(mod, select=c(3))

# Plot all effects
plot(mod, all.terms=TRUE, pages=1)

# Plot the weight effect with colored shading
plot(mod, select = 1, shade=TRUE, shade.col="hotpink")

# Add the intercept value and uncertainty
plot(mod, select = 1, shade=TRUE, shade.col="hotpink", seWithMean=TRUE, shift=coef(mod)[1])


dat <- data.frame(y=c(11.17, 2.81, 12.9, 5.68, 5.58, -1.09, 5.42, 12.13, 4.73, 6.29, 5.74, 8.32, 9.76, 4.78, 9.08, 10.5, 9.4, 9.51, 14.58, 13.84, 4.01, 3.31, 5.32, 6.6, 10.54, 13.19, 10.06, 8.6, -0.62, 4.78, 5.98, 2.75, 1.36, 8.51, 8.12, 4.18, 10.65, 5.92, -0.03, 6.48, 9.12, 6.57, 15.38, 11.76, 7.47, 12, 3.4, 3.39, 0.95, 5.49, 7.92, 8.04, 8.81, 6.65, 8.93, 0.55, 6.73, 3.38, 4.42, 8.23, 12.2, 14.45, 2.82, 5.58, 8.74, 14.14, 5.74, 4.59, 14.54, 6.65, 4.21, 8.71, 1.76, 6.22, 8.87, 10.3, 9.18, 5.05, 5.44, 4.86, 3.25, 4.59, 12.01, 6.69, 6.3, 6.85, 5.45, 15.43, -0.9, 3.43, 9.83, 1.04, 1.16, 16.7, 9.16, 8.46, 7.81, 4.97, 7.46, 1.49, 8.01, 9.48, 9.43, 3.92, 6.2, 7.63, 8.56, 11.53, 9.98, 2.49, 5.67, 3.48, 7.92, 8.62, 7.44, 6.35, 10.88, 9.74, 3.79, 15.43, 6.56, 2.5, 6.66, 9.75, 12.72, 14.64, 8.9, 10.74, 5.93, 2.53, 3.69, 15.25, 0.5, 11.8, 13.19, 6.05, -1.26, 9.09, 9.78, 7.23, 11.67, 12.54, -0.36, 9.4, 7.87, 13.46, 9.33, 2.55, 9.23, 5.95, 10.46, 3.39, 3.81, 7.25, 3.94, 10.18, 8.63, 11.51, 2.42, 9.44, 5.95, 7.75, 10.16, 16.11, 5.16, 3.13, 7.75, 9.96, 7.27, 14.62, 3.88, 10.2, 5.86, 16.18, 5.4, 1.55, 2.91, 9.16, 9.77, 2.25, 5.01, 8.79, 3.34, 7.09, 8.18, 3.34, 8.02, 8.12, 6.69, 3.22, 8.15, 5.01, 11.51, 6.62, 7.07, 0.52, 10.26, 7.99, 8.98, 9.87), 
                  x0=c(0.9, 0.27, 0.37, 0.57, 0.91, 0.2, 0.9, 0.94, 0.66, 0.63, 0.06, 0.21, 0.18, 0.69, 0.38, 0.77, 0.5, 0.72, 0.99, 0.38, 0.78, 0.93, 0.21, 0.65, 0.13, 0.27, 0.39, 0.01, 0.38, 0.87, 0.34, 0.48, 0.6, 0.49, 0.19, 0.83, 0.67, 0.79, 0.11, 0.72, 0.41, 0.82, 0.65, 0.78, 0.55, 0.53, 0.79, 0.02, 0.48, 0.73, 0.69, 0.48, 0.86, 0.44, 0.24, 0.07, 0.1, 0.32, 0.52, 0.66, 0.41, 0.91, 0.29, 0.46, 0.33, 0.65, 0.26, 0.48, 0.77, 0.08, 0.88, 0.34, 0.84, 0.35, 0.33, 0.48, 0.89, 0.86, 0.39, 0.78, 0.96, 0.43, 0.71, 0.4, 0.33, 0.76, 0.2, 0.71, 0.12, 0.25, 0.14, 0.24, 0.06, 0.64, 0.88, 0.78, 0.8, 0.46, 0.41, 0.81, 0.6, 0.65, 0.35, 0.27, 0.99, 0.63, 0.21, 0.13, 0.48, 0.92, 0.6, 0.98, 0.73, 0.36, 0.43, 0.15, 0.01, 0.72, 0.1, 0.45, 0.64, 0.99, 0.5, 0.48, 0.17, 0.75, 0.45, 0.51, 0.21, 0.23, 0.6, 0.57, 0.08, 0.04, 0.64, 0.93, 0.6, 0.56, 0.53, 0.99, 0.51, 0.68, 0.6, 0.24, 0.26, 0.73, 0.45, 0.18, 0.75, 0.1, 0.86, 0.61, 0.56, 0.33, 0.45, 0.5, 0.18, 0.53, 0.08, 0.28, 0.21, 0.28, 0.9, 0.45, 0.78, 0.88, 0.41, 0.06, 0.34, 0.72, 0.34, 0.63, 0.84, 0.86, 0.39, 0.38, 0.9, 0.64, 0.74, 0.61, 0.9, 0.29, 0.19, 0.89, 0.5, 0.88, 0.19, 0.76, 0.72, 0.94, 0.55, 0.71, 0.39, 0.1, 0.93, 0.28, 0.59, 0.11, 0.84, 0.32),
                  x1=c(0.78, 0.27, 0.22, 0.52, 0.27, 0.18, 0.52, 0.56, 0.13, 0.26, 0.72, 0.96, 0.1, 0.76, 0.95, 0.82, 0.31, 0.65, 0.95, 0.95, 0.34, 0.26, 0.17, 0.32, 0.51, 0.92, 0.51, 0.31, 0.05, 0.42, 0.85, 0.35, 0.13, 0.37, 0.63, 0.39, 0.69, 0.69, 0.55, 0.43, 0.45, 0.31, 0.58, 0.91, 0.14, 0.42, 0.21, 0.43, 0.13, 0.46, 0.94, 0.76, 0.93, 0.47, 0.6, 0.48, 0.11, 0.25, 0.5, 0.37, 0.93, 0.52, 0.32, 0.28, 0.79, 0.7, 0.17, 0.06, 0.75, 0.62, 0.17, 0.06, 0.11, 0.38, 0.17, 0.3, 0.19, 0.26, 0.18, 0.48, 0.77, 0.03, 0.53, 0.88, 0.37, 0.05, 0.14, 0.32, 0.15, 0.13, 0.22, 0.23, 0.13, 0.98, 0.33, 0.51, 0.68, 0.1, 0.12, 0.05, 0.93, 0.67, 0.09, 0.49, 0.46, 0.38, 0.99, 0.18, 0.81, 0.07, 0.4, 0.14, 0.19, 0.84, 0.72, 0.27, 0.5, 0.08, 0.35, 0.97, 0.62, 0.66, 0.31, 0.41, 1, 0.86, 0.95, 0.81, 0.78, 0.27, 0.76, 0.99, 0.29, 0.4, 0.81, 0.08, 0.36, 0.44, 0.16, 0.58, 0.97, 0.99, 0.18, 0.54, 0.38, 0.68, 0.27, 0.47, 0.17, 0.37, 0.73, 0.49, 0.06, 0.78, 0.42, 0.98, 0.28, 0.85, 0.08, 0.89, 0.47, 0.11, 0.33, 0.84, 0.28, 0.59, 0.84, 0.07, 0.7, 0.7, 0.46, 0.44, 0.56, 0.93, 0.23, 0.22, 0.42, 0.33, 0.86, 0.18, 0.49, 0.43, 0.56, 0.66, 0.98, 0.23, 0.24, 0.8, 0.83, 0.11, 0.96, 0.15, 0.14, 0.93, 0.51, 0.15, 0.35, 0.66, 0.31, 0.35), 
                  x2=c(0.15, 0.66, 0.19, 0.95, 0.9, 0.94, 0.72, 0.37, 0.78, 0.01, 0.94, 0.99, 0.36, 0.75, 0.79, 0.71, 0.48, 0.49, 0.31, 0.7, 0.82, 0.43, 0.51, 0.66, 0.14, 0.34, 0.41, 0.09, 0.93, 0.84, 0.88, 0.94, 0.07, 0.38, 0.54, 0.11, 0.8, 0.74, 0.05, 0.48, 0.92, 0.04, 0.29, 0.5, 0.61, 0.26, 0.42, 0.37, 0.94, 0.12, 0.07, 0.96, 0.44, 0.37, 0.14, 0.05, 0.66, 0.58, 0.99, 0.6, 0.06, 0.16, 0.48, 0, 0.44, 0.26, 0.94, 0.72, 0.16, 0.48, 0.69, 0.46, 0.96, 0.71, 0.4, 0.12, 0.24, 0.86, 0.44, 0.5, 0.69, 0.76, 0.16, 0.85, 0.95, 0.59, 0.5, 0.19, 0, 0.88, 0.13, 0.02, 0.94, 0.29, 0.16, 0.4, 0.46, 0.43, 0.52, 0.85, 0.06, 0.55, 0.69, 0.66, 0.66, 0.47, 0.97, 0.4, 0.85, 0.76, 0.53, 0.87, 0.47, 0.01, 0.73, 0.72, 0.19, 0.65, 0.54, 0.34, 0.64, 0.83, 0.71, 0.35, 0.13, 0.39, 0.93, 0.8, 0.76, 0.96, 0.99, 0.61, 0.03, 0.34, 0.28, 0.12, 0.04, 0.37, 0.34, 0.17, 0.62, 0.4, 0.96, 0.65, 0.33, 0.2, 0.12, 1, 0.38, 0.56, 0.73, 0.87, 0.57, 0.01, 0.91, 0.77, 0.38, 0.09, 0.05, 0.82, 0.83, 0.65, 0.13, 0.34, 0.73, 0.91, 0.7, 0.24, 0.64, 0.28, 0.96, 0.16, 0.42, 0.25, 0.09, 0.83, 0.53, 0.67, 0.41, 0.84, 0.74, 0.35, 0.95, 0.65, 0.04, 0.6, 0.42, 0.08, 0.53, 0.96, 0.71, 0.55, 0.24, 0.78, 0.65, 0.83, 0.65, 0.48, 0.5, 0.38), 
                  x3=c(0.45, 0.81, 0.93, 0.15, 0.75, 0.98, 0.97, 0.35, 0.39, 0.95, 0.11, 0.93, 0.35, 0.53, 0.54, 0.71, 0.41, 0.15, 0.34, 0.63, 0.06, 0.85, 0.21, 0.77, 0.14, 0.32, 0.62, 0.26, 0.63, 0.49, 0.94, 0.86, 0.37, 0.31, 0.83, 0.45, 0.32, 0.1, 0.06, 0.69, 0.67, 0.9, 0.3, 0.93, 0.2, 0.79, 0.22, 0.03, 0.86, 0.69, 0.94, 0.68, 0.84, 0.36, 0.39, 0.57, 0.1, 0.19, 0.59, 0.75, 0.87, 0.37, 0.8, 0.06, 0.62, 0.36, 0.59, 0.91, 0.2, 0.37, 0.67, 0.77, 0.52, 0.83, 0.53, 0.5, 0.42, 0.36, 0.12, 0.3, 0.28, 0.79, 0.78, 0.14, 0.52, 0.6, 0.51, 0.39, 0.43, 0.01, 0.92, 0.08, 0.51, 0.82, 0.6, 0.42, 0.56, 0.79, 0.17, 0.97, 0.47, 0.93, 0.9, 0.75, 0.68, 0.65, 0.07, 0.42, 0.53, 0.94, 0.71, 0.72, 0.47, 0.12, 0.78, 0.44, 0.43, 0.03, 0.15, 0.42, 0.77, 0, 0.6, 0.91, 0.71, 0.26, 0.85, 0.33, 0.58, 0.43, 0.05, 0.73, 0.55, 0.75, 0.05, 0.71, 0.3, 0.28, 0.83, 0.09, 0.04, 0.35, 0.54, 0.61, 0.27, 0.21, 0.38, 0.47, 0.84, 0.12, 0.68, 0.5, 0.9, 0.55, 0.13, 0.44, 0.19, 0.43, 0.23, 0.96, 0.45, 0.78, 0.16, 0.87, 0.21, 0.18, 0.16, 0.57, 0.73, 0.88, 0.71, 0.48, 0.82, 0.02, 1, 0.63, 0.43, 0.03, 0.75, 0.21, 1, 0.91, 0.71, 0.73, 0.47, 0.86, 0.17, 0.62, 0.29, 0.46, 0.05, 0.18, 0.06, 0.94, 0.34, 0.52, 0.63, 0.24, 0.52, 0.81)
                  )
str(dat)


# Fit the model
mod <- gam(y ~ s(x0, k = 5) + s(x1, k = 5) + s(x2, k = 5) + s(x3, k = 5),
           data = dat, method = "REML")

# Run the check function
gam.check(mod)


# Fit the model
mod <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + s(x2, k = 3) + s(x3, k = 3),
           data = dat, method = "REML")

# Check the diagnostics
gam.check(mod)


# Refit to fix issues
mod2 <- gam(y ~ s(x0, k = 3) + s(x1, k = 3) + s(x2, k = 12) + s(x3, k = 3),
           data = dat, method = "REML")

# Check the new model
gam.check(mod2)


# Fit the model
mod <- gam(hw.mpg ~ s(length) + s(width) + s(height) + s(weight), 
           data = mpg, method = "REML")

# Check overall concurvity
concurvity(mod, full=TRUE)

# Check pairwise concurvity
concurvity(mod, full=FALSE)

```
  
  
  
***
  
Chapter 3 - Spatial GAMs and Interactions  
  
Two-dimensional smooths and spatial data:  
  
* Can expand models to include smooths of multiple variables, including their interactions  
	* y = s(x1, x2)  
    * gam(y ~ s(x1, x2), data = dat, method = "REML")  
    * gam(y ~ s(x1, x2) + s(x3) + x4, data = dat, method = "REML")  
* Will use the meuse dataset, which is about heavy metals in the soil near a river  
	* ?sp::meuse  
  
Plotting and interpreting GAM interactions:  
  
* Can see interactions using plot(mod_2d)  
	* plot(mod_2d)  
    * Creates a topographic map of predicted values  
    * plot(mod_2d, scheme = 1)  # will show the 3D perspective plot  
    * plot(mod_2d, scheme = 2)  # heat map  
    * vis.gam(x, view = NULL, cond = list(), n.grid = 30, too.far = 0, col = NA, color = "heat", contour.col = NULL, se = -1, type = "link", plot.type = "persp", zlim = NULL, nCol = 50, ...) # customizes plots  
* Can run custom plots using the vis.gam() functions  
	* vis.gam(x = mod, view = c("x1", "x2"), plot.type = "persp")  # scheme=1  
    * vis.gam(x = mod, view = c("x1", "x2"), plot.type = "contour")  # scheme=2  
    * vis.gam(mod, view = c("x1", "x2"), plot.type = "contour", too.far = 0.1)  # set a range for not making predictions due to distance from training data  
    * vis.gam(x = mod, view = c("x1", "x2"), plot.type = "persp", se = 2)  # see confidence intervals for the plots  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "persp", theta = 220)  # horizontal rotation  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "persp", phi = 55)  # vertical rotation  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "persp", r = 0.1)  # zoom level (low r can lead to distortions or parallax)  
* Additional options are available for contour plots  
	* vis.gam(g, view = c("x1", "x2"), plot.type = "contour", color = "gray")  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "contour", contour.col = "blue")  
    * vis.gam(g, view = c("x1", "x2"), plot.type = "contour", nlevels = 20)  
  
Visualizing categorical-continuous interactions:  
  
* The categorical-continuous interaction was previously modeled using s(x1, by="x2")  
* Can instead use a factor smooth with argument bs="fs" (best for controlling for categories that may have an impact but are not the primary categories of interest)  
	* model4c <- gam(hw.mpg ~ s(weight, fuel, bs = "fs") + fuel, data = mpg, method = "REML")  
    * plot(model4c)  
    * vis.gam(model4c, theta = 125, plot.type = "persp")  
  
Interactions with different scales: Tensors:  
  
* Tensor smooths allow for interactions on different scales, such as space and time  
* Within the meuse data, horizontal and vertical dstances may have very different wiggliness (incomparable impacts)  
* A tensor has two smoothing parameters, one for each of it variables  
	* gam(y ~ te(x1, x2), data = data, method = "REML")  
    * gam(y ~ te(x1, x2, k = c(10, 20)), data = data, method = "REML")  
* Tensor smooths can also help to tease out interactions and independent effects  
	* gam(y ~ te(x1) + te(x2) + ti(x1, x2), data = data, method = "REML")  
    * gam(y ~ s(x1) + s(x2) + ti(x1, x2), data = data, method = "REML")  
  
Example code includes:  
```{r}

# Inspect the data
data(meuse, package="sp")
head(meuse)
str(meuse)


# Fit the 2-D model
mod2d <- gam(cadmium ~ s(x, y), data=meuse, method="REML")

# Inspect the model
summary(mod2d)
coef(mod2d)


# Models of this form (s(x,y) + s(v1) + ...) are a great way to model spatial data because they incorporate spatial relationships as well as independent predictors

# Fit the model
mod2da <- gam(cadmium ~ s(x, y) +s(elev) + s(dist), 
              data = meuse, method = "REML")

# Inspect the model
summary(mod2da)

# Contour plot
plot(mod2da, pages = 1)

# 3D surface plot
plot(mod2da, scheme=1, pages = 1)

# Colored heat map
plot(mod2da, scheme=2, pages=1)


# Make the perspective plot with error surfaces
vis.gam(mod2d, view = c("x", "y"), plot.type="persp", se=2)

# Rotate the same plot
vis.gam(mod2d, view = c("x", "y"), plot.type="persp", se=2, theta=135)

# Make plot with 5% extrapolation
vis.gam(mod2d, view = c("x", "y"), plot.type = "contour", too.far=0.05)

# Overlay data
points(meuse)


# Make plot with 10% extrapolation
vis.gam(mod2d, view = c("x", "y"), plot.type="contour", too.far=0.1)

# Overlay data
points(meuse)

# Make plot with 25% extrapolation
vis.gam(mod2d, view = c("x", "y"), 
        plot.type = "contour", too.far = 0.25)

# Overlay data
points(meuse)


# Fit a model with separate smooths for each land-use level
mod_sep <- gam(copper ~ s(dist, by = landuse), data = meuse, method = "REML")

# Examine the summary
summary(mod_sep)

# Fit a model with a factor-smooth interaction
mod_fs <- gam(copper ~ s(dist, landuse, bs="fs"), data = meuse, method = "REML")

# Examine the summary
summary(mod_fs)


# Plot both the models with plot()
plot(mod_sep, pages=1)
plot(mod_fs, pages=1)

# Plot both the models with vis.gam()
vis.gam(mod_sep, view = c("dist", "landuse"), plot.type = "persp")
vis.gam(mod_fs, view = c("dist", "landuse"), plot.type = "persp")


# Fit the model
tensor_mod <- gam(cadmium ~ te(x, y, elev), data=meuse, method="REML")

# Summarize and plot
summary(tensor_mod)
plot(tensor_mod)


# Fit the model
tensor_mod2 <- gam(cadmium ~ ti(x, y) + te(elev) + ti(x, y, elev), data=meuse, method="REML")

# Summarize and plot
summary(tensor_mod2)
plot(tensor_mod2, pages = 1)

par(mfrow=c(1, 1))

```
  
  
  
***
  
Chapter 4 - Logistic GAM for Classification  
  
Types of model outcome:  
  
* Can also use GAM for making classifications (binary regression)  
	* GAM output is converted to probability using the logistic function - log-odds - plogis()  
    * The logit function converts probabilities to log-odds - qlogis()  
    * gam(y ~ x1 + s(x2), data = dat, family = binomial, # Add for binary outcomes method = "REML")  
* The csale dataset has anonymized insurance data  
  
Visualizing logistic GAMs:  
  
* Can plot the log-odds  
	* plot(binom_mod)  # on the log-odds scale  
    * plot(binom_mod, pages = 1, trans = plogis)  # convert to probability scale  
    * plot(binom_mod, pages = 1, trans = plogis, shift = coef(binom_mod)[1])  # add the intercept  
    * plot(binom_mod, pages = 1, trans = plogis, shift = coef(binom_mod)[1], seWithMean = TRUE)  # adds intercept uncertainty to smooth uncertainty  
    * plot(binom_mod, pages = 1, trans = plogis, shift = coef(binom_mod)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgreen", col = "purple")  
  
Making predictions:  
  
* Can use the fitted models for making predictions  
	* predict(log_mod2)  # vector of predictions for the model data; returned on the link scale by default (log-odds for logistic)  
    * predict(log_mod2, type="response")  # return on the probability scale  
    * predict(log_mod2, type = "link", se.fit = TRUE)  # first element of the list is the predictions and second element is approx. SE  
* For standard errors, use the link (log-odds) scale, then convert to the probability scales  
* Can also make predictions based on new data  
	* test_predictions <- predict(trained_model, type = "response", newdata = test_df)  
    * predict(log_mod2, type = "terms")  # will show a column for the impact of each of the smooths (sum across columns plus intercept would be prediction)  
    * plogis( sum(predict(log_mod2, type = "terms")[1, ]) + coef(log_mod2)[1] )  
  
Wrap up and next steps:  
  
* Basic theory of smooths for GAMs  
* Interpreting GAMs and plotting partial effects  
* Building and visualizing GAMs with interactions  
* Logistic GAMs for binary classification and predictions  
* Can extend to using the Tidyverse tools - broom, caret, etc.  
* Can further extend the smooths - see the help files in mgcv  
  
Example code includes:  
```{r}

csale <- readRDS("./RInputFiles/csale.rds")

# Examine the csale data frame
head(csale)
str(csale)


# Fit a logistic model
log_mod <- gam(purchase ~ s(mortgage_age), data = csale, family=binomial, method = "REML")

# Fit a logistic model
log_mod2 <- gam(purchase ~ s(n_acts) + s(bal_crdt_ratio) + s(avg_prem_balance) + 
    s(retail_crdt_ratio) + s(avg_fin_balance) + s(mortgage_age) + 
    s(cred_limit), data = csale, family = binomial, method = "REML")

# View the summary
summary(log_mod2)


# Plot on the log-odds scale
plot(log_mod2, pages=1)

# Plot on the probability scale
plot(log_mod2, pages = 1, trans = plogis)

# Plot with the intercept
plot(log_mod2, pages = 1, trans = plogis, shift = coef(log_mod2)[1])

# Plot with intercept uncertainty
plot(log_mod2, pages = 1, trans = plogis, shift = coef(log_mod2)[1], seWithMean = TRUE)


new_credit_data <- data.frame(matrix(data=c(1, 0, 0, 0, 0, 0, 0, 2, 19, 0, 0, 1, 6, 3, 0.3, 4.2, 36.095, 36.095, 25.7, 45.6, 10.8, 61, 967, 2494.414, 2494.414, 2494.414, 195, 2494.414, 11.491, 0, 11.491, 11.491, 11.491, 0, 11.491, 1767.197, 249, 1767.197, 1767.197, 1767.197, 0, 1767.197, 155, 65, 138.96, 138.96, 138.96, 13, 138.96, 0, 10000, 0, 0, 0, 13800, 0), ncol=8, nrow=7, byrow=FALSE))
names(new_credit_data) <- c('purchase', 'n_acts', 'bal_crdt_ratio', 'avg_prem_balance', 'retail_crdt_ratio', 'avg_fin_balance', 'mortgage_age', 'cred_limit')
new_credit_data


# Calculate predictions and errors
predictions <- predict(log_mod2, newdata = new_credit_data, 
                       type = "link", se.fit = TRUE)

# Calculate high and low predictions intervals
high_pred <- predictions$fit + 2*predictions$se.fit
low_pred <- predictions$fit - 2*predictions$se.fit

# Convert intervals to probability scale
high_prob <- 1 / (1/exp(high_pred) + 1)
low_prob <- 1 / (1/exp(low_pred) + 1)

# Inspect
high_prob
low_prob


# Predict from the model
prediction_1 <- predict(log_mod2, newdata = new_credit_data[1, ,drop=FALSE], type = "terms")

# Inspect
prediction_1

```
  
  
  
***
  
### _Machine Learning in the Tidyverse_  
  
Chapter 1 - Foundations of Tidy Machine Learning  
  
Introduction:  
  
* Tidyverse tools center around the tibble, which includes a "list" column that can store complex objects  
* There is also a list-column workflow of nest() - map() - unnest()  
* Course will use a more granular form of the gapminder dataset - 77 countries x 52 years x 6 features per observation  
* The nest makes each portion of the data in to a row of a new tibble  
	* nested <- gapminder %>% group_by(country) %>% nest()  # new column data is created; will contain the relevant data  
    * nested$data[[4]]  # will show the Austria data  
    * nested %>% unnest(data)  # will convert back to a normal tibble, using column data  
  
Map family of functions:  
  
* The map family of functions help to fulfill the second and third elements of the workflows  
* The map functions are all of the form map_(.x=, .f=)  
	* .x is the vector or list  
    * .f is the function  
    * output will be a list  
    * .f = ~mean(.x) and .f=mean will do the same thing  
* Can use the map functions on a nested file  
	* map(.x = nested$data, .f = ~mean(.x$population))  
    * pop_df <- nested %>% mutate(pop_mean = map(data, ~mean(.x$population)))  
    * pop_df %>% unnest(pop_mean)  # gets back the numbers rather than a column of lists  
* Can also use a map function that specifies a requested vector return of a specific type or a list of models such as an lm  
	* nested %>% mutate(pop_mean = map_dbl(data, ~mean(.x$population)))  # return will be a vector of doubles rather than a list  
    * nested %>% mutate(model = map(data, ~lm(formula = population~fertility, data = .x)))  
  
Tidy models with broom:  
  
* Several packages can help with the analysis of the list columns - broom, yardstick, rsample, etc.  
* The core of the broom is to extract relevant data from models  
	* tidy - statistical fundings such as coefficients  
    * glance - concise one-row summary  
    * augment - adds prediction columns to the data being modeled  
* Example of using tidy to extract data from the lm() used previously in the chapter  
	* tidy(algeria_model)  
    * glance(algeria_model)  
    * augment(algeria_model)  
* Plotting the augmented data can give a sense for model fits - predictions vs. actuals  
	* augment(algeria_model) %>% ggplot(mapping = aes(x = year)) + geom_point(mapping = aes(y = life_expectancy)) + geom_line(mapping = aes(y = .fitted), color = "red")  
  
Example code includes:  
```{r eval=FALSE}

# Explore gapminder
data(gapminder, package="gapminder")
head(gapminder)

# Prepare the nested dataframe gap_nested
gap_nested <- gapminder %>% 
  group_by(country) %>% 
  nest()

# Explore gap_nested
head(gap_nested)


# Create the unnested dataframe called gap_unnnested
gap_unnested <- gap_nested %>% 
  unnest()
  
# Confirm that your data was not modified  
identical(gapminder, gap_unnested)


# Extract the data of Algeria
algeria_df <- gap_nested$data[[which(gap_nested$country=="Algeria")]]

# Calculate the minimum of the population vector
min(algeria_df$pop)

# Calculate the maximum of the population vector
max(algeria_df$pop)

# Calculate the mean of the population vector
mean(algeria_df$pop)


# Calculate the mean population for each country
pop_nested <- gap_nested %>%
  mutate(mean_pop = map(.x=data, .f=~mean(.x$pop)))

# Take a look at pop_nested
head(pop_nested)

# Extract the mean_pop value by using unnest
pop_mean <- pop_nested %>% 
  unnest(mean_pop)

# Take a look at pop_mean
head(pop_mean)


# Calculate mean population and store result as a double
pop_mean <- gap_nested %>%
  mutate(mean_pop = map_dbl(.x=data, ~mean(.x$pop)))

# Take a look at pop_mean
head(pop_mean)


# Build a linear model for each country
gap_models <- gap_nested %>%
    mutate(model = map(.x=data, .f=~lm(formula = lifeExp ~ year, data = .x)))
    
# Extract the model for Algeria    
algeria_model <- gap_models$model[[which(gap_models$country=="Algeria")]]

# View the summary for the Algeria model
summary(algeria_model)


# Extract the coefficients of the algeria_model as a dataframe
broom::tidy(algeria_model)

# Extract the statistics of the algeria_model as a dataframe
broom::glance(algeria_model)


# Build the augmented dataframe
algeria_fitted <- broom::augment(algeria_model)

# Compare the predicted values with the actual values of life expectancy
algeria_fitted %>% 
  ggplot(aes(x = year)) +
  geom_point(aes(y = lifeExp)) + 
  geom_line(aes(y = .fitted), color = "red")

```
  
  
  
***
  
Chapter 2 - Multiple Models with broom  
  
Exploring coefficients across models:  
  
* The gapminder models contains data about all of the 77 countries  
	* gap_nested <- gapminder %>% group_by(country) %>% nest()  
    * gap_models <- gap_nested %>% mutate(model = map(data, ~lm(life_expectancy~year, data = .x)))  
* Can extract regression coefficients to better understand trends in the gapminder data  
	* gap_models %>% mutate(coef = map(model, ~tidy(.x))) %>% unnest(coef)  
  
Evaluating fit of many models:  
  
* Can also look at the R-squared across many models  
	* gap_models %>% mutate(fit = map(model, ~glance(.x))) %>% unnest(fit)  
  
Visually inspect the fit of many models:  
  
* Can compare predicted and actual observations from the gapminder dataset  
	* augmented_models <- gap_models %>% mutate(augmented = map(model,~augment(.x))) %>% unnest(augmented)  
    * augmented_model %>% filter(country == "Italy") %>% ggplot(aes(x = year, y = life_expectancy)) + geom_point() + geom_line(aes(y = .fitted), color = "red")  
  
Improve the fit of your models:  
  
* Can instead use multiple linear regressions and again explore the goodness of fits  
	* gap_fullmodels <- gap_nested %>% mutate(model = map(data, ~lm(formula = life_expectancy ~ ., data = .x)))  
    * tidy(gap_fullmodels$model[[1]])  
    * augment(gap_fullmodels$model[[1]])  
    * glance(gap_fullmodels$model[[1]])  
  
Example code includes:  
```{r eval=FALSE}

# Extract the coefficient statistics of each model into nested dataframes
model_coef_nested <- gap_models %>% 
    mutate(coef = map(.x=model, .f=~broom::tidy(.x)))
    
# Simplify the coef dataframes for each model    
model_coef <- model_coef_nested %>%
    unnest(coef)

# Plot a histogram of the coefficient estimates for year         
model_coef %>% 
  filter(term=="year") %>% 
  ggplot(aes(x = estimate)) +
  geom_histogram()


# Extract the fit statistics of each model into dataframes
model_perf_nested <- gap_models %>% 
    mutate(fit = map(.x=model, .f=~broom::glance(.x)))

# Simplify the fit dataframes for each model    
model_perf <- model_perf_nested %>% 
    unnest(fit)
    
# Look at the first six rows of model_perf
head(model_perf)


# Plot a histogram of rsquared for the 77 models    
model_perf %>% 
  ggplot(aes(x=r.squared)) + 
  geom_histogram() 
  
# Extract the 4 best fitting models
best_fit <- model_perf %>% 
  top_n(n = 4, wt = r.squared)

# Extract the 4 models with the worst fit
worst_fit <- model_perf %>% 
  top_n(n = 4, wt = -r.squared)


best_augmented <- best_fit %>% 
  # Build the augmented dataframe for each country model
  mutate(augmented = map(.x=model, .f=~broom::augment(.x))) %>% 
  # Expand the augmented dataframes
  unnest(augmented)

worst_augmented <- worst_fit %>% 
  # Build the augmented dataframe for each country model
  mutate(augmented = map(.x=model, .f=~broom::augment(.x))) %>% 
  # Expand the augmented dataframes
  unnest(augmented)


# Compare the predicted values with the actual values of life expectancy 
# for the top 4 best fitting models
best_augmented %>% 
  ggplot(aes(x=year)) +
  geom_point(aes(y=lifeExp)) + 
  geom_line(aes(y=.fitted), color = "red") +
  facet_wrap(~country, scales = "free_y")

# Compare the predicted values with the actual values of life expectancy 
# for the top 4 worst fitting models
worst_augmented %>% 
  ggplot(aes(x=year)) +
  geom_point(aes(y=lifeExp)) + 
  geom_line(aes(y=.fitted), color = "red") +
  facet_wrap(~country, scales = "free_y")


# Build a linear model for each country using all features
gap_fullmodel <- gap_nested %>% 
  mutate(model = map(data, ~lm(formula = lifeExp ~ year + pop + gdpPercap, data = .x)))

fullmodel_perf <- gap_fullmodel %>% 
  # Extract the fit statistics of each model into dataframes
  mutate(fit = map(model, ~broom::glance(.x))) %>% 
  # Simplify the fit dataframes for each model
  unnest(fit)
  
# View the performance for the four countries with the worst fitting 
# four simple models you looked at before
fullmodel_perf %>% 
  filter(country %in% worst_fit$country) %>% 
  select(country, adj.r.squared)

```
  
  
  
***
  
Chapter 3 - Build, Tune, and Evaluate Regression Models  
  
Training, test, and validation splits:  
  
* Questions of how well the data would perform on new (unseen) data  
* The train-test split can be helpful for assessing out-of-sample performance - rsample has a function for this, with prop= being the proportion in the test data  
	* library(rsample)  
    * gap_split <- initial_split(gapminder, prop = 0.75)  
    * training_data <- training(gap_split)  
    * testing_data <- testing(gap_split)  
* The train data can be further split in to train and validate, with validate being used in model building (cross-validation)  
	* cv_split <- vfold_cv(training_data, v = 3)  # v=3 is for 3-fold cross-validation  
    * cv_data <- cv_split %>% mutate(train = map(splits, ~training(.x)), validate = map(splits, ~testing(.x)))  
  
Measuring cross-validation performance:  
  
* Can compare predictions made on the test or validate data to the actual values in the same datasets  
* The MAE captures the average magnitude of differences in the predictions and the actuals  
	* cv_prep_lm <- cv_models_lm %>% mutate(validate_actual = map(validate, ~.x$life_expectancy))  
* There is also a map2(.x=, .y=, .f=) function that allows for two input columns  
	* cv_prep_lm <- cv_eval_lm %>% mutate(validate_actual = map(validate, ~.x$life_expectancy), validate_predicted = map2(model, validate, ~predict(.x, .y)))  
    * library(Metrics)  
    * cv_eval_lm <- cv_prep_lm %>% mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))  
  
Building and tuning a random-forest model:  
  
* MAE for the gapminder model above is roughly 1.5 years  
* Can instead try a random forest model - can handle non-linear relationships and interactions  
	* rf_model <- ranger(formula = ___, data = ___, seed = ___)  
    * prediction <- predict(rf_model, new_data)$predictions  
    * rf_model <- ranger(formula, data, seed, mtry, num.trees)  # tuning the hyper-parameters  
* Can tune the mtry parameter using the crossing() function for a tidyverse approach  
	* cv_tune <- cv_data %>% crossing(mtry = 1:5)  # expands the frame for each hyperparameter of interest  
    * cv_model_tunerf <- cv_tune %>% mutate(model = map2(train, mtry, ~ranger(formula = life_expectancy~., data = .x, mtry = .y)))  
  
Measuring the test performance:  
  
* Can compare multiple models, including hyper-parameters  
* The final model is then built using ALL the training data, with the OOB error estimate based on the held-out test data  
	* best_model <- ranger(formula = life_expectancy~., data = training_data, mtry = 2, num.trees = 100, seed = 42)  
    * test_actual <- testing_data$life_expectancy  
    * test_predict <- predict(best_model, testing_data)$predictions  
    * mae(test_actual, test_predict)  
  
Example code includes:  
```{r eval=FALSE}

set.seed(42)

# Prepare the initial split object
gap_split <- rsample::initial_split(gapminder, prop = 0.75)

# Extract the training dataframe
training_data <- rsample::training(gap_split)

# Extract the testing dataframe
testing_data <- rsample::testing(gap_split)

# Calculate the dimensions of both training_data and testing_data
dim(training_data)
dim(testing_data)


set.seed(42)

# Prepare the dataframe containing the cross validation partitions
cv_split <- rsample::vfold_cv(training_data, v = 5)

cv_data <- cv_split %>% 
  mutate(
    # Extract the train dataframe for each split
    train = map(splits, ~rsample::training(.x)), 
    # Extract the validate dataframe for each split
    validate = map(splits, ~rsample::testing(.x))
  )

# Use head() to preview cv_data
head(cv_data)


# Build a model using the train data for each fold of the cross validation
cv_models_lm <- cv_data %>% 
  mutate(model = map(train, ~lm(formula = lifeExp ~ ., data = .x)))


cv_prep_lm <- cv_models_lm %>% 
  mutate(
    # Extract the recorded life expectancy for the records in the validate dataframes
    validate_actual = map(.x=validate, .f=~.x$lifeExp),
    # Predict life expectancy for each validate set using its corresponding model
    validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y))
  )


library(Metrics)
# Calculate the mean absolute error for each validate fold       
cv_eval_lm <- cv_prep_lm %>% 
  mutate(validate_mae = map2_dbl(.x=validate_actual, .y=validate_predicted, 
                                 .f=~mae(actual = .x, predicted = .y)
                                 )
         )

# Print the validate_mae column
cv_eval_lm$validate_mae

# Calculate the mean of validate_mae column
mean(cv_eval_lm$validate_mae)


library(ranger)

# Build a random forest model for each fold
cv_models_rf <- cv_data %>% 
  mutate(model = map(train, ~ranger(formula = lifeExp ~ ., data = .x,
                                    num.trees = 100, seed = 42)))

# Generate predictions using the random forest model
cv_prep_rf <- cv_models_rf %>% 
  mutate(validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y)$predictions))


# Calculate validate MAE for each fold
cv_eval_rf <- cv_prep_rf %>% 
  mutate(validate_actual=map(.x=validate, .f=~.x$lifeExp), 
         validate_mae = map2_dbl(.x=validate_actual, .y=validate_predicted, 
                                 .f=~mae(actual = .x, predicted = .y)
                                 )
         )

# Print the validate_mae column
cv_eval_rf$validate_mae

# Calculate the mean of validate_mae column
mean(cv_eval_rf$validate_mae)


# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>% 
  tidyr::crossing(mtry = 2:5) 

# Build a model for each fold & mtry combination
cv_model_tunerf <- cv_tune %>% 
  mutate(model = map2(.x=train, .y=mtry, ~ranger(formula = lifeExp ~ ., 
                                           data = .x, mtry = .y, 
                                           num.trees = 100, seed = 42)))


# Generate validate predictions for each model
cv_prep_tunerf <- cv_model_tunerf %>% 
  mutate(validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y)$predictions))

# Calculate validate MAE for each fold and mtry combination
cv_eval_tunerf <- cv_prep_tunerf %>% 
  mutate(validate_actual=map(.x=validate, .f=~.x$lifeExp),
         validate_mae = map2_dbl(.x=validate_actual, .y=validate_predicted, 
                                 .f=~mae(actual = .x, predicted = .y)
                                 )
         )

# Calculate the mean validate_mae for each mtry used  
cv_eval_tunerf %>% 
  group_by(mtry) %>% 
  summarise(mean_mae = mean(validate_mae))


# Build the model using all training data and the best performing parameter
best_model <- ranger(formula = lifeExp ~ ., data = training_data,
                     mtry = 4, num.trees = 100, seed = 42)

# Prepare the test_actual vector
test_actual <- testing_data$lifeExp

# Predict life_expectancy for the testing_data
test_predicted <- predict(best_model, testing_data)$predictions

# Calculate the test MAE
mae(test_actual, test_predicted)

```
  
  
  
***
  
Chapter 4 - Build, Tune, and Evaluate Classification Models  
  
Logistic Regression Models:  
  
* Binary classification models can also be run using the nest and map approach  
* The dataset of interest is "attrition" based on a fictional employer  
	* glm(formula = ___, data = ___, family = "binomial")  
    * head(cv_data)  
    * cv_models_lr <- cv_data %>% mutate(model = map(train, ~glm(formula = Attrition~., data = .x, family = "binomial")))  
  
Evaluating Classification Models:  
  
* Need actual and predicted classes, plus a relevant metric  
	* validate_prob <- predict(model, validate, type = "response")  
    * table(validate_actual, validate_predicted)  
    * accuracy(validate_actual, validate_predicted)  
    * precision(validate_actual, validate_predicted)  # of observations with predicted==TRUE, how often is actual==TRUE?  
    * recall(validate_actual, validate_predicted)  # of observations with actual==TRUE, how often is predicted==TRUE?  
  
Random Forest for Classification:  
  
* Can tune and build the random-forest model as per previous  
	* cv_tune <- cv_data %>% crossing(mtry = c(2, 4, 8, 16))  
    * cv_models_rf <- cv_tune %>% mutate(model = map2(train, mtry, ~ranger(formula = Attrition~., data = .x, mtry = .y, num.trees = 100, seed = 42)))  
    * validate_classes <- predict(rf_model, rf_validate)$predictions  
    * validate_predicted <- validate_classes == "Yes"  
  
Wrap Up:  
  
* List Column Workflow  
* Explore Models with Broom  
* Build, Tune, and Evaluate Regression Models  
* Build, Tune, and Evaluate Classification Models  
  
Example code includes:  
```{r eval=FALSE}

attrition <- readRDS("./RInputFiles/attrition.rds")
str(attrition)
head(attrition)


set.seed(42)

# Prepare the initial split object
data_split <- rsample::initial_split(data=attrition, prop=0.75)

# Extract the training dataframe
training_data <- rsample::training(data_split)

# Extract the testing dataframe
testing_data <- rsample::testing(data_split)

set.seed(42)
cv_split <- rsample::vfold_cv(training_data, v=5)

cv_data <- cv_split %>% 
  mutate(
    # Extract the train dataframe for each split
    train = map(.x=splits, .f=~rsample::training(.x)),
    # Extract the validate dataframe for each split
    validate = map(.x=splits, .f=~rsample::testing(.x))
  )


# Build a model using the train data for each fold of the cross validation
cv_models_lr <- cv_data %>% 
    mutate(model = map(train, ~glm(formula = Attrition ~ ., data = .x, family = "binomial")))


# Extract the first model and validate 
model <- cv_models_lr$model[[1]]
validate <- cv_models_lr$validate[[1]]

# Prepare binary vector of actual Attrition values in validate
validate_actual <- validate$Attrition == "Yes"

# Predict the probabilities for the observations in validate
validate_prob <- predict(model, validate, type = "response")

# Prepare binary vector of predicted Attrition values for validate
validate_predicted <- validate_prob > 0.5


library(Metrics)

# Compare the actual & predicted performance visually using a table
table(validate_actual, validate_predicted)

# Calculate the accuracy
accuracy(validate_actual, validate_predicted)

# Calculate the precision
precision(validate_actual, validate_predicted)

# Calculate the recall
recall(validate_actual, validate_predicted)


cv_prep_lr <- cv_models_lr %>% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(.x=validate, ~.x$Attrition == "Yes"),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x=model, .y=validate, .f=~predict(.x, .y, type = "response") > 0.5)
  )


# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_lr %>% 
  mutate(validate_recall = map2_dbl(.x=validate_actual, .y=validate_predicted, .f=~recall(actual = .x, predicted = .y)))

# Print the validate_recall column
cv_perf_recall$validate_recall

# Calculate the average of the validate_recall column
mean(cv_perf_recall$validate_recall)


library(ranger)

# Prepare for tuning your cross validation folds by varying mtry
cv_tune <- cv_data %>%
  crossing(mtry = c(2, 4, 8, 16)) 

# Build a cross validation model for each fold & mtry combination
cv_models_rf <- cv_tune %>% 
  mutate(model = map2(train, mtry, ~ranger(formula = Attrition~., 
                                           data = .x, mtry = .y,
                                           num.trees = 100, seed = 42)))


cv_prep_rf <- cv_models_rf %>% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(validate, ~.x$Attrition == "Yes"),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x=model, .y=validate, ~predict(.x, .y, type = "response")$predictions=="Yes")
  )

# Calculate the validate recall for each cross validation fold
cv_perf_recall <- cv_prep_rf %>% 
  mutate(recall = map2_dbl(.x=validate_actual, .y=validate_predicted, ~recall(actual=.x, predicted=.y)))

# Calculate the mean recall for each mtry used  
cv_perf_recall %>% 
  group_by(mtry) %>% 
  summarise(mean_recall = mean(recall))


# Build the logistic regression model using all training data
best_model <- glm(formula = Attrition ~ ., 
                  data = training_data, family = "binomial")


# Prepare binary vector of actual Attrition values for testing_data
test_actual <- testing_data$Attrition == "Yes"

# Prepare binary vector of predicted Attrition values for testing_data
test_predicted <- predict(best_model, newdata=testing_data, type = "response") > 0.5


# Compare the actual & predicted performance visually using a table
table(test_actual, test_predicted)

# Calculate the test accuracy
accuracy(test_actual, test_predicted)

# Calculate the test precision
precision(test_actual, test_predicted)

# Calculate the test recall
recall(test_actual, test_predicted)

```
  
  
  
***
  
### _Predictive Analytics Using Networked Data in R_  
  
Chapter 1 - Introduction, Networks, and Labeled Networks  
  
Introduction:  
  
* Labeled networks and network structure for predicting node attributes  
	* Predicting Age, Gender, Fraud, Churn, etc., for unknown nodes  
* Course includes labeled social networks, homophily, network featurization, predicting using supervised learning  
* Example of course collaborations - network  
	* library(igraph)  
    * DataScienceNetwork <- data.frame( from = c('A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'D', 'E', 'F', 'F', 'G', 'G', 'H', 'H', 'I'), to = c('B','C','D','E','C','D','D', 'G','E', 'F','G','F','G','I', 'I','H','I','J','J'))  
    * g <- graph_from_data_frame(DataScienceNetwork, directed = FALSE)  
    * pos <- cbind(c(2, 1, 1.5, 2.5, 4, 4. 5, 3, 3.5, 5, 6), c(10.5, 9.5, 8, 8.5, 9, 7.5, 6, 4.5, 5.5, 4))  
    * plot.igraph(g, edge.label = NA, edge.color = 'black', layout = pos, vertex.label = V(g)$name, vertex.color = 'white', vertex.label.color = 'black', vertex.size = 25)  
    * V(g)$technology <- c('R','R','?','R','R', 'R','P','P','P','P')  
    * V(g)$color <- V(g)$technology  
    * V(g)$color <- gsub('R',"blue3", V(g)$color)  
    * V(g)$color <- gsub('P',"green4", V(g)$color)  
    * V(g)$color <- gsub('?',"gray", V(g)$color)  
* Will be using the network edgeList which has relationships among customers  
  
Labeled Networks, Social Influence:  
  
* Example of using node attributes "customers" which defines customers as having churned or not  
* Supposing that churn is a social phenomenon, then churn is likely predictable based on connections  
* Can use the relational neighbor classifier  
	* Percentage of neighbors with a certain trait used to predict trait of unlabeled node  
    * rNeighbors <- c(4,3,3,5,3,2,3,0,1,0)  
    * pNeighbors <- c(0,0,1,1,0,2,2,3,3,2)  
    * rRelationalNeighbor <- rNeighbors / (rNeighbors + pNeighbors)  
    * rRelationalNeighbor  
  
Challenges:  
  
* Desire to evaluate models using a test set (out-of-sample performance)  
	* Harder to do with networks, where connections are important (cannot just select 60% of the nodes)  
    * Often managed by training on one network and testing on another  
* Observations may not be iid (in fact, there tend to be strong correlations between connected nodes)  
* Collective inferencing is another challenge  
	* Infer the unknown nodes based on knowledge of how these nodes tend to interact with each other  
  
Example code includes:  
```{r}

library(igraph)


# load("./RInputFiles/StudentEdgelist.RData")

# Create edgeList
elFrom <- c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 7, 7, 8, 8, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 20, 20, 20, 21, 21, 22, 23, 23, 23, 23, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 27, 28, 28, 28, 29, 29, 29, 29, 32, 32, 32, 32, 32, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 37, 37, 583, 38, 38, 39, 39, 40, 40, 40, 40, 343, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 47, 47, 47, 47, 47, 48, 48, 49, 49, 50, 50, 51, 51, 52, 52, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 56, 56, 56, 56, 56, 684, 57, 57, 58, 58, 58, 58, 191, 59, 59, 59, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 62, 63, 63, 63, 64, 64, 64, 64, 64, 64, 99, 65, 66, 66, 67, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 71, 71, 71, 71, 72, 72, 72, 73, 73, 73, 73, 74, 75, 76, 76, 76, 76, 76, 76, 238, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 179, 80, 81, 707, 82, 83, 83, 83, 84, 84, 84, 84, 85, 85, 609, 85, 86, 86, 689, 86, 86, 87, 87, 87, 87, 87, 286, 459, 633, 88, 88, 89, 89, 89, 89, 90, 90, 90, 90, 91, 91, 92, 583, 92, 93, 93, 93, 94, 94, 358, 532, 95, 96, 97, 97, 707, 97, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 807, 976, 101, 101, 101, 102, 102, 102, 102, 102, 784, 103, 104, 104, 105, 105, 106, 106, 106, 106, 106, 107, 583, 294, 109, 109, 110, 110, 111, 111, 111, 112, 112, 278, 113, 114, 114, 114, 114, 114, 115, 115, 115, 406, 116, 116, 116, 116, 267, 388, 117, 117, 117, 117, 117, 118, 358, 118, 119, 119, 119, 119, 119, 343, 120, 120, 121, 121, 121, 138, 207, 122, 122, 122, 122, 122, 889, 123, 123, 123, 123, 123, 124, 125, 125, 125, 125, 125, 125, 126, 126, 793, 889, 127, 128, 128, 128, 128, 128, 129, 707, 129, 337, 130, 130, 131, 131, 131, 131, 132, 133, 133, 133, 133, 532, 135, 406, 135, 186, 136, 364, 139, 139, 736, 467, 141, 288, 317, 142, 142, 142, 976, 143, 143, 143, 144, 144, 144, 889, 988, 145, 145, 146, 146, 147, 288, 148, 149, 149, 149, 661, 149, 149, 149, 172, 150, 150, 150, 245, 152, 153, 153, 154, 154, 155, 155, 155, 186, 467, 156, 157, 157, 332, 642, 190, 159, 159, 159, 159, 159, 160, 160, 161, 161, 290, 162, 162, 163, 163, 163, 531, 164, 164, 164, 165, 406, 165, 165, 165, 166, 166, 166, 166, 167, 167, 168, 168, 233, 413, 683, 931, 170, 839, 170, 171, 171, 171, 171, 171, 172, 172, 400, 506, 184, 174, 174, 174, 174, 174, 175, 176, 176, 177, 177, 177, 177, 177, 177, 177, 177, 177, 393, 178, 178, 179, 179, 179, 180, 981, 181, 181, 181, 182, 183, 183, 185, 185, 186, 270, 187, 410, 625, 189, 189, 189, 190, 190, 190, 191, 191, 191, 191, 302, 192, 193, 193, 523, 741, 195, 195, 195, 424, 196, 349, 197, 197, 246, 198, 198, 198, 198, 199, 531, 200, 201, 201, 201, 201, 201, 411, 204, 205, 205, 205, 206, 206, 206, 206, 207, 207, 207, 207, 354, 208, 209, 587, 717, 211, 211, 211, 630, 666, 211, 212, 212, 212, 545, 212, 212, 456, 889, 214, 216, 216, 216)
elFrom <- c(elFrom, 216, 216, 927, 883, 422, 946, 219, 406, 734, 874, 221, 221, 222, 222, 853, 222, 222, 222, 223, 223, 223, 224, 224, 224, 274, 406, 227, 227, 228, 228, 229, 230, 230, 230, 964, 231, 231, 231, 864, 597, 232, 233, 675, 235, 964, 237, 416, 482, 568, 236, 237, 238, 238, 238, 239, 671, 734, 975, 240, 684, 240, 650, 241, 424, 817, 243, 683, 243, 243, 307, 456, 245, 245, 246, 968, 259, 247, 247, 669, 247, 487, 248, 248, 249, 689, 250, 250, 250, 251, 251, 251, 251, 891, 375, 855, 252, 253, 253, 253, 690, 789, 255, 255, 284, 587, 730, 256, 545, 257, 257, 281, 336, 614, 258, 258, 259, 259, 975, 274, 260, 260, 260, 260, 698, 261, 817, 262, 262, 262, 262, 803, 816, 406, 264, 264, 853, 265, 488, 601, 266, 267, 269, 269, 269, 269, 269, 270, 290, 271, 271, 927, 594, 274, 274, 274, 274, 963, 276, 276, 276, 454, 278, 278, 279, 279, 279, 329, 384, 280, 281, 822, 281, 482, 282, 284, 284, 284, 284, 285, 285, 285, 286, 286, 286, 286, 287, 287, 288, 288, 288, 289, 290, 449, 291, 669, 291, 788, 659, 292, 293, 522, 296, 296, 803, 296, 297, 299, 300, 300, 300, 300, 643, 301, 302, 302, 302, 302, 303, 303, 303, 303, 531, 704, 304, 305, 789, 306, 306, 306, 306, 308, 336, 309, 309, 745, 312, 312, 313, 313, 313, 637, 649, 790, 802, 960, 343, 524, 316, 317, 317, 318, 335, 319, 319, 319, 689, 320, 964, 321, 321, 321, 321, 323, 323, 323, 324, 324, 868, 698, 327, 529, 329, 329, 329, 329, 960, 332, 746, 811, 822, 865, 335, 334, 334, 334, 424, 335, 335, 336, 336, 336, 633, 931, 401, 341, 407, 412, 842, 343, 343, 343, 532, 637, 344, 553, 597, 399, 758, 346, 347, 347, 348, 348, 348, 348, 349, 349, 349, 817, 351, 351, 807, 855, 353, 353, 976, 354, 354, 354, 356, 356, 371, 358, 358, 707, 859, 994, 690, 360, 360, 361, 361, 361, 361, 973, 375, 362, 784, 365, 365, 365, 732, 890, 972, 637, 368, 369, 369, 369, 370, 643, 370, 370, 371, 372, 404, 373, 997, 375, 375, 376, 568, 377, 377, 378, 402, 840, 380, 425, 465, 569, 382, 382, 803, 894, 384, 384, 608, 526, 388, 388, 390, 390, 390, 390, 390, 390, 672, 392, 393, 660, 395, 395, 743, 397, 398, 399, 996, 460, 401, 401, 401, 402, 403, 422, 403, 413, 406, 406, 406, 406, 408, 408, 930, 409, 409, 409, 409, 410, 410, 410, 411, 411, 412, 413, 414, 415, 415, 855, 416, 416, 445, 417, 417, 417, 871, 420, 946, 420, 420, 422, 877, 423, 424, 424, 425, 426, 822, 430, 429, 429, 429, 732, 429, 429, 430, 433, 669, 433, 467, 433, 434, 435, 436, 436, 812, 437, 437, 853, 963, 438, 439, 446, 440, 608, 440, 914, 441, 692, 441, 862, 442, 442, 443, 446, 744, 446, 880, 449, 482, 556, 912, 957, 453, 453, 453, 994, 454, 454, 551, 561, 998, 457, 457, 788, 473, 458, 689, 459, 689, 460, 460, 679, 812, 884, 886, 717, 741, 466, 467, 467, 760, 834, 529, 738, 470, 470, 503, 471, 471, 472, 472, 473, 473, 999, 474, 474, 474, 643, 835, 476, 690, 859, 478, 479, 877, 480, 480, 480, 503, 774, 482, 483, 584, 733, 487, 487, 488, 488, 642, 488, 577, 491, 802, 492, 493, 494, 494, 498, 496, 496, 642, 726, 497, 914, 498, 499, 499, 499, 500, 500, 942, 581, 915, 501, 543, 701, 770, 503, 503, 748, 506, 506, 506, 506, 737, 982, 913, 510, 566, 511, 692, 767, 932, 737, 660, 742, 552, 518, 700, 854, 640, 583, 521, 521, 522, 522, 524, 683, 718, 525, 526, 527, 759, 529, 530, 530, 998, 531, 531, 531, 532, 532, 532, 532, 532, 577)
elFrom <- c(elFrom, 533, 533, 877, 534, 534, 534, 734, 535, 535, 561, 536, 538, 538, 538, 539, 614, 652, 541, 541, 541, 997, 543, 543, 543, 946, 545, 791, 549, 550, 963, 705, 809, 554, 554, 555, 912, 556, 556, 556, 556, 557, 958, 632, 665, 719, 560, 560, 561, 570, 628, 707, 777, 923, 976, 564, 564, 738, 564, 564, 565, 565, 566, 566, 567, 567, 567, 569, 570, 958, 679, 958, 908, 638, 736, 577, 653, 808, 607, 645, 774, 865, 636, 583, 583, 999, 585, 585, 587, 840, 609, 589, 589, 995, 592, 874, 625, 698, 594, 595, 596, 822, 732, 823, 607, 771, 724, 804, 600, 839, 601, 731, 800, 854, 645, 607, 607, 607, 607, 608, 609, 610, 610, 891, 943, 613, 615, 800, 839, 638, 619, 619, 883, 620, 620, 768, 622, 877, 623, 623, 623, 633, 892, 889, 787, 984, 748, 761, 929, 690, 930, 630, 630, 632, 633, 685, 634, 634, 635, 635, 635, 637, 968, 883, 638, 638, 816, 642, 669, 643, 643, 644, 646, 895, 740, 649, 650, 650, 961, 651, 651, 856, 730, 849, 680, 908, 960, 661, 662, 931, 955, 666, 666, 669, 669, 669, 670, 671, 671, 768, 672, 812, 818, 675, 677, 677, 677, 679, 679, 679, 889, 683, 684, 732, 685, 876, 770, 687, 788, 690, 692, 692, 692, 692, 692, 790, 916, 698, 718, 722, 743, 937, 702, 704, 707, 759, 917, 713, 713, 732, 715, 892, 717, 718, 874, 921, 942, 744, 722, 891, 968, 724, 724, 726, 726, 881, 732, 733, 734, 793, 892, 740, 970, 742, 743, 744, 744, 784, 746, 886, 807, 770, 996, 994, 856, 878, 813, 769, 771, 902, 956, 982, 780, 810, 874, 825, 873, 840, 924, 789, 988, 790, 811, 791, 875, 795, 795, 800, 802, 964, 805, 980, 808, 808, 808, 874, 874, 811, 922, 819, 868, 929, 881, 822, 822, 952, 865, 963, 981, 988, 872, 832, 839, 960, 851, 855, 855, 860, 918, 861, 875, 875, 984, 888, 890, 897, 901, 998, 905, 961, 907, 918, 988, 976, 982, 922, 923, 924, 947, 970, 974, 997, 999, 942, 952, 984, 992)
elTo <- c(250, 308, 413, 525, 803, 894, 332, 433, 474, 847, 963, 968, 147, 290, 337, 393, 474, 179, 193, 233, 737, 793, 838, 684, 718, 237, 404, 698, 724, 285, 641, 86, 285, 376, 689, 758, 889, 145, 410, 544, 583, 835, 96, 788, 924, 43, 91, 446, 181, 289, 378, 406, 547, 784, 189, 399, 482, 822, 262, 308, 817, 832, 260, 997, 81, 229, 839, 56, 840, 183, 186, 397, 676, 760, 344, 534, 980, 303, 343, 395, 925, 988, 483, 522, 132, 335, 506, 643, 304, 704, 871, 872, 466, 524, 567, 683, 997, 264, 279, 896, 105, 356, 460, 568, 726, 789, 865, 902, 951, 988, 138, 293, 38, 614, 633, 224, 550, 64, 224, 463, 521, 41, 347, 566, 746, 885, 99, 424, 442, 459, 571, 613, 689, 807, 84, 106, 257, 883, 191, 222, 265, 631, 681, 853, 207, 296, 546, 726, 866, 161, 665, 640, 816, 160, 669, 284, 313, 371, 973, 270, 407, 748, 230, 410, 445, 587, 644, 651, 936, 961, 964, 320, 804, 284, 476, 506, 755, 919, 57, 730, 754, 85, 259, 609, 975, 59, 278, 360, 413, 454, 589, 609, 889, 170, 184, 215, 365, 426, 707, 828, 548, 294, 479, 671, 473, 497, 642, 914, 942, 999, 65, 358, 491, 669, 326, 310, 531, 717, 852, 882, 960, 172, 331, 416, 552, 643, 453, 607, 732, 994, 245, 428, 943, 97, 255, 279, 570, 238, 412, 235, 271, 532, 722, 927, 964, 77, 286, 638, 736, 216, 351, 526, 745, 911, 927, 971, 79, 970, 839, 82, 814, 288, 556, 595, 456, 560, 563, 976, 98, 249, 85, 802, 149, 661, 86, 788, 953, 312, 630, 696, 740, 793, 88, 88, 88, 682, 864, 400, 424, 460, 947, 208, 354, 683, 744, 467, 946, 195, 92, 932, 402, 411, 828, 321, 670, 95, 95, 931, 766, 129, 458, 97, 968, 116, 161, 336, 422, 493, 802, 877, 281, 442, 651, 826, 907, 100, 100, 352, 364, 414, 190, 618, 839, 914, 924, 103, 887, 635, 961, 182, 772, 380, 488, 510, 662, 670, 520, 107, 109, 584, 597, 267, 388, 338, 862, 933, 691, 998, 113, 763, 475, 506, 790, 835, 898, 420, 494, 819, 116, 440, 535, 692, 886, 117, 117, 437, 661, 853, 876, 877, 219, 118, 798, 156, 672, 731, 766, 825, 120, 972, 981, 551, 842, 963, 122, 122, 349, 527, 529, 650, 770, 122, 150, 354, 377, 811, 922, 617, 130, 190, 337, 523, 742, 880, 409, 539, 126, 127, 937, 152, 159, 448, 527, 591, 317, 129, 973, 130, 702, 831, 324, 472, 492, 659, 436, 168, 172, 515, 538, 134, 329, 135, 714, 136, 706, 137, 241, 912, 140, 141, 660, 142, 142, 470, 735, 773, 142, 282, 705, 930, 300, 625, 715, 144, 144, 319, 774, 610, 741, 375, 148, 909, 383, 585, 593, 149, 718, 874, 894, 150, 178, 545, 873, 152, 487, 768, 999, 868, 995, 269, 560, 860, 156, 156, 645, 425, 849, 158, 158, 159, 342, 407, 595, 620, 849, 514, 645, 553, 802, 162, 549, 981, 239, 540, 864, 164, 537, 795, 800, 391, 165, 630, 761, 891, 333, 427, 581, 982, 855, 907, 398, 515, 169, 169, 169, 169, 184, 170, 890, 176, 356, 613, 679, 884, 426, 796, 173, 173, 174, 211, 274, 403, 734, 812, 511, 302, 363, 204, 246, 251, 283, 465, 557, 664, 704, 891, 178, 884, 915, 242, 302, 637, 273, 180, 289, 587, 696, 569, 671, 831, 305, 789, 277, 187, 341, 188, 188, 222, 439, 590, 620, 867, 880, 278, 356, 968, 989, 192, 419, 464, 673, 194, 194, 372, 403, 863, 196, 810, 197, 601, 666, 198, 401, 716, 719, 771, 594, 200, 756, 381, 666, 688, 788, 867, 202, 979, 498, 525, 898, 253, 539, 818, 859, 236, 448, 790, 880, 208, 879, 906, 210, 210, 261, 384, 545, 211, 211, 817, 346, 443, 451, 212, 752, 947, 213, 213, 674, 300, 322, 543, 671)
elTo <- c(elTo, 881, 216, 217, 218, 218, 239, 220, 220, 220, 649, 957, 265, 269, 222, 918, 920, 989, 675, 692, 809, 394, 463, 854, 225, 225, 658, 960, 307, 975, 695, 791, 811, 905, 230, 449, 452, 556, 231, 232, 769, 886, 234, 700, 235, 236, 236, 236, 236, 929, 503, 769, 822, 905, 517, 239, 239, 239, 297, 240, 808, 241, 769, 242, 242, 374, 243, 744, 930, 244, 244, 251, 642, 922, 246, 247, 291, 444, 247, 690, 248, 663, 944, 632, 249, 515, 769, 986, 334, 335, 465, 801, 251, 252, 252, 870, 322, 402, 721, 254, 254, 627, 852, 256, 256, 256, 987, 257, 909, 996, 258, 258, 258, 956, 996, 392, 845, 259, 260, 567, 600, 769, 916, 261, 781, 261, 306, 319, 323, 776, 263, 263, 264, 420, 795, 265, 862, 266, 266, 958, 876, 602, 743, 862, 888, 911, 441, 271, 324, 608, 271, 272, 437, 564, 600, 824, 274, 699, 743, 974, 277, 593, 821, 724, 865, 897, 280, 280, 992, 774, 281, 991, 282, 705, 398, 476, 730, 919, 746, 758, 823, 342, 635, 800, 821, 370, 390, 420, 556, 861, 779, 324, 291, 648, 291, 688, 291, 292, 954, 953, 296, 533, 577, 296, 849, 357, 596, 668, 715, 732, 881, 301, 753, 369, 419, 518, 987, 500, 655, 672, 825, 304, 304, 919, 653, 305, 434, 519, 540, 612, 894, 309, 485, 691, 311, 316, 617, 557, 596, 621, 314, 314, 314, 314, 314, 315, 316, 856, 537, 714, 895, 319, 361, 391, 534, 320, 804, 320, 590, 670, 875, 985, 447, 733, 812, 363, 655, 325, 326, 836, 328, 492, 621, 673, 984, 330, 901, 333, 333, 333, 333, 334, 417, 588, 966, 335, 589, 954, 614, 682, 802, 337, 338, 340, 368, 342, 342, 342, 588, 716, 892, 344, 344, 980, 345, 345, 346, 346, 834, 558, 623, 489, 559, 632, 850, 359, 588, 770, 351, 849, 971, 352, 352, 603, 854, 353, 819, 843, 856, 789, 823, 357, 536, 561, 359, 359, 359, 360, 884, 942, 450, 558, 701, 918, 361, 362, 461, 364, 580, 598, 685, 365, 366, 366, 367, 713, 499, 571, 700, 390, 370, 753, 921, 994, 863, 373, 471, 373, 508, 942, 435, 377, 645, 917, 948, 379, 379, 680, 382, 382, 382, 579, 768, 383, 383, 738, 837, 385, 386, 501, 636, 435, 462, 499, 606, 794, 923, 392, 845, 870, 394, 866, 987, 396, 909, 856, 758, 399, 400, 417, 722, 823, 721, 419, 403, 550, 405, 547, 886, 934, 999, 486, 870, 408, 470, 581, 600, 604, 502, 961, 964, 430, 441, 897, 803, 897, 628, 833, 415, 798, 908, 417, 588, 966, 984, 419, 861, 420, 983, 985, 479, 422, 777, 913, 954, 887, 796, 428, 429, 578, 652, 667, 429, 739, 808, 759, 431, 431, 452, 433, 920, 554, 923, 580, 673, 436, 445, 743, 437, 437, 480, 647, 440, 505, 440, 709, 440, 603, 441, 772, 441, 777, 878, 752, 668, 446, 955, 448, 577, 450, 450, 450, 452, 607, 678, 787, 453, 806, 980, 455, 455, 455, 468, 584, 457, 458, 530, 458, 538, 459, 902, 979, 461, 461, 462, 462, 463, 466, 939, 797, 813, 468, 468, 469, 469, 905, 949, 471, 516, 949, 701, 800, 521, 530, 473, 644, 759, 987, 475, 475, 495, 477, 477, 799, 676, 479, 507, 605, 634, 481, 481, 929, 610, 484, 484, 638, 682, 544, 571, 488, 918, 490, 510, 492, 950, 773, 634, 876, 495, 829, 955, 497, 497, 833, 497, 652, 606, 696, 767, 655, 825, 500, 501, 501, 938, 502, 502, 502, 558, 949, 504, 571, 724, 766, 790, 508, 508, 509, 615, 511, 654, 513, 513, 513, 516, 517, 517, 518, 598, 518, 518, 519, 520, 617, 622, 733, 952, 567, 524, 524, 528, 541, 820, 528, 699, 575, 903, 530, 697, 871, 882, 565, 575, 590, 682, 968, 533, 622, 753)
elTo <- c(elTo, 533, 903, 923, 980, 535, 926, 970, 536, 791, 600, 709, 934, 544, 540, 541, 703, 866, 944, 541, 701, 799, 824, 543, 850, 547, 786, 606, 551, 554, 554, 827, 939, 711, 555, 729, 855, 872, 966, 621, 558, 559, 559, 559, 582, 636, 998, 562, 562, 562, 562, 562, 563, 604, 651, 564, 786, 844, 782, 836, 654, 810, 612, 769, 916, 910, 777, 570, 573, 573, 575, 576, 576, 656, 578, 578, 579, 579, 579, 579, 582, 606, 968, 584, 851, 857, 636, 588, 589, 711, 967, 590, 677, 593, 594, 594, 825, 981, 621, 596, 598, 598, 599, 599, 600, 600, 934, 601, 875, 602, 603, 603, 604, 798, 849, 940, 993, 902, 967, 611, 967, 611, 611, 843, 686, 618, 618, 619, 764, 876, 619, 916, 926, 622, 778, 622, 703, 720, 819, 624, 624, 625, 626, 626, 627, 627, 627, 629, 629, 740, 789, 892, 779, 634, 765, 813, 728, 821, 873, 644, 637, 638, 895, 934, 640, 694, 643, 753, 861, 987, 708, 647, 648, 957, 722, 905, 650, 786, 844, 654, 656, 656, 657, 658, 658, 852, 786, 662, 664, 751, 817, 694, 711, 985, 786, 734, 831, 672, 825, 673, 674, 725, 775, 798, 945, 749, 884, 886, 679, 931, 754, 685, 814, 686, 687, 783, 688, 942, 695, 790, 809, 833, 910, 695, 697, 725, 699, 699, 699, 699, 928, 919, 777, 708, 709, 749, 935, 715, 935, 716, 852, 937, 719, 720, 721, 722, 905, 723, 723, 829, 865, 865, 966, 728, 881, 796, 755, 737, 740, 926, 740, 830, 974, 753, 803, 745, 791, 749, 754, 757, 758, 760, 762, 764, 765, 846, 873, 775, 775, 776, 977, 781, 781, 782, 785, 786, 788, 876, 789, 887, 791, 905, 792, 857, 858, 810, 869, 804, 990, 806, 875, 928, 955, 810, 811, 884, 811, 843, 819, 820, 821, 959, 991, 827, 829, 830, 831, 831, 832, 928, 965, 840, 857, 870, 872, 910, 860, 985, 941, 985, 887, 969, 977, 954, 904, 903, 949, 905, 933, 910, 910, 912, 921, 937, 1000, 940, 925, 926, 928, 933, 934, 940, 940, 966, 966)

edgeList <- data.frame(from=elFrom, 
                       to=elTo, 
                       stringsAsFactors = FALSE
                       )

# Inspect edgeList
str(edgeList)
head(edgeList)

# Construct the igraph object
network <- graph_from_data_frame(edgeList, directed = FALSE)

# View your igraph object
network


# load("./RInputFiles/StudentCustomers.RData")

custID <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651)
custID <- c(custID, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956)
custChurn <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
custChurn <- c(custChurn, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)

customers <- data.frame(id=custID, churn=custChurn)
# Inspect the customers dataframe
str(customers)
head(customers)

# Count the number of churners and non-churners
table(customers$churn)

matchID <- match(V(network), customers$id)
churnID <- customers$churn[matchID]
table(churnID)
churnID[is.na(churnID)] <- 0
table(churnID)

# Add a node attribute called churn
V(network)$churn <- churnID


# useVerts <- c('1', '10', '100', '1000', '101', '102', '103', '104', '105', '106', '107', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '20', '200', '201', '202', '204', '205', '206', '207', '208', '209', '21', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '22', '220', '221', '222', '223', '224', '225', '227', '228', '229', '23', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '24', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '25', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '26', '260', '261', '262', '263', '264', '265', '266', '267', '269', '27', '270', '271', '272', '273', '274', '276', '277', '278', '279', '28', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '29', '290', '291', '292', '293', '294', '296', '297', '299', '3', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '32', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '34', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '35', '351', '352', '353', '354', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '37', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '38', '380', '381', '382', '383', '384', '385', '386', '388', '39', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '4', '40', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '41', '410', '411', '412', '413', '414', '415', '416', '417', '419', '42', '420', '422', '423', '424', '425', '426', '427', '428', '429', '43', '430', '431', '433', '434', '435', '436', '437', '438', '439', '44', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '45', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '47', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '48', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '49', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499')
# useVerts <- c(useVerts, '5', '50', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '51', '510', '511', '513', '514', '515', '516', '517', '518', '519', '52', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '53', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '54', '540', '541', '543', '544', '545', '546', '547', '548', '549', '55', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '56', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '57', '570', '571', '573', '575', '576', '577', '578', '579', '58', '580', '581', '582', '583', '584', '585', '587', '588', '589', '59', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '6', '60', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '61', '610', '611', '612', '613', '614', '615', '617', '618', '619', '62', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '63', '630', '631', '632', '633', '634', '635', '636', '637', '638', '64', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '65', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '66', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '67', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '68', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '69', '690', '691', '692', '694', '695', '696', '697', '698', '699', '7', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '71', '711', '713', '714', '715', '716', '717', '718', '719', '72', '720', '721', '722', '723', '724', '725', '726', '728', '729', '73', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '74', '740', '741', '742', '743', '744', '745', '746', '748', '749', '75', '751', '752', '753', '754', '755', '756', '757', '758', '759', '76', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '77', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '78', '780', '781', '782', '783', '784', '785', '786', '787', '788', '789', '79', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '8', '80', '800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '81', '810', '811', '812', '813', '814', '816', '817', '818', '819', '82', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '83', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '84', '840', '842', '843', '844', '845', '846', '847', '849', '85', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '86', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '87', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '88', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '89', '890', '891', '892', '894', '895', '896', '897', '898', '90', '901', '902', '903', '904', '905', '906', '907', '908', '909', '91', '910', '911', '912', '913', '914', '915', '916', '917', '918', '919', '92', '920', '921', '922', '923', '924', '925', '926', '927', '928', '929', '93', '930', '931', '932', '933', '934', '935', '936', '937', '938', '939', '94', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '95', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '96', '960', '961', '963', '964', '965', '966', '967', '968', '969', '97', '970', '971', '972', '973', '974', '975', '976', '977', '979', '98', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '99', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999')

# useVertNums <- match(useVerts, V(network))
# useNetwork <- induced_subgraph(network, useVertNums)
useNetwork <- network
useNetwork

# Visualize the network (pretty messy)
plot(useNetwork, vertex.label = NA, edge.label = NA, edge.color = 'black', vertex.size = 2)


# Add a node attribute called color
V(useNetwork)$color <- V(useNetwork)$churn

# Change the color of churners to red and non-churners to white
V(useNetwork)$color <- gsub("1", "red", V(useNetwork)$color) 
V(useNetwork)$color <- gsub("0", "white", V(useNetwork)$color)

# Plot the network (pretty messy)
plot(useNetwork, vertex.label = NA, edge.label = NA, edge.color = 'black', vertex.size = 2)


# Create a subgraph with only churners
churnerNetwork <- induced_subgraph(useNetwork, v = V(useNetwork)[which(V(useNetwork)$churn == 1)])
                    
# Plot the churner network 
plot(churnerNetwork, vertex.label = NA, vertex.size = 2)


ctNeighbors <- function(v) {
    tmp <- V(useNetwork)[neighbors(useNetwork, v, mode="all")]$churn
    c(sum(tmp==0), sum(tmp==1))
}
mtxNeighbors <- sapply(V(useNetwork), FUN=ctNeighbors)
NonChurnNeighbors <- mtxNeighbors[1, ]
ChurnNeighbors <- mtxNeighbors[2, ]


# Compute the churn probabilities
churnProb <- ChurnNeighbors / (ChurnNeighbors + NonChurnNeighbors)

# Find who is most likely to churn
mostLikelyChurners <- which(churnProb == max(churnProb))

# Extract the IDs of the most likely churners
customers$id[mostLikelyChurners]


# Find churn probability of the 44th customer
churnProb[44]

# Update the churn probabilties and the non-churn probabilities
AdjacencyMatrix <- as_adjacency_matrix(useNetwork)
nNeighbors <- colSums(mtxNeighbors)
churnProb_updated <- as.vector((AdjacencyMatrix %*% churnProb) / nNeighbors)

# Find updated churn probability of the 44th customer
churnProb_updated[44]


# Compute the AUC
pROC::auc(churnID, as.vector(churnProb))

# Write a for loop to update the probabilities
for(i in 1:10){
    churnProb <- as.vector((AdjacencyMatrix %*% churnProb) / nNeighbors)
}

# Compute the AUC again
pROC::auc(churnID, as.vector(churnProb))

```
  
  
  
***
  
Chapter 2 - Homophily  
  
Homophily:  
  
* Social networks tend to have reasons for connections (sharing of common properties such as interests, locations, etc.)  
* Homophily is a scientific term for "birds of a feather flock together" (similar nodes are more likely to connect than dissimilar nodes)  
* Can define different types of edges to classify the degree of homophily  
	* edge_rr<-sum(E(g)$label=='rr')  # connects r to r  
    * edge_pp<-sum(E(g)$label=='pp')  # connects p to p  
    * edge_rp<-sum(E(g)$label=='rp')  # connects r to p or p to r  
    * p <- 2*edges/nodes*(nodes-1)  
  
Dyadicity:  
  
* Dyadicity is a measure of connectedness among nodes with the same attributes  
	* Comparison is to a random configuration of the network (1 would be equal to expected, >1 would be greater than expected)  
    * Dyadic (>1) - Random (~1) - Anti-Dyadic (<1)  
  
Heterophilicity:  
  
* Connectedness among nodes with opposite labels  
	* Expected number of connections is nA * nB * p (where p is probability of connection across the network)  
* Heterophilic (>1), Random (~1), and Heterophobic (<1) networks are all possible  
  
Summary:  
  
* Need to evaluate whether node attributes have important relationships - is there structure among the node connections  
* Dyadicity (connectiveness to same type of nodes, relative to expected / average network connectiveness)  
* Hetrophilicity (connectedness to different type of nodes, relative to expected / average network connectiveness)  
  
Example code includes:  
```{r}

# Add the column edgeList$FromLabel
edgeList$FromLabel <- customers[match(edgeList$from, customers$id), 2]
edgeList$FromLabel[is.na(edgeList$FromLabel)] <- 0

# Add the column edgeList$ToLabel
edgeList$ToLabel <- customers[match(edgeList$to, customers$id), 2]
edgeList$ToLabel[is.na(edgeList$ToLabel)] <- 0

# Add the column edgeList$edgeType
edgeList$edgeType <- edgeList$FromLabel + edgeList$ToLabel
 
# Count the number of each type of edge
table(edgeList$edgeType)


# Count churn edges
ChurnEdges <- sum(edgeList$edgeType == 2)
 
# Count non-churn edges
NonChurnEdges <- sum(edgeList$edgeType == 0)
 
# Count mixed edges
MixedEdges <- sum(edgeList$edgeType == 1)
 
# Count all edges
edges <- ChurnEdges + NonChurnEdges + MixedEdges

#Print hte number of edges
edges


# Count the number of churn nodes
ChurnNodes <- sum(customers$churn == 1)
 
# Count the number of non-churn nodes
NonChurnNodes <- sum(customers$churn == 0)
 
# Count the total number of nodes
nodes <- ChurnNodes + NonChurnNodes
 
# Compute the network connectance
connectance <- 2 * edges / nodes / (nodes - 1)

# Print the value
connectance


# Compute the expected churn dyadicity
ExpectedDyadChurn <- ChurnNodes * (ChurnNodes - 1) * connectance / 2
 
# Compute the churn dyadicity
DyadChurn <- ChurnEdges / ExpectedDyadChurn
 
# Inspect the value
DyadChurn


# Compute the expected heterophilicity
ExpectedHet <- NonChurnNodes * ChurnNodes * connectance
 
# Compute the heterophilicity
Het <- MixedEdges / ExpectedHet
 
# Inspect the heterophilicity
Het

```
  
  
  
***
  
Chapter 3 - Network Featurization  
  
Basic Network Features:  
  
* Provided that the labels of the nodes depend on each other, they can be useful for predictive modeling  
* Can begin by getting neighborhood features  
	* degree(g)  # first order degree  
    * neighborhood.size(g, order=2)  # second order degree, or size of neighborhoods counting all 2+ away  
    * count_triangles(g)  # triangles that each node is part of (triangle has full connection among three nodes)  
* Can also look at the centrality features  
	* Betweenness - measure of how often the node is part of a quickest path between two other nodes  
    * betweenness(g)  
    * Closeness - measure of how easily a node can reach the other nodes  
    * closeness(g)  
* Can also look at the transitivity (clustering coefficient)  
	* transitivity(g,type = 'local')  # computed for each node separately  
    * Triangles vs. Triangles + Triads (triangles with a missing edge)  
  
Link-Based Features:  
  
* Link-based features are network features that depend on adjacencies  
	* The adjacency matrix has all nodes on both the x and y axis, with a 1 meaning they are linked by an edge  
    * A <- get.adjacency(g)  
    * Can then run the matrix multiplication (dot product) against (for example) a vector indicating preferences  
    * preference <- c(1,1,1,1,1,1,0,0,0,0)  
    * rNeighbors <- A %*% preference  
    * as.vector(rNeighbors)  
* Could instad get something like the average age of the neighbors  
	* age <- c(23,65,33,36,28,45,41,24,38,39)  
    * degree <- degree(g)  
    * averageAge <- A %*% age / degree  
  
Page Rank:  
  
* The page rank is the basis of search algorithms such as google  
* The page rank algorithm assumes that clicks on one page lead to the next page, and checks the degree of connectivity  
	* Page rank of pages that link to it  
    * Number of links that the referring pages include  
* The page rank formula for all pages simultaneously is PR = alpha * A * PR + (1 - alpha) * epsilon  
	* The alpha is the likelihood of a link being clicked, assumed to be uniform, assuming 85% as the default  
    * The A is the adjacency matrix  
    * The PR is the page rank, and is solved with either matrix inversions or iterations  
    * page.rank(g)  
    * page.rank(g, personalized = c(1,0,0,0,0,0,0,0,0,0))  # a personalized argument where the first node connections drive higher values  
  
Example code includes:  
```{r}

# Extract network degree
V(network)$degree <- degree(network, normalized=TRUE)

# Extraxt 2.order network degree
degree2 <- neighborhood.size(network, 2)

# Normalize 2.order network degree
V(network)$degree2 <- degree2 / (length(V(network)) - 1)

# Extract number of triangles
V(network)$triangles <- count_triangles(network)


# Extract the betweenness
V(network)$betweenness <- betweenness(network, normalized=TRUE)

# Extract the closeness
V(network)$closeness <- closeness(network, normalized=TRUE)

# Extract the eigenvector centrality
V(network)$eigenCentrality <- eigen_centrality(network, scale = TRUE)$vector


# Extract the local transitivity
V(network)$transitivity <- transitivity(network, type="local", isolates='zero')

# Compute the network's transitivity
transitivity(network)


# Extract the adjacency matrix
AdjacencyMatrix <- as_adjacency_matrix(network)

# Compute the second order matrix
SecondOrderMatrix <- AdjacencyMatrix %*% AdjacencyMatrix

# Adjust the second order matrix
SecondOrderMatrix_adj <- ((SecondOrderMatrix) > 0) + 0
diag(SecondOrderMatrix_adj) <- 0

# Inspect the second order matrix
SecondOrderMatrix_adj[1:10, 1:10]


# Compute the number of churn neighbors
V(network)$ChurnNeighbors <- as.vector(AdjacencyMatrix %*% V(network)$churn)

# Compute the number of non-churn neighbors
V(network)$NonChurnNeighbors <- as.vector(AdjacencyMatrix %*% (1 - V(network)$churn))

# Compute the relational neighbor probability
V(network)$RelationalNeighbor <- as.vector(V(network)$ChurnNeighbors / 
    (V(network)$ChurnNeighbors + V(network)$NonChurnNeighbors))


# Compute the number of churners in the second order neighborhood
V(network)$ChurnNeighbors2 <- as.vector(SecondOrderMatrix %*% V(network)$churn)

# Compute the number of non-churners in the second order neighborhood
V(network)$NonChurnNeighbors2 <- as.vector(SecondOrderMatrix %*% (1 - V(network)$churn))

# Compute the relational neighbor probability in the second order neighborhood
V(network)$RelationalNeighbor2 <- as.vector(V(network)$ChurnNeighbors2 / 
    (V(network)$ChurnNeighbors2 + V(network)$NonChurnNeighbors2))


degree <- degree(network)

# Extract the average degree of neighboring nodes
V(network)$averageDegree <- 
    as.vector(AdjacencyMatrix %*% V(network)$degree) / degree

# Extract the average number of triangles of neighboring nodes
V(network)$averageTriangles <- 
    as.vector(AdjacencyMatrix %*% V(network)$triangles) / degree

# Extract the average transitivity of neighboring nodes    
V(network)$averageTransitivity <-
    as.vector(AdjacencyMatrix %*% V(network)$transitivity) / degree

# Extract the average betweeness of neighboring nodes    
V(network)$averageBetweenness <- 
    as.vector(AdjacencyMatrix %*% V(network)$betweenness) / degree


# Compute one iteration of PageRank 
# iter1 <- page.rank(network, algo = 'power', options = list(niter = 1))$vector

# Compute two iterations of PageRank 
# iter2 <- page.rank(network, algo = 'power', options = list(niter = 2))$vector

# Inspect the change between one and two iterations
# sum(abs(iter1 - iter2))

# Inspect the change between nine and ten iterations
# sum(abs(iter9 - iter10))


# Create an empty vector
# value <- c()

# Write a loop to compute PageRank 
# for(i in 1:15){
#   value <- cbind(value, page.rank(network, algo = 'power',options = list(niter = i))$vector)
# }
  
# Compute the differences 
# difference <- colSums(abs(value[,1:14] - value[,2:15]))

# Plot the differences
# plot(1:14, difference)


# boxplots <- function(damping=0.85, personalized=FALSE){
#   if(personalized){
#     V(network)$pp<-page.rank(network,damping=damping,personalized = V(network)$Churn)$vector
#   }
#   else{
#   V(network)$pp<-page.rank(network,damping=damping)$vector
#   }
#   boxplot(V(network)$pp~V(network)$Churn)#
# }

# Look at the distribution of standard PageRank scores
# boxplots(damping = 0.85)

# Inspect the distribution of personalized PageRank scores
# boxplots(damping = 0.85, personalized = TRUE)

# Look at the standard PageRank with damping factor 0.2
# boxplots(damping = 0.2)

# Inspect the personalized PageRank scores with a damping factor 0.99
# boxplots(damping=0.99, personalized = TRUE)


# Compute the default PageRank score
# V(network)$pr_0.85 <- page.rank(network)$vector

# Compute the PageRank score with damping 0.2
# V(network)$pr_0.20 <- page.rank(network, damping=0.2)$vector

# Compute the personalized PageRank score
# V(network)$perspr_0.85 <- page.rank(network, damping=0.85, personalized = V(network)$Churn)$vector

# Compute the personalized PageRank score with damping 0.99
# V(network)$perspr_0.99 <- page.rank(network, damping=0.99, personalized = V(network)$Churn)$vector

```
  
  
  
***
  
Chapter 4 - Putting It All Together  
  
Extract Dataset:  
  
* May want to extract some of the features from the nodes in the igraph  
	* g  # prints the object  
    * as_data_frame(g,what='vertices')  # extracts to data frame - what= can be nodes (vertices) or edges  
* May want to preprocess, particularly for missing values (non-disclosed, non-relevant, processing error, etc.)  
	* sum(is.na(dataset$degree))  
* May want to understand the correlations among the node features  
	* M <- cor(dataset[,-1])  
    * corrplot::corrplot(M, method = 'circle')  
  
Building Predictive Models:  
  
* Building a predictive model using supervised learning  
* Can split the dataset from the graph in to a test and train dataset  
* Can use either logistic regression and random forests  
	* glm(R~degree+pageRank, dataset=training_set,family='binomial')  
    * library(randomForest)  
    * rfModel<-randomForest(R~., dataset=training_set)  
    * varImpPlot(rfModel)  
  
Evaluating Model Performance:  
  
* Can evaluate the model using the test dataset, along with the functions in the pROC library  
	* logPredictions <- predict(logModel, newdata = test_set, type = "response")  # probability based on the logistic regression  
    * rfPredictions<- predict(rfModel, newdata = test_set, type='prob')  # two column matrix, one for the probability of each of the factor states (two in this case)  
* Can evaluate the model based on AUC which is often between 0.5 (random) and 1.0 (perfect)  
	* library(pROC)  
    * auc(test_set$label, logPredictions)  
* Can also assess "top decile" lift - looks at actual churn among the top-10% churn probabilities  
	* library(lift)  
    * TopDecileLift(test_set$label, predictions, plot=TRUE)  
  
Wrap Up:  
  
* Accurately predicting labels for network data  
* Labeled networks that convert edgelists to networks using igraph  
* Homophily - idea that "birds of a feather flock together"  
	* Dyadicity - connectedness of nodes of same labels  
    * Heterophilicity - connectednedd of nodes of different labels  
* Can featurize the network using igraph  
* Can create datasets based on networks, then use that for modeling  
	* dataset <- as_data_frame(g, what='vertices')  
    * glm(R~., dataset=training_set, family='binomial')  
    * logPredictions <- predict(logModel, newdata=test_set, type="response")  
    * auc(test_set$label, logPredictions)  
    * TopDecileLift(test_set$label, predictions, plot=TRUE)  
  
Example code includes:  
```{r}

# Extract the dataset
dataset_full <- as_data_frame(network, what = "vertices")
dataset_full$Future <- 0
dsF1 <- c(404, 550, 41, 613, 48, 230, 294, 852, 93, 520, 617, 523, 714, 282, 705, 153, 995, 511, 204, 273, 194, 756, 979, 879, 843, 713, 837, 636, 469, 478, 938, 654, 751, 775)
dataset_full[match(dsF1, dataset_full$name), "Future"] <- 1

# Inspect the dataset
head(dataset_full)

# Remove customers who already churned
dataset_filtered <- dataset_full[-which(dataset_full$churn == 1), ]

# Remove useless columns
dataset <- dataset_filtered[, -c(1, 2)]


# Inspect the feature
summary(dataset$RelationalNeighbor2)

# Find the indeces of the missing values
toReplace <- which(is.na(dataset$RelationalNeighbor2))

# Replace the missing values with 0
dataset$RelationalNeighbor2[toReplace] <- 0

# Inspect the feature again
summary(dataset$RelationalNeighbor2)


# Generate the correlation matrix
M <- cor(dataset[,])

# Plot the correlations
corrplot::corrplot(M, method = "circle")

# Print the column names
colnames(dataset)

# Create toRemove
# toRemove <- c(10, 13, 19, 22)

# Remove the columns
# dataset <- dataset[, -toRemove]


# Set the seed
set.seed(7)

# Creat the index vector
index_train <- sample(1:nrow(dataset), round((2/3) * nrow(dataset), 0), replace=FALSE)

# Make the training set
training_set <- dataset[index_train,]

# Make the test set
test_set <- dataset[-index_train,]


# Make firstModel
firstModel <- glm(Future ~ degree + degree2 + triangles + betweenness + closeness + transitivity, 
                  family = "binomial", data = training_set
                  )

# Build the model
secondModel <- glm(Future ~ ChurnNeighbors + RelationalNeighbor + ChurnNeighbors2 + RelationalNeighbor2 + averageDegree + averageTriangles + averageTransitivity + averageBetweenness, 
                   family = "binomial", data = training_set
                   )

# Build the model
thirdModel <- glm(Future ~ ., data=training_set, family="binomial")


# Set seed
set.seed(863)

# Build model
rfModel <- randomForest::randomForest(as.factor(Future)~. ,data=training_set)

# Plot variable importance
randomForest::varImpPlot(rfModel)


# Predict with the first model
firstPredictions <- predict(firstModel, newdata = test_set, type = "response")

# Predict with the first model
secondPredictions <- predict(secondModel, newdata = test_set, type = "response")

# Predict with the first model
thirdPredictions <- predict(thirdModel, newdata = test_set, type = "response")

# Predict with the first model
rfPredictions<- predict(rfModel, newdata = test_set, type = "prob")

sapply(list(firstPredictions, secondPredictions, thirdPredictions, rfPredictions[, 2]), 
       FUN=function(x) { pROC::auc(test_set$Future, x) }
       )

```
  
  
***
  
  
### _Bayesian Regression Modeling with rstanarm_  
  
Chapter 1 - Introduction to Bayesian Linear Models  
  
Non-Bayesian Linear Regression:  
  
* Can use the "kidiq" dataset from the rstanarm package  
* Objective is to predict the child IQ from the mom IQ  
	* lm_model <- lm(kid_score ~ mom_iq, data = kidiq)  
    * summary(lm_model)  
    * broom::tidy(lm_model)  
* Challenge of the p-value is that it is only a comparison to the null hypothesis  
* Often interested in understanding the underlying population, as well the statistic calculated  
* Can use the "songs" dataset from Spotify  
  
Bayesian Linear Regression:  
  
* Bayesian methods sample from the posterior distribution, allowing for inferences about values the parameters might take  
* The "rstanarm" package allows a high-level interface to the Stan library  
	* library(rstanarm)  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * summary(stan_model)  
* There are several descriptive statistics in the output  
	* sigma: Standard deviation of errors  
    * mean_PPD: mean of posterior predictive samples  
    * log-posterior: analogous to a likelihood  
    * Rhat: a measure of within chain variance compared to across chain variance (stability of estimates, with a goal of less than 1.1 for all parameters)  
  
Comparing frequentist and Bayesian models:  
  
* The Bayesian and frequentist methods produce very similar outputs  
* The fundamental difference is that frequentists assume fixed parameters and random data while Bayesians assume fixed data and random parameters  
	* Frequentist - the p-value is the probability of a test statistics, given a specific null hypothesis  
    * Bayesian - probabilities of parameters vary in their ability to generate the given data  
* Bayesian approaches use the credible interval, which is very similar to the confidence interval  
	* Confidence interval: Probability that a range contains the true value  
    * Credible interval: Probability that the true value is within a range  
    * posterior_interval(stan_model)  # gives the credible intervals (90% by default)  
    * posterior_interval(stan_model, prob = 0.95)  # gives the 95% credible intervals  
* Can look at both the confidence intervals and the credible intervals for the same dataset and different models  
	* confint(lm_model, parm = "mom_iq", level = 0.95)  
    * posterior_interval(stan_model, pars = "mom_iq", prob = 0.95)  
* Can also look at probabilities that a parameter is between several key points  
	* posterior <- spread_draws(stan_model, mom_iq)  
    * mean(between(posterior_mom_iq, 0.60, 0.65))  # Bayesian methods allow for actual inferences about the parameter  
  
Example code includes:  
```{r eval=FALSE}

# Print the first 6 rows
head(songs)

# Print the structure
str(songs)


# Create the model here
lm_model <- lm(popularity ~ song_age, data = songs)

# Produce the summary
summary(lm_model)

# Print a tidy summary of the coefficients
tidy(lm_model)


# Create the model here
stan_model <- stan_glm(popularity ~ song_age, data = songs)

# Produce the summary
summary(stan_model)

# Print a tidy summary of the coefficients
tidy(stan_model)


# Create the 90% credible intervals
posterior_interval(stan_model)

# Create the 95% credible intervals
posterior_interval(stan_model, prob = 0.95)

# Create the 80% credible intervals
posterior_interval(stan_model, prob = 0.8)

```
  
  
  
***
  
Chapter 2 - Modifying a Bayesian Model  
  
What is in a Bayesian Model?  
  
* Many levers for modifying a Bayesian model  
* Posterior distributions are sampled in groups called chains (iterations) that begin in random areas  
	* Chains move to where there is a good combination of the priors and the parameters  
    * Lengths of the chains allow for more robust parameter estimates  
* Convergence is importance, since it ensures a consistent output - chains start at different places, and so the starting iterations (prior to convergence) are discarded  
    * The chains start with a few thousand warm-up or burn-in for each of the chains  
* The number of iterations is a balancing act - not enough (convergence problems) or too many (run-time problems)  
  
Prior Distributions:  
  
* Priors can inform the posterior distribution, given the same data in the experiment  
	* More informative priors typically have narrower distributions  
    * Can think of the prior as being like an additional data point - more meaningful the less data that we have  
    * Generally a best practice to use a weakly informative prior, absent good cause for a strong belief  
* Example of including prior distributions in rstanarm  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * prior_summary(stan_model){{1}}  
* Can calculate adjusted scales for the intercept as 10 * sd(y), since 10 is the default for the package  
* Can calculate adjusted scales for the coefficients as (2.5 / sd(x)) * sd(y), since 2.5 is the default for the package  
	* prior_summary(stan_model)  
* Can also use unadjusted priors, such as  
	* no_scale <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(autoscale = FALSE), prior = normal(autoscale = FALSE), prior_aux = exponential(autoscale = FALSE) )  
    * prior_summary(no_scale)  
  
User-Specified Priors:  
  
* Can use a specified prior distribution using the same arguments as in the previous paragraph  
	* Research may suggest the parameter should be near a specific value - take advantage of narrow prior  
    * Parameters may be constrained, such as a need to be always positive  
* Can specify the priors to be cast to specific values  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(location = 0, scale = 10), prior = normal(location = 0, scale = 2.5), prior_aux = exponential(rate = 1) )  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(location = 0, scale = 10, autoscale = FALSE), prior = normal(location = 0, scale = 2.5, autoscale = FALSE), prior_aux = exponential(rate = 1, autoscale = FALSE) )  
    * The autoscale=FALSE is needed so that stan_glm does not rescale the data before applying the priors  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = normal(location = 3, scale = 2), prior = cauchy(location = 0, scale = 1), )  
* There are many types of prior distributions that can be used  
	* ?priors  
* Can also set a flat prior (prior provides zero information)  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq, prior_intercept = NULL, prior = NULL, prior_aux = NULL )  
    * This is usually a bad idea; rarely in a state where there is zero information, and a weakly informative prior with an adjusted scale is typically better  
  
Tuning Models for Stability:  
  
* May need to alter estimation parameters  
* Divergent transitions are when the steps are too big - need to take smaller steps (longer run time)  
	* stan_model <- stan_glm(popularity ~ song_age, data = songs, control = list(adapt_delta = 0.95))  # 0.95 is default, increase will decrease step size  
* May have already reached the maximum tree depth in some of the chains (indicates poor efficiency; no good stopping place, and insufficient sampling of the posterior)  
	* stan_model <- stan_glm(popularity ~ song_age, data = songs, control = list(max_treedepth = 10))  # 10 is the default, increases allow more sampling  
  
Example code includes:  
```{r eval=FALSE}

# 3 chains, 1000 iterations, 500 warmup
model_3chains <- stan_glm(popularity ~ song_age, data = songs,
    chains = 3, iter = 1000, warmup = 500)

# Print a summary of model_3chains
summary(model_3chains)

# 2 chains, 100 iterations, 50 warmup
model_2chains <- stan_glm(popularity ~ song_age, data = songs,
    chains = 2, iter = 100, warmup = 50)

# Print a summary of model_1chain
summary(model_2chains)


# Estimate the model
stan_model <- stan_glm(popularity ~ song_age, data = songs)

# Print a summary of the prior distributions
prior_summary(stan_model)


# Calculate the adjusted scale for the intercept
10 * sd(songs$popularity)

# Calculate the adjusted scale for `song_age`
(2.5 / sd(songs$song_age)) * sd(songs$popularity)

# Calculate the adjusted scale for `valence`
(2.5 / sd(songs$valence)) * sd(songs$popularity)


# Estimate the model with unadjusted scales
no_scale <- stan_glm(popularity ~ song_age, data = songs,
    prior_intercept = normal(autoscale = FALSE),
    prior = normal(autoscale = FALSE),
    prior_aux = exponential(autoscale = FALSE)
)

# Print the prior summary
prior_summary(no_scale)


# Estimate a model with flat priors
flat_prior <- stan_glm(popularity ~ song_age, data = songs,
    prior_intercept = NULL, prior = NULL, prior_aux = NULL)

# Print a prior summary
prior_summary(flat_prior)


# Estimate the model with an informative prior
inform_prior <- stan_glm(popularity ~ song_age, data = songs,
    prior = normal(location = 20, scale = 0.1, autoscale = FALSE))

# Print the prior summary
prior_summary(inform_prior)


# Estimate the model with a new `adapt_delta`
adapt_model <- stan_glm(popularity ~ song_age, data = songs,
  control = list(adapt_delta = 0.99))

# View summary
summary(adapt_model)

# Estimate the model with a new `max_treedepth`
tree_model <- stan_glm(popularity ~ song_age, data = songs,
  control = list(max_treedepth = 15))

# View summary
summary(tree_model)

```
  
  
  
***
  
Chapter 3 - Assessing Model Fit  
  
Using R-Squared Statistics:  
  
* The R-squared statistic is a measure of how well the model predicts the dependent variable (proportion of variance explained) - "coefficient of determination"  
	* R-squared = 1 - RSS/TSS where RSS is residual sum-squares and TSS is total sum-squares  
    * lm_model <- lm(kid_score ~ mom_iq, data = kidiq)  
    * lm_summary <- summary(lm_model)  
    * lm_summary$r.squared  
    * ss_res <- var(residuals(lm_model))  
    * ss_total <- var(residuals(lm_model)) + var(fitted(lm_model))  
    * 1 - (ss_res / ss_total)  
* The R-squared is not save by the stan_glm() call, but can be calculated  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * ss_res <- var(residuals(stan_model))  
    * ss_total <- var(fitted(stan_model)) + var(residuals(stan_model))  
    * 1 - (ss_res / ss_total)  
  
Posterior Predictive Model Checks:  
  
* Can use posterior distributions to check the model  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * spread_draws(stan_model, `(Intercept)`, mom_iq) %>% select(-.draw)  
    * predictions <- posterior_linpred(stan_model)  # predicted scores using each of the sets of parameter values and each data point  
* Can compare the distributions of predicted and observed scores  
	* iter1 <- predictions[1,]  
    * iter2 <- predictions[2,]  
    * summary(kidiq$kid_score)  
    * summary(iter1)  
    * summary(iter2)  
  
Model Fit with Posterior Predictive Model Checks:  
  
* Can use Bayesian functions as part of the post-processing  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * r2_posterior <- bayes_R2(stan_model)  
    * summary(r2_posterior)  
    * quantile(r2_posterior, probs = c(0.025, 0.975))  
    * hist(r2_posterior)  
    * pp_check(stan_model, "dens_overlay")  # compare densities  
    * pp_check(stan_model, "stat")  # compare statistic to histogram  
* The mean is only one aspect - can look at many aspects of the dependent variable  
	* pp_check(stan_model, "stat_2d")  # mean and sd plotted on a 2-D plot  
  
Bayesian Model Comparisons:  
  
* Can compare two or more models produced using rstanarm  
* The LOO (leave one out) package runs a modified (approximated) form of LOO cross-validation  
	* library(loo)  
    * stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * loo(stan_model)  
* The values from LOO mainly have value in comparison to similar models (much like ANOVA for comparing nested linear regression)  
	* model_1pred <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * model_2pred <- stan_glm(kid_score ~ mom_iq * mom_hs, data = kidiq)  
    * loo_1pred <- loo(model_1pred)  
    * loo_2pred <- loo(model_2pred)  
    * compare(loo_1pred, loo_2pred)  
* The compare() function provides the difference in loo, along with an SE  
	* Positive means "prefer second model"  
    * Negative means "prefer first model"  
    * The SE helps assess whether it is meaningful - rule of thumb is to require change in loo greater than SE, otherwise prefer the simpler model  
  
Example code includes:  
```{r eval=FALSE}

# Print the R-squared from the linear model
lm_summary$r.squared

# Calulate sums of squares
ss_res <- var(residuals(lm_model))
ss_fit <- var(fitted(lm_model))

# Calculate the R-squared
1 - (ss_res / (ss_res + ss_fit))


# Save the variance of residulas
ss_res <- var(residuals(stan_model))

# Save the variance of fitted values
ss_fit <- var(fitted(stan_model))

# Calculate the R-squared
1 - (ss_res / (ss_res + ss_fit))


# Calculate posterior predictive scores
predictions <- posterior_linpred(stan_model)

# Print a summary of the observed data
summary(songs$popularity)

# Print a summary of the 1st replication
summary(predictions[1,])

# Print a summary of the 10th replication
summary(predictions[10,])


# Calculate the posterior distribution of the R-squared
r2_posterior <- bayes_R2(stan_model)

# Make a histogram of the distribution
hist(r2_posterior)


# Create density comparison
pp_check(stan_model, "dens_overlay")

# Create scatter plot of means and standard deviations
pp_check(stan_model, "stat_2d")


# Estimate the model with 1 predictor
model_1pred <- stan_glm(popularity ~ song_age, data = songs)

# Print the LOO estimate for the 1 predictor model
loo(model_1pred)

# Estimate the model with both predictors
model_2pred <- stan_glm(popularity ~ song_age * artist_name, data = songs)

# Print the LOO estimates for the 2 predictor model
loo(model_2pred)

```
  
  
  
***
  
Chapter 4 - Presenting and Using Bayesian Regression  
  
Visualizing Bayesian Models:  
  
* Can save the model coefficients using tidy() and then plot the regression against the underlying point  
	* stan_model <- stan_glm(kid_score ~ mom_iq, data = kidiq)  
    * tidy(stan_model)  
    * tidy_coef <- tidy(stan_model)  
    * model_intercept <- tidy_coef$estimate[1]  
    * model_slope <- tidy_coef$estimate[2]  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point() + geom_abline(intercept = model_intercept, slope = model_slope,)  
* Can also plot uncertainty using the posterior distribution  
	* draws <- spread_draws(stan_model, `(Intercept)`, mom_iq)  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point()  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point() geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = mom_iq), size = 0.2, alpha = 0.1, color = "skyblue")  
    * ggplot(kidiq, aes(x = mom_iq, y = kid_score)) + geom_point() geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = mom_iq), size = 0.2, alpha = 0.1, color = "skyblue") + geom_abline(intercept = model_intercept, slope = model_slope)  
  
Making Predictions:  
  
* Can make predictions for the observed data  
	* stan_model <- stan_glm(kid_score ~ mom_iq + mom_hs, data = kidiq)  
    * posteriors <- posterior_predict(stan_model)  
    * predict_data <- data.frame( mom_iq = 110, mom_hs = c(0, 1) )  
    * new_predictions <- posterior_predict(stan_model, newdata = predict_data)  
  
Visualizing Predictions:  
  
* Can plot predictions based on the new data  
	* stan_model <- stan_glm(kid_score ~ mom_iq + mom_hs, data = kidiq)  
    * predict_data <- data.frame( mom_iq = 110, mom_hs = c(0, 1) )  
    * posterior <- posterior_predict(stan_model, newdata = predict_data)  
    * posterior <- as.data.frame(posterior)  
    * colnames(posterior) <- c("No HS", "Completed HS")  
    * plot_posterior <- gather(posterior, key = "HS", value = "predict")  
    * ggplot(plot_posterior, aes(x = predict)) + facet_wrap(~ HS, ncol = 1) + geom_density()  
  
Conclusion:  
  
* One solution to inferences - implementing Bayesian models  
    * Differences between frequentist and Bayesian  
    * Importance of making correct inferences  
* Modifying a Bayesian model  
* Evaluating fit of a Bayesian model  
* Using the model to make predictions and communicate results  
* Additional topics for exploration include  
  
Example code includes:  
```{r eval=FALSE}

# Save the model parameters
tidy_coef <- tidy(stan_model)

# Extract intercept and slope
model_intercept <- tidy_coef$estimate[1]
model_slope <- tidy_coef$estimate[2]

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
  geom_point() +
  geom_abline(intercept = model_intercept, slope = model_slope)


# Save the values from each draw of the posterior distribution
draws <- spread_draws(stan_model, `(Intercept)`, `song_age`)

# Print the `draws` data frame to the console
draws

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
  geom_point()

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
    geom_point() +
    geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = song_age), 
                size = 0.1, alpha = 0.2, color = "skyblue"
                )

# Create the plot
ggplot(songs, aes(x = song_age, y = popularity)) +
    geom_point() +
    geom_abline(data = draws, aes(intercept = `(Intercept)`, slope = song_age), 
                size = 0.1, alpha = 0.2, color = "skyblue"
                ) +
    geom_abline(intercept = model_intercept, slope = model_slope)

# Estimate the regression model
stan_model <- stan_glm(popularity ~ song_age + artist_name, data = songs)

# Print the model summary
summary(stan_model)

# Get posteriors of predicted scores for each observation
posteriors <- posterior_predict(stan_model)

# Print 10 predicted scores for 5 songs
posteriors[1:10, 1:5]


# Create data frame of new data
predict_data <- data.frame(song_age = 663, artist_name = "Beyoncé")

# Create posterior predictions for Lemonade album
new_predictions <- posterior_predict(stan_model, newdata = predict_data)

# Print first 10 predictions for the new data
new_predictions[1:10, ]

# Print a summary of the posterior distribution of predicted popularity
summary(new_predictions[, 1])


# View new data predictions
new_predictions[1:10, ]

# Convert to data frame and rename variables
new_predictions <- as.data.frame(new_predictions)
colnames(new_predictions) <- c("Adele", "Taylor Swift", "Beyoncé")

# Create tidy data structure
plot_posterior <- gather(new_predictions, key = "artist_name", value = "predict")

# Print formated data
head(plot_posterior)


# Create plot of 
ggplot(plot_posterior, aes(x = predict)) +
    facet_wrap(~ artist_name, ncol = 1) +
    geom_density()

```
  
  
  
***
  
### _ChIP-seq Workflows in R_  
  
Chapter 1 - Introduction to ChIP-seq  
  
What is ChIP-seq?  
  
* The core of ChIP-seq is understanding how the cells in the body know what to do  
* The function of a cell is largely determined by the expressed genes (DNA - RNA - Proteins)  
	* Inhibitors will stop the process  
    * Regualtors help keep the cell producing the right proteins - disorders lead to diseases like cancer  
* Can use ChIP-seq to find over-represented genes in various patient cohorts (such as cancer patients)  
* Dataset for the course will be prostate cancer - 5 primary tumors, 3 treatment resistent  
	* library(GenomicAlignments)  
    * reads <- readGAlignments('file_name')  
    * seqnames(reads)  # obtaining read coordinates  
    * start(reads)  # obtaining read coordinates  
    * end(reads)  # obtaining read coordinates  
    * coverage(reads)  # computing coverage  
* Can also access the peack cells in the ChIP-seq experiment  
	* library(rtracklayer)  
    * peaks <- import.bed('file_name')  
    * chrom(peaks)  
    * ranges(peaks)  
    * score(peaks)  
  
ChIP-seq Workflow:  
  
* First step of the workflow is read mapping - mapping reads to genomes  
* Then, create a coverage profile (number of overlapping reads)  
* Then, import the mapped reads and verify the quality (these are usually the first steps in R - the above steps are usually in more specialized software)  
* Then, identify peaks by comparing samples - AR sample sites that are over-used  
* Then, interpret the findings for a better understanding of the associated biological processes  
	* Heat maps - differences in cells and similarities in cells  
    * Creating UpSet plots, such as upset(fromList(peak_sets))  
  
ChIP-seq Results Summary:  
  
* The heatmap plot is useful for assessing sample quality - check for expected patterns  
* Heights of individual peaks across samples can be more informative - height of peak by group can be particularly insightful  
  
Example code includes:  
```{r eval=FALSE}

# Print a summary of the 'reads' object
print(reads)

# Get the start position of the first read
start_first <- start(reads)[1]

# Get the end position of the last read
end_last <- end(reads)[length(reads)]

# Compute the number of reads covering each position in the selected region
cvg <- coverage(reads)


# Print a summary of the 'peaks' object
print(peaks)

# Use the score function to find the index of the highest scoring peak
max_idx <- which.max(score(peaks))

# Extract the genomic coordinates of the highest scoring peak using the `chrom` and `ranges` functions
max_peak_chrom <- chrom(peaks)[max_idx]
max_peak_range <- ranges(peaks)[max_idx]


# Create a vector of colors to label groups (there are 2 samples per group)
group <- c(primary = rep("blue", 2), TURP = rep("red", 2))

# Plot the sample correlation matrix `sample_cor` as a heat map
heatmap(sample_cor, ColSideColors = group, RowSideColors = group, 
        cexCol = 0.75, cexRow = 0.75, symm = TRUE)

# Create a heat map of peak read counts
heatmap(read_counts, ColSideColors = group, labRow = "", cexCol = 0.75)


# Take a look at the full gene sets
print(ar_sets)

# Visualise the overlap between the two groups using the `upset` function
upset(fromList(ar_sets))

# Print the genes with differential binding
print(db_sets)

# Visualise the overlap between the two groups using the `upset` function
upset(fromList(db_sets))

```
  
  
  
***
  
Chapter 2 - Back to Basics - Preparing ChIP-seq Data  
  
Importing Data:  
  
* Read mapping and peak calling are typically carried out with specialized (non-R) tools and then stored in a BAM (Binary Sequence Alignment Map) format  
* Can use Rsamtools package to interact with BAM files  
	* Rsamtools provides functions for indexing, reading, filtering and writing of BAM files  
* Use readGAlignments to import mapped reads  
	* library(GenomicAlignments)  
    * reads <- readGAlignments(bam_file)  
* Use BamViews to define regions of interest  
	* library(GenomicRanges)  
    * library(Rsamtools)  
    * ranges <- GRanges(...)  
    * views <- BamViews(bam_file, bamRanges=ranges)  
* Use import.bed to load peak calls from a BED file  
	* library(rtracklayer)  
    * peaks <- import.bed(peak_bed, genome="hg19")  
    * bams <- BamViews(bam_file, bamRanges=peaks)  
    * reads <- readGAlignments(bams)  
  
Closer Look at Peaks:  
  
* Can use Gvix to combine data from multiple sources in to a single plot  
	* Where are the reads located, and the associated coverage  
    * Annotations of key features of read coverage  
* Need to load the key library and set context for data to be plotted  
	* library(Gviz)  
    * ideogram <- IdeogramTrack("chr12", "hg19")  
    * axis <- GenomeAxisTrack()  
    * plotTracks(list(ideogram, axis), from=101360000, to=101380000)  
* Can then add data to the track  
	* cover_track <- DataTrack(cover_ranges,window=100000,type='h',name="Coverage")  
    * plotTracks(list(ideogram, cover_track, axis), from=101360000, to=101380000)  
    * peak_track <- AnnotationTrack(peaks, name="Peaks")  
    * plotTracks(list(ideogram, cover_track, peak_track, axis), from=101360000, to=101380000)  
    * library(TxDb.Hsapiens.UCSC.hg19.knownGene)  
    * tx <- GeneRegionTrack(TxDb.Hsapiens.UCSC.hg19.knownGene, chromosome="chr12", start=101360000, end=101380000, name="Genes")  
    * plotTracks(list(ideogram, cover_track, peak_track, tx, axis), from=101360000, to=101380000)  
  
Cleaning ChIP-seq Data:  
  
* Incorrectly mapped reads can produce false peaks - "genomic repeats"  
	* Particularly problematic if the reference and the sample differ in the number of false reads  
    * Low complexity regions (such as end of chromosomes) tend to have poorer quality  
* Amplification bias can be a concern - just prior to sequencing  
* Quality Control reports can help diagnose the potential issues  
	* library(ChIPQC)  
    * qc_report <- ChIPQC(experiment="sample_info.csv", annotation="hg19")  
    * ChIPQCreport(qc_report)  
    * Input is a CSV file mapping samples to input files and descriptions  
* Cleaning the data has many steps  
	* Remove duplicate reads  
    * Remove reads with multiple hits or low mapping quality  
    * Remove peaks in "blacklisted" regions -- available from the ENCODE project  
  
Assessing Enrichment:  
  
* Reading finds the ends where a protein is created, meaning there will also be more than one read representing the ends of the fragment (?)  
* Can be helpful to do read coverage forward and backwards and to then aggregate findings  
	* reads <- readGAlignments(bam)  
    * reads_gr <- granges(reads[[1]])  
    * frag_length <- fragmentlength(qc_report)["GSM1598218"]  
    * reads_ext <- resize(reads_gr, width=frag_length)  
    * cover_ext <- coverage(reads_ext)  
* Question is how does coverage look in peaks relative to the rest of the genome  
	* bins <- tileGenome(seqinfo(reads), tilewidth=200, cut.last.tile.in.chrom=TRUE)  # create 200 bins along the genome  
    * peak_bins_overlap <- findOverlaps(bins, peaks)  # can find the overlaps  
    * peak_bins <- bins[from(peak_bins_overlap), ]  # subset to just the overlaps  
    * peak_bins$score <- countOverlaps(peak_bins, reads)  # count number of overlapping reads  
* Can create a function for the binning reads process  
	* count_bins <- function(reads, target, bins){  
    *   # Find all bins overlapping peaks  
    *   overlap <- from(findOverlaps(bins, target))
    *   target_bins <- bins[overlap, ]  
    *   # Count the number of reads overlapping each peak bin  
    *   target_bins$score <- countOverlaps(target_bins, reads)  
    * target_bins  
    * }  
* Can find coverage for the blacklisted regions in much the same way  
	* peak_bins <- count_bins(reads_ext, peaks, bins)  
    * bl_bins <- count_bins(reads_ext, blacklist.hg19, bins)  
* Can then get the background coverage by considering all of the remaining bins  
	* bkg_bins <- subset(bins, !bins %in% peak_bins & !bins %in% bl_bins)  
    * bkg_bins$score <- countOverlaps(bkg_bins, reads_ext)  
  
Example code includes:  
```{r eval=FALSE}

# Load reads form chr20_bam file
reads <- readGAlignments(chr20_bam)

# Create a `BamViews` object for the range 29805000 - 29820000 on chromosome 20
bam_views <- BamViews(chr20_bam, bamRanges=GRanges("chr20", IRanges(start=29805000, end=29820000)))

# Load only the reads in that view
reads_sub <- readGAlignments(bam_views)

# Print the `reads_sub` object
str(reads_sub)


# Load peak calls from chr20_peaks
peaks <- import.bed(chr20_peaks, genome="hg19")

# Create a BamViews object
bam_views <- BamViews(chr20_bam, bamRanges=peaks)

# Load the reads
reads <- readGAlignments(bam_views)


# Create tracks
peak_track <- AnnotationTrack(peak_calls, name="Peaks")
cover_track <- DataTrack(cover_ranges, window=10500, type="polygon", name="Coverage", fill.mountain=c("lighgrey", "lightgrey"), col.mountain="grey")

# Highlight peak locations across tracks
peak_highlight <- HighlightTrack(trackList = list(cover_track, peak_track), range = peak_calls)

# Produce plot
plotTracks(list(ideogram, peak_highlight, GenomeAxisTrack()), chromosome="chr20", from=start_pos, to=end_pos)


# Load reads with mapping qualities by requesting the "mapq" entries
reads <- readGAlignments(bam_file, param=ScanBamParam(what="mapq"))

# Identify good quality alignments
high_mapq <- mcols(reads)$mapq >= 20

# Examine mapping quality distribution for high and low quality alignments
boxplot(mcols(reads)$mapq ~ high_mapq, xlab="good quality alignments", ylab="mapping quality")

# Remove low quality alignments
reads_good <- subset(reads, high_mapq)

```
  
  
  
***
  
Chapter 3 - Comparing ChIP-seq Samples  
  
Introduction to Differential Binding:  
  
* Objective is to find differences in prostate cancercells that respond or are resistant to treatment  
* Can use PCA to find differences in the groups - directions of the components with the most variation  
	* Points can be projected on to the plane  
    * qc_result <- ChIPQC("sample_info.csv", "hg19")  
    * counts <- dba.count(qc_results, summits=250)  
    * plotPrincomp(counts)  
* Can use hierarchical clustering to cluster similar samples for easier future visualization  
	* distance <- dist(t(coverage))  
    * dendro <- hclust(distance)  
    * plot(dendro)  
    * dba.plotHeatmap(peaks, maxSites = peak_count, correlations = FALSE)  
  
Testing for Differential Binding:  
  
* Can use the DiffBind package for either DESeq2 or edgeR  
* Can count the records in a peak dataset  
	* peak_counts <- dba.counts(qc_output, summits=250)  
* Can add a contrast for how the sample should be split  
	* peak_counts <- dba.contrast(peak_counts, categories = DBA_CONDITION)  
* Want to assess the coverage differences in comparison to a control sequence  
	* bind_diff <- dba.analyze(peak_counts)  
	* dba.plotPCA(bind_diff, DBA_Condition, contrast=1)  
    * dba.plotHeatmap(bind_diff, DBA_Condition, contrast=1)  
  
Closer Look at Differential Binding:  
  
* Plotting is a good way to look at the results - can use the DiffBind library  
* Can create MA plots - x-axis for concentration, y-axis for resistant vs. log-fold-change of responsive  
	* dba.plotMA(dba_object)  # MA plots  
    * May need to normalize data in the MA plot to eliminate systemic downward bias  
* Can create volcano plots - FDR as a function of log-fold-change  
	* dba.plotVolcano(dba_object)  
* Can create box plots  
	* dba.plotBox(dba_object)  
  
Example code includes:  
```{r eval=FALSE}

# Compute the pairwise distances between samples using `dist`
cover_dist <- dist(t(cover))

# Use `hclust()` to create a dendrogram from the distance matrix
cover_dendro <- hclust(cover_dist)

# Plot the dendrogram
plot(cover_dendro)


# Print the `peaks` object
print(peaks)

# Obtain the coordinates of the merged peaks
merged_peaks <- peaks$merged

# Extract the number of peaks present in the data
peak_count <- nrow(merged_peaks)

# Create a heatmap using the `dba.plotHeatmap()` function
dba.plotHeatmap(peaks, maxSites = peak_count, correlations = FALSE)


# Examine the ar_binding object
print(ar_binding)

# Identify the category corresponding to the tumor type contrast
contrast <- DBA_CONDITION

# Establish the contrast to compare the two tumor types
ar_binding <- dba.contrast(ar_binding, categories=contrast, minMembers=2)

# Examine the ar_binding object again to confirm that the contrast has been added
print(ar_binding)


# Examine the `ar_binding` object to confirm that it contains the required contrast
print(ar_binding)

# Run the differential binding analysis
ar_diff <- dba.analyze(ar_binding)

# Examine the result
print(ar_diff)


# Create a PCA plot using all peaks
dba.plotPCA(ar_diff, DBA_CONDITION)

# Create a PCA plot using only differentially bound peaks
dba.plotPCA(ar_diff, DBA_CONDITION, contrast = 1)

# Create a heatmap using all peaks
dba.plotHeatmap(ar_diff, DBA_CONDITION, correlations = FALSE, maxSites = 440)

# Create a heatmap using only differentially bound peaks
dba.plotHeatmap(ar_diff, DBA_CONDITION, contrast=1, correlations = FALSE)


# Create an MA plot
dba.plotMA(ar_diff)


# Create a volcano plot
dba.plotVolcano(ar_diff)


# Create a box plot of the peak intensities
compare_groups <- dba.plotBox(ar_diff, notch=FALSE)

# Inspect the returned p-values
print(compare_groups)

```
  
  
  
***
  
Chapter 4 - From Peaks to Genes to Function  
  
Interpreting ChIP-seq Peaks:  
  
* Want to find the genes that impact a particular binding site - no way to know for sure, but often look for "closest" genes  
	* Obtain information about gene locations  
    * Assign peaks to genes  
    * Identify genes associated with stronger peaks in one of the conditions  
* Transcript annotations are helpful  
	* library(TxDb.Hsapiens.UCSC.hg19.knownGene)  
    * genes(TxDb.Hsapiens.UCSC.hg19.knownGene)  
    * library(org.Hs.eg.db)  # may be easier for reading/intepreting  
    * select(org.Hs.eg.db, keys=gene_id, columns="SYMBOL", keytype="ENTREZID")  
* Gene symbols are tricky - can have multiple genes given the same symbol  
* Can annotate peaks using the transcript data  
	* library(ChIPpeakAnno)  
    * annoPeaks(peaks, human_genes, bindingType="startSite", bindingRegion=c(-5000,5000))  
    * library(DiffBind)  
    * dba.plotVenn(peaks, mask=1:2)  
* Can use UpSet plots for better interpretation in larger datasets  
	* library(UpSetR)  
    * called_peaks <- as.data.frame(peaks$called)  
    * upset(called_peaks, sets=colnames(peaks$called), order.by='freq')  
  
Interpreting Gene Lists:  
  
* Can use a Gene Set image for very long lists of genes - associations with genes of interest  
	* library(chipenrich)  
	* chipenrich(peaks, genome='hg19', genesets = 'hallmark', locusdef = 'nearest_tss')  
  
Advanced ChIP-seq Analyses:  
  
* Loading and analyzing ChIP-seq data in R  
* First step is to import data and then visualize read coverage, peaks, etc.  
* Need to run quality control procedures  
* Can investigate differential binding with DiffBind  
* Can continue to explore additional datasets  
	* Find datasets to explore at GEO https://www.ncbi.nlm.nih.gov/geo/  
    * The data you worked with in this course is available under accession GSE65478  
    * Bioconductor website: https://bioconductor.org/  
    * Bioconductor support: https://support.bioconductor.org/  
  
Example code includes:  
```{r eval=FALSE}

# Extract peaks from ChIPQCexperiment object
peak_calls <- peaks(ar_calls)

# Only keep samples that passed QC
peak_passed <- peak_calls[qc_pass]

# Find overlaps between peak sets
peaks_combined <- findOverlapsOfPeaks(peak_passed[[1]], peak_passed[[2]], 
                                      peak_passed[[3]], peak_passed[[4]], 
                                      maxgap=50
                                      )

# Examine merged peak set
print(peaks_combined)


# Annotate peaks with closest gene
peak_anno <- annoPeaks(peaks_merged, human_genes, bindingType="startSite", bindingRegion=c(-5000,5000))

# How many peaks were found close to genes?
length(peak_anno)

# Where are peaks located relative to genes?
table(peak_anno$insideFeature)


# Create Venn diagram
dba.plotVenn(ar_diff, mask=1:4)

# Convert the matrix of called peaks into a data frame
called_peaks <- as.data.frame(ar_diff$called)

# Create UpSet plot
upset(called_peaks, keep.order = TRUE, sets=colnames(ar_diff$called), order.by="freq")


# Select all peaks with higher intensity in treatment resistant samples
turp_peaks <- peaks_binding[, "GSM1598218"] + peaks_binding[, "GSM1598219"] < peaks_binding[, "GSM1598223"] + peaks_binding[, "GSM1598225"]

# Run enrichment analysis
enrich_turp <- chipenrich(peaks_comb[turp_peaks, ], genome="hg19", 
                   genesets = "hallmark", out_name = NULL, 
                   locusdef = "nearest_tss", qc_plots=FALSE)

# Print the results of the analysis
print(enrich_turp$results)


# Examine the top gene sets
head(enrich_primary$results)

# Extract the gene IDs for the top ranking set
genes <- enrich_primary$results$Geneset.Peak.Genes[1]

# Split gene IDs into a vector
genes_split <- strsplit(genes, ', ')[[1]]

# Convert gene IDs to gene symbols
gene_symbol <- select(org.Hs.eg.db, keys=genes_split, columns="SYMBOL", keytype="ENTREZID")

# Print the result
print(gene_symbol)


# This is the base URL for all KEGG pathways
base_url <- "https://www.kegg.jp/pathway/"

# Add pathway ID to URL
path_url <- paste0(base_url, top_path, collapse="+")

# Collapse gene IDs into selection string
gene_select <- paste(genes, collapse="+")

# Add gene IDs to URL
path_url <- paste(path_url, gene_select, sep="+")

```
  
  
  
***
  
### _Designing and Analyzing Clinical Trials in R_  
  
Chapter 1 - Principles  
  
Fundamentals:  
  
* Clinical trials are scientific experiments used to evaluate the safety and efficacy of one or more treatments in humans  
	* Pharma, medical devices, medical procedures, etc.  
* Four general phases of clinical trials  
	* Phase I - small group of healthy volunteers to look for effects and side effects  
    * Phase II - small groups of patients with the disease - optimal doses for safety and efficiacy  
    * Phase III - chosen dose evaluated for efficacy and safety against a control  
    * Phase IV - post-marketing surveillance  
* Randomized control trials are considered the gold-standard for treatment - reduces the impact of confoudning variables (such as in cohort studies)  
	* Aim is to have similar patient charcteristics in each of the groups  
    * Blinding is when the patient does not know what they are receiving  
    * Double-blinding is when the researcher and the patient BOTH do not know what they are receiving  
  
Types of Data and Endpoints:  
  
* Clinical trials are highly regulated and must meet various international standards  
* Clinical measures are pre-defined in the protocol (endpoints) and can be a mix of primary or secondary measures related to both efficacy and safety  
* Endpoints can be continuous or categorical - example of continuous  
	* ggplot(data=exercise, aes(x=sbp_change)) + geom_histogram(fill="white", color="black") + xlab("SBP Change, mmHg")  
    * exercise %>% summarise(mean_sbp = mean(sbp_baseline), sd_spb = sd(sbp_baseline))  
* Endpoints can also be categorical, such as did the patient recover in 30 days  
	* finaldata %>% group_by(treatment) %>% filter(!is.na(response)) %>% summarise (n = n()) %>% mutate(prop = n / sum(n))  
    * table(finaldata$response, finaldata$treatment)  
* While binary and continuous endpoints are both common, there are other types also  
	* Discrete values, such as drinks per week or years to progression  
  
Basic Statistical Analysis:  
  
* There is typically a target population (all patients with a disease), a sample population drawn from the target population, and inferences drawn about the target population from the sample population  
* Hypothesis testing is typically run against a null hypothesis - alternative hypothesis can be one-sided or two-sided  
	* The p-value is the probability of observing something at least as extreme as our data if the null hypothesis is true - typical hurdles are set at 0.05  
* For comparing distributions, can use the Wilcox test or the Chi-squared test  
	* wilcox.test(outcome.variable~ group.variable, data=dataset)  
    * table1<-table(care.trial$group, care.trial$recover)  
    * prop.test(table1, correct=FALSE)  
  
Example code includes:  
```{r}

Acupuncture <- readRDS("./RInputFiles/Ex1_1_1.Rds")

#Explore the Acupuncture dataset with the str() function 
str(Acupuncture)

#Display the treatment group frequencies
table(Acupuncture$treatment.group)


#Generate summaries of the variables by treatment group and save results as baselines
baselines <- compareGroups::compareGroups(treatment.group ~ score.baseline + age + sex, data = Acupuncture)

#Use the createTable function to display the results saved in baselines
baseline.table <- compareGroups::createTable(baselines, show.ratio = FALSE, show.p.overall=FALSE)

#Display the created summary table
baseline.table


#Generate a variable for the change from baseline at 12 months
Acupuncture$diff.month12 <- Acupuncture$score.month12 - Acupuncture$score.baseline

#Use the new variable to generate the percentage change from baseline at 12 months
Acupuncture$pct.month12 <- Acupuncture$diff.month12 / Acupuncture$score.baseline * 100

#Generate a histogram for percentage change from baseline within each treatment group
ggplot(data=Acupuncture, aes(x=pct.month12)) + 
  geom_histogram(fill="white", color="black") + facet_wrap( ~ treatment.group) +
  xlab("Percentage Change from Baseline at Month 12")


#Generate the binary response variable. 
Acupuncture$resp35.month12 <- ifelse(Acupuncture$pct.month12 < (-35), 1, 0)

#Encode this new variable as a factor.
Acupuncture$resp35.month12 <- factor(Acupuncture$resp35.month12, 
                                     levels = c(1,0), 
                                     labels=c("greater than 35%", "less than or eq to 35%")
                                     )

#Tabulate the numbers and percentages of patients in each category. 
Acupuncture %>% 
  group_by(resp35.month12) %>% 
  filter(!is.na(resp35.month12)) %>%
  summarise(n = n()) %>% 
  mutate(pct = n / sum(n)*100)


#Dichotomize the variable for complementary therapist visits into 0 or at least 1 visit.
Acupuncture$any.therap.visits <- ifelse(Acupuncture$total.therap.visits == 0, 0, 1)

#Encode the new variable as a factor
Acupuncture$any.therap.visits <- factor(Acupuncture$any.therap.visits, 
                                        levels = c(0,1), 
                                        labels=c("Did not visit CT", "Visited CT")
                                        )

#Dichotomize the variable for complementary therapist visits into 0 or at least 1 visit.
Acupuncture$any.gp.visits <- ifelse(Acupuncture$total.gp.visits == 0, 0, 1)

#Encode the new variable as a factor
Acupuncture$any.gp.visits <- factor(Acupuncture$any.gp.visits, 
                                        levels = c(0,1), 
                                        labels=c("Did not visit GP", "Visited GP")
                                        )

#Dichotomize the variable for complementary therapist visits into 0 or at least 1 visit.
Acupuncture$any.spec.visits <- ifelse(Acupuncture$total.spec.visits == 0, 0, 1)

#Encode the new variable as a factor
Acupuncture$any.spec.visits <- factor(Acupuncture$any.spec.visits, 
                                        levels = c(0,1), 
                                        labels=c("Did not visit specialist", "Visited specialist")
                                        )

#Generate a combined binary endpoint for having any professional visits. 
Acupuncture$combined <- ifelse(Acupuncture$any.therap.visits=="Did not visit CT" &
                                   Acupuncture$any.gp.visits=="Did not visit GP" & 
                                   Acupuncture$any.spec.visits=="Did not visit specialist", 0, 1
                               )

#Encode the new variable as a factor
Acupuncture$combined <- factor(Acupuncture$combined, 
                               levels = c(0,1), 
                               labels=c("No visits", "At least one visit")
                               )

#Tabulate the new composite endpoint.
table(Acupuncture$combined, useNA="ifany")


#Perform the t-test, assuming the variances are equal in the treatment groups
t.test(pct.month12 ~ treatment.group, var.equal=TRUE, data = Acupuncture)

#Use the compareGroups function to save a summary of the results in pct.month12.test
pct.month12.test <- compareGroups::compareGroups(treatment.group ~ pct.month12, data = Acupuncture)

#Use the createTable function to summarize and store the results saved in pct.month12.test.
pct.month12.table <- compareGroups::createTable(pct.month12.test, show.ratio = FALSE, show.p.overall=TRUE)

#Display the results of pct.month12.table
pct.month12.table


#Use a boxplot to visualize the total days off sick by treatment group.  
ggplot(data=Acupuncture, aes(x=treatment.group, y=total.days.sick)) + 
  geom_boxplot(fill="white", color="black") +
  ylab("Total days off sick") +  xlab("Treatment group")

#Use the Wilcoxon Rank Sum test to compare the two distributions.
wilcox.test(total.days.sick ~ treatment.group, data=Acupuncture)


#Perform the test of proportions on resp35.month12 by treatment.group.
prop.test(table(Acupuncture$treatment.group, Acupuncture$resp35.month12), correct=FALSE)

#Use the tidy function to store and display a summary of the test results.
resp35.month12.test <- broom::tidy(prop.test(table(Acupuncture$treatment.group, 
                                                   Acupuncture$resp35.month12
                                                   ), correct=FALSE
                                             )
                                   )
resp35.month12.test

#Calculate the treatment difference
resp35.month12.test$estimate1 - resp35.month12.test$estimate2

```
  
  
  
***
  
Chapter 2 - Trial Designs  
  
Randomization Methods:  
  
* Good randomization ensures the groups are appropriately stratified  
* Simple Randomization has every patient with a 50/50 chance of assignment to either group, regardless of how many are already in each group  
	* treatment <- c("A","B")  
    * simple.list <- sample(treatment, 20, replace=TRUE)  
    * cat(simple.list,sep="\n")  
* May want to instead use blocks, where each block has equal numbers  
	* library(blockrand)  
    * block.list <- blockrand(n=20, num.levels = 2,block.sizes = c(2,2))  
    * Can further randomize the block sizes to avoid the issue of predictability at the end of the block  
    * block.list2 <- blockrand(n=20, num.levels = 2,block.sizes = c(1,2))  
* May want to instead used stratified randomization  
	* Age group, geographical region, disease severity, etc.  
    * over50.severe.list <- blockrand(n=100, num.levels = 2, block.sizes = c(1,2,3,4), stratum='Over 50, Severe', id.prefix='O50_S', block.prefix='O50_S')  
  
Crossover, Factorial, Cluster Randomized Trials:  
  
* Sometimes a desire to have each patient be their own control after a "washout" period (long enough to reverse effects) - called a crossover trial  
	* May increase precision, eliminate inter-pateitn variability, and allow for smaller sample sizes  
    * However, orders of treatments may impact outcomes, washout periods may be too short, and patients may fall out before finishing  
* Factorial designs are such that patients may receive A, B, A and B, or placebo only  
* Sometimes desirable to calculate the odds ratios, especially if the treatments are independent  
	* Odds of Recovery = nRecover / nNotRecover  
    * epitools::oddsratio.wald(recovery.trial$A, recovery.trial$recover)  
* May also want to run cluster-level trials, such as by school or by hospital  
	* Can run in to problems with sample sizes (need to be large, may not have enough, etc.)  
  
Equivalence and Non-Inferiority Trials:  
  
* Objective of an equivalence trial is to show similar efficacy - for example, where a generic is being released  
	* Need to pre-specify a maximum acceptable difference (delta) among the groups  
    * Equivalence is when the confidence intervals are all inside the deltas  
    * Non-inferiority is when at least one of the confidence intervals is outside delta, even with the point estimate being inside  
    * prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "less", conf.level = 0.95, correct=FALSE)  
    * prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "greater", conf.level = 0.95, correct=FALSE)  
    * prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "two.sided", conf.level = 0.90, correct=FALSE)  
* Sometimes the only objective is to show non-inferiority with a one-sided test (Ha: new treatment wose then existing), often at the 2.5% level  
	* prop.test(table(infection.trial$Treatment,infection.trial$Infection), alternative = "less", conf.level = 0.975, correct=FALSE)  
* Need to state in advance the delta, the number of sides, and the significance levels  
	* Lack of superiority does NOT imply equivalence  
  
Bioequivalence trials:  
  
* Bioequivalence is determined by blood draws after someone has taken a drug - pharmacokinetics (PK)  
	* Absorbtion, Excretion  
    * Assumption is often made that similar PK profiles will lead to similar safety and efficacy, saving time on Phase III trials  
* PK profiles are often assessed based on key statistics  
	* Cmax - highest concentration  
    * Tmax - time to highest concentration  
    * T1/2 - half-life  
    * AUC - area under the curve  
    * Crossover designs are frequently used, with washouts being many times greater than the half-life  
* The AUC is often calculated using the trapezoidal method - objective is to be between (0.8, 1.25) of the reference drug for the 90% CI  
    * library(PKNCA)  
    * pk.calc.auc(PKData$plasma.conc.n, PKData$rel.time, interval=c(0.25, 12), method="linear")  
  
Example code includes:  
```{r}

#Generate a vector to store treatment labels “A” and “B”
set.seed(123)
arm<-c("A", "B")

#Randomly select treatment arm 14 times with the sample function and store in a vector
simple <- sample(arm, 14, replace=TRUE)

#Display the contents of the vector
simple

#Tabulate the numbers assigned to each treatment.
table(simple)


#Use the blockrand function for 14 patients, two arms and block size 2.
set.seed(123)
block2 <- blockrand::blockrand(n=14, num.levels = 2,  block.prefix='B', block.sizes = c(1,1))

#Display the list.
block2

#Tabulate the numbers per treatment arm.
table(block2$treatment)


#Use block randomization to produce lists of length 100 and random block sizes between 2 and 8.
set.seed(123)
under55 <- blockrand::blockrand(n=100, num.levels = 2, block.sizes = 1:4, 
                                id.prefix='U55', block.prefix='U55', stratum='<55y'
                                )
above55 <- blockrand::blockrand(n=100, num.levels = 2, block.sizes = 1:4, 
                                id.prefix='A55', block.prefix='A55',stratum='>=55y'
                                )

#Explore the two lists 
head(under55)
head(above55)

#Tabulate the numbers assigned to each treatment within each strata
table(under55$treatment)
table(above55$treatment)


fact.data <- readRDS("./RInputFiles/fact.data.Rds")
str(fact.data)

#Explore the fact.data using the head function.
head(fact.data)

#Display the numbers with and without infections by supplement combination.
fact.data %>% 
    count(glutamine, selenium, infection)

#Display the numbers and proportions with infections for those who received glutamine.
fact.data %>% 
    group_by(glutamine) %>% 
    filter(infection=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))

#Display the numbers and proportions with infections for those who received selenium.
fact.data %>% 
    group_by(selenium) %>% 
    filter(infection=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))


#Display the numbers with and without infections by supplement combination.
fact.data %>% 
    count(glutamine, selenium, infection)

#Display the numbers and proportions with infections for those who received glutamine.
fact.data %>% 
    group_by(infection) %>% 
    filter(glutamine=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))

#Display the numbers and proportions with infections for those who received selenium.
fact.data %>% 
    group_by(infection) %>% 
    filter(selenium=="Yes") %>%
    summarise (n = n()) %>% 
    mutate(prop = n / sum(n))


#Calculate the effect of glutamine on infection
epitools::oddsratio.wald(fact.data$glutamine, fact.data$infection)

#Calculate the effect of selenium on infection
epitools::oddsratio.wald(fact.data$selenium, fact.data$infection)


relapse.trial <- data.frame(Treatment=rep(c("New", "Standard"), times=c(264, 263)), 
                            Relapse=rep(rep(c("At least one relapse", "No relapse"), times=2), 
                                        times=c(184, 80, 169, 94)
                                        ), 
                            stringsAsFactors = TRUE
                            )
str(relapse.trial)
table(relapse.trial)

#Use the head function to explore the relapse.trial dataset
head(relapse.trial)

#Calculate the number of percentages of relapse by treatment group
relapse.trial %>% 
    group_by(Treatment, Relapse) %>% 
    summarise(n = n()) %>% 
    mutate(pct = (n / sum(n))*100)

#Calculate the two-sided 90% confidence interval for the difference
prop.test(table(relapse.trial$Treatment, relapse.trial$Relapse), 
          alternative = "two.sided", conf.level=0.9, correct=FALSE
          )


PKData <- readRDS("./RInputFiles/PKData.Rds")
str(PKData)

#Display the dataset contents
head(PKData)

#Store a numeric version of the concentration variable in plasma.conc.n
PKData$plasma.conc.n <- as.numeric(PKData$plasma.conc)

#Use ggplot to plot the concentration levels against relative time
ggplot(data=PKData, aes(x=rel.time, y=plasma.conc.n)) + 
    geom_line() +
    geom_point() + ggtitle("Individual Concentration Profile") +
    xlab("Time Relative to First Dose, h") + 
    ylab("Plasma Concentration, ng/mL")


#Use the summary function to find the max concentration level
summary(PKData$plasma.conc.n)

#Use pk.calc.tmax to find when Cmax occurred, specifying the concentration and time.
PKNCA::pk.calc.tmax(PKData$plasma.conc.n, PKData$rel.time)

#Use pk.calc.cmax to estimate AUC between 0.25 and 12hrs.
PKNCA::pk.calc.auc(PKData$plasma.conc.n, PKData$rel.time, interval=c(0.25, 12), method="linear")

```
  
  
  
***
  
Chapter 3 - Sample Size and Power  
  
Introduction to Sample Size and Power:  
  
* Only a sample from the target population is appropriate, and it needs to be appropriately sized (too big and too small are both problems)  
* Need to understand the requirements of the trial, endpoints, statistical techniques to be used, smallest clinical meaningful difference, variability, and the significance level as well as the power  
* Type I error is rejecting a true null hypothesis - significance level, often set to 0.05  
* Type II error is failing to reject a false null hypothesis - power is 1 minus Type II error and is usually targeted as 0.8 or 0.9  
	* power.t.test(delta=3, sd=10, power=0.8, type = "two.sample", alternative = "two.sided")  
    * power.t.test(delta=3, sd=10, power=0.9, type = "two.sample", alternative = "two.sided")  
* May want to instead run a test of proportions  
	* power.prop.test(p1=0.3, p2=0.15, power=0.8)  
  
Sample Size Adjustments:  
  
* The alternative hypothesis is often, but not always, based on a two-sided test  
* If the alternative hypothesis is one-sided, then this should be incproproated in the study design  
	* power.t.test(delta=3, sd=10, power=0.8, type = "two.sample", alternative = "one.sided")  
* May want to have unequal group sizes (non 1:1 ratio) if it produces better recruitment or compliance  
	* n.ttest(power = 0.8, alpha = 0.05, mean.diff = 3, sd1 = 10, sd2 = 10, k = 0.5, design = "unpaired", fraction = "unbalanced")  # k of 0.5 means a ratio of 2  
* Can also make adjustments for unequal variances  
	* n.ttest(power = 0.8, alpha = 0.05, mean.diff = 3, sd1 = 9.06, sd2 = 9.06, k = 1, design = "unpaired", fraction = "balanced")  
* There are inevitably drop-outs from clinical trials - ratio is called Q  
	* Sample size needs to be inflated by 1 / (1 - Q)  
    * orig.n <- power.t.test(delta=3, sd=10, power=0.8, type = "two.sample", alternative = "one.sided")$n  
    * ceiling(orig.n/(1-0.1))  # assuming Q = 0.1  
  
Interim Analyses and Stopping Rules:  
  
* Patient recruitment often occurs over a time period of years; can regularly monitor the study prior to completion  
	* May want to stop early if the evidence is very strong for superiority, inferiority, side effects, futility, etc.  
* Interim analyses typically require increasing the same size - Type I error increases with more chances to reject  
* The Pocock rule is also known as the "fixed nominal" rule  
	* library(gsDesign)  
    * Pocock <- gsDesign(k=3, test.type=2, sfu="Pocock")  # sfu is the spending function and k=3 means 2 interim and 1 final  
    * 2*(1-pnorm(Pocock$upper$bound))  
    * Pocock.ss <- gsDesign(k=3, test.type=2, sfu="Pocock", n.fix=200, beta=0.1)  
    * ceiling(Pocock.ss$n.I)  
* Can instead use the O'Brien-Fleming rule which has increasing p-value hurdles as the sample size increases (most of the budget is saved for the full and final sample)  
	* OF <- gsDesign(k=3, test.type=2, sfu="OF")  
    * 2*(1-pnorm(OF$upper$bound))  
    * OF.ss <- gsDesign(k=3, test.type=2, sfu="OF", n.fix=200, beta=0.1)  
    * ceiling(OF.ss$n.I)  
  
Sample Size for Alternative Trial Designs:  
  
* Goal of an equivalence trial is to prove similarity to within a maximum specified delta  
	* library(TOSTER)  
    * powerTOSTtwo.prop(alpha = 0.05, statistical_power = 0.9, prop1 = 0.7, prop2 = 0.7, low_eqbound_prop = -0.05, high_eqbound_prop = 0.05)  
    * powerTOSTtwo.raw(alpha=0.05, statistical_power=0.8, sdpooled=15, low_eqbound=-3,high_eqbound=3)  
* May want to instead run a cluster-level randomized trial  
	* CRTSize::n4means(delta=1, sigma=2.5, m=25, ICC=0.1, alpha=0.05, power=0.90)  
* May want to instead run a factorial design randomized trial - powered such that it assumes independence (would not detect an interaction effect)  
	* power.prop.test(p1=0.40, p2=0.25, power=0.9)  
    * power.prop.test(p1=0.40, p2=0.23, power=0.9)  
  
Example code includes:  
```{r}

#Generate the sample size for delta of 1, with SD of 3 and 80% power.
ss1 <- power.t.test(delta=1, sd=3, power=0.8)
ss1

#Round up and display the numbers needed per group
ceiling(ss1$n)

#Use the sample size from above to show that it provides 80% power
power.t.test(n=ceiling(ss1$n), delta=1, sd=3)


#Generate a vector containing values between 0.5 and 2.0, incrementing by 0.25
delta <- seq(0.5, 2, 0.25)
npergp <- NULL

#Specify the standard deviation and power
for(i in 1:length(delta)){
  npergp[i] <- ceiling(power.t.test(delta = delta[i], sd = 3, power = 0.8)$n)
}

#Create a data frame from the deltas and sample sizes
sample.sizes <- data.frame(delta, npergp)

#Plot the patients per group against the treatment differences
ggplot(data=sample.sizes, aes(x=delta, y=npergp)) + 
    geom_line() + 
    geom_point() + 
    ggtitle("Sample Size Scenarios") + 
    xlab("Treatment Difference") + 
    ylab("Patients per Group")


#Use the power.prop.test to generate sample sizes for the proportions
power.prop.test(p1 = 0.4, p2 = 0.6, power = 0.8)

#Find the minimum detectable percentage for the above using 150 patients per group.
power.prop.test(p1 = 0.4, power = 0.8, n = 150)$p2*100


#Use 90% power, delta 1.5, standard deviations of 2.5, fraction of 0.5
unequalgps <- samplesize::n.ttest(power = 0.9, alpha = 0.05, mean.diff = 1.5, 
                                  sd1 = 2.5, sd2 = 2.5, k = 0.5, 
                                  design = "unpaired", fraction = "unbalanced"
                    )
unequalgps


#Generate sample sizes comparing the proportions using a two-sided test
two.sided <- power.prop.test(p1=0.1, p2=0.3, power=0.8, alternative = "two.sided")
two.sided
ceiling(two.sided$n)

#Repeat using a one-sided test
one.sided <- power.prop.test(p1=0.1, p2=0.3, power=0.8, alternative = "one.sided")
one.sided
ceiling(one.sided$n)

#Display the reduction per group
ceiling(two.sided$n)- ceiling(one.sided$n)


#Use the gsDesign function to generate the p-values for four analyses under the Pocock rule
Pocock <- gsDesign::gsDesign(k=4, test.type=2, sfu="Pocock")
Pocock
2*(1-pnorm(Pocock$upper$bound))

#Repeat for the the O'Brein & Fleming rule
OF <- gsDesign::gsDesign(k=4, test.type=2, sfu="OF")
OF
2*(1-pnorm(OF$upper$bound))


#Use the gsDesign function to generate the sample sizes at each stage under the Pocock rule
Pocock.ss <- gsDesign::gsDesign(k=4, test.type=2, sfu="Pocock", n.fix=500, beta=0.1)
ceiling(Pocock.ss$n.I)

#Repeat for the the O'Brein-Fleming rule
OF.ss <- gsDesign::gsDesign(k=4, test.type=2, sfu="OF", n.fix=500, beta=0.1)
ceiling(OF.ss$n.I)


#Find the sample size  per group for expected rates of 60%, 4% delta, 90% power and 5% significance level.
TOSTER::powerTOSTtwo.prop(alpha = 0.05, statistical_power = 0.9, prop1 = 0.6, prop2 = 0.6, 
                          low_eqbound_prop = -0.04, high_eqbound_prop = 0.04
                          )

#Find the power if the above trial is limited to 2500 per group
TOSTER::powerTOSTtwo.prop(alpha = 0.05, N=2500, prop1 = 0.6, prop2 = 0.6, 
                          low_eqbound_prop = -0.04, high_eqbound_prop = 0.04
                          )

#Find the sample size for a standard deviation of 10, delta of 2, 80% power and 5% significance level.
TOSTER::powerTOSTtwo.raw(alpha=0.05, statistical_power=0.8, sdpooled=10, low_eqbound=-2, high_eqbound=2)


#Find the sample sizes based on standard deviations between 7 and 13.
stdev <- seq(7, 13, 1)
npergp <- NULL
for(i in 1:length(stdev)){
    npergp[i] <- ceiling(TOSTER::powerTOSTtwo.raw(alpha=0.05, statistical_power=0.8, sdpooled=stdev[i],
                                                  low_eqbound=-2, high_eqbound=2
                                                  )
                         )
}
sample.sizes <- data.frame(stdev, npergp)

#Plot npergp against stdev
ggplot(data=sample.sizes, aes(x=stdev, y=npergp)) + 
    geom_line() +
    geom_point() + 
    ggtitle("Equivalence Sample Size Scenarios") +
    xlab("Standard Deviation") + 
    ylab("Patients per Group")

```
  
  
  
***
  
Chapter 4 - Statistical Analysis  
  
Regression Analysis:  
  
* May be additional explanatory variables associated with outcomes of interest; these could differ by treatment arms ("explanatory variables")  
* May be interested to see whether particular variables are associated with trial endpoints of particular interest  
* Can use simple linear regression to investigate the treatment effects  
	* asthma.trial$group <- relevel(asthma.trial$group, ref="Placebo")  
    * asthma.reg1 <- lm(fev.change ~group, asthma.trial)  
    * summary(asthma.reg1)  
    * t.test(fev.change~group, var.equal=TRUE, data=asthma.trial)  
    * asthma.reg2<-lm(fev.change ~group + age, asthma.trial)  
    * summary(asthma.reg2)  
* May want to use logistic regression to model binary outcomes  
	* asthma.logreg1=glm(attack~group + age, family=binomial(link="logit"), asthma.trial)  
    * summary(asthma.logreg1)  
    * exp(coefficients(asthma.logreg1)[2])  
    * exp(confint(asthma.logreg1)[2,])  
  
Analysis Sets, Subgroups, and Interactions:  
  
* Patient adherence may be imperfect for many reasons; as such, analysis is often re-done for sub-groups  
* Intention to treat (ITT) means looking at patient outcomes according to planned treatments rather than just those who complied and completed treatment  
	* Full Analysis Set (FAS) follows IT principles  
    * Per-Protocol Set (PPS) does not follow ITT principles and instead includes everyone  
* Can look at both FAS and PPS to compare outcomes  
	* asthma.fas<-lm(fev.change ~group , asthma.trial)  
    * asthma.pp<-lm(fev.change ~group , asthma.trial, subset = pp==1)  
* Can also look at subgroup analyses  
	* asthma.u65<-lm( fev.change ~group , asthma.trial, subset = age<65)  
    * asthma.o65<-lm( fev.change ~group , asthma.trial, subset = age>=65)  
* There is a risk of p-hacking with subgroup analyses, so they should be pre-specified rather than ad hoc  
* May also want to consider interaction effects for the trial  
	* asthma.ageg <- lm( fev.change ~group + agegroup , asthma.trial)  
    * summary(asthma.ageint)  
  
Multiplicity of Data:  
  
* Multiple subgroups of data - patients, looks, sub-groups, etc.  
* Goal is to keep the probability of Type I error low - roughly 0.05  
	* Multiplicity may make Type I errors cumulatively much more likely  
* Can adjust the p-values to maintain the desired Type I error rate  
	* p / n (Bonferroni) for n tests with a Type I error rate of p  
* There is typically a lack of power within the subgroups due to how the study is designed  
	* Subgroups should thus be limited, and based on hypotheses driven by previous research  
* There are often multiple endpoints for a study, so there needs to be clarity about which are confirmatory and which are exploratory  
* Composite endpoints can increase statistical power - for example, adding the causes of cardiac death  
* May also have repeated measurements due to collecting data at multiple timepoints  
  
Wrap up:  
  
* Well-conducted human clinical trials are common in the medical indistry and need to follow rigorous protocols and analyses  
	* Randomization, including several different methods  
    * Clinical trial designs such as cross-over, equivalence, and non-inferiority  
    * Sample size determination  
    * Statistical analysis for the overll group and sub-groups  
    * T-tests, Wilcoxon rank tests, test for equal proportions, logistic regression  
  
Example code includes:  
```{r}

#Explore the variable names with the str function
str(Acupuncture)

#Use the relevel function to set Control as reference group
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")


#Use lm to run and store the model in linreg1
linreg1 <- lm(pct.month12 ~ treatment.group + sex + score.baseline.4, data=Acupuncture, 
              na.action = na.exclude
              )

#Display the results of linreg1
summary(linreg1)


#Use lm to run and store the model in linreg2
linreg2 <- lm(pct.month12 ~ treatment.group + score.baseline.4, data=Acupuncture, na.action = na.exclude)

#Display the results of linreg2
summary(linreg2)

#Add the predicted values to the Acupuncture dataset for linreg2 using the predict function 
Acupuncture$pred.linreg2 <- predict(linreg2)


#Plot the predicted values against baseline score quartile grouping by treatment.
ggplot(data = subset(Acupuncture, !is.na(pred.linreg2)), 
       aes(x = score.baseline.4, y = pred.linreg2, group = treatment.group)
       ) + 
    geom_line(aes(color = treatment.group)) +
    geom_point(aes(color = treatment.group)) + 
    ggtitle("Predicted Values from Linear Regression Model") + 
    xlab("Baseline Score Quartile") + 
    ylab("Percentage Change from Baseline at M12")


#Use the relevel function to set "Control" as the reference for treatment
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")

#Use the relevel function to set "less than or eq to 35%" as the reference for resp35.month12
Acupuncture$resp35.month12 <- relevel(Acupuncture$resp35.month12, ref="less than or eq to 35%")

#Use glm to run and store the model in logreg1
logreg1 <- glm(resp35.month12 ~ treatment.group + sex  + score.baseline.4, 
               family=binomial(link="logit"), data=Acupuncture
               )

#Display the results of logreg1
summary(logreg1)

#Display the odds ratio and 95% CI for Acupuncture vs Control
exp(coefficients(logreg1)[2])
exp(confint(logreg1)[2,])


#Tabulate withdrawal.reason
table(Acupuncture$withdrawal.reason,  useNA="ifany")

#Tabulate completedacupuncturetreatment by treatment.group
table(Acupuncture$completedacupuncturetreatment, Acupuncture$treatment.group,  useNA="ifany") 

#Create a per protocol flag that is TRUE if patients met the criteria
Acupuncture <- Acupuncture %>%
    mutate(pp = is.na(withdrawal.reason) & 
               ((completedacupuncturetreatment==1 & treatment.group=="Acupuncture") | 
                    (is.na(completedacupuncturetreatment) & treatment.group=="Control")
                )
           )
Acupuncture$pp[is.na(Acupuncture$pp)] <- FALSE
Acupuncture$pp <- as.factor(Acupuncture$pp)

#Tabulate the per protocol flag
table(Acupuncture$pp)


#Use the relevel function to set Control as reference group
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")

#Use lm to run and store the model without interaction in linregnoint
linregnoint <- lm(pct.month12 ~ treatment.group + score.baseline.4, Acupuncture, na.action = na.exclude)

#Use lm to run and store the model with interaction in linregint
linregint <- lm(pct.month12 ~ treatment.group*score.baseline.4, Acupuncture, na.action = na.exclude)

#Display the results of linregnoint and linregint
summary(linregnoint)
summary(linregint)

#Compare the models with the anova command
anova(linregnoint, linregint)


#Tabulate the age group variable to view the categories
table(Acupuncture$age.group)

#Display the adjusted significance level
0.05/4

#Run the Wilcoxon Rank Sum test in each of the age subgroups
age <- c("18-34", "35-44", "45-54", "55-65")
for(group in age){
  subgroup <- broom::tidy(wilcox.test(total.days.sick ~ treatment.group, 
                                      data = subset(Acupuncture, age.group==group), 
                                      exact=FALSE
                                      )
                          )
  print(group)
  print(subgroup)
}


#Tabulate the combined endpoint by treatment group
table(Acupuncture$combined, Acupuncture$treatment.group, useNA="ifany")

#Use the relevel function to set Control as reference group
Acupuncture$treatment.group <- relevel(Acupuncture$treatment.group, ref="Control")

#Use compareGroups to generate and save the treatment effect for the composite endpoint amd each component
combined.test <- compareGroups::compareGroups(treatment.group ~ combined + any.therap.visits + any.gp.visits + any.spec.visits, data = Acupuncture)

# Store the results in a table
combined.test.table <- compareGroups::createTable(combined.test, show.ratio = TRUE, show.p.overall=FALSE)

#Display the results
combined.test.table

```
  
  
  
***
  
### _Financial Analytics in R_  
  
Chapter 1 - Intro to Valuations (Cash is King)  
  
Valuations Overview:  
  
* Can value economic value based on cash flows using R and the tidyverse  
* Focus for the course is valuation of projects within a company  
* Discounted cash flows use an interest rate to discount future expenditures and revenues to the present value  
* Cash flow statements typically have time as the column, with variables such as spend or revenue as rows  
	* May want to tidy data for analysis where the column is the variable and the row is the observation  
  
Business Models and Writing R Functions:  
  
* The business model includes the timing of earning revenue and incurring expenses  
	* Operating revenue is driven by the units sold and the price per unit (for consumer products)  
    * Operating revenue is driven by enrolment, churn, monthly fees, etc. (for subscription products)  
    * Direct Expenses are driven by the units sold and the cost per unit (marginal direct costs)  
    * Operating expenses (overhead) are costs required to run the business but not directly tied to a specific sale - e.g., SG&A, depreciation, etc.  
* The timing of revenues and expenses are on an accrual basis - for example, raw materials may be purchased well before production, but accrual would be when producing  
* Gross Profit = Operative Revenue minus Direct Expenses  
* Can use a function for calculating the business model  
	* calc_business_model <- function(assumptions, price_per_unit, cost_per_unit){  
    *     model <- assumptions  
    *     model$revenue <- model$sales * price_per_unit  
    *     model$direct_expense <- model$sales * cost_per_unit  
    *     model$gross_profit <- model$revenue - model$direct_expenses  
    * model  
    * }  
    * calc_business_model( assumptions, price_per_unit = 10, cost_per_unit = 2 )  
  
Pro-Forma Income Statements:  
  
* The income statement (P&L) translates revenues and expenses to net income  
	* overhead_expense <- sga + depreciation + amortization  
    * operating_profit <- gross_profit - overhead_expense  
* Depreciation is the idea that large capital expenditures should be spread over their useful lifetimes  
	* Amortization is a similar concept for non-physical investments (such as R&D)  
    * Depreciation per Year = (Book Value - Salvage Value) / Years of Useful Life  
    * depreciation_per_period <- (book_value - salvage_value)/useful_life  
    * depreciation <- rep(depreciation_per_period, useful_life)  
* Can calculate valuations as either levered (financing, including tax considerations and interest) or unlevered (ignores financing considerations)  
* Generally need to include the tax considerations  
	* tax <- operating_income * tax_rate  
    * net_income <- operating_income - tax  
  
Income to Cash:  
  
* Net income needs to be adjusted to create free cash flow  
* Cash is king - need to be able to convert the income in to no-string-attached cash  
* Income is recognized based on accrual while cash is recognized when it is received or spent  
	* The income statement may majorly differ from the cash flow particularly due to depreciation  
* To convert from income to free cash flow, convert as follows  
	* Add back depreciation and amortization (only care about actual spending)  
    * Subtract out CAPEX (care about recognizing all of the capital asset when it is purchased)  
    * Adjust for net working capital (NWC)  
    * cashflow <- net_income + depreciation_exp - capex + nwc_changes  
  
Example code includes:  
```{r}

# define inputs
price <- 20
print_cost <- 0.5
ship_cost <- 2

assumptions <- data.frame(year=1:5, sales=c(175, 200, 180, 100, 50))

# add revenue, expense, and profit variables
cashflow <- assumptions
cashflow$revenue <- cashflow$sales * price
cashflow$direct_expense <- cashflow$sales * (print_cost + ship_cost) 
cashflow$gross_profit <- cashflow$revenue - cashflow$direct_expense

# print cashflow
print(cashflow)

prem_ts <- data.frame(MONTH=1:60, COST_PER_SONG=0.01, SONG_LENGTH=3, REV_PER_SUB=10) %>%
    mutate(PCT_ACTIVE=0.95**(MONTH-1), HOURS_PER_MONTH=528-0.08*(MONTH-40)**2)

# premium business models
premium_model <- prem_ts
premium_model$SONGS_PLAYED <- prem_ts$PCT_ACTIVE * prem_ts$HOURS_PER_MONTH * 1 / prem_ts$SONG_LENGTH
premium_model$REV_SUBSCRIPTION <- prem_ts$PCT_ACTIVE * prem_ts$REV_PER_SUB
premium_model$COST_SONG_PLAYED <- premium_model$SONGS_PLAYED * prem_ts$COST_PER_SONG

# inspect results
head(premium_model)


free_ts <- data.frame(MONTH=1:60, PROP_MUSIC=0.95, REV_PER_AD=0.02, REV_PER_CLICK=10, 
                      COST_PER_SONG=0.01, SONG_LENGTH=3, AD_LENGTH=0.25, CLICK_THROUGH_RATE=0.001
                      ) %>%
    mutate(PCT_ACTIVE=0.97**(MONTH-1), HOURS_PER_MONTH=480-0.08*(MONTH-40)**2)

# freemium business models
freemium_model <- free_ts
freemium_model$SONGS_PLAYED <- free_ts$PCT_ACTIVE * free_ts$HOURS_PER_MONTH * free_ts$PROP_MUSIC / free_ts$SONG_LENGTH
freemium_model$ADS_PLAYED <- free_ts$PCT_ACTIVE * free_ts$HOURS_PER_MONTH * (1-free_ts$PROP_MUSIC) / free_ts$AD_LENGTH
freemium_model$REV_AD_PLAYED <- freemium_model$ADS_PLAYED * free_ts$REV_PER_AD
freemium_model$REV_AD_CLICKED <- freemium_model$ADS_PLAYED * free_ts$CLICK_THROUGH_RATE * free_ts$REV_PER_CLICK
freemium_model$COST_SONG_PLAYED <- freemium_model$SONGS_PLAYED * free_ts$COST_PER_SONG

# examine output
head(freemium_model)


# Define function: calc_business_model
calc_business_model <- function(assumptions, price, print_cost, ship_cost){
    cashflow <- assumptions
    cashflow$revenue <- cashflow$sales * price
    cashflow$direct_expense <- cashflow$sales * (print_cost + ship_cost) 
    cashflow$gross_profit <- cashflow$revenue - cashflow$direct_expense
    cashflow
}

# Call calc_business_model function for different sales prices
assumptions
calc_business_model(assumptions, 20, 0.5, 2)$gross_profit
calc_business_model(assumptions, 25, 0.5, 2)$gross_profit


# Inputs
production <- data.frame(Month=1:60, Units=rep(c(60, 50, 40, 30), times=15))

cost <- 100000
life <- 60
salvage <- 10000

# Compute depreciation
production$Depr_Straight <- (cost - salvage)/life
production$Depr_UnitsProd <- (cost - salvage)*(production$Units) / sum(production$Units)

# Plot two depreciation schedules
ggplot(production, aes(x = Month)) + 
    geom_line(aes(y = Depr_Straight)) + 
    geom_line(aes(y = Depr_UnitsProd))


# Business model
cashflow

cashflow$revenue <- cashflow$revenue + 2 * cashflow$sales
cashflow$gross_profit <- cashflow$revenue - cashflow$direct_expense

# Income statement
cashflow$depr_sl <- (1000 - 0) / 5
cashflow$operating_profit <- cashflow$gross_profit - cashflow$depr_sl
cashflow$tax <- cashflow$operating_profit * 0.3
cashflow$net_income <- cashflow$operating_profit - cashflow$tax

# Inspect dataset
cashflow


# Calculate income statement
assumptions <- data.frame(unit_sales=100000*c(1, 2, 4, 8), machines_purchased=c(1, 1, 2, 4), 
                          depreciation=10000000*c(4, 8, 16, 32)
                          )
assumptions
price_per_unit <- 1000
cogs_per_unit <- 450
labor_per_unit <- 50

income_statement <- assumptions
income_statement$revenue <- income_statement$unit_sales * price_per_unit
income_statement$expenses <- income_statement$unit_sales * (cogs_per_unit + labor_per_unit)
income_statement$earnings <- income_statement$revenue - income_statement$expenses - income_statement$depreciation

# Summarize cumulative earnings
sum(income_statement$earnings)
sum(income_statement$earnings) / sum(income_statement$revenue)


# calculate free cashflow
cashflow <- income_statement
cashflow$operating_cf <- cashflow$earnings + cashflow$depreciation
cashflow$capex <- cashflow$machines_purchased * 160000000
cashflow$free_cf <- cashflow$operating_cf - cashflow$capex

# summarize free cashflow
sum(cashflow$free_cf)

```
  
  
  
***
  
Chapter 2 - Key Financial Concepts (Time is Money)  
  
Time Value of Money:  
  
* Money today is generally more valuable than money tomorrow  
	* Can use a compounding interest rate, so V(n) = V(0) * (1 + r)**n  
    * Therefore, PV = FV(n) / (1+r)**n - the discounting rate is r, applied over n time periods  
    * mutate(data, pv = fv / (1 + r)^n)  
  
Different Discount Rates:  
  
* Need to clarify the types of discount rates - need to ensure the same time periods as the cash flows  
* Can convert discount rates between time periods using appropriate compounding  
	* r2 = [(1 + r1)^(# r1 units per 1 r2 unit) ] - 1  
    * r_quart <- (1 + r_mth)^3 - 1  
    * r_quart <- (1 + r_ann)^(1/4) - 1  
* Need to also consider the differences in real and nominal rates  
	* Can think about inflation or deflation as relates to purchaing power for an item  
* Generally easiest for cash flows to reflect real cash-flows and discounted by real interest rates  
	* r_real=r_nominal / (1+inflation_rate)  
    * r_nominal=r_real*(1+inflation_rate)  
  
Discounting Multiple Cash Flows:  
  
* Cash flow differs dramatically based on time in the future when it is received  
* Need to discount the cash flows back to present values prior to summing across them  
	* pv <- calc_pv(fv = 100, r = 0.01, n = 3)  # single caseh flow  
    * cashflows <- c(0, -50, 25, 100, 175, 250, 250)  
    * discounted_cashflows <- calc_pv(cashflows, r = 0.01, n = 0:6)  # vectorized calculation, once for each of 0:6  
* Vectorized functions work well with the Tidyverse  
	* many_cashflows %>% group_by(option) %>% summarize( PV = sum(calc_pv(cashflow, 0.08, n = time))  
  
Example code includes:  
```{r}

# Assign input variables
fv <- 100
r <- 0.08

# Calculate PV if receive FV in 1 year
pv_1 <- 100 / (1 + r)**1
pv_1

# Calculate PV if receive FV in 5 years
pv_5 <- 100 / (1 + r)**5
pv_5

# Calculate difference
pv_1 - pv_5


# Define PV function: calc_pv
calc_pv <- function(fv, r, n){
    pv <- fv / (1+r)**n
    pv
}

# Use PV function for 1 input
calc_pv(100, 0.08, 5)

# Use PV function for range of inputs
n_range <- 1:10
pv_range <- calc_pv(100, 0.08, n_range)
pv_range


# Calculate present values in dataframe
present_values <- data.frame(n = 1:10) %>% mutate(pv = 100 / (1 + 0.08)**n)

# Plot relationship between time periods versus present value
ggplot(present_values, aes(x = n, y = pv)) +
    geom_line() +
    geom_label(aes(label = paste0("$",round(pv,0)))) +
    ylim(0,100) +
    labs(
        title = "Discounted Value of $100 by Year Received", 
        x = "Number of Years in the Future",
        y = "Present Value ($)"
        )


# Calculate present values over range of time periods and discount rates
present_values <- 
    expand.grid(n = 1:10, r = seq(0.05,0.12,0.01)) %>%
    mutate(pv = calc_pv(100, r, n))
     
# Plot present value versus time delay with a separate colored line for each rate
ggplot(present_values, aes(x = n, y = pv, col = factor(r))) +
    geom_line() +
    ylim(0,100) +
    labs(
        title = "Discounted Value of $100 by Year Received", 
        x = "Number of Years in the Future",
        y = "Present Value ($)",
        col = "Discount Rate"
        )


# Convert monthly to other time periods
r1_mth <- 0.005
r1_quart <- (1 + r1_mth)^3 - 1
r1_semi <- (1 + r1_mth)^6 - 1
r1_ann <- (1 + r1_mth)^12 - 1

# Convert years to other time periods
r2_ann <- 0.08
r2_mth <- (1 + r2_ann)^(1/12) - 1
r2_quart <- (1 + r2_ann)^(1/4) - 1


# Convert real to nominal
r1_real <- 0.08
inflation1 <- 0.03
(r1_nom <- (1 + r1_real) * (1 + inflation1) - 1)

# Convert nominal to real
r2_nom <- 0.2
inflation2 <- 0.05
(r2_real <- (1 + r2_nom) / (1 + inflation2) - 1)


# Define cashflows
cashflow_a <- c(5000, rep(0,6))
cashflow_b <- c(0, rep(1000, 6))

# Calculate pv for each time period
disc_cashflow_a <- calc_pv(cashflow_a, 0.06, 0:6)
disc_cashflow_b <- calc_pv(cashflow_b, 0.06, 0:6)

# Calculate and report total present value for each option
(pv_a <- sum(disc_cashflow_a))
(pv_b <- sum(disc_cashflow_b))


# Define cashflows
cashflow_old <- rep(-500, 11)
cashflow_new <- c(-2200, rep(-300, 10))
options <- data.frame(time = rep(0:10, 2),
                      option = c(rep("Old",11), rep("New",11)),
                      cashflow = c(cashflow_old, cashflow_new)
                      )
                
# Calculate total expenditure with and without discounting
options %>%
    group_by(option) %>%
    summarize(sum_cashflow = sum(cashflow),
              sum_disc_cashflow = sum(calc_pv(cashflow, 0.12, 0:10))
              )

```
  
  
  
***
  
Chapter 3 - Prioritizing Profitability (Financial Metrics)  
  
Profitability Metrics and Payback Period:  
  
* Profitability metrics help to quantify how projects bring value to firms  
* Decision rules help to interpret metrics for decisions and comparisons  
	* Absolute vs. relative  
    * Constrained vs. unconstrained  
* All summary metrics have shortcomings; none are perfect in all situations  
	* Decision rules are communication tools, not hard and fast "laws"  
    * There are many corporate objectives over and above discounted cash flows; important input but not the sole answer  
* Payback periods help to assess the amount of time needed before full payback - sooner is better  
	* Does not consider TVM and does not consider profits after the payback period  
    * cashflows <- c(-10000, 2500, 3000, 5000, 6000, 1000)  
    * cumsum(cashflows) + init_investment  
  
NPV, IRR, Profitability Index:  
  
* NPV - Net Present Value is net of the investment costs  
	* In an unconstrained world, any NPV-positive project would be pursued  
    * n <- 0:(length(cashflows) - 1)  
    * npv <- sum( calc_pv(cashflows, r, n) )  
* IRR - Internal Rate of Return is the required rate of return for an investment to break-even over a stated time period  
	* Also called the "hurdle rate" since it is the rate required for a project to hurdle in to profitability  
    * # assume we have calc_npv function with signature:  
    * # calc_npv(cashflows, r)  
    * uniroot(calc_npv, interval = c(0, 1), cashflows = cashflows)$root  
* Profitability index - ratio between sum of discounted cash flows over the cost of the initial investment  
	* A value of 1 is a break-even that only recoups the investment  
    * npv_fcf <- calc_npv(future_cashflow, r)  
    * profitability_index <- npv_fcf / abs(initial_investment)  
  
Terminal Value:  
  
* Terminal Value (TV) is residual value outside the specified time period of the cash flow analysis  
	* Can add TV as a summary metric for the cash flows (discounted) after the final time period  
    * final_cashflow <- cashflow[n]  
    * terminal_value_period_n <- final_cashflow / (discount_rate - growth_rate)  
    * terminal_value_as_present <- terminal_value_period_n / (1 + discount_rate)^n  
* Can also use the exit multiplier method, in which a benchmark is applied to the cash flows (sales, revenues, etc.)  
  
Comparing and Computing Metrics:  
  
* NPV and IRR are important metrics with related information - however, they may give different constrained project decisions  
	* By definition, IRR is the discount rate that lets a project break-even on an NPV basis  
* NPV focuses on profit (favors large investments), while IRR focuses on return (favors small investments with high returns)  
	* Best solution can be to examine both metrics  
    * options %>% group_by(option) %>% summarize(npv=calc_npv(cf,0.08))  
  
Wrap up:  
  
* Strengths, weaknesses, and blind spots of various metrics  
	* Summary metrics alone can be deceptive  
* Additional metrics include ROE (equity) and ROA (assets)  
* Industries and even companies may have their own key metrics of interest  
  
Example code includes:  
```{r}

cashflows <- c(-50000, 1000, 5000, 5000, 5000, 10000, 10000, 10000, 10000, 10000, 10000)

# Inspect variables
cashflows

# Calculate cumulative cashflows
cum_cashflows <- cumsum(cashflows)

# Identify payback period
payback_period <- min(which(cum_cashflows >= 0)) - 1

# View result
payback_period


# Define payback function: calc_payback
calc_payback <- function(cashflows) {
  cum_cashflows <- cumsum(cashflows)
  payback_period <- min(which(cum_cashflows >= 0)) - 1
  payback_period
}

# Test out our function
cashflows <- c(-100, 50, 50, 50)
calc_payback(cashflows) == 2


cashflows <- c(-50000, 1000, 5000, 5000, 5000, 10000, 10000, 10000, 10000, 10000, 10000)

# normal payback period
payback_period <- calc_payback(cashflows)

# discounted payback period
discounted_cashflows <- calc_pv(cashflows, r = 0.06, n = 0:(length(cashflows)-1) )
payback_period_disc <- calc_payback(discounted_cashflows)

# compare results
payback_period
payback_period_disc


# Define NPV function: calc_npv
calc_npv <- function(cashflows, r) {
  n <- 0:(length(cashflows) - 1)
  npv <- sum( calc_pv(cashflows, r, n) )
  npv
}


# The base R function stats::uniroot can help find values between a lower bound (lower) and an upper bound (upper) where the value of a function is zero
# This can help us calculate the internal rate of return (IRR) for which NPV = 0.

# Define IRR function: calc_irr
calc_irr <- function(cashflows) {
    uniroot(calc_npv, 
        interval = c(0, 1), 
        cashflows = cashflows)$root
}

# Try out function on valid input
cashflows <- c(-100, 20, 20, 20, 20, 20, 20, 10, 5)
calc_irr(cashflows)


# Define profitability index function: calc_profitability_index
calc_profitability_index <- function(init_investment, future_cashflows, r) {
    discounted_future_cashflows <- calc_npv(future_cashflows, r)
    discounted_future_cashflows / abs(init_investment)
}

# Try out function on valid input
init_investment <- -100
cashflows <- c(0, 20, 20, 20, 20, 20, 20, 10, 5)
calc_profitability_index(init_investment, cashflows, 0.08)


# pull last year cashflow from vector of cashflows
last_year_cashflow <- cashflow[length(cashflow)]
last_period_n <- length(cashflow) - 1

# calculate terminal value for different discount raes
terminal_value_1 <- last_year_cashflow / ((0.15 - 0.1)*(1 + 0.15)^last_period_n)
terminal_value_2 <- last_year_cashflow / ((0.15 - 0.01)*(1 + 0.15)^last_period_n)
terminal_value_3 <- last_year_cashflow / ((0.15 + 0.05)*(1 + 0.15)^last_period_n)

# inspect results
terminal_value_1 
terminal_value_2
terminal_value_3


cashflow1 <- c(-50000, 100, 2000, 2000, 5000, 10000, 10000, 10000, 10000, 10000, 10000)
cashflow2 <- c(-1e+05, 20000, 20000, 20000, 20000, 20000)
cashflow3 <- c(-8000, 6000, 5000, 4000, 3000, 2000, 1000, 0)

# calculate internal rate of return (IRR) for each stream of cashflows
r1 <- calc_irr(cashflow1)
r2 <- calc_irr(cashflow2)
r3 <- calc_irr(cashflow3)

# calculate net present value (NPV) for each stream of cashflows, assuming r = irr
npv1 <- calc_npv(cashflow1, r1)
npv2 <- calc_npv(cashflow2, r2)
npv3 <- calc_npv(cashflow3, r3)

# examine results
npv1
npv2
npv3


cf1 <- c(-5000, 450, 450, 450, 450, 450, 450, 450, 450, 450, 450)
cf2 <- c(-5000, 2000, 2000, 2000, 2000, 2000, 2000, -2000, -2000, -2000, -2000)
rates <- c(0, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1, 0.105, 0.11, 0.115, 0.12, 0.125, 0.13, 0.135, 0.14, 0.145, 0.15, 0.155, 0.16, 0.165, 0.17, 0.175, 0.18, 0.185, 0.19, 0.195, 0.2, 0.205, 0.21, 0.215, 0.22, 0.225, 0.23, 0.235, 0.24, 0.245, 0.25)

# create dataset of NPV for each cashflow and rate
npv_by_rates <- data.frame(rates) %>%
    group_by(rates) %>%
    mutate(npv1 = calc_npv(cf1, rates), npv2 = calc_npv(cf2, rates))
   
# plot cashflows over different discount rates     
ggplot(npv_by_rates, aes(x = rates, y = npv1)) + 
    geom_line() +
    geom_line(aes(y = npv2)) +
    labs(title = "NPV by Discount Rate", subtitle = "A Tale of Two Troubling Cashflows",
         y = "NPV ($)",x = "Discount Rate (%)"
         ) +
    annotate("text", x = 0.2, y = -500, label = "Two break-even points") +
    annotate("text", x = 0.2, y = -2500, label = "No break-even point")


cashflows <- data.frame(option=rep(1:4, each=11), time=rep(0:10, times=4), 
                        cashflow=c(-10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, -1000, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, -1e+05, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, -10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                        )

# calculate summary metrics
cashflow_comparison <-
  cashflows %>%
  group_by(option) %>%
  summarize(npv = calc_npv(cashflow, 0.1), irr = calc_irr(cashflow))
      
# inspect output
cashflow_comparison
             
# visualize summary metrics
ggplot(cashflow_comparison, aes(x = npv, y = irr, col = factor(option))) +
    geom_point(size = 5) +
    geom_hline(yintercept = 0.1) +
    scale_y_continuous(label = scales::percent) +
    scale_x_continuous(label = scales::dollar) +
    labs(title = "NPV versus IRR for Project Alternatives",
         subtitle = "NPV calculation assumes 10% discount rate",
         caption = "Line shows actual discount rate to asses IRR break-even",
         x = "NPV ($)", y = "IRR (%)",col = "Option"
         )

```
  
  
  
***
  
Chapter 4 - Understanding Outcomes  
  
Building a Business Case:  
  
* Can combine metrics to evaluate potential business models  
* Example of working for a data-driven coffee shop - move from coffee to nitro-brew  
	* Consider only the incremental costs; for example rent and sunk costs (prior to the project) should not be considered  
    * May want to consider side-effects such as self-cannibalization  
* Business modeling is part art and part science; need a good blend  
  
Scenario Analysis with tidyr and purrr:  
  
* May want to consider additional projects, and there may not be funding for pursuing all of these ideas  
* May be some variability in the accuracy of our estimates - "what if analysis", aka "scenario analysis"  
	* scenario1 <- mutate(assumptions, var1 = 1.2 * var1)  
    * cashflow1 <- calc_model(scenario1)  
    * calc_npv(cashflow1)  
* Can use tidyr and purr to automate some of this  
	* all_scenarios %>% nest(-scenario)  
    * all_scenarios %>% nest(-scenario) %>% mutate( cashflow = map_df( data, calc_model) )  # data frame in column 'data' will be passed to calc_model  
    * all_scenarios %>% nest(-scenario) %>% mutate( cashflow = map_df( data, calc_model) ) %>% mutate( npv = map_dbl( cashflow, calc_npv) )  
  
Sensitivity analysis:  
  
* Sensitivity analysis is an approach to uncertainty that assesses the impact of each base assumption on the outcomes  
	* sensitivity <- expand.grid( factor = c(0.5, 1, 1.5), metric = c("vbl1", "vbl2") )  
    * sensitivity <- expand.grid( factor = c(0.5, 1, 1.5), metric = c("vbl1", "vbl2") ) %>% mutate(scenario = map2(metric, factor, ~factor_data(assumptions, .x, .y)))  # The ~ is for the anaonymous function  
* Can then visualize the impact of the sensitivity analysis; change in overall NPV vs. % change in base metric  
* Caution that errors are often correlated - this is just a univariate sensitivity analysis  
* Caution that not all metrics make the same magnitudes of change; analysis could be misinterpreted as a result  
  
Communicating Cashflow Concepts:  
  
* Long data is tidy - one column per metric and one row per observation (time)  
* Financial stakeholders will be more accustomed to non-tidy data  
	* long_cashflow <- gather(cashflow, key = Month, value = Value, -Metric)  # make the data long with key column Month and values in Value and Metric left as a column  
    * tidy_cashflow <- spread(long_cashflow, key = Metric, value = Value, -Metric)  
* Waterfall diagrams may be useful for communicating outcomes  
	* ggplot(data) + geom_rect( aes( xmin = , xmax = , ymin = , ymax = ) )  
    * ggplot(waterfall_data, aes( xmin = rn - 0.25, xmax = rn + 0.25, ymin = start, ymax = end) ) + geom_rect() + scale_x_continuous( breaks = waterfall_data$rn, labels = waterfall_data$category )  
  
Advanced Topics in Cashflow Modeling:  
  
* Many additional topics can be covered in financial modeling  
	* Debt vs. equity financing  
    * Decisions made today change the options available to us in the future  
    * Probabilistic simulation (impacts of uncertainty)  
  
Example code includes:  
```{r}

assumptions <- data.frame(year=0:10, 
                          unit_sales_per_day=c(0, 10, 12, 14, 15, 16, 17, 18, 18, 18, 18),
                          capex=c(5000, rep(0, 10)),
                          pct_cannibalization=c(0, rep(0.25, 10)), 
                          maintenance_cost=c(0, rep(250, 10)), 
                          depreciation_cost=c(0, rep(500, 10)), 
                          profit_margin_per_nitro=3, 
                          profit_margin_per_regular=1, 
                          labor_cost_per_hour=8, 
                          days_open_per_year=250
                          )

# Check the first few rows of the data
head(assumptions)

# Check the variable names of the data
names(assumptions)

# Plot the trend of unit_sales_per_day by year
ggplot(assumptions, aes(x = year, y = unit_sales_per_day)) + 
    geom_line()

tax_rate <- 0.36

# Create the cashflow_statement dataframe
cashflow_statement <-
  mutate(assumptions,
    sales_per_year = unit_sales_per_day * days_open_per_year,
    sales_revenue = sales_per_year * profit_margin_per_nitro,
    total_revenue = sales_revenue,
    labor_cost = days_open_per_year * 0.5 * labor_cost_per_hour,
    cannibalization_cost = sales_per_year * pct_cannibalization * profit_margin_per_regular,
    direct_expense = labor_cost + cannibalization_cost + maintenance_cost,
    gross_profit = total_revenue - direct_expense,
    operating_income = gross_profit - depreciation_cost,
    net_income = operating_income * (1 - tax_rate), 
    cashflow = net_income + depreciation_cost - capex    
  )


# build individual scenarios
optimist <- mutate(assumptions, unit_sales_per_day = unit_sales_per_day * 1.2, pct_cannibalization = 0.1)
pessimist <- mutate(assumptions, unit_sales_per_day = unit_sales_per_day * 0.8, profit_margin_per_nitro = 1)

# combine into one dataset
scenarios <-
  bind_rows(
    mutate(pessimist, scenario = "pessimist"),
    mutate(assumptions, scenario = "realist"),
    mutate(optimist, scenario = "optimist")
  )


calc_model <- function(assumptions){
  mutate( assumptions,
    sales_per_year = unit_sales_per_day * days_open_per_year,
    sales_revenue = sales_per_year * profit_margin_per_nitro,
    total_revenue = sales_revenue,
    labor_cost = days_open_per_year * 0.5 * labor_cost_per_hour,
    cannibalization_cost = sales_per_year * pct_cannibalization * profit_margin_per_regular,
    direct_expense = labor_cost + cannibalization_cost + maintenance_cost,
    gross_profit = total_revenue - direct_expense,
    operating_income = gross_profit - depreciation_cost,
    net_income = operating_income * (1 - 0.36), 
    cashflow = net_income + depreciation_cost - capex    
  )
}

calc_npv_from_cashflow <- function(cashflow, r){
  cashflow_line <- cashflow$cashflow
  sum(calc_pv(cashflow_line, r, 0:(length(cashflow_line)-1)))
}

# calculate scenario NPVs
scenario_analysis <- scenarios %>%
    nest(-scenario) %>%
    mutate(cashflow = map(data, calc_model)) %>%
    mutate(npv = map_dbl(cashflow, calc_npv_from_cashflow, 0.2))

# inspect results
select(scenario_analysis, scenario, npv)


# scenario analysis bar chart
ggplot(data = scenario_analysis, aes(x = scenario, y = npv, fill = scenario)) + 
    geom_bar(stat = "identity") +
    scale_y_continuous(labels = scales::dollar) +
    labs(title = "NPV Scenario Analysis of Nitro Coffee Expansion") +
    guides(fill = FALSE)


# define sensitivity factor function
factor_data <- function(data, metric, factor){
  data[[metric]] <- data[[metric]] * factor
  data
}

# create sensitivity analysis
sensitivity <-
  expand.grid(
    factor = seq(0.5,1.5,0.1), 
    metric = c("profit_margin_per_nitro", "labor_cost_per_hour", "pct_cannibalization", "unit_sales_per_day")) %>%
  mutate(scenario = map2(metric, factor, ~factor_data(assumptions, .x, .y))) %>%
  mutate(cashflow = map(scenario, calc_model)) %>% 
  mutate(npv = map_dbl(cashflow, calc_npv_from_cashflow, r = 0.2))


ggplot(sensitivity,
       aes(x = factor, y = npv, col = metric)
       ) +
  geom_line() +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Sensivity Analysis",
    x = "Factor on Original Assumption",
    y = "Projected NPV",
    col = "Metric"
  )


# examine current cashflow strucutre
tidy_cashflow <- data.frame(Month=1:6, 
                            Received=c(100, 200, 300, 400, 500, 500), 
                            Spent=c(150, 175, 200, 225, 250, 250)
                            )

# create long_cashflow with gather
# long_cashflow <- tidyr::gather(cashflow, key = Month, value = Value, -Metric)

# create tidy_cashflow with spread
# tidy_cashflow <- tidyr::spread(long_cashflow, key = Metric, value = Value)

# examine results
tidy_cashflow


# create long_cashflow with gather
long_cashflow <- tidyr::gather(tidy_cashflow, key = Metric, value = Value, -Month)

# create untidy_cashflow with spread
untidy_cashflow <- tidyr::spread(long_cashflow, key = Month, value = Value)

# examine results
untidy_cashflow


gross_profit_summary <- data.frame(metric=c("Sales Revenue", "Keg Cost", "Cannibalization Cost", "Labor Cost", "Maintenance Cost"), 
                                   value=c(187200, -78240, -31200, -10000, -2500)
                                   )

# compute min and maxes for each line item
waterfall_items <-
  mutate(gross_profit_summary,
         end = cumsum(value), 
         start = lag(cumsum(value), 1, default = 0))

# compute totals row for waterfall metrics
waterfall_summary <- 
  data.frame(metric = "Gross Profit", 
             end = sum(gross_profit_summary$value), 
             start = 0)

# combine line items with summary row
waterfall_data <-
  bind_rows(waterfall_items, waterfall_summary) %>%
  mutate(row_num = row_number())


# Plot waterfall diagram
ggplot(waterfall_data, aes(fill = (end > start))) +
  geom_rect(aes(xmin = row_num - 0.25, xmax = row_num + 0.25, 
                ymin = start, ymax = end)) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(breaks = waterfall_data$row_num, labels = waterfall_data$metric) +
  # Styling provided for you - check out a ggplot course for more information!
  scale_y_continuous(labels = scales::dollar) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  guides(fill = FALSE) +
  labs(
      title = "Gross Profit for Proposed Nitro Coffee Expansion",
      subtitle = "Based on pro forma 10-year forecast")

```
  
  
  
***
  
### _Visualizing Big Data with Trelliscope_  
  
Chapter 1 - General Strategies for Visualizing Big Data  
  
Visualizing summaries:  
  
* Start with summarization plots for large datasets  
* Univariate variable descriptions from the gapminder dataset  
	* Continuous - histograms  
    * ggplot(gapminder, aes(lifeExp)) + geom_histogram()  
    * Discrete - bar chart for counts  
    * ggplot(gapminder, aes(continent)) + geom_bar()  
    * Temporal - bin by time, compute number of observations  
    * by_year <- gapminder %>% group_by(year) %>% summarise(medianGdpPercap = median(gdpPercap, na.rm = TRUE))  
    * ggplot(by_year, aes(year, medianGdpPercap)) + geom_line()  
* Binning is a common strategy for summarizing large datasets - particularly the dplyr groupby function  
* Can extend to the NYC taxi cab dataset - millions of records on the number of taxi trips taken in NYC  
  
Adding more detail to summaries:  
  
* Can bin pairs of variables in two dimensions, and facet vary on additional variables  
* The geom_hex() can be a useful means for showing the 2x2 plot  
	* ggplot(tx, aes(tip_amount, total_amount)) + geom_hex(bins = 75) + scale_x_log10() + scale_y_log10() + geom_abline(slope = 1, intercept = 0)  
* Can use the facet_wrap to see visual subsets of the data - all elements of the main call are added to each of the facets automatically  
	* ggplot(daily_count, aes(pickup_date, n_rides)) + geom_point() + facet_grid(~ pickup_dow)  
  
Visualizing subsets:  
  
* May need to look in more detail than what is available in the summaries  
* A useful technique is to take a natural subset of a large dataset; for example, all of the price data for a single stock  
* Example of examining the "zero tip" nature of the cash rides shown in the NYC taxi dataset; subset to 5,000 rides from UES South to UES North  
	* ggplot(tx_pop, aes(trip_duration, total_amount)) + geom_point(alpha = 0.2)  
    * ggplot(tx_pop, aes(sample = total_amount, color = payment_type)) + geom_qq(distribution = stats::qunif) + ylim(c(3, 20))  # quantile plot against the uniform distribution  
  
Visualizing all subsets:  
  
* May want to extend the analysis to all of the routes in the taxi dataset  
* Faceting will not work with over 20k total panels as in the taxi dataset  
* Refined approaches using trelliscope help enable these higher-volume visualizations  
  
Example code includes:  
```{r cache=TRUE}

load("./RInputFiles/tx_sub.RData")
glimpse(tx)

tx <- tx %>%
    rename(pickup_date=pick_day, pickup_dow=pick_dow)
glimpse(tx)


# Summarize taxi ride count by pickup day
daily_count <- tx %>%
  group_by(pickup_date) %>%
  summarise(n_rides=n())

# Create a line plot
ggplot(daily_count, aes(x=pickup_date, y=n_rides)) +
  geom_line()


# Create a histogram of total_amount
ggplot(tx, aes(x=total_amount)) +
  geom_histogram() +
  scale_x_log10()


# Create a bar chart of payment_type
ggplot(tx, aes(x=payment_type)) +
  geom_bar()


# Create a hexagon-binned plot of total_amount vs. trip_duration
ggplot(tx, aes(x=trip_duration, y=total_amount)) +
  geom_hex(bins=75) +
  scale_x_log10() +
  scale_y_log10()


# Summarize taxi rides count by payment type, pickup date, pickup day of week, and payment type
daily_count <- tx %>%
  filter(payment_type %in% c("Card", "Cash")) %>%
  group_by(payment_type, pickup_date, pickup_dow) %>%
  summarise(n_rides=n())

# Plot the data
ggplot(daily_count, aes(x=pickup_date, y=n_rides)) +
  geom_point() +
  facet_grid(payment_type ~ pickup_dow) +
  coord_fixed(ratio = 0.4)


# Histogram of the tip amount faceted on payment type
ggplot(tx, aes(x=tip_amount+0.01)) +
  geom_histogram() +
  scale_x_log10() +
  facet_wrap(~ payment_type, ncol=1, scales="free_y")


# Get data ready to plot
amount_compare <- tx %>%
    mutate(total_no_tip = total_amount - tip_amount) %>%
    filter(total_no_tip <= 20) %>%
    sample_n(size=round(nrow(.)/20), replace=FALSE) %>%
    select(total_amount, total_no_tip, payment_type) %>%
    tidyr::gather(amount_type, amount, -payment_type)

# Quantile plot
ggplot(amount_compare, aes(sample=amount, color=payment_type)) +
  geom_qq(distribution=stats::qunif, shape = 21) +
  facet_wrap(~ amount_type) +
  ylim(c(3, 20))

```
  
  
  
***
  
Chapter 2 - ggplot2 + Trelliscope JS  
  
Faceting with Trelliscope JS:  
  
* The trelliscope is a powerful tool for viewing moderate and large datasets - will again explore gapminder as a starting point  
	* ggplot(gapminder, aes(year, lifeExp, group = country)) + geom_line()  # enormouse over-plotting problem  
    * ggplot(gapminder, aes(year, lifeExp, group = country, color = continent)) + geom_line() + facet_wrap(~ continent, nrow = 1) + guides(color = FALSE)  # somewhat better to see insights by continent  
    * ggplot(gapminder, aes(year, lifeExp)) + geom_line() + facet_wrap(~ country + continent)  # labels overplot the data (impossible to view)  
* The trelliscope is just a replacement of a single call in the ggplot  
	* It's as easy as swapping out facet_wrap() for facet_trelliscope()  
    * As with facet_wrap(), control rows and columns with nrow and ncol  
  
Interacting with Trelliscope JS displays:  
  
* Can look at just one panel as per the JS user controls; focus in on the most interesting areas of a visualization  
* Can page through the panels using the Prev and Next panels and/or the arrow buttons  
* Can customize the grid layout by clicking on the "Grid" widget on the side of the plot  
* The "cognostics" are also clickable as far as which labels will be shown  
	* Can subset by selecting portions of a histogram and/or categories for viewing  
    * Default sorting order are low-to-high, though this can be customized  
  
Additional Trelliscope JS features:  
  
* Can use plotly for additional interactivity within panels - adds tooltips and pop-ups  
	* gap_life <- select(gapminder, year, lifeExp, country, continent)  
    * ggplot(gap_life, aes(year, lifeExp)) + geom_point() + facet_trelliscope(~ country + continent, name = "lifeExp_by_country", desc = "Life expectancy vs. year for 142 countries.", nrow = 2, ncol = 3, as_plotly = TRUE)  
* Can calculate cognostics automatically for additional useful metrics  
	* ggplot(gap_life, aes(year, lifeExp)) + geom_point() + facet_trelliscope(~ country + continent, name = "lifeExp_by_country", desc = "Life expectancy vs. year for 142 countries.", nrow = 2, ncol = 3, auto_cog = TRUE)  
* Multi-panel displays require good usage of axis limits  
	* Similar to the scales argument in facet_wrap  
    * Default behavior is scales = "same"  
    * When scales = "sliced", the scales have the same ranges but with potentially different starting points (these are currently only in trelliscopejs rather than in ggplot2)  
    * When scales = "free", the scales are independent by facet  
  
Adding your own cognostics:  
  
* Can add new variables as cognostics by adding new variables to the data  
* Example of creating the latest observed life expectancy  
	* gap <- gapminder %>% group_by(country) %>% mutate(latestLifeExp = tail(lifeExp, 1))  # do not summarize; maintain the original data  
* Can create links within the displays; for example  
	* gap <- gapminder %>% group_by(country, continent) %>% mutate(wiki = paste0("https://en.wikipedia.org/wiki/", country))  
* Customizing custom cognostics  
	* A function cog() can be wrapped around a variable to fine-tune how a cognostic is handled in Trelliscope  
    * desc: a meaningful description for the cognostic  
    * default_label: boolean specifying whether the cognostic should be shown as a label by default or not  
  
Example code includes:  
```{r eval=FALSE}

library(trelliscopejs)

data(gapminder, package="gapminder")
glimpse(gapminder)


# Create the plot
gapminder %>%
    filter(continent=="Oceania") %>%
    ggplot(aes(x=year, y=lifeExp)) +
    geom_line() +
    # Facet on country and continent
    facet_trelliscope(~ country + continent, data=gapminder)

gapminder %>%
    filter(continent=="Oceania") %>%
    ggplot(aes(x=year, y=lifeExp)) +
    geom_line() +
    facet_trelliscope(~ country + continent, name = "lifeExp_by_country",
                      desc = "Life expectancy vs. year per country", nrow = 1, ncol = 2
                      )


# Create the plot
gapminder %>%
    filter(continent=="Oceania") %>%
    ggplot(aes(x=year, y=lifeExp)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    facet_trelliscope(~ country + continent, name = "lifeExp_by_country",
                      desc = "Life expectancy vs. year for 142 countries.",
                      nrow = 2, ncol = 3,
                      # Set the scales
                      scales="sliced",
                      # Specify automatic cognistics
                      auto_cog=TRUE
                      )


# Group by country and create the two new variables
gap <- gapminder %>%
    filter(continent=="Oceania") %>%
    group_by(country) %>%
    mutate(delta_lifeExp = tail(lifeExp, 1) - head(lifeExp, 1),
           ihme_link = paste0("http://www.healthdata.org/", country)
           )

# Add the description
gap$delta_lifeExp <- cog(gap$delta_lifeExp, desc = "Overall change in life expectancy")

# Specify the default label
gap$ihme_link <- cog(gap$ihme_link, default_label = TRUE)

ggplot(gap, aes(year, lifeExp)) +
    geom_point() +
    facet_trelliscope(~ country + continent,
                      name = "lifeExp_by_country",
                      desc = "Life expectancy vs. year for 142 countries.",
                      nrow = 2, ncol = 3, scales = c("same", "sliced")
                      )

```
  
  
  
***
  
Chapter 3 - Trelliscope in the Tidyverse  
  
Trelliscope in the Tidyverse:  
  
* Stock market dataset called "stocks" has price data for the 500 highest-traded NASDAQ stocks  
* Can use ggplot2 to understand stock price changes by year  
* Might instead want to use the canbdlestick chart from plotly - can also zoom, plot, and hover based on the plotly characteristics  
	* candlestick_plot <- function(d) plot_ly(d, x = ~date, type = "candlestick", open = ~open, close = ~close, high = ~high, low = ~low)  
    * candlestick_plot(filter(stocks, symbol == "AAPL"))  
* Can use nesting to make for a smaller data frame with column data holding a tibble  
	* by_symbol <- stocks %>% group_by(symbol) %>% nest()  
    * by_symbol <- mutate(by_symbol, last_close = map_dbl(data, function(x) tail(x$close, 1)))  
    * by_symbol <- mutate(by_symbol, panel = map_plot(data, candlestick_plot))  # map_plot is from the trelliscope package; one plot for each stock  
    * trelliscope(by_symbol, name = "candlestick_top500", nrow = 2, ncol = 3)  # show the plot with the given initial layout  
  
Cognostics:  
  
* Can add variables, which will then be included as cognostics in the trelliscope  
* Can add metadata to the stocks database, then use nest to have the metadata as a new variable  
	* by_symbol <- left_join(by_symbol, stocks_meta)  
    * by_symbol <- mutate(by_symbol, volume_stats = map(data, function(x) { data_frame( min_volume = min(x$volume), max_volume = max(x$volume) ) }))  
  
Trelliscope options:  
  
* Can further customize multiple options related to a trelliscope plot  
* Can specify the output directory for sharing the display with others - default is just a temporary directory, and will over-write anything in the directory with the same name  
	* trelliscope(dat, path = "...", ...)  
    * ggplot(...) + ... + facet_trelliscope(path = "...", ...)  
* May want to provide more detailed descriptions of the data - shows in the viewer icon  
	* trelliscope(by_symbol, name = "candlestick_top500", desc = "Candlestick plot of the 500 most-traded NASDAQ stocks in 2016", md_desc = " ## Candlestick Plot A [candlestick plot](https://en.wikipedia.org/wiki/Candlestick_chart) is a financial plot... ... ")  # multi-line markdown in md_desc  
* Plot aspect ratio is important for strong visual displays - default is a square of 500px x 500px  
	* trelliscope(dat, width = 600, height = 300, ...)  # specified in units of pixels  
    * ggplot(...) + ... + facet_trelliscope(width = 600, height = 300, ...)  
* Default state can be further specified - interface is currently undergoing active improvement  
	* trelliscope(dat, state = ..., ...)  
    * ggplot(...) + ... + facet_trelliscope(state = ..., ...)  
  
Visualizing databases of images:  
  
* May want to view large collections of images, which is a strong use case for trelliscope - example of the pokemon dataset  
	* select(pokemon, url_image)  
    * pokemon <- mutate(pokemon, panel = img_panel(url_image))  
* May have local images for comparisons instead  
	* path <- file.path(tempdir(), "pokemon_local")  
    * dir.create(path)  
    * for (url in pokemon$url_image)  
    *     download.file(url, destfile = file.path(path, basename(url)))  
    * pokemon$image <- basename(pokemon$url_image)  
    * pokemon <- mutate(pokemon, panel = img_panel_local(image))  
    * trelliscope(pokemon, name = "pokemon", nrow = 3, ncol = 6, path = path)  
  
Example code includes:  
```{r eval=FALSE}

# do not have dataset 'stocks'
by_symbol <- stocks %>%
  group_by(symbol) %>%
  nest()

min_volume_fn <- function(x) min(x$volume)

# Create new column
by_symbol_min <- by_symbol %>%
  mutate(min_volume = map_dbl(data, min_volume_fn))


ohlc_plot <- function(d) {
  plot_ly(d, x = ~date, type = "ohlc",
    open = ~open, close = ~close,
    high = ~high, low = ~low)
}

by_symbol_plot <- mutate(by_symbol, panel=map_plot(data, ohlc_plot))

trelliscope(by_symbol_plot, name="ohlc_top500")


# Create market_cap_log
by_symbol <- mutate(by_symbol,
  market_cap_log = cog(
    val = log10(market_cap),
    desc = "log base 10 market capitalization"
  )
)


annual_return <- function(x)
  100 * (tail(x$close, 1) - head(x$open, 1)) / head(x$open, 1)

# Compute by_symbol_avg
by_symbol_avg <- mutate(by_symbol,
  stats = map(data, function(x) {
    data_frame(
      mean_close = mean(x$close),
      mean_volume = mean(x$volume),
      annual_return = annual_return(x)
      )
    }
  )
)


# Create the trelliscope display
p <- trelliscope(by_symbol, width=600, height=300, name="ohlc_top500", desc="Example aspect 2 plot")


pokemon %>%
  # Reduce the variables in the dataset
  select(pokemon, type_1, attack, generation_id, url_image) %>%
  mutate(
    # Respecify pokemon
    pokemon = cog(pokemon, default_label=TRUE),
    # Create panel variable
    panel = img_panel(url_image)
  ) %>%
  # Create the display
  trelliscope(name="pokemon", nrow=3, ncol=6)

```
  
  
  
***
  
Chapter 4 - Case Study: Exploring Montreal BIXI Bike Data  
  
Montreal BIXI Bike Data:  
  
* Bike riding data for 2017 for BIXI Montreal - desire to understand patterns of usages  
* Aggregations and summary plots for the big picture, followed by deep-dives with trelliscope  
* Randomly sub-samples down to 1,000,000 records for the examples  
  
Summary Visualization Recap:  
  
* Clear differences in weekday usage (peaks at 08h00 and 17h00) and weekend usage (peak around 15h00)  
* Non-members tend to ride as much on weekends (over 2 days) rather than weekdays (over 5 days)  
	* Hypothesis that members are more likely to be commuters and non-members more likely to be tourists  
  
Top 100 routes dataset:  
  
* May want to examine route-specific behaviors  
* Considering a route as a from-to, we want to look at the top-100 routes, ignoring round-trip routes  
	* route_tab <- bike %>% filter(start_station_code != end_station_code) %>% group_by(start_station_code, end_station_code) %>% summarise(n = n()) %>% arrange(-n)  
    * top_routes <- paste( route_tab$start_station_code[1:100], route_tab$end_station_code[1:100])  
    * top100 <- bike %>% filter(paste(start_station_code, end_station_code) %in% top_routes)  
* Preparing data for visualization  
	* route_hod <- top100 %>% group_by(start_station_code, end_station_code, start_hod, weekday) %>% summarise(n = n())  
    * route_hod <- route_hod %>% left_join(start_stations) %>% left_join(end_stations)  
  
Wrap up:  
  
* Can filter and sort to find key information in the trelliscope facets  
* Can further investigate routes with rush hour peaks (AM, PM, or both) as well as routes with differences in weekend and weekday  
  
Example code includes:  
```{r cache=TRUE}

# DO NOT HAVE FULL BIXI Data
bike04 <- read_csv("./RInputFiles/BIXIData/OD_2017-04.csv")
bike05 <- read_csv("./RInputFiles/BIXIData/OD_2017-05.csv")
bike06 <- read_csv("./RInputFiles/BIXIData/OD_2017-06.csv")
bike07 <- read_csv("./RInputFiles/BIXIData/OD_2017-07.csv")
bike08 <- read_csv("./RInputFiles/BIXIData/OD_2017-08.csv")
bike09 <- read_csv("./RInputFiles/BIXIData/OD_2017-09.csv")
bike10 <- read_csv("./RInputFiles/BIXIData/OD_2017-10.csv")
bike11 <- read_csv("./RInputFiles/BIXIData/OD_2017-11.csv")
stations <- read_csv("./RInputFiles/BIXIData/Stations_2017.csv")

bike <- rbind(bike04, bike05, bike06, bike07, bike08, bike09, bike10, bike11) %>%
    mutate(membership=factor(is_member, levels=c(1, 0), labels=c("member", "non-member")), 
           start_day=as.Date(start_date), 
           start_dow=factor(lubridate::wday(start_date), levels=1:7, labels=c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")),
           weekday=factor(ifelse(start_dow %in% c("Sat", "Sun"), 2, 1), levels=1:2, labels=c("workweek", "weekend")), 
           start_hod=lubridate::hour(start_date), 
           start_mon=lubridate::month(start_date), 
           start_wk=lubridate::week(start_date)
           )
glimpse(bike)


# Compute daily counts
daily <- bike %>%
  group_by(start_day, weekday) %>%
  summarise(n = n())

# Plot the result
ggplot(daily, aes(x=start_day, y=n, color=weekday)) +
  geom_point()


# Compute week_hod
week_hod <- bike %>%
  group_by(start_wk, start_hod, weekday) %>%
  summarize(n=n())

# Plot the result
ggplot(week_hod, aes(x=start_wk, y=n, color=weekday)) +
  geom_point() +
  facet_grid(cols=vars(start_hod)) +
  scale_y_sqrt()


# Compute wk_memb_hod
wk_memb_hod <- bike %>%
  group_by(start_wk, start_hod, weekday, membership) %>%
  summarize(n=n())

# Plot the result
ggplot(wk_memb_hod, aes(x=start_wk, y=n, color=weekday)) +
  geom_point() +
  facet_grid(membership ~ start_hod) +
  scale_y_sqrt()


# Compute daily_may
daily_may <- bike %>%
  filter(start_mon == 5) %>%
  group_by(start_day, start_hod, membership) %>%
  summarise(n = n())

# Plot the result
ggplot(daily_may, aes(x=start_hod, y=n, color=membership)) +
  geom_point() + 
  facet_wrap(~ start_day, ncol=7)


# ggplot(daily_may, aes(x=start_hod, y=n, color = membership)) +
#   geom_point() +
  # Facet on start_day
#   facet_trelliscope(~ start_day, nrow=2, ncol=7)


# Function to construct a Google maps URL with cycling directions
make_gmap_url <- function(start_lat, start_lon, end_lat, end_lon) {
  paste0("https://www.google.com/maps/dir/?api=1",
    "&origin=", start_lat, ",", start_lon,
    "&destination=", end_lat, ",", end_lon,
    "&travelmode=bicycling")
}

load("./RInputFiles/route_hod.RData")
glimpse(route_hod)

# Compute tot_rides, weekday_diff, and map_url
route_hod_updated <- route_hod %>%
  group_by(start_station_code, end_station_code) %>%
  mutate(
    tot_rides = sum(n),
    weekday_diff = mean(n[weekday == "workweek"]) - mean(n[weekday == "weekend"]),
    map_url = make_gmap_url(start_lat, start_lon, end_lat, end_lon))


# Create the plot
# ggplot(route_hod, aes(x=start_hod, y=n, color=weekday)) +
  # geom_point(size=3) + 
  # facet_trelliscope(~start_station_name + end_station_name, nrow=2, ncol=4) + 
  # theme(legend.position = "none")

```
  
  
  
***
  
### _Visualization Best Practices in R_  
  
Chapter 1 - Proportions of a Whole  
  
Course/Grammar of Graphics Information:  
  
* General objective is to think deeply about the data and make the best visualization based on the data at hand  
* Will cover standard visualization techniques and alternative visualization techniques for improvement  
	* Topics are not cut and dry, rules will have exceptions, and the emphasis should be on thinking through the specific issue at hand  
* Dataset will be available from WHO - measles, mumps, etc., for 7 diseases  
	* who_disease  
    * amr_region <- who_disease %>% filter(region == 'AMR')  
    * ggplot(amr_region, aes(x = year, y = cases, color = disease)) + geom_point(alpha = 0.5)  
  
Pie Chart and Friends:  
  
* May want to examine the proportions of a single population - adds up to 100%  
* The pie chart is often one of the first techniques learned; but, often abused (too many slices, 3D, not adding up to 100%, and the like)  
	* Data encoded in angles, but humans are better at comparing lengths  
    * After three slices, it typically becomes hard to compare (not just angles, but offset angles)  
    * who_disease %>% mutate( region = ifelse( region %in% c('EUR', 'AFR'), region, 'Other') ) %>% ggplot(aes(x = 1, fill = region)) + geom_bar(color = 'white') + coord_polar(theta = "y") + theme_void()  
* The waffle chart can be more precise; encoding data by area (add the squares) rather than by angle  
	* obs_by_region <- who_disease %>% group_by(region) %>% summarise(num_obs = n()) %>% mutate(percent = round(num_obs/sum(num_obs)*100))  
    * percent_by_region <- obs_by_region$percent  
    * names(percent_by_region) <- obs_by_region$region  
    * waffle::waffle(percent_by_region, rows = 5)  
  
When to use Bars:  
  
* May want to compare multiple proportions to each other - facets are not ideal for pie or waffle charts  
* Can instead use stacked bar charts with a common y-axis  
	* who_disease %>% filter(region == 'SEAR') %>% ggplot(aes(x = countryCode, y = cases, fill = disease)) + geom_col(position = 'fill')  # position="fill" makes it a proportion chart  
* Stacked bars tend to be worse in isolation by themselves (only the first and last class have a good anchor)  
* Generalized best practices include 1) do not make a stacked bar chart in isolation, and 2) keep the number of groups reasonably small  
  
Example code includes:  
```{r}

who_disease <- readr::read_csv("./RInputFiles/who_disease.csv")
glimpse(who_disease)


# set x aesthetic to region column
ggplot(who_disease, aes(x=region)) +
  geom_bar()


# filter data to AMR region. 
amr_region <- who_disease %>%
    filter(region=="AMR")

# map x to year and y to cases. 
ggplot(amr_region, aes(x=year, y=cases)) + 
    # lower alpha to 0.5 to see overlap.   
    geom_point(alpha=0.5)


# Wrangle data into form we want. 
disease_counts <- who_disease %>%
    mutate(disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other')) %>%
    group_by(disease) %>%
    summarise(total_cases = sum(cases))


ggplot(disease_counts, aes(x = 1, y = total_cases, fill = disease)) +
    # Use a column geometry.
    geom_col() + 
    # Change coordinate system to polar and set theta to 'y'.
    coord_polar(theta="y")


ggplot(disease_counts, aes(x = 1, y = total_cases, fill = disease)) +
    # Use a column geometry.
    geom_col() +
    # Change coordinate system to polar.
    coord_polar(theta = "y") +
    # Clean up the background with theme_void and give it a proper title with ggtitle.
    theme_void() +
    ggtitle('Proportion of diseases')


disease_counts <- who_disease %>%
    group_by(disease) %>%
    summarise(total_cases = sum(cases)) %>% 
    mutate(percent = round(total_cases/sum(total_cases)*100))

# Create an array of rounded percentages for diseases.
case_counts <- disease_counts$percent
# Name the percentage array with disease_counts$disease
names(case_counts) <- disease_counts$disease

# Pass case_counts vector to the waffle function to plot
waffle::waffle(case_counts)


disease_counts <- who_disease %>%
    mutate(disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other')) %>%
    group_by(disease, year) %>% # note the addition of year to the grouping.
    summarise(total_cases = sum(cases))

# add the mapping of year to the x axis. 
ggplot(disease_counts, aes(x=year, y = total_cases, fill = disease)) +
    # Change the position argument to make bars full height
    geom_col(position="fill")


disease_counts <- who_disease %>%
    mutate(
        disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other') %>%
            factor(levels=c('measles', 'other', 'mumps')) # change factor levels to desired ordering
        ) %>%
    group_by(disease, year) %>%
    summarise(total_cases = sum(cases))

# plot
ggplot(disease_counts, aes(x = year, y = total_cases, fill = disease)) +
    geom_col(position = 'fill')


disease_counts <- who_disease %>%
    # Filter to on or later than 1999  
    filter(year >= 1999) %>% 
    mutate(disease = ifelse(disease %in% c('measles', 'mumps'), disease, 'other')) %>%
    group_by(disease, region) %>%    # Add region column to grouping
    summarise(total_cases = sum(cases))

# Set aesthetics so disease is the stacking variable, region is the x-axis and counts are the y
ggplot(disease_counts, aes(x=region, y=total_cases, fill=disease)) +
    # Add a column geometry with the proper position value.
    geom_col(position="fill")

```
  
  
  
***
  
Chapter 2 - Point Data  
  
Point Data:  
  
* Switching from proportions to points (numerical observations for multiple categories) - single observations per item  
* Most common visualization is a bar chart  
	* ggplot(who_disease) + geom_col(aes(x = disease, y = cases))  
* Bar charts are frequently used incorrectly for the task at hand; really best for accumulating measures (money can be thought of a stack of coins)  
	* However, if the bars are something non-stackable (such as likelihood of error), then bars are not appropriate  
    * Humans tend to perceive bars as meaning "everything inside the bar is included, and everything outside the bar is excluded"  
  
Point Charts:  
  
* When a point is not really a stackable quantity (percentile, temperature, log-transform, etc.), alternative to bar charts should be used  
* Instead, a point right at the top of the bar may work best; also known as a dot chart; high precision and simple  
	* who_subset %>% ggplot(aes(y = country, x = log10(cases_2016))) + geom_point()  
* Can reorder the point charts, applied directly in the aesthetic  
	* who_subset %>% mutate(logFoldChange = log2(cases_2016/cases_2006)) %>% ggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) + geom_point()  
  
Tuning Charts:  
  
* Can make the default charts more efficient and attractive  
	* busy_bars <- who_disease %>% filter(region == 'EMR', disease == 'measles', year == 2015) %>% ggplot(aes(x = country, y = cases)) + geom_col()  # Base plot  
* Flipping bar charts can make labels easier to read  
	* busy_bars + coord_flip() # swap x and y axes!  
* Can also get rid of excess grid lines along the categorical axis  
	* # get rid of vertical grid lines  
    * plot + theme( panel.grid.major.x = element_blank() )  
* Lighter backgrounds can make for better contrasts in a point chart  
	* who_subset %>% ggplot(aes(y = reorder(country, cases_2016), x = log10(cases_2016))) + geom_point(size = 2) + theme_minimal()  
  
Example code includes:  
```{r}

who_disease %>% 
    # filter to india in 1980
    filter(country=="India", year==1980) %>% 
    # map x aesthetic to disease and y to cases
    ggplot(aes(x=disease, y=cases)) +
    # use geom_col to draw
    geom_col()


who_disease %>%
    # filter data to observations of greater than 1,000 cases
    filter(cases > 1000) %>%
    # map the x-axis to the region column
    ggplot(aes(x=region)) +
    # add a geom_bar call
    geom_bar()


interestingCountries <- c('NGA', 'SDN', 'FRA', 'NPL', 'MYS', 'TZA', 'YEM', 'UKR', 'BGD', 'VNM')
who_subset <- who_disease %>% 
    filter(countryCode %in% interestingCountries, disease == 'measles', year %in% c(1992, 2002)) %>% 
    mutate(year = paste0('cases_', year)) %>%
    spread(year, cases)

 
# Reorder y axis and change the cases year to 1992
ggplot(who_subset, aes(x = log10(cases_1992), y = reorder(country, cases_1992))) +
    geom_point()


who_subset %>% 
    # calculate the log fold change between 2016 and 2006
    mutate(logFoldChange = log2(cases_2002/cases_1992)) %>% 
    # set y axis as country ordered with respect to logFoldChange
    ggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) +
    geom_point() +
    # add a visual anchor at x = 0
    geom_vline(xintercept=0)


who_subset %>% 
    mutate(logFoldChange = log2(cases_2002/cases_1992)) %>% 
    ggplot(aes(x = logFoldChange, y = reorder(country, logFoldChange))) +
    geom_point() +
    geom_vline(xintercept = 0) +
    xlim(-6,6) +
    # add facet_grid arranged in the column direction by region and free_y scales
    facet_grid(region ~ ., scale="free_y")


amr_pertussis <- who_disease %>% 
    filter(region == 'AMR', year == 1980, disease == 'pertussis')


# Set x axis as country ordered with respect to cases. 
ggplot(amr_pertussis, aes(x = reorder(country, cases), y = cases)) +
    geom_col() +
    # flip axes
    coord_flip()


amr_pertussis %>% 
    # filter to countries that had > 0 cases. 
    filter(cases > 0) %>%
    ggplot(aes(x = reorder(country, cases), y = cases)) +
    geom_col() +
    coord_flip() +
    theme(panel.grid.major.y = element_blank())


amr_pertussis %>% filter(cases > 0) %>% 
    ggplot(aes(x = reorder(country, cases), y = cases)) + 
    # switch geometry to points and set point size = 2
    geom_point(size=2) + 
    # change y-axis to log10. 
    scale_y_log10() +
    # add theme_minimal()
    theme_minimal() +
    coord_flip()

```
  
  
  
***
  
Chapter 3 - Single Distributions  
  
Importance of Distributions:  
  
* Distribution data includes multiple observations from the same population  
* Plotting distribution is valuable in many ways  
	* Identifying errors or outliers  
    * Identifying multiple peaks; potential segmenting variable  
    * Distributions are more accurate and truthful to the data than a summary statistic  
* Histograms are a common approach to showing distributions  
* Box plots are a common approach for comparing multiple distributions  
* Will examing the md_speeding dataset from Montgomery County, MD  
	* md_speeding %>% filter(vehicle_color == 'BLUE') %>% ggplot(aes(x = speed)) + geom_histogram()  
  
Histogram Nuances:  
  
* Histograms have the advantage of being intuitive to readers and easier to interpret; higher bars mean more frequent  
* Histograms need care in setting the number of bins for better interpretation; binning can significantly change the plot  
	* If you have 150+ points, set the bins to 100 and call it a day  
  
Kernel Density Estimates:  
  
* The KDE (kernel density estimator) is a common approach to managing the binning and other challenges associated with histograms  
	* Kernel function is created on top of every point, then summed across the distribution  
    * Typically, a Gaussian kernel is used, though there are other options  
    * sample_n(md_speeding, 100) %>% ggplot(aes(x = percentage_over_limit)) + geom_density( fill = 'steelblue', bw = 8 )  
* For the KDE, need to choose the width of the kernel, which is controlled by the bw parameter (analogous to sd for Gaussians)  
* Can also show all of the data using the rug plot  
	* p <-sample_n(md_speeding, 100) %>% ggplot(aes(x = percentage_over_limit)) + geom_density( fill = 'steelblue', # fill in curve with color bw = 8 # standard deviation of kernel )  
    * p + geom_rug(alpha = 0.4)  
  
Example code includes:  
```{r cache=TRUE}

colKeep <- c('work_zone', 'vehicle_type', 'vehicle_year', 'vehicle_color', 'race', 'gender', 
             'driver_state', 'speed_limit', 'speed', 'day_of_week', 'day_of_month', 'month', 
             'hour_of_day', 'speed_over', 'percentage_over_limit'
             )
colRead <- c("Work Zone", "VehicleType", "Year", "Color", "Race", "Gender", 
             "DL State", "Description", "Date Of Stop", "Time Of Stop")
regFind <- ".*EXCEEDING MAXIMUM SPEED: ([0-9]+) MPH .* POSTED ([0-9]+) MPH .*"

md_speeding <- readr::read_csv("./RInputFiles/MD_Traffic/Traffic_violations.csv", n_max=200000) %>%
    select(colRead) %>%
    filter(grepl("EXCEEDING MAXIMUM SPEED: ", Description)) %>%
    rename(work_zone="Work Zone", vehicle_type=VehicleType, vehicle_year=Year, 
           vehicle_color=Color, race=Race, gender=Gender, driver_state="DL State", 
           stopDate="Date Of Stop", stopTime="Time Of Stop") %>%
    mutate(speed_limit=as.integer(gsub(regFind, "\\2", Description)), 
           speed=as.integer(gsub(regFind, "\\1", Description)), 
           speed_over=speed - speed_limit, 
           percentage_over_limit=100 * speed_over / speed_limit, 
           stopDate=as.Date(stopDate, format="%m/%d/%Y"), 
           day_of_week=lubridate::wday(stopDate), 
           day_of_month=lubridate::day(stopDate), 
           month=lubridate::month(stopDate), 
           hour_of_day=lubridate::hour(stopTime)
           )

# Print data to console
glimpse(md_speeding)

# Change filter to red cars
md_speeding %>% 
    filter(vehicle_color == 'RED') %>% 
    # switch x mapping to speed_over column
    ggplot(aes(x = speed_over)) +
    geom_histogram() +
    # give plot a title
    ggtitle('MPH over speed limit | Red cars')


ggplot(md_speeding) + 
    # Add the histogram geometry with x mapped to speed_over
    geom_histogram(aes(x=speed_over), alpha=0.7) +
    # Add minimal theme
    theme_minimal()


ggplot(md_speeding) +
    geom_histogram(aes(x=hour_of_day, y=stat(density)), alpha=0.8)


# Load md_speeding into ggplot
ggplot(md_speeding) +
  # add a geom_histogram with x mapped to percentage_over_limit
    geom_histogram(
        aes(x=percentage_over_limit), 
        bins=40,     # set bin number to 40
        alpha=0.8)    # reduce alpha to 0.8


ggplot(md_speeding) +
    geom_histogram(
        aes(x = percentage_over_limit),
        bins = 100 ,         # switch to 100 bins
        fill="steelblue",   # set the fill of the bars to 'steelblue'
        alpha = 0.8 )


ggplot(md_speeding, aes(x = hour_of_day)) +
    geom_histogram(
        binwidth=1,  # set binwidth to 1
        center=0.5,  # Center bins at the half (0.5) hour
    ) +
    scale_x_continuous(breaks = 0:24)


# filter data to just heavy duty trucks
truck_speeding <- md_speeding %>% 
    filter(vehicle_type == "06 - Heavy Duty Truck")
 
ggplot(truck_speeding, aes(x = hour_of_day)) +
    # switch to density with bin width of 1.5, keep fill 
    geom_density(fill = 'steelblue', bw=1.5) +
    # add a subtitle stating binwidth
    labs(title = 'Citations by hour', subtitle="Gaussian kernel SD = 1.5")


ggplot(truck_speeding, aes(x = hour_of_day)) +
    # Adjust opacity to see gridlines with alpha = 0.7
    geom_density(bw = 1.5, fill = 'steelblue', alpha=0.7) +
    # add a rug plot using geom_rug to see individual datapoints, set alpha to 0.5.
    geom_rug(alpha=0.5) +
    labs(title = 'Citations by hour', subtitle = "Gaussian kernel SD = 1.5")


ggplot(md_speeding, aes(x = percentage_over_limit)) +
    # Increase bin width to 2.5
    geom_density(fill = 'steelblue', bw = 2.5,  alpha = 0.7) + 
    # lower rugplot alpha to 0.05
    geom_rug(alpha = 0.05) + 
    labs(
        title = 'Distribution of % over speed limit', 
        # modify subtitle to reflect change in kernel width
        subtitle = "Gaussian kernel SD = 2.5"
    )

```
  
  
  
***
  
Chapter 4 - Comparing Distributions  
  
Introduction to Comparing Distributions:  
  
* Box plots can be helpful for comparing across key covariates, especially since multiple histograms are space-inefficient  
	* Include median, IQR, 1.5*IQR, Outliers  
    * Can be helpful to see the actual data rather than just the boxplot  
    * md_speeding %>%  
    *   filter(vehicle_color == 'BLUE') %>%  
    *   ggplot(aes(x = gender, y = speed)) +
    *   geom_jitter(alpha = 0.3, color = 'steelblue') +  
    *   geom_boxplot(alpha = 0) + # make transparent  
    *   labs(title = 'Distribution of speed for blue cars by gender')  
  
Bee Swarms and Violins:  
  
* Jittering is helpful to see quantities of data, but bee swarms or violin plots can help summarize data  
* The bee swarm is a smart version of a jitter plot, with denser points stacking in an area (shape shows quantity of data)  
	* library(ggbeeswarm)  
    * ggplot(data, aes(y = y, x = group)) + geom_beeswarm(color = 'steelblue')  
* Violin plots are KDE that are symmetric around the categorical axis  
	* ggplot(data, aes(y = y, x = group)) + geom_violin(fill = 'steelblue')  
  
Comparing Spatially Related Distributions:  
  
* Bee swarm and violin plots are not ideal when data are spatially connected (ordering to the categorical axis, such as longitudinal time series data)  
* Ridge-line plots can be great for seeing how the KDE evolves over time; good for seeing individual KDE as well as evolutions over time  
	* library(ggridges) # gives us geom_density_ridges()  
    * ggplot(md_speeding, aes(x=speed_over, y=month)) + geom_density_ridges(bandwidth = 2) + xlim(1, 35)  
  
Wrap Up:  
  
* Subtle things can take a visualization fro good to great  
* Proportions - pie charts can be OK, as can waffle charts, stacked bars, etc.  
* Point Data - bar charts are good for stackable data, while points are good for non-stockable data  
	* Remove grid lines where needed and order by value unless categories having inherne tvalue  
* Single distributions - histograms, KDE, rug charts, etc.  
* Comparing distributions - box plots, jitter plots, violin plots, beeswarm plots, etc.  
* Further exploration can include  
	* Flowing data blog  
    * Datawrapper blog  
    * Twitter #datavis  
    * "Data Visualization" by Andy Kirk, "Functional Art and Truthful Art" by Alberto Cairo  
  
Example code includes:  
```{r cache=TRUE}

md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    # Map x and y to gender and speed columns respectively
    ggplot(aes(x=gender, y=speed)) + 
    # add a boxplot geometry
    geom_boxplot() +
    # give plot supplied title
    labs(title = 'Speed of red cars by gender of driver')


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    # add jittered points with alpha of 0.3 and color 'steelblue'
    geom_jitter(alpha=0.3, color="steelblue") +
    # make boxplot transparent with alpha = 0
    geom_boxplot(alpha=0) +
    labs(title = 'Speed of red cars by gender of driver')


# remove color filter
md_speeding %>%
    ggplot(aes(x = gender, y = speed)) + 
    geom_jitter(alpha = 0.3, color = 'steelblue') +
    geom_boxplot(alpha = 0) +
    # add a facet_wrap by vehicle_color
    facet_wrap(~ vehicle_color) +
    # change title to reflect new faceting
    labs(title = 'Speed of different car colors, separated by gender of driver')


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    # change point size to 0.5 and alpha to 0.8
    ggbeeswarm::geom_beeswarm(cex=0.5, alpha=0.8) +
    # add a transparent boxplot on top of points
    geom_boxplot(alpha=0)


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    # Replace beeswarm geometry with a violin geometry with kernel width of 2.5
    geom_violin(bw = 2.5) +
    # add individual points on top of violins
    geom_point(alpha=0.3, size=0.5)


md_speeding %>% 
    filter(vehicle_color == 'RED') %>%
    ggplot(aes(x = gender, y = speed)) + 
    geom_violin(bw = 2.5) +
    # add a transparent boxplot and shrink its width to 0.3
    geom_boxplot(alpha=0, width=0.3) +
    # Reset point size to default and set point shape to 95
    geom_point(alpha = 0.3, shape = 95) +
    # Supply a subtitle detailing the kernel width
    labs(subtitle = 'Gaussian kernel SD = 2.5')


md_speeding %>% 
    ggplot(aes(x = gender, y = speed)) + 
    # replace with violin plot with kernel width of 2.5, change color argument to fill 
    geom_violin(bw = 2.5, fill = 'steelblue') +
    # reduce width to 0.3
    geom_boxplot(alpha = 0, width=0.3) +
    facet_wrap(~vehicle_color) +
    labs(
        title = 'Speed of different car colors, separated by gender of driver',
        # add a subtitle w/ kernel width
        subtitle = "Gaussian kernel width: 2.5"
    )


md_speeding %>% 
    mutate(day_of_week = factor(day_of_week, levels=c(2, 3, 4, 5, 6, 7, 1), 
                                labels = c("Mon","Tues","Wed","Thu","Fri","Sat","Sun")
                                )
           ) %>% 
    ggplot(aes( x = percentage_over_limit, y = day_of_week)) + 
    # Set bandwidth to 3.5
    ggridges::geom_density_ridges(bandwidth=3.5) +
    # add limits of 0 to 150 to x-scale
    scale_x_continuous(limits=c(0, 150)) + 
    # provide subtitle with bandwidth
    labs(subtitle='Gaussian kernel SD = 3.5')


md_speeding %>% 
    mutate(day_of_week = factor(day_of_week, levels=c(2, 3, 4, 5, 6, 7, 1), 
                                labels = c("Mon","Tues","Wed","Thu","Fri","Sat","Sun")
                                )
           ) %>% 
    ggplot(aes( x = percentage_over_limit, y = day_of_week)) + 
    # make ridgeline densities a bit see-through with alpha = 0.7
    ggridges::geom_density_ridges(bandwidth = 3.5, alpha=0.7) +
    # set expand values to c(0,0)
    scale_x_continuous(limits = c(0,150), expand=c(0, 0)) +
    labs(subtitle = 'Guassian kernel SD = 3.5') +
    # remove y axis ticks
    theme(axis.ticks.y=element_blank())


md_speeding %>% 
    mutate(day_of_week = factor(day_of_week, levels=c(2, 3, 4, 5, 6, 7, 1), 
                                labels = c("Mon","Tues","Wed","Thu","Fri","Sat","Sun")
                                )
           ) %>% 
    ggplot(aes( x = percentage_over_limit, y = day_of_week)) + 
    geom_point(
        # make semi-transparent with alpha = 0.2
        alpha=0.2, 
        # turn points to vertical lines with shape = '|'
        shape="|", 
        # nudge the points downward by 0.05
        position=position_nudge(y=-0.05)
    ) +
    ggridges::geom_density_ridges(bandwidth = 3.5, alpha = 0.7) +
    scale_x_continuous(limits = c(0,150), expand  = c(0,0)) +
    labs(subtitle = 'Guassian kernel SD = 3.5') +
    theme( axis.ticks.y = element_blank() )

```
  
  
  
***
  
### _Linear Algebra for Data Science in R_  
  
Chapter 1 - Introduction to Linear Algebra  
  
Motivations:  
  
* Vectors, matrixs, tensors, and associated operations on and between them  
* Vectors are the most basic, non-trivial element for storing data  
	* Generally shown with an arrow above the variable name  
    * Can be transposed between row vectors and column vectors  
* Can create vectors in R in many ways  
	* rep()  
    * seq()  
    * c()  
    * z[n] <- a  # replace element n of z with a  
* Matrices are tables of data with rows and columns; data frames can be considered a form of matrix  
* Can create matrices in R in many ways  
	* matrix(data, nrow, ncol, byrow=FALSE)  # set byrow=TRUE to fill the matrix by row  
    * A[a, b] <- c  # sets the value of row a and column b of matrix A to c  
  
Matrix-Vector Operations:  
  
* May want to convert or run mathematical operations on vectors and matrices  
	* axb %*% bxc will form an axc  
* The asterisk between percents is matrix multiplication (%*%)  
* Vector-vector multiplication is the dot-product  
* Matrix-vector multiplication requires that the vector have the same number of elements as the matrix has columns  
  
Matrix-Matrix Calculations:  
  
* Matrix-matrix calculations can be useful for predictions (e.g., neural networks)  
	* (mxn) %*% (nxr) = (mxr)  
    * Order matters - A%*%B is not the same as B%*%A  
    * The asterisk will give component-wise multiplication; A*B is not the same as A%*%B  
* The identity matrix I is a diagnonal matrix with 1 on the diagonal and 0 elsewhere  
* Additional important types of matrices  
	* Square matrices are a special matrix where columns and rows are the same  
    * Inverse matrices can be multiplied to create the identity matrix  
    * Singular matrices do not have an inverse  
    * Diagonal and triangular matrices, which are more computationally efficient  
  
Example code includes:  
```{r}

# Creating three 3's and four 4's, respectively
rep(3, 3)
rep(4, 4)

# Creating a vector with the first three even numbers and the first three odd numbers
seq(2, 6, by = 2)
seq(1, 5, by = 2)

# Re-creating the previous four vectors using the 'c' command
c(3, 3, 3)
c(4, 4, 4, 4)

c(2, 4, 6)
c(1, 3, 5)


x <- 1:7
y <- 2*x
z <- c(1, 1, 2)

# Add x to y and print
print(x + y)

# Multiply z by 2 and print
print(2 * z)

# Multiply x and y by each other and print
print(x * y)

# Add x to z, if possible, and print
print(x + z)  # should throw a warning for the recycling problem


A <- matrix(1, 2, 2)

# Create a matrix of all 1's and all 2's that are 2 by 3 and 3 by 2, respectively
matrix(1, 2, 3)

print(matrix(2, 3, 2))

# Create a matrix and changing the byrow designation.
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = FALSE)
matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)

# Add A to the previously-created matrix
A + matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)


A <- matrix(data=c(4, 0, 0, 1), nrow=2, ncol=2, byrow=FALSE)
b <- c(1, 1)
B <- matrix(data=c(1, 0, 0, 2/3), nrow=2, ncol=2, byrow=FALSE)


# Multiply A by b on the left
A %*% b

# Multiply B by b on the left
B %*% b


b <- c(2, 1)
A <- matrix(data=c(-1, 0, 0, 1), nrow=2, ncol=2, byrow=FALSE)
B <- matrix(data=c(1, 0, 0, -1), nrow=2, ncol=2, byrow=FALSE)
C1 <- matrix(data=c(-4, 0, 0, -2), nrow=2, ncol=2, byrow=FALSE)

# Multiply A by b on the left
A%*%b

# Multiplby B by b on the left
B%*%b

# Multiply C by b on the left
C1%*%b


A <- matrix(data=sqrt(2)*c(1, 1, -1, 1), nrow=2, ncol=2, byrow=FALSE)
B <- matrix(data=c(1, 0, 0, -1), nrow=2, ncol=2, byrow=FALSE)
b <- c(1, 1)

# Multply A by B on the left
A%*%B

# Multiply A by B on the right
B%*%A

# Multiply b by B then A (on the left)
A%*%B%*%b

# Multiply b by A then B (on the left)
B%*%A%*%b


A <- matrix(data=c(1, -1, 2, 2), nrow=2, ncol=2, byrow=FALSE)

# Take the inverse of the 2 by 2 identity matrix
solve(diag(2))

# Take the inverse of the matrix A
Ainv <- solve(A)

# Multiply A by its inverse on the left
Ainv%*%A

# Multiply A by its inverse on the right
A%*%Ainv

```
  
  
  
***
  
Chapter 2 - Matrix-Vector Equations  
  
Motivation for Solving Matrix-Vector Equations:  
  
* Question is often whether a build from atromic objects is possible and unique  
	* Assuming that this question is linear, it can be solved using matrices and vectors  
* Vectors times constants is called a linear combination  
	* The question is often whether vector b can be produced as linear combinations of vector x  
    * Ax = b  
* Can consider a matrix of performances in an n-team league as follows - Massey matrix  
	* Diagonals are each number of games that team has played (n-1 in a round-robin)  
    * Off-diagonals are all negative the number of games between the two teams (-1 in a round-robin)  
    * Matrix is nxn  
* Attempt is then to multiple the Massey matrix by an unknown vector R to get the actual net point differential of the teams  
  
Matrix-Vector Equations Theory:  
  
* Need to know the circumatsnaces under which a matrix can be uniquely solved  
	* Inconsistent systems cannot be solved  
    * Consistently systems can be solved, but there may be infinitely-many solutions  
* When a matrix-vector has a solution but not infinite solutions, then it will have only one solution  
* If A is an nxn matrix, then the following conditions are equivalent and imply a solution to Ax = b  
	* The matrix A must have an inverse - solve() will not produce an error  
    * The determinant of A must be non-zero - det() will return a non-zero answer  
    * The rows and columns of A form a basis for the set of all vectors with n elements  
  
Solving Matrix-Vector Equations:  
  
* Solving equations in linear algebra is similar to solving equations in regular algebra  
	* Ainv %*% A gives I and I %*% x gives x  
    * x <- solve(A)%*%b  
* There is an additional matrix solution condition related to the zero vector  
	* The homogenous equation Ax = 0 must have only the trivial solution x=0  
  
Other Considerations for Matrix-Vector Equations:  
  
* With more equations than unknowns, then one or more of the equations must be redundant for a solution to exist  
* With fewer equations than unknowns, it is difficult to have a unique solution (cannot rule out the extra solutions)  
	* However, solutions can exist or fail to exist depending on the redundancy of the columns  
* Options for non-square matrices include  
	* Row Reduction  
    * Least Squares  
    * Singular Value Decomposition  
    * Generalized or Pseudo-Inverses  
* Can calculate the Moore-Penrose Generalized Inverse  
	* MASS::ginv(A)  
    * ginv(A)%*%A  # gives the identity matrix  
    * A%*%ginv(A)  # does not give the identity matrix  
    * x <- ginv(A)%*%b  # gives one of the solutions to Ax = b  
  
Example code includes:  
```{r}

M <- readr::read_csv("./RInputFiles/WNBA_Data_2017_M.csv")
glimpse(M)

names(M) <- stringr::str_replace(names(M), " ", ".")
M <- M %>%
    select(-WNBA) %>%
    slice(-n())
    
M <- as.data.frame(M)
row.names(M) <- names(M)


f <- readr::read_csv("./RInputFiles/WNBA_Data_2017_f.csv")
glimpse(f)

f <- slice(f, -n())

f <- as.data.frame(f)
row.names(f) <- names(M)


# Print the Massey Matrix M Here
print(M)

# Print the vector of point differentials f here
print(f)

# Find the sum of the first column 
sum(M[, 1])

# Find the sum of the vector f
sum(f)


M <- as.matrix(M)

# Add a row of 1's
M <- rbind(M, rep(1, 12))

# Add a column of -1's 
M <- cbind(M, rep(-1, 13))

# Change the element in the lower-right corner of the matrix M
M[13, 13] <- 1

# Print M
print(M)


#Find the inverse of M
solve(M)


f <- as.matrix(f)
f <- rbind(f, 0)

# Solve for r and rename column
r <- solve(M)%*%f
colnames(r) <- "Rating"

# Print r
print(r)


# Find the rating vector using ginv
r <- MASS::ginv(M)%*%f
colnames(r) <- "Rating"
print(r)

```
  
  
  
***
  
Chapter 3 - Eigenvalues and Eigenvectors  
  
Introduction to Eigenvalues and Eigenvectors:  
  
* Eigenvalues and eigenvectors take collections of multi-dimensional objects and select a subset of vectors that closely approximate the originals  
* Matrix-vector multiplication can have many impacts on a vector - reverse, reflect, dilate, contract, project, extract, permutations of these, etc.  
* Eigenvalue-eigenvector analysis allows for summing of the component vectors  
	* Scalar c can be multiplied by vector x to create cx - note that cIx = cx  
* Goal is to decompose a matrix in to a few matrices that can be treated similar to scalar multiplication  
  
Definition of Eigenvalues and Eigenvectors:  
  
* For a matrix A, scalar lambda is considered an eigenvalue of A with associate eigenvector v (v not equal to 0) such that A%*%v = lambda*v  
	* The matrix need not be a diagonal matrix  
    * The eigenvalue-eigenvector are often called an eigenpair  
    * The eigenvectors stay fixed as the matrix is applied; need not be the simple x and y axes  
    * If a eigenpair exists, it can be multiplied by any non-zero scalar; eigenvectors are purely about direction, not about magnitude  
  
Computing Eigenvalues and Eigenvectors in R:  
  
* An nxn matrix has, up to multiplicity, n eigenvalues  
* The eigenvalues need not be real; some or all may be complex, though complex eigenvalues come in conjugate pairs of a + b*i and a-b*i  
* Can get the eigenvalues and eigenvectors using eigen()  
	* eigen(A)  
    * E <- eigen(A)  
    * E$values[1]  # the first eigenvalue  
    * E$vectors[, 1]  # the first eigenvector associated to the first eigenvalue  
* Example of matrix with complex eigenvalues  
	* A <- matrix(data=c(1, -2, 2, -1), nrow=2, ncol=2, byrow=FALSE)  
    * eigen(A)  # both the eigenvalues and the eigenvectors will be complex  
  
Some more on Eigenvalues and Eigenvectors:  
  
* If the eigenvalues lambda(1-to-n) of A are distinct with associated set of eigenvectors v(1-to-n), then this set of vectors forms a basis for the set of all n-dimensional vectors  
	* Every n-dimensional vector can be expressed as a linear combination of these eigenvectors  
    * Basically, eigenpair turn matrix multiplication in to a linear combination of scalar multiplications  
* If the matrix multiplication is iterated, then the lambdas become raise to the power of the number of iterations  
* The leading eigenvector, when normalized to probability 1, is called the stationary distribution of the Markov matrix model  
  
Example code includes:  
```{r}

A <- matrix(data=c(1, 0, 0, 2/3), nrow=2, ncol=2, byrow=FALSE)

# A is loaded for you
print(A%*%rep(1, 2))


A <- matrix(data=c(-1, 0, 0, 2, 7, 0, 4, 12, -4), nrow=3, ncol=3, byrow=FALSE)

# Show that 7 is an eigenvalue for A
A%*%c(0.2425356, 0.9701425, 0) - 7*c(0.2425356, 0.9701425, 0)

# Show that -4 is an eigenvalue for A
A%*%c(-0.3789810, -0.6821657, 0.6253186) - (-4)*c(-0.3789810, -0.6821657, 0.6253186)

# Show that -1 is an eigenvalue for A
A%*%c(1, 0, 0) - (-1)*c(1, 0, 0)


# Show the double of the eigenvector
A%*%((2)*c(0.2425356, 0.9701425, 0)) - 7*(2)*c(0.2425356, 0.9701425, 0)

# Show half of the eigenvector
A%*%((0.5)*c(0.2425356, 0.9701425, 0)) - 7*(0.5)*c(0.2425356, 0.9701425, 0)


A <- matrix(data=c(1, 1, 2, 1), nrow=2, ncol=2, byrow=FALSE)

# Compute the eigenvalues of A and store in Lambda
Lambda <- eigen(A)

# Print eigenvalues
print(Lambda$values[1])
print(Lambda$values[2])

# Verify that these numbers satisfy the conditions of being an eigenvalue
det(Lambda$values[1]*diag(2) - A)
det(Lambda$values[2]*diag(2) - A)


# Find the eigenvectors of A and store them in Lambda
Lambda <- eigen(A)

# Print eigenvectors
print(Lambda$vectors[, 1])
print(Lambda$vectors[, 2])

# Verify that these eigenvectors & their associated eigenvalues satisfy Av - lambda v = 0
Lambda$values[1]*Lambda$vectors[, 1] - A%*%Lambda$vectors[, 1]
Lambda$values[2]*Lambda$vectors[, 2] - A%*%Lambda$vectors[, 2]


Mtemp <- matrix(data=c(0.98, 0.005, 0.005, 0.01, 0.005, 0.98, 0.01, 0.005, 0.005, 0.01, 0.98, 0.005, 0.01, 0.005, 0.005, 0.98), nrow=4, ncol=4, byrow=FALSE)
Mtemp

# This code iterates mutation 100 times
x <- c(1, 0, 0, 0)
for (j in 1:1000) {x <- Mtemp%*%x}

# Print x
print(x)

# Print and scale the first eigenvector of M
Lambda <- eigen(M)
v1 <- Lambda$vectors[, 1]/sum(Lambda$vectors[, 1])

print(v1)

```
  
  
  
***
  
Chapter 4 - Principal Component Analysis  
  
Introduction to the Idea of PCA (Principal Component Analysis):  
  
* PCA is a common dimension-reduction technique in machine learning and data science  
* Real-world data often has many rows (observations) and also many columns (features)  
	* More rows is almost always better, but more columns can be problematic (especially if they are correlated)  
* PCA is a useful applied method for linear algebra  
	* Non-parametric manner of extracting information from confusing data sets  
    * Uncovers hidden, low-dimensional structures that underlie data  
    * Often easier to visualize and interpret  
  
Linear Algebra Behind PCA:  
  
* The matrix t(A) is made by interchanging the rows and columns of A  
* Suppose that the matrix A has n rows and also has had each column mean-adjusted to 0  
	* Then, t(A)%*%A / (n-1) is the covariance matrix where the value in cell (I, j) is the covariance between columns I and j of matrix A  
    * As such, cell(I, i) - the diagonal - contains the variance of the i-column  
    * Note that t(A)%*%A is always a square matrix  
* Suppose that A <- matrix(data=c(1:5, 2*(1:5)), nrow=5, ncol=2, byrow=FALSE)  
	* A[, 1] <- A[, 1] - mean(A[, 1])  
    * A[, 2] <- A[, 2] - mean(A[, 2])  
    * t(A)%*%A/(nrow(A) - 1)  
* The total variance of the dataset is the sum of the eigenvalues of t(A) %*% A / (n-1)  
	* The associated eigenvector are called the principal components of the data  
  
Performing PCA in R:  
  
* Can run the PCA using prcomp()  
	* prcomp(A)  # prints the standard deviations, which are just the square roots of the variances from the previous example  
    * summary(prcomp(A))  
* Can extract the principal components and then apply them to the data  
	* head(prcomp(A)$x[, 1:2])  
    * head(cbind(combine[, 1:4], prcomp(A)$x[, 1:2]))  
* Can run many checks after PCA  
	* Data wrangling and quality control - perhaps there is a main grouping variable such as position  
    * Data visualization may be much easier in 2-dimensions, though PCA is generally better for clustering  
    * Can use PCA as an input for supervised learning, since by definition there is no redundancy remaining after PCA  
  
Wrap Up:  
  
* Vectors and matrices and their interactions  
* Matrix-vector equations and their solutions  
* Eigenvalues and eigenvectors  
* PCA and applications to multivariate datasets  
  
Example code inludes:  
```{r}

combine <- readr::read_csv("./RInputFiles/DataCampCombine.csv")
glimpse(combine)


# Print the first few observations of the dataset
head(combine)

# Find the correlation between variables forty and three_cone
cor(combine$forty, combine$three_cone)

# Find the correlation between variables vertical and broad_jump
cor(combine$vertical, combine$broad_jump)


# Extract columns 5-12 of combine
A <- combine[, 5:12]

# Take the matrix of A
A <- as.matrix(A)

# Subtract the mean of all columns
A[, 1] <- A[, 1] - mean(A[, 1])
A[, 2] <- A[, 2] - mean(A[, 2])
A[, 3] <- A[, 3] - mean(A[, 3])
A[, 4] <- A[, 4] - mean(A[, 4])
A[, 5] <- A[, 5] - mean(A[, 5])
A[, 6] <- A[, 6] - mean(A[, 6])
A[, 7] <- A[, 7] - mean(A[, 7])
A[, 8] <- A[, 8] - mean(A[, 8])


# Create matrix B from equation in instructions
B <- t(A)%*%A/(nrow(A) - 1)

# Compare 1st element of B to 1st column of variance of A
B[1,1]
var(A[, 1])

# Compare 1st element of 2nd column and row element of B to 1st and 2nd columns of A 
B[1, 2]
B[2, 1]
cov(A[, 1], A[, 2])


# Find eigenvalues of B
V <- eigen(B)

# Print eigenvalues
V$values


# Scale columns 5-12 of combine
B <- scale(combine[, 5:12])

# Print the first few rows of the data
head(B)

# Summarize the principal component analysis
summary(prcomp(B))


# Subset combine only to "WR"
combine_WR <- subset(combine, position == "WR")

# Scale columns 5-12 of combine
B <- scale(combine_WR[, 5:12])

# Print the first few rows of the data
head(B)

# Summarize the principal component analysis
summary(prcomp(B))

```
  
  
  
***
  
### _HR Analytics in R: Predicting Employee Churn_  
  
Chapter 1 - Introduction  
  
Turnover:  
  
* Employee turnover is about an employee exiting the company (churn, turnover, attrition are roughly equivalent)  
	* Nearly 25% of US workers leave their jobs every year  
    * Replacing an employee can cost ~50% of the employee's first-year salary  
* Turnover may be voluntary (resignation) or involuntary (firing, end of contract, etc.)  
	* Stated and actual drivers of turnover may not always be the same  
  
Exploring the data:  
  
* Can begin with data exploration of the organizational data  
	* emp_id is the employee ID  
    * status is "active" or "inactive"  
    * turnover is 1 if they left, 0 otherwise  
    * cutoff_date is the study end date  
* Turnover rate is the percentage of employees who left the organization in a given period of time  
	* org %>% count(status)  
    * org %>% summarize(turnover_rate = mean(turnover))  
    * df_level <- org %>% group_by(level) %>% summarize(turnover_level = mean(turnover))  
    * ggplot(df_level, aes(x = level, y = turnover_level)) + geom_col()  
  
HR data architecture:  
  
* May want to identify segments of the data for more meaningful insights and interventions  
	* org2 <- org %>% filter(level %in% c("Analyst", "Specialist"))  
* There are many data sources that may need to be integrated for employee analysis  
	* df3 <- left_join(df1, df2, by = "emp_id")  
  
Example code includes:  
```{r eval=FALSE}

# Load the readr and dplyr packages
library(readr)
library(dplyr)

# Import the org data
org <- read_csv("org.csv")

# Check the structure of org dataset, the dplyr way
glimpse(org)


# Count Active and Inactive employees
org %>% 
  count(status)

# Calculate turnover rate
org %>% 
  summarize(avg_turnover_rate = mean(turnover))


# Calculate level wise turnover rate
df_level <- org %>% 
  group_by(level) %>% 
  summarize(turnover_level = mean(turnover))

# Check the results
df_level

# Visualize the results
ggplot(df_level, aes(x = level, y = turnover_level)) + 
  geom_col()


# Calculate location wise turnover rate
df_location <- org %>% 
  group_by(location) %>% 
  summarize(turnover_location = mean(turnover))

# Check the results
df_location

# Visualize the results
ggplot(df_location, aes(x = location, y = turnover_location)) +
  geom_col()


# Count the number of employees across levels
org %>% 
  count(level)

# Select the employees at Analyst and Specialist level
org2 <- org %>%
  filter(level %in% c("Analyst", "Specialist")) 

# Validate the results
org2 %>% 
  count(level)


# View the structure of rating dataset
glimpse(rating)

# Complete the code to join rating to org2 dataset
org3 <- left_join(org2, rating, by = "emp_id")

# Calculate rating wise turnover rate
df_rating <- org3 %>% 
  group_by(rating) %>% 
  summarize(turnover_rating = mean(turnover))

# Check the result
df_rating


# View the structure of survey dataset
glimpse(survey)

# Complete the code to join survey to org3 dataset
org_final <- left_join(org3, survey, by="mgr_id")

# Compare manager effectiveness scores
ggplot(org_final, aes(x = status, y = mgr_effectiveness)) +
  geom_boxplot()


# View the structure of the dataset
glimpse(org_final)

# Number of variables in the dataset
variables <- ncol(org_final)

# Compare the travel distance of Active and Inactive employees
ggplot(org_final, aes(x = status, y = distance_from_home)) +
  geom_boxplot()

```
  
  
  
***
  
Chapter 2 - Feature Engineering  
  
Feature engineering:  
  
* Can create additional variables from the variables already available in the dataset  
* Job-hopping is described as frequently switching jobs; often high turnover  
* Can computer tenure and timespan  
	* org_final %>% mutate(date_of_joining = dmy(date_of_joining), cutoff_date = dmy(cutoff_date), last_working_date = dmy(last_working_date))  
    * date_1 <- ymd("2000-01-01")  
    * date_2 <- ymd("2014-08-09")  
    * time_length(interval(date_1, date_2), "years")  
  
Compensation:  
  
* Compensation is a key factor driving turnover, and there are many drivers  
	* ggplot(emp_tenure, aes(x = compensation)) + geom_histogram()  
    * ggplot(emp_tenure, aes(x = level, y = compensation)) + geom_boxplot()  
* The compa-ratio is the actual compensation divided by the median compensation  
	* emp_compa_ratio <- emp_tenure %>% group_by(level) %>% mutate(median_compensation = median(compensation), compa_ratio = (compensation / median_compensation))  
    * emp_compa_ratio %>% distinct(level, median_compensation)  
  
Information value:  
  
* Can examine the "information value" for each of the independent variables on the dependent variables  
	* Measure of the predictive power of the independent variable on the response  
    * IV <- Information::create_infotables(data = emp_final, y = "turnover")  # y is the target (dependent) variable  
    * IV$Summary  
* Generally, information value >0.4 is strong, <0.15 is weak, and 0.15 < x < 0.4 is moderate  
  
Example code includes:  
```{r eval=FALSE}

# Add age_diff
emp_age_diff <- org_final %>%
  mutate(age_diff = mgr_age - emp_age)

# Plot the distribution of age difference
ggplot(emp_age_diff, aes(x = status, y = age_diff)) + 
  geom_boxplot()


# Add job_hop_index
emp_jhi <- emp_age_diff %>% 
  mutate(job_hop_index = ifelse(no_previous_companies_worked != 0,  total_experience / no_previous_companies_worked, 0))

# Compare job hopping index of Active and Inactive employees
ggplot(emp_jhi, aes(x = status, y = job_hop_index)) + 
  geom_boxplot()


# Add tenure
emp_tenure <- emp_jhi %>%
  mutate(tenure = ifelse(status == "Active", 
                         time_length(interval(date_of_joining, cutoff_date), 
                                     "years"), 
                         time_length(interval(date_of_joining, last_working_date), 
                                     "years")))

# Compare tenure of active and inactive employees
ggplot(emp_tenure, aes(x = status, y = tenure)) + 
  geom_boxplot()


# Plot the distribution of compensation
ggplot(emp_tenure, aes(x = compensation)) + 
  geom_histogram()

# Plot the distribution of compensation across levels
ggplot(emp_tenure, 
       aes(x = level, y = compensation)) +
  geom_boxplot()

# Compare compensation of Active and Inactive employees across levels
ggplot(emp_tenure, 
       aes(x = level, y = compensation, fill = status)) + 
  geom_boxplot()


# Add median_compensation and compa_ratio
emp_compa_ratio <- emp_tenure %>%  
  group_by(level) %>%   
  mutate(median_compensation = median(compensation), 
         compa_ratio = compensation / median_compensation)

# Look at the median compensation for each level           
emp_compa_ratio %>% 
  distinct(level, median_compensation)


# Add compa_level
emp_final <- emp_compa_ratio %>%  
  mutate(compa_level = ifelse(compa_ratio > 1, "Above", "Below"))

# Compare compa_level for Active & Inactive employees
ggplot(emp_final, aes(x = status, fill = compa_level)) + 
  geom_bar(position = "fill")


# Load Information package
library(Information)

# Compute Information Value 
IV <- create_infotables(data = emp_final, y = "turnover")

# Print Information Value 
IV$Summary

```
  
  
  
***
  
Chapter 3 - Predicting Turnover  
  
Data Splitting:  
  
* Can split the data in to test and train, then build the model on the train data and confirm out-of-sample data on the (previously unseen) test data  
	* index_train <- caret::createDataPartition(emp_final$turnover, p = 0.5, list = FALSE)  
    * train_set <- emp_final[index_train, ]  
    * test_set <- emp_final[-index_train, ]  
  
Introduction to Logistic Regression:  
  
* Can use logistic regression to predict the probability of employee turnover  
	* Categorize data based on the independent variable  
    * Target class for this case is turnover  
    * simple_log <- glm(turnover ~ emp_age, family = "binomial", data = train_set)  
    * summary(simple_log)  
* May want to remove some of the independent variables that are non-relevant  
	* train_set_multi <- train_set %>% select(-c(emp_id, mgr_id, date_of_joining, last_working_date, cutoff_date, mgr_age, emp_age, median_compensation, department, status))  
    * multi_log <- glm(turnover ~ ., family = "binomial", data = train_set_multi)  
  
Multicollinearity:  
  
* Strongly related variables do not provide new, linear information  
* Correlation is the measure of linear association between two variables, between -1 and +1  
	* cor(train_set$emp_age, train_set$compensation)  
* Multicollinearity is where a variable is correlated with 2+ of the other variable  
* The VIF (variance inflation factor) can be assessed using car::vif() - considered to be highly correlated for VIF >= 5  
	* multi_log <- glm(turnover ~ ., family = "binomial", data = train_set_multi)  
    * car::vif(multi_log)  
* Generally, remove the highest VIF variable (assuming greater than 5) model, then re-run, the repeat  
	* new_model <- glm(dependent_variable ~ . - variable_to_remove, family = "binomial", data = dataset)  
  
Building final model:  
  
* Can then build the final model based on the relevant, independent variables  
* Can use the model to calculate probabilities  
    * prediction_train <- predict(final_log, newdata = train_set_final, type = "response")  # type="response" gives a vector of probabilities  
* Generally a best practice to explore the range of the probabilities  
	* hist(prediction_train)  
    * prediction_test <- predict(final_log, newdata = test_set, type = "response")  
  
Example code includes:  
```{r eval=FALSE}

# Load caret
library(caret)

# Set seed of 567
set.seed(567)

# Store row numbers for training dataset: index_train
index_train <- createDataPartition(emp_final$turnover, p = 0.7, list = FALSE)

# Create training dataset: train_set
train_set <- emp_final[index_train, ]

# Create testing dataset: test_set
test_set <- emp_final[-index_train, ]


# Calculate turnover proportion in train_set
train_set %>% 
  count(status) %>% 
  mutate(prop = n / sum(n))

# Calculate turnover proportion in test_set
test_set %>% 
  count(status) %>% 
  mutate(prop = n / sum(n))


# Build a simple logistic regression model
simple_log <- glm(turnover ~ percent_hike, 
                  family = "binomial", data = train_set_multi)

# Print summary
summary(simple_log)


# Build a multiple logistic regression model
multi_log <- glm(turnover ~ ., family = "binomial", 
                 data = train_set_multi)

# Print summary
summary(multi_log)


# Load the car package
library(car)

# Model you built in a previous exercise
multi_log <- glm(turnover ~ ., family = "binomial", data = train_set_multi)

# Check for multicollinearity
vif(multi_log)

# Which variable has the highest VIF?
highest <- "level"


# Remove level
model_1 <- glm(turnover ~ . - level, family = "binomial", 
               data = train_set_multi)

# Check multicollinearity again
vif(model_1)

# Which variable has the highest VIF value?
highest <- "compensation"

# Remove level & compensation
model_2 <- glm(turnover ~ . - level - compensation, family = "binomial", 
               data = train_set_multi)

# Check multicollinearity again
vif(model_2)

# Does any variable have a VIF greater than 5?
highest <- FALSE


# Build the final logistic regression model
final_log <- glm(turnover ~ ., family = "binomial", data=train_set_final)

# Print summary 
summary(final_log)


# Make predictions for training dataset
prediction_train <- predict(final_log, newdata = train_set, type = "response")

# Look at the prediction range
hist(prediction_train)

# Make predictions for testing dataset
prediction_test <- predict(final_log, newdata = test_set, type = "response")

# Look at the prediction range
hist(prediction_test)

# Print the probaility of turnover
prediction_test[c(150, 200)]

```
  
  
  
***
  
Chapter 4 - Model Validation, HR Interventions, and ROI  
  
Validating logistic regression results:  
  
* Need to use a cutoff to convert probabilities to binary decisions  
	* pred_cutoff_50_test <- ifelse(predictions_test > 0.5, 1, 0)  
* The confusion matrix will assess the number of correct predictions made by the model  
	* TP - true positives  
    * TN - true positives  
    * FP - predicted positive, actually negative  
    * FN - predicted negative, actually positive  
* Can look at metrics for the confusion matrix  
	* Accuracy = (TP + TN) / (TP + TN + FP + FN)  
    * conf_matrix_50 <- confusionMatrix(table(test_set$turnover, pred_cutoff_50_test))  
  
Designing retention strategy:  
  
* May want to design retention strategies based on the predicted likelihoods of leaving  
	* Calculate probabilitiy of turnover only for the active employees  
    * emp_risk <- emp_final %>% filter(status=="Active") %>% tidypredict::tidypredict_to_column(final_log)  
    * emp_risk %>% select(emp_id, fit) %>% top_n(5, wt = fit)  
* May want to then bucket the risk of leaving for each employee  
	* emp_risk_bucket <- emp_risk %>% mutate(risk_bucket = cut(fit, breaks = c(0, 0.5, 0.6, 0.8, 1), labels = c("no-risk", "low-risk", "medium-risk", "high-risk")))  
* Can then prioritize retention strategies for employees in the higher turnover risk buckets  
  
Return on investment:  
  
* Can calculate ROI based on the retention strategies employed (costs and outcomes)  
  
Wrap up:  
  
* Basics of turnover analysis  
* Integrating data  
* Modeling data  
* Making conclusions  
  
Example code includes:  
```{r eval=FALSE}

# Classify predictions using a cut-off of 0.5
prediction_categories <- ifelse(prediction_test > 0.5, 1, 0)

# Construct a confusion matrix
conf_matrix <- table(prediction_categories, test_set$turnover)
conf_matrix


# Load caret
library(caret)

# Call confusionMatrix
confusionMatrix(conf_matrix)

# What is the accuracy?
accuracy <- round(unname(confusionMatrix(conf_matrix)$overall["Accuracy"]), 3)


# Load tidypredict 
library(tidypredict)

# Calculate probability of turnover
emp_risk <- emp_final %>%
  filter(status == "Active") %>%
  tidypredict_to_column(final_log)

# Run the code
emp_risk %>% 
  select(emp_id, fit) %>% 
  top_n(2)


# Create turnover risk buckets
emp_risk_bucket <- emp_risk %>% 
  mutate(risk_bucket = cut(fit, breaks = c(0, 0.5, 0.6, 0.8, 1), 
                           labels = c("no-risk", "low-risk", 
                                      "medium-risk", "high-risk")))

# Count employees in each risk bucket
emp_risk_bucket %>% 
  count(risk_bucket)


# Plot histogram of percent hike
ggplot(emp_final, aes(x = percent_hike)) +
  geom_histogram(binwidth = 3)

# Create salary hike_range of Analyst level employees
emp_hike_range <- emp_final %>% 
  filter(level == "Analyst") %>% 
  mutate(hike_range = cut(percent_hike, breaks = c(0, 10, 15, 20),
                          include.lowest = TRUE, 
                          labels = c("0 to 10", "11 to 15", "16 to 20")
                          )
        )


# Calculate the turnover rate for each salary hike range
df_hike <- emp_hike_range %>% 
  group_by(hike_range) %>% 
  summarize(turnover_rate_hike = mean(turnover))

# Check the results
df_hike

# Visualize the results
ggplot(df_hike, aes(x = hike_range, y = turnover_rate_hike)) + 
  geom_col()


# Compute extra cost
extra_cost <- median_salary_analyst * (0.05)

# Compute savings
savings <-  turnover_cost * 0.15

# Calculate ROI
ROI <- (savings / extra_cost) * 100

# Print ROI
cat(paste0("The return on investment is ", round(ROI), "%!"))

```
  
  
  
***
  
### _Dealing with Missing Data in R_  
  
Chapter 1 - Why Care About Missing Data?  
  
Introduction to Missing Data:  
  
* Need to be able to work with missing data; common in real-world applications  
* Imputation needs to be done carefully; just plugging in the mean or median may lead to quirky results  
* Missing values are values that should have been recorded but were not  
	* naniar::any_na(x)  
    * naniar::are_na(x)  
    * naniar::n_miss(x)  
    * naniar::prop_miss(x)  
* Working with missing data  
	* NA + <anything> = NA  
* Can also have NaN, which is interpreted by R as a missing number  
	* naniar::any_na(NaN)  # TRUE  
* NULL is an empty value that is not missing  
	* naniar::any_na(NULL)  # FALSE  
* Be careful about boolean operations  
	* NA | TRUE  # TRUE  
    * NA | FALSE  # FALSE  
  
Why care about missing values?  
  
* Basic summaries of missingness return a single number - n_miss() or n_complete() for example  
* The naniar library has a series of functions at various granularity that all start with miss_*()  
	* miss_var_summary(airquality)  
    * miss_case_summary(airquality)  
    * miss_var_table(airquality)  
    * miss_case_table(airquality)  
* Can look at data over a specific span or run  
	* miss_var_span(pedestrian, var = hourly_counts, span_every = 4000)  # each span of 4000 is treated as a group, with statistics reported  
    * miss_var_run(pedestrian, hourly_counts)  # streaks of missingness; length of each run (repeating patterns)  
    * airquality %>% group_by(Month) %>% miss_var_summary()  
  
How to visualize missing values?  
  
* Visualizing the missingness of the data can help highlight issues  
* Can get a bird's-eye view of the data, including spans and groups  
	* vis_miss(airquality)  
    * vis_miss(airquality, cluster = TRUE)  
    * gg_miss_var(airquality)  
    * gg_miss_case(airquality)  
    * gg_miss_var(airquality, facet = Month)  
    * gg_miss_upset(airquality)  # co-occurrence of missing data  
    * gg_miss_fct(x = airquality, fct = Month)  # plots by factor  
    * gg_miss_span(pedestrian, hourly_counts, span_every = 3000)  
  
Example code includes:  
```{r}

# Create x, a vector, with values NA, NaN, Inf, ".", and "missing"
x <- c(NA, NaN, Inf, ".", "missing")

# Use any_na() and are_na() on to explore the missings
naniar::any_na(x)
naniar::are_na(x)


dat_hw <- data.frame(weight=c(95.16, NA, 102.82, 80.98, 112.91, 94, 105.43, 77.79, NA, 98.93, 68.26, 94.16, 105.32, 61.4, 72.89, 85.67, NA, 63.3, 98.98, 72.17, NA, 103.63, 87.52, 89.78, 103.03, 97.26, 82.77, 68.27, 92.93, 74.55, 61.55, 86.09, 80.04, 88.78, 76.25, 80.44, 99.37, 84.21, NA, 88.5, 97.34, 95.35, 91.91, 78.76, NA, 101.57, 68.33, 89.75, 90.96, 87.17, 104.96, NA, 72.18, 74.09, NA, 92.65, 79.61, 110.09, 77.67, 87.46, 66.91, 76.59, 84.96, 80.21, NA, 64.15, 55.14, NA, 84.47, 100.97, NA, 83.26, 42.15, 89.25, 92.04, NA, 72.76, 69.67, 80.37, NA, 58.38, 84.34, 62.84, NA, 94.23, 83.48, 75.54, 79.93, 79.66, NA, 97.61, 77.11, 83.92, 104.56, 105.94, 107.15, 45.75, 76.61, 88.29, 93.05), height=c(1.95, 2.35, 1.64, 2.47, 1.92, 1.9, 0.83, 2.7, 1.98, 1.83, 0.24, NA, 1.67, NA, 2.03, 2.78, 0.59, 1.99, 2.34, 1.99, -0.05, 0.36, NA, 0.88, NA, 1.37, 2.62, 0.71, 0.52, -0.12, 2.25, 1.06, 1.99, 0.94, -1.11, 1.23, 1.31, 2, 1.1, 0.55, 1.84, 2.14, NA, NA, 1.94, 0.66, 0.47, 2.37, 3.4, 1.4, 2.52, 0.15, 2.42, 0.47, NA, 1.08, 1.89, 2.92, 2.71, NA, 2.72, NA, NA, 1.76, 0.73, 1.84, -0.09, 3.62, 2.34, 0.61, 2.15, 0.39, 0.92, NA, 1.41, 0, 3.51, NA, 0.18, 1.31, 1.19, 2.81, 3.32, 0.06, 3.44, NA, 1.32, NA, 2.46, 3.09, 0.13, 0.92, 0.16, 0.88, 1.38, 0.28, 2.51, NA, 1.05, 3.16))

# Use n_miss() to count the total number of missing values in dat_hw
naniar::n_miss(dat_hw)

# Use n_miss() on dat_hw$weight to count the total number of missing values
naniar::n_miss(dat_hw$weight)

# Use n_complete() on dat_hw to count the total number of complete values
naniar::n_complete(dat_hw)

# Use n_complete() on dat_hw$weight to count the total number of complete values
naniar::n_complete(dat_hw$weight)

# Use prop_miss() and prop_complete() on dat_hw to count the total number of missing values in each of the variables
naniar::prop_miss(dat_hw)
naniar::prop_complete(dat_hw)


data(airquality)
str(airquality)

# Summarise missingness in each variable of the `airquality` dataset
naniar::miss_var_summary(airquality)

# Summarise missingness in each case of the `airquality` dataset
naniar::miss_case_summary(airquality)

# Return the summary of missingness in each variable, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_var_summary()

# Return the summary of missingness in each case, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_case_summary()


# Tabulate missingness in each variable and case of the `airquality` dataset
naniar::miss_var_table(airquality)
naniar::miss_case_table(airquality)

# Tabulate the missingness in each variable, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_var_table()

# Tabulate of missingness in each case, grouped by Month, in the `airquality` dataset
airquality %>% 
    group_by(Month) %>% 
    naniar::miss_case_table()


data(pedestrian, package="naniar")
str(pedestrian)

library(naniar)


# need to add so that the RLE can be converted to data.frame in naniar::miss_var_run
as.data.frame.rle <- function(x, ...) do.call(data.frame, x)


# Calculate the summaries for each run of missingness for the variable `hourly_counts`
naniar::miss_var_run(pedestrian, var = hourly_counts)

# Calculate the summaries for each span of missingness, for a span of 4000, for the variable `hourly_counts`
naniar::miss_var_span(pedestrian, var = "hourly_counts", span_every = 4000)

# For each `month` variable, calculate the run of missingness for `hourly_counts`
pedestrian %>% 
    group_by(month) %>% 
    naniar::miss_var_run(var = "hourly_counts")

# For each `month` variable, calculate the span of missingness of a span of 2000
pedestrian %>% 
    group_by(month) %>% 
    naniar::miss_var_span(var = "hourly_counts", span_every = 2000)


data(riskfactors, package="naniar")
str(riskfactors)

# Visualize all of the missingness in the `riskfactors`  dataset
naniar::vis_miss(riskfactors)

# Visualize and cluster all of the missingness in the `riskfactors` dataset
naniar::vis_miss(riskfactors, cluster = TRUE)

# visualise and sort the columns by missingness in the `riskfactors` dataset
naniar::vis_miss(riskfactors, sort_miss = TRUE)


# Visualize the number of missings in cases using `gg_miss_case()`
naniar::gg_miss_case(riskfactors)

# Explore the number of missings in cases using `gg_miss_case()` and facet by the variable `education`
naniar::gg_miss_case(riskfactors, facet = education)

# Visualize the number of missings in variables using `gg_miss_var()`
naniar::gg_miss_var(riskfactors)

# Explore the number of missings in variables using `gg_miss_var()` and facet by the variable `education`
naniar::gg_miss_var(riskfactors, facet = education)


# Using the `airquality` dataset, explore the missingness pattern using `gg_miss_upset()`
naniar::gg_miss_upset(airquality)

# With the `riskfactors` dataset, explore how the missingness changes across the `marital` using `gg_miss_fct()`
naniar::gg_miss_fct(x = riskfactors, fct = marital)

# Using the `pedestrian` dataset Explore how the missingness changes over a span of 3000 
naniar::gg_miss_span(pedestrian, var = hourly_counts, span_every = 3000)

# Using the `pedestrian` dataset: Explore the impact of `month` by facetting by `month`
# and explore how missingness changes for a span of 1000
naniar::gg_miss_span(pedestrian, var = hourly_counts, span_every = 1000, facet = month)

```
  
  
  
***
  
Chapter 2 - Wrangling and Tidying Missing Values  
  
Search for and replace missing values:  
  
* May find that not all missing values are coded as NA - N/A or "missing" or the like  
* Can use the "chaos" dataset  
	* chaos %>% miss_scan_count(search = list("N/A"))  
    * chaos %>% miss_scan_count(search = list("N/A", "N/a"))  
* Can replace the other specifications of NA  
	* chaos %>% replace_with_na(replace = list(grade = c("N/A", "N/a")))  
* Can use the "scoped variance" features similar to dplyr  
	* chaos %>% replace_with_na_all(condition = ~.x == -99)  # ~ is for function, and .x is the reference to a variable  
    * chaos %>% replace_with_na_all(condition = ~.x %in% c("N/A", "missing", "na"))  
  
Filling down missing values:  
  
* May want to manage implied missing values - "missing missing values"  
* Can spread the data to make missing values more obvious; may help to "un-tidy" the data  
	* tetris %>% tidyr::complete(name, time)  # will add the missing combinations, keeping the data in tidy format  
* May want to fill a value down, for example when name is added only the first time the record is shown  
	* tetris %>% tidyr::fill(name)  # default LOCF  
  
Missing data dependence:  
  
* Missing data dependence has theory associate with at  
	* MCAR - missing compleetly at random  
    * MAR - missing at random  
    * MNAR - missing not at random  
* MCAR is where the data has no association with other observed or unobserved data (e.g., missing workers on vacation)  
	* Implications are that imputing is OK; deleting might be OK, but may lose too much data (delete only if ~5% loss or better)  
* MAR is where missingness depends on data observed but not on data unobserved (e.g., missing workers more likely with depression)  
	* Do not delete and be careful with imputing  
* MNAR is where missingness is related to an unobserved variable of interest  
* Can use visualizations to see the potential structures of various types of missing data  
	* vis_miss(mt_cars, cluster = TRUE)  # noisy patterns suggest MCAR  
    * oceanbuoys %>% arrange(year) %>% vis_miss()  # clustering of missingness suggest MAR  
    * vis_miss(ocean, cluster = TRUE)  # blocks of data may suggest MNAR, but can be challenging to find  
  
Example code includes:  
```{r}

pacman <- tibble(year=c('2004', '1991', 'na', '1992', '1988', '2007', '2016', '2011', '2018', '2012', '1983', '1988', '1981', '1990', '1989', '1995', 'missing', 'missing', '2003', '2000', '2012', '2008', '2007', '1987', '2009', '1987', '2016', '2011', '2008', '1984', '2003', '1988', '2001', '1990', '2018', '1985', '2010', '1986', '1980', '1982', '2009', '1998', '1991', '1987', '1982', '1998', '2004', '2007', '2000', '2014', '1980', '1983', '2011', '2003', '2013', '2018', '2006', '2005', '1994', '2009', '2004', '1991', 'na', '2004', '1993', '1989', '2004', '2011', '1990', '1985', '2017', '1992', '1999', '2014', '1996', '2007', '2008', '1998', '1996', '1998', '2017', '1998', '2016', '1983', '2009', 'missing', '1993', '1989', '1994', '1980', '1983', '2004', 'missing', '1997', '1994', '2008', 'missing', '2007', '2016', '1992', '2000', '2002', '2004', '2007', '2013', '1983', '2005', '1999', '1990', '1998', '1982', '2002', 'na', '1998', '2006', '2004', '2012', '1981', '2000', '2014', '1999', '1997', '2003', '1993', '1982', '1992', '2008', '1985', '2016', '1990', '1991', '1980', '2000', 'na', '2018', 'na', '2014', '1988', 'missing', '2002', '2012', '2017', '1987', '1998', '1999', '1985', '1989', '2017', '1982', '1994', '2003', 'na', '2011', 'missing', 'missing', '1986', '2007', '2006', 'missing', '2010', '1982', '2008', '1983', '2018', '1987', '1983', 'missing', 'missing', '1998', '1988', '2010', '1981', 'na', '2016', 'na', '1992', '2001', '1995', '1999', '2009', 'na', 'na', '2003', '2017', 'na', '1982', '2005', '2013', '1990', '2004', '2004', '2006', '2009', '1984', '2007', '1987', 'na', '2001', '1983', '2012'), 
                 month=c('6', '11', 'na', '11', '9', '12', '9', '1', '4', '9', '4', '11', '6', '7', '10', '8', 'missing', 'missing', '5', '5', '8', '1', '10', '11', '6', '7', '10', '8', '7', '1', '9', '10', '11', '7', '1', '5', '10', '6', '8', '11', '11', '8', '10', '8', '1', '9', '9', '7', '11', '11', '10', '7', '12', '9', '12', '8', '11', '4', '11', '1', '1', '9', 'na', '7', '10', '10', '3', '3', '9', '5', '8', '1', '5', '12', '6', '3', '7', '9', '12', '2', '5', '8', '4', '6', '1', 'missing', '8', '1', '12', '2', '5', '8', 'missing', '7', '2', '7', 'missing', '4', '11', '6', '5', '11', '12', '3', '3', '5', '1', '6', '12', '1', '11', '7', 'na', '9', '11', '7', '9', '10', '10', '11', '5', '11', '6', '6', '4', '10', '10', '1', '4', '6', '8', '12', '11', 'na', '11', 'na', '10', '9', 'missing', '5', '1', '5', '4', '3', '2', '11', '2', '9', '3', '3', '5', 'na', '12', 'missing', 'missing', '4', '1', '2', 'missing', '7', '2', '1', '4', '9', '4', '3', 'missing', 'missing', '1', '12', '2', '4', 'na', '3', 'na', '11', '7', '2', '5', '1', 'na', 'na', '1', '4', 'na', '10', '5', '4', '8', '2', '9', '11', '7', '10', '2', '9', 'na', '4', '6', '10'), 
                 day=c('1', '22', 'na', '16', '16', '4', '5', '25', '14', '25', '1', '8', '17', '14', '15', '18', 'missing', 'missing', '21', '18', '21', '2', '15', '18', '22', '26', '25', '3', '24', '6', '18', '6', '4', '1', '23', '3', '10', '23', '11', '4', '11', '17', '23', '10', '8', '19', '10', '6', '24', '9', '25', '18', '7', '25', '24', '23', '17', '8', '10', '17', '8', '8', 'na', '24', '25', '7', '6', '19', '10', '13', '24', '13', '26', '4', '5', '21', '28', '15', '22', '10', '11', '15', '20', '23', '6', 'missing', '3', '2', '24', '11', '21', '21', 'missing', '24', '13', '6', 'missing', '14', '13', '17', '11', '18', '24', '9', '6', '1', '11', '21', '3', '12', '23', '27', 'na', '1', '13', '7', '17', '11', '13', '11', '20', '7', '2', '10', '9', '24', '21', '12', '25', '17', '14', '24', '18', 'na', '7', 'na', '22', '8', 'missing', '12', '14', '15', '21', '21', '1', '11', '18', '9', '18', '15', '1', 'na', '9', 'missing', 'missing', '28', '13', '12', 'missing', '3', '16', '5', '3', '19', '14', '7', 'missing', 'missing', '4', '25', '15', '4', 'na', '21', 'na', '5', '11', '10', '11', '15', 'na', 'na', '2', '10', 'na', '4', '16', '12', '11', '19', '5', '23', '24', '22', '27', '11', 'na', '23', '21', '27'), 
                 initial=c('XGB', 'VGP', 'UAW', 'MXL', 'ZPM', 'ESF', 'YKM', 'ABS', 'NDT', 'GAS', 'IFA', 'OUH', 'PZB', 'EKR', 'TXO', 'NCV', 'XSL', 'ATM', 'LEN', 'QNE', 'CBV', 'DLU', 'LTW', 'TCV', 'BVC', 'GSP', 'LVJ', 'YQD', 'HSX', 'KNX', 'PYK', 'PVD', 'OAB', 'GHB', 'LCI', 'HMU', 'VRQ', 'WAJ', 'AIK', 'YPJ', 'BMO', 'YEH', 'YHK', 'YIA', 'TDA', 'XYF', 'LMH', 'JTO', 'ZFD', 'SXE', 'QYC', 'MPI', 'TSI', 'IVR', 'ILM', 'CME', 'FVU', 'HFJ', 'DEF', 'TCX', 'BGA', 'PBK', 'TIB', 'FYX', 'OJA', 'GEH', 'LJB', 'IHF', 'NMS', 'WSC', 'WTO', 'JBV', 'JQI', 'TCP', 'MLU', 'NBM', 'QMY', 'DLV', 'UHP', 'BGE', 'WCR', 'DNC', 'KZS', 'DBM', 'IUC', 'LRG', 'ONT', 'VKF', 'GFU', 'EQI', 'CUR', 'SAZ', 'CFU', 'SOH', 'QTM', 'CZV', 'QNR', 'LMG', 'SGR', 'DXC', 'BKI', 'CMP', 'VDR', 'CIA', 'QYW', 'CJR', 'HJQ', 'NTE', 'EGA', 'ZUY', 'AMT', 'LKP', 'HFW', 'PZQ', 'PJI', 'QJB', 'LAU', 'XYO', 'OJV', 'OBZ', 'QPV', 'LAH', 'UHW', 'XIT', 'UMB', 'OPM', 'GSC', 'PFU', 'OEC', 'ERU', 'ZWA', 'CJA', 'IGE', 'ZBQ', 'XVO', 'BWF', 'VAW', 'WDQ', 'JWT', 'QCT', 'JAH', 'WAQ', 'RCS', 'JPL', 'KCF', 'NXE', 'OPW', 'WYP', 'RMS', 'LND', 'YVO', 'XIR', 'AUW', 'OLA', 'ORF', 'ZAU', 'FXE', 'ACE', 'FQW', 'BND', 'SKA', 'BZX', 'JKY', 'IOZ', 'IYG', 'YZK', 'FOU', 'ZJT', 'XLA', 'TEZ', 'YKB', 'CYS', 'UBJ', 'DKO', 'EWZ', 'PBU', 'GEU', 'LVW', 'YWO', 'WBH', 'GXH', 'NPY', 'UIW', 'EXP', 'QAX', 'RCH', 'ZFM', 'SML', 'FNC', 'HQI', 'NQO', 'QLM', 'EGI', 'CIQ', 'ORU', 'AGP', 'MPY', 'EFL', 'VXR', 'QYE'), 
                 score=c('892369', '2412494', '1874449', '1583331', '3159043', '2755582', '804088', '2392395', '431430', '1482088', '3099396', '810873', '2410285', '1602619', 'N/A', '1547264', '1086746', '885575', '2464437', '333868', '2991881', '1207552', '332352', '115716', 'N/A', '2551711', '679715', '3033343', '275723', '1677698', '1031285', '3251416', '1812998', '1767317', '2457197', '2194699', '1258734', '535437', '3202731', '899729', '1099688', '2125942', '2407498', '1785754', '2181741', '1058088', '1630900', '1629161', '2378243', '3211114', '65436', '2006229', '2068916', '1653110', '2589346', '1520554', '374610', 'N/A', '2841676', '1001739', '438268', '2476918', '2584965', '702929', '189630', 'N/A', '410549', '1269273', '2658430', '1760979', 'N/A', '2705304', '1560004', '826721', '3291811', '2366950', '832279', '426785', '2898752', '1369821', '2712315', '2123280', '2513951', '1004901', '645429', '846193', '313628', '1791507', '2612127', '836682', '1955459', '1866444', '75834', '532534', '3267355', '235734', '2279669', '2976729', '2297788', '1166581', '15715', '890432', '1670356', '1463904', '2867923', '1761345', '2667484', '2357424', '3053758', '2077402', '1052647', '1661650', '123930', '3171836', '1910536', '2100782', '679137', '1424599', '2194459', '1263044', '1948854', 'N/A', '3092624', '2077243', '1010777', '3289300', '3172553', '891045', '1592747', '728752', 'N/A', '24667', 'N/A', '827488', '1643701', '2844488', '539713', '3160321', '762261', '2505569', '271322', '1479487', '1217212', '2960042', '1825455', '1287888', '2105751', '450550', '894755', '3115431', '781721', '3220718', '767717', '3204211', '1666549', '3128098', '2445271', '1571440', '2088915', '645360', '2321491', '1135310', '1736847', '2378391', '3097570', '1220994', '165122', '2007635', '876910', '1551229', '1357429', '2168680', '1411345', '3290465', '1860365', '3181429', '2872190', '2780599', '2160057', '60716', '2222480', '22113', '2815280', 'N/A', '2517561', '500742', '3077608', '1481553', '1349499', '2539062', '2057675', '2869686', '863857', '2609949', '2337505', '76444', '3062706', '3031438', '759570', '1741154'), 
                 country=c(' ', 'US', ' ', ' ', ' ', 'US', 'NZ', 'CA', 'GB', 'CN', 'ES', 'US', 'NZ', 'AU', 'CN', 'US', 'CA', 'US', 'US', 'CN', 'AU', 'ES', 'NZ', 'CA', 'CN', 'ES', 'NZ', 'NZ', 'CN', 'GB', 'CN', 'US', 'ES', 'CN', 'US', 'CN', 'AU', 'GB', 'ES', 'AT', ' ', 'US', 'NZ', 'AU', ' ', 'US', 'US', 'US', 'ES', 'NZ', 'AT', 'NZ', 'JP', 'ES', 'NZ', 'NZ', 'GB', 'CN', 'AU', 'GB', 'ES', 'GB', 'AT', 'NZ', 'CN', 'US', 'AU', 'GB', 'US', 'JP', 'CA', 'AT', 'AT', 'CN', 'AU', 'JP', 'CA', 'GB', 'AT', 'AU', 'GB', 'CN', 'AU', 'GB', 'AT', 'NZ', 'JP', 'GB', ' ', 'CN', 'US', 'JP', 'CN', 'GB', 'GB', 'GB', 'AT', 'US', 'GB', 'GB', 'JP', 'CN', 'AU', 'AU', 'AT', 'JP', 'US', 'JP', 'NZ', 'JP', 'AT', 'NZ', 'CA', 'CA', 'GB', 'ES', 'ES', 'GB', 'ES', 'GB', 'AU', 'GB', 'AT', 'CN', 'AT', 'ES', ' ', 'CA', 'CA', 'GB', 'AU', 'CN', 'ES', 'NZ', 'CA', 'JP', 'JP', 'NZ', 'GB', 'CA', 'ES', 'AT', 'AU', 'CA', 'CN', 'US', 'JP', 'AT', 'CA', 'JP', ' ', 'GB', 'GB', 'NZ', 'AU', 'JP', 'US', 'US', 'AU', 'US', 'AT', 'GB', 'GB', 'GB', 'AT', 'CN', 'ES', 'US', 'JP', 'GB', 'AT', 'JP', 'AU', 'NZ', 'GB', 'GB', 'ES', 'ES', 'AT', 'GB', 'CN', ' ', 'US', 'JP', 'AT', 'US', 'CA', 'AT', 'US', 'GB', 'US', 'ES', 'US', ' ', 'ES', 'JP', 'CA', 'AU', 'CA', 'US')                 )
pacman


# Explore all of the strange missing values, "N/A", "missing", " ", "na"
naniar::miss_scan_count(data = pacman, search = list("N/A", "missing", " ", "na"))


# Print the top of the pacman data using `head()`
head(pacman)

# Replace the strange missing values "N/A" and "missing" with `NA`
pacman_clean <- naniar::replace_with_na(pacman, replace = list(year = c("N/A", "na", "missing"),
                                                               score = c("N/A", "na", "missing")
                                                               )
                                        )
                                        
# Test if `pacman_clean` still has these values in it?
naniar::miss_scan_count(pacman_clean, search = list("N/A", "na", "missing"))


# Use `replace_with_na_at()` to replace with NA
naniar::replace_with_na_at(pacman, .vars = c("year", "month", "day"),  
                           ~.x %in% c("N/A", "missing", "na", " ")
                           )

# Use `replace_with_na_if()` to replace with NA
naniar::replace_with_na_if(pacman, .predicate = is.character, 
                           ~.x %in% c("N/A", "missing", "na")
                           )

# Use `replace_with_na_all()` to replace with NA
naniar::replace_with_na_all(pacman, ~.x %in% c("N/A", "missing", "na"))


frogger <- tibble(name=factor(c('jesse', 'jesse', 'jesse', 'jesse', 'andy', 'andy', 'andy', 'nic', 'nic', 'dan', 'dan', 'alex', 'alex', 'alex', 'alex')), 
                  time=factor(c('morning', 'afternoon', 'evening', 'late_night', 'morning', 'afternoon', 'late_night', 'afternoon', 'late_night', 'morning', 'evening', 'morning', 'afternoon', 'evening', 'late_night')), 
                  value=as.integer(c(6678, 800060, 475528, 143533, 425115, 587468, 111000, 588532, 915533, 388148, 180912, 552670, 98355, 266055, 121056))
                  )
str(frogger)


# Use `complete()` on the `time` variable to make implicit missing values explicit
frogger
frogger_tidy <- frogger %>% 
    complete(name, time)
frogger_tidy



# Use `fill()` to fill down the name variable in the frogger dataset
frogger
frogger %>% 
    fill(name)


# Correctly fill() and complete() missing values so that our dataset becomes sensible
frogger
frogger %>% 
    fill(name) %>%
    complete(name, time)


data("oceanbuoys", package="naniar")
str(oceanbuoys)

# Arrange by year
oceanbuoys %>% 
    arrange(year) %>% 
    naniar::vis_miss()

# Arrange by latitude
oceanbuoys %>% 
    arrange(latitude) %>% 
    naniar::vis_miss()

# Arrange by wind_ew (wind east west)
oceanbuoys %>% 
    arrange(wind_ew) %>% 
    naniar::vis_miss()

```
  
  
  
***
  
Chapter 3 - Testing Missing Relationships  
  
Tools to explore missing data dependence:  
  
* Example of census data containing income and education  
	* Faceting by education will show differences in income by education  
* Can consider a shadow matrix where 1 means missing and 0 means present  
	* The shadow matrix variables have the same names as the regular data, with _NA as a suffix  
    * Clear values are given; they are either NA or !NA  
    * Can also bind the shadow matrix to the original data ("nabular" data)  
    * bind_shadow(airquality)  
    * airquality %>% bind_shadow() %>% group_by(Ozone_NA) %>% summarise(mean = mean(Wind))  
  
Visualizing missingness across one variable:  
  
* Can explore conditional plots based on the missingness status of the data  
	* ggplot(airquality, aes(x = Temp)) + geom_density()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, color = Ozone_NA)) + geom_density()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Ozone_NA, y = Temp)) + geom_boxplot()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp)) + geom_density() + facet_wrap(~Ozone_NA)  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, y = Wind)) + geom_point() + facet_wrap(~Ozone_NA)  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, y = Wind, color = Ozone_NA)) + geom_point()  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Temp, color = Ozone_NA)) + geom_density() + facet_wrap(~Solar.R_NA)  
  
Visualizing misingness across two variables:  
  
* Missing values are frequently ignored in a scatterplot, but it may be helpful to still consider them  
	* ggplot(airquality, aes(x = Ozone, y = Solar.R)) + geom_miss_point()  # imputes to 10% below the minimum for the missing dimension  
    * ggplot(airquality, aes(x = Wind, y = Ozone)) + geom_miss_point() + facet_wrap(~Month)  
    * airquality %>% bind_shadow() %>% ggplot(aes(x = Wind, y = Ozone)) + geom_miss_point() + facet_wrap(~Solar.R_NA)  
  
Example code includes:  
```{r}

# Create shadow matrix data with `as_shadow()`
naniar::as_shadow(oceanbuoys)

# Create nabular data by binding the shadow to the data with `bind_shadow()`
naniar::bind_shadow(oceanbuoys)

# Bind only the variables with missing values by using bind_shadow(only_miss = TRUE)
naniar::bind_shadow(oceanbuoys, only_miss = TRUE)


# `bind_shadow()` and `group_by()` humidity missingness (`humidity_NA`)
oceanbuoys %>%
    naniar::bind_shadow() %>%
    group_by(humidity_NA) %>% 
    summarise(wind_ew_mean = mean(wind_ew), 
              wind_ew_sd = sd(wind_ew)
              ) 

# Repeat this, but calculating summaries for wind north south (`wind_ns`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    group_by(humidity_NA) %>% 
    summarise(wind_ns_mean = mean(wind_ns), 
              wind_ns_sd = sd(wind_ns)
              )


# Summarise wind_ew by the missingness of `air_temp_c_NA`
oceanbuoys %>% 
    naniar::bind_shadow() %>%
    group_by(air_temp_c_NA) %>%
    summarise(wind_ew_mean = mean(wind_ew),
              wind_ew_sd = sd(wind_ew),
              n_obs = n()
              )


# Summarise wind_ew by missingness of `air_temp_c_NA` and `humidity_NA`
oceanbuoys %>% 
    naniar::bind_shadow() %>%
    group_by(air_temp_c_NA, humidity_NA) %>%
    summarise(wind_ew_mean = mean(wind_ew),
              wind_ew_sd = sd(wind_ew),
              n_obs = n()
              )


# First explore the missingness structure of `oceanbuoys` using `vis_miss()`
naniar::vis_miss(oceanbuoys)

# Explore the distribution of `wind_ew` for the missingness of `air_temp_c_NA` using  `geom_density()`
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = wind_ew, color = air_temp_c_NA)) + 
    geom_density()

# Explore the distribution of sea temperature for the missingness of humidity (humidity_NA) using  `geom_density()`
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = sea_temp_c, color = humidity_NA)) + 
    geom_density()


# Explore the distribution of wind east west (`wind_ew`) for the missingness of air temperature using  `geom_density()` and facetting by the missingness of air temperature (`air_temp_c_NA`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = wind_ew)) + 
    geom_density() + 
    facet_wrap(~air_temp_c_NA)

# Build upon this visualisation by coloring by the missingness of humidity (`humidity_NA`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = wind_ew, color = humidity_NA)) + 
    geom_density() + 
    facet_wrap(~air_temp_c_NA)


# Explore the distribution of wind east west (`wind_ew`) for the missingness of air temperature using  `geom_boxplot()`
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = air_temp_c_NA, y = wind_ew)) + 
    geom_boxplot()

# Build upon this visualisation by facetting by the missingness of humidity (`humidity_NA`).
oceanbuoys %>%
    naniar::bind_shadow() %>%
    ggplot(aes(x = air_temp_c_NA, y = wind_ew)) + 
    geom_boxplot() + 
    facet_wrap(~humidity_NA)


# Explore the missingness in wind and air temperature, and display the missingness using `geom_miss_point()`
ggplot(oceanbuoys, aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point()

# Explore the missingness in humidity and air temperature, and display the missingness using `geom_miss_point()`
ggplot(oceanbuoys, aes(x = humidity, y = air_temp_c)) + 
    naniar::geom_miss_point()


# Explore the missingness in wind and air temperature, and display the missingness using `geom_miss_point()`. Facet by year to explore this further.
ggplot(oceanbuoys, aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_wrap(~year)

# Explore the missingness in humidity and air temperature, and display the missingness using `geom_miss_point()` Facet by year to explore this further.
ggplot(oceanbuoys, aes(x=humidity, y=air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_wrap(~year)


# Use geom_miss_point() and facet_wrap to explore how the missingness in wind_ew and air_temp_c is different for missingness of humidity
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_wrap(~humidity_NA)

# Use geom_miss_point() and facet_grid to explore how the missingness in wind_ew and air_temp_c is different for missingness of humidity AND by year - by using `facet_grid(humidity_NA ~ year)`
naniar::bind_shadow(oceanbuoys) %>%
    ggplot(aes(x = wind_ew, y = air_temp_c)) + 
    naniar::geom_miss_point() + 
    facet_grid(humidity_NA~year)

```
  
  
  
***
  
Chapter 4 - Imputation  
  
Filling in the blanks:  
  
* Imputation can help with understanding data structure, as well as visualizing and analyzing based on imputed data  
	* impute_below(c(5,6,7,NA,9,10))  # imputes the NA to be lower than anything in the data at hand  
    * impute_below_if(data, is.numeric)  # run only for numeric  
    * impute_below_at(data, vars(var1,var2))  # select variables  
    * impute_below_all(data)  # all variables  
* Can also use bind_shadow() to maintain a history of which data points were initially missing and then imputed  
	* aq_imp <- airquality %>% bind_shadow() %>% impute_below_all()  
    * ggplot(aq_imp, aes(x = Ozone, fill = Ozone_NA)) + geom_histogram()  
    * ggplot(aq_imp, aes(x = Ozone, fill = Ozone_NA)) + geom_histogram() + facet_wrap(~Month)  
* Can add labels for whether any of the data are missing  
	* aq_imp <- airquality %>% bind_shadow() %>% add_label_missings() %>% impute_below_all()  
    * ggplot(aq_imp, aes(x = Ozone, y = Solar.R, colour = any_missing)) + geom_point()  
  
What makes a good imputation?  
  
* Imputation should be done with care such that the resulting dataset are still reasonable given the rest of the data and the real world  
* Bad imputations come in many forms  
	* Mean imputation - calculate the mean from the non-missing data (often ignores the underlying structure of the data)  
    * aq_impute_mean <- airquality %>% bind_shadow(only_miss = TRUE) %>% impute_mean_all() %>% add_label_shadow()  
* Can explore imputations using the boxplot, scatterplot, long shadow format histograms, etc.  
	* ggplot(aq_impute_mean, aes(x = Ozone_NA, y = Ozone)) + geom_boxplot()  
    * ggplot(aq_impute_mean, aes(x = Ozone, y = Solar.R, colour = any_missing)) + geom_point()  
    * aq_imp <- airquality %>% bind_shadow() %>% impute_mean_all()  
    * aq_imp_long <- shadow_long(aq_imp, Ozone, Solar.R)  
    * ggplot(aq_imp_long, aes(x = value, fill = value_NA)) + geom_histogram() + facet_wrap(~variable) 
  
Performing imputations:  
  
* Can use linear regression as a tool for imputation - can use the package "simputation"  
	* df %>% bind_shadow(only_miss = TRUE) %>% add_label_shadow() %>% simputation::impute_lm(y ~ x1 + x2)  
    * aq_imp_lm <- airquality %>% bind_shadow() %>% add_label_shadow() %>% simputation::impute_lm(Solar.R ~ Wind + Temp + Month) %>% simputation::impute_lm(Ozone ~ Wind + Temp + Month)  
    * ggplot(aq_imp_lm, aes(x = Solar.R, y = Ozone, colour = any_missing)) + geom_point()  
* Can compare multiple attempts at imputation as well  
	* aq_imp_small <- airquality %>% bind_shadow() %>% impute_lm(Ozone ~ Wind + Temp) %>% simputation::impute_lm(Solar.R ~ Wind + Temp) %>% add_label_shadow()  
    * aq_imp_large <- airquality %>% bind_shadow() %>% impute_lm(Ozone ~ Wind + Temp + Month + Day) %>% simputation::impute_lm(Solar.R ~ Wind + Temp + Month + Day) %>% add_label_shadow()  
    * bound_models <- bind_rows(small = aq_imp_small, large = aq_imp_large, .id = "imp_model")  
    * ggplot(bound_models, aes(x = Ozone, y = Solar.R, colour = any_missing)) + geom_point() + facet_wrap(~imp_model)  
    * bound_models_gather <- bound_models %>% select(Ozone, Solar.R, any_missing, imp_model) %>% gather(key = "variable", value = "value", -any_missing, -imp_model)  
    * ggplot(bound_models_gather, aes(x = imp_model, y = value)) + geom_boxplot() + facet_wrap(~key)
bound_models_gather %>% filter(any_missing == "Missing") %>% ggplot(aes(x = imp_model, y = value)) + geom_boxplot() + facet_wrap(~key)  
  
Evaluating imputations and models:  
  
* Can run a standard linear regression, or a regression only with complete.cases  
	* aq_cc <- airquality %>% na.omit() %>% bind_shadow() %>% add_label_shadow()  
    * aq_imp_lm <- bind_shadow(airquality) %>% add_label_shadow() %>% impute_lm(Ozone ~ Temp + Wind + Month + Day) %>% impute_lm(Solar.R ~ Temp + Wind + Month + Day)  
    * bound_models <- bind_rows(cc = aq_cc, imp_lm = aq_imp_lm, .id = "imp_model")  
    * model_summary <- bound_models %>% group_by(imp_model) %>% nest() %>% mutate(mod = map(data, ~lm(Temp ~ Ozone + Solar.R + Wind + Temp + Days + Month data = .)), res = map(mod, residuals), pred = map(mod, predict), tidy = map(mod, broom::tidy))  
* Can then examine the impacts of the various imputation techniques  
	* model_summary %>% select(imp_model, tidy) %>% unnest()  
    * model_summary %>% select(imp_model, res) %>% unnest() %>% ggplot(aes(x = res, fill = imp_model)) + geom_histogram(position = "dodge")  
    * model_summary %>% select(imp_model, pred) %>% unnest() %>% ggplot(aes(x = pred, fill = imp_model)) + geom_histogram(position = "dodge")  
  
Example code includes:  
```{r}

# Impute the oceanbuoys data below the range using `impute_below`.
ocean_imp <- naniar::impute_below_all(oceanbuoys)

# Visualise the new missing values
ggplot(ocean_imp, aes(x = wind_ew, y = air_temp_c)) +  
  geom_point()

# Impute and track data with `bind_shadow`, `impute_below_all`, and `add_label_shadow`
ocean_imp_track <- naniar::bind_shadow(oceanbuoys) %>% 
  naniar::impute_below_all() %>%
  naniar::add_label_shadow()

# Look at the imputed values
ocean_imp_track
ggplot(ocean_imp_track, aes(x=wind_ew, y=air_temp_c, colour=any_missing)) + 
  geom_point()

# Visualise the missingness in wind and air temperature, coloring missing air temp values with air_temp_c_NA
ggplot(ocean_imp_track, aes(x = wind_ew, y = air_temp_c, color = air_temp_c_NA)) + 
    geom_point()

# Visualise humidity and air temp, coloring any missing cases using the variable any_missing
ggplot(ocean_imp_track, aes(x = humidity, y = air_temp_c, color = any_missing)) + 
    geom_point()


# Explore the values of air_temp_c, visualising the amount of missings with `air_temp_c_NA`.
p <- ggplot(ocean_imp_track, aes(x = air_temp_c, fill = air_temp_c_NA)) + 
    geom_histogram()

# Expore the missings in humidity using humidity_NA
p2 <- ggplot(ocean_imp_track,  aes(x = humidity, fill = humidity_NA)) + 
    geom_histogram()

# Explore the missings in air_temp_c according to year, using `facet_wrap(~year)`.
p + facet_wrap(~year)

# Explore the missings in humidity according to year, using `facet_wrap(~year)`.
p2 + facet_wrap(~year)


# Impute the mean value and track the imputations 
ocean_imp_mean <- naniar::bind_shadow(oceanbuoys) %>% 
  naniar::impute_mean_all() %>% 
  naniar::add_label_shadow()

# Explore the mean values in humidity in the imputed dataset
ggplot(ocean_imp_mean, aes(x = humidity_NA, y = humidity)) + 
    geom_boxplot()

# Explore the values in air temperature in the imputed dataset
ggplot(ocean_imp_mean, aes(x = air_temp_c_NA, y = air_temp_c)) + 
    geom_boxplot()

# Explore imputations in air temperature and humidity, coloring by the variable, any_missing
ggplot(ocean_imp_mean, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point()

# Explore imputations in air temperature and humidity, coloring by the variable, any_missing, and faceting by year
ggplot(ocean_imp_mean, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point() + 
    facet_wrap(~year)


# Gather the imputed data 
ocean_imp_mean_gather <- naniar::shadow_long(ocean_imp_mean, humidity, air_temp_c)

# Inspect the data
ocean_imp_mean_gather

# Explore the imputations in a histogram 
ggplot(ocean_imp_mean_gather, aes(x = as.numeric(value), fill = value_NA)) + 
    geom_histogram() + 
    facet_wrap(~variable)


# Impute humidity and air temperature using wind_ew and wind_ns, and track missing values
ocean_imp_lm_wind <- oceanbuoys %>% 
    naniar::bind_shadow() %>%
    simputation::impute_lm(air_temp_c ~ wind_ew + wind_ns) %>% 
    simputation::impute_lm(humidity ~ wind_ew + wind_ns) %>%
    naniar::add_label_shadow()
    
# Plot the imputed values for air_temp_c and humidity, colored by missingness
ggplot(ocean_imp_lm_wind, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point()


# Bind the models together 
bound_models <- bind_rows(mean = ocean_imp_mean,
                          lm_wind = ocean_imp_lm_wind,
                          .id = "imp_model")

# Inspect the values of air_temp and humidity as a scatterplot
ggplot(bound_models, aes(x = air_temp_c, y = humidity, color = any_missing)) +
    geom_point() + 
    facet_wrap(~imp_model)


# Build a model adding year to the outcome
ocean_imp_lm_wind_year <- bind_shadow(oceanbuoys) %>%
    simputation::impute_lm(air_temp_c ~ wind_ew + wind_ns + year) %>%
    simputation::impute_lm(humidity ~ wind_ew + wind_ns + year) %>%
    naniar::add_label_shadow()

# Bind the mean, lm_wind, and lm_wind_year models together
bound_models <- bind_rows(mean = ocean_imp_mean,
                          lm_wind = ocean_imp_lm_wind,
                          lm_wind_year = ocean_imp_lm_wind_year,
                          .id = "imp_model"
                          )

# Explore air_temp and humidity, coloring by any missings, and faceting by imputation model
ggplot(bound_models, aes(x = air_temp_c, y = humidity, color = any_missing)) + 
    geom_point() + 
    facet_wrap(~imp_model)


# Gather the data and inspect the distributions of the values
bound_models_gather <- bound_models %>%
    select(air_temp_c, humidity, any_missing, imp_model) %>%
    gather(key = "key", value = "value", -any_missing, -imp_model)

# Inspect the distribution for each variable, for each model
ggplot(bound_models_gather, aes(x = imp_model, y = value, color = imp_model)) +
    geom_boxplot() + 
    facet_wrap(~key, scales = "free_y")

# Inspect the imputed values
bound_models_gather %>%
    filter(any_missing == "Missing") %>%
    ggplot(aes(x = imp_model, y = value, color = imp_model)) +
    geom_boxplot() + 
    facet_wrap(~key, scales = "free_y")


# Create an imputed dataset using a linear models
ocean_imp_lm_all <- naniar::bind_shadow(oceanbuoys) %>%
    naniar::add_label_shadow() %>%
    simputation::impute_lm(sea_temp_c ~ wind_ew + wind_ns + year + latitude + longitude) %>%
    simputation::impute_lm(air_temp_c ~ wind_ew + wind_ns + year + latitude + longitude) %>%
    simputation::impute_lm(humidity ~ wind_ew + wind_ns + year + latitude + longitude)

# Bind the datasets
bound_models <- bind_rows(imp_lm_wind_year = ocean_imp_lm_wind_year,
                          imp_lm_wind = ocean_imp_lm_wind,
                          imp_lm_all = ocean_imp_lm_all,
                          .id = "imp_model"
                          )
# Look at the models
bound_models


# Create the model summary for each dataset
model_summary <- bound_models %>% 
    group_by(imp_model) %>%
    nest() %>%
    mutate(mod = map(data, ~lm(sea_temp_c ~ air_temp_c + humidity + year, data = .)), 
           res = map(mod, residuals), pred = map(mod, predict), tidy = map(mod, broom::tidy)
           )

# Explore the coefficients in the model
model_summary %>% 
    select(imp_model, tidy) %>%
    unnest()

best_model <- "imp_lm_all"

```
  
  
  
***
  
### _Analyzing Election and Polling Data in R_  
  
Chapter 1 - Presidential Job Approval Polls  
  
Introduction:  
  
* Basic tools for organizing, analyzing, and visualizing polling data in R  
	* Data wrangling  
    * Prediction of election winners  
    * Mapping and regression at the county level  
    * Overall ensemble of exercises  
* Presidential approval polls are surveys of the public  
	* Tends to have a strong relationship with election outcomes  
    * Gallup dataset of approval polls since 1946  
* Can select and filter approval data in R  
	* library(tidyverse)  
    * data.slim <- data %>% select(variable_1, variable_2, ...)  
    * data.slim %>% filter(variable_1 == "observation)  
    * gallup %>% select(President, Date, Approve) %>% filter(President == "Trump")  
  
Averaging Job Approval by President:  
  
* Can group by variables and then take appropriate summaries  
	* data %>% group_by(variable)  
    * Gallup %>% group_by(President) %>% summarise(MeanApproval = mean(Approve))  
  
Visualizing Trump's Approval Over Time:  
  
* Can create averages of available polling and visualize over time  
	* Example of the RCP aggregate approval data poll  
* Can convert dates using lubridate  
	* library(lubridate)  
    * date <- ymd("2018-01-01")  
    * month(date) # Equal to 1  
    * month(date,label = T) # Equal to "Jan"  
* Creating a moving average is a simple but powerful technique - can use zoo and the rollmean() function, followed by ggplot2  
	* TrumpApproval %>% mutate(AvgApprove = rollmean(Approve, 10, na.pad=TRUE, align = "right"))  
  
Example code includes:  
```{r}

approval_polls <- readr::read_csv("./RInputFiles/gallup_approval_polls.csv")
glimpse(approval_polls)


# Select President, Date, and Approve from approval_polls
approval_polls %>% 
    select(President, Date, Approve) %>%
    head()


# Select the President, Date, and Approve columns and filter to observations where President is equal to "Trump"
approval_polls %>% 
    select(President, Date, Approve) %>%
    filter(President == "Trump")


# Group the approval_polls dataset by president and summarise a mean of the Approve column 
approval_polls %>%
    group_by(President) %>%
    summarise(Approve = mean(Approve))


# Extract, or "pull," the Approve column as a vector and save it to the object "TrumpApproval"
TrumpApproval <- approval_polls %>% 
    select(President, Date, Approve) %>%
    filter(President == "Trump") %>%
    pull(Approve)

# Take a mean of the TrumpApproval vector
mean(TrumpApproval)


# Select the relevant columns from the approval_polls dataset and filter them for the Trump presidency
TrumpPolls <- approval_polls %>% 
    select(President, Date, Approve) %>%
    filter(President == "Trump")
  
# Use the months() and mdy() function to get the month of the day each poll was taken
# Group the dataset by month and summarize a mean of Trump's job approval by month
TrumpPolls %>%
    mutate(Month = months(lubridate::mdy(Date))) %>%
    group_by(Month) %>%
    summarise(Approve = mean(Approve))


# Save Donald Trump's approval polling to a separate object 
TrumpApproval <- approval_polls %>% 
    filter(President == "Trump") %>%
    mutate(Date = lubridate::mdy(Date)) %>%
    arrange(Date) 


# use the rollmean() function from the zoo package to get a moving average of the last 10 polls
TrumpApproval <- TrumpApproval %>%
    mutate(AvgApprove = zoo::rollmean(Approve, 10, na.pad=TRUE, align = "right"))


# Use ggplot to graph Trump's average approval over time
ggplot(data = TrumpApproval, aes(x=Date, y=AvgApprove)) + 
    geom_line()


# Create an moving average of each president's approval rating
AllApproval <- approval_polls %>%
    group_by(President) %>%
    mutate(AvgApprove = zoo::rollmean(Approve, 10, na.pad=TRUE, align = "right"))


# Graph an moving average of each president's approval rating
ggplot(data = AllApproval, aes(x=Days, y=AvgApprove, col=President)) + 
    geom_line()

```
  
  
  
***
  
Chapter 2 - US House and Senate Polling  
  
Elections and Polling Parties:  
  
* Can use polling data to predict elections - "generic ballot"for which party is supported in general  
	* Data are a mix of publicly available data from pollingreport.com and RCP  
    * head(generic_ballot)  
    * ggplot(generic_ballot,aes(x=mdy(Date),y=Democrats)) + geom_point()  
* Can explore and wrangle the "generic ballot" dataset  
  
73 Years of "Generic Ballot" Polls:  
  
* Can analyze the data over time (D-R margin)  
	* data %>% group_by(year, month)  
    * data %>% group_by(year, month) %>% summarise(support = mean(support))  
    * ggplot(data,aes(x=month,y=support)) + geom_point() + geom_smooth(span=0.2)  
  
Calculating and Visualizing Error in Polls:  
  
* Polls have errors - domain knowledge is important in analyzing the estimates and methodologies revealed in polls  
	* Sometimes polls are systemically biased, particularly if similar methodologies are used by all companies  
* Can compare polling data and elections data  
	* poll_error <- generic_ballot %>% mutate(Democrats_Poll_Margin = Democrats - Republicans, Democrats_Vote_Margin = Democrats_vote - Republicans_vote)  
    * poll_error <- poll_error %>% group_by(Year) %>% summarise(Democrats_Poll_Margin = mean(Democrats_Poll_Margin), Democrats_Vote_Margin = mean(Democrats_Vote_Margin))  
    * poll_error %>% mutate(error = Dem.Poll.Margin - Dem.Vote.Margin)  
    * rmse <- sqrt(mean(poll_error$error^2))  
    * CI <- rmse * 1.96  
* May be helpful to visualize the errors over time  
	* ggplot(by_year) + geom_point(aes(x=ElecYear,y=Dem.Poll.Margin,col="Poll")) + geom_point(aes(x=ElecYear,y=Dem.Vote.Margin,col="Vote")) + geom_errorbar(aes(x=ElecYear,ymin=lower, ymax=upper))  
  
Predicting Winners with Linear Regression:  
  
* Can use linear regression to predict seats in Congress based on polls  
	* model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin, by_year)  
  
Example code includes:  
```{r}

generic_ballot <- readr::read_csv("./RInputFiles/generic_ballot.csv")
glimpse(generic_ballot)


# Look at the header and first few rows of the data
head(generic_ballot)

# Filter the election year to 2016 and select the Date, Democrats, and Republicans columns
generic_ballot %>%
    filter(ElecYear == 2016) %>%
    select(Date, Democrats, Republicans)


# Mutate a new variable called "Democratic.Margin" equal to the difference between Democrats' vote share and Republicans'
democratic_lead <- generic_ballot %>%
    mutate(Democratic.Margin = Democrats - Republicans)

# Take a look at that new variable!
democratic_lead %>%
    select(Democratic.Margin)


# Group the generic ballot dataset by year and summarise an average of the Democratic.Margin variable
over_time <- democratic_lead %>% 
    group_by(ElecYear) %>%
    summarize(Democratic.Margin = mean(Democratic.Margin))

# Explore the data.frame
head(over_time)


# Create a month and year variable for averaging polls by approximate date
timeseries <- democratic_lead %>%
    mutate(Date = lubridate::mdy(Date), month = lubridate::month(Date), yr = lubridate::year(Date))

# Now group the polls by their month and year, then summarise
timeseries <- timeseries %>%
    group_by(yr, month) %>%
    summarise(Democratic.Margin = mean(Democratic.Margin))


# Mutate a new variable to use a date summary for the monthly average
timeseries_plot <- timeseries %>%
    mutate(time = sprintf("%s-%s-%s", yr, month, "01"))

# Plot the line over time
ggplot(timeseries_plot, aes(x=lubridate::ymd(time), y=Democratic.Margin)) +
    geom_line()


# Make a ggplot with points for monthly polling averages and one trend line running through the entire time series
ggplot(timeseries_plot, aes(x=lubridate::ymd(time), y=Democratic.Margin)) +
    geom_point() + 
    geom_smooth(span=0.2)


# Mutate two variables for the Democrats' margin in polls and election day votes
poll_error <- generic_ballot %>%
    mutate(Dem.Poll.Margin = Democrats - Republicans,
           Dem.Vote.Margin = DemVote - RepVote
           )

# Average those two variables per year and mutate the "error" variable
poll_error <- poll_error %>%
    group_by(ElecYear) %>%
    summarise(Dem.Poll.Margin = mean(Dem.Poll.Margin), Dem.Vote.Margin = mean(Dem.Vote.Margin)) %>%
    mutate(error = Dem.Poll.Margin - Dem.Vote.Margin)

# Calculate the room-mean-square error of the error variable
rmse <- sqrt(mean(poll_error$error^2))

# Multiply the RMSE by 1.96 to get the 95% confidence interval, or "margin of error"
CI <- rmse * 1.96

# Add variables to our dataset for the upper and lower bound of the `Dem.Poll.Margin` variable
by_year <- poll_error %>%
    mutate(upper = Dem.Poll.Margin + CI, lower = Dem.Poll.Margin - CI)


# Plot estimates for Dem.Poll.Margin and Dem.Vote.Margin on the y axis for each year on the x axis with geom_point
ggplot(by_year) + 
    geom_point(aes(x=ElecYear, y=Dem.Poll.Margin, col="Poll")) +
    geom_point(aes(x=ElecYear, y=Dem.Vote.Margin, col="Vote")) +
    geom_errorbar(aes(x=ElecYear, ymin=lower, ymax=upper))


# Fit a model predicting Democratic vote margin with Democratic poll margin
model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin, data=by_year)
  
# Evaluate the model
summary(model)


# Make a new data.frame that has our prediction variable and value
predictdata <- data.frame("Dem.Poll.Margin" = 5)

# Make the prediction with the coefficients from our model
predict(model, predictdata)


```
  
  
  
***
  
Chapter 3 - Election Results and Political Demography  
  
2016 Presidential Election:  
  
* County-level results are made available by secretaries of state  
	* Can be combined with census data to draw findings - for example, is there a correlation between race and vote share by county?  
* Data are available in the chloroplethr package  
	* county_merged <- left_join(df_county_demographics, uspres_county, by = "county.fips")  
    * ggplot(county_merged, aes(x=percent_white,y=Dem.pct)) + geom_point()  
    * ggplot(county_merged, aes(x=percent_white,y=Dem.pct)) + geom_point() + geom_smooth(method="lm")  
  
Making County-Level Maps in R:  
  
* Mapping can be helpful for identifying trends and areas of interest in the data - a few options include  
	* choroplethr  
    * geom_sf()  
    * leaflet  
* The choroplethr package allows for easy creation of maps with minimal pre-processing  
	* library(choroplethr)  
    * county_map <- county_merged %>% dplyr::rename("region" = county.fips, "value" = Dem.pct)  # names need to be region and value  
    * county_choropleth(county_map)  
  
Analyzing Results with Linear Regression:  
  
* Can further analyze findings using linear regression; both understanding past results and predicting future  
	* fit <- lm(Dem.pct ~ percent_white, data=county_merged)  
    * summary(fit)  
  
2016 Brexit Referendum:  
  
* Data wrangling, modeling, and visualization of 2016 Brexit vote  
* Can either run a short-term average, or a LOESS with various windows  
	* head(brexit_polls)  
    * ggplot(brexit_polls, aes(x = mdy(Date), y = Remain - Leave)) + geom_point() + geom_smooth(method = 'loess')  
  
Example code includes:  
```{r}

uspres_results <- readr::read_csv("./RInputFiles/us_pres_2016_by_county.csv")
glimpse(uspres_results)


# Deselect the is.national.winner, national.count, and national.party.percent variables
uspres_results.slim <- uspres_results %>%
    select(-c(is.national.winner, national.count, national.party.percent))


# Spread party and votes to their own columns
uspres_county <- uspres_results.slim %>%
    tidyr::spread(key=party,value=vote.count)

# Add a variable to the uspres_county dataset to store the Democrat's percentage of votes
uspres_county <- uspres_county %>%
    mutate(Dem.pct = D/county.total.count)


# Load the county demographic data
data(df_county_demographics, package="choroplethr")

# Look at the demographic data
head(df_county_demographics)


# Rename the 'region' variable in df_county_demographics to "county.fips"
df_county_demographics <- df_county_demographics %>%
    rename("county.fips" = region)

# Join county demographic with vote share data via its FIPS code
county_merged <- left_join(df_county_demographics, uspres_county, by = "county.fips")
head(county_merged)


# plot percent_white and Dem.pct on the x and y axes. add points and a trend line
ggplot(county_merged, aes(x=percent_white, y=Dem.pct)) +
    geom_point() +
    geom_smooth(method="lm")


# Rename the county.fips and Dem.pct variables from our dataset to "region" and "value"
county_map <- county_merged %>%
    rename("region" = county.fips, "value" = Dem.pct)

# Create the map with choroplethrMaps's county_choropleth()
democratic_map <- choroplethr::county_choropleth(county_map)

# Print the map
democratic_map


# Rename variables from our dataset
county_map <- county_merged %>%
    rename("region" = county.fips, "value" = percent_white)

# Create the map with choroplethr's county_choropleth()
white_map <- choroplethr::county_choropleth(county_map)

# Graph the two maps (democratic_map and white_map) from the previous exercises side-by-side
gridExtra::grid.arrange(democratic_map, white_map)


# Fit a linear model to predict Dem.pct dependent on percent_white in each county
fit <- lm(Dem.pct ~ percent_white, data=county_merged)

# Evaluate the model
summary(fit)


# Fit a linear model to predict Dem.pct dependent on percent_white and per_capita_income in each county
fit <- lm(Dem.pct ~ percent_white + per_capita_income, data=county_merged)

# Evaluate the model
summary(fit)


brexit_polls <- readr::read_csv("./RInputFiles/brexit_polls.csv")
glimpse(brexit_polls)

brexit_results <- readr::read_csv("./RInputFiles/brexit_results.csv")
glimpse(brexit_results)


# Filter the dataset to polls only released after June 16th, 2016, and mutate a variable for the Remain campaign's lead
brexit_average <- brexit_polls %>%
    filter(lubridate::mdy(Date)>lubridate::ymd("2016-06-16") )%>%
    mutate(RemainLead = Remain - Leave)  

# Average the last seven days of polling
mean(brexit_average$RemainLead)

# Summarise the Remain lead from the entire month of the referendum 
ggplot(brexit_polls, aes(x=lubridate::mdy(Date), y=Remain-Leave)) +
    geom_point() + 
    geom_smooth(method='loess')


# Familiarize yourself with the data using the head() function
head(brexit_results)

# Chart the counstituency-by-constituency relationship between voting for the Labour Party and voting to leave the EU
ggplot(brexit_results,aes(x=lab_2015, y=leave_share)) + 
  geom_point()


# Show the relationship between UKIP and Leave vote share with points and a line representing the linear relationship between the variables
ggplot(brexit_results,aes(x=ukip_2015, y=leave_share)) + 
    geom_point() +
    geom_smooth(method = "lm")


# predict leave's share with the percentage of a constituency that holds a college degree and its 2015 UKIP vote share
model.multivar <- lm(leave_share ~ ukip_2015 + degree, brexit_results)
summary(model.multivar)

```
  
  
  
***
  
Chapter 4 - Predicting the Future of Politics  
  
US House 2018:  
  
* Can make predictions based on publicly available polling data and also make predictions using past data to evaluate general error rates  
	* polls_2018 %>% filter(date > "2018-06-01")  
    * polls_2018 %>% mutate(Dem.Margin = Dem - Rep)  
    * polls_2018 %>% pull(Dem.Margin)  
    * mean(polls_2018$Dem.Margin)  
* Can also extend with group_by() function  
  
Training a Model to Predict Future with Polls:  
  
* Can start with the base model  
	* lm(Dem.Vote.Margin ~ Dem.Poll.Margin)  
* There may be additional variables however; perhaps party in office systemically leads to over/under performance of polls  
	* ggplot(generic_ballot,aes(x=Dem.Poll.Margin,y=Dem.Vote.Margin, col=party_in_power) + geom_text(aes(label=ElecYear)) + geom_smooth(method='lm')  
    * model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin + party_in_power, data=polls_predict)  
    * predict(model, data.frame(Dem.Poll.Margin = 8, party_in_power=-1))  
* May want to extend with the predictive margin of error  
	* sqrt(mean(c(model$fitted.values - data$actual_results)^2)) * 1.96  
    * sqrt(mean(c(model$fitted.values - polls_predict$Dem.Vote.Margin)^2)) *1.96  
  
Presidency in 2020:  
  
* Presidential elections have many moving parts and with a small sample size of data  
	* Popular vote - nationwide tally  
    * Electoral vote - tally by states (weighted by size of state)  
* Popular vote can be modeled based on approval rating, economic growth, and length of party incumbency  
	* lm(vote_share ~ pres_approve + q2_gdp + two_plus_terms, pres_elecs)  
    * ggplot(pres_elecs,aes(x=predict,y=vote_share,label=Year)) + geom_abline() + geom_text()  
    * sqrt(mean(c(pres_elecs$predict-pres_elecs$vote_share)^2)) * 1.96  
  
Wrap-up:  
  
* Approval polls - wrangling and visualizing data  
* Polls and linear regression  
* Mapping election results and running multi-regressions  
* Prediction and applied examples  
  
Example code includes:  
```{r}

polls_2018 <- tibble::tibble(Democrat=c(45, 47, 49, 44, 41, 48, 45, 44, 45, 51, 42, 52, 46, 44, 41, 42, 44, 42, 44, 44, 42, 41, 51, 47, 49, 45, 44, 45, 48, 42, 42, 47, 42, 44, 47, 43, 50, 43, 43, 41, 45, 44, 44, 42, 45, 50, 48, 48, 43, 45, 48, 46, 48, 44, 43, 44, 49, 42, 39, 42, 44, 43, 40, 42, 42, 38, 43, 44, 39, 42, 47, 42, 43, 43, 48, 49, 46, 43, 43, 45, 44, 43, 44, 44, 47, 44, 38, 42, 43, 43, 41, 41, 42, 42, 50, 50, 44, 46, 44, 40, 42, 43, 38, 43, 49, 43, 38, 50, 44, 40, 37, 41, 47, 54, 46, 43, 38, 42, 39, 38, 49, 49, 43, 38, 42, 45, 47, 42, 37, 41, 38, 43, 42, 51, 51, 42, 37, 41, 53, 46, 44, 40, 44, 42, 38, 44, 44, 39, 44, 56, 51, 44, 51, 37, 41, 50, 42, 37, 40, 41, 36, 42, 37, 42, 43, 43, 42, 38, 44, 51, 40, 38, 38, 51, 51, 39, 40, 43, 44, 50, 40, 36, 42, 41, 42, 54, 40, 43, 39, 41, 40, 48, 42, 49, 39, 43, 40, 40, 39, 43, 40, 40, 39, 49, 41, 46, 41, 40, 47, 39, 51, 43, 39, 44, 40, 40, 40, 50, 42, 39, 43, 37, 43, 47, 41, 48, 42, 38, 43, 38, 42, 50, 41, 42, 39, 43, 38, 41, 40, 42, 49, 42, 40, 42, 38, 41, 47, 39, 50, 40, 47, 47, 38, 40, 45, 40, 43, 41, 48, 47, 46, 46, 49, 45, 48), 
                             Republican=c(39, 34, 38, 38, 37, 43, 36, 40, 36, 42, 38, 41, 39, 41, 39, 36, 40, 36, 37, 38, 37, 38, 39, 40, 37, 37, 35, 37, 40, 38, 37, 34, 38, 34, 37, 40, 41, 35, 38, 38, 36, 37, 37, 36, 39, 42, 43, 41, 37, 35, 43, 40, 39, 38, 32, 37, 42, 39, 37, 37, 37, 38, 37, 36, 37, 37, 38, 35, 38, 35, 44, 39, 37, 34, 43, 41, 38, 34, 38, 34, 35, 38, 34, 39, 43, 36, 30, 36, 36, 34, 37, 37, 35, 35, 39, 44, 39, 41, 38, 37, 38, 38, 30, 36, 40, 37, 31, 41, 37, 38, 31, 39, 32, 38, 39, 35, 30, 38, 30, 39, 41, 38, 37, 31, 38, 39, 45, 37, 31, 37, 30, 37, 37, 39, 41, 36, 32, 38, 39, 40, 37, 31, 36, 36, 31, 36, 35, 27, 34, 38, 40, 36, 36, 29, 36, 37, 36, 31, 38, 35, 30, 36, 28, 33, 36, 40, 34, 31, 36, 36, 33, 30, 39, 40, 40, 36, 31, 38, 37, 35, 33, 28, 36, 33, 37, 38, 33, 37, 32, 39, 34, 37, 38, 43, 33, 37, 31, 37, 33, 38, 32, 36, 33, 35, 36, 40, 33, 38, 40, 34, 42, 36, 34, 37, 34, 40, 34, 40, 40, 32, 40, 35, 39, 41, 35, 38, 40, 35, 37, 36, 39, 40, 37, 39, 33, 39, 36, 37, 33, 35, 38, 37, 35, 36, 35, 41, 42, 34, 41, 44, 42, 41, 32, 40, 38, 37, 40, 39, 43, 38, 41, 43, 41, 42, 40), 
                             end_date=as.Date(c('2018-08-28', '2018-08-28', '2018-08-21', '2018-08-21', '2018-08-21', '2018-08-19', '2018-08-18', '2018-08-14', '2018-08-14', '2018-08-13', '2018-08-12', '2018-08-12', '2018-08-12', '2018-08-07', '2018-08-07', '2018-08-06', '2018-07-31', '2018-07-31', '2018-07-30', '2018-07-24', '2018-07-24', '2018-07-23', '2018-07-23', '2018-07-22', '2018-07-22', '2018-07-17', '2018-07-17', '2018-07-14', '2018-07-11', '2018-07-10', '2018-07-10', '2018-07-10', '2018-07-03', '2018-07-02', '2018-07-02', '2018-07-01', '2018-07-01', '2018-06-29', '2018-06-26', '2018-06-26', '2018-06-25', '2018-06-24', '2018-06-19', '2018-06-19', '2018-06-18', '2018-06-17', '2018-06-17', '2018-06-13', '2018-06-12', '2018-06-12', '2018-06-12', '2018-06-10', '2018-06-06', '2018-06-05', '2018-06-05', '2018-06-04', '2018-05-30', '2018-05-29', '2018-05-29', '2018-05-29', '2018-05-22', '2018-05-22', '2018-05-22', '2018-05-19', '2018-05-15', '2018-05-15', '2018-05-14', '2018-05-08', '2018-05-08', '2018-05-07', '2018-05-05', '2018-05-01', '2018-05-01', '2018-05-01', '2018-05-01', '2018-04-30', '2018-04-30', '2018-04-24', '2018-04-24', '2018-04-24', '2018-04-23', '2018-04-17', '2018-04-17', '2018-04-13', '2018-04-11', '2018-04-10', '2018-04-10', '2018-04-07', '2018-04-03', '2018-04-03', '2018-04-01', '2018-03-27', '2018-03-27', '2018-03-27', '2018-03-25', '2018-03-25', '2018-03-21', '2018-03-21', '2018-03-20', '2018-03-20', '2018-03-19', '2018-03-13', '2018-03-13', '2018-03-12', '2018-03-08', '2018-03-06', '2018-03-06', '2018-03-05', '2018-03-05', '2018-02-27', '2018-02-27', '2018-02-26', '2018-02-24', '2018-02-23', '2018-02-21', '2018-02-20', '2018-02-20', '2018-02-13', '2018-02-13', '2018-02-12', '2018-02-11', '2018-02-07', '2018-02-06', '2018-02-06', '2018-02-04', '2018-02-01', '2018-01-30', '2018-01-30', '2018-01-30', '2018-01-23', '2018-01-23', '2018-01-21', '2018-01-20', '2018-01-18', '2018-01-18', '2018-01-16', '2018-01-16', '2018-01-16', '2018-01-15', '2018-01-10', '2018-01-09', '2018-01-09', '2018-01-05', '2018-01-02', '2018-01-02', '2017-12-26', '2017-12-19', '2017-12-19', '2017-12-18', '2017-12-17', '2017-12-12', '2017-12-12', '2017-12-12', '2017-12-12', '2017-12-11', '2017-12-07', '2017-12-05', '2017-12-05', '2017-12-03', '2017-11-28', '2017-11-28', '2017-11-25', '2017-11-22', '2017-11-21', '2017-11-19', '2017-11-15', '2017-11-14', '2017-11-14', '2017-11-11', '2017-11-09', '2017-11-07', '2017-11-07', '2017-11-06', '2017-11-05', '2017-11-01', '2017-10-31', '2017-10-31', '2017-10-30', '2017-10-30', '2017-10-24', '2017-10-24', '2017-10-24', '2017-10-23', '2017-10-16', '2017-10-16', '2017-10-15', '2017-10-10', '2017-10-09', '2017-10-03', '2017-10-01', '2017-09-26', '2017-09-25', '2017-09-24', '2017-09-20', '2017-09-19', '2017-09-17', '2017-09-12', '2017-09-11', '2017-09-05', '2017-09-03', '2017-08-29', '2017-08-28', '2017-08-22', '2017-08-21', '2017-08-19', '2017-08-17', '2017-08-15', '2017-08-14', '2017-08-12', '2017-08-08', '2017-08-06', '2017-08-06', '2017-08-01', '2017-07-29', '2017-07-25', '2017-07-24', '2017-07-18', '2017-07-17', '2017-07-15', '2017-07-11', '2017-07-09', '2017-07-04', '2017-06-30', '2017-06-27', '2017-06-27', '2017-06-25', '2017-06-24', '2017-06-20', '2017-06-19', '2017-06-13', '2017-06-12', '2017-06-11', '2017-06-06', '2017-06-02', '2017-05-30', '2017-05-30', '2017-05-23', '2017-05-22', '2017-05-16', '2017-05-14', '2017-05-14', '2017-05-11', '2017-05-09', '2017-05-06', '2017-05-02', '2017-04-30', '2017-04-25', '2017-04-25', '2017-04-25', '2017-04-24', '2017-04-20', '2017-04-18', '2017-04-18', '2017-04-15', '2017-04-12', '2017-04-11', '2017-04-09', '2017-04-01', '2017-03-28', '2017-03-27', '2017-03-12', '2017-02-22', '2017-02-08', '2017-01-31', '2017-01-24'))
                             )

# average all of the generic ballot polls that have been taken since June
polls_2018 %>% 
    filter(lubridate::month(end_date) > 6, lubridate::year(end_date)==2018) %>% 
    mutate(Dem.Margin = Democrat - Republican) %>%
    pull(Dem.Margin) %>% 
    mean()


# Filter the dataset to include polls from August and September
# Mutate a variable for the Democratic vote margin in that year
polls_predict <- generic_ballot %>%
    filter(lubridate::month(lubridate::mdy(Date)) %in% c(8, 9), ElecYear >= 1980) %>%
    mutate(Dem.Poll.Margin = Democrats - Republicans,
           Dem.Vote.Margin = DemVote - RepVote
           ) %>%
    group_by(ElecYear) %>%
    summarise(Dem.Poll.Margin = mean(Dem.Poll.Margin),
              Dem.Vote.Margin = mean(Dem.Vote.Margin)
              ) %>%
    arrange(ElecYear) %>%
    mutate(error=Dem.Poll.Margin - Dem.Vote.Margin, 
           party_in_power=c(-1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1)
           )


# Fit a model to predict Democrats' November vote margin with the Democratic poll margin and party in power variable
model <- lm(Dem.Vote.Margin ~ Dem.Poll.Margin + party_in_power, data=polls_predict)

# Evaluate the model
summary(model)


# Make a prediction for November if Democrats are up 7.5 points in the generic ballot and the party_in_power is the Republicans (-1)
predict(model, data.frame(Dem.Poll.Margin = 7.5, party_in_power=-1))

# Multiply the root-mean-square error by 1.96
sqrt(mean(c(model$fitted.values - polls_predict$Dem.Vote.Margin)^2)) * 1.96


pres_elecs <- tibble::tibble(Year=c(2016, 2012, 2008, 2004, 2000, 1996, 1992, 1988, 1984, 1980, 1976, 1972, 1968, 1964, 1960, 1956, 1952, 1948), 
                             q2_gdp=c(2.3, 1.3, 1.3, 2.6, 8, 7.1, 4.3, 5.2, 7.1, -7.9, 3, 9.8, 7, 4.7, -1.9, 3.2, 0.4, 7.5), 
                             pres_approve=c(7, -0.8, -37, -0.5, 19.5, 15.5, -18, 10, 20, -21.7, 5, 26, -5, 60.3, 37, 53.5, -27, -6), 
                             two_plus_terms=c(1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1), 
                             vote_share=c(51.1, 51.96, 46.3, 51.2, 50.3, 54.7, 46.5, 53.9, 59.2, 44.7, 48.9, 61.8, 49.6, 61.3, 49.9, 57.8, 44.5, 52.4)
                             )
pres_elecs


#  Make a plot with points representing a year's presidential approval and vote share and a line running through them to show the linear relationship
ggplot(pres_elecs, aes(x=pres_approve, y=vote_share, label=Year)) + 
    geom_text() + 
    geom_smooth(method='lm')


# Make a model that predict the vote_share variable with pres_approve, q2_gdp, and two_plus_terms
fit <- lm(vote_share ~ pres_approve + q2_gdp + two_plus_terms, pres_elecs)

# Evaluate the model
summary(fit)


# Save the predicted vote shares to a variable called predict
pres_elecs$predict <- predict(fit, pres_elecs)

# Graph the predictions and vote shares with a label for each election year
ggplot(pres_elecs,aes(x=predict, y=vote_share, label=Year)) + 
    geom_abline() +
    geom_text()

# Calculate the model's root-mean-square error
sqrt(mean(c(pres_elecs$predict - pres_elecs$vote_share)^2)) * 1.96

# Make a prediction for hypothetical data
predict(fit, data.frame(pres_approve=-15, q2_gdp=2, two_plus_terms=0))

```
  
  
  
***
  
### _Analyzing US Census Data in R_  
  
Chapter 1 - Census Data in R with tidycensus  
  
Overview:  
  
* Acquiring census and ACS data through tidycensus, wrangling with tidyverse, acquiring boundary data with tigris, and visualizing with ggplot2  
* The dicennial (every 10 years) census is a full count of the population  
	* ACS is a detailed study of 3 million households per year  
* Need to acquire a census API key and provide to the tidycensus function  
	* state_pop <- get_decennial(geography = "state", variables = "P001001")  # geography is the level of aggregation and variables are the desired variables  
    * state_income <- get_acs(geography = "state", variables = "B19013_001")  # default dataset is the 5-year 2012-2016 data; returns both an estimate and an extra column "moe" (margin of error)  
  
Basic tidycensus functionality:  
  
* Can acquire census data by legal entities (county, state) or statistical entities (census tracks)  
	* geography = "county"  
    * geography = "tract"  
    * county_income <- get_acs(geography = "county", variables = "B19013_001")  
* Can request the variable(s) of interest  
* Can further subset the data to only include states or counties of interest  
	* texas_income <- get_acs(geography = "county", variables = c(hhincome = "B19013_001"), state = "TX")  
* By default, census data are returned in tidy format, though wide (spread) data may be desirable at times  
	* get_acs(geography = "county", variables = c(hhincome = "B19013_001", medage = "B01002_001"), state = "TX", output = "wide")  
  
Searching for data with tidycensus:  
  
* There are 1000s of variables across ACS and the census, so it can be challenging to find variables of interest  
	* https://censusreporter.org/  
* Builit-in search capability in tidycensus  
	* v16 <- load_variables(year = 2016, dataset = "acs5", cache = TRUE)   # year is the end year, so 2012-2016; cache=TRUE will store the dataset locally for future browsing  
* The ACS variable structure, as shown using B19001_002E (income < 10k dollars)  
	* B means "base", while C means "collapse", P means "profile", and S means "subject"  
    * The 19001 is the table ID (household income)  
    * The 002 is a specific variable  
    * The E means that it is an estimate (tidycensus by default returns both error and margin of error)  
  
Visualizing census data with ggplot2:  
  
* Can plot census data using ggplot2  
	* ne_income <- get_acs(geography = "state", variables = "B19013_001", survey = "acs1", state = c("ME", "NH", "VT", "MA", "RI", "CT", "NY"))  
    * ggplot(ne_income, aes(x = estimate, y = NAME)) + geom_point()  
* Can improve plots using sorting and formatting  
	* ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + geom_point(color = "navy", size = 4) + scale_x_continuous(labels = scales::dollar) + theme_minimal(base_size = 14) + labs(x = "2016 ACS estimate", y = "", title = "Median household income by state")  
  
Example code includes:  
```{r eval=FALSE}

# Load the tidycensus package into your R session
library(tidycensus)

# Define your Census API key and set it with census_api_key()
api_key <- "INSERT HERE"
census_api_key(api_key)

# Check your API key
Sys.getenv("CENSUS_API_KEY")


# Obtain and view state populations from the 2010 US Census
state_pop <- get_decennial(geography = "state", variables = "P001001")

head(state_pop)

# Obtain and view state median household income from the 2012-2016 American Community Survey
state_income <- get_acs(geography = "state", variables = "B19013_001")

head(state_income)


# Get an ACS dataset for Census tracts in Texas by setting the state
tx_income <- get_acs(geography = "tract",
                     variables = "B19013_001",
                     state = "TX")

# Inspect the dataset
head(tx_income)

# Get an ACS dataset for Census tracts in Travis County, TX
travis_income <- get_acs(geography = "tract",
                         variables = "B19013_001", 
                         state = "TX",
                         county = "Travis")

# Inspect the dataset
head(travis_income)

# Supply custom variable names
travis_income2 <- get_acs(geography = "tract", 
                          variables = c(hhincome = "B19013_001"), 
                          state = "TX",
                          county = "Travis")

# Inspect the dataset
head(travis_income2)


# Return county data in wide format
or_wide <- get_acs(geography = "county", state = "OR",
                   variables = c(hhincome = "B19013_001", medage = "B01002_001"), 
                   output = "wide"
                   )

# Compare output to the tidy format from previous exercises
head(or_wide)

# Create a scatterplot
plot(or_wide$hhincomeE, or_wide$medageE)


# Load variables from the 2012-2016 ACS
v16 <- load_variables(year = 2016, dataset = "acs5", cache = TRUE)

# Get variables from the ACS Data Profile
v16p <- load_variables(year = 2016, dataset = "acs5/profile", cache = TRUE)

# Set year and dataset to get variables from the 2000 Census SF3
v00 <- load_variables(year = 2000, dataset = "sf3", cache = TRUE)


# Filter for table B19001
filter(v16, str_detect(name, "B19001"))

# Use public transportation to search for related variables
filter(v16p, str_detect(label, fixed("public transportation", ignore_case = TRUE)))


# Access the 1-year ACS  with the survey parameter
ne_income <- get_acs(geography = "state",
                     variables = "B19013_001", 
                     survey = "acs1", 
                     state = c("ME", "NH", "VT", "MA", 
                               "RI", "CT", "NY"))

# Create a dot plot
ggplot(ne_income, aes(x = estimate, y = NAME)) + 
  geom_point()

# Reorder the states in descending order of estimates
ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + 
  geom_point()


# Set dot color and size
g_color <- ggplot(ne_income, aes(x = estimate, y = reorder(NAME, estimate))) + 
  geom_point(color = "navy", size = 4)

# Format the x-axis labels
g_scale <- g_color + 
  scale_x_continuous(labels = scales::dollar) + 
  theme_minimal(base_size = 18) 

# Label your x-axis, y-axis, and title your chart
g_label <- g_scale + 
  labs(x = "2016 ACS estimate", y = "", title = "Median household income by state")
  
g_label

```
  
  
  
***
  
Chapter 2 - Wrangling US Census Data  
  
Tables and summary variables in tidycensus:  
  
* Can grab all of the variables from a table at once using the table= option  
	* wa_income <- get_acs(geography = "county", state = "WA", table = "B19001")  
* Can normalize based on denominators  
	* race_vars <- c(White = "B03002_003", Black = "B03002_004", Native = "B03002_005", Asian = "B03002_006", HIPI = "B03002_007", Hispanic = "B03002_012")  
    * tx_race <- get_acs(geography = "county", state = "TX", variables = race_vars, summary_var = "B03002_001")  
    * tx_race_pct <- tx_race %>% mutate(pct = 100 * (estimate / summary_est)) %>% select(NAME, variable, pct)  
  
Census data wrangling with tidy tools:  
  
* Can use the tidyverse to interact with data that has been retrieved from ACS and the census  
* The split-apply-combine approach to analysis is commonly used  
	* tx_largest <- tx_race %>% group_by(GEOID) %>% filter(estimate == max(estimate)) %>% select(NAME, variable, estimate)  
    * tx_largest %>% group_by(variable) %>% tally()  
* Can recode variables for group-wise analysis using case_when()  
	* wa_grouped <- wa_income %>% filter(variable != "B19001_001") %>% mutate(incgroup = case_when( variable < "B19001_008" ~ "below35k", variable < "B19001_013" ~ "35kto75k", TRUE ~ "above75k")) %>% group_by(NAME, incgroup) %>% summarize(group_est = sum(estimate))  
* Can use purrr to gather data from multiple years and then combine  
	* mi_cities <- map_df(2012:2016, function(x) { get_acs(geography = "place", variables = c(totalpop = "B01003_001"), state = "MI", survey = "acs1", year = x) %>% mutate(year = x) })  
  
Working with margins of error in tidycensus:  
  
* Margins of error exist in the ACS data since it is an annual survey of ~3 million; the MOE is the value needed to obtain the 95% CI  
	* get_acs(geography = "county", variables = c(median_age = "B01002_001"), state = "OR")  
* With sparse data (small geographies or niche queries), moe can be a meaningful percentage of the estimate, even exceeding it at times  
	* vt_eldpov <- get_acs(geography = "tract", variables = c(eldpovm = "B17001_016", eldpovf = "B17001_030"), state = "VT")  
* There are multiple tidycensus functions for calculating margin of error  
	* moe_sum(): MOE for a derived sum  
    * moe_product(): MOE for a derived product  
    * moe_ratio(): MOE for a derived ratio  
    * moe_prop(): MOE for a derived proportion  
    * vt_eldpov2 <- vt_eldpov %>% group_by(GEOID) %>% summarize( estmf = sum(estimate), moemf = moe_sum(moe = moe, estimate = estimate) )  
  
Visualizing margins of error from ACS:  
  
* Can use an error-bar plot to show the range of uncertainty - geom_errorbar() and geom_errorbarh()  
	* wyoming_age <- get_acs(geography = "county", variables = c(medianage = "B01002_001"), state = "WY")  
    * ggplot(wyoming_age, aes(x = estimate, y = NAME)) + geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + geom_point()  
* Can clean up visualizations - remove redundancies in names and order values  
	* wyoming_age2 <- wyoming_age %>% mutate(NAME = str_replace(NAME, " County, Wyoming", ""))  
    * ggplot(wyoming_age2, aes(x = estimate, y = reorder(NAME, estimate))) + geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + geom_point(size = 3, color = "darkgreen") + theme_grey(base_size = 14) + labs(title = "Median age, counties in Wyoming", subtitle = "2012-2016 American Community Survey", x = "ACS estimate (bars represent margins of error)", y = "")  
  
Example code includes:  
```{r eval=FALSE}

library(tidycensus)

# Download table "B19001"
wa_income <- get_acs(geography = "county", 
                 state = "WA", 
                 table = "B19001")

# Check out the first few rows of wa_income
head(wa_income)


# Assign Census variables vector to race_vars
race_vars <- c(White = "B03002_003", Black = "B03002_004", Native = "B03002_005", 
               Asian = "B03002_006", HIPI = "B03002_007", Hispanic = "B03002_012"
               )

# Request a summary variable from the ACS
ca_race <- get_acs(geography = "county", 
                   state = "CA",
                   variables = race_vars, 
                   summary_var = "B03002_001")

# Calculate a new percentage column and check the result
ca_race_pct <- ca_race %>%
  mutate(pct = 100 * (estimate / summary_est))

head(ca_race_pct)


# Group the dataset and filter the estimate
ca_largest <- ca_race %>%
  group_by(GEOID) %>%
  filter(estimate == max(estimate)) 

head(ca_largest)

# Group the dataset and get a breakdown of the results
ca_largest %>% 
  group_by(variable) %>%
  tally()


# Use a tidy workflow to wrangle ACS data
wa_grouped <- wa_income %>%
  filter(variable != "B19001_001") %>%
  mutate(incgroup = case_when(
    variable < "B19001_008" ~ "below35k", 
    variable < "B19001_013" ~ "35kto75k", 
    TRUE ~ "above75k"
  )) %>%
  group_by(NAME, incgroup) %>%
  summarize(group_est = sum(estimate))

wa_grouped


# Map through ACS1 estimates to see how they change through the years
mi_cities <- map_df(2012:2016, function(x) {
  get_acs(geography = "place", 
          variables = c(totalpop = "B01003_001"), 
          state = "MI", 
          survey = "acs1", 
          year = x) %>%
    mutate(year = x)
})

mi_cities %>% arrange(NAME, year)


# Get data on elderly poverty by Census tract in Vermont
vt_eldpov <- get_acs(geography = "tract", 
                     variables = c(eldpovm = "B17001_016", 
                                   eldpovf = "B17001_030"), 
                     state = "VT")

vt_eldpov

# Identify rows with greater margins of error than their estimates
moe_check <- filter(vt_eldpov, moe > estimate)

# Check proportion of rows where the margin of error exceeds the estimate
nrow(moe_check) / nrow(vt_eldpov)


# Calculate a margin of error for a sum
moe_sum(moe = c(55, 33, 44, 12, 4))

# Calculate a margin of error for a product
moe_product(est1 = 55, est2 = 33, moe1 = 12, moe2 = 9)

# Calculate a margin of error for a ratio
moe_ratio(num = 1000, denom = 950, moe_num = 200, moe_denom = 177)

# Calculate a margin of error for a proportion
moe_prop(num = 374, denom = 1200, moe_num = 122, moe_denom = 333)


# Group the dataset and calculate a derived margin of error
vt_eldpov2 <- vt_eldpov %>%
  group_by(GEOID) %>%
  summarize(
    estmf = sum(estimate), 
    moemf = moe_sum(moe = moe, estimate = estimate)
  )

# Filter rows where newly-derived margin of error exceeds newly-derived estimate
moe_check2 <- filter(vt_eldpov2, moemf > estmf)

# Check proportion of rows where margin of error exceeds estimate
nrow(moe_check2) / nrow(vt_eldpov2)


# Request median household income data
maine_inc <- get_acs(geography = "county", 
                     variables = c(hhincome = "B19013_001"), 
                     state = "ME") 

# Generate horizontal error bars with dots
ggplot(maine_inc, aes(x = estimate, y = NAME)) + 
  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + 
  geom_point()


# Remove unnecessary content from the county's name
maine_inc2 <- maine_inc %>%
  mutate(NAME = str_replace(NAME, " County, Maine", ""))

# Build a margin of error plot incorporating your modifications
ggplot(maine_inc2, aes(x = estimate, y = reorder(NAME, estimate))) + 
  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) + 
  geom_point(size = 3, color = "darkgreen") + 
  theme_grey(base_size = 14) + 
  labs(title = "Median household income", 
       subtitle = "Counties in Maine", 
       x = "ACS estimate (bars represent margins of error)", 
       y = "") + 
  scale_x_continuous(labels = scales::dollar)

```
  
  
  
***
  
Chapter 3 - US Census Geographic Data in R  
  
Understanding census geography and tigris basics:  
  
* The TIGER line shape files are made publicly available by the US Census Bureau  
* The tigris package simplifies the process of downloading and mapping with TIGER shapes  
	* library(tigris)  
    * az_counties <- counties(state = "AZ")  
    * plot(az_counties)  
    * nh_roads <- primary_secondary_roads(state = "NH")  
    * plot(nh_roads)  
* By default, tigris returns objects in Spatial DF format - slots encode characteristics of the data  
  
Customizing tigris options:  
  
* The tigris package allows for customization of plotting options  
	* ri_tiger <- counties("RI")  
    * ri_cb <- counties("RI", cb = TRUE)  
    * par(mfrow = c(1, 2))  
    * plot(ri_tiger, main = "TIGER/Line")  
    * plot(ri_cb, main = "Cartographic boundary")  
* Can use the sf package for further plotting of tigris data  
	* options(tigris_class = "sf")  
    * az_sf <- counties("AZ", cb = TRUE)  
    * class(az_sf)  
* Can cache tigris shape files on a local computer  
	* options(tigris_use_cache = TRUE)  
* The defaults in tigris are to the most recent year of data, though this can be overridden by argument  
	* williamson90 <- tracts(state = "TX", county = "Williamson", cb = TRUE, year = 1990)  
    * williamson16 <- tracts(state = "TX", county = "Williamson", cb = TRUE, year = 2016)  
  
Combining and joining census geographic districts:  
  
* Can combine data obtained from multiple tigris data pulls; for example, studying Kansas City MO/KS  
	* missouri <- tracts("MO", cb = TRUE)  
    * kansas <- tracts("KS", cb = TRUE)  
    * kansas_missouri <- rbind_tigris(kansas, missouri)  
    * plot(kansas_missouri$geometry)  
* Can also create a geography such as New England using iteration  
	* new_england <- c("ME", "NH", "VT", "MA")  
    * ne_tracts <- map(new_england, function(x) { tracts(state = x, cb = TRUE) }) %>% rbind_tigris()  
* Can use the standard sf joining tool provided that the data are available in sf format  
	* tx_house <- state_legislative_districts(state = "TX", house = "lower", cb = TRUE)  
    * tx_joined <- left_join(tx_house, tx_members, by = c("NAME" = "District"))  
  
Plotting data with tigris and ggplot2:  
  
* Can create maps of legislative districts by political party currently holding the office  
	* ggplot(tx_joined) + geom_sf()  
    * ggplot(tx_joined, aes(fill = Party)) + geom_sf()  
    * ggplot(tx_joined, aes(fill = Party)) + geom_sf() + scale_fill_manual(values = c("R" = "red", "D" = "blue"))  
    * ggplot(tx_joined, aes(fill = Party)) + geom_sf() + coord_sf(crs = 3083, datum = NA) + scale_fill_manual(values = c("R" = "red", "D" = "blue")) + theme_minimal() + labs(title = "State House Districts in Texas")  
  
Example code includes:  
```{r eval=FALSE}

library(tigris)

# Get a counties dataset for Colorado and plot it
co_counties <- counties(state = "CO")
plot(co_counties)


# Get a Census tracts dataset for Denver County, Colorado and plot it
denver_tracts <- tracts(state = "CO", county = "Denver")
plot(denver_tracts)


# Plot area water features for Lane County, Oregon
lane_water <- area_water(state = "OR", county = "Lane")
plot(lane_water)

# Plot primary & secondary roads for the state of New Hampshire
nh_roads <- primary_secondary_roads(state = "NH")
plot(nh_roads)


# Check the class of the data
class(co_counties)

# Take a look at the information in the data slot
head(co_counties@data)

# Check the coordinate system of the data
co_counties@proj4string


# Get a counties dataset for Michigan
mi_tiger <- counties("MI")

# Get the equivalent cartographic boundary shapefile
mi_cb <- counties("MI", cb = TRUE)

# Overlay the two on a plot to make a comparison
plot(mi_tiger)
plot(mi_cb, add = TRUE, border = "red")


# Get data from tigris as simple features
options(tigris_class = "sf")

# Get countries from Colorado and view the first few rows
colorado_sf <- counties("CO")
head(colorado_sf)

# Plot its geometry column
plot(colorado_sf$geometry)


# DO NOT ADD CACHE FOR NOW
# Set the cache directory
# tigris_cache_dir("Your preferred cache directory path would go here")

# Set the tigris_use_cache option
# options(tigris_use_cache = TRUE)

# Check to see that you've modified the option correctly
# getOption("tigris_use_cache")


# Get a historic Census tract shapefile from 1990 for Williamson County, Texas
williamson90 <- tracts(state = "TX", county = "Williamson", 
                       cb = TRUE, year = 1990)

# Compare with a current dataset for 2016
williamson16 <- tracts(state = "TX", county = "Williamson", 
                       cb = TRUE, year = 2016)

# Plot the geometry to compare the results                       
par(mfrow = c(1, 2))
plot(williamson90$geometry)
plot(williamson16$geometry)


# Get Census tract boundaries for Oregon and Washington
or_tracts <- tracts("OR", cb = TRUE)
wa_tracts <- tracts("WA", cb = TRUE)

# Check the tigris attributes of each object
attr(or_tracts, "tigris")
attr(wa_tracts, "tigris")

# Combine the datasets then plot the result
or_wa_tracts <- rbind_tigris(or_tracts, wa_tracts)
plot(or_wa_tracts$geometry)


# Generate a vector of state codes and assign to new_england
new_england <- c("ME", "NH", "VT", "MA")

# Iterate through the states and request tract data for state
ne_tracts <- map(new_england, function(x) {
  tracts(state = x, cb = TRUE)
}) %>%
  rbind_tigris()

plot(ne_tracts$geometry)


# Get boundaries for Texas and set the house parameter
tx_house <- state_legislative_districts(state = "TX", house = "lower", cb = TRUE)

# Merge data on legislators to their corresponding boundaries
tx_joined <- left_join(tx_house, tx_members, by = c("NAME" = "District"))

head(tx_joined)


# Plot the legislative district boundaries
ggplot(tx_joined) + 
  geom_sf()

# Set fill aesthetic to map areas represented by Republicans and Democrats
ggplot(tx_joined, aes(fill = Party)) + 
  geom_sf()

# Set values so that Republican areas are red and Democratic areas are blue
ggplot(tx_joined, aes(fill = Party)) + 
  geom_sf() + 
  scale_fill_manual(values = c("R" = "red", "D" = "blue"))


# Draw a ggplot without gridlines and with an informative title
ggplot(tx_joined, aes(fill = Party)) + 
  geom_sf() + 
  coord_sf(crs = 3083, datum = NA) + 
  scale_fill_manual(values = c("R" = "red", "D" = "blue")) + 
  theme_minimal(base_size = 16) + 
  labs(title = "State House Districts in Texas")

```
  
  
  
***
  
Chapter 4 - Mapping US Census Data  
  
Simple feature geometry and tidycensus:  
  
* The tidycensus package can wrap the tigris package for some of its less complex data requests  
	* geometry = TRUE is available for the following geographies: state, county, tract, 'block group', block, zcta  
    * cook_value <- get_acs(geography = "tract", state = "IL", county = "Cook", variables = "B25077_001", geometry = TRUE)  
* For geographies that are not available for grabbing geography by default, can use join instead  
	* idaho_income <- get_acs(geography = "school district (unified)", variables = "B19013_001", state = "ID")  
    * idaho_school <- school_districts(state = "ID", type = "unified", class = "sf")  
    * id_school_joined <- left_join(idaho_school, idaho_income, by = "GEOID")  
* Maps may move and rescale AK and HI - set shift_geo=TRUE to implement this  
	* state_value <- get_acs(geography = "state", variables = "B25077_001", survey = "acs1", geometry = TRUE, shift_geo = TRUE)  
  
Mapping demographic data with ggplot2:  
  
* Can use fills to create instructive maps, and can edit for improved clarity  
	* ggplot(cook_value, aes(fill = estimate)) + geom_sf()  
    * ggplot(cook_value, aes(fill = estimate, color = estimate)) + geom_sf() + scale_fill_viridis_c() + scale_color_viridis_c()  
    * ggplot(cook_value, aes(fill = estimate, color = estimate)) + geom_sf() + scale_fill_viridis_c(labels = scales::dollar) + scale_color_viridis_c(guide = FALSE) + theme_minimal() + coord_sf(crs = 26916, datum = NA) + labs(title = "Median home value by Census tract", subtitle = "Cook County, Illinois", caption = "Data source: 2012-2016 ACS.\nData acquired with the R tidycensus package.", fill = "ACS estimate")  
  
Advance demographic mapping:  
  
* There are many visual variables in cartography - position, size, shape, value, hue, orientation, texture  
* The graduated symbol map is a popular plotting method - shapes are differentially sized based on underlying variable values  
	* centers <- st_centroid(state_value)  
    * ggplot() + geom_sf(data = state_value, fill = "white") + geom_sf(data = centers, aes(size = estimate), shape = 21, fill = "lightblue", alpha = 0.7, show.legend = "point") + scale_size_continuous(range = c(1, 20))  
* The "small multiples" plot is another common plot type - achieved with faceting  
	* ggplot(dc_race, aes(fill = percent, color = percent)) + geom_sf() + coord_sf(datum = NA) + facet_wrap(~variable)  
* Interactive visualizations are continually more prevalent, and many of the libraries are wrapped for use in R - leaflet, plotly, htmlwidgets  
	* library(mapview)  
    * mapview(cook_value, zcol = "estimate", legend = TRUE)  
  
Cartographic workflows with tigris and tidycensus:  
  
* Can create maps using a dot-density distribution; dots are scattered by a sub-unit (such as census tract), proportional to the size (such as population), colored optionally by a third attriubute (such as race)  
	* dc_dots <- map(c("White", "Black", "Hispanic", "Asian"), function(group) { dc_race %>% filter(variable == group) %>% st_sample(., size=.$value / 100) %>% st_sf() %>% mutate(group = group) }) %>% reduce(rbind)  
    * dc_dots <- dc_dots %>% group_by(group) %>% summarize()  # for faster plotting  
    * dc_dots_shuffle <- sample_frac(dc_dots, size = 1)  # ensures random dots rather than clumped dots by group  
    * plot(dc_dots_shuffle, key.pos = 1)  
* Can be valuable to supplement maps with roadways and bodies of water  
	* options(tigris_class = "sf")  
    * dc_roads <- roads("DC", "District of Columbia") %>%  filter(RTTYP %in% c("I", "S", "U"))  
    * dc_water <- area_water("DC", "District of Columbia")  
    * dc_boundary <- counties("DC", cb = TRUE)  
    * plot(dc_water$geometry, col = "lightblue")  
    * ggplot() + geom_sf(data = dc_boundary, color = NA, fill = "white") + geom_sf(data = dc_dots, aes(color = group, fill = group), size = 0.1) + geom_sf(data = dc_water, color = "lightblue", fill = "lightblue") + geom_sf(data = dc_roads, color = "grey") + coord_sf(crs = 26918, datum = NA) + scale_color_brewer(palette = "Set1", guide = FALSE) + scale_fill_brewer(palette = "Set1") + labs(title = "The racial geography of Washington, DC", subtitle = "2010 decennial U.S. Census", fill = "", caption = "1 dot = approximately 100 people.\nData acquired with the R tidycensus and tigris packages.")  
* Note that ggplot2 plots the layers in order, meaning that they tend to over-write lower layers  
  
Next steps for working with demographic data in R:  
  
* Additional R packages for deomgraphic data include censusapi, ipumsr (MN data), cancensus (demographic and census data from Canadian census)  
  
Example code includes:  
```{r eval=FALSE}

library(sf)

# Get dataset with geometry set to TRUE
orange_value <- get_acs(geography = "tract", state = "CA", county = "Orange", 
                        variables = "B25077_001", geometry = TRUE
                        )

# Plot the estimate to view a map of the data
plot(orange_value["estimate"])


# Get an income dataset for Idaho by school district
idaho_income <- get_acs(geography = "school district (unified)", 
                        variables = "B19013_001", 
                        state = "ID")

# Get a school district dataset for Idaho
idaho_school <- school_districts(state = "ID", type = "unified", class = "sf")

# Join the income dataset to the boundaries dataset
id_school_joined <- left_join(idaho_school, idaho_income, by = "GEOID")

plot(id_school_joined["estimate"])


# Get a dataset of median home values from the 1-year ACS
state_value <- get_acs(geography = "state", 
                       variables = "B25077_001", 
                       survey = "acs1", 
                       geometry = TRUE, 
                       shift_geo = TRUE)

# Plot the dataset to view the shifted geometry
plot(state_value["estimate"])


# Create a choropleth map with ggplot
ggplot(marin_value, aes(fill = estimate)) + 
  geom_sf()


# Set continuous viridis palettes for your map
ggplot(marin_value, aes(fill = estimate, color = estimate)) + 
  geom_sf() + 
  scale_fill_viridis_c() +  
  scale_color_viridis_c()


# Set the color guide to FALSE and add a subtitle and caption to your map
ggplot(marin_value, aes(fill = estimate, color = estimate)) + 
  geom_sf() + 
  scale_fill_viridis_c(labels = scales::dollar) +  
  scale_color_viridis_c(guide = FALSE) + 
  theme_minimal() + 
  coord_sf(crs = 26911, datum = NA) + 
  labs(title = "Median owner-occupied housing value by Census tract", 
       subtitle = "Marin County, California", 
       caption = "Data source: 2012-2016 ACS.\nData acquired with the R tidycensus package.", 
       fill = "ACS estimate")


# Generate point centers
centers <- st_centroid(state_value)

# Set size parameter and the size range
ggplot() + 
  geom_sf(data = state_value, fill = "white") + 
  geom_sf(data = centers, aes(size = estimate), shape = 21, 
          fill = "lightblue", alpha = 0.7, show.legend = "point") + 
  scale_size_continuous(range = c(1, 20))


# Check the first few rows of the loaded dataset dc_race
head(dc_race)

# Remove the gridlines and generate faceted maps
ggplot(dc_race, aes(fill = percent, color = percent)) + 
  geom_sf() + 
  coord_sf(datum = NA) + 
  facet_wrap(~variable)


# Map the orange_value dataset interactively
m <- mapview(orange_value)
m@map

# Map your data by the estimate column
m <- mapview(orange_value, zcol = "estimate")
m@map

# Add a legend to your map
m <- mapview(orange_value, zcol = "estimate", legend=TRUE)

m@map


# Generate dots, create a group column, and group by group column
dc_dots <- map(c("White", "Black", "Hispanic", "Asian"), function(group) {
  dc_race %>%
    filter(variable == group) %>%
    st_sample(., size = .$value / 100) %>%
    st_sf() %>%
    mutate(group = group) 
}) %>%
  reduce(rbind) %>%
  group_by(group) %>%
  summarize()


# Filter the DC roads object for major roads only
dc_roads <- roads("DC", "District of Columbia") %>%
  filter(RTTYP %in% c("I", "S", "U"))

# Get an area water dataset for DC
dc_water <- area_water("DC", "District of Columbia")

# Get the boundary of DC
dc_boundary <- counties("DC", cb = TRUE)


# Plot your datasets and give your map an informative caption
ggplot() + 
  geom_sf(data = dc_boundary, color = NA, fill = "white") + 
  geom_sf(data = dc_dots, aes(color = group, fill = group), size = 0.1) + 
  geom_sf(data = dc_water, color = "lightblue", fill = "lightblue") + 
  geom_sf(data = dc_roads, color = "grey") + 
  coord_sf(crs = 26918, datum = NA) + 
  scale_color_brewer(palette = "Set1", guide = FALSE) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "The racial geography of Washington, DC", 
       subtitle = "2010 decennial U.S. Census", 
       fill = "", 
       caption = "1 dot = approximately 100 people.\nData acquired with the R tidycensus and tigris packages.")

```
  
  
  
***
  
### _Multivariate Probability Distributions in R_  
  
Chapter 1 - Reading and Plotting Mutivariate Data  
  
Reading multivariate data:  
  
* Multivariate distributions describe 2+ variables (particularly when correlated) at the same time  
* Generally, multivariate data are in the tidy format - rows are observations, columns are variables/attributes  
	* iris_url <- "http://mlg.eng.cam.ac.uk/teaching/3f3/1011/iris.data"  
    * iris_raw <- read.table(iris_url, sep ="", header = FALSE)  
    * head(iris_raw, n = 4)  
    * colnames(iris_raw) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species" )  
    * bwt <- read.csv("birthweight.csv", row.names = 1)  
* Can access specific portions of the data and change data types as needed  
	* iris_raw$species <- as.factor(iris_raw$species)  
    * iris_raw$Species <- recode(iris_raw$Species, " 1 ='setosa'; 2 = 'versicolor'; 3 = 'virginica'")  
  
Mean vector and variance-covariance matrix:  
  
* Can use summary statistics to explore the dataset  
* The mean vector is the vector containing the means of each of the dimensions, while the variance-covariance matrix shows the variance and angles of the data  
	* colMeans(iris_raw[, 1:4])  
    * by(data = iris[,1:4], INDICES = iris$Species, FUN = colMeans)  
    * aggregate(. ~ Species, iris_raw, mean)  
* The variance-covariance matrix is produced using var() and cor()  
	* var(iris_raw[, 1:4])  
    * cor(iris_raw[, 1:4])  
    * corrplot(cor(iris_raw[, 1:4]), method = "ellipse")  
  
Plotting mutivariate data:  
  
* Can look at multiple bivariate plots or mutivariate data in many manners  
	* pairs(iris_raw[, 1:4])  # pair plot  
    * pairs(iris_raw[, 1:4], col = iris_raw$Species)  
    * lattice::splom(~iris_raw[, 1:4], col = iris_raw$Species, pch = 16)  
    * Ggally::ggpairs(data = iris_raw, columns = 1:4, mapping = aes(color = Species))  
    * scatterplot3d::scatterplot3d(iris_raw[, c(1, 3, 4)], color = as.numeric(iris_raw$Species))  
  
Example code includes:  
```{r cache=TRUE}

# Read in the wine dataset
wine <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep = ",")

# Print the first four entries
head(wine, n=4)

# Find the dimensions of the data
dim(wine)

# Check the names of the wine dataset 
names(wine)


# Assign new names
names(wine) <- c('Type', 'Alcohol', 'Malic', 'Ash', 'Alcalinity', 'Magnesium', 'Phenols', 'Flavanoids', 'Nonflavanoids','Proanthocyanins', 'Color', 'Hue', 'Dilution', 'Proline')
                      
# Check the new column names
names(wine)

# Check data type of each variable
str(wine)

# Change the Type variable data type
wine$Type <- as.factor(wine$Type)

# Check data type again 
str(wine)


# Calculate the mean of the Alcohol, Malic, Ash, and Alcalinity variables 
colMeans(wine[, 2:5])

# Calculate the mean of the variables by wine type
by(wine[, 2:5], wine$Type, FUN=colMeans)


# Calculate the variance-covariance matrix of the variables Alcohol, Malic, Ash, Alcalinity
var.wine <- var(wine[, 2:5])

# Round the matrix values to two decimal places 
round(var.wine, 2)


# Calculate the covariance matrix 
cor.wine <- cor(wine[, 2:5])

# Round the matrix to two decimal places 
round(cor.wine, 2)

# Plot the correlations 
corrplot::corrplot(cor.wine, method = "ellipse")


# Scatter plot matrix with base R 
pairs(wine[, 2:5])

# Scatter plot matrix with lattice  
lattice::splom(~wine[, 2:5])

# Scatter plot matrix colored by groups
lattice::splom( ~ wine[2:5], pch = 16, col=wine$Type)


# Produce a matrix of plots for the first four variables 
wine.gg <- GGally::ggpairs(wine[, 2:5])
wine.gg

# Produce a matrix of plots for the first four variables 
wine.gg <- GGally::ggpairs(wine, columns=2:5)
wine.gg

# Color the points by wine type 
wine.gg <- GGally::ggpairs(data = wine, columns = 2:5, aes(color=Type))
wine.gg


# Plot the three variables 
scatterplot3d::scatterplot3d(wine[, c("Alcohol", "Malic", "Alcalinity")], color=wine$Type)

```
  
  
  
***
  
Chapter 2 - Multivariate Normal Distribution  
  
Multivariate Normal Distribution:  
  
* Important probability distribution - generalization of the univariate normal, but with a variance-covariance matrix  
	* Elliptical, with center at the joint mean  
    * Shape of the ellipse depends on the variance-covariance matrix  
    * If the covariance is 0 and the variances are equal, then this is a circle  
    * If the covariance is high, then this will become like a line  
* Can use mvtnorm::rmvnorm() and the related functions  
  
Density of a multivariate normal distribution:  
  
* Can use the dnorm() function to get the height of the density curve at any given location  
* Can use densities to estimate the likelihood that a data point is generated from a given distribution  
	* dmvnorm(x, mean, sigma)  
    * mu1 <- c(1, 2)  
    * sigma1 <- matrix(c(1, .5, .5, 2), 2)  
    * dmvnorm(x = c(0, 0), mean = mu1, sigma = sigma1)  
    * x <- rbind(c(0, 0), c(1, 1), c(0, 1)); x  
    * dmvnorm(x = x, mean = mu, sigma = sigma)  
* Can create a perspective plot using the persp() function  
	* d <- expand.grid(seq(-3, 6, length.out = 50 ), seq(-3, 6, length.out = 50))  
    * dens1 <- dmvnorm(as.matrix(d), mean=c(1,2), sigma=matrix(c(1, .5, .5, 2), 2))  
    * dens1 <- matrix(dens1, nrow = 50 )  
    * persp(dens1, theta = 80, phi = 30, expand = 0.6, shade = 0.2, col = "lightblue", xlab = "x", ylab = "y", zlab = "dens")  
  
Cumulative distribution and inverse CDF:  
  
* Can be valuable to calculate the CDF and the inverse CDF for statistical purposes  
	* mu1 <- c(1, 2)  
    * sigma1 <- matrix(c(1, 0.5, 0.5, 2), 2)  
    * pmvnorm(upper = c(2, 4), mean = mu1, sigma = sigma1)  
    * pmvnorm(lower = c(1, 2), upper = c(2, 4), mean = mu1, sigma = sigma1)  
* Suppose that you want to find the smallest ellipse that contains 95% of the density  
	* sigma1 <- diag(2)  # force the contours to be circular  
    * qmvnorm(p = 0.95, sigma = sigma1, tail = "both")  
  
Checking normality of multivariate data:  
  
* Normality is a convenient assumption for simplifying many common statistical tests  
	* qqnorm(iris_raw[, 1])  
    * qqline(iris_raw[, 1])  
    * uniPlot(iris_raw[, 1:4])  
* Can use several tests for normality of the multivariate distributions  
	* mardiaTest(iris_raw[, 1:4])  
    * mardiaTest(iris_raw[, 1:4], qqplot = TRUE)  
    * hzTest(iris_raw[,1:4])  
    * mardiaTest(iris[iris_raw$Species == "setosa", 1:4])  
  
Example code includes:  
```{r}

mu.sim <- c(2, -2)
sigma.sim <- matrix(data=c(9, 5, 5, 4), nrow=2, byrow=FALSE)

mu.sim
sigma.sim


# Generate 100 bivariate normal samples
multnorm.sample <- mvtnorm::rmvnorm(100, mean=mu.sim, sigma=sigma.sim)

# View the first 6 samples
head(multnorm.sample)

# Scatterplot of the bivariate samples 
plot(multnorm.sample)


# Calculate density
multnorm.dens <- mvtnorm::dmvnorm(multnorm.sample, mean = mu.sim, sigma = sigma.sim)

# Create scatter plot of density heights 
scatterplot3d::scatterplot3d(cbind(multnorm.sample, multnorm.dens), color="blue", pch="", 
                             type = "h", xlab = "x", ylab = "y", zlab = "density"
                             )


mvals <- expand.grid(seq(-5, 10, length.out = 40), seq(-8, 4, length.out = 40))
str(mvals)


# Calculate density over the specified grid
mvds <- mvtnorm::dmvnorm(mvals, mean=mu.sim, sigma=sigma.sim)
matrix_mvds <-  matrix(mvds, nrow = 40)

# Create a perspective plot
persp(matrix_mvds, theta = 80, phi = 30, expand = 0.6, shade = 0.2, 
      col = "lightblue", xlab = "x", ylab = "y", zlab = "dens"
      )


# Volume under a bivariate standard normal
mvtnorm::pmvnorm(lower = c(-1, -1), upper = c(1, 1))

# Volume under specified mean and variance-covariance matrix
mvtnorm::pmvnorm(lower = c(-5, -5), upper = c(5, 5), mean = mu.sim, sigma = sigma.sim)


# Probability contours for a standard bivariate normal
mvtnorm::qmvnorm(p=0.9, tail = "both", sigma = diag(2))

# Probability contours for a bivariate normal 
mvtnorm::qmvnorm(p=0.95, tail = "both", mean=mu.sim, sigma=sigma.sim)


# Test sample normality 
qqnorm(multnorm.sample[, 1])
qqline(multnorm.sample[, 1])


# requires RJAGS 4+
# Create qqnorm plot (no longer exported from MVN)
# MVN::uniPlot(wine[, c("Alcohol", "Malic", "Ash", "Alcalinity")], type = "qqplot")
# MVN::mvn(wine[, c("Alcohol", "Malic", "Ash", "Alcalinity")], univariatePlot = "qq")

# requires RJAGS 4+
# mardiaTest qqplot 
# wine.mvntest <- MVN::mardiaTest(wine[, 2:5])  # 'MVN::mardiaTest' is deprecated.\nUse 'mvn' instead.\nSee help(\"Deprecated\")
# wine.mvntest <- MVN::mvn(wine[, 2:5])
# wine.mvntest


# requires RJAGS 4+
# Use mardiaTest
# MVN::mvn(multnorm.sample)

# requires RJAGS 4+
# Use hzTest
# MVN::hzTest(wine[, 2:5])  # 'MVN::hzTest' is deprecated.  Use 'mvn' instead.
# MVN::mvn(wine[, 2:5], mvnTest="hz")

```
  
  
  
***
  
Chapter 3 - Other Multivariate Distributions  
  
Other common multivariate distributions:  
  
* Not all multivariate distributions are normal - may be skewed or follow a different distribution  
	* Normal has mean, sigma  
    * t has delta, sigma with DF  
    * Skew-normal has xi, Omega  
    * Skew-t has xi, Omega with DF  
* For the t-distribution, tails are fatter than they would be for the normal (which behaves like a t-distribution with oo degrees of freedom)  
	* rmvt(n, delta, sigma, df)  
    * dmvt(x, delta, sigma, df)  
    * qmvt(p, delta, sigma, df)  
    * pmvt(upper, lower, delta, sigma, df)  
  
Density and cumulative density for mutlivariate-T:  
  
* Individual stocks are often modeled by a univariate t-distribution  
* Portfolios are often valued using a multivariate t-distribution  
	* dmvt(x, delta = rep(0, p), sigma = diag(p), log = TRUE)  
    * x <- seq(-3, 6, by = 1); y <- seq(-3, 6, by = 1)  
    * d <- expand.grid(x = x, y = x)  
    * del1 <- c(1, 2); sig1 <- matrix(c(1, .5, .5, 2), 2)  
    * dens <- dmvt(as.matrix(d), delta = del1, sigma = sig1, df = 10, log = FALSE)  
    * scatterplot3d(cbind(d, dens), type = "h", zlab = "density")  
* Can claculate the pmvt()  
	* pmvt(lower = -Inf, upper = Inf, delta, sigma, df, ...)  
    * pmvt(lower = c(-1, -2), upper = c(2, 2), delta = c(1, 2), sigma = diag(2), df = 6)  
    * qmvt(p, interval, tail, delta, sigma, df)  
  
Multivariate skewed distributions:  
  
* Normal and t distributions both model symmetric (non-skewed) data, though real-world data are frequently skewed  
* The univariate skew-normal includes psi (location) and omega (variance-covariance) and alpha (skew)  
* The sn library offers skew-normal as a distribution  
	* dmsn(x, xi, Omega, alpha)  
    * pmsn(x, xi, Omega, alpha)  
    * rmsn(n, xi, Omega, alpha)  
    * dmst(x, xi, Omega, alpha, nu)  # nu is df  
    * pmst(x, xi, Omega, alpha, nu)  # nu is df  
    * rmst(n, xi, Omega, alpha, nu )  # nu is df  
* Example of creating random data  
	* xi1 <- c(1, 2, -5)  
    * Omega1 <- matrix(c(1, 1, 0, 1, 2, 0, 0, 0, 5), 3, 3)  
    * alpha1 <- c(4, 30, -5)  
    * skew.sample <- rmsn(n = 2000, xi = xi1, Omega = Omega1, alpha = alpha1)  
    * skewt.sample <- rmst(n = 2000, xi = xi1, Omega = Omega1, alpha = alpha1, nu = 4)  
* Can estimate the parameters from the data  
	* msn.mle(y = skew.sample, opt.method = "BFGS")  
  
Example code includes:  
```{r}

# Generate the t-samples 
multt.sample <- mvtnorm::rmvt(200, delta=mu.sim, sigma=sigma.sim, df=5)

# Print the first 6 samples
head(multt.sample)

# Requires RJAGS 4+
# Check multivariate normality
# MVN::mvn(multt.sample, univariatePlot="qq", mvnTest="mardia")


# Calculate densities
multt.dens <- mvtnorm::dmvt(multt.sample, delta=mu.sim, sigma=sigma.sim, df=5) 

# Plot 3D heights of densities
scatterplot3d::scatterplot3d(cbind(multt.sample, multt.dens), color = "blue", pch = "", 
                             type = "h", xlab = "x", ylab = "y", zlab = "density"
                             )

# Calculate the volume under the specified t-distribution
mvtnorm::pmvt(lower = c(-5, -5), upper = c(5, 5), delta=mu.sim, sigma=sigma.sim, df=5)

# Calculate the equal probability contour
mvtnorm::qmvt(p=0.9, tail="both", delta=0, sigma=diag(2), df=5)


# Generate the skew-normal samples 
skewnorm.sample <- sn::rmsn(n=100, xi=mu.sim, Omega=sigma.sim, alpha=c(4, -4)) 

# Print first six samples
head(skewnorm.sample)

# Generate the skew-t samples 
skewt.sample <- sn::rmst(n = 100, xi = mu.sim, Omega = sigma.sim, alpha = c(4, -4), nu=5)

# Print first six samples
head(skewt.sample)


skewnorm.sampleDF <- data.frame(x=skewnorm.sample[, 1], y=skewnorm.sample[, 2])
str(skewnorm.sampleDF)


# Contour plot for skew-normal sample
ggplot(skewnorm.sampleDF, aes(x=x, y=y)) + 
    geom_point() + 
    geom_density_2d()

# Requires RJAGS 4+
# Normality test for skew-normal sample
# skewnorm.Test <- MVN::mvn(skewnorm.sample, mvnTest="mardia", univariatePlot="qq")

# Requires RJAGS 4+
# Normality test for skew-t sample
# skewt.Test <- MVN::mvn(skewt.sample, mvnTest="mardia", univariatePlot="qq") 


ais.female <- data.frame(Ht=c(195.9, 189.7, 177.8, 185, 184.6, 174, 186.2, 173.8, 171.4, 179.9, 193.4, 188.7, 169.1, 177.9, 177.5, 179.6, 181.3, 179.7, 185.2, 177.3, 179.3, 175.3, 174, 183.3, 184.7, 180.2, 180.2, 176, 156, 179.7, 180.9, 179.5, 178.9, 182.1, 186.3, 176.8, 172.6, 176, 169.9, 183, 178.2, 177.3, 174.1, 173.6, 173.7, 178.7, 183.3, 174.4, 173.3, 168.6, 174, 176, 172.2, 182.7, 180.5, 179.8, 179.6, 171.7, 170, 170, 180.5, 173.3, 173.5, 181, 175, 170.3, 165, 169.8, 174.1, 175, 171.1, 172.7, 175.6, 171.6, 172.3, 171.4, 178, 162, 167.3, 162, 170.8, 163, 166.1, 176, 163.9, 173, 177, 168, 172, 167.9, 177.5, 162.5, 172.5, 166.7, 175, 157.9, 158.9, 156.9, 148.9, 149), 
                         Wt=c(78.9, 74.4, 69.1, 74.9, 64.6, 63.7, 75.2, 62.3, 66.5, 62.9, 96.3, 75.5, 63, 80.5, 71.3, 70.5, 73.2, 68.7, 80.5, 72.9, 74.5, 75.4, 69.5, 66.4, 79.7, 73.6, 78.7, 75, 49.8, 67.2, 66, 74.3, 78.1, 79.5, 78.5, 59.9, 63, 66.3, 60.7, 72.9, 67.9, 67.5, 74.1, 68.2, 68.8, 75.3, 67.4, 70, 74, 51.9, 74.1, 74.3, 77.8, 66.9, 83.8, 82.9, 64.1, 68.85, 64.8, 59, 72.1, 75.6, 71.4, 69.7, 63.9, 55.1, 60, 58, 64.7, 87.5, 78.9, 83.9, 82.8, 74.4, 94.8, 49.2, 61.9, 53.6, 63.7, 52.8, 65.2, 50.9, 57.3, 60, 60.1, 52.5, 59.7, 57.3, 59.6, 71.5, 69.7, 56.1, 61.1, 47.4, 56, 45.8, 47.8, 43.8, 37.8, 45.1)
                         )
str(ais.female)


# Fit skew-normal parameters
fit.ais <- sn::msn.mle(y = cbind(ais.female$Ht, ais.female$Wt), opt.method = "BFGS")

# Print the skewness parameters
fit.ais$dp$alpha

# Fit skew-normal parameters
fit.ais <- sn::msn.mle(y = ais.female[, c("Ht", "Wt")])

# Print the skewness parameters
fit.ais$dp$alpha

```
  
  
  
***
  
Chapter 4 - Principal Component Analysis and Multidimensional Scaling  
  
Principal Component Analysis:  
  
* Creation of uncorrelated (orthogonal) components that can be linearly combined to create the original dataset  
	* PC1 explains the maximum variance  
    * PC2 is orthogonal to PC1 and explains the maximum remaining variance  
    * PC3 is orthogonal to PC1/PC2 and explains the maximum remaning variance  
    * princomp(x, cor = FALSE, scores = TRUE)  # if cor=TRUE, use the correlation/covariance matrix to create PCA rather than the original data provided  
* Can run an example using the mtcars dataset  
	* mtcars.sub <- mtcars[ , -c(8,9)]  
    * cars.pca <- princomp(mtcars.sub, cor = TRUE, scores = TRUE)  
  
Choosing the number of components:  
  
* Several methods for assessing the ideal number of PC to use  
	* screeplot(cars.pca, type = "lines")  # pick an elbow point as the cutoff  
    * summary(cars.pca)  # pick the number of components that explain X% of the variance, with X specified prior to analysis  
  
Interpreting PCA attributes:  
  
* There are many attributes of the principal components object, especially when scores=TRUE is set  
	* cars.pca <- princomp(mtcars.sub, cor = TRUE, scores = TRUE)  
    * cars.pca$loadings # or loadings(cars.pca)  
    * biplot(cars.pca, col = c("gray","steelblue"), cex = c(0.5, 1.3))  
* PCA scores are the projection of the original dataset on the principal components  
	* head(cars.pca$scores)  
    * head(cars.pca$scores[, 1:2])  
    * biplot(cars.pca, col = c("steelblue", "white"), cex = c(0.8, 0.01))  
    * scores <- data.frame(cars.pca$scores)  
    * ggplot(data = scores, aes(x = Comp.1, y = Comp.2, label = rownames(scores))) + geom_text(size = 4, col = "steelblue")  
    * cylinder <- factor(mtcars$cyl)  
    * ggplot(data = scores, aes(x = Comp.1, y = Comp.2, label = rownames(scores), color = cylinder)) + geom_text(size = 4)  
* Can use functions from the factoextra library  
	* fviz_pca_biplot(cars.pca)  
    * fviz_pca_ind(cars.pca)  
    * fviz_pca_var(cars.pca)  
  
Multi-dimensional scaling:  
  
* Placing objects conceptually in a 2D or 3D space such that distances match as closesly as possible - MDS (multi-dimensional scaling)  
	* cmdscale(d, k = 2, ...)  # default k=2  
    * usloc <- cmdscale(UScitiesD)  
    * ggplot(data = data.frame(usloc), aes(x = X1, y = X2, label = rownames(usloc))) + geom_text()  
* Can apply MDS using the mtcars dataset  
	* cars.dist <- dist(mtcars)  
    * cars.mds <- cmdscale(cars.dist, k = 2)  
    * cars.mds <- data.frame(cars.mds)  
    * ggplot(data = cars.mds, aes(x = X1, y = X2, label = rownames(cars.mds))) + geom_text()  
* Can run in higher dimensions by changing the k= parameter  
	* cars.dist <- dist(mtcars)  
    * cmds3 <- data.frame(cmdscale(cars.dist, k = 3))  
    * scatterplot3d(cmds3, type = "h", pch = 19, lty.hplot = 2)  
  
Wrap-Up:  
  
* Reading and reformatting data  
* Summary statistics - Mean Vector, Variance-covariance matrix, Correlation matrix  
* Plotting in 2D and 3D  
* Multivariate probability distributions: Normal, T, Skew-normal, Skew-t  
* Dimension reduction: PCA, MDS  
  
Example code includes:  
```{r}

data(state)
str(state.x77)


par(mfrow=c(1, 1))
par(mfcol=c(1, 1))


# Calculate PCs
pca.state <- princomp(state.x77, cor=TRUE, scores=TRUE) 

# Plot the PCA object  
plot(pca.state) 

# Print the summary of the PCs
summary(pca.state) 


# Variance explained by each PC
pca.var <- pca.state$sdev^2  

# Proportion of variance explained by each PC
pca.pvar <- pca.var/sum(pca.var) 


# Proportion of variance explained by each principal component
pca.pvar

# Cumulative variance explained plot
plot(cumsum(pca.pvar), xlab = "Principal component", 
     ylab = "Cumulative Proportion of variance explained", ylim = c(0,1), type = 'b')
grid()

# Add a horizontal line
abline(h=0.95, col="blue")


# Draw screeplot
screeplot(pca.state, type = "l")
grid()


# Create dataframe of scores
scores.state <- data.frame(pca.state$scores)

# Plot of scores labeled by state name
ggplot(data = scores.state, aes(x = Comp.1, y = Comp.2, label = rownames(scores.state))) + 
    geom_text( alpha = 0.8, size = 3) + 
    ggtitle("PCA of states data")


# Create dataframe of scores
scores.state <- data.frame(pca.state$scores)

# Plot of scores colored by region
ggplot(data=scores.state, aes(x=Comp.1, y=Comp.2, label=rownames(scores.state), color=state.region)) + 
    geom_text(alpha = 0.8, size = 3) + 
    ggtitle("PCA of states data colored by region")


# Plot the scores
factoextra::fviz_pca_ind(pca.state)

# Plot the PC loadings
factoextra::fviz_pca_var(pca.state)

# Create a biplot
factoextra::fviz_pca_biplot(pca.state)


# Calculate distance 
state.dist <- dist(state.x77)

# Perform multidimensional scaling 
mds.state <- cmdscale(state.dist) 
mds.state_df <- data.frame(mds.state)

# Plot the representation of the data in two dimensions 
ggplot(data = mds.state_df, aes(x = X1, y = X2, label = rownames(mds.state), color = state.region)) + 
    geom_text(alpha = 0.8, size = 3)


# Calculate distance 
wine.dist <- dist(wine[, -1])

# Perform multidimensional scaling 
mds.wine <- cmdscale(wine.dist, k=3) 
mds.wine_df <- data.frame(mds.wine)

# Plot the representation of the data in three dimensions 
scatterplot3d::scatterplot3d(mds.wine_df, color = wine$Type, pch = 19, type = "h", lty.hplot = 2)

```
  
  
  
***
  
### _Intermediate Functional Programming with purrr_  
  
Chapter 1 - Programming with purrr  
  
Refresher of purrr Basics:  
  
* The map() function is one of the most basic purrr calls  
	* map(.x, .f, …)  # for each element of .x do .f  
* OpenData files available from French city St Malo  
    * JSON format; nested list  
* The map() function will always return a list by default  
	* res <- map(visit_2015, sum)  # returns a list  
* Can override to other preferred outputs, such as map_dbl()  
	* res <- map_dbl(visit_2015, sum)  # returns a numeric  
* Can also extend to map2(.x, .y, .f, …) which resolves to do .f(.x, .y, …)  
	* res <- map2(visit_2015, visit_2016, sum)  
    * res <- map2_dbl(visit_2015, visit_2016, sum)  
* Can use pmap() to run operations on 3+ items, though these need to be passed in as a list  
	* l <- list(visit_2014, visit_2015, visit_2016)  
    * res <- pmap(l, sum)  
    * res <- pmap_dbl(l, sum)  
  
Introduction to mappers:  
  
* The .f is the action element - function applied to every element, number n to extract the nth element, character vector of named elements to extract  
* The functions can either be regular functions or lambda (anonymous) functions  
	* map_dbl(visit_2014, function(x) { round(mean(x)) })   
* The anonymous function with a one-sided formula can be written in any of several ways  
	* map_dbl(visits2017, ~ round(mean(.x)))  # typically the default  
    * map_dbl(visits2017, ~ round(mean(.)))  
    * map_dbl(visits2017, ~ round(mean(..1)))  
    * map2(visits2016, visits2017, ~ .x + .y)  
    * map2(visits2016, visits2017, ~ ..1 + ..2)  
* Can extend to data with more than 2 parameters  
	* pmap(list, ~ ..1 + ..2 + ..3)  
* Can use as_mapper to create mapper objects from lambda functions  
	* round_mean <- function(x){ round(mean(x)) }  
    * round_mean <- as_mapper(~ round(mean(.x))))  
* Mappers have several benefits  
	* More concise  
    * Easier to read than functions  
    * Reusable  
  
Using Mappers to Clean Data:  
  
* Can use set_names from purrr to set the names of a list  
	* visits2016 <- set_names(visits2016, month.abb)  
    * all_visits <- list(visits2015, visits2016, visits2017)  
    * named_all_visits <- map(all_visits, ~ set_names(.x, month.abb))  
* The keep() function extracts elements that satisfy a condition  
	* over_30000 <- keep(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * over_mapper <- keep(visits2016, limit)  
* The discard() function removes elements that satisfy a condition  
	* under_30000 <- discard(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * under_mapper <- discard(visits2016, limit)  
    * names(under_mapper)  
* Can use keep() and discard() with map() to clean up lists  
	* df_list <- list(iris, airquality) %>% map(head)  
    * map(df_list, ~ keep(.x, is.factor))  
  
Predicates:  
  
* Predicates return either TRUE or FALSE - example of is.numeric()  
* Predicate functionals take an element and a predicate, and then use the predicate on the element  
	* keep(airquality, is.numeric)  # keep all elements that return TRUE when run against the predicate  
* There are also extensions of every() and some()  
	* every(visits2016, is.numeric)  
    * every(visits2016, ~ mean(.x) > 1000)  
    * some(visits2016, ~ mean(.x) > 1000)  
* The detect_index() returns the first and last element that satisfies a condition  
	* detect_index(visits2016, ~ mean(.x) > 1000)  # index of first element that satisfies  
    * detect_index(visits2016, ~ mean(.x) > 1000, .right = TRUE)  # index of last element that satisfies  
* The detect() returns the value rather than the index  
	* detect(visits2016, ~ mean(.x) > 1000, .right = TRUE)  
* The has_element() tests whether an object contains an item  
	* visits2016_mean <- map(visits2016, mean)  
    * has_element(visits2016_mean,981)  
  
Example code includes:  
```{r}

visit_a <- c(117, 147, 131, 73, 81, 134, 121)
visit_b <- c(180, 193, 116, 166, 131, 153, 146)
visit_c <- c(57, 110, 68, 72, 87, 141, 67)

# Create the to_day function
to_day <- function(x) {
    x*24
}

# Create a list containing both vectors: all_visits
all_visits <- list(visit_a, visit_b)

# Convert to daily number of visits: all_visits_day
all_visits_day <- map(all_visits, to_day)

# Map the mean() function and output a numeric vector 
map_dbl(all_visits_day, mean)


# You'll test out both map() and walk() for plotting
# Both return the "side effects," that is to say, the changes in the environment (drawing plots, downloading a file, changing the working directory...), but walk() won't print anything to the console.

# Create all_tests list  and modify with to_day() function
all_tests <- list(visit_a, visit_b, visit_c)
all_tests_day <- map(all_tests, to_day)

# Plot all_tests_day with map
map(all_tests_day, barplot)

# Plot all_tests_day
walk(all_tests_day, barplot)

# Get sum of all visits and class of sum_all
sum_all <- pmap(all_tests_day, sum)
class(sum_all)


# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) { x*24 })

# Turn visit_a into daily number of visits by using a mapper
map(visit_a, ~.x*24)

# Create a mapper object called to_day
to_day <- as_mapper(~.x*24)

# Use it on the three vectors
map(visit_a, to_day)
map(visit_b, to_day)
map(visit_c, to_day)


# Round visit_a to the nearest tenth with a mapper
map_dbl(visit_a, ~ round(.x, -1))

# Create to_ten, a mapper that rounds to the nearest tenth
to_ten <- as_mapper(~ round(.x, -1))

# Map to_ten on visit_b
map_dbl(visit_b, to_ten)

# Map to_ten on visit_c
map_dbl(visit_c, to_ten)


# Create a mapper that test if .x is more than 100 
is_more_than_hundred <- as_mapper(~ .x > 100)

# Run this mapper on the all_visits object
map(all_visits, ~ keep(.x, is_more_than_hundred) )

# Use the  day vector to set names to all_list
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
full_visits_named <- map(all_visits, ~ set_names(.x, day))

# Use this mapper with keep() 
map(full_visits_named, ~ keep(.x, is_more_than_hundred))


# Set the name of each subvector
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
all_visits_named <- map(all_visits, ~ set_names(.x, day))

# Create a mapper that will test if .x is over 100 
threshold <- as_mapper(~.x > 100)

# Run this mapper on the all_visits object: group_over
group_over <- map(all_visits, ~ keep(.x, threshold) )

# Run this mapper on the all_visits object: group_under
group_under <-  map(all_visits, ~ discard(.x, threshold) )


# Create a threshold variable, set it to 160
threshold <- 160

# Create a mapper that tests if .x is over the defined threshold
over_threshold <- as_mapper(~ .x > threshold)

# Are all elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ every(.x, over_threshold))

# Are some elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ some(.x, over_threshold))

```
  
  
  
***
  
Chapter 2 - Functional Programming from Theory to Practice  
  
Functional Programming in R:  
  
* Everything that exists is an object and everything that happens is a function call  
	* This means that a function is an object and can be treated as such  
    * Every action in R is performed by a function  
    * Functions are first-class citizens, and behave like any other object  
    * Functions can be manipulated, stored as variables, lambda (anonymous), stored in a list, arguments of a function, returned by a function  
    * R is a functional programming language  
* In a "pure function", output depends only on input, and there are no side-effects (no changes to the environment)  
	* Sys.Date() depends on the enviornment and is thus not pure  
    * write.csv() is called solely for the side effect (writing a file) and is thus not pure  
  
Tools for Functional Programming in purrr:  
  
* A high order function can take functions as input and return functions as output  
	* nop_na <- function(fun){  
    *     function(...){ fun(..., na.rm = TRUE) }  
    * }  
    * sd_no_na <- nop_na(sd)  
    * sd_no_na( c(NA, 1, 2, NA) )  
* There are three types of high-order functions  
	* Functionals take another function and return a vector - like map()  
    * Function factories take a vector and create a function  
    * Function operators take functions and return functions - considered to be "adverbs"  
* Two of the most common adverbs in purrr are safely() and possibly()  
	* The safely() call returns a function that will return $result and $error when run; helpful for diagnosing issues with code rather than losing the information  
    * safe_log <- safely(log)  
    * safe_log("a")  # there will be $result of NULL and $error with the error code  
    * map( list(2, "a"), safely(log) )  
  
Using possibly():  
  
* The possibly() function is an adverb that returns either the value of the function OR the value specified in the otherwise element  
	* possible_sum <- possibly(sum, otherwise = "nop")  
    * possible_sum("a")  # result will be "nop"  
* Note that possibly() cannot be made to run a function; it will just return a pre-specified value  
  
Handling adverb results:  
  
* Can use transpose() to change the output (converts the list to inside out)  
	* Transpose turn a list of n elements a and b to a list of a and b, with each n elements  
* The compact() function will remove the NULL elements  
	* l <- list(1,2,3,"a")  
    * possible_log <- possibly(log, otherwise = NULL)  
    * map(l, possible_log) %>% compact()  
* Can use the httr package specifically for http requests  
	* httr::GET(url) will return the value from attempting to connect to url - 200 is good, 404 is unavailable, etc.  
  
Example code includes:  
```{r cache=TRUE}

# `$` is a function call, of a special type called 'infix operator', as they are put between two elements, and can be used without parenthesis.

# Launch Sys.time(), Sys.sleep(1), & Sys.time()
Sys.time()
Sys.sleep(1)
Sys.time()


data(iris)
str(iris)


# Launch nrow(iris), Sys.sleep(1), & nrow(iris)
nrow(iris)
Sys.sleep(1)
nrow(iris)


# Launch ls(), create an object, then rerun the ls() function
# ls()
# this <- 12
# ls()

# Create a plot of the iris dataset
plot(iris)


urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')


# Create a safe version of read_lines()
safe_read <- safely(readr::read_lines)

# Map it on the urls vector
res <- map(urls, safe_read)

# Set the name of the results to `urls`
named_res <-  set_names(res, urls)

# Extract only the "error" part of each sublist
map(named_res, "error")


# Code a function that discard() the NULL from safe_read()
safe_read_discard <- function(url){
    safe_read(url) %>%
        discard(is.null)
}

# Map this function on the url list
res <- map(urls, safe_read_discard)


# Create a possibly() version of read_lines()
possible_read <- possibly(readr::read_lines, otherwise = 404)

# Map this function on urls, pipe it into set_names()
res <- map(urls, possible_read) %>% set_names(urls)

# Paste each element of the list 
res_pasted <- map(res, paste, collapse=" ")

# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)


url_tester <- function(url_list){
    url_list %>%
        # Map a version of read_lines() that otherwise returns 404
        map( possibly(readr::read_lines, otherwise = 404) ) %>%
        # Set the names of the result
        set_names( urls ) %>% 
        # paste() and collapse each element
        map(paste, collapse =" ") %>%
        # Remove the 404 
        discard(~.x==404) %>%
        names() # Will return the names of the good ones
}

# Try this function on the urls object
url_tester(urls)


url_tester <- function(url_list, type = c("result", "error")){
    res <- url_list %>%
        # Create a safely() version of read_lines() 
        map( safely(readr::read_lines) ) %>%
        set_names( url_list ) %>%
        # Transpose into a list of $result and $error
        transpose() 
    # Complete this if statement
    if (type == "result") return( res$result ) 
    if (type == "error") return( res$error ) 
}

# Try this function on the urls object
url_tester(urls, type = "error") 


url_tester <- function(url_list){
    url_list %>%
        # Map a version of GET() that would otherwise return NULL 
        map( possibly(httr::GET, otherwise=NULL) ) %>%
        # Set the names of the result
        set_names( urls ) %>%
        # Remove the NULL
        compact() %>%
        # Extract all the "status_code" elements
        map("status_code")
}

# Try this function on the urls object
url_tester(urls)

```
  
  
  
***
  
Chapter 3 - Better Code with purrr  
  
Rationale for cleaner code:  
  
* Cleaner code is easier to debug (spot typos), easier to interpret, and easier to modify  
	* tidy_iris_lm <- compose( as_mapper(~ filter(.x, p.value < 0.05)), tidy, partial(lm, data=iris, na.action = na.fail) )  
    * list( Petal.Length ~ Petal.Width, Petal.Width ~ Sepal.Width, Sepal.Width ~ Sepal.Length ) %>% map(tidy_iris_lm)  
* Clean code characteristics  
	* Light - no unnecessary code  
    * Readable - less repition makes for easier reading (one piece of code for one task)  
    * Interpretable  
    * Maintainable  
* The compose() function is used to compose a function from two or more functions  
	* rounded_mean <- compose(round, mean)  
  
Building functions with compose() and negate():  
  
* There is a limitless amount of functions that can be included in compose()  
	* clean_aov <- compose(tidy, anova, lm)  
* Can use negate() to flip the predicate - TRUE becomes FALSE and FALSE becomes TRUE  
	* is_not_na <- negate(is.na)  
    * under_hundred <- as_mapper(~ mean(.x) < 100)  
    * not_under_hundred <- negate(under_hundred)  
    * map_lgl(98:102, under_hundred)  
    * map_lgl(98:102, not_under_hundred)  
* The "good" status return codes from GET() are in the low-200s  
	* good_status <- c(200, 201, 202, 203)  
    * status %in% good_status  
  
Prefilling functions:  
  
* The partial() allows for pre-filling a function  
	* mean_na_rm <- partial(mean, na.rm = TRUE)  
    * lm_iris <- partial(lm, data = iris)  
* Can also combine partial() and compose()  
	* rounded_mean <- compose( partial(round, digits = 2), partial(mean, na.rm = TRUE) )  
* Can use functions from rvest for web scraping  
	* read_html()  
    * html_nodes()  
    * html_text()  
    * html_attr()  
  
List columns:  
  
* A list column is part of a nested data frame - one or more of the data frame columns is itself a list (requires use of tibble rather than data.frame)  
	* df <- tibble( classic = c("a", "b","c"), list = list( c("a", "b","c"), c("a", "b","c", "d"), c("a", "b","c", "d", "e") ) )  
    * a_node <- partial(html_nodes, css = "a")  
    * href <- partial(html_attr, name = "href")  
    * get_links <- compose( href, a_node,  read_html )  
    * urls_df <- tibble( urls = c("https://thinkr.fr", "https://colinfay.me", "https://datacamp.com", "http://cran.r-project.org/") )  
    * urls_df %>% mutate(links = map(urls, get_links))  
* Can also unnest the data from the list columns  
	* urls_df %>% mutate(links = map(urls, get_links)) %>% unnest()  
* Can also nest() a standard data.frame  
	* iris_n <- iris %>% group_by(Species) %>% tidyr::nest()  
* Since the list column is a list, the purrr functions can be run on them  
	* iris_n %>% mutate(lm = map(data, ~ lm(Sepal.Length ~ Sepal.Width, data = .x)))  
    * summary_lm <- compose(summary, lm)  
    * iris %>% group_by(Species) %>% nest() %>% mutate(data = map(data, ~ summary_lm(Sepal.Length ~ Sepal.Width, data = .x)), data = map(data, "r.squared")) %>% unnest()  
  
Example code includes:  
```{r cache=TRUE}

urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')


# Compose a status extractor (compose is also an igraph function)
status_extract <- purrr::compose(httr::status_code, httr::GET)

# Try with "https://thinkr.fr" & "http://datacamp.com"
status_extract("https://thinkr.fr")
status_extract("http://datacamp.com")

# Map it on the urls vector, return a vector of numbers
oldUrls <- urls
urls <- oldUrls[c(1, 2, 4, 5)]
map_dbl(urls, status_extract)


# Negate the %in% function 
`%not_in%` <- negate(`%in%`)

# Compose a status extractor 
status_extract <- purrr::compose(httr::status_code, httr::GET)

# Complete the function
strict_code <- function(url){
    code <- status_extract(url)
    if (code %not_in% c(200:203)){ return(NA) } else { return(code) } 
}


# Map the strict_code function on the urls vector
res <- map(urls, strict_code)

# Set the names of the results using the urls vector
res_named <- set_names(res, urls)

# Negate the is.na function
is_not_na <- negate(is.na)

# Run is_not_na on the results
is_not_na(res_named)


# Prefill html_nodes() with the css param set to h2
get_h2 <- partial(rvest::html_nodes, css="h2")

# Combine the html_text, get_h2 and read_html functions
get_content <- purrr::compose(rvest::html_text, get_h2, xml2::read_html)

# Map get_content to the urls list
res <- map(urls, get_content) %>%
    set_names(urls)

# Print the results to the console
res


# Create a partial version of html_nodes(), with the css param set to "a"
a_node <- partial(rvest::html_nodes, css="a")

# Create href(), a partial version of html_attr()
href <- partial(rvest::html_attr, name = "href")

# Combine href(), a_node(), and read_html()
get_links <- purrr::compose(href, a_node, xml2::read_html)

# Map get_links() to the urls list
res <- map(urls, get_links) %>%
    set_names(urls)


df <- tibble::tibble(urls=urls)
df


# Create a "links" columns, by mapping get_links() on urls
df2 <- df %>%
    mutate(links = map(urls, get_links)) 

# Print df2 to see what it looks like
df2

# unnest() df2 to have a tidy dataframe
df2 %>%
    tidyr::unnest()

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Discovering the Dataset:  
  
* The dataset is available from https://github.com/ThinkR-open/datasets  
	* rstudioconf: a list of 5055 tweets  
    * length(rstudioconf)  
    * length(rstudioconf[[1]])  
    * purrr::vec_depth(rstudioconf)  
* JSON is a standard data format for the web, and typically consists of key-value pairs which are read as nested lists by R  
* Refresher of keep() and discard() usage  
	* keep(1:10, ~ .x < 5)  
    * discard(1:10, ~ .x < 5)  
  
Extracting Information from the Dataset:  
  
* Can manipulate functions for list cleaning using high-order functions - includes partial() and compose()  
	* sum_no_na <- partial(sum, na.rm = TRUE)  
    * map_dbl(airquality, sum_no_na)  
    * rounded_sum <- compose(round, sum_no_na)  
    * map_dbl(airquality, rounded_sum)  
* Can also clean lists using compact() to remove NULL and flatten() to remove one level from a nested list  
	* l <- list(NULL, 1, 2, 3, NULL)  
    * compact(l)  
    * my_list <- list( list(a = 1), list(b = 2) )  
    * flatten(my_list)  
  
Manipulating URL:  
  
* Can use the mapper functions to create a re-usable function  
	* mult <- as_mapper(~ .x * 2)  
* Can use str_detect inside the mapper function  
	* lyrics <- c("Is this the real life?", "Is this just fantasy?", "Caught in a landslide", "No escape from reality")  
    * stringr::str_detect(a, "life")  
  
Identifying Influencers:  
  
* Can use the map_at() function to run a function at a specific portion of the list  
	* my_list <- list( a = 1:10, b = 1:100, c = 12 )  
    * map_at(.x = my_list, .at = "b", .f = sum)  
* Can also use negate() to reverse the actio of a predicate  
	* not_character <- negate(is.character)  
    * my_list <- list( a = 1:10, b = "a", c = iris )  
    * map(my_list, not_character)  
  
Wrap up:  
  
* Lambda functions and reusable mappers  
	* map(1:5, ~ .x*10)  
    * ten_times <- as_mapper(~ .x * 10)  
    * map(1:5, ten_times)  
* Function manipulation using functionals (functions that take functions as inputs and return vectors)  
	* map() & friends  
    * keep() & discard()  
    * some() & every()  
* Function operators take functions and return (modified) functions  
	* safely() & possibly()  
    * partial()  
    * compose()  
    * negate()  
* Cleaner code is easier to read, understand, and maintain  
	* rounded_mean <- compose( partial(round, digits = 1), partial(mean, trim = 2, na.rm = TRUE) )  
    * map( list(airquality, mtcars), ~ map_dbl(.x, rounded_mean) )  
  
Example code includes:  
```{r}

rstudioconfDF <- readRDS("./RInputFiles/#RStudioConf.RDS")
dim(rstudioconfDF)
rstudioconf <- as.list(as.data.frame(t(rstudioconfDF)))
length(rstudioconf)
length(rstudioconf[[1]])


# Print the first element of the list to the console 
rstudioconf[[1]]

# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")

# Extract the favorite count element of each non_rt sublist
fav_count <- map_dbl(non_rt, "favorite_count")

# Get the median of favorite_count for non_rt
median(fav_count)


# Keep the RT, extract the user_id, remove the duplicate
rt <- keep(rstudioconf, "is_retweet") %>%
    map("user_id") %>% 
    unique()

# Remove the RT, extract the user id, remove the duplicate
non_rt <- discard(rstudioconf, "is_retweet") %>%
    map("user_id") %>% 
    unique()

# Determine the total number of users
union(rt, non_rt) %>% length()

# Determine the number of users who has just retweeted
setdiff(rt, non_rt) %>% length()


# Prefill mean() with na.rm, and round() with digits = 1
mean_na_rm <- partial(mean, na.rm=TRUE)
round_one <- partial(round, digits=1)

# Compose a rounded_mean function
rounded_mean <- purrr::compose(round_one, mean_na_rm)

# Extract the non retweet  
non_rt <- discard(rstudioconf, "is_retweet")

# Extract "favorite_count", and pass it to rounded_mean()
map_dbl(non_rt, "favorite_count") %>%
    rounded_mean()


# Combine as_vector(), compact(), and flatten()
flatten_to_vector <- purrr::compose(as_vector, compact, flatten)

# Complete the fonction
extractor <- function(list, what = "mentions_screen_name"){
    map(list, what) %>%
        flatten_to_vector()
}

# Create six_most, with tail(), sort(), and table()
six_most <- purrr::compose(tail, sort, table)

# Run extractor() on rstudioconf
extractor(rstudioconf) %>% 
    six_most()


# Extract the "urls_url" elements, and flatten() the result
urls_clean <- map(rstudioconf, "urls_url") %>%
    flatten()

# Remove the NULL
compact_urls <- compact(urls_clean)
compact_urls <- discard(urls_clean, is.na)  # Due to creation of the list above, NULL became NA

# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x, "github"))

# Look for the "github" pattern, and sum the result
map_lgl( compact_urls, has_github ) %>%
    sum()


# Complete the function
ratio_pattern <- function(vec, pattern){
    n_pattern <- str_detect(vec, pattern) %>% sum()
    n_pattern / length(vec)
}

# Create flatten_and_compact()
extraDiscard <- function(x) { discard(x, is.na) }  # address same NA issue as above
flatten_and_compact <- purrr::compose(compact, extraDiscard, flatten)

# Complete the pipe to get the ratio of URLs with "github"
map(rstudioconf, "urls_url") %>%
    flatten_and_compact() %>% 
    ratio_pattern("github")


# Create mean_above, a mapper that tests if .x is over 3.3
mean_above <- as_mapper(~ .x > 3.3)

# Prefil map_at() with "retweet_count", mean_above for above, 
# and mean_above negation for below
above <- partial(map_at, .at = "retweet_count", .f = mean_above )
below <- partial(map_at, .at = "retweet_count", .f = negate(mean_above) )

# Map above() and below() on non_rt, keep the "retweet_count"
# ab <- map(non_rt, above) %>% keep("retweet_count")
# bl <- map(non_rt, below) %>% keep("retweet_count")

# Compare the size of both elements
# length(ab)
# length(bl)


# Get the max() of "retweet_count" 
max_rt <- map_dbl(non_rt, "retweet_count") %>% 
    max()

# Prefill map_at() with a mapper testing if .x equal max_rt
# max_rt_calc <- partial(map_at, .at = "retweet_count", .f = ~.x==max_rt )

# Map max_rt_calc on non_rt, keep the retweet_count & flatten
# res <- map(non_rt, max_rt_calc) %>% 
#     keep("retweet_count") %>% 
#     flatten()

# Print the "screen_name" and "text" of the result
# res$screen_name
# res$text

```
  
  
  
***
  
### _Foundations of Functional Programming with purrr_  
  
Chapter 1 - Simplifying Iteration and Lists with purrr  
  
The power of iteration:  
  
* Iteration is the process of repeating commands over elements of a dataset  
	* d <- list()  
    * for(i in 1:10){ d[[i]] <- read_csv(files[i]) }  
* Typos are a risk when using for loops; purrr simplifies this by reducing everything to the commands map_*()  
	* map(.x=object, .f=function)  
    * d <- map(files, read_csv)  
* Can use the bird_counts data as an example  
	* bird_sum <- map(bird_counts, sum)  
  
Subsetting lists:  
  
* Lists can store multiple data types - data frames, models, numbers, text, whatever  
* Indexing a data frame  
	* mtcars[1, "wt"]  
    * mtcars$wt  
* List indexing differs from data frames and vectors  
	* lo[[2]]  # pull the second element  
    * lo[["model"]]  # pull by name  
* Calculate something on each element with purrr  
	* map(survey_data, ~nrow(.x))  # The ~ symbolizes that this is a user-written function  
  
Multiple flavors of map():  
  
* May want to output data in a different class, and the map_*() help with this  
	* map_dbl(survey_data, ~nrow(.x))  # return as a vector of doubles  
    * map_lgl(survey_data, ~nrow(.x)==14)  # return as logical vector (boolean)  
    * map_chr(species_names, ~.x)  # return as character vector 
    * survey_rows <- data.frame(names = names(survey_data), rows = NA) 
    * survey_rows$rows <- map_dbl(survey_data, ~nrow(.x)) 
  
Example code includes:  
```{r}

files <- list('data_from_1990.csv', 'data_from_1991.csv', 'data_from_1992.csv', 'data_from_1993.csv', 'data_from_1994.csv', 'data_from_1995.csv', 'data_from_1996.csv', 'data_from_1997.csv', 'data_from_1998.csv', 'data_from_1999.csv', 'data_from_2000.csv', 'data_from_2001.csv', 'data_from_2002.csv', 'data_from_2003.csv', 'data_from_2004.csv', 'data_from_2005.csv'
              )
files <- map(files, function(x) { paste0("./RInputFiles/", x) })


# Initialize list
all_files <- list()

# For loop to read files into a list
for(i in seq_along(files)){
  all_files[[i]] <- readr::read_csv(file = files[[i]])
}

# Output size of list object
length(all_files)


# Use map to iterate
all_files_purrr <- purrr::map(files, read_csv) 

# Output size of list object
length(all_files_purrr)


temp <- c("1", "2", "3", "4")
list_of_df <- list(temp, temp, temp, temp, temp, temp, temp, temp, temp, temp)


# Check the class type of the first element
class(list_of_df[[1]])

# Change each element from a character to a number
for(i in seq_along(list_of_df)){
    list_of_df[[i]] <- as.numeric(list_of_df[[i]])
}

# Check the class type of the first element
class(list_of_df[[1]])

# Print out the list
list_of_df


temp <- c("1", "2", "3", "4")
list_of_df <- list(temp, temp, temp, temp, temp, temp, temp, temp, temp, temp)


# Check the class type of the first element
class(list_of_df[[1]])  

# Change each character element to a number
list_of_df <- map(list_of_df, as.numeric)

# Check the class type of the first element again
class(list_of_df[[1]])

# Print out the list
list_of_df


# Load wesanderson dataset
data(wesanderson, package="repurrrsive")

# Get structure of first element in wesanderson
str(wesanderson[[1]])

# Get structure of GrandBudapest element in wesanderson
str(wesanderson$GrandBudapest)


# Third element of the first wesanderson vector
wesanderson[[1]][3]

# Fourth element of the GrandBudapest wesanderson vector
wesanderson$GrandBudapest[4]


data(sw_films, package="repurrrsive")


# Subset the first element of the sw_films data
sw_films[[1]]

# Subset the first element of the sw_films data, title column 
sw_films[[1]]$title


# Map over wesanderson to get the length of each element
map(wesanderson, length)

# Map over wesanderson, and determine the length of each element
map(wesanderson, ~length(.x))


# Map over wesanderson and determine the length of each element
map(wesanderson, length)

# Create a numcolors column and fill with length of each wesanderson element
data.frame(numcolors = map_dbl(wesanderson, ~length(.x)))

```
  
  
  
***
  
Chapter 2 - More Complex Iterations  
  
Working with unnamed lists:  
  
* The purrr package works well with the piping (%>%) operator  
	* names(survey_data)  
    * survey_data %>% names()  
    * sw_films <- sw_films %>% set_names(map_chr(sw_films, "title"))  # make the sw_films list in to a named list based on the title of each movie in the last (sw_films is Star Wars films from repurrrsive)  
    * map(waterfowl_data, ~.x %>% sum() %>% log())  
  
More map():  
  
* Can use map() to simulate data, run models, test models, etc.  
	* list_of_df <- map(list_of_means, ~data.frame(a=rnorm(mean = .x, n = 200, sd = (5/2))))  
    * models <- education_data %>% map(~ lm(income ~ education_level, data=.x)) %>% map(summary)  
    * map(livingthings, ~.x[["species"]])  
* There are many flavors of map_*() which can be applied  
	* map_lgl - logical vector output  
    * map_dbl - double vector output  
    * map_int - integer vector output  
    * map_chr - character vector output  
    * map_dbl(bird_measurements, ~.x[["wing length"]])  
* Can use map_df to create a data frame, and using the data_frame() function  
	* bird_measurements %>% map_df(~ data_frame(weight=.x[["weight"]], wing_length = .x[["wing length"]]))  
  
map2() and pmap():  
  
* The map2() function allows for pulling out information from two inputs - .x is list 1 and .y is list 2  
	* simdata <- map2(list_of_means, list_of_sd, ~data.frame(a = rnorm(mean=.x, n=200, sd=.y), b = rnorm(mean=200, n=200, sd=15)))  
* Can also use the pmap() function for as many lists as we want to use (takes a list of lists as an input)  
	* input_list <- list( means = list_of_means, sd = list_of_sd, samplesize = list_of_samplesize)  
    * simdata <- pmap(inputs_list, function(means, sd, samplesize) data.frame(a = rnorm(mean=means, n=samplesize, sd=sd)))  
  
Example code includes:  
```{r}

# Use pipes to check for names in sw_films
sw_films %>%
    names()


# Set names so each element of the list is named for the film title
sw_films_named <- sw_films %>% 
  set_names(map_chr(., "title"))

# Check to see if the names worked/are correct
names(sw_films_named)


# Create a list of values from 1 through 10
numlist <- list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

# Iterate over the numlist 
map(numlist, ~.x %>% sqrt() %>% sin())


# List of sites north, east, and west
sites <- list("north", "east", "west")

# Create a list of dataframes, each with a years, a, and b column 
list_of_df <-  map(sites,  
  ~data.frame(years = .x,
       a = rnorm(mean = 5, n = 200, sd = 5/2),
       b = rnorm(mean = 200, n = 200, sd = 15)))

map(list_of_df, head)


# Map over the models to look at the relationship of a vs b
list_of_df %>%
    map(~ lm(a ~ b, data = .)) %>%
    map(~summary(.))


# Pull out the director element of sw_films in a list and character vector
map(sw_films, ~.x[["director"]])
map_chr(sw_films, ~.x[["director"]])

# Compare outputs when checking if director is George Lucas
map(sw_films, ~.x[["director"]] == "George Lucas")
map_lgl(sw_films, ~.x[["director"]] == "George Lucas")


# Pull out episode_id element as list
map(sw_films, ~.x[["episode_id"]])

# Pull out episode_id element as double vector
map_dbl(sw_films, ~.x[["episode_id"]])

# Pull out episode_id element as list
map(sw_films, ~.x[["episode_id"]])

# Pull out episode_id element as integer vector
map_int(sw_films, ~.x[["episode_id"]])


# List of 1 through 3
means <- list(1, 2, 3)

# Create sites list
sites <- list("north", "west", "east")

# Map over two arguments: years and mu
list_of_files_map2 <- map2(sites, means, ~data.frame(sites = .x,
                           a = rnorm(mean = .y, n = 200, sd = (5/2))))

map(list_of_files_map2, head)


means <- list(1, 2, 3)
sigma <- list(1, 2, 3)
means2 <- list(0.5, 1, 1.5)
sigma2 <- list(0.5, 1, 1.5)


# Create a master list, a list of lists
pmapinputs <- list(sites = sites,  means1 = means, sigma1 = sigma, 
                   means2 = means2, sigma2 = sigma2)

# Map over the master list
list_of_files_pmap <- pmap(pmapinputs, 
                           function(sites, means1, sigma1, means2, sigma2) {
                               data.frame(years = sites, 
                                          a = rnorm(mean = means1, n = 200, sd = sigma1), 
                                          b = rnorm(mean = means2, n = 200, sd = sigma2)
                                          )
                               }
                           )
                           
map(list_of_files_pmap, head)

```
  
  
  
***
  
Chapter 3 - Troubleshooting Lists with purrr  
  
How to purrr safely():  
  
* Can be QA/QC challenges if elements of the large list are not as expected  
* The map_*() will only work if the list is as expected  
* Can use the safely() command to find out where the issues are occurring, keep running, and putting a default output using the otherwise= element  
	* a <- list("unknown", 10) %>% map(safely(function(x) x * 10, otherwise = NA_real_))  
    * Each element has both an output element and an error-flagging element  
* Can pipe the transpose function to make it easier to investigate the error messages  
	* a <- list("unknown",10) %>% map(safely(function(x) x * 10, otherwise = NA_real_)) %>% transpose()  
  
Another way to possibly() purrr:  
  
* The possibly() function helps to get past the errors in the input list  
* Typical workflow is to find the issues using safely(), then address them using possibly()  
	* a <- list(-10, "unknown", 10) %>% map(safely(function(x) x * 10, otherwise = NA_real_))  
    * a <- list(-10, "unknown", 10) %>% map(possibly(function(x) x * 10, otherwise = NA_real_))  
  
purr is a walk() in the park:  
  
* The walk() function helps create human-readable results in a compact manner  
	* short_list <- list(-10, 1, 10)  
    * walk(short_list, print)  
* The walk() function has both a .x and a .f element  
	* walk(plist, print)  # no output created for the console  
  
Example code includes:  
```{r}

# Map safely over log
a <- list(-10, 1, 10, 0) %>% 
    map(safely(log, otherwise = NA_real_)) %>%
    # Transpose the result
    transpose()

# Print the list
a

# Print the result element in the list
a[["result"]]

# Print the error element in the list
a[["error"]]


# Load sw_people data
data(sw_people, package="repurrrsive")


# Map over sw_people and pull out the height element
height_cm <- map(sw_people, "height") %>%
    map(function(x) { ifelse(x == "unknown", NA, as.numeric(x)) })


# Map over sw_people and pull out the height element
height_ft <- map(sw_people , "height") %>% 
    map(safely(function(x){ ifelse(x == "unknown", NA, as.numeric(x) * 0.0328084) }, quiet = FALSE)) %>%
    transpose()

# Print your list, the result element, and the error element
walk(height_ft, function(x) { print(x[1:10]) })
height_ft[["result"]][1:10]
height_ft[["error"]][1:10]


# Take the log of each element in the list
a <- list(-10, 1, 10, 0) %>% 
    map(possibly(function(x){ log(x) }, otherwise=NA_real_))


# Create a piped workflow that returns double vectors
height_cm %>%  
    map_dbl(possibly(function(x){ x * 0.0328084 }, otherwise=NA_real_)) 


films <- map_chr(sw_films, "url")
people <- map(sw_films, "characters")

people_by_film <- tibble::tibble(films = rep(films, times=map_int(people, length)), 
                                 film_url = unlist(people)
                                 )

# Print with walk
walk(people_by_film, print)


data(gapminder, package="gapminder")
str(gapminder)

gap_split <- split(gapminder, gapminder$country)
gap_split[[1]]


# Map over the first 10 elements of gap_split
plots <- map2(gap_split[1:10], names(gap_split[1:10]), 
              ~ ggplot(.x, aes(year, lifeExp)) + geom_line() + labs(title = .y)
              )

# Object name, then function name
walk(plots, print)

```
  
  
  
***
  
Chapter 4 - Problem Solving with purrr  
  
Using purrr in your workflow:  
  
* Can set the names for a list using the purrr approach  
	* sw_films <- sw_films %>% set_names(map_chr(sw_films, "title"))  
    * map_chr(sw_films, ~.x[["episode_id"]]) %>% set_names(map_chr(sw_films, "title")) %>% sort()  
  
Even more complex problems:  
  
* Values may be buried inside lists inside lists inside etc.  
	* forks <- gh_repos %>% map( ~map(.x, "forks"))  
    * bird_measurements %>% map_df(~ data_frame( weight = .x[["weight"]], wing_length = .x[["wing length"]], taxa = "bird")) %>% select_if(is.numeric) %>% summary(.x)  
  
Graphs in purrr:  
  
* Can use ggplot2 to plot the elements using purrr  
  
Wrap up:  
  
* Iteration, data stored in lists, easy to read/write code  
  
Example code includes:  
```{r}

# Load the data
data(gh_users, package="repurrrsive")

# Check if data has names
names(gh_users)

# Map over name element of list
map(gh_users, ~.x[["name"]])

# Name gh_users with the names of the users
gh_users <- gh_users %>% 
    set_names(map_chr(gh_users, "name"))


# Check gh_repos structure
data(gh_repos, package="repurrrsive")
# str(gh_repos)  # List is much too long for str() printing

# Name gh_repos with the names of the repo owner 
gh_repos_named <- gh_repos %>% 
    map_chr(~map_chr(.x, ~.x$owner$login)[1]) %>% 
    set_names(gh_repos, .)


# Determine who joined github first
map_chr(gh_users, ~.x[["created_at"]]) %>%
    set_names(map_chr(gh_users, "name")) %>%
    sort()

# Determine user versus organization
map_lgl(gh_users, ~.x[["type"]] == "User") %>%
    sum() == length(gh_users)

# Determine who has the most public repositories
map_int(gh_users, ~.x[["public_repos"]]) %>%
    set_names(map_chr(gh_users, "name")) %>%
    sort()


# Set names of gh_repos with name subelement
gh_repos <- gh_repos %>% 
    map_chr(~map_chr(.x, ~.x$owner$login)[1]) %>% 
    set_names(gh_repos, .)

# Check to make sure list has the right names
names(gh_repos)


# Map over gh_repos to generate numeric output
map(gh_repos, 
    ~map_dbl(.x, ~.x[["size"]])) %>%
    # Grab the largest element
    map(~max(.x))


gh_users_df <- tibble::tibble(public_repos=map_int(gh_users, ~.x[["public_repos"]]), 
                              followers=map_int(gh_users, "followers")
                              )

# Scatter plot of public repos and followers
ggplot(data = gh_users_df, aes(x = public_repos, y = followers)) + 
    geom_point()


map(gh_repos_named, "followers")

# Histogram of followers        
gh_users_df %>%
    ggplot(aes(x = followers)) + 
    geom_histogram()


# Create a dataframe with four columns
map_df(gh_users, `[`, c("login", "name", "followers", "public_repos")) %>%
    # Plot followers by public_repos
    ggplot(., aes(x = followers, y = public_repos)) + 
    # Create scatter plots
    geom_point()


# Turn data into correct dataframe format
film_by_character <- tibble(filmtitle = map_chr(sw_films, "title")) %>%
    transmute(filmtitle, characters = map(sw_films, "characters")) %>%
    unnest()

# Pull out elements from sw_people
sw_characters <- map_df(sw_people, `[`, c("height", "mass", "name", "url"))

# Join the two new objects
inner_join(film_by_character, sw_characters, by = c("characters" = "url")) %>%
    # Make sure the columns are numbers
    mutate(height1 = ifelse(height=="unknown", NA, as.numeric(height)), 
           mass1 = ifelse(mass=="unknown", NA, as.numeric(stringr::str_replace(mass, ",", "")))
           ) %>% 
    filter(!is.na(height)) %>%
    ggplot(aes(x = height)) +
    geom_bar(stat="count") + 
    # geom_histogram(stat = "count") + 
    facet_wrap(~filmtitle)

```
  
  
  
***
  
### _Joining Data in R with data.table_  
  
Chapter 1 - Joining Multiple data.tables  
  
Introduction:  
  
* Combining data from multiple data set can be valuable for analysis  
* Need to identfy the table keys for a successful join  
	* demographics <- data.table(name = c("Trey", "Matthew", "Angela"), gender = c(NA, "M", "F"), age = c(54, 43, 39))  
    * shipping <- data.table(name = c("Matthew", "Trey", "Angela"), address = c("7 Mill road", "12 High street", "33 Pacific boulevard"))  
    * tables()  # will show all the data.table in the session along with rows, columns, space, and keys
str(demographics)  
  
Merge function:  
  
* Can run all of inner, outer, left, and right joins using merge()  
	* merge(x = demographics, y = shipping, by.x = "name", by.y = "name")  # default is an inner merge  
    * merge(x = demographics, y = shipping, by = "name")  # if the tables have the same key name  
    * merge(x = demographics, y = shipping, by = "name", all = TRUE)  # full outer join  
  
Left and right joins:  
  
* Can run left joins or right joins using all.x and all.y  
	* merge(x = demographics, y = shipping, by = "name", all.x = TRUE)  
    * merge(x = demographics, y = shipping, by = "name", all.y = TRUE)  
    * merge(x = demographics, y = shipping, by = "name", all.y = TRUE) is the same as merge(x = shipping, y = demographics, by = "name", all.x = TRUE)  
  
Example code includes:  
```{r}

library(data.table)

netflix <- fread("./RInputFiles/netflix_2017.csv", sep=",")
imdb <- fread("./RInputFiles/imdb_ratings.csv", sep=",")


# What data.tables are in my R session?
tables()

# View the first six rows 
head(netflix)
head(imdb)

# Print the structure
str(netflix)
str(imdb)


# Print the data.tables in your R session
netflix
imdb


# Inner join netflix and imdb
merge(netflix, imdb, by = "title")

# Full join netflix and imdb
merge(netflix, imdb, by = "title", all=TRUE)

# Full join imdb and netflix
merge(imdb, netflix, by = "title", all = TRUE)

# Left join imdb to netflix
merge(netflix, imdb, by="title", all.x=TRUE)

# Right join imdb to netflix
merge(netflix, imdb, by="title", all.y=TRUE)

# Compare to a left join of netflix to imdb
merge(imdb, netflix, by="title", all.x=TRUE)


australia_area <- fread("./RInputFiles/australia_area.csv", sep=",")
australia_capitals <- fread("./RInputFiles/australia_capitals.csv", sep=",")
australia_cities_top20 <- fread("./RInputFiles/australia_cities_top20.csv", sep=",")


# Identify the key for joining capitals and population
capitals_population_key <- "city"

# Left join population to capitals
capital_pop <- merge(australia_capitals, 
                     australia_cities_top20[, c("city", "population")], 
                     by=capitals_population_key, all.x=TRUE
                     )
capital_pop


# Identify the key for joining capital_pop and area
capital_pop_area_key <- "state"

# Inner join area to capital pop
australia_stats <- merge(capital_pop, australia_area[, c("state", "area_km2")], by=capital_pop_area_key)

# Print the final result
australia_stats

```
  
  
  
***
  
Chapter 2 - Joins Using data.table Syntax  
  
Joins using data.table syntax:  
  
* The general form of data.table syntax includes  
	* DT[i, j, by]  # grouped by "by", action j is taken on rows i  
    * DT[i, on]  # this will merge data.table "i" to data.table "DT" by key "on"  
    * demographics[shipping, on = .(name)]  
* Variables inside list() or .() are looked up in the column names of both data.tables  
	* shipping[demographics, on = list(name)]  # all records in demographics will be kept, plus records in shipping that have a match by on= to demographics  
    * shipping[demographics, on = .(name)]  
    * join_key <- c("name")  
    * shipping[demographics, on = join_key]  
* For an inner join, supply nomatch=0L; full joins are not possible using the data.table syntax, so it is necessary to use merge() instead  
	* shipping[demographics, on = .(name), nomatch = 0L]  
* The anti-join is possible using the negation operator  
	* demographics[!shipping, on = .(name)]  # all records in demographics that are NOT in shipping  
  
Setting and viewing data.table keys:  
  
* With keys set, the on= argument is no longer needed for joins on that key  
	* Setting a key also sorts the data.table by that key in memory (makes merge operations faster)  
    * setkey(DT, ...)  
    * setkey(DT, key1, key2, key3)  
    * setkey(DT, "key1", "key2", "key3")  
* The setkeyv() function allows for passing in a character vector for the key column names  
	* keys <- c("key1", "key2", "key3")  
    * setkeyv(dt, keys)  
* Can check for keys and find their names if they exist  
	* haskey(dt1)  
    * key(dt1)  
  
Incorporating joins in the data.table workflow:  
  
* Joins can be included in the data.table workflows, enabling rapid analysis  
	* DT1[DT2, on][i, j, by]  # join, followed by standard data.table operations  
    * customers[purchases, on = .(name)][sales > 1, j = .(avg_spent = sum(spent) / sum(sales)), by = .(gender)]  
* Can also incorporate calculations and new column creations with joins as follows  
	* DT1[DT2, on, j]  
    * customers[purchases, on = .(name), return_customer := sales > 1]  
    * DT1[DT2, on, j, by = .EACHI]  # will have a groupby for each match in DT1  
    * shipping[customers, on = .(name), j = .("# of shipping addresses" = .N), by = .EACHI]  
    * customers[shipping, on = .(name), .(avg_age = mean(age)), by = .(gender)]  
  
Example code includes:  
```{r}

# Right join population to capitals using data.table syntax
australia_capitals[australia_cities_top20, on = "city"]

# Right join using merge
merge(australia_capitals, australia_cities_top20, by = "city", all.y = TRUE)


# Inner join with the data.table syntax
australia_capitals[australia_cities_top20, on="city", nomatch=0L]


# Anti-join capitals to population
australia_cities_top20[!australia_capitals, on="city"]

# Anti-join capitals to area
australia_area[!australia_capitals, on="state"]


# Set the keys
setkey(netflix, "title")
setkey(imdb, "title")

# Inner join
netflix[imdb, nomatch=0L]


# Check for keys
haskey(netflix)
haskey(imdb)

# Find the key
the_key <- "title"

# Set the key for the other data.table
setkeyv(imdb, the_key)


# Inner join capitals to population
australia_cities_top20[australia_capitals, on="city", nomatch=0L]

# Join and sum
australia_cities_top20[australia_capitals, on = .(city), nomatch = 0, j = sum(percentage)]


continents <- fread("./RInputFiles/continents.csv", sep=",")
life_exp <- fread("./RInputFiles/gapminder_life_expectancy_2010.csv", sep=",")
life_exp <- life_exp %>% rename(years = life_expectancy)
str(continents)
str(life_exp)


# What countries are listed in multiple continents?
continents[life_exp, on = .(country), .N, by = .EACHI][N > 1]

# Calculate average life expectancy per continent:
avg_life_expectancy <- continents[life_exp, on = .(country), nomatch=0L][, j = mean(years), by = continent]
avg_life_expectancy

```
  
  
  
***
  
Chapter 3 - Diagnosing and Fixing Common Join Problems  
  
Complex keys:  
  
* A misspecified join is when an incorrect join key has been used  
* A malformed join is when they keys have no values in common (stacked join)  
* Best practice is to study the data in each column prior to attempting a join  
* When the keys have different names, can manage the merge with code  
	* merge(customers, web_visits, by.x = "name", by.y = "person")  
    * customers[web_visits, on = .(name = person)]  
    * customers[web_visits, on = c("name" = "person")]  
* Can also merge with multiple keys  
	* merge(purchases, web_visits, by = c("name", "date"))  
    * merge(purchases, web_visits, by.x = c("name", "date"), by.y = c("person", "date")  # matches in the order that the appear  
    * purchases[web_visits, on = c("name" = "person", "date")]  
  
Tricky columns:  
  
* Tables sometimes share column names that are not intended as join keys  
* With the data.table syntax, duplicate names from the i= area have i. as a prefix  
	* By contrast, with merge() duplicate names may have a .x and .y suffix  
* Can be helpful instead to rename columns prior to the join using setnames()  
	* setnames(parents, c("parent", "parent.gender", "parent.age"))  
* May want to join a data.frame and data.table, but rownames is the key for the data.table  
	* parents <- as.data.table(parents, keep.rownames = "parent")  
  
Duplicate matches:  
  
* May want to join columns that have many-many on the keys  
	* This will throw a long error message, and requires setting parameters to show this is intentional  
    * Note that NA will match all other NA; behavior can be over-ridden by filtering out the NA  
    * site2_ecology <- site2_ecology[!is.na(genus)]  
* Can keep only the first match or only the last match  
	* site1_ecology[site2_ecology, on = .(genus), mult = "first"]  
    * children[parents, on = .(parent = name), mult = "last"]  
* Can find the duplicated values  
	* duplicated(site1_ecology)  # boolean  
    * duplicated(site1_ecology, by = "genus")  # boolean, lookin just at column genus  
    * unique(site1_ecology, by = "genus")  # removes the duplicates  
    * duplicated(site1_ecology, by = "genus", fromLast = TRUE)  
    * unique(site1_ecology, by = "genus", fromLast = TRUE)  
  
Example code includes:  
```{r}

guardians <- fread("./RInputFiles/school_db_guardians.tsv")
locations <- fread("./RInputFiles/school_db_locations.tsv")
students <- fread("./RInputFiles/school_db_students.tsv")
subjects <- fread("./RInputFiles/school_db_subjects.tsv")
teachers <- fread("./RInputFiles/school_db_teachers.tsv")


# Full join
merge(students, guardians, by="name", all=TRUE)
students[guardians, on="name"]

# Change the code to an inner join
students[guardians, on = .(name), nomatch=0L]

# What are the correct join key columns?
students[guardians, on = c("guardian"="name"), nomatch = 0L]


# Intentionally errors out due to type mismatch
# subjects[locations, on=c("class", "semester")]

# Structure 
str(subjects)
str(locations)

# Does semester have the same class? 
same_class <- FALSE

# Fix the column class
locations[, semester := as.integer(semester)]

# Right join
subjects[locations, on=c("class", "semester")]


# Identify and set the keys
join_key <- c("subject"="class")

# Right join
teachers[locations, on=join_key]


# Inner join 1
capital_pop <- merge(australia_capitals, australia_cities_top20, by="city", nomatch=0L)

# Inner join 2
merge(capital_pop, australia_area, by="state", suffixes=c(".pop", ".area"), nomatch=0L)


netflixOrig <- fread("./RInputFiles/netflix_2017.csv", sep=",")
imdb <- fread("./RInputFiles/imdb_ratings.csv", sep=",")
netflix <- as.data.frame(netflixOrig)[, c("episodes", "release_date")]
rownames(netflix) <- netflixOrig$title

# Convert netflix to a data.table
netflix <- as.data.table(netflix, keep.rownames="series")

# Rename "title" to "series" in imdb
setnames(imdb, c("series", "rating"))

# Right join
imdb[netflix, on="series"]


cardio <- fread("./RInputFiles/affymetrix_chd_genes.csv")
framingham <- fread("./RInputFiles/framingham_chd_genes.csv")
heart <- fread("./RInputFiles/illumina_chd_genes.csv")


# Try an inner join
merge(heart, cardio, by=c("gene"), allow.cartesian=TRUE)

# Filter missing values
heart_2 <- heart[!is.na(gene)]
cardio_2 <- cardio[!is.na(gene)]

# Repeat the inner join
merge(heart_2, cardio_2, by=c("gene"), allow.cartesian=TRUE)


# Keep only the last probe for each gene
heart_3 <- unique(heart_2, by="gene", fromLast=TRUE)
cardio_3 <- unique(cardio_2, by="gene", fromLast=TRUE)

# Inner join
reproducible <- merge(heart_3, cardio_3, by="gene", suffixes=c(".heart", ".cardio"))
reproducible


# Right join taking the first match
heart_2[framingham, on="gene", mult="first"]

# Anti-join
reproducible[!framingham, on="gene"]

```
  
  
  
***
  
Chapter 4 - Concatenating and Reshaping data.table  
  
Concatenating two or more data.table:  
  
* Can concatenate tables that have rows split across multiple tables (e.g., multiple file reads)  
	* rbind(sales_2015, sales_2016)  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year")  # new column year will use the names for the tables  
    * rbind(sales_2015, sales_2016, idcol = TRUE)  # column gets the default name .id  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year", fill = TRUE)  # allows for misaligned columns, filled with NA as appropriate  
* Can also rbindlist() for tables that are stored as lists  
	* table_files <- c("sales_2015.csv", "sales_2016.csv")  
    * list_of_tables <- lapply(table_files, fread)  
    * rbindlist(list_of_tables)  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year", use.names = TRUE)  # use.names matches columns by name  
    * rbind("2015" = sales_2015, "2016" = sales_2016, idcol = "year", use.names = FALSE)  # matches columns by ORDER rather than by name (name mismatch explicitly allowed)  
  
Set operations:  
  
* Can run set operations with data.tables()  
	* fintersect(): what rows do these two data.tables share in common?  Duplicate rows are ignored by default, but can set all=TRUE to get all the copies printed  
    * fsetdiff(): what rows are unique to this data.table?  Will return the rows in the first table that are not in the second table; can set all=TRUE to get ALL the rows from the first table  
    * funion(): what is the unique set of rows across these two data.tables?  Returns all the unique rows found in either table, with duplicates ignired by default; set all=TRUE to get everything, which is equivalent to rbind  
  
Melting data.tables:  
  
* May want to melt a data.table from wide format to long format  
	* melt(sales_wide, measure.vars = c("2015", "2016"))  # the default for the new column names are "variable" and "value"  
    * melt(sales_wide, id.vars = "quarter", variable.name = "year", value.name = "amount")  # give the columns names that differ from the defaults; keeps only the columns specified, dropping all the others (?)  
  
Casting data.tables:  
  
* Can cast a long data.table to a wide format  
	* sales_wide <- dcast(sales_long, quarter ~ year, value.var = "amount")  # the value.var is the name of the column that will be split, while a ~ b means a will be the new rows and b is the column to throw in to the new columns  
    * dcast(profit_long, quarter ~ year, value.var = c("revenue", "profit"))  
    * dcast(sales_long, quarter ~ department + year, value.var = "amount")  # by default, department and year will be combined with an "_"  
    * sales_wide <- dcast(sales_long, season ~ year, value.var = "amount")  
    * mat <- as.matrix(sales_wide, rownames = "season")  
  
Example code includes:  
```{r}

ebola_W50 <- fread("./RInputFiles/ebola_2014_W50.csv")
ebola_W51 <- fread("./RInputFiles/ebola_2014_W51.csv")
ebola_W52 <- fread("./RInputFiles/ebola_2014_W52.csv")


# Concatenate case numbers from weeks 50 and 51
rbind(ebola_W50, ebola_W51)

# Intentionally throws an error
# Concatenate case numbers from all three weeks
# rbind(ebola_W50, ebola_W51, ebola_W52)

# Modify the code
rbind(ebola_W50, ebola_W51, ebola_W52, fill=TRUE)


gdp_africa <- fread("./RInputFiles/gdp_africa_2000.csv")
gdp_asia <- fread("./RInputFiles/gdp_asia_2000.csv")
gdp_europe <- fread("./RInputFiles/gdp_europe_2000.csv")
gdp_north_america <- fread("./RInputFiles/gdp_north_america_2000.csv")
gdp_oceania <- fread("./RInputFiles/gdp_oceania_2000.csv")
gdp_south_america <- fread("./RInputFiles/gdp_south_america_2000.csv")

gdp <- list(africa=gdp_africa, asia=gdp_asia, europe=gdp_europe, 
            north_america=gdp_north_america, oceania=gdp_oceania, south_america=gdp_south_america
            )


# Concatenate its data.tables
gdp_all_1 <- rbindlist(gdp)

# Concatenate its data.tables
gdp_all_2 <- rbindlist(gdp, idcol="continent")
str(gdp_all_2)
gdp_all_2[95:105]

# Fix the problem
gdp_all_3 <- rbindlist(gdp, idcol = "continent", use.names=TRUE)
gdp_all_3


# Obtain countries in both Asia and Europe
fintersect(gdp$europe, gdp$asia)

# Concatenate all data tables
gdp_all <- rbindlist(gdp, use.names=TRUE)

# Find all countries that span multiple continents
gdp_all[duplicated(gdp_all)]


# Get all countries in either Asia or Europe
funion(gdp$europe, gdp$asia)

# Concatenate all data tables
gdp_all <- rbindlist(gdp, use.names=TRUE)

# Print all unique countries
unique(gdp_all)


gdp_middle_east <- fread("./RInputFiles/gdp_middle_east_2000.csv")

# Which countries are in Africa but not considered part of the middle east?
fsetdiff(gdp$africa, gdp_middle_east)

# Which countries are in Asia but not considered part of the middle east?
fsetdiff(gdp$asia, gdp_middle_east)

# Which countries are in Europe but not considered part of the middle east?
fsetdiff(gdp$europe, gdp_middle_east)


gdp_per_capita_wrong <- fread("./RInputFiles/gdp_per_capita_oceania.csv")

colNames <- gdp_per_capita_wrong$V1
colNames[1] <- "year"
numData <- t(gdp_per_capita_wrong[, -1])

gdp_per_capita <- as.data.table(numData)
colnames(gdp_per_capita) <- colNames


# Print gdp_per_capita
gdp_per_capita

# Reshape gdp_per_capita to the long format
melt(gdp_per_capita, id.vars="year")

# Rename the new columns
melt(gdp_per_capita, id.vars = "year", variable.name="country", value.name="gdp_pc")


# Print ebola_wide
ebola_wide <- rbind(ebola_W50, ebola_W51) %>% 
    mutate(Week_50=ifelse(period_code=="2014-W50", Confirmed, NA), 
           Week_51=ifelse(period_code=="2014-W51", Confirmed, NA)
           ) %>%
    select(Location, period_start, period_end, Week_50, Week_51) %>%
    arrange(Location, period_start)
ebola_wide


# Stack Week_50 and Week_51
melt(ebola_wide, measure.vars=c("Week_50", "Week_51"), variable.name="period", value.name="cases")

# Modify the code
melt(ebola_wide, measure.vars = c("Week_50", "Week_51"), 
     variable.name = "period", value.name = "cases", id.vars="Location"
     )


gdp_oceania <- fread("./RInputFiles/gdp_and_pop_oceania.csv")
gdp_oceania$continent <- "Oceania"


# Split the population column by year
dcast(gdp_oceania, formula = country ~ year, value.var = "population")

# Split the gdp column by country
dcast(gdp_oceania, formula = year ~ country, value.var = "gdp")


# Reshape from wide to long format
wide <- dcast(gdp_oceania, formula = country ~ year, value.var = c("gdp", "population"))

# convert to a matrix
as.matrix(wide, rownames="country")

# Modify your previous code
dcast(gdp_oceania, formula = continent + country ~ year, value.var = c("gdp", "population"))


gdp_by_industry_oceania <- fread("./RInputFiles/gdp_by_industry_oceania.tsv")


# Split gdp by industry and year
gdp_by_industry_oceania
dcast(gdp_by_industry_oceania, formula = country ~ industry + year, value.var=c("gdp"))

```
  
  
  
***
  
### _Fraud Detection in R_  
  
Chapter 1 - Introduction and Motivation  
  
Introduction and Motivation:  
  
* Fraud is an uncommon, well-considered, imperceptibly concealed, time-evolving and often carefully organized crime which appears in many types and forms  
* Fraud is rare, but the cost of not detecting fraud is very large  
* Fraud detection models need to have multiple charcteristics  
	* Statistical accuracy  
    * Interpretability  
    * Regulatory compliance  
    * Economical impact - total cost of ownership and ROI  
    * Complement expert-based findings  
* Multiple challenges with fraud detection  
	* Imbalanced samples - credit card fraud is typically less than 0.5% ("needle in a haystack" problem)  
    * Operational efficiency (e.g., credit card decisions need to be made in ~8 seconds)  
    * Avoid harassing good customers over legitimate transactions  
* Confusion matrices are commonly used - true positive, true negative, false positive, false negative  
	* caret::confusionMatrix(data = predictions, reference = fraud_label)  
    * Accuracy is not a good measure when there is an imbalanced sample  
  
Time features:  
  
* Time is an important aspect of fraud detection; transactions tend to occur at similar hours  
	* There is no perfect natural ordering to time though, since time is cricular; arithmetic mean does not accout for the periodicity  
* Can create a circular histogram based off the time series data  
	* ts <- as.numeric(lubridate::hms(timestamps)) / 3600  
    * clock <- ggplot(data.frame(ts), aes(x = ts)) + geom_histogram(breaks = seq(0, 24), colour = "blue", fill = "lightblue") + coord_polar()  
    * arithmetic_mean <- mean(ts)  
    * clock + geom_vline(xintercept = arithmetic_mean, linetype = 2, color = "red", size = 2)  
* Can instead use the von Mises distribution for modeling time (variable wrapped around a circle)  
	* mu - periodic mean  
    * kappa - 1/k is the periodic variance (k is a measure of concentration)  
    * library(circular)  
    * ts <- circular(ts, units = "hours", template = "clock24")  
    * estimates <- mle.vonmises(ts)  
    * p_mean <- estimates$mu %% 24  
    * concentration <- estimates$kappa  
* Can then obtain a binary feature as to whether a given timestamp falls inside the interval  
	* densities <- dvonmises(ts, mu = p_mean, kappa = concentration)  
    * alpha <- 0.90  
    * quantile <- qvonmises((1 - alpha)/2, mu = p_mean, kappa = concentration) %% 24  
    * cutoff <- dvonmises(quantile, mu = p_mean, kappa = concentration)  
    * time_feature <- densities >= cutoff  
* Can also run confidence intervals with rolling windows  
	* time_feature = c(NA, NA)  
    * for (i in 3:length(ts)) {  
    *     ts_history <- ts[1:(i-1)]  
    *     estimates <- mle.vonmises(ts_history)  
    *     p_mean <- estimates$mu %% 24  
    *     concentration <- estimates$kappa  
    *     dens_i <- dvonmises(ts[i], mu = p_mean, kappa = concentration)  
    *     alpha <- 0.90  
    *     quantile <- qvonmises((1-alpha)/2, mu=p_mean, kappa=concentration) %% 24  
    *     cutoff <- dvonmises(quantile, mu = p_mean, kappa = concentration)  
    *     time_feature[i] <- dens_i >= cutoff  
    * }  
  
Frequency features:  
  
* May need to add features for a good-performing fraud detection algorithm  
	* trans %>% select(fraud_flag, orig_account_id, benef_country, authentication_cd, channel_cd, amount)  
* For example, may want to look at the authentication methods by person and associations to fraud  
	* trans <- trans %>% arrange(timestamp)  
    * trans_Alice <- trans %>% filter(account_name == "Alice")  
    * frequency_fun <- function(steps, auth_method) {  
    *     n <- length(steps)  
    *     frequency <- sum(auth_method[1:n] == auth_method[n + 1])  
    *     return(frequency)  
    * }  
    * freq_auth <- zoo::rollapply(trans_Alice$transfer_id, width=list(-1:-length(trans_Alice$transfer_id)),  partial = TRUE, FUN = frequency_fun, trans_Alice$authentication_cd)  
    * freq_auth <- c(0, freq_auth)  
* Can run a similar process for multiple accounts  
	* trans %>% group_by(account_name)  
    * trans <- trans %>% group_by(account_name) %>% mutate(freq_auth = c(0, zoo::rollapplyr(transfer_id, width = list(-1:-length(transfer_id)), partial = TRUE, FUN = count_fun, authentication_cd) ) )  
  
Recency features:  
  
* Recency features capture the dimension of time in a manner designed to highlight potential fraud  
	* Example of an authentication method that has not recently been used  
    * Recency=0 means not used recently (or never used before), while recency=1 means used recently  
    * recency=exp(-gamma*time)  # gamma is a tuning parameter, typically a small number  
    * Gamma is often chosen such that recency will be 0.01 after 180 days  
* Can create the recency feature in R  
	* recency_fun <- function(t, gamma, auth_cd, freq_auth) {  
    *     n_t <- length(t)  
    *     if (freq_auth[n_t] == 0) { recency <- 0 } else {  
    *         time_diff <- t[1] - max(t[2:n_t][auth_cd[(n_t-1):1] == auth_cd[n_t]])
    *         recency <- exp(-gamma * time_diff)  
    *     }
    *     return(recency)  
    * }  
* Can then calculate gamma and apply to each individual account  
	* gamma <- -log(0.01)/180 # = 0.0256  
    * trans <- trans %>% 
    *     group_by(account_name) %>%   
    *     mutate(rec_auth = zoo::rollapply(timestamp, width = list(0:-length(transfer_id)), partial = TRUE, FUN = recency_fun, gamma, authentication_cd, freq_auth))  
  
Example code includes:  
```{r}

load("./RInputFiles/transfers02_v2.RData")  # data.frame transfers is 628x12

# Print the first 6 rows of the dataset
head(transfers)

# Display the structure of the dataset
str(transfers)

# Determine fraction of legitimate and fraudulent cases
class_distribution <- prop.table(table(transfers$fraud_flag))
print(class_distribution)

# Make pie chart of column fraud_flag
df <- data.frame(class = c("no fraud", "fraud"), pct = as.numeric(class_distribution)) %>%
    mutate(class = factor(class, levels = c("no fraud", "fraud")),
           cumulative = cumsum(pct), midpoint = cumulative - pct / 2,
           label = paste0(class, " ", round(pct*100, 2), "%")
           )

ggplot(df, aes(x = 1, weight = pct, fill = class)) +
    scale_fill_manual(values = c("dodgerblue", "red")) +
    geom_bar(width = 1, position = "stack") +
    coord_polar(theta = "y") +
    geom_text(aes(x = 1.3, y = midpoint, label = label)) +
    ggmap::theme_nothing()


# Create vector predictions containing 0 for every transfer
predictions <- factor(rep(0, nrow(transfers)), levels = c(0, 1))

# Compute confusion matrix
caret::confusionMatrix(data = predictions, reference = transfers$fraud_flag)

# Compute cost of not detecting fraud
cost <- sum(transfers$amount[transfers$fraud_flag == 1])
print(cost)


# load("./RInputFiles/timestamps_circular.RData")  # 'circular' num[1:25] ts
load("./RInputFiles/timestamps_digital.RData")  # chr[1:25] timestamps

# Convert the plain text to hours
ts <- as.numeric(lubridate::hms(timestamps)) / 3600

# Convert the data to class circular
ts <- circular::circular(ts, units = "hours", template = "clock24")

# Plot a circular histogram
clock <- ggplot(data.frame(ts), aes(x = ts)) +
    geom_histogram(breaks = seq(0, 24), colour = "blue", fill = "lightblue") +
    coord_polar() + 
    scale_x_continuous("", limits = c(0, 24), breaks = seq(0, 24))
plot(clock)

# Create the von Mises distribution estimates
estimates <- circular::mle.vonmises(ts)

# Extract the periodic mean from the estimates
p_mean <- estimates$mu %% 24

# Add the periodic mean to the circular histogram
clock <- ggplot(data.frame(ts), aes(x = ts)) +
    geom_histogram(breaks = seq(0, 24), colour = "blue", fill = "lightblue") +
    coord_polar() + 
    scale_x_continuous("", limits = c(0, 24), breaks = seq(0, 24)) +
    geom_vline(xintercept = as.numeric(p_mean), color = "red", linetype = 2, size = 1.5)
plot(clock)


# Estimate the periodic mean and concentration on the first 24 timestamps
alpha <- 0.95
p_mean <- estimates$mu %% 24
concentration <- estimates$kappa

# Estimate densities of all 25 timestamps
densities <- circular::dvonmises(ts, mu = p_mean, kappa = concentration)

# Check if the densities are larger than the cutoff of 95%-CI
cutoff <- circular::dvonmises(circular::qvonmises((1 - alpha)/2, mu = p_mean, kappa = concentration), 
                              mu = p_mean, kappa = concentration
                              )

# Define the variable time_feature
time_feature <- densities >= cutoff
print(cbind.data.frame(ts, time_feature))


load("./RInputFiles/transfers_Bob.RData")  # data.frame trans_Bob 17x12

# Frequency feature based on channel_cd
frequency_fun <- function(steps, channel) {
    n <- length(steps)
    frequency <- sum(channel[1:n] == channel[n+1])
    return(frequency)
}

# Create freq_channel feature
freq_channel <- zoo::rollapply(trans_Bob$transfer_id, width = list(-1:-length(trans_Bob$transfer_id)),
                               partial = TRUE, FUN = frequency_fun, trans_Bob$channel_cd
                               )

# Print the features channel_cd, freq_channel and fraud_flag next to each other
freq_channel <- c(0, freq_channel)
cbind.data.frame(trans_Bob$channel_cd, freq_channel, trans_Bob$fraud_flag)


# load("./RInputFiles/transfers_AliceBob.RData")  # data.frame trans 40x12
load("./RInputFiles/transfers_AliceBob_freq.RData")  # data.frame trans 40x14

# Group the data
trans <- trans %>% group_by(account_name) %>%
    # Mutate the data to add a new feature
    mutate(freq_channel = c(0, zoo::rollapply(transfer_id, width = list(-1:-length(transfer_id)),
                                              partial = TRUE, FUN = frequency_fun, channel_cd
                                              )
                            )
           )

# Print the features as columns next to each other
as.data.frame(trans %>% select(account_name, channel_cd, freq_channel, fraud_flag))


# Create the recency function
recency_fun <- function(t, gamma, channel_cd, freq_channel) {
    n_t <- length(t)
    # If the channel has never been used, return 0 else, return the exponent
    if (freq_channel[n_t] == 0) { 
        return(0) 
    } else {
        time_diff <- t[1] - max(t[2:n_t][channel_cd[(n_t-1):1] == channel_cd[n_t]])
        exponent <- -gamma * time_diff
        return(exp(exponent))
    }
}

# Group, mutate and rollapply
gamma <- -log(0.01)/90
trans <- trans %>% 
    group_by(account_name) %>%
    mutate(rec_channel = zoo::rollapply(timestamp, width = list(0:-length(transfer_id)), 
                                        partial = TRUE, FUN = recency_fun, 
                                        gamma, channel_cd, freq_channel
                                        )
           )

# Print a new dataframe
as.data.frame(trans %>% 
                  select(account_name, channel_cd, timestamp, rec_channel, fraud_flag)
              )


load("./RInputFiles/transfers_chap1_L4.RData")  # data.frame transfers 222x16

# Statistics of frequency & recency features of legitimate transactions:
summary(transfers %>% 
            filter(fraud_flag==0) %>% 
            select(freq_channel, freq_auth, rec_channel, rec_auth)
        )

# Statistics of frequency & recency features of fraudulent transactions:
summary(transfers %>% 
            filter(fraud_flag==1) %>% 
            select(freq_channel, freq_auth, rec_channel, rec_auth)
        )

```
  
  
  
***
  
Chapter 2 - Social Network Analytics  
  
Social network analytics:  
  
* Social networks include both nodes and edges  
* Edges could include money transfers, and may be weighted based on frequency, amount, intensity, etc.  
	* Negative weights are rare, but can be used to show animosity  
    * Incoming and outgoing edges can be reflected using a directed network  
* Connectivity matrices can be valuable; for example, square matrix of 1s and 0s  
* Adjacency lists can also be used as a list of form (Node1, Node2, weight)  
* Can move from a transactional database to a network  
	* network <- igraph::graph_from_data_frame(transactions, directed = FALSE)  
    * plot(network)  
    * E(network)  
    * V(network)  
    * V(network)$name  
    * plot(net)  
    * E(net)$width <- count.multiple(net)  
    * edge_attr(net)  
    * E(net)$curved <- FALSE  
    * plot(net)  
  
Fraud and social network analytics:  
  
* Social network analytics can help improve fraud detection  
* Fraudsters tend to cluster together; same activities, resources, victims, etc.  
* Homophily is the concept that fraudsters are more likely to be connected to fraudsters  
* Social networks can be helpful for finding identity theft  
	* Legitimate customers tend to call their frequent contacts  
    * Fraudsters tend to call different people  
* Money mules transfer money illegally; can potentially be identified by nodes  
	* Assign colors to node by status, then add information to the network  
  
Social network based inference:  
  
* Can attempt to predict the behavior of a node based on the nodes elsewhere in the network  
	* Non-relational models - only use local information (logit, decision trees)  
    * Relational models - network links included (relational neighbor classifiers)  
* Assuming that there is homophily, then relational neighbor classifiers may be appropriate  
	* Can be as simple as proportion of known links belonging to each class (may be weighted by the strength of the edge)  
    * subnetwork <- subgraph(network, v = c("?", "B", "D"))  
    * prob_fraud <- strength(subnetwork, v = "?") / strength(network, v = "?")  
    * prob_fraud  
  
Social network metrics:  
  
* The geodesic is the shortest path between two nodes (may include weights in the calculation)  
* Generally, the closer to a fraudulent node, the greater the potential impact  
* The maximum degree possible for a network with N nodes is N-1 (normalizing means dividing the degree by N-1)  
* Closeness is the inverse of the sum of the distance of a node to all other nodes in the network  
	* Closeness is always at most 1 / (N-1) so normalizing is to multiply by (N-1)  
* Betweenness is the number of times that a node is in the geodesic for two other nodes  
	* betweenness(network)  
    * betweenness(network, normalized = TRUE)  
  
Example code includes:  
```{r}

load("./RInputFiles/network_data.RData")  # data.frame transfers 60x6 and data.frame account_info 38x2


library(igraph)

# Have a look at the data
head(transfers)

# Create an undirected network from the dataset
net <- graph_from_data_frame(transfers, directed = FALSE)

# Plot the network with the vertex labels in bold and black
plot(net, vertex.label.color = "black", vertex.label.font = 2)


load("./RInputFiles/network_data_simple.RData")  # data.frame edges 16x2

# Create a network from the data frame
net <- graph_from_data_frame(edges, directed = FALSE)

# Plot the network with the multiple edges
plot(net, layout = layout_in_circle)

# Specify new edge attributes width and curved
E(net)$width <- count.multiple(net)
E(net)$curved <- FALSE

# Check the new edge attributes and plot the network with overlapping edges
edge_attr(net)
plot(net, layout = layout_in_circle)


# Create an undirected network from the dataset
net <- graph_from_data_frame(transfers, directed = FALSE)

# Add account_type as an attribute to the nodes of the network
V(net)$account_type <- account_info$type

# Have a look at the vertex attributes
print(vertex_attr(net))

# Check for homophily based on account_type
assortativity_nominal(net, types = V(net)$account_type, directed = FALSE)


# Each account type is assigned a color
vertex_colors <- c("grey", "lightblue", "darkorange")

# Add attribute color to V(net) which holds the color of each node depending on its account_type
V(net)$color <- vertex_colors[V(net)$account_type]

# Plot the network
plot(net)


load("./RInputFiles/network_data_v2.RData")  # data.frame transfers 60x6 and data.frame account_info 38x3

# From data frame to graph
net <- graph_from_data_frame(transfers, directed = FALSE)

# Plot the network; color nodes according to isMoneyMule-variable
V(net)$color <- ifelse(account_info$isMoneyMule, "darkorange", "slateblue1")
plot(net, vertex.label.color = "black", vertex.label.font = 2, vertex.size = 18)

# Find the id of the money mule accounts
print(account_info$id[account_info$isMoneyMule])

# Create subgraph containing node "I41" and all money mules nodes
subnet <- induced_subgraph(net, v = c("I41", "I47", "I87", "I20"))

# Compute the money mule probability of node "I41" based on the neighbors
strength(subnet, v="I41") / strength(net, v="I41")


load("./RInputFiles/kite.RData")  # list[1:10] kite

# Plot network kite
plot(kite)

# Find the degree of each node
degree(kite)

# Which node has the largest degree?
which.max(degree(kite))

# Plot kite with vertex.size proportional to the degree of each node
plot(kite, vertex.size = 6 * degree(kite))

# Find the closeness of each node
closeness(kite)

# Which node has the largest closeness?
which.max(closeness(kite))

# Plot kite with vertex.size proportional to the closeness of each node
plot(kite, vertex.size = 500 * closeness(kite))

# Find the betweenness of each node
betweenness(kite)

# Which node has the largest betweenness?
which.max(betweenness(kite))

# Plot kite with vertex.size proportional to the betweenness of each node
plot(kite, vertex.size = 5 * betweenness(kite))


# Plot network and print account info
plot(net)
legend("bottomleft", legend = c("known money mule", "legit account"), 
       fill = c("darkorange", "lightblue"), bty = "n"
       )
print(account_info)

# Degree
account_info$degree <- degree(net, normalized = TRUE)

# Closeness
account_info$closeness <- closeness(net, normalized = TRUE)

# Betweenness
account_info$betweenness <- betweenness(net, normalized = TRUE)

print(account_info)

```
  
  
  
***
  
Chapter 3 - Imbalanced Class Distributions  
  
Dealing with imbalanced datasets:  
  
* Key challenge is labelling events as fraud or not (classification or anomaly detection)  
* Meaningful risk of a model that sees a very large class and classifies everything to that large class  
* Preference for a balanced distribution; tends to drive improved model performance  
	* Increase (over-sample) the number of fraud (minority) cases  
    * Decrease (under-sample) the number of normal (majority) cases  
* Example of random over-sampling of the dataset  
	* Increase the minority (fraud) cases in the training sample through randomly over-sampling the fraud data  
    * ROSE package: Random Over-Sampling Examples  
    * ovun.sample() for random over-sampling, under-sampling or combination!  
    * n_legit <- 24108  
    * new_frac_legit <- 0.50  
    * new_n_total <- n_legit/new_frac_legit # = 21408/0.50 = 42816  
    * library(ROSE)  
    * oversampling_result <- ovun.sample(Class ~ ., data = creditcard, method = "over", N = new_n_total, seed = 2018)  
    * oversampled_credit <- oversampling_result$data  
    * table(oversampled_credit$Class)  
  
Random under-sampling:  
  
* Can instead change the class ditribution through random under-sampling  
	* Remove some of the non-fraud cases rather than double-counting some of the fraud cases  
* Can again use ovun.sample for this task  
	* new_n_total <- n_fraud/new_frac_fraud # = 492/0.50 = 984  
    * undersampling_result <- ovun.sample(Class ~ ., data = creditcard, method = "under", N = new_n_total, seed = 2018)  
* Can also combine both over-sampling and under-sampling  
	* n_new <- nrow(creditcard) # = 24600  
    * fraction_fraud_new <- 0.50  
    * sampling_result <- ovun.sample(Class ~ ., data = creditcard, method = "both", N = n_new, p = fraction_fraud_new, seed = 2018)  
    * sampled_credit <- sampling_result$data  
  
Synthetic minority over-sampling:  
  
* SMOTE - Synthetic Minority Oversampling Technique  
	* Find the nearest k fraudulent neighbors of a minority class data point  
    * Randomly choose one of the k neighbors  
    * Choose a random number between 0 and 1  
    * Linear combination of the minority class data point and the randomly chosen neighbor, using the random number above as the weighting  
    * Repeat the process "dup_size" number of times  
* Can run the process using the "smotefamily" library  
	* library(smotefamily)  
    * smote_output = SMOTE(X = transfer_data[, -1], target = transfer_data$isFraud, K = 4, dup_size = 10)  
    * oversampled_data = smote_output$data  
  
From dataset to detection model:  
  
* General SMOTE modeling process  
	* Divide data in to train and test  
    * Run SMOTE on the training data only  
    * Train the model on the SMOTE-balanced dataset  
    * Test performance on the test dataset  
* Example of running using SMOTE  
	* smote_result = SMOTE(X = train[, -17], target = train$Class, K = 5, dup_size = 10)  
    * train_oversampled = smote_result$data  
    * colnames(train_oversampled)[17] = "Class"  
    * model2 = rpart::rpart(Class ~ ., data = train_oversampled)  
    * scores2 = predict(model2, newdata = test, type = "prob")[, 2]  
    * predicted_class2 = factor(ifelse(scores2 > 0.5, 1, 0))  
    * CM2 = caret::confusionMatrix(data = predicted_class2, reference = test$Class)  
    * auc(roc(response = test$Class, predictor = scores2))  
* Modeling can be based on costs - associated with fraud, and associated with wrongly flagging fraud  
	* cost_model <- function(predicted.classes, true.classes, amounts, fixedcost) {  
    *     cost <- sum(true.classes * (1 - predicted.classes) * amounts + predicted.classes * fixedcost)
    *     return(cost)  
    * }  
    * cost_model(predicted_class1, test$Class, test$Amount, fixedcost = 10)  
    * cost_model(predicted_class2, test$Class, test$Amount, fixedcost = 10)  
  
Example code includes:  
```{r}

load("./RInputFiles/transfers02_v2.RData")  # data.frame transfers is 628x12

# Make a scatter plot
ggplot(transfers, aes(x = amount, y = orig_balance_before)) +
    geom_point(aes(color = fraud_flag, shape = fraud_flag)) +
    scale_color_manual(values = c('dodgerblue', 'red'))


load("./RInputFiles/creditcard5.RData")  # data.frame creditcard is 9840x32

# Calculate the required number of cases in the over-sampled dataset
n_new <- sum(creditcard$Class==0) / (1-0.3333)

# Over-sample
oversampling_result <- ROSE::ovun.sample(formula = Class ~ ., data = creditcard, 
                                         method = "over", N = n_new, seed = 2018
                                         )

# Verify the Class-balance of the over-sampled dataset
oversampled_credit <- oversampling_result$data
prop.table(table(oversampled_credit$Class))


# Calculate the required number of cases in the over-sampled dataset
n_new <- sum(creditcard$Class == 1) / (0.4)

# Under-sample
undersampling_result <- ROSE::ovun.sample(formula = Class ~ ., data = creditcard,
                                          method = "under", N = n_new, seed = 2018
                                          )

# Verify the Class-balance of the under-sampled dataset
undersampled_credit <- undersampling_result$data
prop.table(table(undersampled_credit$Class))


# Specify the desired number of cases in the balanced dataset and the fraction of fraud cases
n_new <- 10000
fraud_fraction <- 0.3

# Combine ROS & RUS!
sampling_result <- ROSE::ovun.sample(formula = Class ~ ., data = creditcard, method = "both", 
                                     N = n_new,  p = fraud_fraction, seed = 2018
                                     )

# Verify the Class-balance of the re-balanced dataset
sampled_credit <- sampling_result$data
prop.table(table(sampled_credit$Class))


# Set the number of fraud and legitimate cases, and the desired percentage of legitimate cases
n0 <- sum(creditcard$Class==0)
n1 <- sum(creditcard$Class==1)
r0 <- 0.6

# Calculate the value for the dup_size parameter of SMOTE
ntimes <- ((1 - r0) / r0) * (n0 / n1) - 1

# Create synthetic fraud cases with SMOTE
smote_output <- smotefamily::SMOTE(X = creditcard[ , -c(1, 31, 32)], target = creditcard$Class, 
                                   K = 5, dup_size = ntimes
                                   )

# Make a scatter plot of the original and over-sampled dataset
credit_smote <- smote_output$data
colnames(credit_smote)[30] <- "Class"
prop.table(table(credit_smote$Class))

ggplot(creditcard, aes(x = V1, y = V2, color = Class)) +
    geom_point() +
    scale_color_manual(values = c('dodgerblue2', 'red'))

ggplot(credit_smote, aes(x = V1, y = V2, color = Class)) +
    geom_point() +
    scale_color_manual(values = c('dodgerblue2', 'red'))


set.seed(1903172344)
testIdx <- sort(sample(1:nrow(creditcard), round(0.5*nrow(creditcard)), replace=FALSE))
test <- creditcard[testIdx, ]
train_original <- creditcard[-testIdx, ]

n_new <- 7380
fraud_fraction <- 0.3
train_oversampled <- ROSE::ovun.sample(formula = Class ~ ., data = train_original, 
                                       method = "over", N = n_new, seed = 2018
                                       )$data

# Train the rpart algorithm on the original training set and the SMOTE-rebalanced training set
model_orig <- rpart::rpart(Class ~ ., data = train_original)
model_smote <- rpart::rpart(Class ~ ., data = train_oversampled)

# Predict the fraud probabilities of the test cases
scores_orig <- predict(model_orig, newdata = test, type = "prob")[, 2]
scores_smote <- predict(model_smote, newdata = test, type = "prob")[, 2]

# Convert the probabilities to classes (0 or 1) using a cutoff value
predicted_class_orig <- factor(ifelse(scores_orig > 0.5, 1, 0))
predicted_class_smote <- factor(ifelse(scores_smote > 0.5, 1, 0))

# Determine the confusion matrices and the model's accuracy
CM_orig <- caret::confusionMatrix(data = predicted_class_orig, reference = test$Class)
CM_smote <- caret::confusionMatrix(data = predicted_class_smote, reference = test$Class)
print(CM_orig$table)
print(CM_orig$overall[1])
print(CM_smote$table)
print(CM_smote$overall[1])


cost_model <- function(predicted.classes, true.classes, amounts, fixedcost) {
    predicted.classes <- hmeasure::relabel(predicted.classes)
    true.classes <- hmeasure::relabel(true.classes)
    cost <- sum(true.classes * (1 - predicted.classes) * amounts + predicted.classes * fixedcost)
    return(cost)
}

# Calculate the total cost of deploying the original model
cost_model(predicted_class_orig, test$Class, test$Amount, fixedcost=10)

# Calculate the total cost of deploying the model using SMOTE
cost_model(predicted_class_smote, test$Class, test$Amount, fixedcost=10)

```
  
  
  
***
  
Chapter 4 - Digit Analysis and Robust Statistics  
  
Digit analysis using Benford's Law:  
  
* Benford's Law considers the left-most digit in a number  
	* The distribution will often have 30% 1s with only 4.6% 9s  
    * Generally, frequencies will be proportional to log10(1 + 1/d) where d is the digit  
    * Pinkham discovered that Benford's Law is scale invariant (e.g., Euros to pesos should preserve the distribution)  
* Example of Benford's Law in code  
	* benlaw <- function(d) log10(1 + 1 / d)  
    * df <- data.frame(digit = 1:9, probability = benlaw(1:9))  
    * ggplot(df, aes(x = digit, y = probability)) + geom_bar(stat = "identity", fill = "dodgerblue") + xlab("First digit") + ylab("Expected frequency") + scale_x_continuous(breaks = 1:9, labels = 1:9) + ylim(0, 0.33) + theme(text = element_text(size = 25))  
    * n <- 1000  
    * fibnum <- numeric(len)  
    * fibnum[1] <- 1  
    * fibnum[2] <- 1  
    * for (i in 3:n) { fibnum[i] <- fibnum[i-1]+fibnum[i-2] }  
    * pow2 <- 2^(1:n)  
    * library(benford.analysis)  
    * bfd.fib <- benford(fibnum, number.of.digits = 1)  
    * plot(bfd.fib)  
    * bfd.pow2 <- benford(pow2, number.of.digits = 1)  
    * plot(bfd.pow2)  
  
Benford's Law for fraud detection:  
  
* Many datasets follow Benford's law  
	* data where numbers represent sizes of facts or events  
    * data in which numbers have no relationship to each other  
    * data sets that grow exponentially or arise from multiplicative fluctuations  
    * mixtures of different data sets  
    * Some well-known infinite integer sequences  
* Fraud is typically committed by changing numbers, and changes often fail to conform with Benford's Law  
* Caution that many types of data can never be expected to comply with Benford's Law  
	* If there is lower and/or upper bound or data is concentrated in narrow interval, e.g. hourly wage rate, height of people  
    * If numbers are used as identification numbers or labels, e.g. social security number, flight numbers, car license plate numbers, phone numbers  
    * Additive fluctuations instead of multiplicative fluctuations, e.g. heartbeats on a given day  
* Benford's Law can be applied for the first two digits of a series of numbers  
	* P(d1d2) = log10(1 + 1/(d1d2))  # note that this is NOT multiplicative; d1d2 for d1=1 and d2=2 is 12, not 2  
    * bfd.cen <- benford(census.2009$pop.2009,number.of.digits = 2)  
    * plot(bfd.cen)  
  
Detecting univariate outliers:  
  
* Can use global statistics to detect outliers; not all outliers are fraudulent, so follow-up and validation are needed  
* One popular tool for outlier detection is the z-score - (x - mu) / sigma  
	* One challenge is that the sample mean and sample standard deviation are artificially driven by an extreme outlier  
* "Robust statistics" attempt to flag outliers better than the classical z-scores  
	* Classical statistical methods rely on (normality) assumptions, but even single outlier can influence conclusions significantly and may lead to misleading results  
    * Robust statistics produce also reliable results when data contains outliers and yield automatic outlier detection tools  
    * It is perfect to use both classical and robust methods routinely, and only worry when they differ enough to matter... But when they differ, you should think hard. J.W. Tukey (1979)  
* The median is more robust than the mean, while the median-absolute-deviation (MAD) and IQR are more robust than the standard deviation  
* The Boxplot is a common way to identify outliers  
	* ggplot(data.frame(los), aes(x = "", y = los)) + geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 3, fill = "lightblue", width = 0.5) + xlab("") + ylab("Length Of Stay (LOS)") + theme(text = element_text(size = 25))  
* The boxplot has assumptions of normality, even though in reality data may not be normally distributed  
	* At asymmetric distributions, boxplot may flag many regular points as outliers  
    * The skewness-adjusted boxplot corrects for this by using a robust measure of skewness in determining the fence  
    * library(robustbase)  
    * adjbox_stats <- adjboxStats(los)$stats  
    * ggplot(data.frame(los), aes(x = "", y = los)) +  
    *     stat_boxplot(geom = "errorbar", width = 0.2, coef = 1.5*exp(3*mc(los))) +  
    *     geom_boxplot(ymin = adjbox_stats[1], ymax = adjbox_stats[5], middle = adjbox_stats[3], upper = adjbox_stats[4], lower = adjbox_stats[2], outlier.shape = NA, fill = "lightblue", width = 0.5) +  
    *     geom_point(data=subset(data.frame(los), los < adjbox_stats[1] | los > adjbox_stats[5]), col = "red", size = 3, shape = 16) + xlab("") +  
    *     ylab("Length Of Stay (LOS)") +  
    *     theme(text = element_text(size = 25))  
    * adjbox(los,col="lightblue", ylab="LOS data")$out  
  
Detecting multivariate outliers:  
  
* Multivariate outliers have unusual combinations of data across multiple dimensions  
* Example of animals data - brain vs. body  
	* X <- data.frame(body = log(Animals$body), brain = log(Animals$brain))  
    * fig <- ggplot(X, aes(x = body, y = brain)) + geom_point(size = 5) + xlab("log(body)") + ylab("log(brain)") + ylim(-5, 15) + scale_x_continuous(limits = c(-10, 16), breaks = seq(-15, 15, 5)))  
* Mahalanobis distance is a multidimensional distance (distance on each axis is scaled based on length of ellipse in that direction  
	* Classical Mahalanobis distances : sample mean as estimate for location and sample covariance matrix as estimate for scatter  
    * To detect multivariate outliers the mahalanobis distance is compared with a cut-off value, which is derived from the chisquare distribution  
    * In two dimensions we can construct corresponding 97.5% tolerance ellipsoid, which is defined by those observations whose Mahalanobis distance does not exceed the cut-off value  
* Extending the Mahalanobis distance to the animal data  
	* animals.clcenter <- colMeans(X)  
    * animals.clcov <- cov(X)  
    * rad <- sqrt(qchisq(0.975, df = ncol(X)))  
    * library(car)  
    * ellipse.cl <- data.frame(ellipse(center = animals.clcenter, shape = animals.clcov,radius = rad, segments = 100, draw = FALSE))  
    * colnames(ellipse.cl) <- colnames(X)  
    * fig <- fig + geom_polygon(data=ellipse.cl, color = "dodgerblue", fill = "dodgerblue", alpha = 0.2) + geom_point(aes(x = animals.clcenter[1], y = animals.clcenter[2]), color = "blue", size = 6)  
    * fig  
* Can improve the Mahalanobis distance with a more robust estimate of location and scatter  
	* Minimum Covariance Determinant (MCD) estimator of Rousseeuw is a popular robust estimator of multivariate location and scatter  
    * MCD looks for those hh observations whose classical covariance matrix has the lowest possible determinant  
    * MCD estimate of location is then mean of these hh observations  
    * MCD estimate of scatter is then sample covariance matrix of these hh points (multiplied by consistency factor)  
    * Reweighting step is applied to improve efficiency at normal data  
    * Computation of MCD is difficult, but several fast algorithms are proposed  
* Examples of using robust statistics on the animals data  
	* library(robustbase)  
    * animals.mcd <- covMcd(X)  
    * animals.mcd$center  
    * animals.mcd$cov  
    * library(robustbase)  
    * animals.mcd <- covMcd(X)  
    * ellipse.mcd <- data.frame(ellipse(center = animals.mcd$center, shape = animals.mcd$cov, radius=rad, segments=100, draw=FALSE))  
    * colnames(ellipse.mcd) <- colnames(X)  
    * fig <- fig + geom_polygon(data=ellipse.mcd, color="red", fill="red", alpha=0.3) + geom_point(aes(x = animals.mcd$center[1], y = animals.mcd$center[2]), color = "red", size = 6)  
    * fig  
* The distance-distance plot is a common alternative when the number of dimensions is 4+  
	* When p>3 it is not possible to visualize the tolerance ellipsoid  
    * The distance-distance plot shows the robust distance of each observation versus its classical Mahalanobis distance, obtained immediately from MCD object  
    * plot(animals.mcd, which = "dd")  
  
Example code includes:  
```{r}

# Implement Benford's Law for first digit
benlaw <- function(d) log10(1 + 1 / d)

# Calculate expected frequency for d=5
benlaw(d=5)

# Create a dataframe of the 9 digits and their Benford's Law probabilities
df <- data.frame(digit = 1:9, probability = benlaw(1:9))

# Create barplot with expected frequencies
ggplot(df, aes(x = digit, y = probability)) + 
    geom_bar(stat = "identity", fill = "dodgerblue") + 
    xlab("First digit") + 
    ylab("Expected frequency") + 
    scale_x_continuous(breaks = 1:9, labels = 1:9) + 
    ylim(0, 0.33) + 
    theme(text = element_text(size = 25))


data(census.2009, package="benford.analysis")

# Check conformity
bfd.cen <- benford.analysis::benford(census.2009$pop.2009, number.of.digits = 1) 
plot(bfd.cen, except = c("second order", "summation", "mantissa", "chi squared", 
                         "abs diff", "ex summation", "Legend"), 
     multiple = F
     ) 

# Multiply the data by 3 and check conformity again
data <- census.2009$pop.2009 * 3
bfd.cen3 <- benford.analysis::benford(data, number.of.digits=1)
plot(bfd.cen3, except = c("second order", "summation", "mantissa", "chi squared", 
                          "abs diff", "ex summation", "Legend"), 
     multiple = F
     )


load("./RInputFiles/fireinsuranceclaims.RData")  # num[1:40000] fireinsuranceclaims

# Validate data against Benford's Law using first digit
bfd.ins <- benford.analysis::benford(fireinsuranceclaims, number.of.digits = 1) 
plot(bfd.ins, except=c("second order", "summation", "mantissa", "chi squared",
                       "abs diff", "ex summation", "Legend"), 
     multiple = F
     )

# Validate data against Benford's Law using first-two digits
bfd.ins2 <- benford.analysis::benford(fireinsuranceclaims, number.of.digits = 2)
plot(bfd.ins2, except=c("second order", "summation", "mantissa", "chi squared",
                        "abs diff", "ex summation", "Legend"), 
     multiple = F
     )


load("./RInputFiles/expensesCEO.RData")  # num[1:988] expensesCEO

# Validate data against Benford's Law using first digit
bfd.exp <- benford.analysis::benford(expensesCEO, number.of.digits = 1) 
plot(bfd.exp, except=c("second order", "summation", "mantissa", "chi squared",
                       "abs diff", "ex summation", "Legend"), 
     multiple = F
     )

# Validate data against Benford's Law using first-two digits
bfd.exp2 <- benford.analysis::benford(expensesCEO, number.of.digits = 2) 
plot(bfd.exp2, except=c("second order", "summation", "mantissa", "chi squared",
                        "abs diff", "ex summation", "Legend"), 
     multiple = F
     )


load("./RInputFiles/transfers_chap1_L4.RData")  # data.frame transfers 222x16

# Get observations identified as fraud
which(transfers$fraud_flag == 1)

# Compute median and mean absolute deviation for `amount`
m <- median(transfers$amount)
s <- mad(transfers$amount)

# Compute robust z-score for each observation
robzscore <- abs((transfers$amount - m) / (s))

# Get observations with robust z-score higher than 3 in absolute value
which(abs(robzscore) > 3)


thexp <- c(40517, 33541, 5182, 40385, 40302, 23189, 13503, 5110, 15754, 40763, 23061, 30839, 25206, 15891, 38821, 11766, 4934, 13754, 14142, 27813, 21005, 11511, 41750, 32855, 62043, 19415, 51815, 26961, 19185, 19704, 21831, 40768, 49079, 12766, 13030, 8841, 17943, 6214, 21114, 7898, 30707, 69698, 70155, 15032, 55858, 31747, 11562, 12390, 7016, 96396, 24614, 22735, 20483, 36907, 31822, 13619, 34401, 10281, 32165, 52226, 13941, 40850, 15270, 21143, 26029, 10209, 10950, 19745, 54153, 33668, 7562, 34231, 34219, 25784, 52952, 32959, 17459, 25611, 14998, 36229, 26485, 20563, 41865, 29821, 26792, 42406, 20083, 10205, 31353, 33674, 13523, 51835, 18136, 54736, 33499, 95389, 44967, 67707, 40879, 17729, 15643, 15648, 19150, 9789, 27978, 40469, 30696, 48195, 12817, 10527, 42946, 72281, 13773, 17189, 14340, 47962, 29063, 34477, 84354, 37943, 13584, 12184, 49563, 36263, 18313, 25399, 50235, 14230, 25617, 18226, 31542, 24262, 17617, 22068, 43534, 14574, 6471, 57500, 8535, 85065, 22749, 10481, 42094, 24436, 27975, 28347, 32929, 20106, 30992, 22202, 17005, 29900, 16871, 10790, 38355, 10315, 7782, 16084, 11788, 20005, 70859, 21706, 11929, 69816, 6351, 27217, 30178, 10597, 13715, 13687, 17116, 27426, 56579, 31655, 86577, 27051, 10477, 11178, 49785, 12626, 44817, 15758, 21396, 5590, 40538, 38834, 32693, 47330, 17823, 92957, 44439, 27188, 22972, 40020, 33067, 24562, 8408, 36088, 8823, 8022, 36395, 14523, 49188, 19744, 17536, 32456, 38400, 6451, 31766, 24727, 60013, 15664, 17356, 50482, 20752, 28048, 10932, 35337, 18755, 6572, 16065, 67257, 36303, 14846, 50468, 27237, 7165, 38067, 22040, 23794, 106032, 19303, 63934, 16818, 11621, 40566, 14921, 15188, 28087, 21026, 38907, 39727, 49794, 49112, 6886, 31674, 25053, 23835, 12160, 45640, 7898, 107065, 58206, 8270, 69529, 25776, 57742, 18456, 53523, 27514, 14089, 7291, 4727, 7319, 22650, 8462, 14980, 39085, 13627, 14998, 29686, 22750, 25487, 40127, 22844, 7597, 19265, 14869, 33010, 74958, 30320, 16602, 36376, 16467, 26946, 26870, 33433, 61134, 20121, 58389, 61594, 24150, 32594, 20177, 3133, 22330, 16905, 25053, 10837, 20807, 6647, 29696, 34457, 78286, 21551, 26533, 52237, 58745, 19607, 25062, 23823, 108756, 12067, 28813, 17803, 35795, 20860, 14307, 11991, 71409, 74111, 25916, 25914, 14092, 24780, 43417, 12767, 43919, 28205, 34075, 68173, 28509, 78760, 52329, 31858, 54088, 6670, 29371, 30066, 16554, 13866, 13806, 40504, 49841, 19729, 30881, 35484, 11373, 10624, 7544, 49465, 12499, 18316, 27963, 31601, 52243, 48927, 42339, 34707, 13034, 9452, 69461, 64335, 9964, 8993, 11217, 116262, 16693, 28622, 35251, 8587, 28414, 15552, 49460, 57721, 16398, 20597, 28592, 13795, 19534, 37065, 22847, 10679, 37552, 63611, 12023, 19659, 16040, 63519, 56897, 55381, 44055, 65130, 8251, 4857, 6225, 74197, 20986, 47412, 35014, 39263, 19486, 22648, 7826, 52697, 9196, 31890, 8836, 62723, 60814, 42018, 20671, 7444, 21584, 13213, 23959, 16394, 21742, 71587, 16866, 63788, 39375, 37087, 9370, 37440, 10666, 60304, 34391, 55988, 9921, 11709, 69198, 49491, 39201, 10065, 32063, 10380, 18818, 35576, 15695, 30336, 74993, 48089, 24255, 56063, 11932, 23251, 9537, 7757, 67473, 44949, 11842, 12681, 36156, 71957, 37576, 36120, 53232, 6743, 36142, 62565, 49525, 33161, 28272, 24179, 15227, 46151, 32764, 32374, 22462, 9593, 9442, 13202, 24634, 35284, 44800, 11044, 45655, 5502, 18124, 18007, 39303, 14012, 26558, 10926, 25389, 16447, 8720, 12977, 12942, 25020, 6430, 18919, 24916, 55667, 51081, 99834, 31840, 147498)
thexp <- c(thexp, 27913, 40044, 24278, 41614, 50038, 54307, 27598, 18022, 34789, 24503, 51856, 12160, 19107, 40845, 46171, 66460, 22241, 15245, 10925, 17671, 5666, 25305, 9394, 20357, 32948, 7942, 14555, 24012, 25235, 17292, 81646, 46737, 15012, 49861, 10012, 15076, 7693, 18513, 46293, 50770, 36122, 25598, 8633, 15568, 16954, 47036, 38076, 18458, 8092, 38576, 28692, 27211, 37485, 15162, 32968, 55021, 7060, 16714, 34581, 14939, 27056, 15090, 56905, 29528, 21282, 39487, 24239, 35466, 21982, 10334, 15133, 41591, 23260, 12882, 32149, 16219, 41605, 8346, 8549, 23818, 36217, 42766, 11239, 59532, 31806, 52218, 73118, 31701, 32761, 18745, 17949, 8017, 12833, 25583, 36468, 8706, 94587, 42900, 74298, 17201, 25618, 14888, 16308, 75043, 68056, 50797, 15956, 13820, 13985, 22742, 17692, 30214, 57582, 17273, 31885, 14307, 22597, 46389, 23366, 35128, 51769, 9251, 35663, 34474, 18748, 60091, 31137, 14366, 25347, 32175, 16065, 16672, 45192, 42039, 19665, 24933, 29570, 23400, 13517, 23993, 18140, 9545, 16042, 24425, 28400, 25035, 28316, 19001, 27203, 8016, 15199, 14069, 12037, 30455, 13877, 10696, 11010, 41384, 37241, 38328, 54434, 27174, 14015, 27354, 12944, 19718, 21558, 22239, 31076, 32940, 17810, 13462, 16122, 20417, 36205, 9871, 11892, 50737, 32511, 29767, 17032, 40276, 24005, 33884, 16278, 11326, 7187, 50434, 70436, 38353, 27723, 32811, 14833, 28465, 83158, 18866, 21823, 39125, 45372, 33933, 29469, 79147, 24190, 37007, 4259, 41346, 25087, 27216, 32780, 21190, 29067, 11316, 36103, 15389, 27257, 26051, 22853, 10551, 6661, 15878, 17131, 18220, 12045, 10573, 34645, 19517, 13933, 14452, 33694, 35605, 48376, 19567, 35762, 12931, 6286, 25321, 22167, 24243, 19433, 26852, 25802, 26647, 26423, 11537, 16011, 56547, 22690, 20391, 10487, 16994, 25690, 19260, 57525, 17802, 28135, 14365, 21640, 20817, 55771, 31693, 31859, 27496, 39715, 22775, 27933, 58875, 39133, 7604, 29409, 29296, 44377, 33107, 21235, 59129, 33427, 10164, 14595, 4744, 49674, 27827, 48830, 36196, 24979, 47800, 108752, 52300, 38343, 19381, 35881, 43688, 32938, 13341, 29297, 38603, 30202, 14797, 38529, 29055, 61303, 27109, 53496, 16665, 65132, 23903, 36096, 21247, 42292, 11176, 7542, 15210, 5289, 58444, 33295, 30456, 60595, 59624, 19642, 13317, 9262, 17611, 35079, 45469, 59510, 26852, 51484, 20195, 27751, 33555, 27692, 70407, 18102, 130773, 16637, 60463, 11653, 19275, 47114, 6117, 29645, 57846, 51033, 11790, 24970, 32391, 19278, 27778, 19596, 17761, 56884, 66230, 40617, 80495, 27704, 22815, 23390, 18092, 13037, 27954, 6979, 6942, 46155, 34240, 24484, 22375, 45916, 32788, 28017, 31922, 25357, 7314)

# Create boxplot
bp.thexp <- boxplot(thexp, col = "lightblue", main = "Standard boxplot", 
                    ylab = "Total household expenditure"
                    )

# Extract the outliers from the data
bp.thexp$out

# Create adjusted boxplot
adj.thexp <- robustbase::adjbox(thexp, col = "lightblue", main = "Adjusted boxplot", 
                                ylab = "Total household expenditure"
                                )


load("./RInputFiles/hailinsurance.RData")  # matrix num[1:100, 1:2] hailinsurance

# Create a scatterplot
plot(hailinsurance, xlab = "price house", ylab = "claim")

# Compute the sample mean and sample covariance matrix
clcenter <- colMeans(hailinsurance)
clcov <- cov(hailinsurance)

# Add 97.5% tolerance ellipsoid
rad <- sqrt(qchisq(0.975, df=ncol(hailinsurance)))
car::ellipse(center = clcenter, shape = clcov, radius = rad,col = "blue", lty = 2)


# Create a scatterplot of the data
plot(hailinsurance, xlab = "price house", ylab = "claim")

# Compute robust estimates for location and scatter
mcdresult <- robustbase::covMcd(hailinsurance)
robustcenter <- mcdresult$center
robustcov <- mcdresult$cov

# Add robust 97.5% tolerance ellipsoid
rad <- sqrt(qchisq(0.975, df=ncol(hailinsurance)))
car::ellipse(center = robustcenter, shape = robustcov, radius = rad, col = "red")

```
  
  
  
***
  
### _Dimensionality Reduction in R_  
  
Chapter 1 - Principal Component Analysis  
  
The curse of dimensionality:  
  
* The "curse of dimensionality" can make it challenging to understand the structure of a data set  
	* As the dimensionalities of the data grow, the feature space grows rapidly  
    * Big computational cost to handle high-dimensional data  
    * Estimation accuracy decreases  
    * Difficult interpretation of the data  
* Example of using the mtcars dataset (11 features)  
	* Most of the dimensions could probably be reduced due to a small set of latent dimensions, such as:  
    * the size of the car or  
    * the country of origin or  
    * the construction year  
* Need to convert any factor variables (e.g., cylinders) and then run correlations  
	* mtcars$cyl <- as.numeric(as.character(mtcars$cyl))  
    * mtcars_correl <- cor(mtcars, use = "complete.obs")  
    * library(ggcorrplot)  
    * ggcorrplot(mtcars_correl)  
* Several options for dealing with the curse of dimensionality  
	* Feature Engineering: Requires domain knowledge  
    * Remove redundancy  
  
Getting PCA to work with FactoMineR:  
  
* PCA removes noise introduced by correlation, changes the coordinate dimensions to best explain variability in the data, etc.  
	* Center and standardize  
    * Rotation and projection  
    * Projection and reduction  
* The screeplot can be helpful for finding elbow points for percentage of explained variance  
* Can implement using prcomp() or FactoMineR::PCA() in R  
	* mtcars_pca <- prcomp(mtcars)  
    * mtcars_pca <- FactoMineR::PCA(mtcars)  
* The eigenvalues and factor map can help with interpreting the outputs  
	* mtcars_pca$eig  
    * mtcars_pca$var$cos2  # squared cosine (closer to 1 is better)  
    * mtcars_pca$var$contrib  # contribution of each variable to the components  
    * dimdesc(mtcars_pca)  
  
Interpreting and visualizing PCA models:  
  
* Can plot the contributions of the individual principal components  
	* fviz_pca_var(mtcars_pca, col.var = "contrib", gradient.cols = c("#bb2e00", "#002bbb"), repel = TRUE)  
    * fviz_pca_var(mtcars_pca, select.var = list(contrib = 4), repel = TRUE)  
    * fviz_contrib(mtcars_pca, choice = "var", axes = 1, top = 5)  
    * fviz_pca_ind(mtcars_pca, col.ind="cos2", gradient.cols = c("#bb2e00", "#002bbb"), repel = TRUE)  
    * fviz_pca_ind(mtcars_pca, select.ind = list(cos2 = 0.8), gradient.cols = c("#bb2e00", "#002bbb"), repel = TRUE)  
    * fviz_cos2(mtcars_pca, choice = "ind", axes = 1, top = 10)  
    * fviz_pca_biplot(mtcars_pca)  
* Can add ellispoids to the data  
	* mtcars$cyl <- as.factor(mtcars$cyl)  
    * fviz_pca_ind(mtcars_pca, label="var", habillage=mtcars$cyl, addEllipses=TRUE)  
  
Example code includes:  
```{r}

cars <- as.data.frame(data.table::fread("./RInputFiles/04carsdata.csv"))

rowsDelete <- c(59, 65, 71, 83, 84, 85, 108, 109, 116, 119, 124, 127, 128, 138, 143, 146, 147, 148, 205, 239, 240, 244, 245, 247, 248, 256, 291, 293, 295, 296, 297, 304, 315, 321, 325, 355, 399, 400, 401, 414, 415)
cars$`Vehicle Name`[rowsDelete]

rowsMod <- c(182, 183, 252, 253, 255, 256)
cars$`Vehicle Name`[rowsMod]

rowNames <- cars$`Vehicle Name`
rowNames[182] <- paste0(rowNames[182], " RWD")
rowNames[183] <- paste0(rowNames[183], " AWD")
rowNames[252] <- paste0(rowNames[252], " RWD")
rowNames[253] <- paste0(rowNames[253], " AWD")
rowNames[255] <- paste0(rowNames[255], " RWD")
rowNames[256] <- paste0(rowNames[256], " AWD")

row.names(cars) <- rowNames
cars <- cars[-rowsDelete, ]

cars$`Vehicle Name` <- NULL
cars$type <- factor(c(3, 3, 5, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 4, 6, 3, 4, 4, 4, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 4, 4, 3, 3, 3, 3, 3, 5, 3, 3, 5, 3, 3, 3, 5, 3, 5, 4, 1, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 6, 3, 3, 5, 5, 5, 5, 1, 3, 3, 3, 3, 3, 4, 6, 3, 3, 3, 3, 3, 3, 1, 1, 5, 1, 5, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 3, 3, 3, 6, 3, 3, 1, 4, 4, 3, 6, 3, 4, 5, 1, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 3, 1, 1, 5, 4, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 4, 3, 3, 6, 6, 3, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 5, 5, 5, 3, 3, 3, 3, 6, 1, 5, 3, 3, 3, 5, 5, 5, 3, 3, 3, 5, 3, 3, 6, 3, 5, 5, 4, 5, 3, 3, 3, 3, 5, 3, 3, 3, 1, 4, 4, 5, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 6, 3, 5, 5, 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 1, 5, 6, 3, 3, 3, 3, 3, 4, 4, 5, 3, 4, 5, 5, 4, 4, 3, 3, 3, 3, 6, 5, 5, 1, 1, 3, 3, 3, 5, 3, 3, 1, 5, 3, 3, 3, 1, 1, 3, 3, 6, 4, 4, 4, 4, 4, 4, 5, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 3, 5, 3, 6, 6, 3, 4, 4, 3, 3, 6, 3, 3, 3, 3, 3, 6, 3, 3, 3, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 5, 5, 6, 4, 3, 5, 5, 1, 1, 3, 3, 6, 3, 3, 3, 3, 6, 3, 3, 6, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 5), levels=c(1, 2, 3, 4, 5, 6), labels=c('Minivan', 'Pickup', 'Small.Sporty..Compact.Large.Sedan', 'Sports.Car', 'SUV', 'Wagon'))
cars$wheeltype <- factor(c(1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1), levels=c(1, 2), labels=c("AWD", "RWD"))

colNames <- c('Small.Sporty..Compact.Large.Sedan', 'Sports.Car', 'SUV', 'Wagon', 'Minivan', 'Pickup', 'AWD', 'RWD', 'Retail.Price', 'Dealer.Cost', 'Engine.Size..l.', 'Cyl', 'HP', 'City.MPG', 'Hwy.MPG', 'Weight', 'Wheel.Base', 'Len', 'Width', 'type', 'wheeltype')
for (intCtr in seq_along(colNames)) {
    cat("Original Name: ", names(cars)[intCtr], " ---> New Name: ", colNames[intCtr], "\n")
}
names(cars) <- colNames

cars <- cars %>% mutate(City.MPG=as.integer(City.MPG), Hwy.MPG=as.integer(Hwy.MPG), 
                        Weight=as.integer(Weight), Wheel.Base=as.integer(Wheel.Base), 
                        Len=as.integer(Len), Width=as.integer(Width)
                        )
str(cars)


# Explore cars with summary()
summary(cars)

# Get the correlation matrix with cor()
correl <- cor(cars[,9:19], use = "complete.obs")

# Use ggcorrplot() to explore the correlation matrix
ggcorrplot::ggcorrplot(correl)

# Conduct hierarchical clustering on the correlation matrix
ggcorrplot_clustered <- ggcorrplot::ggcorrplot(correl, hc.order = TRUE, type = "lower")
ggcorrplot_clustered


# Run a PCA for the 10 non-binary numeric variables of cars
pca_output_ten_v <- FactoMineR::PCA(cars[,9:19], ncp = 4, graph = F)

# Get the summary of the first 100 cars
summary(pca_output_ten_v, nbelements = 100)

# Get the variance of the first 3 new dimensions
pca_output_ten_v$eig[,2][1:3]

# Get the cumulative variance
pca_output_ten_v$eig[,3][1:3]


# Run a PCA with active and supplementary variables
pca_output_all <- FactoMineR::PCA(cars, quanti.sup = 1:8, quali.sup = 20:21, graph = F)

# Get the most correlated variables
FactoMineR::dimdesc(pca_output_all, axes = 1:2)

# Run a PCA on the first 100 car categories
pca_output_hundred <- FactoMineR::PCA(cars, quanti.sup = 1:8, quali.sup = 20:21, 
                                      ind.sup = 1:100, graph = F
                                      )

# Trace variable contributions in pca_output_hundred
pca_output_hundred$var$contrib


# Run a PCA using the 10 non-binary numeric variables
cars_pca <- ade4::dudi.pca(cars[,9:19], scannf = FALSE, nf = 4)

# Explore the summary of cars_pca
summary(cars_pca)

# Explore the summary of pca_output_ten_v
summary(pca_output_ten_v)


# Create a factor map for the variables
factoextra::fviz_pca_var(pca_output_all, select.var = list(cos2 = 0.7), repel = TRUE)

# Modify the code to create a factor map for the individuals
factoextra::fviz_pca_ind(pca_output_all, select.ind = list(cos2 = 0.7), repel = TRUE)

# Create a barplot for the variables with the highest cos2 in the 1st PC
factoextra::fviz_cos2(pca_output_all, choice = "var", axes = 1, top = 10)

# Create a barplot for the variables with the highest cos2 in the 2nd PC
factoextra::fviz_cos2(pca_output_all, choice = "var", axes = 2, top = 10)


# Create a factor map for the top 5 variables with the highest contributions
factoextra::fviz_pca_var(pca_output_all, select.var = list(contrib = 5), repel = TRUE)

# Create a factor map for the top 5 individuals with the highest contributions
factoextra::fviz_pca_ind(pca_output_all, select.ind = list(contrib = 5), repel = TRUE)

# Create a barplot for the variables with the highest contributions to the 1st PC
factoextra::fviz_contrib(pca_output_all, choice = "var", axes = 1, top = 5)

# Create a barplot for the variables with the highest contributions to the 2nd PC
factoextra::fviz_contrib(pca_output_all, choice = "var", axes = 2, top = 5)


# Create a biplot with no labels for all individuals with the geom argument.
factoextra::fviz_pca_biplot(pca_output_all)

# Create ellipsoids for wheeltype columns respectively.
factoextra::fviz_pca_ind(pca_output_all, habillage = cars$wheeltype, addEllipses = TRUE)

# Create the biplot with ellipsoids
factoextra::fviz_pca_biplot(pca_output_all, habillage=cars$wheeltype, addEllipses=TRUE, alpha.var="cos2")

```
  
  
  
***
  
Chapter 2 - Advanced PCA and Non-Negative Matrix Factorization (NNMF)  
  
Determining the right number of PCs:  
  
* Desire to have stopping rules to determine the proper number of principal components  
* The screeplot can help to find an elbow, which is often a good stopping point  
	* mtcars_pca <- PCA(mtcars)  
    * fviz_screeplot(mtcars_pca, ncp=5)  
* The Kaiser-Guttman rule recommends keeping components with eigenvalues > 1  
	* mtcars_pca$eig  
    * get_eigenvalue(mtcars_pca)  
* Parallel analysis is superior to the previous tests - compares variance explained by PC to what would be expected with random data - select "above the line"  
	* library(paran)  
    * mtcars_pca_ret <- paran(mtcars_pca, graph = TRUE)  
  
Performing PCA on datasets with missing values:  
  
* Missing values can cause problems for PCA  
	* Skipping rows with missing values can lead to too little data while also biasing the results  
    * Mean imputation is quick and dirty, but distorts the distribution of the variable  
    * A preferred approach is to estimate each NA as a linear combination of the other parameters  
* The missMDA library can help with these estimations  
	* library(missMDA)  
    * nPCs <- estim_ncpPCA(as.matrix(sleep))  
    * completed_sleep <- imputePCA(sleep, ncp = nPCs$ncp, scale = TRUE)  
    * PCA(completed_sleep$completeObs)  
* Can also impute missing values in the pca() function  
	* library(pcaMethods)  
    * sleep_pca_methods <- pca(sleep, nPcs=2, method="ppca", center = TRUE)  
    * imp_air_pcamethods <- completeObs(sleep_pca_methods)  
  
NNMF and Topic Detection with nmf():  
  
* Non-negative matrix factorization is designed for cases where negative values should never be predicted  
	* Tries to decompose an mxn matrix in to mxr and rxn  
* Dimensionality reduction is critical for sparse data (e.g., text by document)  
* The NMF library implements nmf in R  
	* library(NMF)  
    * bbc_res <- nmf(bbc_tdm, 5)  # sets r=5  
    * W <- basis(bbc_res)  # terms as rows, topic as columns  
    * H <- coef(bbc_res)  # topics as rows, documents as columns  
    * colnames(W) <- c("topic1", "topic2", "topic3", "topic4", "topic5")  
    * W %>% rownames_to_column('words') %>% arrange(. , desc(topic1))%>% column_to_rownames('words')  
  
Example code includes:  
```{r}

data(airquality, package="datasets")
airquality <- airquality[complete.cases(airquality), ]

# Conduct a PCA on the airquality dataset
pca_air <- FactoMineR::PCA(airquality)

# Apply the Kaiser-Guttman rule
summary(pca_air, ncp = 4)

# Perform the screeplot test
factoextra::fviz_screeplot(pca_air, ncp = 5)


data(airquality, package="datasets")

# Conduct a parallel analysis with paran().
air_paran <- paran::paran(airquality[complete.cases(airquality), ])

# Check out air_paran's suggested number of PCs to retain.
air_paran$Retained

# Conduct a parallel analysis.
air_fa_parallel <- psych::fa.parallel(airquality)

# Check out air_fa_parallel's suggested number of PCs to retain.
air_fa_parallel$ncomp


# Check out the summary of airquality
summary(airquality)

# Check out the number of cells with missing values.
sum(is.na(airquality))

# Check out the number of rows with missing values.
nrow(airquality[!complete.cases(airquality), ])

# Estimate the optimal number of dimensions for imputation.
missMDA::estim_ncpPCA(airquality, ncp.max=5)


bbc_res <- readRDS("./RInputFiles/bbc_res.rds")

# Get a 5-rank approximation of corpus_tdm.
# bbc_res <- NMF::nmf(corpus_tdm, 5)

# Get the term-topic matrix W.
W <- NMF::basis(bbc_res)

# Check out the dimensions of W.
dim(W)

# Normalize W.
normal <- function(x) { x / sum(x) }
normal_W <- apply(W, 2, FUN=normal)


# Get the topic-text matrix H.
H <- coef(bbc_res)

# Check out the dimensions of H.
dim(H)

# Normalize H.
normal_H <- apply(H, 2, FUN=normal)


# Explore the nmf's algorithms.
alg <- NMF::nmfAlgorithm()

# Choose the algorithms implemented in R.
R_alg <- NMF::nmfAlgorithm(version="R")

# Use the two-version algorithms.
# bbc_double_opt <- NMF::nmf(x=corpus_tdm, rank=5, method=R_alg, .options="v")

```
  
  
  
***
  
Chapter 3 - Exploratory Factor Analysis  
  
Intro to EFA:  
  
* Variance and covariance are only partially explained by factors  
	* Latent constructs drive observed variables by way of loadings (weightings)  
    * Model accepts that not all variance can be explained by the latent constructs  
* Example of analyzing a single variable  
	* Check for data factorability  
    * Extract factors  
    * Choose the "right" number of factors to retain  
    * Rotate factors  
    * Interpret the results  
* Example of running the process on the bfi dataset from psych  
  
Intro to EFA: Data Factorability:  
  
* Need to check whether the dataset is factorable  
* Bartlett sphericity test - drawback is that it is always significant for large datasets  
	* H0: There is no significant difference between the correlation matrix and the identity matrix of the same dimensionality  
    * H1: There is significant difference betweeen them and, thus, we have strong evidence that there are underlying factors  
    * library(polycor)  
    * bfi_s <- bfi[1:200, 1:25]  
    * bfi_hetcor <- hetcor(bfi_s)  
    * bfi_c <- bfi_hetcor$correlations  
    * bfi_factorability <- cortest.bartlett(bfi_c)  
* Kaiser-Meyer-Olkin (KMO) is also known as the measure of sampling adequacy  
	* KMO(bfi_c)  
    * Should be in the 0.60s to be acceptable  
  
Extraction methods:  
  
* Can extract factors using fa(), with extraction methods including  
	* minres: minimum residual [default] (slightly modified methods: ols, wls, gls)  
    * mle: Maximum Likelihood Estimation (MLE)  
    * paf: Principal Axes Factor (PAF) extraction  
    * minchi: minimum sample size weighted chi square  
    * minrank: minimum rank  
    * alpha: alpha factoring  
* Example of using the minres extraction method  
	* library(psych)  
    * library(GPArotation)  
    * f_bfi_minres <- fa(bfi_c, nfactors = 3, rotate = "none")  
    * f_bfi_minres_common <- sort(f_bfi_minres$communality, decreasing = TRUE)  
    * data.frame(f_bfi_minres_common)  
    * f_bfi_mle <- fa(bfi_c, nfactors = 3, fm = "mle", rotate = "none")  
    * f_bfi_mle_common <- sort(f_bfi_mle$communality, decreasing = TRUE)  
    * data.frame(f_bfi_mle_common)  
  
Choosing the right number of factors:  
  
* Arriving at the right number of factors is not always an easy task  
	* fa.parallel(bfi_c, n.obs = 200, fa = "fa", fm = "minres")  
    * fa.parallel(bfi_c, n.obs = 200, fa = "fa", fm = "mle")  
  
Example code includes:  
```{r cache=TRUE}

hsq <- readr::read_delim("./RInputFiles/humor_dataset.csv", delim=";")
str(hsq, give.attr=FALSE)


# Check out the dimensionality of hsq.
dim(hsq)

# Explore the correlation object hsq_correl.
hsq_correl <- psych::mixedCor(hsq, c=NULL, p=1:32)
str(hsq_correl)

# Getting the correlation matrix of the dataset.
hsq_polychoric <- hsq_correl$poly$rho

# Explore the correlation structure of the dataset.
ggcorrplot::ggcorrplot(hsq_polychoric)


# Apply the Bartlett test on the correlation matrix.
psych::cortest.bartlett(hsq_polychoric)

# Check the KMO index.
psych::KMO(hsq_polychoric)


# EFA with four factors. 
f_hsq <- psych::fa(hsq_polychoric, nfactors=4)

# Inspect the resulting EFA object.
str(f_hsq)

# Use maximum likelihood for extracting factors.
psych::fa(hsq_polychoric, nfactors=4, fm="mle")


# Use PAF on hsq_polychoric.
hsq_correl_pa <- psych::fa(hsq_polychoric, nfactors=4, fm="pa")

# Sort the communalities of the f_hsq_pa.
f_hsq_pa_common <- sort(hsq_correl_pa$communality, decreasing = TRUE)

# Sort the uniqueness of the f_hsq_pa.
f_hsq_pa_unique <- sort(hsq_correl_pa$uniqueness, decreasing = TRUE)


# Check out the scree test and the Kaiser-Guttman criterion.
psych::scree(hsq_polychoric)

# Use parallel analysis for estimation with the minres extraction method.
psych::fa.parallel(hsq_polychoric, n.obs = 1069, fm = "minres", fa = "fa")

# Use parallel analysis for estimation with the mle extraction method.
psych::fa.parallel(hsq_polychoric, n.obs = 1069, fm = "mle", fa = "fa")

```
  
  
  
***
  
Chapter 4 - Advanced EFA  
  
Interpretation of EFA and factor rotation:  
  
* Factor rotation is a standard step for simplifying interpretation  
	* Orthogonal - uncorrelated factors with 90-degree angles (generally good for unrelated factors, and easier to interpret since they are fully orthogonal)  
    * Oblique - uncorrelated factors, but can have slight differences from 90-degree angles (generally good for related factors)  
* Example of the psych::bfi data analysis  
	* f_bfi_varimax <- fa(bfi_c, fm = "minres", nfactors = 5, rotate = "varimax")  
  
Interpretation of EFA and path diagrams:  
  
* Interpretation is an important component of the modeling process  
	* fa.diagram(f_bfi_varimax)  
    * print(f_bfi_varimax$loadings, cut=0)  # cut=0 will show the values between 0.0 and 0.1 (the default would be to exclude the 0.1 and under)  
  
EFA case study:  
  
* The "short dark triad" combines machiavellianism, narcissism, and psychopathy  
* A short version of the test is available at https://openpsychometrics.org/tests/SD3/  
	* sdt_test <- read.csv("SD3.csv", sep = "\t")  
    * dim(sdt_test)  
    * head(sdt_test)  
* General steps for EFA on the data will include  
	* Check for data factorability  
    * Extract factors  
    * Choose the "right" number of factors to retain  
    * Rotate factors  
    * Interpret the results  
  
Wrap up:  
  
* Biggest challenge is in handling large amounts of data - computation, interpretation, etc.  
* PCA and NNMF (positive entries only) are popular choices for many types of datasets  
* EFA for exploratory purposes  
* Common steps in dimensionality reduction  
    * Factor/Component/Dimension extraction  
    * Decision on the number of Factor/Component/Dimension to retain - parsimonious, minimum information loss, easy interpretation  
    * Use visual aid for interpretation (e.g. biplot)  
  
Example code includes:  
```{r}

# Check the default rotation method.
f_hsq$rotation

# Try Promax, another oblique rotation method.
f_hsq_promax <- psych::fa(hsq_polychoric, nfactors=4, rotate="promax")

# Try Varimax, an orthogonal method.
f_hsq_varimax <- psych::fa(hsq_polychoric, nfactors=4, rotate="varimax") 


# Check the factor loadings.
print(f_hsq$loadings, cut=0)

# Create the path diagram of the latent factors.
psych::fa.diagram(f_hsq)


SD3 <- readRDS("./RInputFiles/SD3.RDS")
# SD3_mod <- SD3 %>% mutate_all(factor, levels=1:5)
sdt_sub_correl <- polycor::hetcor(SD3)


# Explore sdt_sub_correl.
str(sdt_sub_correl)

# Get the correlation matrix of the sdt_sub_correl.
sdt_polychoric <- sdt_sub_correl$correlations

# Apply the Bartlett test on the correlation matrix.
psych::cortest.bartlett(sdt_polychoric)

# Check the KMO index.
psych::KMO(sdt_polychoric)


# Check out the scree test.
psych::scree(sdt_polychoric)

# Use parallel analysis for estimation with the minres extraction method.
psych::fa.parallel(sdt_polychoric, n.obs = 100, fa = "fa")

# Perform EFA with MLE. 
f_sdt <- psych::fa(sdt_polychoric, fm = "ml", nfactors = 4)


# Check the factor loadings.
print(f_sdt$loadings, cut=0)

# Create the path diagram of the latent factors.
psych::fa.diagram(f_sdt)

```
  
  
  
***
  
### _Anomaly Detection in R_  
  
Chapter 1 - Statistical Outlier Detection  
  
Meaning of anomalies:  
  
* Anomalies are data points that do not seem to follow the same patterns as the rest of the data  
	* Point anomaly - a single point that is unusual compared with the rest of the data  
    * boxplot(temperature, ylab = "Celsius")  
    * Collective anomaly - series of points that are unusual compared with the rest of the data  
  
Testing extremes with Grubbs' test:  
  
* Grubbs' test assumes data are normally distributed (should be checked prior to running the analysis)  
	* hist(temperature, breaks = 6)  
    * grubbs.test(temperature)  
* Can get the row number that was flagged based on whether Grubbs' test flagged the maximum or the mimimum value  
	* which.max(weights)  
    * which.min(weights)  
  
Anomalies in time series:  
  
* Can begin by visualizing a time series using plot  
	* plot(sales ~ month, data = msales, type = 'o')  
* The Seasonal Hybrid ESD algorithm usies the AnomalyDetection library  
	* library(AnomalyDetection)  
    * sales_ad <- AnomalyDetectionVec(x = msales$sales, period = 12, direction = 'both')  # direction can also be "small" or "large"  
    * sales_ad$anoms  
    * AnomalyDetectionVec(x = msales$sales, period = 12, direction = 'both', plot = T)  # plot the anomalies in blue  
  
Example code includes:  
```{r}

river <- data.frame(index=1:291, 
                    nitrate=c(1.581, 1.323, 1.14, 1.245, 1.072, 1.483, 1.162, 1.304, 1.14, 1.118, 1.342, 1.245, 1.204, 1.14, 1.204, 1.118, 1.025, 1.118, 1.285, 1.14, 0.949, 0.922, 0.949, 1.118, 1.265, 1.095, 1.183, 1.162, 1.118, 1.285, 1.049, 0.922, 0.775, 0.866, 0.922, 1.643, 1.323, 1.285, 1.095, 1.049, 1.095, 0.922, 0.866, 1.049, 0.922, 1.095, 1.183, 1.304, 1.162, 1.225, 1.285, 1.072, 1.533, 1.095, 1.396, 1.025, 0.922, 0.949, 1.118, 1.342, 1.36, 1.36, 1.204, 1.265, 1, 1.183, 1.025, 0.866, 1.072, 1.049, 1.049, 1.049, 1.095, 1.183, 1.095, 0.975, 1.118, 0.975, 1.049, 0.837, 0.922, 1.118, 1.072, 1.204, 0.975, 1.095, 1.049, 0.866, 0.922, 1.049, 1.127, 1.072, 0.975, 1.049, 1.183, 1.245, 1.225, 1.225, 1.265, 1.118, 1.14, 1.072, 1.095, 0.671, 1.183, 0.949, 1.162, 1.095, 1.323, 1.342, 1.277, 1.015, 1, 0.922, 0.894, 1, 1.049, 0.922, 1.517, 1.265, 1.414, 1.304, 1.14, 1.14, 1.049, 1.068, 0.906, 1.095, 0.883, 1.14, 1.025, 1.36, 1.183, 1.265, 1.304, 0.964, 0.975, 0.99, 0.877, 1.049, 0.975, 1, 1.183, 1.225, 1.265, 1.183, 1.049, 0.97, 0.894, 0.98, 0.964, 0.894, 0.922, 1.14, 1.183, 1.897, 1.095, 1.14, 1.414, 1.14, 1, 1.049, 0.889, 0.872, 1, 1.095, 0.671, 1.095, 1.14, 1.304, 1.025, 0.975, 1, 0.877, 0.949, 0.866, 1.058, 1.086, 1.118, 1.162, 1.221, 1.265, 1.122, 1.015, 1.162, 0.825, 0.906, 0.849, 0.985, 1.118, 1.077, 1.237, 1.237, 1.063, 1.01, 0.933, 0.922, 0.806, 0.748, 0.592, 0.911, 0.806, 0.98, 1.077, 1.212, 1.277, 0.954, 0.837, 0.917, 0.9, 1.068, 0.872, 0.99, 1.131, 1.068, 1.208, 1.319, 1.281, 0.905, 0.819, 0.826, 0.974, 0.888, 0.804, 0.996, 1.127, 1.17, 1.166, 1.261, 1.275, 1.179, 1.079, 0.951, 0.852, 0.872, 0.834, 0.859, 1.077, 1.095, 1.285, 1.323, 1.16, 1.125, 0.957, 0.948, 0.907, 0.89, 0.999, 0.999, 0.953, 0.9, 0.986, 1.187, 1.054, 1.079, 0.997, 0.851, 0.803, 0.971, 1.025, 1.086, 1.114, 1.068, 1.091, 1.034, 0.871, 0.781, 0.865, 0.7, 0.673, 0.881, 0.782, 0.97, 1.044, 1.17, 1.196, 1.091, 1.068, 0.967, 0.823, 0.73, 0.693, 0.788, 1.095, 1.183, 0.996, 1.105, 0.939, 0.914, 0.813, 0.775), 
                    month=factor(month.name[c(rep(1:12, times=24), 1:3)], levels=month.name)
                    )
str(river)


# Explore contents of dataset
head(river)

# Summary statistics of river nitrate concentrations
summary(river$nitrate)

# Plot the distribution of nitrate concentration
boxplot(river$nitrate)


# Plot a histogram of the nitrate column
hist(river$nitrate)

# Add a Nitrate concentration label 
hist(river$nitrate, xlab="Nitrate concentration")

# Separate the histogram into 40 bins 
hist(river$nitrate, xlab = "Nitrate concentration", breaks = 40)


# Apply Grubbs' test to the nitrate data
outliers::grubbs.test(river$nitrate)

# Use which.max to find row index of the max
which.max(river$nitrate)

# Runs Grubbs' test excluding row 156
outliers::grubbs.test(river$nitrate[-156])

# Print the value tested in the second Grubbs' test
min(river$nitrate[-156])


# View contents of dataset
head(river)

# Show the time series of nitrate concentrations with time
plot(nitrate ~ month, data = river, type = "o")

# Calculate the mean nitrate by month
monthly_mean <- tapply(river$nitrate, river$month, FUN = mean)
monthly_mean

# Plot the monthly means 
plot(monthly_mean, type = "o", xlab = "Month", ylab = "Monthly mean")

# Create a boxplot of nitrate against months
boxplot(nitrate ~ month, data=river)


# Package ‘anomalyDetection’ was removed from the CRAN repository.
# Formerly available versions can be obtained from the archive. 
# Archived on 2019-03-01 as check problems were not corrected in time. 

# Run Seasonal-Hybrid ESD for nitrate concentrations
# AnomalyDetectionVec(river$nitrate, period=12, direction = 'both', plot = T)

# Use Seasonal-Hybrid ESD for nitrate concentrations
# river_anomalies <- AnomalyDetectionVec(x = river$nitrate, period = 12, direction = 'both', plot = T)

# Print the anomalies
# river_anomalies$anoms

# Print the plot
# print(river_anomalies$plot)

```
  
  
  
***
  
Chapter 2 - Distance and Density Based Anomaly Detection  
  
k-Nearest-Neighbors Score:  
  
* Example dataset for heights and widths of furniture  
	* plot(Width ~ Height, data = furniture)  
* Anomalies are usually far away from their neighbors  
	* library(FNN)  
    * furniture_knn <- get.knn(data = furniture, k = 5)  
    * get.knn() returns two matrices - "nn.index" "nn.dist"  
    * head(furniture_knn$nn.dist, 3)  # distance matrix  
    * furniture_score <- rowMeans(furniture_knn$nn.dist)  # average distance to 5 nearest neighbors  
  
Visualizing kNN distance:  
  
* Can be important to standardize distances prior to kNN (unless it is desirable that a single variable dominate the scoring)  
	* furniture_scaled <- scale(furniture)  
    * furniture_scaled <- scale(furniture)  
    * furniture_knn <- get.knn(furniture_scaled, 5)  
    * furniture$score <- rowMeans(furniture_knn$nn.dist)  
    * plot(Width ~ Height, cex = sqrt(score), data = furniture, pch = 20)  
  
Local outlier factor (LOF):  
  
* LOF uses density rather than distance to set up a distance for points - score is a ratio that tends to be near 1  
	* kNN is helpful for finding global anomalies, while LOF is helpful for finding local anomalies  
    * library(dbscan)  
    * furniture_lof <- lof(scale(furniture), k = 5)  
* LOF is the density around the point relative to the density around the kNN  
	* LOF > 1 is more likely to be anomalous while LOF < 1 is much less likely to be anomalous  
    * furniture$score_lof <- furniture_lof  
    * plot(Width ~ Height, data = furniture, cex = score_lof, pch = 20)  
  
Example code includes:  
```{r}

wineOrig <- readr::read_csv("./RInputFiles/big_wine.csv")
str(wineOrig, give.attr=FALSE)
wine <- wineOrig %>% select(pH, alcohol)

# View the contents of the wine data
head(wine)

# Scatterplot of wine pH against alcohol
plot(pH ~ alcohol, data = wine)


# Calculate the 5 nearest neighbors distance
wine_nn <- FNN::get.knn(wine, k = 5)

# View the distance matrix
head(wine_nn$nn.dist)

# Distance from wine 5 to nearest neighbor
wine_nn$nn.dist[5, 1]

# Row index of wine 5's nearest neighbor 
wine_nn$nn.ind[5, 1]

# Return data for wine 5 and its nearest neighbor
wine[c(5, wine_nn$nn.ind[5, 1]), ]

# Create score by averaging distances
wine_nnd <- rowMeans(wine_nn$nn.dist)

# Print row index of the most anomalous point
which.max(wine_nnd)


# Observe differences in column scales 
summary(wine)

# Standardize the wine columns
wine_scaled <- scale(wine)

# Observe standardized column scales
summary(wine_scaled)


# Print the 5-nearest neighbor distance score
wine_nnd[1:5]

# Add the score as a new column 
wine$score <- wine_nnd


# Scatterplot showing pH, alcohol and kNN score
plot(pH ~ alcohol, data=wine, cex = sqrt(score), pch = 20)


# Calculate the LOF for wine data
wine$score <- NULL
wine_lof <- dbscan::lof(scale(wine), k=5)

# Append the LOF score as a new column
wine$score <- wine_lof


# Scatterplot showing pH, alcohol and LOF score
plot(pH ~ alcohol, data=wine, cex=score, pch=20)


# Calculate and append kNN distance as a new column
wine_nn <- FNN::get.knn(wine_scaled, k = 10)
wine$score_knn <- rowMeans(wine_nn$nn.dist)     

# Calculate and append LOF as a new column
wine$score_lof <- dbscan::lof(wine_scaled, k = 10)

# Find the row location of highest kNN
which.max(wine$score_knn)

# Find the row location of highest LOF
which.max(wine$score_lof)

```
  
  
  
***
  
Chapter 3 - Isolation Forest  
  
Isolation Trees:  
  
* Example of deer organizing in to a pack and predators trying to find isolated deer to attack  
	* Points that are more easily separated from other points are considered to be more anomalous  
    * Keep splitting data until every point lies within its own sub-region OR there are fewer than n points in each sub-region  
* Example of running the isolation forest  
	* library(isofor)  
    * furniture_tree <- iForest(data = furniture, nt = 1)  # nt is the number of trees to be grown  
    * furniture_score <- predict(furniture_tree, newdata = furniture)  # generates the isolation score  
* The isolation score is the average number of random splits needed to isolate a point  
	* The score returned by predict is normalized, with scores near 1 more likely to be anomalies (short path length, easy to split)  
  
Isolation Forest:  
  
* Can use sampling to build multiple trees (called an isolation forest, with score averaged over trees)  
	* furniture_tree <- iForest(data = furniture, nt = 1, phi = 100)  
    * furniture_forest <- iForest(data = furniture, nt = 100)  
* The forest of many trees is more robust and faster to grow  
	* The anomaly score should generally converge after "sufficient" trees have been run (100 is a typical default)  
    * Scores should largely be stable if more or less trees are added  
    * plot(trees_500 ~ trees_1000, data = furniture_scores)  
    * abline(a = 0, b = 1)  
  
Visualizing Isolation Scores:  
  
* Can visualize the isolation score using the contour plot  
	* h_seq <- seq(min(furniture$Height), max(furniture$Height), length.out = 20)  
    * w_seq <- seq(min(furniture$Width), max(furniture$Width), length.out = 20)  
    * furniture_grid <- expand.grid(Width = w_seq, Height = h_seq)  
    * furniture_grid$score <- predict(furniture_forest, furniture_grid)  
    * library(lattice)  
    * contourplot(score ~ Height + Width, data = furniture_grid, region = TRUE)  
  
Example code includes:  
```{r}

wine <- wine %>% select(pH, alcohol)
str(wine, give.attr=FALSE)


# CRAN - package ‘isofor’ is not available (for R version 3.5.1)

# Build an isolation tree 
# wine_tree <- iForest(wine, nt = 1)

# Create isolation score
# wine$tree_score <- predict(wine_tree, newdata = wine)

# Histogram plot of the scores
# hist(wine$tree_score, breaks=40)


# Fit isolation forest
# wine_forest <- iForest(wine, nt=100)

# Fit isolation forest
# wine_forest <- iForest(wine, nt = 100, phi = 200)

# Create isolation score from forest
# wine_score <- predict(wine_forest, newdata=wine)

# Append score to the wine data
# wine$score <- wine_score


# View the contents of the wine scores
# head(wine_scores)

# Score scatterplot 2000 vs 1000 trees 
# plot(trees_2000 ~ trees_1000, data = wine_scores)

# Add reference line of equality
# abline(a = 0, b = 1)


# Sequence of values for pH and alcohol
ph_seq <- seq(min(wine$pH), max(wine$pH), length.out = 25)
alcohol_seq <- seq(min(wine$alcohol), max(wine$alcohol) , length.out = 25)

# Create a data frame of grid coordinates
wine_grid <- expand.grid(pH = ph_seq, alcohol = alcohol_seq)

# Plot the grid
plot(pH ~ alcohol, data=wine_grid, pch = 20)


# Calculate isolation score at grid locations
# wine_grid$score <- predict(wine_forest, newdata=wine_grid)


# Contour plot of isolation scores
# contourplot(score ~ alcohol + pH, data=wine_grid, region = TRUE)

```
  
  
  
***
  
Chapter 4 - Comparing Performance  
  
Labeled Anomalies:  
  
* Sometimes, the anomalies are already labelled, allowing for supervised learning  
	* table(sat$label)  
    * plot(V2 ~ V3, data = sat, col = as.factor(label), pch = 20)  
    * sat_for <- iForest(sat[, -1], nt = 100)  
    * sat$score <- predict(sat_for, features)  
    * boxplot(score ~ label, data = sat, col = "olivedrab4")  
* Challenges with modeling can include  
	* Detecting rare cases  
    * Rapidly changing exploits (e.g., card fraud)  
  
Measuring Performance:  
  
* Decision threshholds are sometimes based off quantiles; flag a percentage of the data as anomalous  
	* high_score <- quantile(sat$score, probs = 0.99)  
    * sat$binary_score <- as.numeric(score >= high_score)  
    * table(sat$label, sat$binary_score)  
* Can calculate recall as correctly detected anomalies divided by total actual anomalies (1 means all anomalies detected)  
* Can calculate precision as correctly detected anomalies divided by total predicted anomalies (1 means no false detections)  
  
Working with Categorical Features:  
  
* Begin by checking for any non-numeric features - fct or chr classes  
	* sapply(X = sat, FUN = class)  
* The isolation forest can take factor variables (but not character variables directly) provided they have at most 32 unique levels  
	* sat$high_low <- as.factor(sat$high_low)  
    * class(sat$high_low)  
    * sat_for <- iForest(sat[, -1], nt = 100)  
* Can run LOF with factors using the Gower distance (all distances are standardized to be between 0 and 1) from the daisy package  
	* library(cluster)  
    * sat_dist <- daisy(sat[, -1], metric = "gower")  
    * sat_lof <- lof(sat_dist, k = 10)  
    * sat_distmat <- as.matrix(sat_dist)  
    * range(sat_distmat)  
  
Wrap Up:  
  
Example code includes:  
```{r}

thyroidOrig <- readr::read_csv("./RInputFiles/thyroid.csv")
str(thyroidOrig, give.attr=FALSE)
thyroid <- thyroidOrig


# View contents of thryoid data
head(thyroid)

# Tabulate the labels
table(thyroid$label)

# Proportion of thyroid cases
prop_disease <- mean(thyroid$label)


# Plot of TSH and T3
plot(TSH ~ T3, data=thyroid, pch=20)

# Plot of TSH, T3 and labels
plot(TSH ~ T3, data = thyroid, pch = 20, col = label + 1)

# Plot of TT4, TBG and labels
plot(TT4 ~ TBG, data = thyroid, pch = 20, col = label + 1)


# Package isofor not available on CRAN
# Fit isolation forest
# thyroid_forest <- isofor::iForest(thyroid[, -1], nt = 200)

# Anomaly score 
# thyroid$iso_score <- predict(thyroid_forest, thyroid[, -1])

# Boxplot of the anomaly score against labels
# boxplot(iso_score ~ label, data=thyroid, col = "olivedrab4")


# Create a LOF score for thyroid
lof_score <- dbscan::lof(scale(thyroid[, -1]), k = 10)
                 
# Calculate high threshold for lof_score
high_lof <- quantile(lof_score, probs = 0.98) 

# Append binary LOF score to thyroid data
thyroid$binary_lof <- as.numeric(lof_score >= high_lof)


iso_score <- c(394, 442, 408, 369, 431, 420, 398, 374, 384, 452, 478, 461, 356, 357, 405, 437, 357, 366, 488, 671, 395, 367, 346, 387, 354, 386, 411, 548, 423, 344, 355, 459, 413, 389, 373, 360, 520, 382, 690, 676, 388, 486, 530, 561, 423, 409, 352, 441, 395, 416, 367, 377, 426, 418, 378, 357, 422, 431, 526, 380, 450, 434, 462, 360, 529, 382, 390, 371, 385, 382, 367, 416, 384, 400, 377, 391, 380, 403, 361, 355, 418, 498, 649, 465, 413, 377, 383, 375, 422, 360, 353, 380, 569, 430, 377, 418, 374, 413, 369, 378, 456, 357, 559, 375, 370, 543, 410, 548, 380, 382, 362, 390, 460, 438, 392, 742, 665, 400, 393, 382, 382, 511, 375, 363, 422, 399, 358, 448, 399, 450, 392, 369, 435, 437, 375, 529, 370, 369, 429, 532, 485, 439, 429, 363, 366, 480, 408, 622, 358, 489, 520, 393, 388, 431, 378, 400, 400, 354, 405, 388, 416, 442, 382, 348, 347, 375, 366, 397, 467, 518, 387, 376, 353, 369, 442, 380, 391, 390, 358, 401, 409, 414, 452, 377, 362, 360, 380, 381, 412, 412, 418, 381, 432, 391, 448, 395, 418, 509, 525, 398, 432, 359, 499, 444, 383, 405, 467, 418, 721, 399, 421, 527, 481, 371, 364, 459, 398, 373, 388, 434, 428, 439, 381, 405, 352, 363, 352, 403, 362, 396, 367, 365, 432, 392, 396, 367, 404, 384, 381, 364, 366, 376, 369, 379, 379, 426, 401, 380, 404, 394, 368, 361, 393, 455, 396, 540, 368, 360, 466, 365, 377, 411, 442, 408, 373, 394, 344, 352, 345, 344, 346, 344, 378, 366, 401, 436, 366, 367, 382, 356, 362, 402, 405, 376, 368, 381, 371, 391, 359, 707, 367, 370, 387, 385, 373, 354, 354, 362, 358, 364, 353, 365, 374, 385, 395, 362, 461, 374, 362, 456, 405, 426, 385, 387, 387, 375, 503, 378, 370, 358, 377, 461, 357, 353, 346, 350, 393, 456, 425, 418, 371, 380, 477, 383, 382, 349, 360, 412, 395, 409, 441, 371, 420, 455, 358, 654, 365, 507, 508, 443, 364, 381, 468, 368, 362, 454, 381, 357, 432, 374, 379, 383, 389, 367, 393, 424, 378, 361, 512, 449, 522, 352, 354, 367, 359, 396, 486, 367, 409, 427, 351, 381, 357, 362, 369, 364, 356, 730, 353, 399, 383, 523, 429, 425, 420, 455, 414, 475, 433, 528, 425, 476, 352, 350, 413, 443, 435, 381, 472, 486, 376, 402, 361, 377, 391, 380, 355, 400, 394, 353, 379, 376, 469, 398, 464, 388, 378, 397, 396, 521, 417, 365, 420, 377, 350, 407, 364, 368, 426, 344, 351, 411, 412, 502, 381, 495, 350, 350, 344, 362, 389, 388, 370, 354, 394, 363, 564, 549, 387, 378, 411, 421, 427, 382, 385, 496, 372, 416, 365, 375, 406, 355, 362, 442, 410, 477, 361, 379, 386, 375, 351, 351, 360, 360, 412, 400, 409, 458, 351, 376, 400, 360, 499, 362, 476, 396, 407, 437, 358, 385, 373, 432, 353, 352, 369, 405, 376, 383, 462, 375, 361, 395, 426, 431, 418, 500, 585, 616, 372, 529, 418, 456, 360, 429, 397, 366, 384, 359, 515, 401, 389, 429, 371, 357, 398, 380, 371, 354, 403, 355, 356, 368, 363, 481, 545, 367, 350, 345, 344, 344, 426, 438, 464, 365, 460, 462, 419, 358, 428, 433, 352, 384, 416, 387) 
iso_score <- c(iso_score, 384, 366, 367, 487, 628, 638, 472, 349, 351, 351, 420, 422, 347, 347, 422, 396, 443, 419, 509, 507, 398, 375, 427, 492, 388, 387, 354, 390, 439, 358, 392, 379, 361, 392, 375, 407, 663, 442, 390, 437, 432, 420, 397, 413, 477, 494, 522, 354, 354, 357, 381, 384, 412, 406, 411, 448, 508, 411, 352, 345, 511, 386, 364, 396, 476, 389, 355, 464, 363, 380, 366, 423, 396, 407, 415, 504, 440, 406, 449, 394, 432, 397, 428, 434, 401, 363, 395, 404, 392, 454, 357, 380, 352, 382, 389, 389, 345, 348, 461, 390, 371, 345, 442, 402, 386, 375, 382, 382, 404, 373, 586, 426, 600, 368, 382, 358, 407, 379, 402, 367, 366, 385, 706, 352, 384, 363, 486, 366, 433, 373, 397, 434, 402, 378, 376, 376, 359, 380, 363, 351, 543, 435, 385, 503, 359, 353, 365, 405, 457, 345, 463, 445, 363, 353, 369, 370, 355, 681, 439, 360, 417, 383, 376, 416, 428, 386, 426, 420, 462, 370, 367, 398, 373, 354, 418, 364, 357, 420, 628, 442, 403, 478, 370, 367, 399, 413, 453, 423, 376, 385, 415, 447, 349, 551, 390, 438, 384, 401, 458, 526, 449, 480, 405, 388, 391, 361, 362, 387, 429, 391, 413, 391, 380, 511, 411, 376, 374, 436, 362, 434, 437, 517, 397, 406, 372, 345, 345, 345, 347, 423, 639, 373, 397, 358, 369, 399, 464, 453, 406, 358, 350, 395, 386, 454, 396, 373, 394, 444, 377, 376, 459, 393, 353, 349, 685, 382, 419, 394, 446, 346, 442, 426, 390, 422, 568, 365, 443, 353, 513, 364, 349, 373, 422, 389, 509, 411, 443, 375, 438, 556, 349, 445, 446, 413, 455, 419, 385, 358, 381, 375, 372, 497, 589, 386, 493, 539, 769, 351, 511, 456, 373, 378, 411, 523, 448, 400, 368, 428, 381, 444, 378, 402, 377, 411, 367, 446, 374, 435, 429, 409, 399, 349, 360, 468, 400, 366, 372, 446, 384, 524, 348, 384, 371, 381, 347, 357, 369, 359, 405, 390, 363, 419, 469, 410, 413, 365, 377, 482, 398, 347, 467, 446, 442, 399, 367, 502, 424, 452, 364, 372, 355, 386, 399, 399, 370, 409, 412, 409, 396, 380, 446, 470, 375, 386, 454, 350, 514, 396, 411, 402, 360, 458, 439, 349, 345, 398, 368, 378, 355, 528, 384, 397, 543, 410, 370, 389, 506, 412, 454, 442, 602, 383, 367, 377, 489, 371, 471, 361, 366, 355, 508, 368, 390, 368, 375, 406, 512, 374, 380, 378, 344, 381, 400, 544, 375, 527, 390, 398, 455, 393, 427, 435, 512, 379, 367, 380)
iso_score <- iso_score / 1000


# Calculate high threshold for iso_score
high_iso <- quantile(iso_score, probs=0.98)  

# Append binary isolation score to thyroid data
thyroid$binary_iso <- as.numeric(iso_score >= high_iso)         


# Tabulate agreement of label and binary isolation score 
table(thyroid$label, thyroid$binary_iso)

# Tabulate agreement of label and binary LOF score 
table(thyroid$label, thyroid$binary_lof)

# Proportion of binary_iso and label that agree
iso_prop <- mean(thyroid$label == thyroid$binary_iso)

# Proportion of binary_lof and label that agree
lof_prop <- mean(thyroid$label == thyroid$binary_lof)


table(thyroid$label, thyroid$binary_iso)
table(thyroid$label, thyroid$binary_lof)

# Precision for binary scores
precision_iso <- sum(thyroid$label == 1 & thyroid$binary_iso == 1) / sum(thyroid$binary_iso == 1)
precision_lof <- sum(thyroid$label == 1 & thyroid$binary_lof == 1) / sum(thyroid$binary_lof == 1)

# Recall for binary scores
recall_iso <- sum(thyroid$label == 1 & thyroid$binary_iso == 1) / sum(thyroid$label == 1)
recall_lof <- sum(thyroid$label == 1 & thyroid$binary_lof == 1) / sum(thyroid$label == 1)


age <- c('35-60', '0-35', '35-60', '60+', '0-35', '0-35', '0-35', '0-35', '35-60', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '60+', '35-60', '60+', '35-60', '35-60', '60+', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '60+', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '0-35', '0-35', '35-60', '0-35', '60+', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '35-60', '60+', '60+', '35-60', '35-60', '0-35', '0-35', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '60+', '0-35', '0-35', '60+', '35-60', '0-35', '35-60', '0-35', '0-35', '60+', '0-35', '0-35', '0-35', '35-60', '60+', '35-60', '35-60', '35-60', '60+', '0-35', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '35-60', '60+', '35-60', '60+', '60+', '0-35', '35-60', '60+', '60+', '0-35', '60+', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '0-35', '35-60', '35-60', '60+', '60+', '60+', '0-35', '60+', '0-35', '0-35', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '0-35', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '60+', '60+', '0-35', '0-35', '0-35', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '35-60', '60+', '0-35', '60+', '60+', '60+', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '35-60', '60+', '60+', '60+', '0-35', '0-35', '0-35', '0-35', '35-60', '60+', '35-60', '35-60', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '60+', '0-35', '60+', '60+', '60+', '0-35', '60+', '60+', '60+', '60+', '35-60', '0-35', '60+', '60+', '35-60', '60+', '0-35', '0-35', '60+', '60+', '60+', '0-35', '0-35', '35-60', '60+', '60+', '35-60', '35-60', '35-60', '60+', '0-35', '60+', '0-35', '60+', '35-60', '60+', '60+', '0-35', '35-60', '35-60', '0-35', '35-60', '60+', '0-35', '60+') 
age <- c(age, '60+', '60+', '0-35', '60+', '35-60', '0-35', '0-35', '60+', '35-60', '35-60', '0-35', '60+', '60+', '0-35', '60+', '35-60', '0-35', '35-60', '0-35', '35-60', '0-35', '60+', '0-35', '60+', '60+', '0-35', '60+', '60+', '60+', '60+', '60+', '0-35', '0-35', '60+', '0-35', '60+', '0-35', '60+', '60+', '35-60', '35-60', '60+', '60+', '60+', '60+', '60+', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '60+', '35-60', '60+', '0-35', '0-35', '0-35', '0-35', '35-60', '60+', '35-60', '0-35', '60+', '60+', '0-35', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '60+', '0-35', '0-35', '60+', '35-60', '60+', '0-35', '60+', '0-35', '60+', '35-60', '0-35', '35-60', '0-35', '60+', '60+', '0-35', '60+', '60+', '60+', '35-60', '35-60', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '60+', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '60+', '35-60', '35-60', '0-35', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '60+', '0-35', '60+', '0-35', '35-60', '0-35', '60+', '60+', '60+', '35-60', '60+', '60+', '35-60', '60+', '0-35', '0-35', '0-35', '60+', '60+', '60+', '35-60', '0-35', '0-35', '35-60', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '60+', '0-35', '35-60', '0-35', '0-35', '0-35', '0-35', '60+', '35-60', '60+', '35-60', '0-35', '60+', '35-60', '35-60', '60+', '35-60', '0-35', '60+', '60+', '35-60', '0-35', '60+', '35-60', '60+', '0-35', '60+', '0-35', '35-60', '0-35', '0-35', '35-60', '35-60', '0-35', '35-60', '60+', '35-60', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '0-35', '0-35', '60+', '0-35', '60+', '0-35', '60+', '60+', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '0-35', '35-60', '35-60', '35-60', '0-35', '60+', '35-60', '60+', '60+', '60+', '0-35', '0-35', '35-60', '0-35', '0-35', '0-35', '60+', '60+', '60+', '0-35', '0-35', '0-35', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '60+', '0-35', '60+', '35-60', '0-35', '35-60', '60+', '35-60', '0-35', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '60+', '35-60', '60+', '35-60', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '60+', '60+', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '35-60', '60+', '0-35', '35-60', '35-60', '0-35', '35-60', '60+', '35-60', '35-60', '60+', '60+', '35-60', '0-35', '60+', '35-60', '0-35', '35-60', '35-60', '60+', '35-60', '60+', '60+', '60+', '60+', '0-35', '0-35', '60+', '35-60', '0-35', '60+', '35-60', '60+', '60+', '60+', '35-60', '0-35', '0-35', '60+', '0-35', '35-60', '35-60', '35-60', '60+', '0-35', '35-60', '60+', '35-60', '0-35', '60+', '0-35', '35-60')
age <- c(age, '60+', '35-60', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '35-60', '60+', '60+', '60+', '60+', '60+', '0-35', '60+', '60+', '35-60', '60+', '60+', '60+', '0-35', '60+', '35-60', '60+', '35-60', '35-60', '0-35', '35-60', '0-35', '35-60', '0-35', '35-60', '60+', '0-35', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '60+', '0-35', '0-35', '0-35', '35-60', '35-60', '60+', '35-60', '35-60', '35-60', '0-35', '35-60', '0-35', '35-60', '35-60', '60+', '35-60', '60+', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '0-35', '60+', '60+', '35-60', '0-35', '60+', '35-60', '60+', '60+', '0-35', '0-35', '35-60', '0-35', '35-60', '60+', '0-35', '0-35', '0-35', '60+', '35-60', '60+', '60+', '35-60', '35-60', '60+', '0-35', '60+', '60+', '60+', '35-60', '60+', '60+', '60+', '0-35', '60+', '60+', '0-35', '0-35', '0-35', '60+', '0-35', '0-35', '0-35', '60+', '60+', '60+', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '35-60', '35-60', '60+', '60+', '35-60', '35-60', '0-35', '60+', '0-35', '35-60', '35-60', '35-60', '0-35', '35-60', '35-60', '60+', '60+', '60+', '0-35', '60+', '0-35', '0-35', '0-35', '0-35', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '0-35', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '60+', '60+', '35-60', '0-35', '0-35', '60+', '60+', '35-60', '35-60', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '0-35', '0-35', '35-60', '0-35', '60+', '0-35', '35-60', '60+', '60+', '0-35', '35-60', '35-60', '60+', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '60+', '0-35', '0-35', '60+', '0-35', '35-60', '35-60', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '60+', '0-35', '0-35', '60+', '60+', '0-35', '60+', '35-60', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '60+', '60+', '60+', '35-60', '35-60', '0-35', '60+', '60+', '35-60', '60+', '60+', '60+', '35-60', '0-35', '35-60', '60+', '60+', '35-60', '0-35', '60+', '0-35', '35-60', '35-60', '60+', '60+', '35-60', '0-35', '60+', '60+', '35-60', '35-60', '0-35', '35-60', '35-60', '0-35', '0-35', '60+', '0-35', '60+', '35-60', '35-60', '60+', '0-35', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '60+', '0-35', '35-60', '35-60', '0-35', '0-35', '35-60', '35-60', '60+', '35-60', '60+', '60+', '0-35', '35-60', '0-35', '0-35', '35-60', '60+', '0-35', '35-60', '35-60', '60+', '60+', '35-60', '0-35', '0-35', '60+', '60+', '0-35', '0-35', '60+', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '0-35', '35-60', '60+', '0-35', '35-60', '60+', '60+', '35-60', '60+', '60+', '35-60', '0-35', '60+', '60+', '0-35', '35-60', '35-60', '35-60', '60+', '60+', '0-35', '60+', '35-60', '0-35', '0-35', '0-35', '35-60', '35-60', '60+', '0-35', '35-60', '0-35', '35-60', '35-60', '35-60', '60+', '0-35', '35-60', '60+', '60+', '60+', '35-60', '0-35', '0-35', '0-35', '0-35', '0-35', '60+', '60+', '35-60', '60+', '0-35', '35-60', '0-35', '60+')

sex <- c('F', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F')
sex <- c(sex, 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'F', 'F', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'M', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'F', 'F', 'F', 'M', 'F', 'M', 'M', 'M', 'F', 'M', 'M', 'M', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F')


thyroid$age <- age
thyroid$sex <- sex

# Print the column classes in thyroid
sapply(X = thyroid, FUN = class)

# Convert column with character class to factor
thyroid$age <- as.factor(thyroid$age)
thyroid$sex <- as.factor(thyroid$sex)

# Check that all columns are factor or numeric
sapply(X = thyroid, FUN = class)


# Check the class of age column
class(thyroid$age)

# Check the class of sex column
class(thyroid$sex)

# Fit an isolation forest with 100 trees
# thyroid_for <- iForest(thyroid[, -1], nt=100)


# Calculate Gower's distance matrix
thyroid_dist <- cluster::daisy(thyroid[, -1], metric = "gower")

# Generate LOF scores for thyroid data
thyroid_lof <- dbscan::lof(thyroid_dist, k = 10)

# Range of values in the distance matrix
range(as.matrix(thyroid_dist))

```
  
  
  
***
  
### _GARCH Models in R_  
  
Chapter 1 - Standard GARCH Model as the Workhorse  
  
Analyzing volatility:  
  
* GARCH models in R help maximize returns while properly managing risk  
	* Relative gains are key; gain divided by starting point  
* Properties of daily returns include that average return is zero and variability changes over time  
	* sd(sp500ret)  # daily  
    * sqrt(252)*sd(sp500ret)  # annualized, assuming 252 trading days  
* Can use rolling windows to estimate how variability is changing over time - commonly, window is a multiple of 22 (trading days in a month)  
	* library(PerformanceAnalytics)  
    * chart.RollingPerformance(R = sp500ret , width = 22, FUN = "sd.annualized", scale = 252, main = "Rolling 1 month volatility")  
* The GARCH model helps drive greater precision and accuracy than the simple rolling window  
  
GARCH equation for volatility prediction:  
  
* GARCH models attempt to model forward-looking volatility  
	* Input is a time series of returns  
    * May want to predict a future return based on all current and previous returns - arithmeitc mean or ARMA  
    * Can also predict the future volatility based on previous returns and volatilities - weighted average (more recent weighted more highly) most recent prediction errors   
* ARCH is the overall model while GARCH(1, 1) is a generalized form of the model  
	* omega, alpha, beta should all be positive  
    * alpha plus beta must be less than 1 (meaning that variance always reverts to its long-run average)  
* Example of implementing GARCH variance using R (loop starting at 2 because of the lagged predictor)  
    * alpha <- 0.1  
    * beta <- 0.8  
    * omega <- var(sp500ret)*(1-alpha-beta)  # Then: var(sp500ret) = omega/(1-alpha-beta)  
    * e <- sp500ret - mean(sp500ret) # Constant mean  
    * e2 <- e^2  
    * nobs <- length(sp500ret)  
    * predvar <- rep(NA, nobs)  
    * predvar[1] <- var(sp500ret)  # Initialize the process at the sample variance  
    * for (t in 2:nobs){  
    *     predvar[t] <- omega + alpha * e2[t - 1] + beta * predvar[t-1]  # GARCH(1,1) equation  
    * }  
    * predvol <- sqrt(predvar)  # Volatility is sqrt of predicted variance  
    * predvol <- xts(predvol, order.by = time(sp500ret))  
    * uncvol <- sqrt(omega / (1 - alpha-beta))  # We compare with the unconditional volatility  
    * uncvol <- xts(rep(uncvol, nobs), order.by = time(sp500ret))  
  
rugarch package:  
  
* The normal GARCH(1, 1) model with a constant mean is a starting point  
	* Can use maximum likelihood to estimate mean, omega, alpha, and beta  
    * library(rugarch)  
    * garchspec <- ugarchspec( mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH"), distribution.model = "norm")  # first step is ugarchspec  
    * garchfit <- ugarchfit(data = sp500ret , spec = garchspec)  # ugarchfit is the second step  
    * garchforecast <- ugarchforecast(fitORspec = garchfit, n.ahead = 5)  # ugarchforecast is final step  
* Can access the results in a ugarchfit object  
	* The ugarchfit yields an object that contains all the results related to the estimation of the garch model  
    * Methods coef, uncvar, fitted and sigma:  
    * garchcoef <- coef(garchfit)  
    * garchuncvar <- uncvariance(garchfit)  
    * garchmean <- fitted(garchfit)  
    * garchvol <- sigma(garchfit)  
    * sigma(garchforecast)   
    * fitted(garchforecast)   
* Frequently, portfolios are set to a target volatility, with a portion of the portfolio held in cash to manage that  
  
Example code includes:  
```{r eval=FALSE}

library(xts)
library(PerformanceAnalytics)


load("./RInputFiles/sp500prices.RData")
str(sp500prices)


# Plot daily S&P 500 prices
plot(sp500prices)

# Compute daily returns
sp500ret <- CalculateReturns(sp500prices)

# Check the class of sp500ret
class(sp500ret)

# Plot daily returns
plot(sp500ret)


# Compute the daily standard deviation for the complete sample   
sd(sp500ret)

# Compute the annualized volatility for the complete sample
sd(sp500ret) * sqrt(252)

# Compute the annualized standard deviation for the year 2009 
sqrt(252) * sd(sp500ret["2009"])

# Compute the annualized standard deviation for the year 2017 
sqrt(252) * sd(sp500ret["2017"])


# Showing two plots onthe same figure
par(mfrow=c(2,1)) 

# Compute the rolling 1 month estimate of annualized volatility
chart.RollingPerformance(R = sp500ret["2000::2017"], width = 22,
     FUN = "sd.annualized", scale = 252, main = "One month rolling volatility")

# Compute the rolling 3 months estimate of annualized volatility
chart.RollingPerformance(R = sp500ret["2000::2017"], width = 66,
     FUN = "sd.annualized", scale = 252, main = "Three months rolling volatility")

par(mfrow=c(1,1)) 


sp500ret <- sp500ret[2:length(sp500ret), ]

# Compute the mean daily return
m <- mean(sp500ret)

# Define the series of prediction errors
e <- sp500ret - m

# Plot the absolute value of the prediction errors
par(mfrow = c(2,1), mar = c(3, 2, 2, 2))
plot(abs(e))

# Plot the acf of the absolute prediction errors
acf(abs(e))
par(mfrow = c(1,1), mar = c(5.1, 4.1, 4.1, 2.1))


nobs <- length(sp500ret)
predvar <- numeric(nobs)
omega <- 1.2086e-05
alpha <- 0.1
beta <- 0.8
e2 <- e**2

# Compute the predicted variances
predvar[1] <- var(sp500ret) 
for(t in 2:nobs){
   predvar[t] <- omega + alpha * e2[t-1] + beta * predvar[t-1]
}

# Create annualized predicted volatility
ann_predvol <- xts(sqrt(predvar) * sqrt(252), order.by = time(sp500ret))

# Plot the annual predicted volatility in 2008 and 2009
plot(ann_predvol["2008::2009"], main = "Ann. S&P 500 vol in 2008-2009")


# Specify a standard GARCH model with constant mean
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0, 0)),
                 variance.model = list(model = "sGARCH"), 
                 distribution.model = "norm")

# Estimate the model
garchfit <- rugarch::ugarchfit(data = sp500ret, spec = garchspec)

# Use the method sigma to retrieve the estimated volatilities 
garchvol <- rugarch::sigma(garchfit) 

# Plot the volatility for 2017
plot(garchvol["2017"])


# Compute unconditional volatility
sqrt(rugarch::uncvariance(garchfit))

# Print last 10 ones in garchvol
tail(garchvol, 10)

# Forecast volatility 5 days ahead and add 
garchforecast <- rugarch::ugarchforecast(fitORspec = garchfit, n.ahead = 5)

# Extract the predicted volatilities and print them
print(rugarch::sigma(garchforecast))


# Compute the annualized volatility
annualvol <- sqrt(252) * rugarch::sigma(garchfit)

# Compute the 5% vol target weights  
vt_weights <- 0.05 / annualvol

# Compare the annualized volatility to the portfolio weights in a plot
plot(merge(annualvol, vt_weights), multi.panel = TRUE)

```
  
  
  
***
  
Chapter 2 - Improvements of the Normal GARCH Model  
  
Non-normality of standardized returns:  
  
* Normal distributions are generally inconsistent with stock markets - "stairs up and elevators down"  
	* garchspec <- ugarchspec( mean.model=list(armaOrder=c(0,0)), variance.model=list(model="sGARCH"), distribution.model = "sstd")  
    * Caveat: The normality of the standardized returns follows from an assumption  
    * Let's compute the standardized returns and test whether the assumption is correct.  
    * stdret <- residuals(garchfit, standardize = TRUE)  
    * chart.Histogram(sp500ret, methods = c("add.normal", "add.density"), colorset=c("gray","red","blue"))  
* A more realistic distribution needs fat tails and skew  
	* The shape (nu) determines the fatness of the tails  
    * The skew parameter of 1 is symmetry while less than 1 is negative skew  
    * garchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH"), distribution.model = "sstd")  
  
Leverage effect:  
  
* Volatility predictions are unsigned, though the sign of the error matters (large negative returns drive more volatility than large positive returns)  
	* In the case of a negative surprise, it is common to scale-up the squared prediction error - alpha becomes (alpha + gamma) with gamma being positive  
    * GJR model proposed Glosten, Jagannathan and Runkle.  
    * garchspec <- ugarchspec( mean.model=list(armaOrder=c(0,0)), variance.model=list(model="gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * out <- newsimpact(garchfit)  
    * plot(out$zx, out$zy, xlab = "prediction error", ylab = "predicted variance")  
  
Mean model:  
  
* Higher returns come with greater risks; can quantify these using GARCH mean models  
	* The lambda parameter is typically a positive parameter that relates increases in volatility to increases in return  
    * garchspec <- ugarchspec( mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit( data = sp500ret , spec = garchspec)  
    * plot(fitted(garchfit))  
* The AR(1) model is a common approach used to drive correlation in returns - can be a negative or a positive parameter  
	* garchspec <- ugarchspec( mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = sp500ret, spec = garchspec)  
    * round(coef(garchfit)[1:2], 4)  
* The MA(1) and ARMA(1, 1) are also popular models  
	* garchspec <- ugarchspec( mean.model = list(armaOrder = c(0,1)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchspec <- ugarchspec( mean.model = list(armaOrder = c(1,1)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
  
Avoid unnecessary complexity:  
  
* Complexity has a price including loss of parsimony; avoid unneeded complexity  
* Can use setfixed() or setbounds() to set boundaries for the data  
	* setfixed(garchspec) <- list(alpha1 = 0.05, shape = 6)  
    * garchfit <- ugarchfit(data = EURUSDret, spec = garchspec)  
    * setbounds(garchspec) <- list("alpha1" = c(0.05,0.2), "beta1" = c(0.8,0.95))  
* Can also use variance targeting  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH", variance.targeting = TRUE), distribution.model = "std")  
    * garchfit <- ugarchfit(data = EURUSDret, spec = garchspec)  
  
Example code includes:  
```{r eval=FALSE}

load("./RInputFiles/ret.RData")
str(ret)


# Plot the return series
plot(ret)

# Specify the garch model to be used
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                 variance.model = list(model = "sGARCH"),
                                 distribution.model = "sstd"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data = ret, spec = garchspec)

# Inspect the coefficients
rugarch::coef(garchfit)


# Compute the standardized returns
stdret <- rugarch::residuals(garchfit, standardize = TRUE)

# Compute the standardized returns using fitted() and sigma()
stdret <- (ret - rugarch::fitted(garchfit)) / rugarch::sigma(garchfit)

# Load the package PerformanceAnalytics and make the histogram
chart.Histogram(stdret, methods = c("add.normal","add.density" ), colorset = c("gray","red","blue"))


load("./RInputFiles/msftret.RData")
str(msftret)


# Specify the GJR GARCH model
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                 variance.model = list(model = "gjrGARCH"),
                                 distribution.model = "sstd"
                                 )

# Estimate the model and compute volatility
gjrgarchfit <- rugarch::ugarchfit(data = msftret, spec = garchspec)
gjrgarchvol <- rugarch::sigma(gjrgarchfit)

# Compare volatility
plotvol <- plot(abs(msftret), col = "grey")
plotvol <- addSeries(gjrgarchvol, col = "red", on=1)
# plotvol <- addSeries(sgarchvol, col = "blue", on=1)
plotvol


# Specify AR(1)-GJR GARCH model
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1, 0)),
                                 variance.model = list(model = "gjrGARCH"),
                                 distribution.model = "sstd"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data=msftret, spec=garchspec)

# Print the first two coefficients
rugarch::coef(garchfit)[c(1:2)]


# GARCH-in-Mean specification and estimation
gim_garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2),
                                     variance.model = list(model = "gjrGARCH"), 
                                     distribution.model = "sstd"
                                     )
gim_garchfit <- rugarch::ugarchfit(data = msftret , spec = gim_garchspec)

# Predicted mean returns and volatility of GARCH-in-mean
gim_mean <- rugarch::fitted(gim_garchfit)
gim_vol <- rugarch::sigma(gim_garchfit)


ar1_garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0), archm = TRUE, archpow = 2),
                                     variance.model = list(model = "sGARCH"), 
                                     distribution.model = "sstd"
                                     )
ar1_garchfit <- rugarch::ugarchfit(data = msftret , spec = ar1_garchspec)
ar1_mean <- rugarch::fitted(ar1_garchfit)
ar1_vol <- rugarch::sigma(ar1_garchfit)

cmu_garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2),
                                     variance.model = list(model = "sGARCH"), 
                                     distribution.model = "sstd"
                                     )
cmu_garchfit <- rugarch::ugarchfit(data = msftret , spec = cmu_garchspec)
constmean_mean <- rugarch::fitted(cmu_garchfit)
constmean_vol <- rugarch::sigma(cmu_garchfit)

# Correlation between predicted return using AR(1) and GARCH-in-mean models
cor(ar1_mean, gim_mean)

# Correlation between predicted volatilities across mean.models
cor(merge(constmean_vol, ar1_vol, gim_vol))


load("./RInputFiles/EURUSDret.RData")
str(EURUSDret)


flexgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0), archm = FALSE),
                                     variance.model = list(model = "sGARCH"), 
                                     distribution.model = "sstd"
                                     )
flexgarchfit <- rugarch::ugarchfit(data = EURUSDret , spec = flexgarchspec)


# Print the flexible GARCH parameters
rugarch::coef(flexgarchfit)

# Restrict the flexible GARCH model by impose a fixed ar1 and skew parameter
rflexgarchspec <- flexgarchspec
rugarch::setfixed(rflexgarchspec) <- list(ar1 = 0, skew = 1)

# Estimate the restricted GARCH model
rflexgarchfit <- rugarch::ugarchfit(data = EURUSDret,  spec = rflexgarchspec)

# Compare the volatility of the unrestricted and restriced GARCH models
plotvol <- plot(abs(EURUSDret), col = "grey")
plotvol <- addSeries(rugarch::sigma(flexgarchfit), col = "black", lwd = 4, on=1 )
plotvol <- addSeries(rugarch::sigma(rflexgarchfit), col = "red", on=1)
plotvol


# Define bflexgarchspec as the bound constrained version
bflexgarchspec <- flexgarchspec
rugarch::setbounds(bflexgarchspec) <- list(alpha1 = c(0.05, 0.2), beta1 = c(0.8, 0.95))

# Estimate the bound constrained model
bflexgarchfit <- rugarch::ugarchfit(data = EURUSDret, spec = bflexgarchspec)

# Inspect coefficients
rugarch::coef(bflexgarchfit)

# Compare forecasts for the next ten days
cbind(rugarch::sigma(rugarch::ugarchforecast(flexgarchfit, n.ahead = 10)),
      rugarch::sigma(rugarch::ugarchforecast(bflexgarchfit, n.ahead = 10))
      )


# Complete the specification to do variance targeting
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                 variance.model = list(model = "sGARCH", variance.targeting = TRUE),
                                 distribution.model = "std"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data = EURUSDret, spec = garchspec)

# Print the GARCH model implied long run volatility
sqrt(rugarch::uncvariance(garchfit))

# Verify that it equals the standard deviation (after rounding)
all.equal(sqrt(rugarch::uncvariance(garchfit)), sd(EURUSDret), tol = 1e-4)

```
    
  
  
***
  
Chapter 3 - Performance Evaluation  
  
Statistical Significance:  
  
* The model should only use meaningful and significant variables  
	* Can simplify by removing non-significant parameters  
    * Since we do not know the true parameters, the zero parameters need to be estimated  
    * round(coef(flexgarchfit), 6)  
* Tests for statistical significance are available in the rgarch package - general rule of thumb is to keep is abs(t-value) >= 2  
	* flexgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * flexgarchfit <- ugarchfit(data = msftret, spec = flexgarchspec)  
    * round(flexgarchfit@fit$matcoef, 6)  
  
Goodness of Fit:  
  
* Evaluation criterion depend on the elements that you want to assess  
* Goodness of fit for mean predictions  
	* e <- residuals(tgarchfit)  
    * mean(e^2)  
* Goodness of fit for variance predictions  
	* e <- residuals(tgarchfit)  
    * d <- e^2 - sigma(tgarchfit)^2  
    * mean(d^2)  
* Goodness of fit for the distribution - higher density means more likely to see returns  
	* The higher the density, the more likely the return is under the estimated GARCH model  
    * The likelihood of the sample is based on the product of all these densities. It measures how likely it is to that the observed returns come from the estimated GARCH model  
    * The higher the likelihood, the better the model fits with your data  
    * likelihood(tgarchfit)  # compare with likelihood achieved using other models; higher likelihood is better  
* There is a risk of over-fitting since the in-sample data is used for the goodness of fit tests - should add penalties for non-parsimonious models  
	* information criteria = - likelihood + penalty(number of parameters)  
    * infocriteria(tgarchfit)  
  
Diagnosing Absolute Standardized Returns:  
  
* Can check the standardized returns - scaled for mean 0 and standard deviation 1  
	* Standardized returns should be constant over time, provided the model is making good estimates of mean and sigma over time  
    * Should also be no correlations (auto-correlations) in the standardized returns over time  
* Example application to the daily MSFT returns  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * stdmsftret <- residuals(garchfit, standardize = TRUE)  
    * acf(abs(msftret), 22)  
    * acf(abs(stdmsftret), 22)  
* Can check the auto-correlations using the Ljung-Box test - desire for p to be GREATER than 5% (no significant auto-correlations found)  
	* Box.test(abs(stdmsftret), 22, type = "Ljung-Box")  
  
Back-testing using ugarchroll:  
  
* Can use rolling estimation to avoid look-ahead bias  
	* tgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "sGARCH"), distribution.model = "std")  
    * garchroll <- ugarchroll(tgarchspec, data = EURUSDret, n.start = 2500, refit.window = "moving", refit.every = 500)  
* There are five arguments to specify  
	* GARCH specification used  
    * data : return data to use  
    * n.start: the size of the initial estimation sample  
    * refit.window: how to change that sample through time: "moving" or "expanding  
    * refit.every: how often to re-estimate the model  
* Can convert results to data frames  
	* preds <- as.data.frame(garchroll)  
    * preds$Mu: series of predicted mean values  
    * preds$Sigma: series of predicted volatility values  
    * garchvol <- xts(preds$Sigma, order.by = as.Date(rownames(preds)))  
    * plot(garchvol)  
    * preds <- as.data.frame(garchroll)  
    * Evaluate accuracy of preds$Mu and preds$Sigma by comparing with preds$Realized  
    * e <- preds$Realized - preds$Mu  
    * mean(e^2)  
    * d <- e^2 - preds$Sigma^2  
    * mean(d^2)  
* Can run a comparison of two models  
	* tgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "sGARCH"), distribution.model = "std")  
    * garchroll <- ugarchroll(tgarchspec, data = EURUSDret, n.start = 2500, refit.window = "moving", refit.every = 500)  
    * gjrgarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * gjrgarchroll <- ugarchroll(gjrgarchspec, data = EURUSDret, n.start = 2500, refit.window = "moving", refit.every = 500)  
  
Example code includes:  
```{r eval=FALSE}

# Specify model with AR(1) dynamics, GJR GARCH and skewed student t
flexgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)),
                                     variance.model = list(model = "gjrGARCH"),
                                     distribution.model = "sstd"
                                     )

# Estimate the model
flexgarchfit <- rugarch::ugarchfit(data = EURUSDret, spec = flexgarchspec)

# Complete and study the statistical significance of the estimated parameters  
round(flexgarchfit@fit$matcoef, 6)


# Specify model with constant mean, standard GARCH and student t
tgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1, 0)),
                                  variance.model = list(model = "sGARCH", variance.targeting = TRUE),
                                  distribution.model = "sstd"
                                  )

# Fix the mu parameter at zero
rugarch::setfixed(tgarchspec) <- list("mu" = 0)

# Estimate the model
tgarchfit <- rugarch::ugarchfit(data = EURUSDret, spec = tgarchspec)

# Verify that the differences in volatility are small
plot(rugarch::sigma(tgarchfit) - rugarch::sigma(flexgarchfit))


# Compute prediction errors
garcherrors <- rugarch::residuals(flexgarchfit)
gjrerrors  <- rugarch::residuals(tgarchfit)

# Compute MSE for variance prediction of garchfit model
mean((rugarch::sigma(flexgarchfit)**2 - garcherrors^2)**2)

# Compute MSE for variance prediction of gjrfit model
mean((rugarch::sigma(tgarchfit)**2 - gjrerrors^2)**2)


# Print the number of estimated parameters
length(rugarch::coef(flexgarchfit))
length(rugarch::coef(tgarchfit))

# Print likelihood of the two models
rugarch::likelihood(flexgarchfit)
rugarch::likelihood(tgarchfit)

# Print the information criteria of the two models
rugarch::infocriteria(flexgarchfit)
rugarch::infocriteria(tgarchfit)


# Compute the standardized returns
stdEURUSDret <- rugarch::residuals(tgarchfit, standardize = TRUE)

# Compute their sample mean and standard deviation
mean(stdEURUSDret)
sd(stdEURUSDret)

# Correlogram of the absolute (standardized) returns
par(mfrow = c(1, 2))
acf(abs(EURUSDret), 22)
acf(abs(stdEURUSDret), 22)
par(mfrow = c(1, 1))

# Ljung-Box test
Box.test(abs(stdEURUSDret), 22, type = "Ljung-Box")


# Estimate the model on the last 2500 observations
tgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0,0)),
                                  variance.model = list(model = "sGARCH"),
                                  distribution.model = "std"
                                  )
tgarchfit <- rugarch::ugarchfit(data = tail(EURUSDret, 2500) , spec = tgarchspec)

# Compute standardized returns
stdEURUSDret <- rugarch::residuals(tgarchfit, standardize = TRUE)

# Do the Ljung-Box test on the absolute standardized returns
Box.test(abs(stdEURUSDret), 22, type = "Ljung-Box")


# Estimate the GARCH model using all the returns and compute the in-sample estimates of volatility
garchinsample <- rugarch::ugarchfit(data = sp500ret, spec = flexgarchspec)
garchvolinsample <- rugarch::sigma(garchinsample)

# Use ugarchroll for rolling estimation of the GARCH model 
garchroll <- rugarch::ugarchroll(flexgarchspec, data = sp500ret, 
                                 n.start = 2000, refit.window = "moving",  refit.every = 2500
                                 )

# Set preds to the data frame with rolling predictions
preds <- rugarch::as.data.frame(garchroll)

# Compare in-sample and rolling sample volatility in one plot
garchvolroll <- xts(preds$Sigma, order.by = as.Date(rownames(preds)))
volplot <- plot(garchvolinsample, col = "darkgrey", lwd = 1.5, 
                main = "In-sample versus rolling vol forecasts"
                )
volplot <- addSeries(garchvolroll, col = "blue", on = 1)
plot(volplot)


# Inspect the first three rows of the dataframe with out of sample predictions
head(preds, 3)

# Compute prediction errors
e <- preds$Realized - preds$Mu  
d <- e^2 - preds$Sigma^2 

# Compute MSE for the garchroll variance prediction
garchMSE <- mean(d^2)

# Use ugarchroll for rolling estimation of the GARCH model 
gjrgarchroll <- rugarch::ugarchroll(tgarchspec, data = sp500ret, 
                                    n.start = 2000, refit.window = "moving",  refit.every = 2500
                                    )

# Compute MSE for gjrgarchroll
gjrgarchpreds <- rugarch::as.data.frame(gjrgarchroll)
e <- gjrgarchpreds$Realized - gjrgarchpreds$Mu  
d <- e^2 - gjrgarchpreds$Sigma^2 
gjrgarchMSE <- mean(d**2)

```
  
  
  
***
  
Chapter 4 - Applications  
  
Value at Risk:  
  
* A common metrics is the 5% VaR, which is the amount that would be lost in the "best case" of the bottom 5% of returns over a specified period of time (such as a year)  
	* Target is to predict the VaR forward rather than assess it backwards  
* Workflow for assessing VaR in R  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchroll <- ugarchroll(garchspec, data = sp500ret, n.start = 2500, refit.window = "moving", refit.every = 100)  
    * garchVaR <- quantile(garchroll, probs = 0.05)  
    * actual <- xts(as.data.frame(garchroll)$Realized, time(garchVaR))  
    * VaRplot(alpha = 0.05, actual = actual, VaR = garchVaR)  
* The VaR exceedence should be 5% (if the VaR is set at the best of the 5%) - called the "coverage" of the VaR  
  
Production and Simulation:  
  
* Data used for modeling will be different than data used during production  
	* Use ugarchfilter() for analyzing the recent dynamics in the mean and volatility  
    * Use ugarchforecast() applied to a ugarchspec object (instead of ugarchfit()) object for making the predictions about the future mean and volatility  
* Example of running the full process  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret["/2010-12"], spec = garchspec)  
    * progarchspec <- garchspec  
    * setfixed(progarchspec) <- as.list(coef(garchfit))  
    * garchfilter <- ugarchfilter(data = msftret, spec = progarchspec)  
    * plot(sigma(garchfilter))  
    * garchforecast <- ugarchforecast(data = msftret, fitORspec = progarchspec, n.ahead = 10)  
    * cbind(fitted(garchforecast), sigma(garchforecast))  
* Can use the model to simulate log returns - useful to assess randomness in future returns  
	* msftlogret <- diff(log(MSFTprice))[(-1)]  
    * garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftlogret, spec = garchspec)  
    * simgarchspec <- garchspec  
    * setfixed(simgarchspec) <- as.list(coef(garchfit))  
* Can run simulations using ugarchpath()  
	* spec : completely specified GARCH model  
    * m.sim : number of time series of simulated returns you want  
    * n.sim: number of observations in the simulated time series (e.g. 252)  
    * rseed : any number to fix seed used to generate the simulated series (needed for reproducibility)  
    * simgarch <- ugarchpath(spec = simgarchspec, m.sim = 4, n.sim = 10 * 252, rseed = 12345)  
    * simret <- fitted(simgarch)  
    * plot.zoo(simret)  
    * plot.zoo(sigma(simgarch))  
    * simprices <- exp(apply(simret, 2, "cumsum"))  
    * matplot(simprices, type = "l", lwd = 3)  
  
Model Risk:  
  
* Model averaging is often more valuable than a single point estimate from a single model  
	* Sensitivity analysis and outlier removal can be valuable  
    * variance.models <- c("sGARCH", "gjrGARCH")  
    * distribution.models <- c("norm", "std", "std")  
    * c <- 1  
    * for (variance.model in variance.models) {  
    *     for (distribution.model in distribution.models) {  
    *         garchspec <- ugarchspec(mean.model = list(armaOrder = c(0, 0)), variance.model = list(model = variance.model), distribution.model = distribution.model)  
    *         garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    *         if (c==1) { msigma <- sigma(garchfit) } else { msigma <- merge(msigma, sigma(garchfit)) }
    *         c <- c + 1  
    *     }
    * }  
    * avesigma <- xts(rowMeans(msigma), order.by = time(msigma))  
* Robustness to starting values - additional source of risk to the GARCH models  
	* garchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)), variance.model = list(model = "sGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = sp500ret, spec = garchspec)  
    * coef(garchfit)  
    * likelihood(garchfit)  
    * setstart(garchspec) <- list(alpha1 = 0.05, beta1 = 0.9, shape = 8)  
    * garchfit <- ugarchfit(data = sp500ret, spec = garchspec)  
* Outliers in the underlying return data are an additional source of potential error  
	* library(PerformanceAnalytics)  
    * clmsftret <- Return.clean(msftret, method = "boudt")  
    * plotret <- plot(msftret, col = "red")  
    * plotret <- addSeries(clmsftret, col = "blue", on = 1)  
    * garchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)), variance.model = list(model = "gjrGARCH"), distribution.model = "sstd")  
    * garchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * clgarchfit <- ugarchfit(data = clmsftret, spec = garchspec)  
    * plotvol <- plot(abs(msftret), col = "gray")  
    * plotvol <- addSeries(sigma(garchfit), col = "red", on = 1)  
    * plotvol <- addSeries(sigma(clgarchfit), col = "blue", on = 1)  
    * plotvol  
  
GARCH Covariance:  
  
* Covariances of asset returns may vary over time  
* GARCH covariance can be estimated in four steps  
	* Step 1: Use ugarchfit() to estimate the GARCH model for each return series.  
    * msftgarchfit <- ugarchfit(data = msftret, spec = garchspec)  
    * wmtgarchfit <- ugarchfit(data = wmtret, spec = garchspec)  
    * Step 2: Use residuals() to compute the standardized returns.  
    * stdmsftret <- residuals(msftgarchfit, standardize = TRUE)  
    * stdwmtret <- residuals(wmtgarchfit, standardize = TRUE)  
    * Step 3: Use cor() to estimate \rho? as the sample correlation of the standardized returns.  
    * msftwmtcor <- as.numeric(cor(stdmsftret, stdwmtret))  
    * msftwmtcor  
    * Step 4: Compute the GARCH covariance by multiplying the estimated correlation and volatilities  
    * msftwmtcov <- msftwmtcor * sigma(msftgarchfit) * sigma(wmtgarchfit)  
* Covariance has many applications in finance  
	* Optimization of portfolio variance  
    * msftvar <- sigma(msftgarchfit)^2  
    * wmtvar <- sigma(wmtgarchfit)^2  
    * msftwmtcov <- msftwmtcor * sigma(msftgarchfit) * sigma(wmtgarchfit)  
    * msftweight <- (wmtvar - msftwmtcov) / (msftvar + wmtvar - 2 * msftwmtcov)  
    * Dynamic beta (systematic risk) of a specific stock  
    * msftsp500cor <- as.numeric(cor(stdmsftret, stdsp500ret))  
    * msftsp500cov <- msftsp500cor * sigma(msftgarchfit) * sigma(sp500garchfit)  
    * sp500var <- sigma(sp500garchfit)^2  
    * msftbeta <- msftsp500cov / sp500var  
  
Wrap Up:  
  
* Language of GARCH models  
	* Volatility and volatility clustering  
    * Information set and predictions  
    * Leverage effect and GJR GARCH  
    * Skewness, fat tails, and student t  
    * Ljung-Box test, MSE, and other model validation methods  
* Language of rugarch  
	* ugarchspec()  
    * ugarchfit()  
    * ugarchroll()  
    * ugarchforecast()  
    * ugarchfilter()  
    * ugarchpath()  
    * Many useful methods sigma(), fitted(), coef(), infocriteria(), likelihood(), setfixed(), setbounds(), quantile()...  
  
Example code includes:  
```{r eval=FALSE}

flexgarchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)),
                                     variance.model = list(model = "gjrGARCH"),
                                     distribution.model = "sstd"
                                     )
garchroll <- rugarch::ugarchroll(flexgarchspec, data = msftret, 
                                 n.start = 2000, refit.window = "moving",  refit.every = 2500
                                 )

# Extract the dataframe with predictions from the rolling GARCH estimation
garchpreds <- rugarch::as.data.frame(garchroll)

# Extract the 5% VaR 
garchVaR <- rugarch::quantile(garchroll, probs = 0.05)

# Extract the volatility from garchpreds
garchvol <- xts(garchpreds$Sigma, order.by = time(garchVaR))

# Analyze the comovement in a time series plot
garchplot <- plot(garchvol, ylim = c(-0.1, 0.1))
garchplot <- addSeries(garchVaR, on = 1, col = "blue")
plot(garchplot, main = "Daily vol and 5% VaR")


# Take a default specification a with a normal and skewed student t distribution
normgarchspec <- rugarch::ugarchspec(distribution.model = "norm")
sstdgarchspec <- rugarch::ugarchspec(distribution.model = "sstd")

# Do rolling estimation
normgarchroll <- rugarch::ugarchroll(normgarchspec, data = msftret, n.start = 2500, 
                                     refit.window = "moving", refit.every = 2000
                                     )
sstdgarchroll <- rugarch::ugarchroll(sstdgarchspec, data = msftret, n.start = 2500, 
                                     refit.window = "moving", refit.every = 2000
                                     )

# Compute the 5% value at risk
normgarchVaR <- rugarch::quantile(normgarchroll, probs = 0.05)
sstdgarchVaR <- rugarch::quantile(sstdgarchroll, probs = 0.05)

# Compute the coverage
actual <- xts(rugarch::as.data.frame(normgarchroll)$Realized, time(normgarchVaR))
mean(actual < normgarchVaR)
mean(actual < sstdgarchVaR)


garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)), 
                                 variance.model = list(model = "gjrGARCH"), 
                                 distribution.model = "sstd"
                                 )

# Estimate the model
garchfit <- rugarch::ugarchfit(data = sp500ret["/2006-12"], spec = garchspec)

# Fix the parameters
progarchspec <- garchspec
rugarch::setfixed(progarchspec) <- as.list(rugarch::coef(garchfit))

# Use ugarchfilter to obtain the estimated volatility for the complete period
garchfilter <- rugarch::ugarchfilter(data = sp500ret, spec = progarchspec)
plot(rugarch::sigma(garchfilter))

# Compare the 252 days ahead forecasts made at the end of September 2008 and September 2017
garchforecast2008 <- rugarch::ugarchforecast(data = sp500ret["/2008-09"], 
                                             fitORspec = progarchspec, n.ahead = 252
                                             )
garchforecast2017 <- rugarch::ugarchforecast(data = sp500ret["/2017-09"], 
                                             fitORspec = progarchspec, n.ahead = 252
                                             )
par(mfrow = c(2, 1), mar = c(3, 2, 3, 2))
plot(rugarch::sigma(garchforecast2008), main = "/2008-09", type = "l")
plot(rugarch::sigma(garchforecast2017), main = "/2017-09", type = "l")
par(mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1))


simgarchspec <- garchspec
rugarch::setfixed(simgarchspec) <- as.list(rugarch::coef(garchfit))

# Complete the code to simulate 4 time series of 10 years of daily returns
simgarch <- rugarch::ugarchpath(spec=simgarchspec, m.sim = 4, n.sim = 10*252, rseed = 210) 

# Plot the simulated returns of the four series
simret <- rugarch::fitted(simgarch)
plot.zoo(simret)
plot.zoo(rugarch::sigma(simgarch))

# Compute the corresponding simulated prices and plot them
simprices <- exp(apply(simret, 2, "cumsum"))
matplot(simprices, type = "l", lwd = 3)


# Specify model with constant mean, standard GARCH and student t
garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(0, 0)),
                                 variance.model = list(model = "sGARCH", variance.targeting = FALSE),
                                 distribution.model = "std"
                                 )

# Estimate using default starting values
garchfit <- rugarch::ugarchfit(spec=garchspec, data=EURUSDret)

# Print the estimated parameters and the likelihood
rugarch::coef(garchfit)
rugarch::likelihood(garchfit)

# Set other starting values and re-estimate
rugarch::setstart(garchspec) <- list(alpha1 = 0.05, beta1 = 0.9, shape = 6) 
garchfit <- rugarch::ugarchfit(spec=garchspec, data=EURUSDret)

# Print the estimated parameters and the likelihood
rugarch::coef(garchfit)
rugarch::likelihood(garchfit)


garchspec <- rugarch::ugarchspec(mean.model = list(armaOrder = c(1,0)), 
                                 variance.model = list(model = "gjrGARCH"), 
                                 distribution.model = "sstd"
                                 )
usgarchfit <- rugarch::ugarchfit(spec=garchspec, data=ret["2009/2017"])
eugarchfit <- rugarch::ugarchfit(spec=garchspec, data=msftret["2009/2017"])

# Compute the standardized US and EU returns, together with their correlation 
stdusret <- rugarch::residuals(usgarchfit, standardize = TRUE)
stdeuret <- rugarch::residuals(eugarchfit, standardize = TRUE)

useucor <- as.numeric(cor(stdusret, stdeuret))
print(useucor)

# Compute the covariance and variance of the US and EU returns 
useucov <- useucor * rugarch::sigma(usgarchfit) * rugarch::sigma(eugarchfit)
usvar <- rugarch::sigma(usgarchfit)**2
euvar <- rugarch::sigma(eugarchfit)**2

# Compute the minimum variance weight of the US ETF in the US-EU ETF portfolio 
usweight <- (euvar - useucov) / (usvar + euvar - 2*useucov)
plot(usweight)


# Compute standardized returns
# stdmsftret <- residuals(msftgarchfit, standardize=TRUE)
# stdwmtret <- residuals(wmtgarchfit, standardize=TRUE)

# Print the correlation
# cor(stdmsftret, stdwmtret)

# Load the package PerformanceAnalytics
# library(PerformanceAnalytics)

# Plot the 3-month rolling correlation
# chart.RollingCorrelation(stdmsftret, stdwmtret, width = 66, main = "3-month rolling correlation between MSFT and WMT daily returns")

```
  
  
  
***
  
### _RNA-Seq Differential Expression Analysis_  
  
Chapter 1 - Introduction to RNA-Seq Theory and Workflow  
  
Introduction to RNA-Seq:  
  
* Discovery of genes that are differentially expressed between groups  
* The genome contains the instructions for life - double-stranded DNA with chromosomes built of nucleotides (G-A-C-T)  
	* A-T and G-C nucleotides are paired up; the order is called the DNA sequence  
    * Within the sequence, there are genes, which create messenger RNA which produces proteins  
    * To convert to proteins, messenger RNA must undergo transcription (DNA -> pre-mRNA -> Mature-mRNA -> Protein)  
* Muscle cells, nerve cells, and other cells activate different portions of the DNA  
	* Mutations can effect the types of quantities of DNA that are produced  
* Differential expression analysis looks at differences in gene expression across groups, over time, correlated with other changes, driven by what pathways, etc.  
  
RNA-Seq Workflow:  
  
* Experiment planning is important  
	* Technical replicates: Generally low technical variation, so unnecessary  
    * Biological replicates: Crucial to the success of RNA-Seq differential expression analyses. The more replicates the better, but at the very least have 3  
    * Batch effects: Avoid as much as possible and note down all experimental variables  
* Biological sample preparation - isolate RNA, generate cDNA, FASTQ sequencing, RNA reads (sequence ends)  
	* Followed by mapping to the genome (exon and intron alignment is needed)  
    * Counting reads associated with genes - drives estimates of gene counts  
* Can run the process using Bioconductor  
	* wt_rawcounts <- read.csv("fibrosis_wt_rawcounts.csv")  
  
Differential Gene Expression Theory:  
  
* Differential expression is based on normalized counts, adjusted for variation in the data  
	* Are the differences between groups significant?  
* Course will use a publicly available dataset based on SMOC2 data as related to kidney fibrosis  
* Often, there is a long right tail - Poisson might be a good model if there were no biological variation  
	* ggplot(raw_counts) + geom_histogram(aes(x = wt_normal1), stat = "bin", bins = 200) + xlab("Raw expression counts") + ylab("Number of genes")  
* The negative binomial model works better for the real-world data  
	* wt_rawcounts <- read.csv("fibrosis_wt_rawcounts.csv")  
    * genotype <- c("wt", "wt", "wt", "wt", "wt", "wt", "wt")  
    * condition <- c("normal", "fibrosis", "normal", "fibrosis", "normal", "fibrosis", "fibrosis")  
    * wt_metadata <- data.frame(genotype, wildtype)  
  
Example code includes:  
```{r eval=FALSE}

# Load library for DESeq2
library(DESeq2)

# Load library for RColorBrewer
library(RColorBrewer)

# Load library for pheatmap
library(pheatmap)

# Load library for tidyverse
library(tidyverse)


# Explore the first six observations of smoc2_rawcounts
head(smoc2_rawcounts)

# Explore the structure of smoc2_rawcounts
str(smoc2_rawcounts)


# Create genotype vector
genotype <- rep("smoc2_oe", 7)

# Create condition vector
condition <- c(rep("fibrosis", 4), rep("normal", 3))

# Create data frame
smoc2_metadata <- data.frame(genotype, condition)

# Assign the row names of the data frame
rownames(smoc2_metadata) <- paste0("smoc2_", condition, c(1:4, 1, 3, 4))

```
  
  
  
***
  
Chapter 2 - Exploratory Data Analysis  
  
Introduction to Differential Expression Analysis:  
  
* The DESeq2 is a common method for running differential expression analysis  
	* vignette(DESeq2)  
* The DE analysis comes after quality control  
	* Quality control - normalization and unsupervised clustering analysis  
    * DE analysis - modelling raw counts, shrinking log2 fold changes, differential expression analysis  
* Need to bring in the raw counts  
	* wt_rawcounts <- read.csv("fibrosis_wt_rawcounts.csv")  
    * wt_metadata <- read.csv("fibrosis_wt_metadata_unordered.csv")  
  
Organizing the data for DESeq2:  
  
* Need to have the metadata and the sample data in the same order and with the same rownames/colnames  
	* all(rownames(wt_metadata) == colnames(wt_rawcounts))  
    * match(vector1, vector2)  # vector 1 has the desired order, vector2 has the target vector for reordering, output are the indices for reogranizing vector2 to align with vector1  
    * idx <- match(colnames(wt_rawcounts), rownames(wt_metadata))  
    * reordered_wt_metadata <- wt_metadata[idx, ]  
    * all(rownames(reordered_wt_metadata) == colnames(wt_rawcounts))  
* Can then create the DESeq2 object  
	* dds_wt <- DESeqDataSetFromMatrix(countData = wt_rawcounts, colData = reordered_wt_metadata, design = ~ condition)  
  
Count Normalization:  
  
* First step is to normalize raw counts to assess sample-level consistencies  
	* Factors other than RNA expression can drive the expression of genes - library depth, gene length (longer gene means more fragments for sequencing), RNA composition  
    * dds_wt <- estimateSizeFactors(dds_wt)  
    * sizeFactors(dds_wt)  
    * normalized_wt_counts <- counts(dds_wt, normalized=TRUE)  
  
Hierarchical Heatmap:  
  
* Can compare the normalized counts across samples - starting with visualization and clustering methods such as hclust() or PCA  
	* vsd_wt <- vst(dds_wt, blind=TRUE)  # vst is a logarithmic transformation that moderates variance; blind=TRUE means that it should be blinded to the sampling metadata, and should be set for QC  
* Can run hierarchical clustering combines with heatmaps to assess clustering - generally, should see correlations > 0.8 unless there are outliers suggestive of quality errors (can confirm with PCA)  
	* vsd_mat_wt <- assay(vsd_wt)  
    * vsd_cor_wt <- cor(vsd_mat_wt)  
    * library(pheatmap)  
    * pheatmap(vsd_cor_wt, annotation = select(wt_metadata, condition))  
  
Principal Component Analysis:  
  
* PCA is a technique to emphasize the variation within a dataset - first component represents the greatest variance  
	* Genes can be given scores based on the degree to which they influence eachprincipal component  
* PCA can be performed using native DESeq2 functions  
	* plotPCA(vsd_wt, intgroup="condition")  # intgroup is the metadata variable for coloring  
  
Example code includes:  
```{r eval=FALSE}

# Use the match() function to reorder the columns of the raw counts
match(rownames(smoc2_metadata), colnames(smoc2_rawcounts))

# Reorder the columns of the count data
reordered_smoc2_rawcounts <- smoc2_rawcounts[, match(rownames(smoc2_metadata), colnames(smoc2_rawcounts))]

# Create a DESeq2 object
dds_smoc2 <- DESeqDataSetFromMatrix(countData =  reordered_smoc2_rawcounts,
                              colData =  smoc2_metadata,
                              design = ~ condition)


# Determine the size factors to use for normalization
dds_smoc2 <- estimateSizeFactors(dds_smoc2)

# Extract the normalized counts
smoc2_normalized_counts <- counts(dds_smoc2, normalized=TRUE)


# Transform the normalized counts 
vsd_smoc2 <- vst(dds_smoc2, blind=TRUE)

# Extract the matrix of transformed counts
vsd_mat_smoc2 <- assay(vsd_smoc2)

# Compute the correlation values between samples
vsd_cor_smoc2 <- cor(vsd_mat_smoc2) 

# Plot the heatmap
pheatmap(vsd_cor_smoc2, annotation = select(smoc2_metadata, condition))


# Transform the normalized counts 
vsd_smoc2 <- vst(dds_smoc2, blind = TRUE)

# Plot the PCA of PC1 and PC2
plotPCA(vsd_smoc2, intgroup="condition")

```
  
  
  
***
  
Chapter 3 - Differential Expression Analysis with DESeq2  
  
DE Analysis:  
  
* The DE analysis workflow includes 1) fitting raw counts to the negative binomial model, 2) shrinking the log2 fold changes, and 3) visualizing results  
* Begin by fitting raw counts to the negative binomial model  
	* dds_wt <- DESeqDataSetFromMatrix(countData = wt_rawcounts, colData = reordered_wt_metadata, design = ~ condition)  # can have ~ a + b + c in the design=, with the main component (e.g., treatment) last  
    * ~ strain + sex + treatment + sex:treatment  # interaction term for sex and treatment; include as the last term  
    * dds_wt <- DESeq(dds_wt)  
  
DESeq2 Model - Dispersion:  

* Can begin by exploring the model fit - are genes expressed differently across rather than within groups  
	* Log2(TreatmentMean / ControlMean)  
    * mean_counts <- apply(wt_rawcounts[, 1:3], 1, mean)  
    * variance_counts <- apply(wt_rawcounts[, 1:3], 1, var)  
    * df <- data.frame(mean_counts, variance_counts)  
    * ggplot(df) + geom_point(aes(x=mean_counts, y=variance_counts)) + scale_y_log10() + scale_x_log10() + xlab("Mean counts per gene") + ylab("Variance per gene")  
* Variance vs. mean is called "dispersion" in the DESeq2 modeling  
	* Dispersion: Variance = Mean + Dispersion * Mean**2  
    * Increase in variance leads to increase in dispersion  
    * Increase in mean leads to descrease in dispersion  
    * plotDispEsts(dds_wt)  
  
DESeq2 Model - Contrasts:  
  
* The negative binomial model is good for representing RNA Seq data  
* By default, the Wald test is run based on the condition  
	* results(wt_dds, alpha = 0.05)  # alpha is the desired significance  
* Can supply own contrasts using the contrasts argument  
	* GENERAL SYNTAX: results(dds, contrast = c("condition_factor", "level_to_compare", "base_level"), alpha = 0.05)  
    * wt_res <- results(dds_wt, contrast = c("condition", "fibrosis", "normal"), alpha = 0.05)  
* Can use the MA plot for further explorations  
	* plotMA(wt_res, ylim=c(-8,8))  
* LFC shrinkage can be helpful for addressing high dispersions or low means  
	* wt_res <- lfcShrink(dds_wt, contrast=c("condition", "fibrosis", "normal"), res=wt_res)  
    * plotMA(wt_res, ylim=c(-8,8))  
  
DESeq2 Results:  
  
* Can assess the results using the DESeq2 workflows  
	* mcols(wt_res)  
    * head(wt_res, n=10)  
* Multiple test corrections are run using the BH method  
	* summary(wt_res)  
    * wt_res <- results(dds_wt, contrast = c("condition", "fibrosis", "normal"), alpha = 0.05, lfcThreshold = 0.32)  
    * wt_res <- lfcShrink(dds_wt, contrast=c("condition", "fibrosis", "normal"), res=wt_res)  
    * summary(wt_res)  
* Can annotate the gene names for easier exploration and further analysis  
	* library(annotables)  
    * grcm38  
    * wt_res_all <- data.frame(wt_res) %>% rownames_to_column(var = "ensgene") %>% left_join(x = wt_res_all, y = grcm38[, c("ensgene", "symbol", "description")], by = "ensgene")  
    * wt_res_sig <- subset(wt_res_all, padj < 0.05)  
    * wt_res_sig <- wt_res_sig %>% arrange(padj)  
  
Eample code includes:  
```{r eval=FALSE}

# Create DESeq2 object
dds_smoc2 <- DESeqDataSetFromMatrix(countData = reordered_smoc2_rawcounts, colData = smoc2_metadata, design = ~ condition)

# Run the DESeq2 analysis
dds_smoc2 <- DESeq(dds_smoc2)


# Plot dispersions
plotDispEsts(dds_smoc2)


# Extract the results of the differential expression analysis
smoc2_res <- results(dds_smoc2, 
                contrast = c("condition", "fibrosis", "normal"), 
                alpha = 0.05)


# Shrink the log2 fold change estimates to be more accurate
smoc2_res <- lfcShrink(dds_smoc2, 
                    contrast =  c("condition", "fibrosis", "normal"),
                    res = smoc2_res)


# Explore the results() function
?results

# Extract results
smoc2_res <- results(dds_smoc2, 
                contrast = c("condition", "fibrosis", "normal"), 
                alpha = 0.05, 
                lfcThreshold = 0.32)

# Shrink the log2 fold changes
smoc2_res <- lfcShrink(dds_smoc2, 
                       contrast = c("condition", "fibrosis", "normal"),
                       res = smoc2_res)


# Get an overview of the results                    
summary(smoc2_res)


# Save results as a data frame
smoc2_res_all <- data.frame(smoc2_res)

# Subset the results to only return the significant genes with p-adjusted values less than 0.05
smoc2_res_sig <- subset(smoc2_res_all, padj < 0.05)

```
  
  
  
***
  
Chapter 4 - Exploration of Differential Expression Results  
  
Visualization of Results:  
  
* Expression heatmaps explore the expression of key genes  
	* sig_norm_counts_wt <- normalized_counts_wt[wt_res_sig$ensgene, ]  
    * library(RColorBrewer)  
    * heat_colors <- brewer.pal(6, "YlOrRd")  
    * display.brewer.all()  
    * pheatmap(sig_norm_counts_wt, color = heat_colors, cluster_rows = T, show_rownames = F, annotation = select(wt_metadata, condition), scale = "row")  
* Can also plot using the volcano plot  
	* wt_res_all <- wt_res_all %>% rownames_to_column(var = "ensgene") %>% mutate(threshold = padj < 0.05)  
    * ggplot(wt_res_all) + geom_point(aes(x = log2FoldChange, y = -log10(padj), color = threshold)) + xlab("log2 fold change") + ylab("-log10 adjusted p-value") + theme(legend.position = "none", plot.title = element_text(size = rel(1.5), hjust = 0.5), axis.title = element_text(size = rel(1.25)))  
* The expression plot can also be helpful - e.g., top 20 genes plot  
	* top_20 <- data.frame(sig_norm_counts_wt)[1:20, ] %>% rownames_to_column(var = "ensgene")  
    * top_20 <- gather(top_20, key = "samplename", value = "normalized_counts", 2:8)  
    * top_20 <- inner_join(top_20, rownames_to_column(wt_metadata, var = "samplename"), by = "samplename")  
    * ggplot(top_20) + geom_point(aes(x = ensgene, y = normalized_counts, color = condition)) + scale_y_log10() + xlab("Genes") + ylab("Normalized Counts") + ggtitle("Top 20 Significant DE Genes") + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(plot.title = element_text(hjust = 0.5))  
  
RNA-Seq DE Analysis Setup:  
  
* Can explore and filter for relevant and/or expected genes  
	* Samples/library preparation  
    * Sequence reads  
    * Quality control  
    * Splice-aware mapping to genome  
    * Counting reads associated with genes  
    * Statistical analysis to identify differentially expressed genes  
* The initial preparation uses the DESeq2 library  
	* dds <- DESeqDataSetFromMatrix(countData = rawcounts, colData = metadata, design = ~ condition)  
  
RNA-Seq DE Analysis Summary:  
  
* Need to normalize the data for DESeq analysis  
	* dds <- estimateSizeFactors(dds)  
    * normalized_counts <- counts(dds, normalized=TRUE)  
* Can cluster and plot for quality assurance and exploratory analysis  
	* vsd <- vst(dds, blind=TRUE)  
    * vsd %>% assay() %>% # Extract the vst matrix from the object cor() %>% # Compute pairwise correlation values pheatmap(annotation = metadata[ , c("column_name1", "column_name2])  
    * plotPCA(vsd, intgroup="condition")  
* Can then run the DE analysis  
	* dds <- DESeqDataSetFromMatrix(countData = rawcounts, colData = metadata, design = ~ source_of_variation + condition)  
    * dds <- DESeq(dds)  
* Can then explore fits using dispersions - should decrease with increasing mean - and then run LFC shrinkage  
	* plotDispEsts(dds)  
    * res <- results(dds, contrast=c("condition_factor", "level_to_compare", "base_level"), alpha = 0.05)  
    * res_all <- data.frame(res) %>% rownames_to_column(var = "ensgene")  
    * res_all <- left_join(x=res_all, y=grcm38[, c("ensgene", "symbol", "description")], by = "ensgene")  
    * res_all <- arrange(res_all, padj)  
* Identify significantly differently expressed genes  
	* res_sig <- subset(res_all, padj < 0.05)  
    * plotMA(), volcano plot, expression heatmap  
  
RNA-Seq Next Steps:  
  
* Several resources available for additional exploration  
	* http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html  
    * https://support.bioconductor.org/  
* May want to run more complicated experiments and analyses  
* Course covered differential expression of genes associated with fibrosis in wild type and SMOC2  
	* Can further analyze differentially expressed genes  
    * Can compare with the list of expected genes for differential expression  
  
Example code includes:  
```{r eval=FALSE}

# Create MA plot
plotMA(smoc2_res)

# Generate logical column 
smoc2_res_all <- data.frame(smoc2_res) %>% mutate(threshold = padj < 0.05)
              
# Create the volcano plot
ggplot(smoc2_res_all) + 
        geom_point(aes(x = log2FoldChange, y = -log10(padj), color = threshold)) + 
        xlab("log2 fold change") + 
        ylab("-log10 adjusted p-value") + 
        theme(legend.position = "none", 
              plot.title = element_text(size = rel(1.5), hjust = 0.5), 
              axis.title = element_text(size = rel(1.25)))


# Subset normalized counts to significant genes
sig_norm_counts_smoc2 <- normalized_counts_smoc2[rownames(smoc2_res_sig), ]

# Choose heatmap color palette
heat_colors <- brewer.pal(n = 6, name = "YlOrRd")

# Plot heatmap
pheatmap(sig_norm_counts_smoc2, 
         color = heat_colors, 
         cluster_rows = TRUE, 
         show_rownames = FALSE,
         annotation = select(smoc2_metadata, condition), 
         scale = "row")


# Check that all of the samples are in the same order in the metadata and count data
all(colnames(all_rawcounts) %in% rownames(all_metadata))

# DESeq object to test for the effect of fibrosis regardless of genotype
dds_all <- DESeqDataSetFromMatrix(countData = all_rawcounts,
                        colData = all_metadata,
                        design = ~ genotype + condition)

# DESeq object to test for the effect of genotype on the effect of fibrosis                        
dds_complex <- DESeqDataSetFromMatrix(countData = all_rawcounts,
                        colData = all_metadata,
                        design = ~ genotype + condition + genotype:condition)


# Log transform counts for QC
vsd_all <- vst(dds_all, blind = TRUE)

# Create heatmap of sample correlation values
vsd_all %>% 
        assay() %>%
        cor() %>%
        pheatmap(annotation = select(all_metadata, c("condition", "genotype")))

# Create the PCA plot for PC1 and PC2 and color by condition       
plotPCA(vsd_all, intgroup="condition")

# Create the PCA plot for PC1 and PC2 and color by genotype       
plotPCA(vsd_all, intgroup="genotype")


# Select significant genese with padj < 0.05
smoc2_sig <- subset(res_all, padj < 0.05) %>%
    data.frame() %>%
    rownames_to_column(var = "geneID")

# Extract the top 6 genes with padj values
smoc2_sig %>%
    arrange(padj) %>%
    select(geneID, padj) %>%
    head()

```
  
  
  
***
  
### _Survival Analysis in R_  
  
Chapter 1 - What is Survival Analysis?  
  
The term "survival analysis":  
  
* Survival analysis methods are the same whether they are time to progression, time to finding a job, etc.  
* Can be called "time to event" since the event need not be death; "survival analysis" is just the most commonly used metric  
* Data sets for the course  
	* data(GBSG2, package = "TH.data")  # time to death in breast cancer  
    * data(UnempDur, package = "Ecdat")  # time to re-employment  
    * help(UnempDur, package = "Ecdat")  
  
Why learn survival methods?  
  
* Survival analysis requires something more than a linear model  
	* Times are always positive; cannot be negative, nor have a negative number of survivors  
    * Different measures are of interest - hazard functions  
    * Censoring is almost always an issue - often do not know when the event will happen, only that it did not happen by time X  
* Can use the R package survival  
	* time <- c(5, 6, 2, 4, 4)  
    * event <- c(1, 0, 0, 1, 1)  
    * library("survival")  
    * Surv(time, event)  
* R packages that are available for survival analysis  
	* library("survival")  
    * library("survminer")  
  
Measures used in survival analysis:  
  
* Typical survival analysis questions include  
	* What is the probability that a breast cancer patient survives longer than 5 years?  
    * What is the typical waiting time for a cab?  
    * Out of 100 unemployed people, how many do we expect to have a job again after 2 months?  
* The survival function is S(t) = P(T > t) = 1 - F(t) where F is the CFD  
	* At any point in time, how probable is it to survive at least that long  
    * The median is one of the common metrics used  
  
Example code includes:  
```{r}

# Check out the help page for this dataset
# help(GBSG2, package = "TH.data")

# Load the data
data(GBSG2, package = "TH.data")

# Look at the summary of the dataset
summary(GBSG2)


# Count censored and uncensored data
num_cens <- table(GBSG2$cens)
num_cens

# Create barplot of censored and uncensored data
barplot(table(GBSG2$cens))


# Create Surv-Object
sobj <- survival::Surv(GBSG2$time, GBSG2$cens)

# Look at 10 first elements
sobj[1:10]

# Look at summary
summary(sobj)

# Look at structure
str(sobj)

```
  
  
  
***
  
Chapter 2 - Estimation of Survival Curves  
  
Kaplan-Meier Estimate:  
  
* The KM estimate is the step function derived from the raw data  
	* Censoring puts a mark on the curve, but dose not step down the curve  
    * S(t) = S(t-1) * e(t) / n(t)  where e(t) is events occuring at t and n(t) is the number at-risk (not previously censored) at time t  
* Can run KM estimates using the survival function  
	* km <- survfit(Surv(time, event) ~ 1)  
    * ggsurvplot(km, conf.int = FALSE, risk.table = "nrisk_cumevents", legend = "none")  
  
Understanding and Visualizing Kaplan-Meier Curves:  
  
* The ggsurvplot function can be useful for visualizations  
	* library(survminer)  
    * ggsurvplot(fit)  
    * ggsurvplot( fit, palette = NULL, linetype = 1, surv.median.line = "none", risk.table = FALSE, cumevents = FALSE, cumcensor = FALSE, tables.height = 0.25, ... )  
    * ggsurvplot( fit = km, palette = "blue", linetype = 1, surv.median.line = "hv", risk.table = TRUE, cumevents = TRUE, cumcensor = TRUE, tables.height = 0.1 )  # hv is horizontal and vertical  
* Can also revisit the survfit() object, which produces the KM curve  
	* If object is a formula: Kaplan-Meier estimation  
    * Other options for object - coxph, survreg  
  
Weibull Model for Estimating Survival Curves:  
  
* The Weibull model extends the KM curve through smoothing which can be more valuable for predictions  
	* wb <- survreg(Surv(time, event) ~ 1, data)  # reg is regression which is Weibull  
    * km <- survfit(Surv(time, event) ~ 1, data)  # fit is step-function KM  
    * predict(wb, type = "quantile", p = 1 - 0.9, newdata = data.frame(1))  # dummy newdata since there are no covariates  
* Can also create the survival curve  
	* surv <- seq(.99, .01, by = -.01)  
    * t <- predict(wb, type = "quantile", p = 1 - surv, newdata = data.frame(1))  
    * head(data.frame(time = t, surv = surv))  
  
Visualizing Results of Weibull Model:  
  
* Most of the visualization tools, such as ggsurvplot(), work for step functions rather than on curves  
	* wb <- survreg(Surv(time, cens) ~ 1)  
    * surv <- seq(.99, .01, by = -.01)  
    * t <- predict(wb, type = "quantile", p = 1 - surv, newdata = data.frame(1))  
    * surv_wb <- data.frame(time = t, surv = surv, upper = NA, lower = NA, std.err = NA)  
    * ggsurvplot_df(fit = surv_wb, surv.geom = geom_line)  
  
Example code includes:  
```{r}

# Create time and event data
time <- c(5, 6, 2, 4, 4)
event <- c(1, 0, 0, 1, 1)

# Compute Kaplan-Meier estimate
km <- survival::survfit(survival::Surv(time, event) ~ 1)
km

# Take a look at the structure
str(km)

# Create data.frame
data.frame(time = km$time, n.risk = km$n.risk, n.event = km$n.event, 
           n.censor = km$n.censor, surv = km$surv
           )


# Create dancedat data
dancedat <- data.frame(name = c("Chris", "Martin", "Conny", "Desi", "Reni", "Phil", "Flo", "Andrea", "Isaac", "Dayra", "Caspar"),
                       time = c(20, 2, 14, 22, 3, 7, 4, 15, 25, 17, 12),
                       obs_end = c(1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0)
                       )

# Estimate the survivor function pretending that all censored observations are actual observations.
km_wrong <- survival::survfit(survival::Surv(time) ~ 1, data = dancedat)

# Estimate the survivor function from this dataset via kaplan-meier.
km <- survival::survfit(survival::Surv(time, obs_end) ~ 1, data = dancedat)

# Plot the two and compare
survminer::ggsurvplot_combine(list(correct = km, wrong = km_wrong))


# Kaplan-Meier estimate
km <- survival::survfit(survival::Surv(time, cens) ~ 1, data = GBSG2)

# plot of the Kaplan-Meier estimate
survminer::ggsurvplot(km)

# add the risk table to plot
survminer::ggsurvplot(km, risk.table = TRUE)

# add a line showing the median survival time
survminer::ggsurvplot(km, risk.table = TRUE, surv.median.line = "hv")


# Weibull model
wb <- survival::survreg(survival::Surv(time, cens) ~ 1, data = GBSG2)

# Compute the median survival from the model
predict(wb, type = "quantile", p = 0.5, newdata = data.frame(1))

# 70 Percent of patients survive beyond time point...
predict(wb, type = "quantile", p = 1-0.7, newdata = data.frame(1))

# Retrieve survival curve from model probabilities 
surv <- seq(.99, .01, by = -.01)

# Get time for each probability
t <- predict(wb, type = "quantile", p = 1 - surv, newdata = data.frame(1))

# Create data frame with the information
surv_wb <- data.frame(time = t, surv = surv)

# Look at first few lines of the result
head(surv_wb)


# Create data frame with the information needed for ggsurvplot_df
surv_wb <- data.frame(time = t, surv = surv, upper = NA, lower = NA, std.err = NA)

# Plot
survminer::ggsurvplot_df(fit = surv_wb, surv.geom = geom_line)

```
  
  
  
***
  
Chapter 3 - Weibull Model  
  
Why Use the Weibull Model?  
  
* Often, there is a goal to predict the life expectancy based on covariates such as treatment, tumor size, etc.  
	* wbmod <- survreg(Surv(time, cens) ~ horTh + tsize, data = GBSG2)  
    * coef(wbmod)  
  
Visualizing Weibull Models:  
  
* Steps to produce the visualization include  
	* Compute Weibull model  
    * Decide on "imaginary patients"  
    * Compute survival curves  
    * Create data.frame with survival curve information  
    * Plot  
* Example steps  
	* wbmod <- survreg(Surv(time, cens) ~ horTh + tsize, data = GBSG2)  # compute model  
    * newdat <- expand.grid( horTh = levels(GBSG2$horTh), tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75)) )  # create all combinations of therapy and tumor sizes  
    * surv <- seq(.99, .01, by = -.01)  
    * t <- predict(wbmod, type = "quantile", p = 1 - surv, newdata = newdat)  
    * surv_wbmod_wide <- cbind(newdat, t)  
    * surv_wbmod <- reshape2::melt(surv_wbmod_wide, id.vars = c("horTh", "tsize"), variable.name = "surv_id", value.name = "time")  
    * surv_wbmod$surv <- surv[as.numeric(surv_wbmod$surv_id)]  
    * surv_wbmod[, c("upper", "lower", "std.err", "strata")] <- NA  
    * ggsurvplot_df(surv_wbmod, surv.geom = geom_line, linetype = "horTh", color = "tsize", legend.title = NULL)  
  
Other Distributions:  
  
* The choice of distribution will depend on the assumptions around the underlying physical process - will see different curves  
	* survreg(Surv(time, cens) ~ horTh, data = GBSG2)  
    * survreg(Surv(time, cens) ~ horTh, data = GBSG2, dist = "exponential")  
    * survreg(Surv(time, cens) ~ horTh, data = GBSG2, dist = "lognormal")  
* The choice of distribution depends on both domain expertise and goodness of fit; Weibull models are generally more flexible than exponential models  
  
Example code includes:  
```{r}

dfTime <- c(306, 455, 1010, 210, 883, 1022, 310, 361, 218, 166, 170, 654, 728, 71, 567, 144, 613, 707, 61, 88, 301, 81, 624, 371, 394, 520, 574, 118, 390, 12, 473, 26, 533, 107, 53, 122, 814, 965, 93, 731, 460, 153, 433, 145, 583, 95, 303, 519, 643, 765, 735, 189, 53, 246, 689, 65, 5, 132, 687, 345, 444, 223, 175, 60, 163, 65, 208, 821, 428, 230, 840, 305, 11, 132, 226, 426, 705, 363, 11, 176, 791, 95, 196, 167, 806, 284, 641, 147, 740, 163, 655, 239, 88, 245, 588, 30, 179, 310, 477, 166, 559, 450, 364, 107, 177, 156, 529, 11, 429, 351, 15, 181, 283, 201, 524, 13, 212, 524, 288, 363, 442, 199, 550, 54, 558, 207, 92, 60, 551, 543, 293, 202, 353, 511, 267, 511, 371, 387, 457, 337, 201, 404, 222, 62, 458, 356, 353, 163, 31, 340, 229, 444, 315, 182, 156, 329, 364, 291, 179, 376, 384, 268, 292, 142, 413, 266, 194, 320, 181, 285, 301, 348, 197, 382, 303, 296, 180, 186, 145, 269, 300, 284, 350, 272, 292, 332, 285, 259, 110, 286, 270, 81, 131, 225, 269, 225, 243, 279, 276, 135, 79, 59, 240, 202, 235, 105, 224, 239, 237, 173, 252, 221, 185, 92, 13, 222, 192, 183, 211, 175, 197, 203, 116, 188, 191, 105, 174, 177)
dfStatus <- c(2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)
dfSex <- factor(ifelse(c(1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2)==1, "male", "female"), levels=c("male", "female"))
dat <- data.frame(time=dfTime, status=dfStatus, sex=dfSex)


# Look at the data set
str(dat)

# Estimate a Weibull model
wbmod <- survival::survreg(survival::Surv(time, status) ~ sex, data = dat)
coef(wbmod)


# Weibull model
wbmod <- survival::survreg(survival::Surv(time, cens) ~ horTh, data = GBSG2)

# Retrieve survival curve from model
surv <- seq(.99, .01, by = -.01)
t_yes <- predict(wbmod, type = "quantile", p = 1-surv, newdata = data.frame(horTh = "yes"))

# Take a look at survival curve
str(t_yes)


# Weibull model
wbmod <- survival::survreg(survival::Surv(time, cens) ~ horTh + tsize, data = GBSG2)

# Imaginary patients
newdat <- expand.grid(horTh = levels(GBSG2$horTh), 
                      tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75))
                      )

# Compute survival curves
surv <- seq(.99, .01, by = -.01)
t <- predict(wbmod, type = "quantile", p = 1-surv, newdata = newdat)

# How many rows and columns does t have?
dim(t)


# Use cbind() to combine the information in newdat with t
surv_wbmod_wide <- cbind(newdat, t)
  
# Use melt() to bring the data.frame to long format
surv_wbmod <- reshape2::melt(surv_wbmod_wide, id.vars = c("horTh", "tsize"), 
                             variable.name = "surv_id", value.name = "time"
                             )

# Use surv_wbmod$surv_id to add the correct survival probabilities surv
surv_wbmod$surv <- surv[as.numeric(surv_wbmod$surv_id)]

# Add columns upper, lower, std.err, and strata to the data.frame
surv_wbmod[, c("upper", "lower", "std.err", "strata")] <- NA

# Take a look at the structure of the object
str(surv_wbmod)

# Plot the survival curves
survminer::ggsurvplot_df(surv_wbmod, surv.geom = geom_line,
                         linetype = "horTh", color = "tsize", legend.title = NULL
                         )


# Weibull model
wbmod <- survival::survreg(survival::Surv(time, cens) ~ horTh, data = GBSG2)

# Log-Normal model
lnmod <- survival::survreg(survival::Surv(time, cens) ~ horTh, data = GBSG2, dist = "lognormal")

# Newdata
newdat <- data.frame(horTh = levels(GBSG2$horTh))

# Surv
surv <- seq(0.99, .01, by = -.01)

# Survival curve from Weibull model and log-normal model
wbt <- predict(wbmod, type = "quantile", p = 1-surv, newdata = newdat)
lnt <- predict(lnmod, type = "quantile", p = 1-surv, newdata = newdat)


dfWbtLnt <- as.data.frame(rbind(wbt, lnt))
names(dfWbtLnt) <- as.character(1:99)

surv_wide <- cbind(data.frame(horTh=factor(c("no", "yes", "no", "yes"), levels=c("no", "yes"))), 
                   dfWbtLnt, 
                   data.frame(dist=factor(c("weibull", "weibull", "lognormal", "lognormal")))
                   )

# Melt the data.frame into long format.
surv_long <- reshape2::melt(surv_wide, id.vars = c("horTh", "dist"), 
                            variable.name = "surv_id", 
                            value.name = "time"
                            )

# Add column for the survival probabilities
surv_long$surv <- surv[as.numeric(surv_long$surv_id)]

# Add columns upper, lower, std.err, and strata contianing NA values
surv_long[, c("upper", "lower", "std.err", "strata")] <- NA


# Plot the survival curves
survminer::ggsurvplot_df(surv_long, surv.geom = geom_line, 
                         linetype = "horTh", color = "dist", legend.title = NULL
                         )

```
  
  
  
***
  
Chapter 4 - Cox Model  
  
Cox Model - most widely used model in survival analysis:  
  
* Semi-parametric (vs. Weibull model which is fully parametric)  
* Also called the "proportional hazards" model; instantaneous probability is assumed to be proportional, meaning that curves cannot cross  
* Computing the Cox model is very similar to the Weibull model; will be no intercept, and negative coefficient mean positive impact on survival  
	* cxmod <- coxph(Surv(time, cens) ~ horTh, data = GBSG2)  
  
Visualizing the Cox Model:  
  
* Five steps for visualizing the Cox model  
	* Compute Cox model  
    * Decide on covariate combinations ("imaginary patients")  
    * Compute survival curves  
    * Create data.frame with survival curve information  
    * Plot  
* Example code includes  
	* cxmod <- coxph(Surv(time, cens) ~ horTh + tsize, data = GBSG2)  
    * newdat <- expand.grid( horTh = levels(GBSG2$horTh), tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75)) )  
    * rownames(newdat) <- letters[1:6]  
    * cxsf <- survfit(cxmod, data = GBSG2, newdata = newdat, conf.type = "none")  
    * surv_cxmod0 <- surv_summary(cxsf)  
    * surv_cxmod <- cbind(surv_cxmod0, newdat[as.character(surv_cxmod0$strata), ])  
    * ggsurvplot_df(surv_cxmod, linetype = "horTh", color = "tsize", legend.title = NULL, censor = FALSE)  
  
Recap:  
  
* Concepts - survival analysis, censoring, survival curves  
* Methods - Kaplan-Meier, Weibull, Cox  
* Focus - understand, estimate, and visualize survival curves  
  
Wrap Up:  
  
* Ability to interpret model estimates is a valuable skill is a useful follow-up step  
	* Statistical inference, confidence intervals, etc.  
* Competing risk models is a valuable follow-on skill  
* Many additional modeling libraries available on CRAN  
  
Example code includes:  
```{r}

dat$performance <- c(90, 90, 90, 90, 100, 50, 70, 60, 70, 70, 80, 70, 90, 60, 80, 80, 90, 50, 60, 90, 80, 100, 70, 90, 90, 90, 100, 60, 80, 70, 90, 60, 60, 50, 70, 50, 70, 70, 50, 80, 80, 60, 90, 70, 60, 60, 90, 80, 90, 90, 90, 80, 90, 100, 90, 90, 100, 70, 80, 90, 70, 90, 80, 90, 80, 70, 70, 90, 100, 80, 90, 80, 70, 80, 90, 90, 100, 80, 90, 90, 100, 70, 80, 80, 80, 80, 80, 100, 90, 70, 100, 80, 90, 80, 100, 80, 80, 90, 90, 90, 100, 80, 70, 90, 50, 80, 80, 90, 100, 60, 90, 80, 80, 90, 80, 70, 70, 60, 70, 80, 90, 70, 70, 60, 90, 80, 80, 80, 80, 90, 80, 80, 100, 80, 90, 60, 80, 80, 90, 100, 70, 80, 70, 80, 80, 90, 100, 90, 100, 100, 70, 90, 90, 80, 70, 70, 90, 70, 80, 80, 90, 90, 60, 90, 80, 90, 80, 100, 90, 100, 90, 90, 90, 100, 90, 80, 60, 80, 80, 100, 100, 100, 90, 80, 90, 90, 70, 90, 80, 90, 80, 60, 90, 90, 90, 100, 80, 90, 100, 90, 90, 60, 90, 100, 100, NA, 80, 60, 80, 90, 100, 80, 90, 70, 80, 90, 90, 80, 70, 80, 80, 80, 80, 80, 90, 60, 90, 80)
str(dat)


# Compute Cox model
cxmod <- survival::coxph(survival::Surv(time, status) ~ performance, data = dat)

# Show model coefficient
coef(cxmod)


# Cox model
cxmod <- survival::coxph(survival::Surv(time, cens) ~ horTh + tsize, data = GBSG2)

# Imaginary patients
newdat <- expand.grid(horTh = levels(GBSG2$horTh), 
                      tsize = quantile(GBSG2$tsize, probs = c(0.25, 0.5, 0.75))
                      )
rownames(newdat) <- letters[1:6]

# Compute survival curves
cxsf <- survival::survfit(cxmod, data = GBSG2, newdata = newdat, conf.type = "none")

# Look at first 6 rows of cxsf$surv and time points
head(cxsf$surv)
head(cxsf$time)


# Remove conf.type="none" per https://github.com/kassambara/survminer/issues/355
cxsf <- survival::survfit(cxmod, data = GBSG2, newdata = newdat)

# Compute data.frame needed for plotting
surv_cxmod0 <- survminer::surv_summary(cxsf)

# Get a character vector of patient letters (patient IDs)
pid <- as.character(surv_cxmod0$strata)

# Multiple of the rows in newdat so that it fits with surv_cxmod0
m_newdat <- newdat[pid, ]

# Add patient info to data.frame
surv_cxmod <- cbind(surv_cxmod0, m_newdat)

# Plot
survminer::ggsurvplot_df(surv_cxmod, linetype = "horTh", color = "tsize", 
                         legend.title = NULL, censor = FALSE
                         )


# Compute Cox model and survival curves
cxmod <- survival::coxph(survival::Surv(time, status) ~ performance, data = dat)
new_lung <- data.frame(performance = c(60, 70, 80, 90))
cxsf <- survival::survfit(cxmod, data = dat, newdata = new_lung)

# Use the summary of cxsf to take a vector of patient IDs
surv_cxmod0 <- survminer::surv_summary(cxsf)
pid <- as.character(surv_cxmod0$strata)

# Duplicate rows in newdat to fit with surv_cxmod0 and add them in
m_newdat <- new_lung[pid, , drop = FALSE]
surv_cxmod <- cbind(surv_cxmod0, m_newdat)

# Plot
survminer::ggsurvplot_df(surv_cxmod, color = "performance", legend.title = NULL, censor = FALSE)


# Compute Kaplan-Meier curve
km <- survival::survfit(survival::Surv(time, status) ~ 1, data = dat)

# Compute Cox model
cxmod <- survival::coxph(survival::Surv(time, status) ~ performance, data = dat)

# Compute Cox model survival curves
new_lung <- data.frame(performance = c(60, 70, 80, 90))
cxsf <- survival::survfit(cxmod, data = dat, newdata = new_lung)

# Plot Kaplan-Meier curve
survminer::ggsurvplot(km, conf.int = FALSE)

# Plot Cox model survival curves
# survminer::ggsurvplot(cxsf, censor = FALSE)

```
  
  
  
***
  
### _Building Response Models in R_  
  
Chapter 1 - Response Models for Product Sales  
  
Fundamentals of Market Response Models:  
  
* Market response models are statistical tools to optimize A&P for the marketing mix  
* Dataset for the course will be for retial sales  
	* str(sales.data)  
  
Linear Response Models:  
  
* The model will have sales as the response variable and the other variables as predictors  
* Example of the simple linear model for Sales vs. Price  
	* linear.model <- lm(SALES ~ PRICE , data = sales.data)  
    * coef(linear.model)  
    * coef(linear.model)[1] + 0.95 * coef(linear.model)[2]  
    * plot(SALES ~ PRICE, data = sales.data)  
    * abline(coef(linear.model))  
  
Non-Linear Response Models:  
  
* Non-linear response is a more common real-world marketing mix behavior  
* The exponential response function assumes a constant elasticity  
	* Sales = B0 * exp(B1 * Price)  
    * log(Sales) = log(B0) + (B1*Price)  
    * log.model <- lm(log(SALES) ~ PRICE, data = sales.data)  
    * coef(log.model)  
    * plot(log(SALES) ~ PRICE, data = sales.data)  
    * log.model <- lm(log(SALES) ~ PRICE, data = sales.data)  
    * abline(coef(log.model))  
  
Example code includes:  
```{r}

load("./RInputFiles/sales.data.RData")
load("./RInputFiles/choice.data.RData")
str(sales.data)
str(choice.data)


# Tail of sales.data
tail(sales.data)

# Mean SALES
mean(sales.data$SALES)

# Minimum SALES
min(sales.data$SALES)

# Maximum SALES
max(sales.data$SALES)


# Linear model explaining SALES by PRICE
linear.model <- lm(SALES ~ PRICE, data = sales.data)

# Obtain the model coefficients
coef(linear.model)


# Obtain the intercept coefficient
coef(linear.model)[1]

# Obtain the slope coefficient
coef(linear.model)[2]

# Predict the SALES for the decreased PRICE of 1.05 
coef(linear.model)[1] + 1.05 * coef(linear.model)[2]

# Predict the SALES for the decreased PRICE of 0.95 
coef(linear.model)[1] + 0.95 * coef(linear.model)[2]


# Linear model explaining SALES by PRICE
linear.model <- lm(SALES ~ PRICE, data = sales.data)

# Plot SALES against PRICE
plot(SALES ~ PRICE, data = sales.data)

# Adding the model predictions
abline(coef(linear.model))


# Linear model explaining log(SALES) by PRICE
log.model <- lm(log(SALES) ~ PRICE, data=sales.data)

# Obtaining the model coefficients
coef(log.model)


# Plot log(SALES) against PRICE
plot(log(SALES) ~ PRICE, data=sales.data)

# Linear model explaining log(SALES) by PRICE
log.model <- lm(log(SALES) ~ PRICE, data=sales.data)

# Adding the model predictions
abline(coef(log.model))

```
  
  
  
***
  
Chapter 2 - Extended Sales Response Modeling  
  
Model Extension Part 1: Dummy Variables:  
  
* Dummy variables typically take on the value of 0 or 1  
* Can start by summarizing dummy variables in the sales.data file  
	* table(sales.data$DISPLAY)  
    * table(sales.data$DISPLAY)/sum(table(sales.data$DISPLAY))  
    * mean(sales.data$DISPLAY)  
* Can also include dummy variables in the regression  
	* dummy.model <- lm(log(SALES) ~ DISPLAY, data = sales.data)  
    * coef(dummy.model)  
    * exp(coef(dummy.model)[1])  # Average unit sales for no display  
    * exp(coef(dummy.model)[2] - 1)  # Percentage increase in sales for a display  
* Can add dummy variables for areas like discounts  
	* summary(sales.data[,c("DISPLAY","COUPON","DISPLAYCOUPON")])  
    * dummy.model <- lm(log(SALES) ~ DISPLAY + COUPON + DISPLAYCOUPON, data = sales.data)  
    * coef(dummy.model)  
    * lm(update(dummy.model, . ~ . + PRICE), data = sales.data)  # adds PRICE to the model  
  
Model Extensions Part 2: Dynamic Variables:  
  
* The carryover effect is the time span between marketing activities and response times  
	* Typically evaluated using lags  
    * head(cbind(sales.data$PRICE, lag(sales.data$PRICE, n = 1)))  
    * Price.lag <- lag(sales.data$PRICE)  
    * lag.model <- lm(log(SALES) ~ PRICE + Price.lag, data = sales.data)  
    * Coupon.lag <- lag(sales.data$COUPON)  
    * lm(update(lag.model, . ~ . + COUPON + Coupon.lag), data = sales.data)  
    * lag.model <- lm(log(SALES) ~ PRICE + Price.lag + DISPLAY + Display.lag + COUPON + Coupon.lag + DISPLAYCOUPON + DisplayCoupon.lag, data = sales.data)  
    * plot(log(SALES) ~ OBS, data = sales.data)  
    * lines(c(NA, fitted.values(lag.model)) ~ OBS, data = sales.data)  
  
Number of Extensions Needed:  
  
* Can summarize the model to view top-level findings, including R-squared and p-values for each variable  
	* summary(extended.model)  
    * AIC(extended.model)  # information criteria  
    * AIC(lm(update(extended.model, . ~ . - Coupon.lag), data = sales.data))  
* Can also run an elimination process using MASS  
	* library(MASS)  
    * final.model <- stepAIC(extended.model, direction = "backward", trace = FALSE)  
    * summary(final.model)  
  
Example code includes:  
```{r}

# Proportion of DISPLAY and no-DISPLAY activity
table(sales.data$DISPLAY) / sum(table(sales.data$DISPLAY))

# Mean of DISPLAY activity
mean(sales.data$DISPLAY)

# Mean of no-DISPLAY activity
1 - mean(sales.data$DISPLAY)

# Linear model explaining log(SALES) by DISPLAY
dummy.model <- lm(log(SALES) ~ DISPLAY, data = sales.data)

# Obtaining the coefficients
coef(dummy.model)


# Mean DISPLAY activity
mean(sales.data$DISPLAY)

# Mean COUPON activity
mean(sales.data$COUPON)

# Mean DISPLAY and COUPON activity
mean(sales.data$DISPLAYCOUPON)

# Summarize DISPLAY, COUPON, DISPLAYCOUPON activity
summary(sales.data[,c("DISPLAY", "COUPON", "DISPLAYCOUPON")])


# Linear model explaining log(SALES) by DISPLAY, COUPON and DISPLAYCOUPON
dummy.model <- lm(log(SALES) ~ DISPLAY + COUPON + DISPLAYCOUPON, data = sales.data)

# Obtain the model coefficients
coef(dummy.model)


# Dummy.mod updated for PRICE
update(dummy.model, . ~ . + PRICE)


# Compare lagged PRICE and original PRICE
head(cbind(sales.data$PRICE, lag(sales.data$PRICE)))


# Create the lagged PRICE variable
Price.lag <- lag(sales.data$PRICE)

# Linear model explaining log(SALES) by PRICE and Price.lag
lag.model <- lm(log(SALES) ~ PRICE + Price.lag, data = sales.data)

# Obtain the coefficients
coef(lag.model)


# Create the lagged COUPON variable
Coupon.lag <-  lag(sales.data$COUPON)

# Update lag.model for COUPON and C_lag
update(lag.model, . ~ . + COUPON + Coupon.lag)


sales.data2 <- sales.data %>%
    mutate(Price.lag = lag(PRICE, 1), 
           Display.lag = lag(DISPLAY, 1),
           Coupon.lag = lag(COUPON, 1),
           DisplayCoupon.lag = lag(DISPLAYCOUPON, 1)
           )

# Extended sales resonse model
extended.model <- lm(log(SALES) ~ PRICE + Price.lag + DISPLAY + Display.lag + COUPON + 
                         Coupon.lag + DISPLAYCOUPON + DisplayCoupon.lag, data = sales.data2
                     )

# Plot log(SALES) against OBS
plot(log(SALES) ~ OBS, data = sales.data2)

# Add the model predictions
lines(c(NA, fitted.values(extended.model)) ~ OBS, data = sales.data2)


# Summarize the model
summary(extended.model)

# AIC of the extended response model
AIC(extended.model)

# Single term deletion
AIC(lm(update(extended.model, . ~ . -Coupon.lag), data = sales.data2))


# Backward elemination
final.model <- MASS::stepAIC(extended.model, direction = "backward", trace = FALSE)

# Summarize the final model
summary(final.model)

```
  
  
  
***
  
Chapter 3 - Response Models for Individual Demand  
  
Models for Individual Demand:  
  
* Data are available about customer purchases  
	* str(choice.data)  
    * OBS-ervation week  
    * HOUSEHOLDID of the purchase records  
    * LASTPURCHASE recorded of the household  
* The probability to purchase cannnot be well modeled with a purely linear model  
	* probability.model <- lm(HOPPINESS ~ PRICE.HOP, data = choice.data)  
    * plot(HOPPINESS ~ PRICE.HOP, data = choice.data) abline(probability.model)  
    * abline(probability.model)  
* Can define a new variable, price ratio  
	* price.ratio <- log(choice.data$PRICE.HOP/choice.data$PRICE.BUD)  
    * probability.model <- lm(HOPPINESS ~ price.ratio, data = choice.data)  
    * plot(HOPPINESS ~ price.ratio, data = choice.data)  
    * abline(probability.model)  
  
Logistic Response Models:  
  
* The logistic response function is a better predictor for choice data since it is bounded between 0 and 1  
	* logistic.model <- glm(HOPPINESS ~ price.ratio, family = binomial, data = choice.data)  
    * coef(logistic.model)  
    * plot(HOPPINESS ~ price.ratio, data = choice.data)  
    * curve(predict(logistic.model, data.frame(price.ratio = x), type = "response"), add = TRUE)  
    * margins(logistic.model)  
* Can also look at an effects plot using the margins package  
	* x <- seq(-1.25, 1.25, by = 0.25)  
    * cplot(logistic.model, "price.diff", xvals = x)  
  
Probit Response Models:  
  
* Can use the probit response function in lieu of the logit response function, with slight changes to the tails  
	* probit.model <- glm(HOPPINESS ~ price.ratio, family = binomial(link = probit), data = choice.data)  
    * coef(probit.model)  
    * cbind(coef(logistic.model), coef(probit.model))  
    * margins(logistic.model)  # interpretable log-odds  
    * margins(probit.model)  # non-interpretable z-values  
  
Example code includes:  
```{r}

# Structure of choice.data
str(choice.data)

# Summarize purchases of HOPPINESS, BUD and PRICE.HOP and PRICE.BUD
summary(choice.data[,c("HOPPINESS", "BUD", "PRICE.HOP", "PRICE.BUD")]) 


# Plot HOPPINESS against PRICE.HOP
plot(HOPPINESS ~ PRICE.HOP, data = choice.data)

# Linear probability model explaining HOPPINESS by PRICE.HOP
probability.model <- lm(HOPPINESS ~ PRICE.HOP, data = choice.data)

# Add the model predictions
abline(coef(probability.model))


# Calculate the price ratio for HOPPINESS and BUD
choice.data$price.ratio <- log(choice.data$PRICE.HOP / choice.data$PRICE.BUD)

# Plot HOPPINESS purchases against the price ratio
plot(HOPPINESS ~ price.ratio, data = choice.data)

# Linear probability model explaining HOPPINESS by price.ratio
probability.model <- lm(HOPPINESS ~ price.ratio, data = choice.data)

# Add the model predictions
abline(probability.model)


# Logistic model explaining HOPPINESS by price.ratio
logistic.model <- glm(HOPPINESS ~ price.ratio, family = binomial, data = choice.data)

# Obtain the coefficients
coef(logistic.model)


# Plot HOPPINESS choices against price.diff
plot(HOPPINESS ~ price.ratio, data = choice.data)

# Add the predictions of the logistic model
curve(predict(logistic.model, data.frame(price.ratio = x), type = "response"), add = TRUE)


# Linear probability model
coef(probability.model)

# Logistic model
margins::margins(logistic.model)


# Sequence of x values
x <- seq(-1, 1, by = 0.10)

# Conditional effect plot
margins::cplot(logistic.model, "price.ratio", xvals = x)


# Probit model explaining HOPPINESS by price.ratio
probit.model <- glm(HOPPINESS ~ price.ratio, family = binomial(link=probit), data = choice.data)

# Obtain the coefficients
coef(probit.model)


# Compare the coefficients
cbind(coef(probit.model), coef(logistic.model))


# Logistic model
margins::margins(logistic.model)

# Probit model
margins::margins(probit.model)

```
  
  
  
***
  
Chapter 4 - Extended Demand Modeling  
  
Model Selection:  
  
* Can extend the model to include display feature data  
	* summary(choice.data[,c("FEAT.HOP","DISPL.HOP","FEATDISPL.HOP")])  
    * extended.model <- glm(HOPPINESS ~ price.ratio + DISPL.HOP + FEAT.HOP + FEATDISPL.HOP, family = binomial, data = choice.data)  
    * margins(extended.model)  
    * summary(extended.model)  
* The null deviance and the residual deviance can be assessed using an intercept-only model and an ANOVA  
	* null.model <- glm(HOPPINESS ~ 1, family = binomial, data = choice.data)  
    * anova(extended.model, null.model, test = "Chisq")  
* Can also run a stepwise model and assess using AIC  
	* final.model <- stepAIC(extended.model, direction = "backward", trace = FALSE)  
    * summary(final.model)  
  
Predictive Performance:  
  
* Can use cutoffs on the predicted probabilities to create a classifier  
	* predicted <- ifelse(fitted.values(extended.mod) >= 0.5, 1, 0)  
    * observed <- choice.data$HOPPINESS  
    * table(observed, predicted)/2798  # There are 2798 samples (confusion matrix)  
    * Roc <- pROC::roc(predictor = fitted.values(extended.mod), response = observed)  
    * plot(Roc)  
  
Model Validation:  
  
* Can evaluate model on independent (unseen) datasets - example of subsetting on LASTPURCHASE  
	* train.data <- subset(choice.data, subset = LASTPURCHASE == 0)  
    * test.data <- subset(choice.data, subset = LASTPURCHASE == 1)  
    * train.model <- glm(HOPPINESS ~ price.diff + FEAT.HOP + FEATDISPL.HOP, family = binomial, data = train.data)  
    * margins(train.model)  
    * prob <- predict(train.model, test.data, type = "response")  
    * predicted <- ifelse(prob >= 0.5, 1, 0)   
    * observed <- test.data$HOPPINESS  
    * table(predicted, observed)/300  
  
Wrap Up:  
  
* Linear and non-linear response models  
* Logit and probit models  
* Dummy and lagged variables  
* Test and control approaches  
  
Example code includes:  
```{r}

# Summarizing DISPLAY.HOP, FEAT.HOP, FEATDISPL.HOP actions
summary(choice.data[, c("DISPL.HOP", "FEAT.HOP", "FEATDISPL.HOP")])

# Logistic model explaining HOPPINESS by price.diff, DISPL.HOP, FEAT.HOP, FEATDISPL.HOP
extended.model <- glm(HOPPINESS ~ price.ratio + DISPL.HOP + FEAT.HOP + FEATDISPL.HOP, 
                      family = binomial, data  = choice.data
                      )

# Marginal effects for the extended logistic response model
margins::margins(extended.model)

# Summarize the model
summary(extended.model)


# Null model explaining HOPPINESS by the intercept only
null.model <- glm(HOPPINESS ~ 1, family = binomial, data = choice.data)

# Compare null.mod against extended.mod
anova(extended.model, null.model, test = "Chisq")


# Backward elemination
final.model <- MASS::stepAIC(extended.model, direction = "backward", trace = FALSE)

# Summarize the final model
summary(final.model)


# Classifying the predictions
predicted <- ifelse(fitted.values(extended.model) >= 0.5, 1, 0)

# Obtain the purchase predictions
table(predicted)


# Obtain the observed purchases
observed <- choice.data$HOPPINESS

# Cross-tabulating the observed vs. the predicted purchases
table(predicted, observed)/2798


# Creating the Roc object
Roc <- pROC::roc(predictor = fitted.values(extended.model), response = observed)

# Plot the ROC curve
pROC::plot.roc(Roc)


# Create the training dataset
train.data <- subset(choice.data, LASTPURCHASE == 0)

# Create the test dataset
test.data <- subset(choice.data, LASTPURCHASE == 1)


# Fit logistic response model to the training data set
train.model <- glm(HOPPINESS ~ price.ratio + FEAT.HOP + FEATDISPL.HOP, 
                   family = binomial, data = train.data
                   )


# Predict the purchase probabilities for test.data
prob <- predict(train.model, newdata=test.data, type = "response") 

# Classify the predictions
predicted <- ifelse(prob >= 0.5, 1, 0) 

# Obtain the observed purchases from test.data
observed <- test.data$HOPPINESS

# Cross-tabulate  the predicted vs. the observed purchases
table(predicted, observed)/300

```
  
  
  
***
  
### _Time Series with data.table in R_  
  
Chapter 1 - Review of data.table  
  
Introduction:  
  
* Data frames are a general purpose data structure - rectangular structure reflecting a list of lists, though unlike matrices, the lists may be of multiple data types  
* The data.table package is an extension of data.frame - more efficient memory usage and more expressive syntax  
    * library(data.table)  
    * someDT <- data.table(x = rnorm(100), y = rep(TRUE, 100))  
    * str(someDT)  # Classes ‘data.table’ and 'data.frame':    100 obs. of  2 variables:  
* Can select columns using .()  
	* baseballDT[, .(timestamp, winning_team)]  
* Can use .SD for column selection  
	* cols <- c("timestamp", "winning_team")  
    * baseballDT[, .SD, .SDcols = cols]  
* Can use grep() to help with the matching  
	* count_cols <- grep('COUNT$', names(baseballDT), value = TRUE)  
    * countDT <- baseballDT[, .SD, .SDcols = count_cols]  
* Can simultaneously select rows and columns, and they are objects inside the data.table environment  
	* cols <- c("timestamp", "winning_team")  
    * baseballDT[ which.max(timestamp), .SD, .SDcols = cols ]  
  
Flexible Data Selection:  
  
* The get() function allows for evaluating a string as a column reference  
	* locDT <- data.table( cities = c("Chicago", "Boston", "Milwaukee"), ppl_mil = c(2.7, 0.673, 0.595) )  
    * city_col <- "cities"  
    * locDT[, get(city_col)]  
    * square_col <- function(DT, col_name){ return(DT[, get(col_name) ^ 2]) }  
* The () mean that you are accessing something that is external to the data.table  
	* add_bil_ppl <- function(DT, new_name){ DT[, (new_name) := ppl_mil * 1000 }  
    * add10 <- function(DT, cols){ for (col in cols){ new_name <- paste0(col, "_plus10") ; DT[, (new_name) := get(col) + 10] } }  
    * add10(locDT, cols = "ppl_mil")  
* Can change names using setnames() - modifies the table in place (no copies)  
	* setnames(locDT, old = "cities", new = "city_names")  
    * tag_important_columns <- function(DT, cols){ setnames(DT, old = cols, new = paste0(cols, "_important")) }  
    * tag_important_columns(locDT, "ppl_mil")  
  
Executing Functions Inside data.tables:  
  
* Can evaluate functions inside the data.table operators  
* Functions in the "I" block can be used to select rows, with booleans used for subsetting  
	* stockDT <- data.table( close_date = seq.POSIXt(as.POSIXct("2017-01-01"), as.POSIXct("2017-01-30"), length.out = 100), MSFT = runif(100, 70, 80), AAPL = runif(100, 140, 180) )  
    * stockDT[close_date > max(close_date) - 60 * 60 * 8]  
* Functions in the "j" block can be used to summarize data  
	* cor(stockDT[, .SD, .SDcols = c('AAPL', 'MSFT')])  
    * corr_mat <- stockDT[, cor(.SD), .SDcols = c('AAPL', 'MSFT')]  
    * stockDT[, rand_noise := AAPL + rnorm(100)]  
* Can use the "by" group to dynamically group data  
	* stockDT[, hour_of_day := as.integer(strftime(close_date, "%H"))]  
    * stockDT[, mean(AAPL), by = hour_of_day][order(hour_of_day)]  
    * stockDT[, mean(AAPL), by = .( hour_of_day = as.integer(strftime(close_date, "%H")) )][order(hour_of_day)]  # same as above, but with a 1-step process  
* Since a data.table is a list of lists, can use lapply or sapply on the data.table  
	* Use lapply() if you want a data.table back  
    * Use sapply() if you want a vector or list back  
    * stockDT[, lapply(.SD, function(x){mean(is.na(x))})]  
    * num_obs <- stockDT[, sapply(.SD, function(x){sum(!is.na(x), na.rm = TRUE)})]  
  
Example code includes:  
```{r}

library(data.table)

diagnosticDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/diagnosticDT.feather"))
str(diagnosticDT)


# Select system voltage directly
voltageDT <- diagnosticDT[, .(timestamp, system_voltage)]

# Select system voltage with .SD
voltageDT <- diagnosticDT[, .SD, .SDcols = c("timestamp", "system_voltage")]

# Select system voltage with .SD + a vector of names
voltage_cols <- c("timestamp", "system_voltage")
voltageDT <- diagnosticDT[, .SD, .SDcols = voltage_cols]

diagnosticDT[which.max(timestamp), .SD, .SDcols=c("timestamp", "system_voltage")]


# Store the names of all columns starting with "engine_" in a vector
engine_cols <- grep(pattern = "engine_", x = names(diagnosticDT), value = TRUE)

# Use that vector to create a new data.table with only engine signals
engineDT <- diagnosticDT[, .SD, .SDcols = engine_cols]


# Complete the function
add_interaction <- function(someDT, col1, col2){
    new_col_name <- paste0(col1, "_times_", col2)
    someDT[, (new_col_name) := get(col1) * get(col2)]
}

# Add an interaction
add_interaction(diagnosticDT, "engine_speed", "engine_temp")

# Check it out!
head(diagnosticDT)


# Write a function to scale a column by 10
scale_by_10 <- function(someDT, col_to_scale, new_col_name){
    someDT[, (new_col_name) := get(col_to_scale) * 10]
}

# Try it out
scale_by_10(diagnosticDT, "engine_temp", "temp10")

# Check the state of the data.table
head(diagnosticDT)


# Write a function that squares every numeric column
add_square_features <- function(someDT, cols){
    for (col_name in cols){
        new_col_name <- paste0(col_name, "_squared")
        someDT[, (new_col_name) := get(col_name)^2 ]
    }
}

# Look at the difference!
add_square_features(diagnosticDT, c("engine_speed", "engine_temp", "system_voltage"))
head(diagnosticDT)


# Change names
setnames(diagnosticDT, old = c("timestamp"), new = "obs_time")

# Tag all the numeric columns with "_NUMERIC"
tag_numeric_cols <- function(DT, cols){
    setnames(DT, old = cols, new = paste0(cols, "_NUMERIC"))
}

# Tag numeric columns
tag_numeric_cols(diagnosticDT, c("engine_speed", "engine_temp", "system_voltage"))


diagnosticDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/diagnosticDT.feather"))
str(diagnosticDT)
diagnosticDT2 <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/diagnosticDT.feather"))
str(diagnosticDT2)


# Mean of engine temp
diagnosticDT[, mean(engine_temp)]

# Correlation between engine_temp and system_voltage
diagnosticDT[, cor(engine_temp, system_voltage)]

# Get classes of column names
correlations <- function(DT){
    # Find numeric columns
    num_cols <- diagnosticDT[, sapply(.SD, is.numeric)]
    numeric_cols <- names(diagnosticDT)[num_cols]
    return(DT[, cor(.SD), .SDcols = numeric_cols])
}


# Mean of system voltage
diagnosticDT[, lapply(.SD, mean), .SDcols = c("system_voltage")]

# Mean of all engine cols
engine_cols <- c("engine_speed", "engine_temp")
meanDT <- diagnosticDT[, lapply(.SD, mean), .SDcols = engine_cols]
print(meanDT)


get_numeric_cols <- function(DT){
    num_cols <- DT[, sapply(.SD, is.numeric)]
    return(names(DT)[num_cols])
}

# Function to get correlation matrix from a data.table
corrmat_from_dt <- function(DT){
    numeric_cols <- get_numeric_cols(DT)
    return(DT[, cor(.SD), .SDcols = numeric_cols])
}

# Get correlation matrices
corrmat_from_dt(diagnosticDT)
corrmat_from_dt(diagnosticDT2)

```
  
  
  
***
  
Chapter 2 - Getting Time Series Data into data.table  
  
Overview of the POSIXct Type:  
  
* The name POSIX means "Portable Operating System for Unix"  
	* POSIXlt = a list object with date-time components like year and day stored in individual attributes  
    * POSIXct = a signed integer representing seconds since 1970-01-01, with a single attribute capturing timezone  
    * POSIXct is generally preferred in databases, since it is easiest to sort and manipulate  
    * This course always uses UTC (constant time zone)  
* Can convert other formats to POSIXct  
	* as.POSIXct("2004-10-27", tz = "UTC")  
    * as.POSIXct(1540153601, origin = "1970-01-01", tz = "UTC")  # if the integer is number of seconds since 1970-01-01  
    * as.POSIXct(as.Date(42885, origin = "1900-01-01"), tz = "UTC")  # if the integer is number of days since 1970-01-01  
* The as.POSIXct is vectorized  
	* dates <- c("2004-10-24", "2004-10-25", "2004-10-26")  
    * as.integer(as.POSIXct(dates, tz = "UTC"))  
    * someDT <- data.table(dates = c("2004-10-24", "2004-10-25", "2004-10-26"))  
    * someDT[, posix := as.POSIXct(dates, tz = "UTC")]  
* Example for converting columns in a dataset without overwriting the original data  
	* gameDT <- data.table( game_date = c("2004-10-23", "2004-10-24", "2004-10-26", "2004-10-27") )  
    * gameDT[, posix_date := as.POSIXct(game_date, tz = "UTC")]  
* The lubridate family of functions can be applied also  
	* the_date <- "10-27-2004 22:29:00"  
    * lubridate::mdy_hms(the_date)  
  
Creating data.tables from vectors:  
  
* Can create data.tables from vectors sinmilar to the process for data.frames  
	* candyDT <- data.table( color = c("red", "blue", "green"), size = c("S", "L", "S"), num = c(100, 50, 210) )  
    * testDT <- data.table( rand_numbers = rnorm(100), rand_strings = sample(LETTERS, n = 100, replace = TRUE), simple_index = 1:100, sample_dates = seq.POSIXt( from = as.POSIXct("1990-01-01"), to = as.POSIXct("1992-08-01"), length.out = 100), fifty_fifty_split = c(rep(TRUE, 50), rep(FALSE, 50)) )  
* The seq.POSIXt() is an extension of seq() for time data  
	* start <- as.POSIXct("2010-06-17", tz = "UTC")  
    * end <- as.POSIXct("2010-06-18", tz = "UTC")  
    * hourlyDT <- data.table( timestamp = seq.POSIXt(start, end, length.out = 1 + 24) )  
    * minuteDT <- data.table( timestamp = seq.POSIXt(start, end, length.out = 1 + 24 * 60) )  
* Can use the .N to dynamically size the input vector  
	* add_stock_data <- function(DT){ DT[, COMPANY1 := rnorm(n = .N)] DT[, COMPANY2 := rnorm(n = .N)] }  
  
Coercing from xts:  
  
* The xts format is popular, and can be used for conversions to data.table  
	* dates <- seq.POSIXt(from = as.POSIXct("2017-06-15"), to = as.POSIXct("2017-06-16"), length.out = 24)  
    * ex_tee_ess<- xts::xts( x = rnorm(24), order.by = dates )  
* The xts object has several attributes  
	* tclass = R class for the date-time index  
    * tzone = timezone for date-time index  
    * attr(ex_tee_ess, "tclass")  
    * attr(ex_tee_ess, "tzone")  
* The expressive subsetting is one of the reasons for xts popularity  
	* ['/'] = "the whole dataset"  
    * ['2017'] = "data from 2017"  
    * ['2017-01/'] = "data from January 2017 to the end of the data"  
    * ['2014/2015'] = "data from 2014 to 2015"  
* Can also use functions for converting the units of time  
	* to.minutes(), to.minutes10(), to.daily()  
    * xts::to.daily(hourlyXTS)  
* Can convert to data.table using as.data.table() - may need to modify column names to be more meaningful  
	* hourlyDT <- data.table::as.data.table( hourlyXTS )  
    * data.table::setnames(hourlyDT, "V1", "stock_price")  
  
Combining datasets with merge() and rbindlist():  
  
* Merging with timestamps depends on numeric precision, since POSIXct is a number  
* Precision-safe merges can be managed using the round() function  
	* secDT[, timestamp := as.POSIXct(round(as.numeric(timestamp)), origin = "1970-01-01")]  
    * milliDT[, timestamp := as.POSIXct(round(as.numeric(timestamp)), origin = "1970-01-01")]  
    * merge(secDT, milliDT, by = "timestamp", all = TRUE)   
* May need to run down-sampling process to match up records appropriately  
	* salesDT[, .(ts, year = year(ts), mday = mday(ts), hour = hour(ts))]  
    * dailySalesDT[, day_int := mday(timestamp)]  
    * dailyPriceDT <- hourlyPriceDT[, .(price = mean(price)), by = mday(timestamp)]  
    * mergeDT <- merge( dailySalesDT, dailyPriceDT, by.x = "day_int", by.y = "day" )  
* Can also use rbindlist to stack data.tables - but, be careful about matching data types and names  
	* allDT <- rbindlist(list(DT1, DT2, DT3), fill = TRUE)  
  
Example code includes:  
```{r}

excelDT <- data.table(timecol=42885:42889, sales=c(105, 92, 500, 81, 230))
stringDT <- data.table(timecol=c("2017-06-01", "2017-06-02", "2017-06-03", "2017-06-04", "2017-06-05"),
                       sales=c(105, 92, 500, 81, 230)
                       )
epochSecondsDT <- data.table(timecol=1496275200 + 24*60*60*0:4, sales=c(105, 92, 500, 81, 230))
epochMillisDT <- data.table(timecol=1000 * c(1496275200 + 24*60*60*0:4), sales=c(105, 92, 500, 81, 230))


# Create POSIXct dates from a hypothetical Excel dataset
excelDT[, posix := as.POSIXct(as.Date(timecol, origin = "1900-01-01"), tz = "UTC")]

# Convert strings to POSIXct
stringDT[, posix := as.POSIXct(timecol, tz = "UTC")]

# Convert epoch seconds to POSIXct
epochSecondsDT[, posix := as.POSIXct(timecol, tz = "UTC", origin = "1970-01-01")]

# Convert epoch milliseconds to POSIXct
epochMillisDT[, posix := as.POSIXct(timecol/1000, origin = "1970-01-01", tz="UTC")]


stringDT <- data.table(timecol1=c("2017-06-01 10", "2017-06-02 5", "2017-06-03 10", 
                                  "2017-06-04 7", "2017-06-05 9"
                                  ),
                       timecol2=c("06-01-2017 10:00:00", "06-02-2017 05:00:00", "06-03-2017 10:00:00", 
                                  "06-05-2017 07:00:00", "06-04-2017 09:00:00"
                                  ),
                       sales=c(105, 92, 500, 81, 230)
                       )
stringDT


# Convert timecol1
str(stringDT)
stringDT[, posix1 := lubridate::ymd_h(timecol1)]
str(stringDT)

# Convert timecol2
str(stringDT)
stringDT[, posix2 := lubridate::mdy_hms(timecol2)]
str(stringDT)


# Generate a series of dates
march_dates <- seq.POSIXt(as.POSIXct("2017-03-01", tz="UTC"), 
                          as.POSIXct("2017-03-31", tz="UTC"), 
                          length.out = 31
                          )

# Generate hourly data
hourly_times <- seq.POSIXt(as.POSIXct("2017-05-01 00:00:00", tz="UTC"), 
                           as.POSIXct("2017-05-02 00:00:00", tz="UTC"), 
                           length.out = 1 + 24
                           )

# Generate sample IoT data
iotDT <- data.table(timestamp = seq.POSIXt(as.POSIXct("2016-04-19 00:00:00", tz="UTC"), 
                                           as.POSIXct("2016-04-20 00:00:00", tz="UTC"), 
                                           length.out = 1 + 24
                                           ),
                    engine_temp = rnorm(n=1+24),
                    ambient_temp = rnorm(n=1+24)
                    )
head(iotDT)


# Create a 500-row data.table
start_date <- "2016-01-01"
end_date <- "2018-01-01"
someDT <- data.table(timestamp = seq.POSIXt(as.POSIXct(start_date), 
                                            as.POSIXct(end_date), 
                                            length.out = 500
                                            )
                     )

# Function to add random columns
add_random_cols <- function(DT, colnames){
    for (colname in colnames){
        DT[, (colname) := rnorm(n = .N)]
    }
}

# Check out the new data.table
add_random_cols(someDT, c("copper", "chopper", "stopper"))


# Simulated data
some_data <- rnorm(100)
some_dates <- seq.POSIXt(from = as.POSIXct("2017-06-15 00:00:00Z", tz = "UTC"),
                         to = as.POSIXct("2017-06-15 01:00:00Z", tz = "UTC"),
                         length.out = 100
                         )

# Make your own 'xts' object
myXTS <- xts::xts(some_data, order.by=some_dates)

# View the timezone
print(attr(myXTS, "tzone"))


nickelXTS <- readRDS("./RInputFiles/nickelXTS.rds")

# All observations after 2018-01-01 00:45:00
fifteenXTS <- nickelXTS["2018-01-01 00:45:00/"]

# Check the structure
str(fifteenXTS)

# 10-minute aggregations
tenMinuteXTS <- xts::to.minutes10(nickelXTS)
print(tenMinuteXTS)

# 1-minute aggregations
oneMinuteXTS <- xts::to.minutes(nickelXTS)


# Convert to a data.table
nickelDT <- as.data.table(nickelXTS)
str(nickelDT)

# Change names
setnames(nickelDT, old="index", new="spot_price_timestamp")
print(nickelDT)


treasuryDT <- data.table(timestamp=as.POSIXct("2018-03-01 00:00:00", tz="UTC") + 0:4 + 0.001, 
                         treasury_10y=c(0.71, 0.8, 0.78, 0.77, 0.73)
                         )
oilDT <- data.table(timestamp=as.POSIXct("2018-03-01 00:00:00", tz="UTC") + 0:4, 
                    oil=c(44.07, 44.15, 44.14, 44.06, 44.09)
                    )

# Naive approach (merge on timestamp)
newDT <- merge(treasuryDT, oilDT, on = "timestamp")
str(newDT)

# Check out the precision
treasuryDT[, as.numeric(timestamp)]
oilDT[, as.numeric(timestamp)]

# Clean up and merge
treasuryDT[, timestamp := as.POSIXct(round(as.numeric(timestamp)), origin = "1970-01-01")]
newDT <- merge(treasuryDT, oilDT, on = "timestamp")
str(newDT)


# Add grouping indicator 
# fxDT[, yearmonth := paste0(year(timestamp), "_", month(timestamp))]
# exportDT[, yearmonth := paste0(year(timestamp), "_", month(timestamp))]

# Monthly exchange rate
# monthlyFXDT <- fxDT[, .(exch_rate = mean(exchange_rate)), by = yearmonth]

# Merge 
# merge(exportDT, monthlyFXDT, by="yearmonth")

```
  
  
  
***
  
Chapter 3 - Generating Lags, Differences, and Windowed Aggregations  
  
Generating Lags:  
  
* The lag represents the value of a time series n time-periods ago (common for forecasting)  
	* dailyDT[, lag15 := shift(sales, type = "lag", n = 15)]  
* The shift capability in data.table can either lag or lead, filling with NA when the data cannot be known  
	* someDT[, col1_lag1 := shift(col1, n = 1, type = "lag")]  
    * someDT[, col1_lead1 := shift(col1, n = 1, type = "lead")]  
* Need to ensure the proper sort order prior to lag/lead; the functions are time-naïve  
	* setorderv(backwardsDT, "timestamp")  # operation is run in-place  
    * backwardsDT[, somenums_lag1 := shift(somenums, type = "lag", n = 1)]  
* Can also generate lage on the fly while modeling  
	* mod <- lm(sales ~ shift(sales, n = 21), data = dailyDT)  
* With long datasets, may need to add a "by" so that the lagging is done by subject  
	* experimentDT[, lag1 := shift(result, type = "lag", n = 1), by = subject_id]  
  
Generating Growth Rates and Differences:  
  
* Often a goal to find a stationary time series - same mean, variance, over time  
* Typically, change in metric can make an increasing time series in to a stationary differences series  
	* gdpDT[, lag1 := shift(gdp, type = "lag", n = 1)]  
    * gdpDT[, diff1 := gdp - lag1]  
    * gdpDT[, diff1 := gdp - shift(gdp, type = "lag", n = 1)]  # same as above, but in a single shot  
* May instead want to capture the pace of change, such as a growth rate  
	* gdpDT[, growth1 := (gdp - shift(gdp, type = "lag", n = 1)) / shift(gdp, type = "lag", n = 1) ]  
    * gdpDT[, growth1 := (gdp / shift(gdp, type = "lag", n = 1)) - 1 ]  # algebraic simplification  
  
Windowing with j and by:  
  
* Windowing is the process of evaluating a metric over a time period - e.g., heartbeats per minute  
	* salesDT[, nearest_month := month(timestamp)]  # create a grouping column  
    * aggDT <- salesDT[, .( min = min(sales), total = sum(sales), num_obs = length(sales) ), by = nearest_month ]  
* Can also do on-the-fly grouping by passing an expression to the by-clause  
	* aggDT <- malfunctionDT[, .( min = min(sales), total = sum(sales), num_obs = length(sales) ), by = month(timestamp) ]  
  
Example code includes:  
```{r}

dailyDT <- data.table(timestamp=as.POSIXct("2018-08-01", tz="UTC") + lubridate::days(0:152), 
                      sales=c(483.08, 449.25, 523.6, 498.36, 448, 487.03, 502.91, 475.69, 535.39, 471.54, 494.57, 509.6, 538.43, 603.55, 560.84, 482.39, 456.42, 550.68, 526.83, 577.16, 515.7, 450.44, 522.18, 546.44, 530.86, 452.47, 498.56, 486.58, 523.58, 424.25, 587.53, 533.11, 477.74, 582.16, 449.59, 575.78, 523.92, 475.5, 556.5, 487.27, 515.98, 523.78, 528.1, 548.19, 484.26, 542.97, 540.72, 475.16, 483.19, 598.89, 419.74, 448.57, 494.05, 438.82, 460.1, 343.01, 525.2, 527.51, 461.07, 557.52, 577.24, 499.41, 431.83, 487.61, 412.54, 454.56, 471.44, 520.61, 519.03, 547.59, 541.78, 507.67, 448.96, 468.08, 494.02, 520.78, 442.87, 507.98, 553.78, 486.46, 476.9, 546.92, 502.69, 557.93, 445.11, 501.94, 491.04, 534.49, 533.16, 543.76, 484.38, 610.28, 528.18, 483.56, 509.4, 496.62, 439.98, 488.11, 475.01, 514.07, 567.83, 506.74, 496.28, 417.83, 499.35, 556.75, 511, 596.06, 537.87, 562.97, 496.55, 499.85, 460.23, 478.96, 451.44, 576.34, 466.04, 433.66, 530.26, 554.76, 469.11, 477.79, 542.85, 582.55, 464.29, 458.92, 585.33, 487.18, 576.68, 488.35, 441.12, 509.81, 464.99, 464, 506.53, 459.36, 554.13, 444, 436.21, 528.15, 480.87, 541.93, 496.3, 423.1, 546.8, 499.21, 543.36, 534.85, 523.89, 524.99, 522.67, 524.51, 502.6)
                      )
str(dailyDT)


# Sort by time
setorderv(dailyDT, "timestamp")

# 1-day lag
dailyDT[, sales_lag1 := shift(sales, type = "lag", n = 1)]

# 5-day lag
dailyDT[, sales_lag5 := shift(sales, type = "lag", n = 5)]


experimentDT <- data.table(day=c(1:3, 1:3), 
                           result=c(1, 3.3, 2.5, 1.1, 3.9, 3.8), 
                           subject_id=LETTERS[c(1, 1, 1, 2, 2, 2)]
                           )
experimentDT


# Yesterday
experimentDT[, yesterday := shift(result, type="lag", n=1), by=subject_id]

# Two days ago
experimentDT[, two_days_ago := shift(result, type="lag", n=2), by=subject_id]

# Preview experimentDT
print(experimentDT)


aluminumDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/aluminumDF.feather") %>%
                                rename(timestamp=Date, price=`Cash Buyer`) %>%
                                select(timestamp, price)
                            )
str(aluminumDT)


# Add 1-period and 2-period lags
aluminumDT[, lag1 := shift(price, type = "lag", n = 1)]
aluminumDT[, lag2 := shift(price, type = "lag", n = 2)]

# Fit models with 1 and 2 lags
mod1 <- lm(price ~ lag1, data = aluminumDT)
mod2 <- lm(price ~ lag1 + lag2, data = aluminumDT)

# Compare
stargazer::stargazer(list(mod1, mod2), type = "text")


# One-period lag
dailyDT[, sales_lag1 := shift(sales, type = "lag", n = 1)]

# One-period diff
dailyDT[, sales_diff1 := sales - sales_lag1]

# Two-period diff
dailyDT[, sales_diff2 := sales - shift(sales, type="lag", n=2)]


# Add 1-period percentage change
dailyDT[, sales_pctchng1 := sales_diff1 / sales_lag1]

# Add 2-period percentage change
dailyDT[, sales_pctchng2 := (sales / shift(sales, type="lag", n=2) - 1)]


passengerDT <- data.table(obs_time=lubridate::ymd_hms("2017-08-01 00:00:00") + lubridate::minutes(15*0:96), 
                          passengers=c(506, 513, 554, 427, 439, 476, 509, 382, 457, 498, 398, 385, 529, 442, 393, 500, 557, 439, 453, 488, 520, 546, 542, 492, 528, 493, 498, 530, 515, 537, 535, 518, 396, 623, 499, 467, 523, 499, 535, 383, 546, 552, 436, 556, 452, 512, 514, 476, 437, 432, 522, 492, 537, 480, 543, 485, 491, 512, 555, 498, 452, 502, 514, 452, 446, 458, 538, 414, 499, 433, 503, 466, 553, 473, 473, 546, 447, 545, 492, 554, 466, 618, 530, 568, 541, 433, 524, 433, 571, 506, 485, 466, 490, 467, 528, 427, 480)
                          )
str(passengerDT)


# Generation time in seconds
passengerDT[, obs_time_in_seconds := as.numeric(obs_time)]

# Add floor time for each time stamp
seconds_in_an_hour <- 60 * 60
passengerDT[, hour_end := floor(obs_time_in_seconds / seconds_in_an_hour)]

# Count number of observations in each hour
passengerDT[, .N, by = hour_end]


# Mean passengers per hour
passengerDT[, mean(passengers), by=hour_end]

# Cleaner names
passengerDT[, .(mean_passengers = mean(passengers)), by=hour_end]

# Generate hourly summary statistics
passengerDT[, .(min_passengers = min(passengers), max_passengers = max(passengers)), by=hour_end]

```
  
  
  
***
  
Chapter 4 - Case Study: Financial Data  
  
Modeling Metals Prices:  
  
* Data sets have been pulled from quandl; can then convert to data.table, including fixing variable/column names  
	* aluminumDF <- Quandl::Quandl(code = "LME/PR_AL", start_date = "2001-12-31", end_date = "2018-03-12")  
    * aluminumDT <- as.data.table(aluminumDF)  
    * newDT <- aluminumDT[, .(obstime = Date, aluminum_price = `Cash Seller & Settlement` )]  # The .() allows for both selecting by name and changing names  
* Can also apply functions using .()  
	* newDT <- aluminumDT[, .(obstime = as.POSIXct(Date, tz = "UTC"), aluminum_price = `Cash Seller & Settlement` )]  
* Can also apply merges based on timestamps  
	* mergedDT <- merge( x = aluminumDT, y = nickelDT, all = TRUE, by = "obstime" )  
* Can also use Reduce()  
	* Reduce( f = function(x,y){paste0(x, y, "|")}, x = c("a", "b", "c") )  
  
Time Series Feature Engineering:  
  
* May want to add differences and growth rates to the data  
	* gdpDT[, diff1 := gdp - shift(gdp, type = "lag", n = 1)]  
    * add_diffs <- function(DT){ DT[, diff1 := gdp - shift(gdp, type = "lag", n = 1)] ; return(invisible(NULL)) }  # note that a copy of DT is passed and so DT is edited  
* Can use parentheses for a user-defined name in the data.table  
	* colname <- "abc"  
    * someDT[, (colname) := rnorm(10)]  
    * add_diffs <- function(DT, newcol){ DT[, (newcol) := gdp - shift(gdp, type = "lag", n = 1)] ; return(invisible(NULL)) }  
    * add_diffs(DT, "diff1")  
* Can also use get() to have a flexible column for changing  
	* colname <- "def"  
    * someDT[, random_stuff := get(colname) * rnorm(10)]  
    * add_diffs <- function(DT, newcol, dcol){ DT[, (newcol) := get(dcol) - shift(get(dcol), type = "lag", n = 1)] ; return(invisible(NULL)) }  
    * add_diffs <- function(DT, newcol, dcol, ndiff){ DT[, (newcol) := get(dcol) - shift(get(dcol), type = "lag", n = ndiff)] return(invisible(NULL)) }  # allows for passing the number of time periods  
* Can also extend the methodology to growth rates  
	* add_growth_rates <- function(DT, newcol, dcol, ndiff){ DT[, (newcol) := (get(dcol) / shift(get(dcol), type = "lag", n = ndiff)) - 1 ] return(invisible(NULL)) }  
  
EDA and Model Building:  
  
* Feature selection is sometimes needed for modeling - for example, regressions may perform poorly with too many features  
* Can look at the correlations using data.table  
	* someDT <- data.table(x = rnorm(100), y = rnorm(100), z = rnorm(100))  
    * someDT[complete.cases(someDT)]  
    * cor(someDT)  
    * cmat <- cor(someDT[complete.cases(someDT)])  
    * cmat[, "x"]  
    * feat_cols <- c("var_1", "var_5")  
    * mod1 <- lm(target ~ ., data = trainDT[, .SD, .SDcols = feat_cols])  
  
Wrap Up:  
  
* Modify data.tables by reference  
* Growth rates and differences  
* Flexible code for changes in variables  
  
Example code includes:  
```{r}

copperDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/copperDF.feather"))
str(copperDT)
nickelDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/nickelDF.feather"))
str(nickelDT)
cobaltDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/cobaltDF.feather"))
str(cobaltDT)
tinDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/tinDF.feather"))
str(tinDT)
aluminumDT <- as.data.table(feather::read_feather("./RInputFiles/Feather_Data/aluminumDF.feather"))
str(aluminumDT)


# Rename "Cash Buyer" to "copper_price"
setnames(copperDT, old="Cash Buyer", new="copper_price")
setnames(cobaltDT, old="Cash Buyer", new="cobalt_price")
setnames(tinDT, old="Cash Buyer", new="tin_price")
setnames(aluminumDT, old="Cash Buyer", new="aluminum_price")

# Convert `"Date"` to POSIXct
copperDT[, close_date := as.POSIXct(Date, tz="UTC")]
cobaltDT[, close_date := as.POSIXct(Date, tz="UTC")]
tinDT[, close_date := as.POSIXct(Date, tz="UTC")]
aluminumDT[, close_date := as.POSIXct(Date, tz="UTC")]

# Create copperDT2 with "close_date" and "copper_price"
copperDT2 <- copperDT[, .(close_date, copper_price)]
cobaltDT2 <- cobaltDT[, .(close_date, cobalt_price)]
tinDT2 <- tinDT[, .(close_date, tin_price)]
aluminumDT2 <- aluminumDT[, .(close_date, aluminum_price)]

# Create a new data.table using .() subsetting
nickelDT2 <- nickelDT[, .(
    close_date = as.POSIXct(Date, tz = "UTC"),
    nickel_price = `Cash Buyer`
)]
str(nickelDT2)


# Merge copperDT and cobaltDT with merge()
mergedDT <- merge(cobaltDT2, copperDT2, by="close_date", all=TRUE)

# Merge five tables into one
mergedDT <- Reduce(f = function(x, y) { merge(x, y, by="close_date", all=TRUE) },
                   x = list(aluminumDT2, copperDT2, cobaltDT2, nickelDT2, tinDT2)
                   )


# Function to add differences
add_diffs <- function(DT, cols, ndiff){
    for (colname in cols){
        new_name <- paste0(colname, "_diff", ndiff)
        DT[, (new_name) := get(colname) - shift(get(colname), type = "lag", n = ndiff)]
    }
}

# Add 2-period diffs
add_diffs(mergedDT, paste0(c("aluminum", "cobalt", "copper", "nickel", "tin"), "_price"), 2)


# Function to add growth rates
add_growth_rates <- function(DT, cols, ndiff){
    for (colname in cols){
        new_name <- paste0(colname, "_pctchg", ndiff)
        DT[, (new_name) := (get(colname) / shift(get(colname), type = "lag", n = ndiff)) - 1]
    }
}

# Add 1-period growth rate
add_growth_rates(mergedDT, paste0(c("aluminum", "cobalt", "copper", "nickel", "tin"), "_price"), 1)


# Function to get correlation matrix from a data.table
corrmat_from_dt <- function(DT, cols){
    # Subset to the requested columns
    subDT <- DT[, .SD, .SDcols=cols]
    subDT <- subDT[complete.cases(subDT)]
    return(cor(subDT))
}

# Get correlations of prices
corrmat_from_dt(mergedDT, paste0(c("aluminum", "cobalt", "copper", "nickel", "tin"), "_price"))


# Add 1-period first differences
price_cols <- c("aluminum_price", "cobalt_price", "copper_price", "nickel_price", "tin_price")
add_diffs(DT = mergedDT, cols = price_cols, ndiff = 1)

# Rename aluminum first difference to "target"
setnames(mergedDT, "aluminum_price_diff1", "target")

# Add 1-period growth rates
add_growth_rates(DT = mergedDT, cols = price_cols, ndiff = 1)

# Correlation matrix
diff_cols <- grep("_diff", x = names(mergedDT), value = TRUE)
growth_cols <- grep("_pctchg", x = names(mergedDT), value = TRUE)
corrmat_from_dt(DT = mergedDT, cols = c(diff_cols, growth_cols, "target"))[, "target"]


# Add 1-period differences
add_diffs(mergedDT, paste0(c("cobalt", "copper", "nickel", "tin"), "_price"), 1)

# Add 3-period growth rates
add_growth_rates(mergedDT, paste0(c("cobalt", "copper", "nickel", "tin"), "_price"), 3)

# Add 12-period difference in nickel price
add_diffs(mergedDT, paste0(c("nickel"), "_price"), 12)

# Top 4 difference / growth columns
top_features <- c("copper_price_diff1", "nickel_price_diff1", "tin_price_diff1", "copper_price_pctchg3")

```
  
  
  
***
  
### _Interactive Data Visualization with plotly in R_  
  
Chapter 1 - Introduction to plotly  
  
What is plotly?  
  
* Interface to the plotly javascript library - plots can work in multiple formats  
	* Active development and support community  
* Need to consider the relative values of the static graphic and the interactive graphic  
* Example of the wine plot using ggplot2  
	* static <- wine %>% ggplot(aes(x = Flavanoids, y = Proline, color = Type)) + geom_point()  
* Can use ggplotly() to convert a static object to an interactive plot  
	* plotly::ggplotly(static)  
  
Univariate Graphics:  
  
* May want to look at distributions of a categorical variable - example using the wine dataset  
	* wine %>%  
    *     count(Type) %>%                 # create a frequency table  
    *     plot_ly(x = ~Type, y = ~n) %>%  # specify aesthetics (similar to ggplot aes() function)  
    *     add_bars()                      # add the bars trace  
    * wine %>% count(Type) %>% mutate(Type = forcats::fct_reorder(Type, n, .desc = TRUE)) %>% plot_ly(x = ~Type, y = ~n) %>% add_bars()  # example of using forcats for reordering  
* Can instead run a histogram using plotly  
	* wine %>%  
    *     plot_ly(x = ~Phenols) %>%  # specify aesthetics  
    *     add_histogram()            # add the histogram trace  
    * wine %>% plot_ly(x = ~Phenols) %>% add_histogram(nbinsx = 10)  # adjust the number of bins to a precise number  
    * wine %>% plot_ly(x = ~Phenols) %>% add_histogram(xbins = list(start = 0.8, end = 4, size = 0.25))  # provide specific bin ranges  
  
Bivariate Graphics:  
  
* Can extend to looking at scatterplots, boxplots, and the like  
* Example scatterplot for exploring relationships in numeric variables  
	* winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers()  
* Example stacked bars  
	* winequality %>% count(type, quality_label) %>% plot_ly(x = ~type, y = ~n, color = ~quality_label) %>% add_bars() %>% layout(barmode = "stack")  
* Converting to proportional bars  
	* winequality %>%  
    *     count(type, quality_label) %>%  
    *     group_by(type) %>%                # group the table  
    *     mutate(prop = n / sum(n)) %>%     # calculate the proportions  
    *     plot_ly(x = ~type, y = ~n, color = ~quality_label) %>%  
    *     add_bars() %>%  
    *     layout(barmode = "stack")  
* Can also run boxplots to explore numeric distributions vs. a categorical variable  
	* winequality %>% plot_ly(x = ~quality_label, y = ~alcohol) %>% add_boxplot()  
  
Example code includes:  
```{r eval=FALSE}

vgsales <- readr::read_csv("./RInputFiles/vgsales.csv")
glimpse(vgsales)


# Store the scatterplot of Critic_Score vs. NA_Sales sales in 2016
scatter <- vgsales %>%
  filter(Year == 2016) %>%
  ggplot(aes(x = NA_Sales, y = Critic_Score)) +
  geom_point(alpha = 0.3)

# Convert the scatterplot to a plotly graphic
plotly::ggplotly(scatter)


library(plotly)

# Create a histogram of Critic_Score
vgsales %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram()

# Create a histogram of Critic_Score with at most 25 bins
vgsales %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(nbinsx = 25)

# Create a histogram with bins of width 10 between 0 and 100
vgsales %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(xbins = list(start=0, end=100, size=10))


# Create a frequency for Genre
genre_table <- vgsales %>%
    count(Genre)

# Reorder the bars for Genre by n
genre_table %>%
    filter(!is.na(Genre)) %>%
    mutate(Genre = fct_reorder(Genre, n, .desc=TRUE)) %>%
    plot_ly(x = ~Genre, y = ~n) %>% 
    add_bars()


# Create a scatter plot of User_Score against Critic_Score
vgsales %>% 
    filter(!is.na(Critic_Score) & !is.na(User_Score)) %>%
    plot_ly(x=~Critic_Score, y=~User_Score) %>%
    add_markers()


# Filter out the 2016 video games
vg2016 <- vgsales %>%
    filter(Year == 2016)

# Create a stacked bar chart of Rating by Genre
vg2016 %>%
    count(Genre, Rating) %>%
    plot_ly(x = ~Genre, y = ~n, color = ~Rating) %>%
    add_bars() %>%
    layout(barmode = "stack")

# Create boxplots of Global_Sales by Genre for above data
vg2016 %>% 
  plot_ly(x=~Global_Sales, y=~Genre) %>%
  add_boxplot()

```
  
  
  
***
  
Chapter 2 - Styling and Customizing Graphics  
  
Customize Traces:  
  
* Can change colors; for example, non-blue histograms  
	* winequality %>% plot_ly(x = ~fixed_acidity) %>% add_histogram()  
    * winequality %>% plot_ly(x = ~fixed_acidity) %>% add_histogram(color = I("red"))  # The I() function is for as-is, since plotly otherwise assumes mapping of a color  
* Can change opacity, for example in scatterplots with over-plotting  
	* winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers()  
    * winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers(marker = list(opacity = 0.2))  
* Can change the plotting symbol from filled to open  
	* winequality %>% plot_ly(x = ~residual_sugar, y = ~fixed_acidity) %>% add_markers(marker = list(symbol = "circle-open"))  
* Additional marker options include  
	* opacity  
    * color  
    * symbol (scatter/box)  
    * size (scatter)  
    * width (bar/histogram)  
  
Thoughtful Use of Color:  
  
* Color can be used thoughtfully to represent variables in a dataset  
* Coloring by category can make the trends more evident  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Type) %>% add_markers()  
* Coloring can add a third quantitative variable to the plot (gradient)  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Color) %>% add_markers()  
* Can also add the RColorBrewer palettes or use a custom color palette  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Type) %>% add_markers(colors = "Dark2")  
    * wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, color = ~Type) %>% add_markers(colors = c("orange", "black", "skyblue"))  # note that as-is I() is not needed here, since color=~Type already requested the aesthetic  
  
Hover Info:  
  
* Hover information is added in plotly automtically - coordinate pairs, but without variable names for scatter  
* Can change the defaults to hovering to make the charts more intuitive  
	* wine %>%  
    *     count(Type) %>%  
    *     plot_ly(x = ~Type, y = ~n, hoverinfo = "y") %>%  
    *     add_bars()  
* Can customize hover to add variable names to a scatter plot  
	* wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol, hoverinfo = "text", text = ~paste("Flavanoids:", Flavanoids, "<br>", "Alcohol:", Alcohol) ) %>% add_markers()  # uses the html "<br>" and the ~means columns are mapping as aesthetics  
  
Customizing Layout:  
  
* The layout() function is the workhorse for many of the desired changed to plotly - like labs() and theme() in ggplot2  
* Example for adding axis labels (by list) and axis titles(by string)  
	* winequality %>% plot_ly(x = ~free_so2, y = ~total_so2) %>% add_markers(marker = list(opacity = 0.2)) %>% layout(xaxis = list(title = "Free SO2 (ppm)"), yaxis = list(title = "Total SO2 (ppm)"), title = "Does free SO2 predict total SO2 in wine?")  
    * winequality %>% plot_ly(x = ~free_so2, y = ~total_so2) %>% add_markers(marker = list(opacity = 0.2)) %>% layout(xaxis = list(title = "Free SO2 (ppm, log scale)", type = "log"), yaxis = list(title = "Total SO2 (ppm, log scale)", type = "log"), title = "Does free SO2 predict total SO2 in wine?")  
* Can remove the zero lines and grids from the plot  
	* winequality %>% plot_ly(x = ~free_so2, y = ~total_so2) %>% add_markers(marker = list(opacity = 0.5)) %>% layout(xaxis = list(title = "Free SO2 (ppm)", zeroline = FALSE), yaxis = list(title = "Total SO2 (ppm)", zeroline = FALSE, showgrid = FALSE))  
* Can make additional customizations to plotting canvas - paper_bgcolor=toRGB() and plot_bgcolor=toRBG()  
  
Example code includes:  
```{r eval=FALSE}

# Filter out the 2016 video games
vgsales2016 <- vgsales %>%
    filter(Year == 2016)
str(vgsales2016)


# Create a histogram of Critic_Score with navy bars that are 50% transparent
vgsales2016 %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(color = I("navy"), opacity = 0.5)


# Change the color of the histogram using a hex code
vgsales2016 %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(color=I("#111e6c"))

# Change the color of the histogram using rgb()
vgsales2016 %>%
    filter(!is.na(Critic_Score)) %>%
    plot_ly(x = ~Critic_Score) %>%
    add_histogram(marker = list(color = "rgb(17, 30, 108)"))


# Set the plotting symbol to diamond and the size to 4
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score)) %>%
    plot_ly(x = ~User_Score, y = ~Critic_Score) %>% 
    add_markers(marker = list(symbol="diamond", size=4))


# Use color to add Genre as a third variable
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(Genre)) %>%
    plot_ly(x=~Critic_Score, y=~User_Score, color=~Genre) %>%
    add_markers(colors="Dark2")


# Create a scatterplot of User_Score against Critic_Score coded by Rating
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(Rating)) %>%
    plot_ly(x=~Critic_Score, y=~User_Score, symbol=~Rating) %>%
    add_markers()


# Create a scatterplot of User_Score vs. Critic_Score colored by User_Count
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(User_Count)) %>%
    plot_ly(x = ~Critic_Score, y = ~User_Score, color=~User_Count) %>%
    add_markers()

# Create a scatterplot of User_Score vs. Critic_Score colored by log User_Count
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(User_Count)) %>%
    plot_ly(x = ~Critic_Score, y = ~User_Score, color=~log(User_Count)) %>%
    add_markers()


# Create a bar chart of Platform with hoverinfo only for the bar heights
vgsales2016 %>%
    filter(!is.na(Platform)) %>%
    count(Platform) %>%
    plot_ly(x=~Platform, y=~n, hoverinfo="y") %>%
    add_bars()


# Create a scatterplot of User_Score vs. Critic score
vgsales2016 %>%
    filter(!is.na(Critic_Score), !is.na(User_Score), !is.na(Name)) %>%
    # Add video game Name to the hover info text
    plot_ly(x=~Critic_Score, y=~User_Score, hoverinfo="text", text=~Name) %>% 
    add_markers()


# Format the hover info for NA_Sales, EU_Sales, and Name
vgsales2016 %>% 
    filter(!is.na(NA_Sales), !is.na(EU_Sales), !is.na(Name)) %>%
    plot_ly(x = ~NA_Sales, y = ~EU_Sales, hoverinfo = "text",
            text = ~paste("NA_Sales:", NA_Sales, "<br>", "EU_Sales:", EU_Sales, "<br>", "Name:", Name)
            ) %>%
    add_markers()


# Polish the scatterplot by transforming the x-axis and labeling both axes
vgsales2016 %>%
    filter(!is.na(Global_Sales), !is.na(Critic_Score)) %>%
    plot_ly(x = ~Global_Sales, y = ~Critic_Score) %>%
    add_markers(marker = list(opacity = 0.5)) %>%
    layout(xaxis = list(title="Global sales (millions of units)", type="log"),
           yaxis = list(title="Critic score")
           )


# Set the background color to #ebebeb and remove the vertical grid
vgsales %>%
    filter(!is.na(Year)) %>%
    group_by(Year) %>%
    summarize(Global_Sales = sum(Global_Sales, na.rm=TRUE)) %>%
    plot_ly(x = ~Year, y = ~Global_Sales) %>%
    add_lines() %>%
    layout(xaxis=list(showgrid=FALSE), paper_bgcolor="#ebebeb")

```
  
  
  
***
  
Chapter 3 - Advanced Charts  
  
Layering Traces:  
  
* Layering traces allows for creating more complex charts - goal is the simplest chart that communicates the message effectively  
* Example for adding a loess smooth to a scatterplot - need to calculate the smooth, then pass it using add_lines with a y aesthetic  
	* m <- loess(Alcohol ~ Flavanoids, data = wine, span = 1.5)  
    * wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol) %>% add_markers() %>% add_lines(y = ~fitted(m)) %>% layout(showlegend = FALSE)  
    * m2 <- lm(Alcohol ~ poly(Flavanoids, 2), data = wine)  
    * wine %>% plot_ly(x = ~Flavanoids, y = ~Alcohol) %>% add_markers(showlegend = FALSE) %>% add_lines(y = ~fitted(m), name = "LOESS") %>% add_lines(y = ~fitted(m2), name = "Polynomial")  
* Can also use layers to compare densities and distributions  
	* d1 <- filter(wine, Type == 1)  
    * d2 <- filter(wine, Type == 2)  
    * d3 <- filter(wine, Type == 3)  
    * density1 <- density(d1$Flavanoids)  
    * density2 <- density(d2$Flavanoids)  
    * density3 <- density(d3$Flavanoids)  
    * plot_ly(opacity = 0.5) %>%  
    *     add_lines(x = ~density1$x, y = ~density1$y, name = "Type 1") %>%  
    *     add_lines(x = ~density2$x, y = ~density2$y, name = "Type 2") %>%  
    *     add_lines(x = ~density3$x, y = ~density3$y, name = "Type 3") %>%  
    *     layout(xaxis = list(title = 'Flavonoids'), yaxis = list(title = 'Density'))  
  
Subplots:  
  
* A series of subplots can be a powerful way to explore the interactions of various variables - example of base code that would be better with subplots  
	* vgsales2016 %>% plot_ly(x = ~Critic_Score, y = ~User_Score, color = ~Genre) %>% add_markers()  # too many colors, hard to read  
* Example of creating a single subplot  
	* action_df <- vgsales2016 %>% filter(Genre == "Action")  
	* action_df %>% plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers()  
* Example of creating two subplots  
	* p1 <- action_df %>% plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers()  
    * p2 <- vgsales2016 %>% filter(Genre == "Adventure") %>% plot_ly(x = ~life.expectancy, y = ~happiness) %>% add_markers()  
    * subplot(p1, p2, nrows = 1)  
* Can add legends to the individuals subplots, which are then used by the subplot() command  
	* p1 <- plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers(name = ~Genre)  
    * p2 <- vgsales2016 %>% filter(Genre == "Adventure") %>% plot_ly(x = ~Critic_Score, y = ~User_Score) %>% add_markers(name = ~Genre)  
    * subplot(p1, p2, nrows = 1)  
* Can create shared axis labels  
	* subplot(p1, p2, nrows = 1, shareY = TRUE, shareX = TRUE)  
* Can also create facets more automatically using group_by() and do()  
	* library(dplyr)  
    * vgsales2016 %>%  
    *     group_by(region) %>%  
    *     do(plot = plot_ly(data = ., x = ~Critic_Score, y = ~User_Score) %>%  
    *     add_markers(name = ~Genre)) %>%  
    *     subplot(nrows = 2)  
  
Scatterplot Matrices:  
  
* Can explore the pairwise relationship (pairs plot) for some or all of the variables in the dataset  
	* data %>% plot_ly() %>% add_trace( type = 'splom', dimensions = list( list(label='string-1', values=X1), list(label='string-2', values=X2), . . . list(label='string-n', values=Xn)) )  
    * Need to pass that we want "splom" (scatterplot matrix), then give each of the variables and their names  
    * wine %>% plot_ly() %>% add_trace( type = 'splom', dimensions = list( list(label='Alcohol', values=~Alcohol), list(label='Flavonoids', values=~Flavanoids), list(label='Color', values=~Color) ) )  
* There is linked brushing in plotly, so changes in one panel will carry through to other panels  
* Can also add color to the scatterplot matrices  
	* wine %>% plot_ly(color = ~Type) %>% add_trace( type = 'splom', dimensions = list( list(label='Alcohol', values=~Alcohol), list(label='Flavonoids', values=~Flavanoids), list(label='Color', values=~Color) ) )  
  
Binned Scatterplots:  
  
* With large datasets, some charts (e.g., scatterplots) will perform poorly  
* Binned scatterplots can be a solution to the overplotting problem of a large scatterplot  
	* sim_data %>% plot_ly(x = ~x, y = ~y) %>% add_histogram2d()  
    * sim_data %>% plot_ly(x = ~x, y = ~y) %>% add_histogram2d(nbinsx = 200, nbinsy = 100)  
  
Example code includes:  
```{r eval=FALSE}

vgsales2016 <- vgsales %>%
    mutate(User_Score = as.numeric(User_Score)) %>%
    filter(Year == 2016, !is.na(User_Score), !is.na(Critic_Score))
str(vgsales2016)


# Fit the regression model of User_Score on Critic_Score
m <- lm(User_Score ~ Critic_Score, data = vgsales2016)

# Create the scatterplot with smoother
vgsales2016 %>%
   select(User_Score, Critic_Score) %>%
   na.omit() %>%
   plot_ly(x = ~Critic_Score, y = ~User_Score) %>%
   add_markers(showlegend = FALSE) %>%
   add_lines(y = ~fitted(m))


activision <- vgsales2016 %>% filter(Publisher == "Activision")
ea <- vgsales2016 %>% filter(Publisher == "Electronic Arts")
nintendo <- vgsales2016 %>% filter(Publisher == "Nintendo")

# Compute density curves
d.a <- density(activision$Critic_Score, na.rm = TRUE)
d.e <- density(ea$Critic_Score, na.rm = TRUE)
d.n <- density(nintendo$Critic_Score, na.rm = TRUE)

# Overlay density plots
plot_ly() %>%
  add_lines(x = ~d.a$x, y = ~d.a$y, name = "Activision", fill = 'tozeroy') %>%
  add_lines(x = ~d.e$x, y = ~d.e$y, name = "Electronic Arts", fill = 'tozeroy') %>%
  add_lines(x = ~d.n$x, y = ~d.n$y, name = "Nintendo", fill = 'tozeroy') %>%
  layout(xaxis = list(title = 'Critic Score'),
         yaxis = list(title = 'Density'))


# Create a scatterplot of User_Score against Critic_Score for PS4 games
p1 <- vgsales2016 %>%
   filter(Platform == "PS4") %>%
   plot_ly(x = ~Critic_Score, y = ~User_Score) %>% 
   add_markers(name = "PS4")

# Create a scatterplot of User_Score against Critic_Score for XOne games
p2 <- vgsales2016 %>%
   filter(Platform == "XOne") %>%
   plot_ly(x = ~Critic_Score, y = ~User_Score) %>% 
   add_markers(name = "XOne")

# Create a facted scatterplot containing p1 and p2
subplot(p1, p2, nrows=2)


# Create a faceted scatterplot of User_Score vs. Critic_Score with 3 rows
vgsales2016 %>%
  group_by(Platform) %>%
  do(plot = plot_ly(data = ., x=~Critic_Score, y=~User_Score) %>% 
         add_markers(name = ~Platform)
     ) %>%
  subplot(nrows = 3, shareY = TRUE, shareX = TRUE)


# Add x-axis and y-axis labels, and a title
sp2 <- subplot(p1, p2, nrows = 2, shareX=TRUE, shareY=TRUE) %>%
    layout(title="User score vs. critic score by platform, 2016")
sp2

# Add x-axis and y-axis labels, and a title to  sp2
sp2 %>%
   layout(xaxis = list(title=""), xaxis2 = list(title="Year"), 
          yaxis = list(title="Global Sales (M units)"), yaxis2 = list(title="Global Sales (M units)")
          )


# Create a SPLOM of NA_Sales, EU_Sales, and JP_Sales
vgsales2016 %>%
  plot_ly() %>%
  add_trace(type = "splom", dimensions = list(list(label = "N. America", values = ~NA_Sales),
                                              list(label = "Europe", values = ~EU_Sales),
                                              list(label = "Japan", values = ~JP_Sales)
                                              )
            )


# Color the SPLOM of NA_Sales, EU_Sales, and JP_Sales by nintendo
vgsales2016 %>%
  mutate(nintendo = ifelse(Publisher == "Nintendo", "Nintendo", "Other")) %>%
  plot_ly(color=~nintendo) %>% 
  add_trace(type="splom", dimensions = list(list(label = "N. America", values = ~NA_Sales),
                                            list(label = "Europe", values = ~EU_Sales),
                                            list(label = "Japan", values = ~JP_Sales)
                                            )
            )


# Delete the diagonal plots in splom
splom %>%
   style(diagonal = list(visible=FALSE))

# Delete the plots in the upper half of splom
splom %>%
   style(showupperhalf=FALSE)

# Delete the plots in the lower half of splom
splom %>%
   style(showlowerhalf=FALSE)


# Create a binned scatterplot of User_Score vs. Critic_Score
vgsales %>%
  plot_ly(x=~Critic_Score, y=~User_Score) %>%
  add_histogram2d(nbinsx=50, nbinsy=50)

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Introduction to 2018 Election Data:  
  
* The 2018 US midterm election featured 435 House seats, 35 Senate seats, and 36 governors  
* Can use tunrout data and fundraising data from the US elections project and US FEC respectively  
	* glimpse(turnout)  
    * glimpse(fundraising)  
* Can also see results of key races using  
	* glimpse(senate_winners)  
  
Choropleth Maps:  
  
* The choropleth can be used to show key variables by geography  
	* turnout %>%  
    *     plot_geo(locationmode = 'USA-states') %>%  
    *     add_trace(z = ~turnout, locations = ~state.abbr) %>%  
    *     layout(geo = list(scope = 'usa')) # restricts map only to USA  
* Not all world regions are available in plot_geo - currently just locationmode: "USA-states" | "ISO-3" | "country names"  
* Mapping options can be passed to the geo-layout  
	* scope = "world" | "usa" | "europe" | "asia" | "africa" | "north america" | "south america"  
    * projection = list(type = "mercator")  "conic conformal" | "mercator" | "robinson" | "stereographic" | and 18 more…  
    * scale = 1  (Larger values = tighter zoom)  
    * center = list(lat = ~c.lat, lon = ~c.lon)  # Set c.lat and c.lon to center the map  
  
From Polygons to Maps:  
  
* Since not all regions are available, may need to cutom-create a choropleth  
* Need to start with a boundaries dataset, such as might be available in "us_states"  
	* head(us_states)  
* Can join data, after cleaning up potential merging issues  
	* turnout <- turnout %>% mutate(state = tolower(state)) # make state names lowercase  
    * states_map <- left_join(us_states, turnout, by = c("region" = "state"))  
* Can then plot the data  
	* states_map %>%  
    *     group_by(group) %>%  
    *     plot_ly( x=~long, y=~lat, color=~turnout2018, split=~region ) %>%  
    *     add_polygons(line = list(width = 0.4), showlegend = FALSE)  
* May want to further clean up the map - axes, ticks, etc.  
	* state_turnout_map %>% layout( title = "2018 Voter Turnout by State", xaxis = list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE), yaxis = list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE) ) %>% colorbar(title = "Turnout")  
  
Wrap Up:  
  
* Chapter 1 - Basics of Plotly and conversion of ggplot2 graphics - univariate, bivariate distributions  
* Chapter 2 - Customizing Plotly Charts - reduce over-plotting, change plotting symbols, adding color, hovering, custom layouts  
* Chapter 3 - Advanced Charts - layering traces, subplots, scatterplot matrices, binned scatterplots  
* Chapter 4 - Mapping Data - choropleths, polygons, customizing map apearances  
  
Example code includes:  
```{r eval=FALSE}

turnout <- readr::read_csv("./RInputFiles/TurnoutRates.csv")
str(turnout)


# Create a scatterplot of turnout2018 against turnout2014
p <- turnout %>%
    plot_ly(x=~turnout2014, y=~turnout2018) %>%
    add_markers() %>%
    layout(xaxis = list(title="2014 voter turnout"),
           yaxis = list(title="2018 voter turnout")
           )

p


# Add the line y = x to the scatterplot
p %>%
  add_lines(x = c(0.25, 0.6), y = c(0.25, 0.6)) %>%
  layout(showlegend=FALSE)


# Create a dotplot of voter turnout in 2018 by state ordered by turnout
turnout %>%
    top_n(15, wt = turnout2018) %>%
    plot_ly(x = ~turnout2018, y = ~fct_reorder(state, turnout2018)) %>%
    add_markers() %>%
    layout(yaxis=list(title="State", type="category"), xaxis=list(title="Elgible voter turnout"))


fundraising <- readr::read_csv("./RInputFiles/fec_candidate_summary_2018.csv")
str(fundraising)


# Create a histogram of receipts for the senate races
fundraising %>%
    filter(office=="S") %>%
    plot_ly(x=~receipts) %>%
    add_histogram() %>%
    layout(title="Fundraising for 2018 Senate races", xaxis=list(title="Total contributions received"))


# Create a dotplot of the top 15 Senate campaigns
fundraising %>%
    filter(office == "S") %>%
    top_n(15, wt = receipts) %>%
    plot_ly(x = ~receipts, y = ~fct_reorder(state, receipts), color = ~fct_drop(party), 
            hoverinfo = "text", text = ~paste("Candidate:", name, "<br>", "Party:", party, "<br>",
                                              "Receipts:", receipts, "<br>",
                                              "Disbursements:", disbursement
                                              )
            ) %>%
    add_markers(colors = c("blue", "red")) 


# Create a choropleth map of the change in voter turnout from 2014 to 2018
turnout %>%
    mutate(change = turnout2018 - turnout2014) %>%
    plot_geo(locationmode = 'USA-states') %>%
    add_trace(z=~change, locations=~state.abbr) %>%
    layout(geo = list(scope="usa"))


senate_winners <- readr::read_csv("./RInputFiles/senate_winners.csv")
str(senate_winners)


# Create a choropleth map displaying the Senate results
senate_winners %>%
    plot_geo(locationmode = "USA-states") %>%
    add_trace(z=~as.numeric(as.factor(party)), locations=~state, 
              colors = c("dodgerblue", "mediumseagreen", "tomato"),
              hoverinfo = "text", text = ~paste("Candidate:", name, "<br>",
                                                "Party:", party, "<br>",
                                                "% vote:", round(pct.vote, 1)
                                                )
              ) %>%
    layout(geo = list(scope = 'usa')) %>% 
    hide_colorbar()


# Map President Trump's rallies in 2018
# rallies2018 %>%
#     plot_geo(locationmode = 'USA-states') %>%
#     add_markers(x=~long, y=~lat, size=~no.speakers, 
#                 hoverinfo = "text", text = ~paste(city, state, sep = ",")
#                 ) %>%
#     layout(title = "2018 Trump Rallies", geo = list(scope = "usa"))


# Customize the geo layout
g <- list(scope = 'usa', 
          showland = TRUE, landcolor = "gray90",
          showlakes = TRUE, lakecolor = "white",
          showsubunit = TRUE, subunitcolor = "white"
          )

# Apply the geo layout to the map
# rallies2018 %>%
#     plot_geo(locationmode = 'USA-states') %>%
#     add_markers(x = ~long, y = ~lat, size = ~no.speakers, 
#                 hoverinfo = "text", text = ~paste(city, state, sep = ",")
#                 ) %>%
#     layout(title = "2018 Trump Rallies", geo = list(scope="usa"))


# Customize the geo layout
g <- list(scope = 'usa', 
          showland = TRUE, landcolor = toRGB("gray90"),
          showlakes = TRUE, lakecolor = toRGB("white"),
          showsubunit = TRUE, subunitcolor = toRGB("white")
          )

# Apply the geo layout to the map
# rallies2018 %>%
#     plot_geo(locationmode = 'USA-states') %>%
#     add_markers(x = ~long, y = ~lat, size = ~no.speakers, 
#                 hoverinfo = "text", text = ~paste(city, state, sep = ",")
#                 ) %>%
#     layout(title = "2018 Trump Rallies", geo = g)


fl_boundaries <- readr::read_csv("./RInputFiles/fl_boundaries.csv")
str(fl_boundaries)
fl_results <- readr::read_csv("./RInputFiles/fl_results.csv")
str(fl_results)


# Create a choropleth map displaying the Senate winners
# senate_vote %>%
#     group_by(group) %>%
#     plot_ly(x=~long, y=~lat, color=~PartyCode, split=~region) %>%
#     add_polygons(line = list(width=0.4), showlegend=FALSE)

# Adjust the polygon colors and boundaries
# senate_map %>%
#     group_by(group) %>%
#     plot_ly(x = ~long, y = ~lat, color = ~party, split = ~region, 
#             colors=c("dodgerblue", "mediumseagreen", "tomato")
#             ) %>%
#     add_polygons(line = list(width = 0.4, color=toRGB("gray60")), showlegend = FALSE)

# Define the layout settings to polish the axes
# map_axes <- list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE)

# Apply the layout to both axes
# senate_map %>%
#     group_by(group) %>%
#     plot_ly(x = ~long, y = ~lat, color = ~party, split = ~region, 
#             colors = c("dodgerblue", "mediumseagreen", "tomato")
#             ) %>%
#     add_polygons(line = list(width = 0.4, color = toRGB("gray60")), showlegend = FALSE) %>%
#     layout(xaxis=map_axes, yaxis=map_axes)


# Join the fl_boundaries and fl_results data frames
senate_vote <- left_join(fl_boundaries, fl_results, by = c("subregion" = "CountyName"))

# Specify the axis settings to polish the map
map_axes <- list(title = "", showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE)

# Create a polished county-level choropleth map of Pctvote
senate_vote %>%
    group_by(group) %>%
    plot_ly(x = ~long, y = ~lat, color = ~Pctvote, split = ~subregion) %>%
    add_polygons(line = list(width = 0.4), showlegend = FALSE, colors = c("blue", "red")) %>%
    layout(xaxis = map_axes, yaxis = map_axes)

```
  
  
  
***
  
### _Hyperparameter Tuning in R_  
  
Chapter 1 - Introduction to hyperparameters  
  
Parameters vs. Hyperparameters:  
  
* The hyper-parameters differ from the model parameters  
	* Model parameters are being fit during training  
    * Hyper-parameters are set prior to training and specify how the training should happen  
  
Recap of machine learning basics:  
  
* First step for hyoer-parameter training is to split the data in to test/train subsets  
	* index <- caret::createDataPartition(breast_cancer_data$diagnosis, p = .70, list = FALSE)  
    * bc_train_data <- breast_cancer_data[index, ]  
    * bc_test_data  <- breast_cancer_data[-index, ]  
* Can then train the model using caret  
	* fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * tictoc::tic()  # timing function  
    * set.seed(42)  
    * rf_model <- train(diagnosis ~ ., data = bc_train_data, method = "rf", trControl = fitControl, verbose = FALSE)  
    * tictoc::toc()  # timing function  
* The caret package automatically attempts a few hyper-parameters and picks the best for the final results  
  
Hyperparameter tuning in caret:  
  
* The caret package automatically performs some hyper-parameter tuning (e.g., tries a few values for mtry)  
* Many models are available in caret, with details available at  
	* modelLookup(model)  
    * https://topepo.github.io/caret/available-models.html  
* Example for running an SVM using caret  
	* fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * tictoc::tic()  # timing function  
    * set.seed(42)  
    * svm_model <- train(diagnosis ~ ., data = bc_train_data, method = "svmPoly", trControl = fitControl, verbose= FALSE)  
    * tictoc::toc()  # timing function  
* Can specify the number of values per hyperparameter to attempt for automatic tuning  
	* svm_model_2 <- train(diagnosis ~ ., data = bc_train_data, method = "svmPoly", trControl = fitControl, verbose = FALSE, tuneLength = 5)  # tuneLenght=5 means "try 5 of each"  
* Can also manually specify hyperparameters for tuning  
	* hyperparams <- expand.grid(degree = 4, scale = 1, C = 1)  
    * svm_model_3 <- train(diagnosis ~ ., data = bc_train_data, method = "svmPoly", trControl = fitControl, tuneGrid = hyperparams, verbose = FALSE)  
  
Example code includes:  
```{r}

breast_cancer_data <- readr::read_csv("./RInputFiles/breast_cancer_data.csv")
str(breast_cancer_data)
# bc_train_data <- readr::read_csv("./RInputFiles/bc_train_data.csv")
# str(bc_train_data)


# Fit a linear model on the breast_cancer_data.
linear_model <- lm(concavity_mean ~ symmetry_mean, data=breast_cancer_data)

# Look at the summary of the linear_model.
summary(linear_model)

# Extract the coefficients.
coef(linear_model)


# Plot linear relationship.
ggplot(data = breast_cancer_data, aes(x = symmetry_mean, y = concavity_mean)) +
    geom_point(color = "grey") +
    geom_abline(slope = coef(linear_model)[2], intercept = coef(linear_model)[1])


# Create partition index
index <- caret::createDataPartition(breast_cancer_data$diagnosis, p = 0.7, list = FALSE)

# Subset `breast_cancer_data` with index
bc_train_data <- breast_cancer_data[index, ]
bc_test_data  <- breast_cancer_data[-index, ]

# Define 3x5 folds repeated cross-validation
fitControl <- caret::trainControl(method = "repeatedcv", number = 5, repeats = 3)

# Run the train() function
gbm_model <- caret::train(diagnosis ~ ., data = bc_train_data, method="gbm", 
                          trControl=fitControl, verbose = FALSE
                          )

# Look at the model
gbm_model



set.seed(42)  # Set seed.
tictoc::tic()  # Start timer.
gbm_model <- caret::train(diagnosis ~ ., data = bc_train_data, method = "gbm", 
                          trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3),
                          verbose = FALSE, tuneLength=4
                          )
tictoc::toc()  # Stop timer.


# Define hyperparameter grid.
hyperparams <- expand.grid(n.trees = 200, interaction.depth = 1, 
                           shrinkage = 0.1, n.minobsinnode = 10
                           )

# Apply hyperparameter grid to train().
set.seed(42)
gbm_model <- caret::train(diagnosis ~ ., data = bc_train_data, method = "gbm", 
                          trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3),
                          verbose = FALSE, tuneGrid=hyperparams
                          )

```
  
  
  
***
  
Chapter 2 - Hyperparameter Tuning with caret  
  
Hyperparameter tuning in caret:  
  
* Example for using 2016 US voter turnout dataset  
	* fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * gbm_model_voters <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE) 
    * gbm_model_voters  
* Can use a Cartesian grid for hyperparameters - increases the model run-time  
	* man_grid <- expand.grid(n.trees = c(100, 200, 250), interaction.depth = c(1, 4, 6), shrinkage = 0.1, n.minobsinnode = 10)  
    * fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5)  
    * gbm_model_voters_grid <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE, tuneGrid = man_grid)  
* Can plot the hyper-parameter performance either on accuracy or Kappa  
	* plot(gbm_model_voters_grid)  
    * plot(gbm_model_voters_grid, metric = "Kappa", plotType = "level")  
 
Grid vs. Random Search:  
  
* Can continue with the grid search methodology using ranges and seq()  
	* big_grid <- expand.grid(n.trees = seq(from = 10, to = 300, by = 50), interaction.depth = seq(from = 1, to = 10, length.out = 6), shrinkage = 0.1, n.minobsinnode = 10)  
* Can instead run random search, looking for good parameters randomly  
	* fitControl <- trainControl(method = "repeatedcv", number = 3, repeats = 5, search = "random")  
    * gbm_model_voters_random <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE, tuneLength = 5)  # tuneLength would likely need to be ~100  
  
Adaptive Resampling:  
  
* Adaptive resampling is a blend between grid sampling and random sampling  
	* Hyperparameter combinations are resampled with values near combinations that performed well  
    * Adaptive Resampling is, therefore, faster and more efficient!  
* Can implement adaptive resampling using caret  
	* min: minimum number of resamples per hyperparameter  
    * alpha: confidence level for removing hyperparameters  
    * method: "gls" for linear model or "BT" for Bradley-Terry (better for large number of hyper-parameters and/or models that are already close to optimal)  
    * complete: if TRUE generates full resampling set  
    * fitControl <- trainControl(method = "adaptive_cv", adaptive = list(min = 2, alpha = 0.05, method = "gls", complete = TRUE), search = "random")  
    * fitControl <- trainControl(method = "adaptive_cv", number = 3, repeats = 3, adaptive = list(min = 2, alpha = 0.05, method = "gls", complete = TRUE), search = "random")  
    * gbm_model_voters_adaptive <- train(turnout16_2016 ~ ., data = voters_train_data, method = "gbm", trControl = fitControl, verbose = FALSE, tuneLength = 7)  # tuneLength would generally be at least ~100  
  
Example code includes:  
```{r cache=TRUE}

tgtData <- rep(c("Did not vote", "Voted"), each=40)
vecVoteData <- c(2, 2, 3, 2, 2, 3, 3, 1, 2, 3, 4, 4, 4, 3, 1, 2, 2, 2, 3, 2, 1, 2, 3, 2, 1, 3, 3, 3, 3, 4, 2, 4, 1, 4, 3, 3, 2, 4, 2, 1, 3, 2, 2, 1, 3, 3, 3, 4, 3, 4, 3, 4, 3, 3, 2, 3, 4, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 4, 2, 3, 3, 3, 3, 3, 2, 4, 1, 3, 4, 3, 3, 2, 2, 3, 3, 2, 2, 1, 2, 4, 2, 3, 2, 3, 4, 3, 2, 2, 2, 4, 1, 2, 2, 3, 2, 1, 3, 4, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 3, 4, 1, 4, 4, 1, 3, 4, 4, 2, 2, 3, 3, 3, 2, 3, 1, 1, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 4, 2, 2, 2, 1, 4, 2, 2, 3, 3, 4, 2, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 3, 1, 3, 2, 1, 1, 3, 2, 2, 1, 2, 2, 1, 1, 1, 3, 2, 2, 1, 1, 2, 1, 3, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 4, 2, 1, 4, 2, 2, 3, 2, 3, 2, 3, 1, 3, 2, 1, 2, 2, 3, 2, 1, 1, 1, 3, 2, 1, 2, 2, 2, 2, 2, 2, 1, 3, 3, 1, 3, 3, 1, 3, 3, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2, 2, 4, 3, 3, 4, 1, 4, 4, 1, 2, 1, 3, 4, 4, 2, 3, 1, 3, 1, 3, 1, 1, 2, 3, 1, 2, 1, 3, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 4, 1, 1, 3, 1, 2, 2, 2, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 4, 3, 3, 2, 1, 1, 2, 3, 3, 1, 2, 3, 2, 2, 3, 2, 3, 3, 1, 1, 2, 2, 2, 2, 2, 3, 1, 3, 2, 2, 3, 4, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 1, 4, 1, 3, 3, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 3, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 3, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, 2, 2, 1, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 4, 2, 2, 1, 2, 2, 3, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 1, 1, 2, 3, 2, 2, 2, 3, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 4, 2, 3, 2, 1, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 3, 1, 2, 2, 2, 2, 3, 1, 2, 2, 3, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2)
vecVoteData <- c(vecVoteData, 2, 3, 1, 2, 2, 2, 3, 2, 4, 1, 1, 2, 2, 2, 3, 4, 3, 4, 2, 2, 3, 2, 2, 4, 1, 1, 3, 4, 2, 4, 3, 3, 2, 4, 3, 3, 2, 2, 1, 3, 3, 1, 2, 2, 1, 1, 1, 1, 3, 2, 2, 1, 2, 2, 3, 3, 2, 1, 3, 2, 3, 3, 1, 3, 1, 2, 1, 2, 3, 3, 2, 3, 3, 2, 4, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 1, 1, 2, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1, 3, 3, 1, 2, 2, 3, 1, 3, 3, 4, 2, 3, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 2, 2, 3, 2, 4, 1, 1, 3, 3, 3, 3, 4, 3, 2, 3, 3, 1, 3, 1, 4, 1, 1, 4, 4, 4, 4, 3, 3, 4, 3, 2, 3, 4, 2, 4, 1, 1, 3, 4, 3, 1, 1, 3, 4, 4, 4, 1, 1, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 4, 4, 3, 3, 4, 2, 2, 3, 2, 3, 4, 2, 3, 4, 3, 3, 2, 2, 1, 2, 2, 8, 2, 8, 8, 2, 2, 2, 2, 2, 1, 2, 2, 8, 8, 8, 2, 1, 2, 2, 2, 1, 2, 1, 8, 8, 2, 2, 8, 8, 1, 2, 2, 2, 8, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 8, 2, 1, 1, 1, 1, 2, 1, 1, 2, 8, 2, 1, 1, 2, 1, 2, 1, 3, 8, 3, 3, 1, 3, 2, 2, 3, 2, 1, 3, 3, 3, 3, 3, 3, 8, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 8, 2, 3, 8, 3, 2, 3, 2, 3, 3, 3, 3, 2, 3, 1, 3, 1, 8, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 1, 8, 8, 3, 1, 3, 3, 3, 3, 8, 3, 3, 3, 3, 8, 3, 3, 8, 3, 1, 3, 3, 3, 2, 3, 8, 3, 3, 3, 3, 8, 2, 2, 3, 1, 1, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 8, 2, 1, 1, 1, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 8, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 8, 2)
vecVoteData <- c(vecVoteData, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 8, 2, 1, 2, 2, 1, 2, 2, 2, 8, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 8, 8, 2, 1, 1, 8, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 8, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 8, 2, 1, 1, 2, 2, 1, 1, 8, 2, 1, 1, 8, 1, 1, 1, 2, 8, 1, 1, 8, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 2, 1, 2, 1, 4, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 3, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 4, 2, 1, 1, 2, 1, 1, 3, 2, 4, 2, 2, 1, 1, 1, 2, 1, 1, 2, 4, 1, 2, 2, 3, 1, 2, 2, 4, 3, 2, 1, 1, 1, 3, 1, 4, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 4, 1, 3, 1, 1, 2, 2, 2, 3, 2, 2, 1, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 3, 2, 1, 1, 1, 1, 2, 4, 2, 2, 2, 1, 2, 1, 3, 1, 2, 2, 3, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 4, 1, 2, 3, 2, 1, 3, 1, 2, 2, 1, 3, 2, 2, 1, 2, 3, 4, 1, 1, 2, 3, 1, 1, 1, 2, 3, 4, 2, 1, 4, 2, 2, 2, 2, 1, 2, 3, 1, 1, 2, 1, 1, 4, 1, 1, 4, 1, 4, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 3, 4, 1, 2, 1, 1, 3, 1, 1, 3, 1, 1, 2, 1, 2, 2, 1, 2, 2, 3, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 4, 1, 1, 2, 1, 2, 1, 1, 1, 4, 1, 1, 2, 2, 2, 3, 3, 2, 2, 1, 2, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 3, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 3, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 2, 1, 1, 1, 2)
vecVoteData <- c(vecVoteData, 1, 1, 1, 2, 3, 2, 1, 1, 2, 3, 4, 1, 1, 2, 2, 2, 4, 2, 2, 4, 3, 3, 2, 4, 4, 3, 3, 4, 2, 2, 4, 4, 4, 2, 3, 1, 4, 2, 4, 4, 3, 3, 1, 4, 3, 3, 3, 1, 3, 2, 1, 2, 1, 1, 3, 2, 4, 2, 3, 2, 2, 1, 4, 3, 3, 3, 1, 2, 3, 3, 1, 3, 4, 4, 4, 3, 3, 4, 1, 1, 2, 4, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 3, 1, 2, 2, 1, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 3, 1, 2, 2, 4, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 3, 2, 2, 1, 2, 1, 3, 2, 1, 3, 1, 1, 4, 2, 2, 1, 3, 2, 2, 3, 2, 1, 2, 2, 2, 3, 2, 2, 1, 1, 4, 1, 1, 2, 4, 2, 3, 2, 2, 2, 4, 4, 2, 1, 1, 2, 1, 2, 2, 2, 3, 3, 3, 4, 2, 4, 3, 1, 4, 3, 3, 2, 2, 2, 2, 3, 1, 2, 2, 4, 1, 1, 3, 4, 1, 1, 1, 3, 2, 4, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 4, 1, 2, 1, 4, 1, 3, 1, 3, 1, 1, 1, 3, 2, 2, 2, 1, 1, 2, 1, 1, 2, 3, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 4, 2, 1, 2, 2, 4, 2, 1, 3, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 3, 4, 2, 1, 1, 3, 2, 2, 1, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 4, 2, 1, 1, 4, 3, 1, 3, 1, 1, 2, 4, 3, 1, 4, 1, 2, 1, 3, 1, 2, 2, 4, 1, 2, 3, 1, 4, 1, 2, 4, 1, 1, 1, 1, 1, 1, 2, 3, 1, 4, 1, 4, 4, 2, 1, 4, 1, 4, 2, 2, 4, 2, 3, 1, 4, 3, 4, 3, 1, 3, 4, 1, 1, 1, 3, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 4, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 3, 1, 1, 2, 2, 2, 2, 1, 1, 2, 3, 3, 1, 2, 2, 4, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 3, 2, 2, 1, 2, 1, 2, 2, 3, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 3, 1, 3, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1)
vecVoteData <- c(vecVoteData, 1, 1, 2, 1, 4, 2, 2, 2, 2, 1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 2, 1, 2, 3, 1, 3, 1, 4, 1, 3, 3, 3, 2, 2, 2, 3, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 4, 1, 2, 2, 4, 3, 1, 3, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 4, 2, 2, 2, 3, 1, 2, 2, 4, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 3, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 3, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 4, 4, 3, 1, 4, 2, 2, 2, 3, 1, 1, 2, 1, 1, 4, 4, 1, 4, 3, 2, 1, 2, 4, 3, 3, 4, 1, 2, 1, 2, 2, 3, 3, 1, 4, 3, 3, 4, 3, 4, 1, 1, 3, 3, 1, 1, 3, 1, 1, 3, 1, 3, 3, 3, 3, 4, 4, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 4, 1, 1, 1, 2, 1, 4, 3, 4, 4, 2, 2, 2, 3, 1, 2, 1, 2, 2, 3, 3, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 3, 2, 2, 1, 1, 1, 2, 1, 3, 2, 1, 1, 2, 1, 1, 3, 3, 2, 2, 2, 2, 1, 3, 3, 4, 3, 3, 3, 3, 1, 3, 2, 3, 1, 1, 2, 2, 1, 3, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 3, 4, 1, 1, 2, 3, 3, 2, 2, 2, 2, 1, 2, 1, 4, 3, 3, 1, 1, 2, 1, 1, 4, 1, 1, 4, 2, 3, 2, 3, 3, 3, 2, 2, 2, 3, 1, 1, 1, 1, 3, 1, 2, 2, 3, 4, 3, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 3, 2, 3, 3, 3, 4, 2, 1, 3, 2, 2, 1, 2, 2, 1, 3, 1, 2, 4, 4, 2, 1, 1, 3, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 3, 1, 1, 2, 2, 1, 3, 1, 1, 4, 3, 3, 2, 4, 3, 3, 2, 2, 1, 2, 3, 2, 2, 2, 3, 1, 2, 2, 3, 2, 3, 2, 1, 4, 2, 3, 1, 1, 1, 2, 1, 1, 2, 2, 4, 1, 3, 4, 3, 4, 2, 1, 4, 2, 3, 2, 1, 2, 2, 4, 1, 2, 4, 4, 3, 3, 3, 4, 1, 2, 1, 2)
voters_train_data <- tibble(turnout16_2016=tgtData) %>% 
    bind_cols(as.data.frame(matrix(vecVoteData, nrow=80, byrow=FALSE)))
names(voters_train_data) <- c('turnout16_2016', 'RIGGED_SYSTEM_1_2016', 'RIGGED_SYSTEM_2_2016', 'RIGGED_SYSTEM_3_2016', 'RIGGED_SYSTEM_4_2016', 'RIGGED_SYSTEM_5_2016', 'RIGGED_SYSTEM_6_2016', 'track_2016', 'persfinretro_2016', 'econtrend_2016', 'Americatrend_2016', 'futuretrend_2016', 'wealth_2016', 'values_culture_2016', 'US_respect_2016', 'trustgovt_2016', 'trust_people_2016', 'helpful_people_2016', 'fair_people_2016', 'imiss_a_2016', 'imiss_b_2016', 'imiss_c_2016', 'imiss_d_2016', 'imiss_e_2016', 'imiss_f_2016', 'imiss_g_2016', 'imiss_h_2016', 'imiss_i_2016', 'imiss_k_2016', 'imiss_l_2016', 'imiss_m_2016', 'imiss_n_2016', 'imiss_o_2016', 'imiss_p_2016', 'imiss_r_2016', 'imiss_s_2016', 'imiss_t_2016', 'imiss_u_2016', 'imiss_x_2016', 'imiss_y_2016')
glimpse(voters_train_data)


# Define Cartesian grid
man_grid <- expand.grid(degree = c(1, 2, 3), scale = c(0.1, 0.01, 0.001), C = 0.5)

fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5)

# Start timer, set seed & train model
tictoc::tic()
set.seed(42)
svm_model_voters_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "svmPoly", 
                                      trControl = fitControl, verbose= FALSE, tuneGrid = man_grid
                                      )
tictoc::toc()


# Plot default
plot(svm_model_voters_grid)

# Plot Kappa level-plot
plot(svm_model_voters_grid, metric = "Kappa", plotType = "level")


# Define the grid with hyperparameter ranges
big_grid <- expand.grid(size = seq(from = 1, to = 5, by = 1), decay = c(0, 1))

# Train control with grid search
fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5, search = "grid")

# Train neural net
tictoc::tic()
set.seed(42)
nn_model_voters_big_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, 
                                         method = "nnet", trControl = fitControl, verbose = FALSE
                                         )
tictoc::toc()

# Train neural net
tictoc::tic()
set.seed(42)
nn_model_voters_big_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "nnet", 
                                         trControl = fitControl, verbose = FALSE, tuneGrid = big_grid
                                         )
tictoc::toc()


# Train control with random search
fitControl <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5, search = "random")

# Test 6 random hyperparameter combinations
tictoc::tic()
nn_model_voters_big_grid <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "nnet", 
                                         trControl = fitControl, verbose = FALSE, tuneLength = 6
                                         )
tictoc::toc()


# Define trainControl function
fitControl <- caret::trainControl(method="adaptive_cv", number = 3, repeats = 3)

# Define trainControl function
fitControl <- caret::trainControl(method = "adaptive_cv", number = 3, repeats = 3, search="random")

# Define trainControl function
fitControl <- caret::trainControl(method = "adaptive_cv", number = 3, repeats = 3,
                                  adaptive = list(min=3, alpha = 0.05, method = "BT", complete = FALSE),
                                  search = "random"
                                  )

# Start timer & train model
tictoc::tic()
svm_model_voters_ar <- caret::train(turnout16_2016 ~ ., data = voters_train_data, method = "nnet", 
                                    trControl = fitControl, verbose = FALSE, tuneLength = 6
                                    )
tictoc::toc()

```
  
  
  
***
  
Chapter 3 - Hyperparameter Tuning with mlr  
  
Machine Learning with mlr:  
  
* The package mlr can be used as a framework for machine learning in R  
	* 1.  Define the task  
    * 2.  Define the learner  
    * 3.  Fit the model  
* There are many possible tasks in mlr  
	* RegrTask() for regression  
    * ClassifTask() for binary and multi-class classification  
    * MultilabelTask() for multi-label classification problems  
    * CostSensTask() for general cost-sensitive classification  
* Example of making a classification  
	* task <- makeClassifTask(data = knowledge_train_data, target = "UNS")  
    * listLearners()  
    * lrn <- makeLearner("classif.h2o.deeplearning", fix.factors.prediction = TRUE, predict.type = "prob")  
    * model <- train(lrn, task)  
  
Grid and Random Search with mlr:  
  
* There are three things to define for hyperparameter tuning using mlr  
	* the search space for every hyperparameter  
    * the tuning method (e.g. grid or random search)  
    * the resampling method  
* Example of defining the search space for every hyperparameter  
	* makeParamSet( makeNumericParam(), makeIntegerParam(), makeDiscreteParam(), makeLogicalParam(), makeDiscreteVectorParam() )  
    * getParamSet("classif.h2o.deeplearning")  
    * param_set <- makeParamSet( makeDiscreteParam("hidden", values = list(one = 10, two = c(10, 5, 10))), makeDiscreteParam("activation", values = c("Rectifier", "Tanh")), makeNumericParam("l1", lower = 0.0001, upper = 1), makeNumericParam("l2", lower = 0.0001, upper = 1) )  
* Example of tuning - can use either grid search (requires discrete search) or random search (all types)  
	* ctrl_grid <- makeTuneControlGrid()  
    * ctrl_random <- makeTuneControlRandom()  
* Example of defining the resampling strategy  
	* cross_val <- makeResampleDesc("RepCV", predict = "both", folds = 5 * 3)  
    * param_set <- makeParamSet( makeDiscreteParam("mtry", values = c(2,3,4,5)) )  
    * ctrl_grid <- makeTuneControlGrid()  
    * task <- makeClassifTask(data = knowledge_train_data, target = "UNS")  
    * lrn <- makeLearner("classif.h2o.deeplearning", predict.type = "prob", fix.factors.prediction = TRUE)  
    * lrn_tune <- tuneParams(lrn, task, resampling = cross_val, control = ctrl_grid, par.set = param_set)  
  
Evaluating Hyperparameters with mlr:  
  
* Generally, the goal of hyperparameter tuning is to understand impact on performance as well as convergence on optimal parameters  
* Recap of the basic process, this time using a holdout sample rather than repeated cross-validation  
	* getParamSet("classif.h2o.deeplearning")  
    * param_set <- makeParamSet( makeDiscreteParam("hidden", values = list(one = 10, two = c(10, 5, 10))), makeDiscreteParam("activation", values = c("Rectifier", "Tanh")), makeNumericParam("l1", lower = 0.0001, upper = 1), makeNumericParam("l2", lower = 0.0001, upper = 1) )  
    * ctrl_random <- makeTuneControlRandom(maxit = 50)  
    * holdout <- makeResampleDesc("Holdout")  
    * task <- makeClassifTask(data = knowledge_train_data, target = "UNS")  
    * lrn <- makeLearner("classif.h2o.deeplearning", predict.type = "prob", fix.factors.prediction = TRUE)  
    * lrn_tune <- tuneParams(lrn, task, resampling = holdout, control = ctrl_random, par.set = param_set)  
* Can see the impact of each of the hyperparameter tunings  
	* generateHyperParsEffectData(lrn_tune, partial.dep = TRUE)  
    * hyperpar_effects <- generateHyperParsEffectData(lrn_tune, partial.dep = TRUE)  
    * plotHyperParsEffect(hyperpar_effects, partial.dep.learn = "regr.randomForest", x = "l1", y = "mmce.test.mean", z = "hidden", plot.type = "line")  
  
Advanced Tuning with mlr:  
  
* There are several advanced tuning capabilities available in mlr  
	* makeTuneControlCMAES: CMA Evolution Strategy  
    * makeTuneControlDesign: Predefined data frame of hyperparameters  
    * makeTuneControlGenSA: Generalized simulated annealing  
    * makeTuneControlIrace: Tuning with iterated F-Racing  
    * makeTuneControlMBO: Model-based / Bayesian optimization  
* Can choose the evaluation metrics within mlr  
	* ctrl_gensa <- makeTuneControlGenSA()  
    * bootstrap <- makeResampleDesc("Bootstrap", predict = "both")  
    * lrn_tune <- tuneParams(learner = lrn, task = task, resampling = bootstrap, control = ctrl_gensa, par.set = param_set, measures = list(acc, mmce))  
    * lrn_tune <- tuneParams(learner = lrn, task = task, resampling = bootstrap, control = ctrl_gensa, par.set = param_set, measures = list(acc, setAggregation(acc, train.mean), mmce, setAggregation(mmce, train.mean)))  
* Can also run using nested cross-validation  
	* lrn_wrapper <- makeTuneWrapper(learner = lrn, resampling = bootstrap, control = ctrl_gensa, par.set = param_set, measures = list(acc, mmce))  
    * model_nested <- train(lrn_wrapper, task)  
    * getTuneResult(model_nested)  
    * cv2 <- makeResampleDesc("CV", iters = 2)  
    * res <- resample(lrn_wrapper, task, resampling = cv2, extract = getTuneResult)  
    * generateHyperParsEffectData(res)  
    * lrn_best <- setHyperPars(lrn, par.vals = list(minsplit = 4, minbucket = 3, maxdepth = 6))  
    * model_best <- train(lrn_best, task)  
  
Example code includes:  
```{r cache=TRUE}

vecData <- c(0.08, 0.18, 0.1, 0.12, 0.09, 0.08, 0.2, 0.2, 0.13, 0.18, 0.24, 0.18, 0.31, 0.28, 0.325, 0.323, 0.299, 0.32, 0.329, 0.315, 0.325, 0.325, 0.312, 0.299, 0.48, 0.46, 0.48, 0.49, 0.495, 0.43, 0.4, 0.44, 0.49, 0.44, 0.46, 0.495, 0.49, 0.42, 0.78, 0.85, 0.06, 0.08, 0.2, 0.06, 0.1, 0.15, 0.12, 0.06, 0.15, 0.1, 0.02, 0.09, 0.1, 0.08, 0.09, 0.2, 0.28, 0.265, 0.275, 0.295, 0.32, 0.25, 0.27, 0.27, 0.29, 0.288, 0.255, 0.295, 0.243, 0.295, 0.276, 0.258, 0.28, 0.255, 0.265, 0.255, 0.39, 0.38, 0.37, 0.38, 0.1, 0.1, 0.2, 0.18, 0.1, 0.12, 0.19, 0.14, 0.18, 0.17, 0.1, 0.23, 0.18, 0.2, 0.09, 0.06, 0.15, 0.29, 0.3, 0.27, 0.3, 0.295, 0.29, 0.258, 0.32, 0.3, 0.29, 0.26, 0.305, 0.32, 0.295, 0.285, 0.3, 0.4, 0.4, 0.41, 0.41, 0.44, 0.42, 0.43, 0.08, 0.18, 0.1, 0.12, 0.3, 0.325, 0.45, 0.49, 0.39, 0.34, 0.75, 0.51, 0.1, 0.16, 0.25, 0.32, 0.32, 0.28, 0.55, 0.69, 0.61, 0.9, 0.8, 0.7, 0.12, 0.2, 0.3, 0.245, 0.276, 0.45, 0.33, 0.33, 0.34, 0.55, 0.78, 0.82, 0.9, 0.7, 0.21, 0.05, 0.06, 0.08, 0.14, 0.06, 0.25, 0.32, 0.28, 0.29, 0.295, 0.42, 0.33, 0.55, 0.6, 0.58, 0.61, 0.68, 0.1, 0.06, 0.1, 0.2, 0.12, 0.29, 0.1, 0.31, 0.29, 0.31, 0.305, 0.25, 0.27, 0.29, 0.255, 0.31, 0.65, 0.75, 0.76, 0.72, 0.05, 0.1, 0.06, 0.01, 0.1, 0.1, 0.2, 0.3, 0.27, 0.245, 0.38, 0.49, 0.33, 0.36, 0.39, 0.7, 0.72, 0.52, 0.6, 0.77, 0.79, 0.06, 0.08, 0.12, 0.2, 0.25, 0.3, 0.28, 0.255, 0.27, 0.3, 0.28, 0.255, 0.27, 0.59, 0.64, 0.85, 0.18, 0.12, 0.18, 0.09, 0.08, 0.21, 0.305, 0.1, 0.55, 0.7, 0.75, 0.68, 0.62, 0.28, 0.6, 0.85, 0.71, 0.32, 0.58, 0.41, 0.69, 0.38, 0.89, 0.31, 0.72, 0.02, 0.28, 0.46, 0.52, 0.67, 0.95, 0.28, 0.76, 0.15, 0.38, 0.58, 0.27, 0.12, 0.59, 0.88, 0.11, 0.38, 0.67, 0.52, 0.72, 0.68, 0.91, 0.05, 0.08, 0.35, 0.51, 0.1, 0.05, 0.2, 0.35, 0.75, 0.22, 0.36, 0.12, 0.33, 0.6, 0.53, 0.73, 0.12, 0.57, 0.72, 0.86, 0.79, 0.15, 0.1, 0.32, 0.4, 0.79, 0.86, 0.73, 0.08, 0.31, 0.81, 0.88, 0.4, 0.35, 0.8, 0.72, 0.02, 0.4, 0.32, 0.53, 0.15, 0.52, 0.7, 0.37, 0.31, 0.75, 0.38, 0.55, 0.61, 0.8, 0.75, 0.19, 0.37, 0.36, 0.66, 0.72, 0.78, 0.19, 0.4, 0.37, 0.52, 0.26, 0.52, 0.64, 0.55, 0.31, 0.56, 0.6, 0.63, 0.52, 0.29, 0.18, 0.54, 0.26, 0.41, 0.33, 0.58, 0.8, 0.87, 0.51, 0.24, 0.3, 0.15, 0.35, 0.18, 0.94, 0.31, 0.2, 0.38, 0.71, 0.18, 0.33, 0.42, 0.33, 0.31, 0.32, 0.33, 0.89, 0.4, 0.8, 0.32, 0.49, 0.92, 0.22, 0.7, 0.95, 0.65, 0.14, 0.77, 0.27, 0.3, 0.53, 0.75, 0.26, 0.24, 0.01, 0.9, 0.3, 0.65, 0.8, 0.25, 0.98, 0.72, 0.41, 0.08, 0.27, 0.78, 0.76, 0.65, 0.72, 0.76, 0.78, 0.42, 0.64, 0.75, 0.48, 0.28, 0.75, 0.1, 0.44, 0.76, 0.48, 0.7, 0.41, 0.78, 0.23, 0.62, 0.77, 0.42, 0.76, 0.27, 0.4, 0.65, 0.72, 0.28, 0.63, 0.06, 0.48, 0.78, 0.27, 0.65, 0.78, 0.3, 0.12, 0.29, 0.31, 0.49, 0.29, 0.64, 0.14, 0.31, 0.51, 0.29, 0.84, 0.19, 0.19, 0.3, 0.55, 0.02, 0.29, 0.3, 0.12, 0.09, 0.29, 0.78, 0.31, 0.25, 0.29, 0.4, 0.81, 0.31, 0.61, 0.25, 0.26, 0.1, 0.31, 0.18, 0.22, 0.56, 0.09, 0.9, 0.81, 0.9, 0.8, 0.85, 0.56, 0.78, 0.78, 0.77, 0.9, 0.86, 0.82, 0.75, 0.78, 0.79, 0.8, 0.87, 0.58, 0.79, 0.7, 0.81, 0.76, 0.5, 0.66, 0.71, 0.65, 0.77, 0.86, 0.83, 0.89, 0.9, 0.85, 0.71, 0.83, 0.89, 0.93, 0.47, 0.8, 0.75, 0.68, 0.33, 0.24, 0.25, 0.3, 0.33, 0.29, 0.2, 0.25, 0.24, 0.26, 0.1, 0.05, 0.26, 0.1, 0.01, 0.28, 0.32, 0.1, 0.3, 0.28, 0.24, 0.26, 0.25, 0.28, 0.18, 0.24, 0.15, 0.19, 0.29, 0.1, 0.33, 0.3, 0.13, 0.25, 0.28, 0.14, 0.34, 0.26, 0.1, 0.3, 0.3, 0.34, 0.6, 0.66, 0.65, 0.59, 0.45, 0.6, 0.25, 0.66, 0.62, 0.45, 0.55, 0.25, 0.59, 0.56, 0.51, 0.51, 0.67, 0.58, 0.53, 0.67, 0.67, 0.56, 0.34, 0.54, 0.67, 0.59, 0.54, 0.3, 0.55, 0.45, 0.83, 0.67, 0.65, 0.5, 0.58, 0.56, 0.48, 0.64)

knowledge_train_data <- tibble(UNS=rep(c("High", "Low", "Medium"), each=40))
mtxData <- data.frame(matrix(vecData, nrow=120, byrow=FALSE))
names(mtxData) <- c("STG", "SCG", "STR", "LPR", "PEG")
knowledge_train_data <- bind_cols(as.tibble(mtxData), knowledge_train_data)
glimpse(knowledge_train_data)


library(mlr)

# Create classification taks
task <- mlr::makeClassifTask(data = knowledge_train_data, target = "UNS")

# Call the list of learners
mlr::listLearners() %>%
    as.data.frame() %>%
    select(class, short.name, package) %>%
    filter(grepl("classif.", class))

# Create learner
lrn <- mlr::makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)


# Get the parameter set for neural networks of the nnet package
ParamHelpers::getParamSet("classif.nnet")

# Define set of parameters
param_set <- ParamHelpers::makeParamSet(ParamHelpers::makeDiscreteParam("size", values = c(2,3,5)),
                                        ParamHelpers::makeNumericParam("decay", lower = 0.0001, upper = 0.1)
                                        )

# Print parameter set
print(param_set)

# Define a random search tuning method.
ctrl_random <- mlr::makeTuneControlRandom()


# Define task
task <- makeClassifTask(data = knowledge_train_data, target = "UNS")

# Define learner
lrn <- makeLearner("classif.nnet", predict.type = "prob", fix.factors.prediction = TRUE)

# Define set of parameters
param_set <- makeParamSet(makeDiscreteParam("size", values = c(2,3,5)),
                          makeNumericParam("decay", lower = 0.0001, upper = 0.1)
                          )

# Define a random search tuning method.
ctrl_random <- mlr::makeTuneControlRandom(maxit = 6)

# Define a 2 x 2 repeated cross-validation scheme
cross_val <- mlr::makeResampleDesc("RepCV", folds = 2 * 2)

# Tune hyperparameters
tictoc::tic()
lrn_tune <- mlr::tuneParams(lrn, task, resampling = cross_val, control = ctrl_random, par.set=param_set)
tictoc::toc()


task <- makeClassifTask(data = knowledge_train_data, target = "UNS")
lrn <- makeLearner(cl = "classif.rpart", fix.factors.prediction = TRUE)
param_set <- makeParamSet(makeIntegerParam("minsplit", lower = 1, upper = 30),
                          makeIntegerParam("minbucket", lower = 1, upper = 30),
                          makeIntegerParam("maxdepth", lower = 3, upper = 10)
                          )
ctrl_random <- makeTuneControlRandom(maxit = 10)


# Create holdout sampling
holdout <- makeResampleDesc("Holdout")

# Perform tuning
lrn_tune <- tuneParams(learner = lrn, task = task, resampling = holdout, 
                       control = ctrl_random, par.set = param_set
                       )

# Generate hyperparameter effect data
hyperpar_effects <- generateHyperParsEffectData(lrn_tune, partial.dep = TRUE)

# Plot hyperparameter effects
plotHyperParsEffect(hyperpar_effects, partial.dep.learn = "regr.glm",
                    x = "minsplit", y = "mmce.test.mean", z = "maxdepth", plot.type = "line"
                    )


task <- makeClassifTask(data = knowledge_train_data, target = "UNS")
lrn <- makeLearner(cl = "classif.nnet", fix.factors.prediction = TRUE)
param_set <- makeParamSet(makeIntegerParam("size", lower = 1, upper = 5),
                          makeIntegerParam("maxit", lower = 1, upper = 300),
                          makeNumericParam("decay", lower = 0.0001, upper = 1)
                          )
ctrl_random <- makeTuneControlRandom(maxit = 10)


# Create holdout sampling
holdout <- makeResampleDesc("Holdout", predict = "both")

# Perform tuning
lrn_tune <- tuneParams(learner = lrn, task = task, resampling = holdout, control = ctrl_random, 
                       par.set = param_set,
                       measures = list(mmce, setAggregation(mmce, train.sd), 
                                       acc, setAggregation(acc, train.sd)
                                       )
                       )


task <- makeClassifTask(data = knowledge_train_data, target = "UNS")
lrn <- makeLearner(cl = "classif.nnet", fix.factors.prediction = TRUE)


# Set hyperparameters
lrn_best <- setHyperPars(lrn, par.vals = list(size=1, maxit = 150, decay = 0))

# Train model
model_best <- train(lrn_best, task)

```
  
  
  
***
  
Chapter 4 - Hyperparameter Tuning with h2o  
  
Machine Learning with h2o:  
  
* The h2o package in an open-source package that is designed for scalability (distributed clusters)  
* Need to start by initiating a cluster  
	* library(h2o)  
    * h2o.init()  
* Need to then convert the data to an h2o file and define the feature and target names  
	* glimpse(seeds_data)  
    * seeds_data_hf <- as.h2o(seeds_data)  
    * y <- "seed_type"  
    * x <- setdiff(colnames(seeds_data_hf), y)  
    * seeds_data_hf[, y] <- as.factor(seeds_data_hf[, y])  
* Can also create train, test, and validation sets  
	* sframe <- h2o.splitFrame(data = seeds_data_hf, ratios = c(0.7, 0.15), seed = 42)  
    * train <- sframe[[1]]  
    * valid <- sframe[[2]]  
    * test <- sframe[[3]]  
    * summary(train$seed_type, exact_quantiles = TRUE)  
* There are many model algorithms available in h2o  
	* Gradient Boosted models with h2o.gbm() & h2o.xgboost()  
    * Generalized linear models with h2o.glm()  
    * Random Forest models with h2o.randomForest()  
    * Neural Networks with h2o.deeplearning()  
* Example of running GBM using h2o  
	* gbm_model <- h2o.gbm(x = x, y = y, training_frame = train, validation_frame = valid)  
    * perf <- h2o.performance(gbm_model, test)  
    * h2o.confusionMatrix(perf)  
    * h2o.logloss(perf)  
    * h2o.predict(gbm_model, test)  
  
Grid and random search with h2o:  
  
* Can use the help files to obtain all of the available hyperparameters  
	* ?h2o.gbm  
* Brief recap of data preparation steps  
	* seeds_data_hf <- as.h2o(seeds_data)  
    * y <- "seed_type"  
    * x <- setdiff(colnames(seeds_data_hf), y)  
    * sframe <- h2o.splitFrame(data = seeds_data_hf, ratios = c(0.7, 0.15), seed = 42)  
    * train <- sframe[[1]]  
    * valid <- sframe[[2]]  
    * test <- sframe[[3]]  
* Can then define a hyperparameter grid  
	* gbm_params <- list(ntrees = c(100, 150, 200), max_depth = c(3, 5, 7), learn_rate = c(0.001, 0.01, 0.1))  
    * gbm_grid <- h2o.grid("gbm", grid_id = "gbm_grid", x = x, y = y, training_frame = train, validation_frame = valid, seed = 42, hyper_params = gbm_params)  
    * gbm_gridperf <- h2o.getGrid(grid_id = "gbm_grid", sort_by = "accuracy", decreasing = TRUE)  
    * best_gbm <- h2o.getModel(gbm_gridperf@model_ids[[1]])  # Top GBM model chosen by validation accuracy has id position 1 due to the decreasing accuracy sort above  
    * print(best_gbm@model[["model_summary"]])  
    * h2o.performance(best_gbm, test)  
* Can also run random search using h2o  
	* gbm_params <- list(ntrees = c(100, 150, 200), max_depth = c(3, 5, 7), learn_rate = c(0.001, 0.01, 0.1))  
    * search_criteria <- list(strategy = "RandomDiscrete", max_runtime_secs = 60, seed = 42)  
    * gbm_grid <- h2o.grid("gbm", grid_id = "gbm_grid", x = x, y = y, training_frame = train, validation_frame = valid, seed = 42, hyper_params = gbm_params, search_criteria = search_criteria)  
    * search_criteria <- list(strategy = "RandomDiscrete", stopping_metric = "mean_per_class_error", stopping_tolerance = 0.0001, stopping_rounds = 6)  
    * gbm_grid <- h2o.grid("gbm", x = x, y = y, training_frame = train, validation_frame = valid, seed = 42, hyper_params = gbm_params, search_criteria = search_criteria)  
  
Automatic machine learning with h2o:  
  
* Can also run automatic machine learning algorithms in h2o, merely by specifying a total run time  
	* Generalized Linear Model (GLM)  
    * (Distributed) Random Forest (DRF)  
    * Extremely Randomized Trees (XRT)  
    * Extreme Gradient Boosting (XGBoost)  
    * Gradient Boosting Machines (GBM)  
    * Deep Learning (fully-connected multi-layer artificial neural network)  
    * Stacked Ensembles (of all models & of best of family)  
* Can tune the hyperparameters prior to the full automatic machine learning algorithm runs (for GBM and deep learning)  
	* automl_model <- h2o.automl(x = x, y = y, training_frame = train, validation_frame = valid, max_runtime_secs = 60, sort_metric = "logloss", seed = 42)  
* Can view the leaderboard  
	* lb <- automl_model@leaderboard  
    * model_ids <- as.data.frame(lb)$model_id  
    * aml_leader <- h2o.getModel(model_ids[1])  
  
Wrap up:  
  
* Hyperparameters and benefits of tuning in machine learning models  
	* Cartesian Grid Search  
    * Random Search  
    * Adaptive Resampling  
    * Automatic Machine Learning  
    * Evaluating tuning results with performance metrics  
    * Stopping criteria  
  
Example code includes:  
```{r eval=FALSE}

# Code runs OK in Console, does not run in knitr
library(mlr)

vecSeed <- c(15.26, 14.29, 13.84, 16.14, 14.38, 14.69, 15.26, 13.89, 13.78, 13.74, 14.59, 13.99, 15.69, 14.7, 12.72, 14.11, 15.01, 13.02, 14.11, 13.45, 13.16, 15.49, 14.09, 13.94, 15.05, 17.08, 14.8, 13.5, 13.16, 15.5, 13.8, 15.36, 14.99, 14.43, 15.78, 17.63, 16.84, 19.11, 16.82, 16.77, 20.71, 17.12, 18.72, 20.2, 19.57, 19.51, 18.88, 18.98, 20.88, 18.81, 18.59, 18.36, 16.87, 18.17, 18.72, 19.46, 19.18, 18.95, 18.83, 17.63, 18.55, 18.45, 19.38, 19.13, 19.14, 20.97, 19.06, 18.96, 19.15, 20.24, 13.07, 13.34, 12.22, 11.82, 11.21, 11.43, 12.49, 10.79, 11.83, 12.01, 12.26, 11.18, 11.19, 11.34, 11.75, 11.49, 12.54, 12.02, 12.05, 12.55, 11.14, 12.1, 12.15, 10.8, 11.26, 11.41, 12.46, 12.19, 11.65, 11.56, 11.81, 10.91, 11.23, 11.27, 11.87, 14.84, 14.09, 13.94, 14.99, 14.21, 14.49, 14.85, 14.02, 14.06, 14.05, 14.28, 13.83, 14.75, 14.21, 13.57, 14.26, 14.76, 13.76, 14.18, 14.02, 13.82, 14.94, 14.41, 14.17, 14.68, 15.38, 14.52, 13.85, 13.55, 14.86, 14.04, 14.76, 14.56, 14.4, 14.91, 15.98, 15.67, 16.26, 15.51, 15.62, 17.23, 15.55, 16.19, 16.89, 16.74, 16.71, 16.26, 16.66, 17.05, 16.29, 16.05, 16.52, 15.65, 16.26, 16.34, 16.5, 16.63, 16.42, 16.29, 15.86, 16.22, 16.12, 16.72, 16.31, 16.61, 17.25, 16.45, 16.2, 16.45, 16.91, 13.92, 13.95, 13.32, 13.4, 13.13, 13.13, 13.46, 12.93, 13.23, 13.52, 13.6, 13.04, 13.05, 12.87, 13.52, 13.22, 13.67, 13.33, 13.41, 13.57, 12.79, 13.15, 13.45, 12.57, 13.01, 12.95, 13.41, 13.36, 13.07, 13.31, 13.45, 12.8, 12.82, 12.86, 13.02, 0.87, 0.9, 0.9, 0.9, 0.9, 0.88, 0.87, 0.89, 0.88, 0.87, 0.9, 0.92, 0.91, 0.92, 0.87, 0.87, 0.87, 0.86, 0.88, 0.86, 0.87, 0.87, 0.85, 0.87, 0.88, 0.91, 0.88, 0.89, 0.9, 0.88, 0.88, 0.89, 0.89, 0.88, 0.89, 0.87, 0.86, 0.91, 0.88, 0.86, 0.88, 0.89, 0.9, 0.89, 0.88, 0.88, 0.9, 0.86, 0.9, 0.89, 0.91, 0.85, 0.86, 0.86, 0.88, 0.9, 0.87, 0.88, 0.89, 0.88, 0.89, 0.89, 0.87, 0.9, 0.87, 0.89, 0.89, 0.91, 0.89, 0.89, 0.85, 0.86, 0.87, 0.83, 0.82, 0.83, 0.87, 0.81, 0.85, 0.82, 0.83, 0.83, 0.83, 0.86, 0.81, 0.83, 0.84, 0.85, 0.84, 0.86, 0.86, 0.88, 0.84, 0.86, 0.84, 0.86, 0.87, 0.86, 0.86, 0.82, 0.82, 0.84, 0.86, 0.86, 0.88, 5.76, 5.29, 5.32, 5.66, 5.39, 5.56, 5.71, 5.44, 5.48, 5.48, 5.35, 5.12, 5.53, 5.21, 5.23, 5.52, 5.79, 5.39, 5.54, 5.52, 5.45, 5.76, 5.72, 5.58, 5.71, 5.83, 5.66, 5.35, 5.14, 5.88, 5.38, 5.7, 5.57, 5.58, 5.67, 6.19, 6, 6.15, 6.02, 5.93, 6.58, 5.85, 6.01, 6.29, 6.38, 6.37, 6.08, 6.55, 6.45, 6.27, 6.04, 6.67, 6.14, 6.27, 6.22, 6.11, 6.37, 6.25, 6.04, 6.03, 6.15, 6.11, 6.3, 6.18, 6.26, 6.56, 6.42, 6.05, 6.25, 6.32, 5.47, 5.39, 5.22, 5.31, 5.28, 5.18, 5.27, 5.32, 5.26, 5.41, 5.41, 5.22, 5.25, 5.05, 5.44, 5.3, 5.45, 5.35, 5.27, 5.33, 5.01, 5.11, 5.42, 4.98, 5.19, 5.09, 5.24, 5.24, 5.11, 5.36, 5.41, 5.09, 5.09, 5.09, 5.13, 3.31, 3.34, 3.38, 3.56, 3.31, 3.26, 3.24, 3.2, 3.16, 3.11, 3.33, 3.38, 3.51, 3.47, 3.05, 3.17, 3.25, 3.03, 3.22, 3.06, 2.98, 3.37, 3.19, 3.15, 3.33)
vecSeed <- c(vecSeed, 3.68, 3.29, 3.16, 3.2, 3.4, 3.15, 3.39, 3.38, 3.27, 3.43, 3.56, 3.48, 3.93, 3.49, 3.44, 3.81, 3.57, 3.86, 3.86, 3.77, 3.8, 3.76, 3.67, 4.03, 3.69, 3.86, 3.48, 3.46, 3.51, 3.68, 3.89, 3.68, 3.75, 3.79, 3.57, 3.67, 3.77, 3.79, 3.9, 3.74, 3.99, 3.72, 3.9, 3.82, 3.96, 2.99, 3.07, 2.97, 2.78, 2.69, 2.72, 2.97, 2.65, 2.84, 2.78, 2.83, 2.69, 2.67, 2.85, 2.68, 2.69, 2.88, 2.81, 2.85, 2.97, 2.79, 2.94, 2.84, 2.82, 2.71, 2.77, 3.02, 2.91, 2.85, 2.68, 2.72, 2.67, 2.82, 2.8, 2.95, 2.22, 2.7, 2.26, 1.36, 2.46, 3.59, 4.54, 3.99, 3.14, 2.93, 4.18, 5.23, 1.6, 1.77, 4.1, 2.69, 1.79, 3.37, 2.75, 3.53, 0.86, 3.41, 3.92, 2.12, 2.13, 2.96, 3.11, 2.25, 2.46, 4.71, 1.56, 1.37, 2.96, 3.98, 5.59, 4.08, 4.67, 2.94, 4, 4.92, 4.45, 2.86, 5.32, 5.17, 1.47, 2.96, 1.65, 3.69, 5.02, 3.24, 6, 4.93, 3.7, 2.85, 2.19, 4.31, 3.36, 3.37, 2.55, 3.75, 1.74, 2.23, 3.68, 2.11, 6.68, 4.68, 2.25, 4.33, 3.08, 5.9, 5.3, 6, 5.47, 4.47, 6.17, 2.22, 4.42, 5.46, 5.2, 6.99, 4.76, 3.33, 5.81, 3.35, 4.38, 5.39, 3.08, 4.27, 4.99, 4.42, 6.39, 2.2, 3.64, 4.77, 5.34, 4.96, 4.99, 4.86, 5.21, 4.06, 4.9, 4.18, 7.52, 3.98, 3.6, 5.22, 4.83, 4.8, 5.17, 4.96, 5.22, 5.31, 4.74, 4.87, 4.83, 4.78, 4.78, 5.05, 4.65, 4.91, 5.22, 5, 4.83, 5.04, 5.1, 5.06, 5.23, 5.3, 5.01, 5.36, 5.48, 5.31, 5.18, 4.78, 5.53, 4.96, 5.13, 5.17, 5.14, 5.14, 6.06, 5.88, 6.08, 5.84, 5.8, 6.45, 5.75, 5.88, 6.19, 6.27, 6.18, 6.11, 6.5, 6.32, 6.05, 5.88, 6.45, 5.97, 6.27, 6.1, 6.01, 6.23, 6.15, 5.88, 5.93, 5.89, 5.79, 5.96, 5.92, 6.05, 6.32, 6.16, 5.75, 6.18, 6.19, 5.39, 5.31, 5.22, 5.18, 5.28, 5.13, 5, 5.19, 5.31, 5.27, 5.36, 5, 5.22, 5, 5.31, 5.31, 5.49, 5.31, 5.05, 5.18, 5.05, 5.06, 5.34, 5.06, 5.09, 4.83, 5.15, 5.16, 5.13, 5.18, 5.35, 4.96, 4.96, 5, 5.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3)
seeds_train_data <- as.data.frame(matrix(vecSeed, nrow=105, byrow=FALSE))
names(seeds_train_data) <- c('area', 'perimeter', 'compactness', 'kernel_length', 'kernel_width', 'asymmetry', 'kernel_groove', 'seed_type')
seeds_train_data$seed_type <- as.factor(seeds_train_data$seed_type)
glimpse(seeds_train_data)


# Initialise h2o cluster
h2o::h2o.init()

# Convert data to h2o frame
seeds_train_data_hf <- h2o::as.h2o(seeds_train_data)

# Identify target and features
y <- "seed_type"
x <- setdiff(colnames(seeds_train_data_hf), y)

# Split data into train & validation sets
sframe <- h2o::h2o.splitFrame(seeds_train_data_hf, seed = 42)
train <- sframe[[1]]
valid <- sframe[[2]]

# Calculate ratio of the target variable in the training set
summary(seeds_train_data_hf$seed_type, exact_quantiles = TRUE)


# Train random forest model
rf_model <- h2o::h2o.randomForest(x = x, y = y, training_frame = train, validation_frame = valid)

# Calculate model performance
perf <- h2o::h2o.performance(rf_model, valid = TRUE)

# Extract confusion matrix
h2o::h2o.confusionMatrix(perf)

# Extract logloss
h2o::h2o.logloss(perf)


# Define hyperparameters
dl_params <- list(hidden = list(c(50, 50), c(100, 100)), epochs = c(5, 10, 15), 
                  rate = c(0.001, 0.005, 0.01)
                  )


# Define search criteria
search_criteria <- list(strategy = "RandomDiscrete",
                        max_runtime_secs = 10, # this is way too short & only used to keep runtime short!
                        seed = 42
                        )

# Train with random search
dl_grid <- h2o::h2o.grid("deeplearning", grid_id = "dl_grid", x = x, y = y,
                         training_frame = train, validation_frame = valid, seed = 42,
                         hyper_params = dl_params, search_criteria = search_criteria
                         )


# Define early stopping
stopping_params <- list(strategy = "RandomDiscrete", stopping_metric = "misclassification",
                        stopping_rounds = 2, stopping_tolerance = 0.1, seed = 42
                        )


# Run automatic machine learning
automl_model <- h2o::h2o.automl(x = x, y = y, training_frame = train, max_runtime_secs = 10,
                                sort_metric = "mean_per_class_error", leaderboard_frame = valid, seed = 42
                                )


# Extract the leaderboard
lb <- automl_model@leaderboard
head(lb)

# Assign best model new object name
aml_leader <- automl_model@leader

# Look at best model
summary(aml_leader)

```
  
  
  
***
  
### _Intermediate Functional Programming with purrr_  
  
Chapter 1 - Programming with purrr  
  
Refresher of purrr Basics:  
  
* The map() function is one of the most basic purrr calls  
	* map(.x, .f, …)  # for each element of .x do .f  
* OpenData files available from French city St Malo  
	* JSON format; nested list  
* The map() function will always return a list by default  
	* res <- map(visit_2015, sum)  # returns a list  
* Can override to other preferred outputs, such as map_dbl()  
	* res <- map_dbl(visit_2015, sum)  # returns a numeric  
* Can also extend to map2(.x, .y, .f, …) which resolves to do .f(.x, .y, …)  
	* res <- map2(visit_2015, visit_2016, sum)  
    * res <- map2_dbl(visit_2015, visit_2016, sum)  
* Can use pmap() to run operations on 3+ items, though these need to be passed in as a list  
	* l <- list(visit_2014, visit_2015, visit_2016)  
    * res <- pmap(l, sum)  
    * res <- pmap_dbl(l, sum)  
  
Introduction to mappers:  
  
* The .f is the action element - function applied to every element, number n to extract the nth element, character vector of named elements to extract  
* The functions can either be regular functions or lambda (anonymous) functions  
	* map_dbl(visit_2014, function(x) { round(mean(x)) })  
* The anonymous function with a one-sided formula can be written in any of several ways  
	* map_dbl(visits2017, ~ round(mean(.x)))  # typically the default  
    * map_dbl(visits2017, ~ round(mean(.)))  
    * map_dbl(visits2017, ~ round(mean(..1)))  
    * map2(visits2016, visits2017, ~ .x + .y)  
    * map2(visits2016, visits2017, ~ ..1 + ..2)  
* Can extend to data with more than 2 parameters  
	* pmap(list, ~ ..1 + ..2 + ..3)  
* Can use as_mapper to create mapper objects from lambda functions  
	* round_mean <- function(x){ round(mean(x)) }  
    * round_mean <- as_mapper(~ round(mean(.x))))  
* Mappers have several benefits  
	* More concise  
    * Easier to read than functions  
    * Reusable  
  
Using Mappers to Clean Data:  
  
* Can use set_names from purrr to set the names of a list  
	* visits2016 <- set_names(visits2016, month.abb)  
    * all_visits <- list(visits2015, visits2016, visits2017)  
    * named_all_visits <- map(all_visits, ~ set_names(.x, month.abb))  
* The keep() function extracts elements that satisfy a condition  
	* over_30000 <- keep(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * over_mapper <- keep(visits2016, limit)  
* The discard() function removes elements that satisfy a condition  
	* under_30000 <- discard(visits2016, ~ sum(.x) > 30000)  
    * limit <- as_mapper(~ sum(.x) > 30000)  
    * under_mapper <- discard(visits2016, limit)  
    * names(under_mapper)  
* Can use keep() and discard() with map() to clean up lists  
	* df_list <- list(iris, airquality) %>% map(head)  
    * map(df_list, ~ keep(.x, is.factor))  
  
Predicates:  
  
* Predicates return either TRUE or FALSE - example of is.numeric()  
* Predicate functionals take an element and a predicate, and then use the predicate on the element  
	* keep(airquality, is.numeric)  # keep all elements that return TRUE when run against the predicate  
* There are also extensions of every() and some()  
	* every(visits2016, is.numeric)  
    * every(visits2016, ~ mean(.x) > 1000)  
    * some(visits2016, ~ mean(.x) > 1000)  
* The detect_index() returns the first and last element that satisfies a condition  
	* detect_index(visits2016, ~ mean(.x) > 1000)  # index of first element that satisfies  
    * detect_index(visits2016, ~ mean(.x) > 1000, .right = TRUE)  # index of last element that satisfies  
* The detect() returns the value rather than the index  
	* detect(visits2016, ~ mean(.x) > 1000, .right = TRUE)  
* The has_element() tests whether an object contains an item  
	* visits2016_mean <- map(visits2016, mean)  
    * has_element(visits2016_mean,981)  
  
Example code includes:  
```{r}

# Create the to_day function
to_day <- function(x) {
 x*24
}

visit_a <- c(117, 147, 131, 73, 81, 134, 121)
visit_b <- c(180, 193, 116, 166, 131, 153, 146)
visit_c <- c(57, 110, 68, 72, 87, 141, 67)

# Create a list containing both vectors: all_visits
all_visits <- list(visit_a, visit_b)

# Convert to daily number of visits: all_visits_day
all_visits_day <- map(all_visits, to_day)

# Map the mean() function and output a numeric vector 
map_dbl(all_visits_day, mean)


# You'll test out both map() and walk() for plotting
# Both return the "side effects," that is to say, the changes in the environment (drawing plots, downloading a file, changing the working directory...), but walk() won't print anything to the console.

# Create all_tests list  and modify with to_day() function
all_tests <- list(visit_a, visit_b, visit_c)
all_tests_day <- map(all_tests, to_day)

# Plot all_tests_day with map
map(all_tests_day, barplot)

# Plot all_tests_day
walk(all_tests_day, barplot)

# Get sum of all visits and class of sum_all
sum_all <- pmap(all_tests_day, sum)
class(sum_all)


# Turn visit_a into daily number using an anonymous function
map(visit_a, function(x) {
  x*24
})

# Turn visit_a into daily number of visits by using a mapper
map(visit_a, ~.x*24)

# Create a mapper object called to_day
to_day <- as_mapper(~.x*24)

# Use it on the three vectors
map(visit_a, to_day)
map(visit_b, to_day)
map(visit_c, to_day)


# Round visit_a to the nearest tenth with a mapper
map_dbl(visit_a, ~ round(.x, -1))

# Create to_ten, a mapper that rounds to the nearest tenth
to_ten <- as_mapper(~ round(.x, -1))

# Map to_ten on visit_b
map_dbl(visit_b, to_ten)

# Map to_ten on visit_c
map_dbl(visit_c, to_ten)


# Create a mapper that test if .x is more than 100 
is_more_than_hundred <- as_mapper(~ .x > 100)

# Run this mapper on the all_visits object
all_visits <- list(visit_a, visit_b, visit_c)
map(all_visits, ~ keep(.x, is_more_than_hundred) )

# Use the  day vector to set names to all_list
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
full_visits_named <- map(all_visits, ~ set_names(.x, day))

# Use this mapper with keep() 
map(full_visits_named, ~ keep(.x, is_more_than_hundred))


# Set the name of each subvector
day <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun")
all_visits_named <- map(all_visits, ~ set_names(.x, day))

# Create a mapper that will test if .x is over 100 
threshold <- as_mapper(~.x > 100)

# Run this mapper on the all_visits object: group_over
group_over <- map(all_visits, ~ keep(.x, threshold) )

# Run this mapper on the all_visits object: group_under
group_under <-  map(all_visits, ~ discard(.x, threshold) )


# Create a threshold variable, set it to 160
threshold <- 160

# Create a mapper that tests if .x is over the defined threshold
over_threshold <- as_mapper(~ .x > threshold)

# Are all elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ every(.x, over_threshold))

# Are some elements in every all_visits vectors over the defined threshold? 
map(all_visits, ~ some(.x, over_threshold))

```
  
  
  
***
  
Chapter 2 - Functional Programming from Theory to Practice  
  
Functional Programming in R:  
  
* Everything that exists is an object and everything that happens is a function call  
	* This means that a function is an object and can be treated as such  
    * Every action in R is performed by a function  
    * Functions are first-class citizens, and behave like any other object  
    * Functions can be manipulated, stored as variables, lambda (anonymous), stored in a list, arguments of a function, returned by a function  
    * R is a functional programming language  
* In a "pure function", output depends only on input, and there are no side-effects (no changes to the environment)  
	* Sys.Date() depends on the enviornment and is thus not pure  
    * write.csv() is called solely for the side effect (writing a file) and is thus not pure  
  
Tools for Functional Programming in purrr:  
  
* A high order function can take functions as input and return functions as output  
	* nop_na <- function(fun) {  
    *     function(...){fun(..., na.rm = TRUE)}  
    * }  
    * sd_no_na <- nop_na(sd)  
    * sd_no_na( c(NA, 1, 2, NA) )  
* There are three types of high-order functions  
	* Functionals take another function and return a vector - like map()  
    * Function fatories take a vector and create a function  
    * Function operators take functions and return functions - considered to be "adverbs"  
* Two of the most common adverbs in purrr are safely() and possibly()  
	* The safely() call returns a function that will return $result and $error when run; helpful for diagnosing issues with code rather than losing the information  
    * safe_log <- safely(log)  
    * safe_log("a")  # there will be $result of NULL and $error with the error code  
    * map( list(2, "a"), safely(log) )  
  
Using possibly():  
  
* The possibly() function is an adverb that returns either the value of the function OR the value specified in the otherwise element  
	* possible_sum <- possibly(sum, otherwise = "nop")  
    * possible_sum("a")  # result will be "nop"  
* Note that possibly() cannot be made to run a function; it will just return a pre-specified value  
  
Handling adverb results:  
  
* Can use transpose() to change the output (converts the list to inside out)  
	* Transpose turn a list of n elements a and b to a list of a and b, with each n elements  
* The compact() function will remove the NULL elements  
	* l <- list(1,2,3,"a")  
    * possible_log <- possibly(log, otherwise = NULL)  
    * map(l, possible_log) %>% compact()  
* Can use the httr package specifically for http requests  
	* httr::GET(url) will return the value from attempting to connect to url - 200 is good, 404 is unavailable, etc.  
  
Example code includes:  
```{r cache=TRUE}

# `$` is a function call, of a special type called 'infix operator', as they are put between two elements, and can be used without parenthesis.

# Launch Sys.time(), Sys.sleep(1), & Sys.time()
Sys.time()
Sys.sleep(1)
Sys.time()

# Launch nrow(iris), Sys.sleep(1), & nrow(iris)
data(iris)
nrow(iris)
Sys.sleep(1)
nrow(iris)

# Launch ls(), create an object, then rerun the ls() function
# ls()
# this <- 12
# ls()

# Create a plot of the iris dataset
plot(iris)


urls <- c('https://thinkr.fr', 'https://colinfay.me', 'http://not_working.org', 'https://datacamp.com', 'http://cran.r-project.org/', 'https://not_working_either.org')
# Create a safe version of read_lines()
safe_read <- safely(read_lines)

# Map it on the urls vector
res <- map(urls, safe_read)

# Set the name of the results to `urls`
named_res <-  set_names(res, urls)

# Extract only the "error" part of each sublist
map(named_res, "error")


# Code a function that discard() the NULL from safe_read()
safe_read_discard <- function(url){
    safe_read(url) %>%
        discard(is.null)
}

# Map this function on the url list
res <- map(urls, safe_read_discard)


# Create a possibly() version of read_lines()
possible_read <- possibly(read_lines, otherwise = 404)

# Map this function on urls, pipe it into set_names()
res <- map(urls, possible_read) %>% set_names(urls)

# Paste each element of the list 
res_pasted <- map(res, paste, collapse=" ")

# Keep only the elements which are equal to 404
keep(res_pasted, ~ .x == 404)


url_tester <- function(url_list){
    url_list %>%
        # Map a version of read_lines() that otherwise returns 404
        map( possibly(read_lines, otherwise = 404) ) %>%
        # Set the names of the result
        set_names( urls ) %>% 
        # paste() and collapse each element
        map(paste, collapse =" ") %>%
        # Remove the 404 
        discard(~.x==404) %>%
        names() # Will return the names of the good ones
}

# Try this function on the urls object
url_tester(urls)


url_tester <- function(url_list, type = c("result", "error")){
    res <- url_list %>%
        # Create a safely() version of read_lines() 
        map( safely(read_lines) ) %>%
        set_names( url_list ) %>%
        # Transpose into a list of $result and $error
        purrr::transpose() 
    # Complete this if statement
    if (type == "result") return( res$result ) 
    if (type == "error") return( res$error ) 
}

# Try this function on the urls object
url_tester(urls, type = "error") 


url_tester <- function(url_list){
    url_list %>%
        # Map a version of GET() that would otherwise return NULL 
        map( possibly(httr::GET, otherwise=NULL) ) %>%
        # Set the names of the result
        set_names( urls ) %>%
        # Remove the NULL
        compact() %>%
        # Extract all the "status_code" elements
        map("status_code")
}

# Try this function on the urls object
url_tester(urls)

```
  
  
  
***
  
Chapter 3 - Better Code with purrr  
  
Rationale for cleaner code:  
  
* Cleaner code is easier to debug (spot typos), easier to interpret, and easier to modify  
	* tidy_iris_lm <- compose( as_mapper(~ filter(.x, p.value < 0.05)), tidy, partial(lm, data=iris, na.action = na.fail) )  
    * list( Petal.Length ~ Petal.Width, Petal.Width ~ Sepal.Width, Sepal.Width ~ Sepal.Length ) %>% map(tidy_iris_lm)  
* Clean code characteristics  
	* Light - no unnecessary code  
    * Readable - less repition makes for easier reading (one piece of code for one task)  
    * Interpretable  
    * Maintainable  
* The compose() function is used to compose a function from two or more functions  
	* rounded_mean <- compose(round, mean)  
  
Building functions with compose() and negate():  
  
* There is a limitless amount of functions that can be included in compose()  
	* clean_aov <- compose(tidy, anova, lm)  
* Can use negate() to flip the predicate - TRUE becomes FALSE and FALSE becomes TRUE  
	* is_not_na <- negate(is.na)  
    * under_hundred <- as_mapper(~ mean(.x) < 100)  
    * not_under_hundred <- negate(under_hundred)  
    * map_lgl(98:102, under_hundred)  
    * map_lgl(98:102, not_under_hundred)  
* The "good" status return codes from GET() are in the low-200s  
	* good_status <- c(200, 201, 202, 203)  
    * status %in% good_status  
  
Prefilling functions:  
  
* The partial() allows for pre-filling a function  
	* mean_na_rm <- partial(mean, na.rm = TRUE)  
    * lm_iris <- partial(lm, data = iris)  
* Can also combine partial() and compose()  
	* rounded_mean <- compose( partial(round, digits = 2), partial(mean, na.rm = TRUE) )  
* Can use functions from rvest for web scraping  
	* read_html()  
    * html_nodes()  
    * html_text()  
    * html_attr()  
  
List columns:  
  
* A list column is part of a nested data frame - one or more of the data frame columns is itself a list (requires use of tibble rather than data.frame)  
	* df <- tibble( classic = c("a", "b","c"), list = list( c("a", "b","c"), c("a", "b","c", "d"), c("a", "b","c", "d", "e") ) )  
    * a_node <- partial(html_nodes, css = "a")  
    * href <- partial(html_attr, name = "href")  
    * get_links <- compose( href, a_node,  read_html )  
    * urls_df <- tibble( urls = c("https://thinkr.fr", "https://colinfay.me", "https://datacamp.com", "http://cran.r-project.org/") )  
    * urls_df %>% mutate(links = map(urls, get_links))  
* Can also unnest the data from the list columns  
	* urls_df %>% mutate(links = map(urls, get_links)) %>% unnest()  
* Can also nest() a standard data.frame  
	* iris_n <- iris %>% group_by(Species) %>% tidyr::nest()  
* Since the list column is a list, the purrr functions can be run on them  
	* iris_n %>% mutate(lm = map(data, ~ lm(Sepal.Length ~ Sepal.Width, data = .x)))  
    * summary_lm <- compose(summary, lm)  
    * iris %>% group_by(Species) %>% nest() %>% mutate(data = map(data, ~ summary_lm(Sepal.Length ~ Sepal.Width, data = .x)), data = map(data, "r.squared")) %>% unnest()  
  
Example code includes:  
```{r cache=TRUE}

urls <- c('https://thinkr.fr', 'https://colinfay.me', 'https://datacamp.com', 'http://cran.r-project.org/')

    
# Compose a status extractor 
status_extract <- purrr::compose(httr::status_code, httr::GET)

# Try with "https://thinkr.fr" & "http://datacamp.com"
status_extract("https://thinkr.fr")
status_extract("http://datacamp.com")

# Map it on the urls vector, return a vector of numbers
map_dbl(urls, status_extract)


# Negate the %in% function 
`%not_in%` <- negate(`%in%`)

# Complete the function
strict_code <- function(url){
    code <- status_extract(url)
    if (code %not_in% c(200:203)){
        return(NA)
    } else {
        return(code)
    } 
}


# Map the strict_code function on the urls vector
res <- map(urls, strict_code)

# Set the names of the results using the urls vector
res_named <- set_names(res, urls)

# Negate the is.na function
is_not_na <- negate(is.na)

# Run is_not_na on the results
is_not_na(res_named)


# Prefill html_nodes() with the css param set to h2
get_h2 <- partial(rvest::html_nodes, css="h2")

# Combine the html_text, get_h2 and read_html functions
get_content <- purrr::compose(rvest::html_text, get_h2, xml2::read_html)

# Map get_content to the urls list
res <- map(urls, get_content) %>%
    set_names(urls)

# Print the results to the console
res


# Create a partial version of html_nodes(), with the css param set to "a"
a_node <- partial(rvest::html_nodes, css="a")

# Create href(), a partial version of html_attr()
href <- partial(rvest::html_attr, name = "href")

# Combine href(), a_node(), and read_html()
get_links <- purrr::compose(href, a_node, xml2::read_html)

# Map get_links() to the urls list
res <- map(urls, get_links) %>%
    set_names(urls)


# Create a "links" columns, by mapping get_links() on urls
df2 <- tibble::tibble(urls=urls) %>%
    mutate(links = map(urls, get_links)) 

# Print df2 to see what it looks like
df2

# unnest() df2 to have a tidy dataframe
df2 %>%
    unnest()

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Discovering the Dataset:  
  
* The dataset is available from https://github.com/ThinkR-open/datasets  
	* rstudioconf: a list of 5055 tweets  
    * length(rstudioconf)  
    * length(rstudioconf[[1]])  
    * purrr::vec_depth(rstudioconf)  
* JSON is a standard data format for the web, and typically consists of key-value pairs which are read as nested lists by R  
* Refresher of keep() and discard() usage  
	* keep(1:10, ~ .x < 5)  
    * discard(1:10, ~ .x < 5)  
  
Extracting Information from the Dataset:  
  
* Can manipulate functions for list cleaning using high-order functions - includes partial() and compose()  
	* sum_no_na <- partial(sum, na.rm = TRUE)  
    * map_dbl(airquality, sum_no_na)  
    * rounded_sum <- compose(round, sum_no_na)  
    * map_dbl(airquality, rounded_sum)  
* Can also clean lists using compact() to remove NULL and flatten() to remove one level from a nested list  
	* l <- list(NULL, 1, 2, 3, NULL)  
    * compact(l)  
    * my_list <- list( list(a = 1), list(b = 2) )  
    * flatten(my_list)  
  
Manipulating URL:  
  
* Can use the mapper functions to create a re-usable function  
	* mult <- as_mapper(~ .x * 2)  
* Can use str_detect inside the mapper function  
	* lyrics <- c("Is this the real life?", "Is this just fantasy?", "Caught in a landslide", "No escape from reality")  
    * stringr::str_detect(a, "life")  
  
Identifying Influencers:  
  
* Can use the map_at() function to run a function at a specific portion of the list  
	* my_list <- list( a = 1:10, b = 1:100, c = 12 )  
    * map_at(.x = my_list, .at = "b", .f = sum)  
* Can also use negate() to reverse the actio of a predicate  
	* not_character <- negate(is.character)  
    * my_list <- list( a = 1:10, b = "a", c = iris )  
    * map(my_list, not_character)  
  
Wrap up:  
  
* Lambda functions and reusable mappers  
	* map(1:5, ~ .x*10)  
    * ten_times <- as_mapper(~ .x * 10)  
    * map(1:5, ten_times)  
* Function manipulation using functionals (functions that take functions as inputs and return vectors)  
	* map() & friends  
    * keep() & discard()  
    * some() & every()  
* Function operators take functions and return (modified) functions  
	* safely() & possibly()  
    * partial()  
    * compose()  
    * negate()  
* Cleaner code is easier to read, understand, and maintain  
	* rounded_mean <- compose( partial(round, digits = 1), partial(mean, trim = 2, na.rm = TRUE) )  
    * map( list(airquality, mtcars), ~ map_dbl(.x, rounded_mean) )  
  
Example code includes:  
```{r cache=TRUE}

rstudioconfDF <- readRDS("./RInputFiles/#RStudioConf.RDS")
rstudioconfListA <- split(rstudioconfDF, seq(nrow(rstudioconfDF)))
rstudioconf <- lapply(rstudioconfListA, FUN=as.list)


# Print the first element of the list to the console 
rstudioconf[[1]]

# Create a sublist of non-retweets
non_rt <- discard(rstudioconf, "is_retweet")

# Extract the favorite count element of each non_rt sublist
fav_count <- map_dbl(non_rt, "favorite_count")

# Get the median of favorite_count for non_rt
median(fav_count)


# Keep the RT, extract the user_id, remove the duplicate
rt <- keep(rstudioconf, "is_retweet") %>%
    map("user_id") %>%
    unique()

# Remove the RT, extract the user id, remove the duplicate
non_rt <- discard(rstudioconf, "is_retweet") %>%
    map("user_id") %>% 
    unique()

# Determine the total number of users
union(rt, non_rt) %>% 
    length()

# Determine the number of users who has just retweeted
setdiff(rt, non_rt) %>% 
    length()


# Prefill mean() with na.rm, and round() with digits = 1
mean_na_rm <- partial(mean, na.rm=TRUE)
round_one <- partial(round, digits=1)

# Compose a rounded_mean function
rounded_mean <- purrr::compose(round_one, mean_na_rm)

# Extract the non retweet  
non_rt <- discard(rstudioconf, "is_retweet")

# Extract "favorite_count", and pass it to rounded_mean()
map_dbl(non_rt, "favorite_count") %>%
    rounded_mean()


# Combine as_vector(), compact(), and flatten()
flatten_to_vector <- purrr::compose(as_vector, compact, flatten)

# Complete the fonction
extractor <- function(list, what = "mentions_screen_name"){
    map(list, what) %>%
        flatten_to_vector()
}

# Create six_most, with tail(), sort(), and table()
six_most <- purrr::compose(tail, sort, table)

# Run extractor() on rstudioconf
extractor(rstudioconf) %>% 
    six_most()


# Extract the "urls_url" elements, and flatten() the result
urls_clean <- map(rstudioconf, "urls_url") %>%
    lapply(FUN=function(x) { ifelse(is.na(x), list(NULL), x) }) %>%
    flatten()

# Remove the NULL
compact_urls <- compact(urls_clean)

# Create a mapper that detects the patten "github"
has_github <- as_mapper(~ str_detect(.x[1], "github"))

# Look for the "github" pattern, and sum the result
map_lgl( compact_urls, has_github ) %>%
    sum()


# Complete the function
ratio_pattern <- function(vec, pattern){
    n_pattern <- str_detect(vec, pattern) %>%
        sum()
    n_pattern / length(vec)
}

# Create flatten_and_compact()
flatten_and_compact <- purrr::compose(compact, flatten)

# Complete the pipe to get the ratio of URLs with "github"
map(rstudioconf, "urls_url") %>%
    lapply(FUN=function(x) { ifelse(is.na(x), list(NULL), x) }) %>%
    flatten_and_compact() %>% 
    ratio_pattern("github")


# Create mean_above, a mapper that tests if .x is over 3.3
mean_above <- as_mapper(~ . > 3.3)

# Prefil map_at() with "retweet_count", mean_above for above, 
# and mean_above negation for below
above <- partial(map_at, .at = "retweet_count", .f = mean_above )
below <- partial(map_at, .at = "retweet_count", .f = negate(mean_above) )

# Map above() and below() on non_rt, keep the "retweet_count"
# ab <- map(non_rt, above) %>% keep("retweet_count")
# bl <- map(non_rt, below) %>% keep("retweet_count")

rtCounts <- sapply(map(non_rt, "retweet_count"), FUN=function(x) { x })
ab <- rtCounts[rtCounts > 3.3]
bl <- rtCounts[rtCounts <= 3.3]

# Compare the size of both elements
length(ab)
length(bl)


# Get the max() of "retweet_count" 
max_rt <- map_dbl(non_rt, "retweet_count") %>% 
    max()

# Prefill map_at() with a mapper testing if .x equal max_rt
# max_rt_calc <- partial(map_at, .at = "retweet_count", .f = ~.x==max_rt )

idxMatch <- which(map(non_rt, "retweet_count") == max_rt)

# Map max_rt_calc on non_rt, keep the retweet_count & flatten
# res <- map(non_rt, max_rt_calc) %>% 
#     keep("retweet_count") %>% 
#     flatten()

# Print the "screen_name" and "text" of the result
res <- non_rt[[idxMatch]]
res$screen_name
res$text

```
  
  	
  
***
  
### _Longitudinal Analysis in R_  
  
Chapter 1 - Introduction to Longitudinal Data  
  
Introduction to Longitudinal Data:  
  
* Longitudinal data are data with 3+ measurements on the same unit (individual)  
	* Blood pressure every week for 6 weeks  
    * Match scores for a student from grade 3-8  
    * Extracurricular yes/no for each semester of high school  
* Dichotomous values are TRUE/FALSE while continuous have multiple values  
	* Multiple measurements allow for time-series modeling  
    * Two measurements allow for ANCOVA or t-tests  
* Example longitudinal data for rat weights from nlme  
	* library(nlme)  
    * head(BodyWeight, n = 10)  
    * count(BodyWeight, Rat)  
    * count(BodyWeight, Time)  
    * count(BodyWeight, Diet)  
  
Data Restructuring and Correlations:  
  
* Data are often stored in wide format, with each individual being a row  
* Analysis in R is generally best in long format, with time as one of the columns  
	* tidyr::gather() function for wide to long  
    * tidyr::spread() function for long to wide  
* Long to wide transforms can be helpful for calculating correlations  
	* BodyWeight %>% mutate(Time = paste0('Time_', Time)) %>% spread(Time, weight) %>% select(Rat, Diet, Time_1, Time_8, everything())  
* Wide to long transforms are more common  
	* gather(BodyWeight_wide, key = Time, value = weight, Time_1:Time_64)  # the colon operator : can be very helpful for these transforms  
* Can explore correlations over time and how the dependencies change over time  
* The corrr package can help with exploring correlations  
	* correlate(): to compute correlation matrix  
    * shave(): to remove extra information from matrix  
    * fashion(): to format correlation matrix  
    * BodyWeight %>% mutate(Time = paste0('T_', Time)) %>% spread(Time, weight) %>% select(Time_1, Time_8, Time_15:Time_64) %>% correlate() %>% shave(upper = FALSE) %>% fashion(decimals = 3)  
  
Descriptive Statistics:  
  
* Numeric summaries are often the most useful when broken down by time or other factors of interest  
* The group_by() and summarize() functions can be very helpful  
	* BodyWeight %>% group_by(Time) %>% summarize(mean_wgt = mean(weight, na.rm = TRUE), med_wgt = median(weight, na.rm = TRUE), min_wgt = min(weight, na.rm = TRUE), max_wgt = max(weight, na.rm = TRUE), sd_wgt = sd(weight, na.rm = TRUE), num_miss = sum(is.na(weight)), n = n())  
* Violin plots can be useful for assessing distributions at each point in time  
	* ggplot(BodyWeight, aes(x = factor(Time), y = weight)) + geom_violin(aes(fill = Diet)) + xlab("Time (in days)") + ylab("Weight") + theme_bw(base_size = 16)  
  
Example code includes:  
```{r}

data(calcium, package="lava")
str(calcium)


# Individuals with data at each visit number
count(calcium, visit)

# Individuals in each group
count(calcium, person)

# Individuals in each group
count(calcium, group)

# Individuals with each visit number in each group
count(calcium, visit, group)


# Restructure data into wide format for correlations
calcium_wide <- calcium %>%
    mutate(visit_char = paste0('visit_', visit)) %>%
    select(bmd, person, visit_char) %>%
    spread(visit_char, bmd)

# Calculate correlations across time
calcium_corr <- calcium_wide %>%
    select(-person) %>%
    corrr::correlate(method="pearson") %>%
    corrr::shave(upper=FALSE) %>%
    corrr::fashion(decimals=3)


# Convert data from wide to long format
calcium_wide %>% 
    gather(key="visit", value="bmd", -person)


# Calculate descriptive statistics
calcium %>%
    group_by(visit, group) %>%
    summarize(avg_bmd = mean(bmd, na.rm = TRUE), median_bmd = median(bmd, na.rm = TRUE),
              minimum_bmd = min(bmd, na.rm = TRUE), maximum_bmd = max(bmd, na.rm = TRUE),
              standev_bmd = sd(bmd, na.rm = TRUE), num_miss = sum(is.na(bmd)), n = n()
              )


# Visualize distributions of outcome over time
ggplot(calcium, aes(x = factor(visit), y = bmd)) + 
    geom_violin(aes(fill=group)) + 
    xlab("Visit Number") + 
    ylab("Bone Mineral Density") + 
    theme_bw(base_size = 16)

```
  
  
  
***
  
Chapter 2 - Modeling Continuous Longitudinal Outcomes  
  
Longitudinal Analysis for Continuous Outcomes:  
  
* Dependencies are introduced based on repeated observations of the same individual  
	* library(nlme)  
    * ggplot(BodyWeight, aes(x = Time, y = weight)) + geom_line(aes(group = Rat), alpha = 0.6) + geom_smooth(se = FALSE, size = 2) + theme_bw(base_size = 16) + xlab("Number of Days") + ylab("Weight (grams)")  # se is invalid due to the dependencies  
* The lmer stands for "Linear Mixed Effects Regression" - aka Hierarchical Linear Model, Linear Mixed Models, Multi-level Models, Growth Models  
	* lmer(outcome ~ fixed_effects + (random_effects | individual), data = data)  
    * outcome ~ fixed_effects + (random_effects | individual)  # outcome is the variable to be explained, fixed_effects are like a simple regression, random_effects represents deviation per individual, individual are the individual IDs  
* Example of model fitting with the rat data  
	* BodyWeight <- mutate(BodyWeight, Time = Time - 1)  
    * body_ri <- lmer(weight ~ 1 + Time + (1 | Rat), data = BodyWeight)  
    * summary(body_ri)  
* The fixed effects represent the average effects across all the rats  
  
Addition of Random Slope Terms:  
  
* Random slopes are commonly used along with random intercepts - each individual can have their own random slopes  
	* weight ~ 1 + Time + (1 + Time | Rat)  # fixed stays the same (though terms may be added)  
    * BodyWeight <- mutate(BodyWeight, Time = Time - 1)  
    * body_rs <- lmer(weight ~ 1 + Time + (1 + Time | Rat), data = BodyWeight)  
* Sometimes a question of whether random intercepts or random slopes fit the data better - can use anova()  
	* Akaike Information Criterion (AIC): smaller is better (recommended when the true model is unknown)  
    * Bayesian Information Criterion (BIC): smaller is better (imposes larger penalties for adding predictors)  
    * Log Likelihood: value minimized during estimation: smaller is better  
* Nested models can be compared using anova() - subset or simplification of another model  
	* body_ri <- lmer(weight ~ 1 + Time + (1 | Rat), data = BodyWeight)  
    * body_rs <- lmer(weight ~ 1 + Time + (1 + Time | Rat), data = BodyWeight)  
    * anova(body_rs, body_ri)  
  
Visualize and Interpret Output:  
  
* Visualizing model results is an important component of presenting and interpreting the data  
	* body_agg <- BodyWeight %>% mutate(pred_values = predict(body_ri))  
    * ggplot(body_agg, aes(x = Time, y = pred_values)) + geom_line(aes(group = Rats), alpha = 0.6) + theme_bw(base_size = 16) + xlab("Number of Days") + ylab("Model Implied Values")  
* Random effects help control for dependencies due to repeated measurements  
	* Custom function (next slide) will help explore model implied correlations  
* Example of creating and applying a custom function for correlations ("compound symmetry")  
	* corr_structure <- function(object, num_timepoints, intercept_only = TRUE) {  
    *     variance <- VarCorr(object)  
    *     if(intercept_only) {  
    *         random_matrix <- as.matrix(object@pp$X[1:num_timepoints, 1])  
    *         var_cor <- random_matrix %*% variance[[1]][1] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)  
    *     } else {  
    *     random_matrix <- as.matrix(object@pp$X[1:num_timepoints, ])  
    *     var_cor <- random_matrix %*% variance[[1]][1:2, 1:2] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)  
    *     }  
    *     Matrix::cov2cor(var_cor)  
    * }  
    * body_ri <- lmer(weight ~ 1 + Time + (1 | Rat), data = BodyWeight)  
    * corr_structure(body_ri, 11) %>% round(3)  
* Can visually show correlation dependencies  
	* Ggally::ggcorr(data = NULL, cor_matrix = corr_structure(body_ri, 11), label = TRUE, label_round = 3, label_size = 3.5, palette = 'Set2', nbreaks = 5)  
  
Example code includes:  
```{r}

# Visualize trajectories
ggplot(calcium, aes(x = visit, y = bmd)) +
    geom_line(aes(group = person), alpha = .4) +
    geom_smooth(se = FALSE, size = 2) +
    theme_bw(base_size = 14) +
    xlab('Visit Number') +
    ylab('Bone Mineral Density (g/cm^2)')


# Unconditional model
uncond_model <- lme4::lmer(bmd ~ 1 + visit + (1 | person), data = calcium)

# Show model output
summary(uncond_model)


# Alter the visit variable to start at 0
calcium <- calcium %>%
    mutate(visit_0 = visit - 1)

# Fit random intercept model with new time variable
uncond_model_0 <- lme4::lmer(bmd ~ 1 + visit_0 + (1 | person), data = calcium)
summary(uncond_model_0)


# Random slope
uncond_model_rs <- lme4::lmer(bmd ~ 1 + visit_0 + (1 + visit_0 | person), data = calcium)
summary(uncond_model_rs)


# Compare random slopes and random intercept only models
anova(uncond_model_rs, uncond_model_0)


# Create predicted values for random intercept only model
calcium_vis <- calcium %>%
    mutate(pred_values_ri = predict(uncond_model_0))
  
# Visualize random intercepts
ggplot(calcium_vis, aes(x = visit_0, y = pred_values_ri)) + 
    geom_line(size = 1, color = 'gray70', aes(group = person)) + 
    theme_bw() + 
    xlab("Visit Number") +
    ylab("Model Predicted Bone Mineral Density (g/cm^2)")


# Create predicted values for random intercept and slope model
calcium_vis <- calcium %>%
    mutate(pred_values_rs = predict(uncond_model_rs))

# Visualize random intercepts and slopes
ggplot(calcium_vis, aes(x = visit_0, y = pred_values_rs)) + 
    geom_line(size = 1, color = 'gray70', aes(group = person)) + 
    theme_bw() + 
    xlab("Visit Number") +
    ylab("Model Predicted Bone Mineral Density (g/cm^2)")


corr_structure <- function(object, num_timepoints, intercept_only = TRUE) {
    variance <- lme4::VarCorr(object)
    if(intercept_only) {
        random_matrix <- as.matrix(object@pp$X[1:num_timepoints, 1])
        var_cor <- random_matrix %*% variance[[1]][1] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)
    } else {
        random_matrix <- as.matrix(object@pp$X[1:num_timepoints, ])
        var_cor <- random_matrix %*% variance[[1]][1:2, 1:2] %*% t(random_matrix) + diag(attr(variance, "sc")^2, nrow = num_timepoints, ncol = num_timepoints)
    }
    Matrix::cov2cor(var_cor)
}

# Random intercept and slope model
random_slope <- lme4::lmer(bmd ~ 1 + visit_0 + (1 + visit_0 | person), data = calcium)

# Generate model implied correlation matrix
mod_corr <- corr_structure(random_slope, num_timepoints = 5, intercept_only = FALSE)
round(mod_corr, 3)

# Create visualization for correlation structure
GGally::ggcorr(data = NULL, cor_matrix = mod_corr, midpoint = NULL, 
               limits = NULL, label = TRUE, label_round = 3, label_size = 5, 
               nbreaks = 100, palette = 'PuBuGn'
               )

```
  
  
  
***
  
Chapter 3 - Add Fixed Predictor Variables  
  
Adding Predictors:  
  
* Predictors will build up the fixed effects to help explain the outcomes - reduces the variance of the intercepts/slopes  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + (1 + Time | Rat), data = BodyWeight)  
* Visualizing predictors can be especially helpful  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + (1 + Time | Rat), data = BodyWeight)  
    * bodyweight_agg <- BodyWeight %>% mutate(pred_values = predict(body_weight, re.form = NA)) %>% group_by(Time, Diet) %>% summarize(mean_diet_pred = mean(pred_values))  
    * ggplot(bodyweight_agg, aes(x = Time, y = mean_diet_pred, color = Diet)) + geom_point(data = BodyWeight, aes(x = Time, y = weight)) + geom_line(size = 2) + ylab("Body Weight") + xlab("Time (in days)") + theme_bw(base_size = 16)  
  
Adding Predictors - Interactions:  
  
* Can add predictor variables to the model  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight)  
* Need to check assumptions to ensure that model results are trustworthy  
	* Ensure model results are trustworthy  
    * Explore the distribution of residuals and random effects  
    * body_weight <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight)  
    * BodyWeight <- BodyWeight %>% mutate(model_residuals = residuals(body_weight))
ggplot(BodyWeight, aes(x = model_residuals)) + geom_density(aes(color = diet_f), size = 1.25) + xlab("Residuals") + theme_bw(base_size = 14) + scale_color_brewer(palette = "Set2")  
* Can use the ranef() function to extract random effects from the model  
	* body_weight <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight)  
    * random_effects <- ranef(body_weight)$Rat %>% mutate(id = 1:n()) %>% gather("variable", "value", -id) 
* Can also run Q-Q plots to asses normality  
	* ggplot(random_effects, aes(sample = value)) + geom_qq() + geom_qq_line() + facet_wrap(~variable, scales = 'free_y') + theme_bw(base_size = 14)  
  
Model Comparisons and Eplained Variance:  
  
* Model comparisons may be made using AIC, AICc (corrected AIC), or BIC  
	* AICc converges to AIC with large samples  
* Example of model comparisons  
	* BodyWeight <- BodyWeight %>% mutate(Time = Time - 1, diet_f = paste("Diet", Diet, sep = " "))  
    * body_weight_rs <- lmer(weight ~ 1 + Time + (1 + Time | Rat), data = BodyWeight, REML = FALSE)  # REML=TRUE would be for only changes in random effects; REML=FALSE is needed when changing fixed effects (random effects should stay the same)  
    * body_weight_diet <- lmer(weight ~ 1 + Time + diet_f + (1 + Time | Rat), data = BodyWeight, REML = FALSE)  
    * body_weight_diet_int <- lmer(weight ~ 1 + Time + diet_f + diet_f:Time + (1 + Time | Rat), data = BodyWeight, REML = FALSE)  
* Can calculate AICc from AICcmodavg  
	* AICcmodavg::aictab(list(body_weight_rs, body_weight_diet, body_weight_diet_int), modnames = c('random slope', 'diet intercept', 'diet interaction'))  
* Explained variance can also be helpful in model assessment - larger values are better  
	* MuMIn::r.squaredGLMM(body_weight_rs)  
    * MuMIn::r.squaredGLMM(body_weight_diet) 
    * MuMIn::r.squaredGLMM(body_weight_diet_int)  
  
Example code includes:  
```{r}

# Add a categorical predictor
bmd_group <- lme4::lmer(bmd ~ 1 + visit_0 + group + (1 + visit_0 | person), data = calcium)
summary(bmd_group)

# Add a continuous predictor
bmd_group_age <- lme4::lmer(bmd ~ 1 + visit_0 + group + age + (1 + visit_0 | person), data = calcium)
summary(bmd_group_age)


# Calculate aggregate trends
calcium_agg <- calcium %>%
    mutate(pred_values = predict(bmd_group_age, re.form = NA)) %>%
    group_by(visit_0, group) %>%
    summarize(pred_group = mean(pred_values))

# Visualize the model results
ggplot(calcium_agg, aes(x = visit_0, y = pred_group, color = group)) +
    geom_point(data = calcium, aes(x = visit_0, y = bmd, color = group)) +
    geom_line(size = 1.25) +
    xlab('Visit Number') +
    ylab('Model Predicted Bone Mineral Density (g/cm^2)')


# Add an interaction
bmd_group_age_int  <- lme4::lmer(bmd ~ 1 + visit_0 + age + group + visit_0:group + visit_0:age + (1 + visit_0 | person), data = calcium)
summary(bmd_group_age_int)


# Add residuals original data
calcium <- calcium %>%
    mutate(model_residuals = residuals(bmd_group_age_int))

# Visualize residuals
ggplot(calcium, aes(x = model_residuals)) + 
    geom_density(aes(color = group), size = 1.25) + 
    theme_bw(base_size = 14) + 
    xlab("Model Residuals")


# Extract random effects
random_effects <- lme4::ranef(bmd_group_age_int)$person %>%
    mutate(id = 1:n()) %>%
    gather("variable", "value", -id)

# Visualize random effects
ggplot(random_effects, aes(sample = value)) + 
    geom_qq() + 
    geom_qq_line() + 
    facet_wrap(~variable, scales = 'free_y') + 
    theme_bw(base_size = 14)


# Compare random slope, model with group variable, model with group and age, and model with interactions.
AICcmodavg::aictab(list(uncond_model_rs, bmd_group, bmd_group_age, bmd_group_age_int),
                   modnames = c('random slope', 'group intercept', 'group and age', 
                                'group and age interaction'
                                )
                   )


# Compute explained variance for random slope only model
MuMIn::r.squaredGLMM(uncond_model_rs)

# Compute explained variance for group and age predicting intercepts model
MuMIn::r.squaredGLMM(bmd_group_age)

# Compute explained variance for interaction model
MuMIn::r.squaredGLMM(bmd_group_age_int)

```
  
  
  
***
  
Chapter 4 - Modeling Longitudinal Dichotomous Outcomes  
  
Exploring and Modeling Dichotomous Outcomes:  
  
* Binary outcomes include yes/no, presence/absence, etc.  
	* library(HSAUR2)  
    * head(toenail, n = 10)  
* Generalized Linear Mixed Models (GLMM) explore the log-odds of success  
	* toenail <- toenail %>% mutate(outcome_dich = ifelse(outcome == "none or mild", 1, 0), visit_0 = visit - 1)  
    * toenail %>% group_by(visit_0) %>% summarise(prop_outcome = mean(outcome_dich), num = n())  
* Two modificiations to lmer to run GLMM  
	* use glmer instead of lmer  
    * specify family = binomial argument  
    * toe_output <- glmer(outcome_dich ~ 1 + visit_0 + treatment + ( 1 | patientID), data = toenail, family = binomial)  
    * summary(toe_output)  
  
Generalized Estimating Functions (GEE):  
  
* GEE is another way to estimate dichotomous data that are not continuous  
* The geepack package contains data for running GEE  
	* toenail <- toenail %>% mutate(outcome_dich = ifelse(outcome == "none or mild", 1, 0), visit_0 = visit - 1)  
    * gee_toe <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE)  
    * summary(gee_toe)  
* An optional argument, corstr is used to control the working correlation matrix  
	* Accounts for the dependency due to repeated measures  
    * The default is independence  
    * gee_toe <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, corstr = 'exchangeable', scale.fix = TRUE)  
    * summary(gee_toe)  
* Additional correlation matrices can be specified  
	* corstr = "ar1"  # specified correlation for each time lag  
    * corstr = "unstructured"  # unique correlation for each time lag  
  
Model Selection:  
  
* The working correlation matrix can significantly impact study results  
* The QIC statistic is the quasi-likelihood under the independence model criterion - lower (smaller) is better  
	* toenail <- toenail %>% mutate(outcome_dich = ifelse(outcome == "none or mild", 1, 0), visit_0 = visit - 1)  
    * gee_toe <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE)  
    * MuMIn::QIC(gee_toe)  
* Can evaluate model criterion  
	* gee_ind <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE)  
    * gee_exch <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE, corstr = 'exchangeable')  
    * gee_ar1 <- geeglm(outcome_dich ~ 1 + visit_0, data = toenail, id = patientID, family = binomial, scale.fix = TRUE, corstr = 'ar1')  
    * MuMIn::QIC(gee_ind, gee_exch, gee_ar1)  
* Can also use the aictab() from AICcmodavg  
	* toe_baseline <- glmer(outcome_dich ~ 1 + visit_0 + ( 1 | patientID), data = toenail, family = binomial)  
    * toe_output <- glmer(outcome_dich ~ 1 + visit_0 + treatment + ( 1 | patientID), data = toenail, family = binomial)  
    * AICcmodavg::aictab(list(toe_baseline, toe_output), c("no treatment", "treatement"))  
  
Interpreting and Visualizing Model Results:  
  
* Can visualize the GLMM outputs  
	* toe_output <- glmer(outcome_dich ~ 1 + visit_0 + treatment + ( 1 | patientID), data = toenail, family = binomial)  
    * toenail <- toenail %>% mutate(pred_values = predict(toe_output))  
    * ggplot(toenail, aes(x = visit_0, y = pred_values)) + geom_line(aes(group = patientID), linetype = 2) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Predicted Values")  
* Probabilities are often more intuitive and can be found using type="response"  
	* toenail <- toenail %>% mutate(pred_values = predict(toe_output, type = "response"))  
    * ggplot(toenail, aes(x = visit_0, y = pred_values)) + geom_line(aes(group = patientID), linetype = 2) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Prob of none or mild separation")  
* Can use predict() using GEE similar to GLMM  
	* gee_toe <- geeglm(outcome_dich ~ 1 + visit_0 + treatment, data = toenail, id = patientID, family = binomial, corstr = 'exchangeable', scale.fix = TRUE)  
    * toenail_gee <- toenail %>% mutate(pred_gee = predict(gee_toe, type = "response"))  
    * ggplot(toenail_gee, aes(x = visit_0, y = pred_gee)) + geom_line(aes(color = treatment)) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Probability of none or mild separation")  
* Compare GLMM and GEE  
	* toenail_glmm <- toenail %>% group_by(visit_0, treatment) %>% summarise(prob = mean(pred_values))  
    * toenail_gee <- toenail_gee %>% group_by(visit_0, treatment) %>% summarise(prob = mean(pred_values))  
    * toenail_agg = bind_rows( mutate(toenail_glmm, model = "GLMM"), mutate(toenail_gee, model = "GEE") )  
    * ggplot(toenail_agg, aes(x = visit_0, y = prob)) + geom_line(aes(color = treatment, linetype = model), size = 1) + theme_bw(base_size = 16) + xlab("Visit Number") + ylab("Prob of non or mild separation")  
  
Example code includes:  
```{r}

ids <- rep(c(1:82, 85:87, 90), each=12)
months <- rep(0:11, times=length(unique(ids)))
symps <- c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, NA, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, NA, NA, NA, NA, NA, NA, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0)
symps <- c(symps, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, NA, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, 0, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, NA, NA, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0)
age <- rep(c('Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', '20 or Older', 'Less than 20', 'Less than 20', '20 or Older', 'Less than 20', '20 or Older', 'Less than 20', 'Less than 20', 'Less than 20'), each=12)
sex <- rep(c('Male', 'Female', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male'), each=12)
madras <- data.frame(id=ids, symptom=symps, month=months, age=age, sex=sex)


# Explore the first few rows of the madras data
str(madras)
head(madras)

# Descriptives about symptom prevalence over time
summary_stats <- madras %>%
    group_by(month) %>%
    summarize(num_symptom = sum(symptom, na.rm = TRUE), 
              num = n(),
              prop_symptom = mean(symptom, na.rm = TRUE)
              )
  
# Print out summary statistics
summary_stats


# Build models
uncond_ri <- lme4::glmer(symptom ~ 1 + month + (1|id), data = madras, family = binomial)
summary(uncond_ri)


# Add in covariates based on trend plot
cond_model <- lme4::glmer(symptom ~ 1 + month + sex + age + sex:age + sex:month + (1 | id), data = madras, family = binomial)

# Generate summary of output
summary(cond_model)


# Fit a GEE model with intercept and time variable
gee_mod <- geepack::geeglm(symptom ~ 1 + month, id = id, family=binomial, data = madras, scale.fix = TRUE)

# Extract model results
summary(gee_mod)

# Fit a GEE model with an ar(1) working correlation matrix
gee_mod_ar1 <- geepack::geeglm(symptom ~ 1 + month, id = id, family = binomial, data = madras, corstr="ar1", scale.fix = TRUE)

# Fit a GEE model with an unstructured working correlation matrix
gee_mod_un <- geepack::geeglm(symptom ~ 1 + month, id = id, family = binomial, data = madras, corstr="unstructured", scale.fix = TRUE)

# Extract model results
summary(gee_mod_ar1)
summary(gee_mod_un)


# Fit a GEE model with an ar(1) working correlation matrix
gee_mod_ar1 <- geepack::geeglm(symptom ~ 1 + month + age + sex + age:sex + month:age + month:sex, id = id, data = madras, family = binomial, corstr = 'ar1', scale.fix = TRUE)

# Extract model results
summary(gee_mod_ar1)


# Fit a GEE model with an exchangeable working correlation matrix
gee_mod_exch <- geepack::geeglm(symptom ~ 1 + month + age + sex, data = madras, id = id, family = binomial, scale.fix = TRUE, corstr = 'exchangeable')


glmm_age <- lme4::glmer(symptom ~ 1 + month + age + month:age + (1 | id), data=madras, family=binomial)

# Generate model implied probabilities
madras <- madras %>%
    na.omit() %>%
    mutate(prob=predict(glmm_age, type="response"))

# Visualize subject specific probabilities
ggplot(madras, aes(x=month, y=prob)) + 
    geom_line(aes(group=id)) + 
    theme_bw() +
    xlab("Month") + 
    ylab("Probabilities")


# Compute the average trajectories
glmm_prob <- madras %>%
    group_by(month, age) %>%
    summarize(prob = mean(prob))

# Visualize average trajectories
ggplot(glmm_prob, aes(x = month, y = prob)) + 
    geom_line(aes(color = age)) + 
    theme_bw() +
    xlab("Month") + 
    ylab("Probability")


gee_age_sex <- geepack::geeglm(formula = symptom ~ 1 + month + age + month:age + sex, 
                               family = binomial, data = madras, id = id, corstr = "ar1", scale.fix = TRUE
                               )

# Compute model implied probabilites using gee_age_sex
madras_gee <- madras %>% 
    select(month, symptom, age, sex) %>% 
    na.omit() %>%
    mutate(prob = predict(gee_age_sex, type = "response"))

# Visualize trajectories
ggplot(madras_gee, aes(x = month, y = prob)) + 
    geom_line(aes(color = age)) + 
    facet_wrap(~ sex) + 
    theme_bw() +
    xlab("Month") +
    ylab("Probability")


# Fit a GLMM mdoel
glmm_age_sex <- lme4::glmer(symptom ~ 1 + month + age + sex + (1 | id), data = madras, family = binomial)

# Generate model implied probabilites
madras <- madras %>% mutate(prob_gee = predict(gee_age_sex, type = "response"),
                            prob_glmm = predict(glmm_age_sex, type = "response")
                            )

# Compute average GEE probabilities
madras_gee <- madras %>%
    group_by(month, age, sex) %>%
    summarize(prob = mean(prob_gee))

# Compute average GLMM probabilities
madras_glmm <- madras %>%
    group_by(month, age, sex) %>%
    summarize(prob = mean(prob_glmm))

# Create combined data object
madras_agg = bind_rows(
    mutate(madras_glmm, model = "GLMM"), 
    mutate(madras_gee, model = "GEE") 
)


# Visualize differences in trajectories across model types
ggplot(madras_agg, aes(x = month, y = prob)) + 
    geom_line(aes(color = sex, linetype = model)) + 
    facet_wrap(~ age) + 
    theme_bw() + 
    xlab("Month") + 
    ylab("Probability")

```
  
  
  
***
  
### _Data Manipulation in R with data.table_  
  
Chapter 1 - Introduction to data.table  
  
Introduction:  
  
* A data.table is a data.frame with extended capabilities  
	* DT[i, j, by]  # which rows, what to do (operates on columns), grouped by what  
* The data.table is fast and parallelizes operations where possible, and is feature-rich  
* There are at least three ways to create a data.table  
	* data.table()  
    * as.data.table()  
    * fread()  
    * x <- data.table(id = 1:2, name = c("a", "b"))  
* All functions that can be used on data.frame are available on data.table  
	* Note that data.table does not convert characters to factors, does not set row names, and uses colon to separate row number from the row data  
  
Filtering Rows in a data.table:  
  
* The data.table syntax consists of DT[i, j, by]  # which rows, what to do (operates on columns), grouped by what  
	* batrips[3:4]  
    * batrips[3:4, ]  # same as above  
    * batrips[-(1:5)]  # exclude rows 1:5  
    * batrips[!(1:5)]  # exclude rows 1:5  
* The data.table has some special symbols that help with calculations  
	* .N is an integer value that contains the number of rows in the data.table (Particularly useful alternative to nrow(x) in i)  
    * batrips[.N]  
    * ans <- batrips[1:(.N-10)]  
    * batrips[subscription_type == "Subscriber"]  # if this is a data.table  
    * batrips[batrips$subscription_type == "Subscriber", ]  # required if it is only a data.frame  
    * batrips[start_terminal == 58 & end_terminal != 65]  # if this is a data.table  
* The data.table automatically creates a key of the columns used to subset the data (future operations on that column will be much faster)  
	* dt <- data.table(x = sample(10000, 10e6, TRUE), y = sample(letters, 1e6, TRUE))  
    * indices(dt)  
    * system.time(dt[x == 900])  
    * indices(dt)  
    * system.time(dt[x == 900])  
  
Helpers for filtering:  
  
* The %like% operator allows for matching a pattern in a column  
	* batrips[start_station %like% "^San Francisco"]  # for data.table, with the ^ being from regex for "starts with"  
    * batrips[grepl("^San Francisco", start_station)]  # equivalent for data.frame  
* The %between% operator allows for finding items in the closed interval (a, b)  
	* batrips[duration %between% c(2000, 3000)]  # using data.table  
    * batrips[duration >= 2000 & duration <= 3000]  # equivalent for data.frame  
* The %chin% operator allows for %in% for character vectors  
	* batrips[start_station %chin% c("Japantown", "Mezes Park", "MLK Library")]  # for data.table, runs MUCH faster  
    * batrips[start_station %in% c("Japantown", "Mezes Park", "MLK Library")]  # alternate, slower syntax  
  
Example code includes:  
```{r}

# Load data.table
library(data.table)

# Create the data.table X 
X <- data.table(id = c("a", "b", "c"), value = c(0.5, 1.0, 1.5))

# View X
X


data(batrips, package="bikeshare14")
batrips <- as.data.table(batrips)

# Get number of columns in batrips
col_number <- ncol(batrips)

# Print the first 8 rows
head(batrips, 8)

# Print the last 8 rows
tail(batrips, 8)

# Print the structure of batrips
str(batrips)


# Filter third row
row_3 <- batrips[3]
row_3

# Filter rows 10 through 20
rows_10_20 <- batrips[10:20]
rows_10_20

# Filter the 1st, 6th and 10th rows
rows_1_6_10 <- batrips[c(1, 6, 10)]
rows_1_6_10


# Select all rows except the first two
not_first_two <- batrips[-c(1:2)]
not_first_two

# Select all rows except 1 through 5 and 10 through 15
exclude_some <- batrips[-c(1:5, 10:15)]
exclude_some

# Select all rows except the first and last
not_first_last <- batrips[-c(1, .N)]
not_first_last


# Filter all rows where start_station is "MLK Library"
trips_mlk <- batrips[start_station == "MLK Library"]
trips_mlk

# Filter all rows where start_station is "MLK Library" AND duration > 1600
trips_mlk_1600 <- batrips[start_station == "MLK Library" & duration > 1600]
trips_mlk_1600

# Filter all rows where `subscription_type` is not `"Subscriber"`
customers <- batrips[subscription_type != "Subscriber"]
customers

# Filter all rows where start_station is "Ryland Park" AND subscription_type is not "Customer"
ryland_park_subscribers <- batrips[start_station=="Ryland Park" & subscription_type != "Customer"]
ryland_park_subscribers


# Filter all rows where end_station contains "Market"
any_markets <- batrips[end_station %like% "Market"]
any_markets

# Filter all rows where end_station ends with "Market" 
end_markets <- batrips[end_station %like% "Market$"]
end_markets


# Filter all rows where trip_id is 588841, 139560, or 139562
filter_trip_ids <- batrips[trip_id %in% c(588841, 139560, 139562)]
filter_trip_ids


# Filter all rows where duration is between [5000, 6000]
duration_5k_6k <- batrips[duration %between% c(5000, 6000)]
duration_5k_6k

# Filter all rows with specific start stations
two_stations <- batrips[start_station %chin% c("San Francisco City Hall", "Embarcadero at Sansome")]
two_stations

```
  
  
  
***
  
Chapter 2 - Selecting and Computing on Columns  
  
Selecting columns from a data.table:  
  
* The data.table syntax consists of DT[i, j, by]  # which rows, what to do (operates on columns), grouped by what  
* The j argument can be passed a vector of column names or column numbers  
	* ans <- batrips[, "trip_id"]  # will still be a data.table (no need for drop=FALSE)  
    * ans <- batrips[, c(2, 4)]  # not recommended, since column numbers may change over time  
    * ans <- batrips[, -c("start_date", "end_date", "end_station")]  # excludes these columns  
* The data.table way allows for computations on columns as part of the j column  
	* ans <- batrips[, list(trip_id, dur = duration)]  # duration will be renamed to dur while selecting  
    * ans <- batrips[, list(trip_id)]  # will return a data.table since it is inside ()  
* The .() is an alias to list()  
	* ans <- batrips[, .(trip_id, duration)]  # select columns trip_id, duration  
    * ans <- batrips[, list(trip_id, duration)]  # same as above  
* To get the return of a vector (like drop=TRUE in data.frame), pass the column name as an unquoted variable, not inside list() and not inside .()  
	* batrips[, duration]  # will return a vector  
  
Computing on columns the data.table way:  
  
* Can perform computations directly on columns in j  
	* ans <- batrips[, mean(duration)]  # will return a single value, the mean of duration (will be a vector)  
* Can run computations for only a subset of rows by combining I and j  
	* batrips[start_station == "Japantown", mean(duration)]  # row subsetting happens PRIOR to mean() calculation  
* Can use .N as part of the j argument  
	* batrips[start_station == "Japantown", .N]  # number of trips starting from Japantown  
  
Advanced computations in j:  
  
* Can compute in j and return a data.table  
	* ans <- batrips[, .(trip_id, dur = duration)]  
    * batrips[, .(mn_dur = mean(duration), med_dur = median(duration))]  
* Can combine with I so that only a subset of rows have the calculations in j selected on them  
	* batrips[start_station == "Japantown", .(mn_dur = mean(duration), med_dur = median(duration))]  
  
Example code includes:  
```{r}

# Select bike_id and trip_id using a character vector
df_way <- batrips[, c("bike_id", "trip_id")]
df_way

# Select start_station and end_station cols without a character vector
dt_way <- batrips[, .(start_station, end_station)]
dt_way


# You can also drop or deselect columns by prepending the character vector of column names with the - or ! Operators
# For e.g., dt[, -c("col1", "col2")] or dt[, !c("col1", "col2")] would both return all columns except col1 and col2

# Deselect start_terminal and end_terminal columns
drop_terminal_cols <- batrips[, -c("start_terminal", "end_terminal")]
drop_terminal_cols


# Calculate median duration using the j argument
median_duration <- batrips[, mean(duration)]
median_duration

# Get median duration after filtering
median_duration_filter <- batrips[end_station == "Market at 10th" & subscription_type == "Subscriber", median(duration)]
median_duration_filter


# Compute duration of all trips
trip_duration <- batrips[, difftime(end_date, start_date, units="min")]
head(trip_duration)


# Have the column mean_durn
mean_duration <- batrips[, .(mean_durn=mean(duration))]
mean_duration

# Get the min and max duration values
min_max_duration <- batrips[, .(min(duration), max(duration))]
min_max_duration

# Calculate the number of unique values
other_stats <- batrips[, .(mean_duration=mean(duration), last_ride=max(end_date))]
other_stats


duration_stats <- batrips[start_station == "Townsend at 7th" & duration < 500, 
                          .(min_dur=min(duration), max_dur=max(duration))]
duration_stats

# Plot the histogram of duration based on conditions
batrips[start_station == "Townsend at 7th" & duration < 500, hist(duration)]

```
  
  
  
***
  
Chapter 3 - Groupwise Operations  
  
Computations by Groups:  
  
* The by argument allows for separate computations by each group number  
	* ans <- batrips[, .N, by = "start_station"]  
    * ans <- batrips[, .N, by = .(start_station)]  # can use .() or list() instead of the character vector  
* Can rename columns on the fly, including calculations and group by  
	* ans <- batrips[, .(no_trips = .N), by = .(start = start_station)]  
* Can calculate grouping variables on the fly as part of the by expression  
	* ans <- batrips[ , .N, by = .(start_station, mon = month(start_date))]  
  
Chaining data.table expressions:  
  
* Chaining expressions is the process of running multiple operations to get a single output  
	* batrips[duration > 360][order(duration)][1:3]  
    * batrips[, .(mn_dur = mean(duration)), by = "start_station"][order(mn_dur)][1:3]  
* The uniqueN() is a helper function that returns the number of unique objects  
	* ans <- batrips[, uniqueN(bike_id), by = month(start_date)]  
  
Computations in j using .SD:  
  
* The .SD means "subset of data" which simplifies calculations  
	* x[, print(.SD), by = id]  # each of the groups is a data.table, which is the power of the .SD helper  
    * x[, .SD[1], by = id]  # returns the first row for each group  
* Can use .SDcols to select just a subset of the columns for return  
	* batrips[, .SD[1], by = start_station]  
    * batrips[, .SD[1], by = start_station, .SDcols = c("trip_id", "duration")]  
    * batrips[, .SD[1], by = start_station, .SDcols = - c("trip_id", "duration")]  
  
Example code includes:  
```{r}

# Compute the mean duration for every start_station
mean_start_stn <- batrips[, .(mean_duration=mean(duration)), by = "start_station"]
mean_start_stn


# Compute the mean duration for every start and end station
mean_station <- batrips[, .(mean_duration=mean(duration)), by = .(start_station, end_station)]
mean_station

# Compute the mean duration grouped by start_station and month
mean_start_station <- batrips[, .(mean_duration=mean(duration)), by=.(start_station, month(start_date))]
mean_start_station


# Compute mean of duration and total trips grouped by start and end stations
aggregate_mean_trips <- batrips[, .(mean_duration=mean(duration), total_trips=.N), by=.(start_station, end_station)]
aggregate_mean_trips

# Compute min and max duration grouped by start station, end station, and month
aggregate_min_max <- batrips[, .(min_duration=min(duration), max_duration=max(duration)), by=.(start_station, end_station, month(start_date))]
aggregate_min_max


# Arrange the total trips grouped by start_station and end_station in decreasing order
trips_dec <- batrips[, .N, by = .(start_station, end_station)][order(-N)]
trips_dec


# Top five most popular destinations
top_5 <- batrips[, .N, by = .(end_station)][order(-N)][1:5]
top_5


# Compute most popular end station for every start station
popular_end_station <- trips_dec[, .(end_station = head(end_station, 1)), by = .(start_station)]
popular_end_station


# Find the first and last ride for each start_station
first_last <- batrips[order(start_date), 
                      .(start_date = c(head(start_date, 1), tail(start_date, 1))), 
                      by = .(start_station)]
first_last


relevant_cols <- c("start_station", "end_station", "start_date", "end_date", "duration")

# Find the row corresponding to the shortest trip per month
shortest <- batrips[, .SD[which.min(duration)], by = month(start_date), .SDcols = relevant_cols]
shortest


# Find the total number of unique start stations and zip codes per month
unique_station_month <- batrips[, lapply(.SD, FUN=uniqueN), 
                                by = month(start_date), 
                                .SDcols = c("start_station", "zip_code")]
unique_station_month

```
  
  
  
***
  
Chapter 4 - Reference Semantics  
  
Adding and Updating Columns by Reference:  
  
* Can add, delete, and update columns in place  
	* df <- data.frame(x = 1:5, y = 6:10)  
    * df <- data.frame(a = 1:3, b = 4:6, c = 7:9, d = 10:12)  
    * df[1:2] <- lapply(df[1:2], function(x) ifelse(x%%2, x, NA))  
* The data.table internals are such that neither the whole frame nor entire columns are deep copied simply to update a few values  
	* data.table updates columns in place, i.e., by reference  
    * This means, you don't need the assign the result back to a variable  
    * No copy of any column is made while their values are changed  
    * data.table uses a new operator := to add/update/delete columns by reference  
* Two ways to use the := operator  
	* batrips[, ("is_dur_gt_1hour", "week_day") := list(duration > 3600, wday(start_date)]  
    * batrips[, is_dur_gt_1hour := duration > 3600]  # can skip the parentheses and quotes if there is just a single variable for processing  
    * batrips[, `:=`(is_dur_gt_1hour = NULL, start_station = toupper(start_station))]  # note that `:=` is the function that is being called and that NULL means "delete the column"  
  
Grouped Aggregations:  
  
* Can run grouped aggregations by combining := and by  
	* batrips[, n_zip_code := .N, by = zip_code]  # nothing will be printed to the console, though number of columns increased by 1  
    * zip_1000 <- batrips[n_zip_code > 1000][, n_zip_code := NULL]  # can delete the intermediate columns by setting them to NULL  
  
Advanced Aggregations:  
  
* Can add multiple columns by reference, each within a specified by group  
	* batrips[, `:=`(end_dur_first = duration[1], end_dur_last = duration[.N]), by = end_station]  
    * batrips[, c("end_dur_first", "end_dur_last") := list(duration[1], duration[.N]), by = end_station]  
* Can use ifelse statements, and the j argument can have multi-line expressions with LHS=RHS wrapped inside {}  
	* batrips[, trip_category := { med_dur = median(duration, na.rm = TRUE) if (med_dur < 600) "short" else if (med_dur >= 600 & med_dur <= 1800) "medium" else "long" }, by = .(start_station, end_station)]  
* Can also apply user-defined functions to achieve the same tasks  
	* bin_median_duration <- function(dur) {  
    *     med_dur <- median(dur, na.rm = TRUE)  
    *     if (med_dur < 600) "short"  
    * else if (med_dur >= 600 & med_dur <= 1800) "medium"  
    * else "long"  
    * }  
    * batrips[, trip_category := bin_median_duration(duration), by = .(start_station, end_station)]  
* Can combine I, j, by all for a single operation  
	* batrips[duration > 500, min_dur_gt_500 := min(duration), by = .(start_station, end_station)]  
  
Example code includes:  
```{r}

data(batrips, package="bikeshare14")
batrips <- as.data.table(batrips)

batrips_new = batrips
makeNA <- sample(1:nrow(batrips), round(0.05*nrow(batrips)), replace=FALSE)
batrips_new[makeNA, "duration"] <- NA


# Add a new column, duration_hour
batrips[, duration_hour := duration/3600]


# Print untidy
# untidy[1:2]

# Fix spelling in the second row of start_station
# untidy[2, start_station:="San Francisco City Hall"]

# Replace negative duration values with NA
# untidy[duration < 0, duration:=NA]


# Add a new column equal to total trips for every start station
batrips[, trips_N:=.N, by = start_station]

# Add new column for every start_station and end_station
batrips[, duration_mean:=mean(duration), by = .(start_station, end_station)]


# Calculate the mean duration for each month
batrips_new[, mean_dur:=mean(duration, na.rm=TRUE), by = month(start_date)]

# Replace NA values in duration with the mean value of duration for that month
batrips_new[, mean_dur := mean(duration, na.rm = TRUE), by = month(start_date)][is.na(duration), duration:=round(mean_dur,0)]

# Delete the mean_dur column by reference
batrips_new[, mean_dur := mean(duration, na.rm = TRUE), by = month(start_date)][is.na(duration), duration := mean_dur][, mean_dur:=NULL]


# Add columns using the LHS := RHS form
batrips[, c("mean_duration", "median_duration"):=.(mean(duration), as.integer(round(median(duration), 0))), by=start_station]

# Add columns using the functional form
batrips[, `:=`(mean_duration=mean(duration), median_duration=as.integer(round(median(duration), 0))), by = start_station]

# Add the mean_duration column
batrips[duration > 600, mean_duration:=mean(duration), by=.(start_station, end_station)]

```
  
  
  
***
  
Chapter 5 - Importing and Exporting Data  
  
Fast data reading with fread():  
  
* The fread() function is a fast flat-file reader since it imports files in parallel (default is to use all available threads)  
	* Can import local files, files from the web, and strings  
    * Intelligent defaults - colClasses, sep, nrows etc.  
    * Note: Dates and Datetimnes are read as character columns but can be converted later with the excellent fasttime or anytime packages  
    * DT1 <- fread("https://bit.ly/2RkBXhV")  
    * DT2 <- fread("data.csv")  
    * DT3 <- fread("a,b\n1,2\n3,4")  
    * DT4 <- fread("1,2\n3,4")  
* The nrows and skip arguments can be helpful for finer control  
	* fread("a,b\n1,2\n3,4", nrows = 1)  # nrows=1 means read 1 row in addition to the header  
    * str <- "# Metadata\nTimestamp: 2018-05-01 19:44:28 GMT\na,b\n1,2\n3,4"  
    * fread(str, skip = 2)  # skips the first two lines entirely before attempting to parse the file  
* Can also pass a string argument to skip  
	* str <- "# Metadata\nTimestamp: 2018-05-01 19:44:28 GMT\na,b\n1,2\n3,4"  
    * fread(str, skip = "a,b")  # everything before the line "a,b" will be skipped  
* The select and drop arguments allow for control of columns to read  
	* str <- "a,b,c\n1,2,x\n3,4,y"  
    * fread(str, select = c("a", "c"))  
    * fread(str, drop = "b")  # same as above for this chunk of data  
    * str <- "1,2,x\n3,4,y"  
    * fread(str, select = c(1, 3))  
    * fread(str, drop = 2)  # same as above for this chunk of data  
  
Advanced file reading:  
  
* By default, R can only read integers less than 2**31 - 1  
	* Large integers are automatically read in as integer64 type, provided by the bit64 package  
* Can override the colClasses() guessing that is otherwise automatic in fread()  
	* str <- "x1,x2,x3,x4,x5\n1,2,1.5,true,cc\n3,4,2.5,false,ff"  
    * ans <- fread(str, colClasses = c(x5 = "factor"))  
    * ans <- fread(str, colClasses = c("integer", "integer", "numeric", "logical", "factor"))  
    * str <- "x1,x2,x3,x4,x5,x6\n1,2,1.5,2.5,aa,bb\n3,4,5.5,6.5,cc,dd"  
    * ans <- fread(str, colClasses = list(numeric = 1:4, factor = c("x5", "x6"))) str(ans)  # specifies that columns 1:4 are numeric and that "x5" and "x6" will be factors  
* The fill argument can be used to direct fread() to fill missing values  
	* str <- "1,2\n3,4,a\n5,6\n7,8,b"  
    * fread(str)  # throws a warning since fill=FALSE is the default  
    * fread(str, fill = TRUE)  # will assume empty strings for the missing fields  
* Can override the defaults for what to treat as NA  
	* str <- "x,y,z\n1,###,3\n2,4,###\n#N/A,7,9"  
    * ans <- fread(str, na.strings = c("###", "#N/A"))  
  
Fast data writing with fwrite():  
  
* The fwrite() function is a fast parallel flat file writer  
	* dt <- data.table(id = c("x", "y", "z"), val = list(1:2, 3:4, 5:6))  
    * fwrite(dt, "fwrite.csv")  # the list is flattened by using the secondary separtor (default is |)  
    * fread("fwrite.csv") 
* Dates and datetimes are saved in ISO format for further clarity  
	* fwrite() provides three additional ways of writing date and datetime format - ISO, squash and epoch  
    * Encourages the use of ISO standards with ISO as default  
    * now <- Sys.time()  
    * dt <- data.table(date = as.IDate(now), time = as.ITime(now), datetime = now)  
    * fwrite(dt, "datetime.csv", dateTimeAs = "ISO")  
    * fread("datetime.csv")  
* Additional date and time formats available - squash  
	* squash writes yyyy-mm-dd hh:mm:ss as yyyymmddhhmmss, for example.  
    * Read in as integer. Very useful to extract month, year etc by simply using modulo arithmetic. e.g., 20160912 %/% 10000 = 2016  
    * Also handles milliseconds (ms) resolution.  
    * POSIXct type (17 digits with ms resolution) is automatically read in as integer64 by fread  
* Additional date and time formats available - epoch  
	* epoch counts the number of days (for dates) or seconds (for time and datetime) since relevant epoch  
    * Relevant epoch is 1970-01-01, 00:00:00 and 1970-01-01T00:00:00Z for date, time and datetime, respectively  
    * fwrite(dt, "datetime.csv", dateTimeAs = "epoch")  
    * fread("datetime.csv")  
  
Example code includes:  
```{r cache=TRUE}

data(batrips, package="bikeshare14")
batrips <- as.data.table(batrips)
readr::write_csv(batrips, "./RInputFiles/_batrips.csv")


# Use read.csv() to import batrips
system.time(read.csv("./RInputFiles/_batrips.csv"))

# Use fread() to import batrips
system.time(fread("./RInputFiles/_batrips.csv"))


cat('id,"name",val
29192,"Robert Whitaker", 200
49301 ,"Elisa Waters",190
', file="./RInputFiles/_sample.csv")


# Import using read.csv()
csv_file <- read.csv("./RInputFiles/_sample.csv", fill = NA, quote = "", stringsAsFactors = FALSE, strip.white = TRUE, header = TRUE)
csv_file

# Import using fread()
csv_file <- fread("./RInputFiles/_sample.csv")
csv_file


cat("id,name,val
29192,Robert Whitaker, 200
49301 ,Elisa Waters,190  
34456 , Karla Schmidt,458
", file="./RInputFiles/_sample.csv")


# Select "id" and "val" columns
select_columns <- fread("./RInputFiles/_sample.csv", select=c("id", "val"))
select_columns

# Drop the "val" column
drop_column <- fread("./RInputFiles/_sample.csv", drop=c("val"))
drop_column


cat('id,"name",val
29192,"Robert Whitaker", 200
49301 , Elisa Waters,190  
34456 , Karla Schmidt,458  

END-OF-DATA
METADATA
attr;value
date;"2018-01-01"
data;"cash payment" 
', file="./RInputFiles/_sample.csv")


# Import the file
entire_file <- fread("./RInputFiles/_sample.csv")
entire_file

# Import the file while avoiding the warning
only_data <- fread("./RInputFiles/_sample.csv", nrows=3)
only_data

# Import only the metadata
only_metadata <- fread("./RInputFiles/_sample.csv", skip="attr;value")
only_metadata


cat('id,name,val
9002019291929192,Robert Whitaker, 200
9200129401349301 ,Elisa Waters,190  
9200149429834456 , Karla Schmidt,458
', file="./RInputFiles/_sample.csv")


# Import the file using fread 
fread_import <- fread("./RInputFiles/_sample.csv")

# Import the file using read.csv 
base_import <- read.csv("./RInputFiles/_sample.csv")

# Check the class of id column
class(fread_import$id)
class(base_import$id)


cat('c1,c2,c3,c3.1,c5,n1,n2,n3,n4,n5
aa,bb,cc,dd,ee,1,2,3,4,5
ff,gg,hh,ii,jj,6,7,8,9,10
', file="./RInputFiles/_sample.csv")


# Import using read.csv with defaults
base_r_defaults <- read.csv("./RInputFiles/_sample.csv")
str(base_r_defaults)

# Import using read.csv
base_r <- read.csv("./RInputFiles/_sample.csv", 
                   colClasses = c(rep("factor", 4), "character", "integer", rep("numeric", 4))
                   )
str(base_r)

# Import using fread
import_fread <- fread("./RInputFiles/_sample.csv", colClasses = list(factor=1:4, numeric=7:10))
str(import_fread)


cat('id,name,val
9002019291929192,Robert Whitaker,
9200129401349301 ,Elisa Waters,190  
9200149429834456 , Karla Schmidt
', file="./RInputFiles/_sample.csv")


# Import the file and note the warning message
incorrect <- fread("./RInputFiles/_sample.csv")
incorrect

# Import the file correctly
correct <- fread("./RInputFiles/_sample.csv", fill=TRUE)
correct


# Import the file using na.strings
missing_values <- fread("./RInputFiles/_sample.csv", na.strings="##")
missing_values


dt <- data.table(id=c(29192L, 49301L, 34456L), 
                 name=c("Robert, Whitaker", "Elisa, Waters", "Karla, Schmidt"), 
                 vals=list(c(144, 48, 32), c(22, 289), 458)
                 )
dt


# Write dt to fwrite.txt
fwrite(dt, "./RInputFiles/_fwrite.txt")

# Import the file using readLines()
readLines("./RInputFiles/_fwrite.txt")

# Import the file using fread()
fread("./RInputFiles/_fwrite.txt")


batrips_dates <- batrips[1:5, c("start_date", "end_date")]
batrips_dates


# Write batrips_dates to file using "ISO" format
fwrite(batrips_dates, "./RInputFiles/_iso.txt", dateTimeAs="ISO")

# Import the file back
iso <- fread("./RInputFiles/_iso.txt")
iso

# Write batrips_dates to file using "squash" format
fwrite(batrips_dates, "./RInputFiles/_squash.txt", dateTimeAs="squash")

# Import the file back
squash <- fread("./RInputFiles/_squash.txt")
squash

# Write batrips_dates to file using "epoch" format
fwrite(batrips_dates, "./RInputFiles/_epoch.txt", dateTimeAs="epoch")

# Import the file back
epoch <- fread("./RInputFiles/_epoch.txt")
epoch


# Use write.table() to write batrips
system.time(write.table(batrips, "./RInputFiles/_base-r.txt"))

# Use fwrite() to write batrips
system.time(fwrite(batrips, "./RInputFiles/_data-table.txt"))

```
  
  
  
***
  
### _Probability Puzzles in R_  
  
Chapter 1 - Introduction and Classic Puzzles  
  
Introduction:  
  
* Chapter 1 - Classic Problems  
* Chapter 2 - Dice Puzzles  
* Chapter 3 - Web Puzzles  
* Chapter 4 - Poker Games  
* Built-in combinatorics functions will help  
	* factorial(3)  
    * choose(5,3)  
* Can run simulations using sample(), rbinom(), replicate(), for, while, set.seed() and the like  
	* for(i in 1:10){ sum(sample(x = c(1,2,3,4,5,6), size = 2, replace = TRUE)) }  
    * rolls <- rep(NA, 10)  
    * for(i in 1:10){ rolls[i] <- sum(sample(x = c(1,2,3,4,5,6), size = 2, replace = TRUE)) }  
  
Birthday Problem:  
  
* Suppose that there are n people in the room, and we want to know the probability that 2+ people share a birthday  
	* Ignore February 29th  
    * Birthdays are uniformly distributed across the remaining 365 days  
    * Each individual in the room is independent  
* Can run a simulation-based approach to the problem  
* There is a built-in function pbirthday() that calculates the exact probability  
	* pbirthday(10)  
    * room_sizes <- c(1:10)  
    * match_probs <- sapply(room_sizes, pbirthday)  
    * plot(match_probs ~ room_size)  
  
Monty Hall:  
  
* Three door problem - one with a prize, and two with nothing  
	* Contestant picks a door  
    * Host opens a door with nothing  
    * Contestant has the choice to switch or not switch  
* Can manage this problem in R with reverse indexing  
	* doors <- 1:3  
    * reveal <- doors[-c(1,2)]  # assumes contestant chose 1 and actual prize is in 2  
    * reveal <- sample(x = doors[-1], size = 1)  # assumes contestant chose 1 and actual prize is in 1  
  
Example code includes:  
```{r}

# Set seed to 1
set.seed(1)


# Write a function to roll k dice
roll_dice <- function(k){
    all_rolls <- sample(c(1,2,3,4,5,6), k, replace = TRUE)
    final_answer <- sum(all_rolls)
    return(final_answer)
}

# Run the function to roll five dice
roll_dice(5)


# Initialize a vector to store the output
output <- rep(NA, 10000)

# Loop for 10000 iterations
for(i in 1:10000){
    # Fill in the output vector with the result from rolling two dice
    output[i] <- roll_dice(2)
}


set.seed(1)
n <- 50
match <- 0

# Simulate 10000 rooms and check for matches in each room
for(i in 1:10000){
    birthdays <- sample(1:365, n, replace = TRUE)
    if(length(unique(birthdays)) < n){ match <- match + 1 } 
}

# Calculate the estimated probability of a match and print it
p_match <- match/10000
print(p_match)


# Calculate the probability of a match for a room size of 50
pbirthday(50)


# Define the vector of sample sizes
room_sizes <- 1:50

# Run the pbirthday function within sapply on the vector of sample sizes
match_probs <- sapply(room_sizes, FUN=pbirthday)

# Create the plot
plot(match_probs ~ room_sizes)


set.seed(1)
doors <- c(1,2,3)

# Randomly select one of the doors to have the prize
prize <- sample(x = doors, size = 1)
initial_choice <- 1

# Check if the initial choice equals the prize
if(prize == initial_choice){
    print("The initial choice was correct!")
}

print(prize)


set.seed(1)
doors <- c(1,2,3)

# Define counter
win_count <- 0

# Run 10000 iterations of the game
for(i in 1:10000){
    prize <- sample(x = doors, size = 1)
    initial_choice <- 1
    if(initial_choice == prize){ win_count <- win_count + 1 }
}

# Print the answer
print(win_count / 10000)


reveal_door <- function(doors, prize, initial_choice){
    if(prize == initial_choice){
        # Sample at random from the two remaining doors
        reveal <- sample(doors[-prize], 1)
    } else {
        reveal <- doors[-c(prize, initial_choice)]
    }  
}

set.seed(1)
prize <- sample(doors,1)
initial_choice <- 1

# Use the reveal_door function to do the reveal
reveal <- reveal_door(doors, prize, initial_choice)

# Switch to the remaining door
final_choice <- doors[-c(initial_choice, reveal)]
print(final_choice)

# Check whether the final choice equals the prize
if(final_choice==prize){
    print("The final choice is correct!")
}

# Initialize the win counter
win_count <- 0

for(i in 1:10000){
    prize <- sample(doors,1)
    initial_choice <- 1
    reveal <- reveal_door(doors, prize, initial_choice)
    final_choice <- doors[-c(initial_choice, reveal)]
    if(final_choice == prize){
        # Increment the win counter
        win_count <- win_count + 1
    }
}

# Print the estimated probability of winning
print(win_count / 10000)

```
  
  
  
***
  
Chapter 2 - Games with Dice  
  
Yahtzee:  
  
* Yahtzee is based on rolling 5 dice, and then optionally re-rolling 0-5 of the dice for two more turns  
* When rolling 3 dice, there are 6**3 possible permutations of the dice  
* Can use factorial functions for combinatorics  
	* factorial(3)  
* Probability of rolling exactly {3, 4, 5} or {2, 3, 4} - probabilities can be added since they are MECE  
    * factorial(3)/6^3 + factorial(3)/6^3  
* The probability of rolling 3-dice Yahtzee  
	* 1/6^3 + 1/6^3 + 1/6^3 + 1/6^3 + 1/6^3 + 1/6^3  
* Can also use the choose() function for combinatorics in R  
	* choose(3,2)  
* Number of ways to roll 5 of one denomiation and 5 of the other denomination  
	* n_denom <- factorial(6) / factorial(4)  
    * n_groupings <- choose(10,5) * choose(5,5)  
    * n_total <- n_denom * n_groupings  
  
Settlers of Catan:  
  
* Typical games lasts ~60 rolls, and each spot is labelled 2-12  
* Can simulate dice rolls  
	* roll_dice <- function(k){  
    *     all_rolls <- sample(c(1,2,3,4,5,6), k, replace = TRUE)  
    *     final_answer <- sum(all_rolls)  
    * }  
    * replicate(10, roll_dice(2))  
    * table(rolls)  
    * sum(rolls == 3)  
  
Craps:  
  
* Pass line bet made prior to the start of a roll  
	* 7 or 11 on the first roll wins  
    * 2, 3, or 12 on the first roll loses  
    * All others become the point, which then needs to be rolled before the next 7 to be a winner  
* There is no set number of rolls, so replicate() and for() od not work as per the previous examples  
	* while(roll != 6){ roll <- roll_dice(1); print(roll) }  
  
Example code includes:  
```{r}

# Calculate the size of the sample space
s_space <- 6**5

# Calculate the probability of a Yahtzee
p_yahtzee <- 6 / s_space

# Print the answer
print(p_yahtzee)


s_space <- 6^5

# Calculate the probabilities
p_12345 <- factorial(5) / s_space
p_23456 <- factorial(5) / s_space
p_large_straight <- p_12345 + p_23456

# Print the large straight probability
print(p_large_straight)


s_space <- 6^5

# Calculate the number of denominations possible
n_denom <- factorial(6) / factorial(4)

# Calculate the number of ways to form the groups
n_groupings <- choose(5, 3)

# Calculate the total number of full houses
n_full_house <- n_denom * n_groupings

# Calculate and print the answer
print(n_full_house / s_space)


set.seed(1)

# Simulate one game (60 rolls) and store the result
rolls <- replicate(60, roll_dice(2))

# Display the result
table(rolls)


set.seed(1)
counter <- 0

for(i in 1:10000){
    # Roll two dice 60 times
    rolls <- replicate(60, roll_dice(2))
    # Check whether 2 or 12 was rolled more than twice
    if(sum(rolls==2) > 2 | sum(rolls==12) > 2) { counter <- counter + 1 }
}

# Print the answer
print(counter/10000)


roll_after_point <- function(point){
    new_roll <- 0
    # Roll until either a 7 or the point is rolled 
    while( (new_roll != point) & (new_roll != 7) ){
        new_roll <- roll_dice(2)
        if(new_roll == 7){ won <- FALSE }
        # Check whether the new roll gives a win
        if(new_roll == point){ won <- TRUE }
    }
    return(won)
}


evaluate_first_roll <- function(roll){
    # Check whether the first roll gives an immediate win
    if(roll %in% c(7, 11)){ won <- TRUE }
    # Check whether the first roll gives an immediate loss
    if(roll %in% c(2, 3, 12)){ won <- FALSE }
    if(roll %in% c(4,5,6,8,9,10) ){
        # Roll until the point or a 7 is rolled and store the win/lose outcome
        won <- roll_after_point(roll)
    }
    return(won)
}


set.seed(1)
won <- rep(NA, 10000)

for(i in 1:10000){
    # Shooter's first roll
    roll <- roll_dice(2)
    # Determine result and store it
    won[i] <- evaluate_first_roll(roll)
}

sum(won)/10000

```
  
  
  
***
  
Chapter 3 - Inspired from the Web  
  
Factoring a Quadratic:  
  
* Given random integers a, b, c, what is the probability that the quadratic will factor?  
	* The definition of "factorable" is that the solution is rational (can be expressed as the ratio of two integers; is not imaginary)  
* The solution is rational only when the discriminant (b**2 - 4*a*c) is a perfect square  
	* sqrt_dscr <- sqrt(3^2 - 4*1*2)  
    * sqrt_dscr == round(sqrt_dscr)  # basically, is it an integer; note that is.integer() will fail since the integer is reprsented as a float, making this FALSE  
  
Four Digit iPhone Passcodes:  
  
* Smudge marks on an iPhone can leave clues as to the passcode  
* Research suggests that using a single repeated digit can ENHANCE the security of the passcode  
* The identical function checks whether the full vector is equivalent, as opposed to the element-wise ==  
	* identical(set1, set2)  
  
Sign Error Cancellations:  
  
* Suppose that the possibility of a sign flip is p < 0.5 and suppose that an even number of sign flips gets a correct answer, is the student guaranteed greater than 50% likelihood of getting the right answer?  
	* sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)  
    * rbinom(n, size, prob)  
    * result <- sapply(X = c(0.25, 0.75, 0.1, 0.9), FUN = rbinom, n = 1, size = 1)  
  
Example code includes:  
```{r}

is_factorable <- function(a,b,c){
    # Check whether solutions are imaginary
    if(b^2 - 4*a*c < 0){ 
        return(FALSE)
        # Designate when the next section should run
    } else {
        sqrt_discriminant <- sqrt(b^2 - 4*a*c) 
        # return TRUE if quadratic is factorable
        return(sqrt_discriminant == round(sqrt_discriminant))
    }
}

counter <- 0

# Nested for loop
for(a in 1:100){
    for(b in 1:100){
        for(c in 1:100){
            # Check whether factorable
            if(is_factorable(a, b, c)){ counter <- counter + 1 }
        }
    }
}

print(counter / 100^3)


counter <- 0

# Store known values 
values <- c(3, 4, 5, 9)
passcode = values

for(i in 1:10000){
    # Create the guess
    guess <- sample(values, replace=FALSE)
    # Check condition 
    if(identical(passcode, guess)){ counter <- counter + 1 }
}

print(counter/10000)


counter <- 0
# Store known values
unique_values <- c(2, 4, 7)
passcode = c(unique_values, unique_values[1])

for(i in 1:10000){
    # Pick repeated value
    all_values <- c(unique_values, sample(unique_values, 1))
    # Make guess
    guess <- sample(all_values, replace=FALSE)
    if(identical(passcode, guess)){ counter <- counter + 1 }
}

print(counter / 10000)


set.seed(1)

# Run 10000 iterations, 0.1 sign switch probability
switch_a <- rbinom(10000, 3, prob=0.1)

# Calculate probability of correct answer
mean(switch_a/2==round(switch_a/2))

# Run 10000 iterations, 0.45 sign switch probability
switch_b <- rbinom(10000, 3, prob=0.45)

# Calculate probability of correct answer
mean(switch_b/2==round(switch_b/2))


set.seed(1)
counter <- 0

for(i in 1:10000){
    # Simulate switches
    each_switch <- sapply(c(0.49, 0.1), FUN=rbinom, size=1, n=1)
    # Simulate switches
    num_switches <- sum(each_switch)
    # Check solution
    if(num_switches/2 == round(num_switches/2)){ counter <- counter + 1 }
}

print(counter/10000)

```
  
  
  
***
  
Chapter 4 - Poker  
  
Texas Hold'em:  
  
* Texas Hold'em is a variant of poker where each player has 2 personal cards and 5 communal cards, with rounds of betting to start and then as the communal cards are shown  
* Can look at probabilities when there are 2 cards left to go or one card left to go  
* Can look at varying numbers of "outs" (winning cards)  
	* outs <- c(0,1,2,3)  
    * p_lose <- choose(10-outs,2) / choose(10,2)  
    * p_win <- 1 - p_lose  
* Can further calculate the expected value of the possible outcomes (sum of profit*probability across all possible outcomes)  
  
Consecutive Cashes:  
  
* WSOP Main Event held in Las Vegas - survival time is all that matters (not number of chips left at any given time)  
	* Roughly the top 10% of players "cash" (win prize money)  
    * Ronnie Bardah cashed five straight years - how likely is that?  
* Simplifying assumptions include  
	* 6000 players, same by year, all of same talent  
    * cash_year1 <- sample(players, 4)  
    * cash_year2 <- sample(players, 4)  
    * intersect(cash_year1, cash_year2)  
* Challenge for this lesson is to get the intersection of multiple years; can use a matrix for this  
	* cashes <- replicate(3, sample(players, 4))  
    * in_all_three <- Reduce(intersect, list(cashes[, 1], cashes[, 2], cashes[, 3]))  # input to Reduce() must be a list  
    * length(in_all_three)  
  
von Neumann Model of Poker:  
  
* Application of game theory to poker - model stipulates each hand is a random draw from Uniform(0, 1)  
	* runif(n, min = 0, max = 1)  
    * runif(n = 1)  
* Model assumes that runif() are compared iff both players wagered; otherwise, the wagering player beats the non-wagering player  
* Can use the ifelse() function for modeling  
  
Wrap Up:  
  
* Combinatorics - choose(), factorial()  
* Simulation - sample(), replicate(), runif()  
* Can continue with more complex combinatorics puzzles  
* Can transfer directly to Monte Carlo and Markov chains  
  
Example code includes:  
```{r}

p_win <- 8 / 46
curr_pot <- 50
bet <- 10

# Define vector of probabilities
probs <- c(p_win, 1-p_win)

# Define vector of values
values <- c(curr_pot, -bet)

# Calculate expected value
sum(probs*values)


outs <- c(0:25)

# Calculate probability of not winning
p_no_outs <- choose(47-outs, 2) /choose(47, 2)

# Calculate probability of winning
p_win <- 1 - p_no_outs

print(p_win)


players <- c(1:60)
count <- 0

for(i in 1:10000){
    cash_year1 <- sample(players, 6)
    cash_year2 <- sample(players, 6)
    # Find those who cashed both years
    cash_both <- intersect(cash_year1, cash_year2)
    # Check whether anyone cashed both years
    if(length(cash_both) > 0){ count <- count + 1 }
}

print(count/10000)


check_for_five <- function(cashed){
    # Find intersection of five years
    all_five <- Reduce(intersect, list(cashed[, 1], cashed[, 2], cashed[, 3], cashed[, 4], cashed[, 5]))
    # Check intersection
    if(length(all_five) > 0){ 
        return(TRUE)
        # Specify when to return FALSE
    } else { return(FALSE) }
}


players <- c(1:6000)
count <- 0

for(i in 1:10000){
    # Create matrix of cashing players
    cashes <- replicate(5, sample(players, 600, replace=FALSE))
    # Check for five time winners
    if(check_for_five(cashes)){ count <- count + 1 }
}

print(count/10000)


# Generate values for both players
A <- runif(1)
B <- runif(1)

# Check winner
if(A > B){
    print("Player A wins")
} else {
    print("Player B wins")
}

print(A)
print(B)


one_round <- function(bet_cutoff){
    a <- runif(n = 1)
    b <- runif(n = 1)
    # Fill in betting condition
    if(b > bet_cutoff){
        # Return result of bet
        return(ifelse(b > a, 1, -1))
    } else {
        return(0)
    }  
}


b_win <- rep(NA, 10000)

for(i in 1:10000){
    # Run one and store result
    b_win[i] <- one_round(0.5)
}

# Print expected value
mean(b_win)

```
  
  
  
***
  
### _Highcharter for Finance in R_  
  
Chapter 1 - Introduction to Highcharter  
  
Introduction:  
  
* Highcharts build strong visualizations - requires a license for professional use  
* The highcharter package on CRAN wraps the highcharts package  
	* Also called an htmlwidget  
    * Extends on package like ggplot by enabling zooming, hovering, etc.  
* Can make an OHLC chart (assuming appropriate underlying data) simply  
	* hchart(spy_prices)  
    * hchart(spy_prices$open, type = "line", color = "purple")  
  
Two highcharter paradigms:  
  
* Can use either highchart() to draw a blank canvas or hchart(object) to plot the object while creating the highchart()  
	* highchart(type = "stock") %>% hc_add_series(spy_prices)  # spy_prices is formatted as xts data  
    * hchart(spy_prices)  # will guess the best type based on the data (if not specified)  
    * hchart(spy_prices_tibble, hcaes(x = date, y = open), type = "line")  # hcaes() is the hchart() equivalent of aes()  
  
Data going forward:  
  
* The xts data holds the full stock data for the portfolio, indexed by date (all xts objects must have an index)  
	* etf_prices_xts  
    * etf_prices_xts$SPY  # will include the index (date)  
    * index(etf_prices_xts)  # will show the index  
* Can also store data as a wide tibble  
	* etf_prices_wide_tibble  # no index, date is an explicit column  
    * etf_prices_wide_tibble$SPY  # date is not shown since it is not selected  
* Can also store data as a tidy (long) tibble  
	* etf_prices_tidy_tibble  
    * etf_tidy_tibble_prices %>% filter(symbol == "SPY")  # grab just the SPY data  
  
Example code includes:  
```{r cache=TRUE}

load("./RInputFiles/stock_prices_xts.RData")
load("./RInputFiles/stock_tidy_tibble_prices.RData")
load("./RInputFiles/stock_wide_tibble_returns.RData")

str(stock_prices_xts)
str(stock_tidy_tibble_prices)
str(stock_wide_tibble_returns)


load("./RInputFiles/commodities_returns.RData")
load("./RInputFiles/commodities-returns-tidy.RData")
load("./RInputFiles/commodities-xts.RData")

str(commodities_returns)
str(commodities_returns_tidy)
str(commodities_xts)


# Load the highcharter package
library(highcharter)
library(zoo)


quantmod::getSymbols("XLK", src="yahoo", from="2012-12-31", to="2017-12-31")
fix_vars <- function(x) { tolower(str_split(x, fixed("."))[[1]][2]) }
xlk_prices <- XLK
names(xlk_prices) <- sapply(names(XLK), FUN=fix_vars) %>% unname()


# Build a candlestick chart
hchart(xlk_prices, type = "candlestick")

# Build a ohlc chart
hchart(xlk_prices, type = "ohlc")

# Build a line chart
hchart(xlk_prices$close, type = "line")


# Show the dates
head(index(xlk_prices))

# Use the base function and set the correct chart type
highchart(type = "stock") %>%
    hc_add_series(xlk_prices)


xlk_prices_tibble <- fortify.zoo(xlk_prices) %>%
    as_tibble() %>%
    rename("date"="Index")
head(xlk_prices_tibble)


# Create a line chart of the 'close' prices
hchart(xlk_prices_tibble, hcaes(x = date, y = close), type = "line")

# Create a line chart of the open prices
hchart(xlk_prices_tibble, hcaes(x = date, y = open), type = "line")


# Inspect the first rows of the xts data object
head(stock_prices_xts)

# Extract and show the GOOG column from the xts object
head(stock_prices_xts$GOOG)

# Display the date index from the xts object
head(index(stock_prices_xts))

# Extract and show the DIS column from the xts object
head(stock_prices_xts$DIS)


stock_wide_tibble_prices <- stock_tidy_tibble_prices %>%
    tidyr::spread(symbol, price)

# Inspect the first rows of the wide tibble object
head(stock_wide_tibble_prices)

# Extract and show the GOOG column from the wide tibble data
head(stock_wide_tibble_prices$GOOG)

# Display the date information from the wide tibble data
head(stock_wide_tibble_prices$date)

# Extract and show the DIS column from the wide tibble data
head(stock_wide_tibble_prices$DIS)


# Inspect the first rows of the tidy tibble object
head(stock_tidy_tibble_prices)

# Extract and show the GOOG price data from the tidy tibble data
stock_tidy_tibble_prices %>%
    filter(symbol == "GOOG") %>%
    head()

# Display the date information from the tidy tibble
head(stock_tidy_tibble_prices$date)

# Extract and show the DIS price data from the tidy tibble data
stock_tidy_tibble_prices %>%
    filter(symbol == "DIS") %>%
    head()

```
  
  
  
***
  
Chapter 2 - Highcharter for xts data  
  
Chart the price of one stock in an xts object:  
  
* Dataset has five ETF prices in a single object  
	* etf_prices_xts  
* Can create charts for just a single series in the ETF data  
	* highchart(type = "stock")  # create a blank chart canvas, informs to look for a date index since type="stock"  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$SPY)  # include data for series SPY, date is grabbed automatically from the index  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$EEM, color = "green")  
  
Chart the price of many stocks from xts:  
  
* Can add multiple series using multiple ncalls to hc_add_series()  
	* highchart(type="stock") %>% hc_add_series(etf_prices_xts$SPY) %>% hc_add_series(etf_prices_xts$IJS)  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$SPY, color = "blue") %>% hc_add_series(etf_prices_xts$IJS, color = "red")  
    * highchart(type = "stock") %>% hc_add_series(etf_prices_xts$SPY, color = "blue", name = "SPY") %>% hc_add_series(etf_prices_xts$IJS, color = "red", name = "IJS")  
  
Adding a title, subtitle, and axis labels:  
  
* Good labelling makes charts easier to read and interpret - best practice is to add title, then subtitle, then the rest of the data  
	* highchart(type = "stock") %>% hc_title(text = "5 ETFs Price History")  
    * highchart(type = "stock") %>% hc_title(text = "5 ETFs Price History") %>% hc_subtitle(text = "daily prices")  
    * highchart(type = "stock") %>%  
    *     hc_title(text = "5 ETFs Price History") %>%  
    *     hc_subtitle(text = "daily prices") %>%  
    *     hc_add_series(etf_prices_xts$SPY, color = "blue", name = "SPY") %>%  
    *     hc_add_series(etf_prices_xts$IJS, color = "red", name = "IJS") %>%  
    *     hc_add_series(etf_prices_xts$EEM, color = "green", name = "EEM") %>%  
    *     hc_add_series(etf_prices_xts$EFA, color = "purple", name = "EFA") %>%  
    * hc_add_series(etf_prices_xts$AGG, color = "orange", name = "AGG")  
* Can modify axes - labels, positions, number formats, etc. - add hc_yAxis() at the end of plotting code  
	* hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)  # opposite=FALSE puts the y-axis on the left (default opposite=TRUE goes on the right)  
  
Tooltips and legends:  
  
* The tooltip is like a magnifying glass for the viewer  
	* hc_tooltip(pointFormat = "text in the tooltip")  
    * hc_tooltip(pointFormat = "${point.y}")  # the text should be a '$' followed by the point the user is hovering over  
    * hc_tooltip(pointFormat = "${point.y: .2f}")  # round the text that is hovered over to 2 decimal points, and format with a $ sign in front  
    * hc_tooltip(pointFormat = "{point.series.name}: ${point.y: .2f})  # will show the series name followed by$price, rounded to 2 digits  
* Can also add legends to the chart - legends are interactive, and clicking them turns series on or off  
	* hc_legend(enabled = TRUE)  
  
Example code includes:  
```{r cache=TRUE}

# Chart the price of KO
highchart(type = "stock") %>%
    hc_add_series(stock_prices_xts$KO)


# Fill in the complete highchart code flow to chart GOOG in green
highchart(type = "stock") %>%
    hc_add_series(stock_prices_xts$GOOG, color = "green")

# Fill in the complete highchart code flow to chart DIS in purple
highchart(type = "stock") %>%
    hc_add_series(stock_prices_xts$DIS, color = "purple")


highchart(type = "stock") %>% 
    # Add the price of GOOG, colored orange
    hc_add_series(stock_prices_xts$GOOG, color = "orange") %>% 
    # Add the price of DIS, colored black
    hc_add_series(stock_prices_xts$DIS, color = "black")

highchart(type = "stock") %>% 
    # Add the price of KO, colored green
    hc_add_series(stock_prices_xts$KO, color = "green") %>%
    # Add the price of JPM, colored pink
    hc_add_series(stock_prices_xts$JPM, color = "pink")


highchart(type = "stock") %>%
    # Add JPM as a blue line called JP Morgan
    hc_add_series(stock_prices_xts$JPM, color = "blue", name = "JP Morgan") %>%
    # Add KO as a red line called Coke
    hc_add_series(stock_prices_xts$KO, color = "red", name = "Coke") %>%
    # Add GOOG as a green line named Google
    hc_add_series(stock_prices_xts$GOOG, color = "green", name = "Google") %>%
    # Add DIS as a purple line named Disney
    hc_add_series(stock_prices_xts$DIS, color = "purple", name = "Disney")


highchart(type = "stock") %>%
    # Add the stocks to the chart with the correct color and name
    hc_add_series(stock_prices_xts$JPM, color = "blue", name = "jpm") %>%
    hc_add_series(stock_prices_xts$KO, color = "red", name = "coke") %>%
    hc_add_series(stock_prices_xts$GOOG, color = "green", name = "google") %>%
    hc_add_series(stock_prices_xts$DIS, color = "purple", name = "disney") %>%
    hc_add_series(stock_prices_xts$AMZN, color = "black", name = "amazon")


highchart(type = "stock") %>%
    # Supply the text of the title to hc_title()
    hc_title(text = "A history of two stocks") %>%
    # Supply the text of the subtitle to hc_subtitle()
    hc_subtitle(text = "told with lines") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>%
    # Supply the text and format of the y-axis
    hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)


highchart(type = "stock") %>%
    # Add a title
    hc_title(text = "A history of two stocks") %>% 
    # Add a subtitle
    hc_subtitle(text = "told with lines") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>%
    # Change the y-axis title
    hc_yAxis(title = list(text = "in $$$s"), labels = list(format = "{value} USD"), opposite = FALSE)


highchart(type = "stock") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>%
    # Add the dollar sign and y-values on a new line
    hc_tooltip(pointFormat = "Daily Price:<br> ${point.y}")


highchart(type = "stock") %>% 
    hc_add_series(stock_prices_xts$AMZN, color = "blue", name = "AMZN") %>% 
    hc_add_series(stock_prices_xts$DIS, color = "red", name = "DIS")  %>% 
    hc_add_series(stock_prices_xts$GOOG, color = "green", name = "GOOG") %>%
    # Add stock names and round the price
    hc_tooltip(pointFormat = "{point.series.name}: ${point.y: .2f}") %>%
    # Enable the legend
    hc_legend(enabled = TRUE)


# Choose the type of highchart
highchart(type = "stock") %>%
    # Add gold, platinum and palladium
    hc_add_series(commodities_xts$gold, color = "yellow", name= "Gold") %>% 
    hc_add_series(commodities_xts$platinum, color = "grey", name= "Platinum") %>% 
    hc_add_series(commodities_xts$palladium, color = "blue", name= "Palladium") %>%
    # Customize the pointFormat of the tooltip
    hc_tooltip(pointFormat = "{point.series.name}: ${point.y} ") %>%
    hc_title(text = "Gold, Platinum and Palladium 2017") %>%
    hc_yAxis(labels = list(format = "${value}"))


```
  
  
  
***
  
Chapter 3 - Highcharter for wide tibble data
  
Visualizing one stock from wide tibble data:  
  
* The tibble format includes a column for date, since there is no index as in the xts - requires use of hcaes() to map the columns to x and y  
	* hchart(etf_prices_wide_tibble, hcaes(x = date, y = SPY), type = "line")  
    * hchart(etf_prices_wide_tibble, hcaes(x = date, y = SPY), type="line", color = "green", name = "SPY")  
  
Visualizing multiple stocks from wide tibble data:  
  
* Can plot multiple series on the same chart, using hchart() %>% hc_add_series() - can use multiple hc_add_series()  
	* hchart(etf_prices_wide_tibble, hcaes(x = date, y = SPY), type = "line") %>%  
    *     hc_add_series(etf_prices_wide_tibble, hcaes(x = date, y = EEM) type = "line")  
* The tooltip will hover only over a specific line, rather than all lines for that time point as with xts plotting  
	* hc_tooltip(shared = TRUE)  # will display all series tooltips at the same time  
    * hc_tooltip(shared = TRUE, pointFormat = "{point.series.name}: ${point.y: .2f}")  # format acordingly  
    * hc_tooltip(shared = TRUE, pointFormat = "{point.series.name}: ${point.y: .2f}<br>")  # <br> is html for line break  
* Can also customize the y-axis labels and formats, as well as adding a legend  
	* hc_yAxis(title = list(text = "prices (USD)"), labels = list(format = "${value}"))  
    * hc_legend(enabled = TRUE)  
* Each of the hc_add_series() calls needs to also specify inclusion in the legend  
	* hc_add_series(etf_prices_wide_tibble, hcaes(x = date, y = EEM) name = "EEM", type = "line", showInLegend = TRUE)  
  
Scatterplots from etf_wide_tibble:  
  
* The wide tiblle is more flexible, specifically in allowing for different chart types including scatter  
	* hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter")  
    * hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter", color = "pink", name = "EEM v. SPY")  
* Can format the tooltips for better readability  
	* hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter", color = "pink", name = "EEM v. SPY") %>% hc_tooltip(pointFormat = "{point.date} <br> EEM: {point.y: .2f}% <br> SPY: {point.x: .2f}%")  
  
Mixing chart types from wide tibble data:  
  
* Can create multiple chart types on the same plot; for example, regression line on a scatter plot  
	* model <- lm(EEM ~ SPY, data = etf_wide_tibble_returns)  
    * slope <- coef(model)[2]  
    * hchart(etf_wide_tibble_returns, hcaes(x = SPY, y = EEM), type = "scatter", color = "pink", name = "EEM v. SPY") %>% hc_add_series(etf_wide_tibble_returns, hcaes(x = SPY, y = (SPY * slope), type = "line", color = "blue", lineWidth = 3)  
* Can further customize titles, axes, tooltips and the like, by using %>% to the next commands  
	* hc_title(text = "Scatter plot with regression line") %>%  
    *     hc_yAxis(title = list(text = "EEM Daily returns (%)"), labels = list(format = "{value}%"), opposite = FALSE) %>%  
    *     hc_xAxis(title = list(text = "SPY Daily returns (%)"), labels = list(format = "{value}%")) %>%  
    *     hc_tooltip(pointFormat = "{point.date} <br> EEM {point.y: .2f}% <br> SPY: {point.x: .2f}%")  
* May want to change the tooltip for the regression line, so that it shows the relevant data for the regression rather than the scatter  
	* hc_add_series(etf_wide_tibble_returns, hcaes(x = SPY, y = (SPY * slope), type = "line", color = "blue", lineWidth = 3, tooltip = list( headerFormat = "", pointFormat = ""))  
    * tooltip = list( headerFormat = "regression line", pointFormat = "{point.y: .2f}%"))  
    * hc_add_series(etf_wide_tibble_returns, hcaes(x = SPY, y = (SPY * coef(model)[2])), type = "line", color="blue", lineWidth=3, tooltip=list( headerFormat="regression line", pointFormat = "{point.y: .2f}%"))  
  
Example code includes:  
```{r cache=TRUE}

# Visualize DIS as a line chart	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = DIS), 	
       type = "line",
       # Specify the name
       name = "DIS", 
       # Specify the color
       color = "orange"
       )


# Create a green line chart of KO	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = KO), type = "line", color = "green", name = "KO")

# Create a black line chart of JPM	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = JPM), type = "line", color = "black", name = "JPM")


# Create a line chart of KO	
hchart(stock_wide_tibble_prices, hcaes(x = date, y = KO), name = "KO", type = "line") %>%
    # Add JPM to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x = date, y = JPM), name = "JPM", type = "line") %>%
    # Add DIS to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x = date, y = DIS), name = "DIS", type = "line") %>%
    # Add AMZN to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x = date, y = AMZN), name = "AMZN", type = "line") %>%
    # Enable a shared tooltip
    hc_tooltip(shared = TRUE)


hchart(stock_wide_tibble_prices, hcaes(x=date, y=KO), name="KO", type="line", showInLegend = TRUE) %>%	
    # Add JPM to the chart and show it in the legend
    hc_add_series(stock_wide_tibble_prices, hcaes(x=date, y=JPM), name="JPM", type="line", showInLegend=TRUE) %>%
    # Add DIS to the chart and show it in the legend
    hc_add_series(stock_wide_tibble_prices, hcaes(x=date, y=DIS), name="DIS", type="line", showInLegend=TRUE) %>%
    # Add a legend to the chart
    hc_legend(enabled = TRUE)


hchart(stock_wide_tibble_prices, hcaes(x = date, y = KO), name = "KO", type = "line") %>%
    # Add JPM to the chart
    hc_add_series(stock_wide_tibble_prices, hcaes(x=date, y=JPM), name = "JPM", type = "line") %>%
    # Enable a shared tooltip
    hc_tooltip(shared = TRUE, pointFormat = "{point.series.name}: ${point.y: .2f}<br>") %>%
    # Change the text of the title of the y-axis
    hc_yAxis(title = list(text = "prices (USD)"))


# Specify a green scatter plot	
hchart(stock_wide_tibble_returns, hcaes(x = GOOG, y = JPM),	
       type = "scatter", color = "green", name = "GOOG v. JPM"
       ) %>%
    # Make the tooltip display the x and y points and percentage sign
    hc_tooltip(pointFormat = "GOOG: {point.x: .2f}% <br>JPM: {point.y: .2f}%")


hchart(stock_wide_tibble_returns, hcaes(x = KO, y = AMZN), type = "scatter", 
       color = "pink",	name = "GOOG v. AMZN"
       ) %>%
    # Add a custom tooltip format
    hc_tooltip(pointFormat = "{point.date} <br>AMZN: {point.y: .2f}% <br>KO: {point.x: .2f}%")


# Create a scatter plot	
hchart(stock_wide_tibble_returns, hcaes(x = KO, y = GOOG), type = "scatter") %>%
    # Add the slope variable
    hc_add_series(stock_wide_tibble_returns, hcaes(x = KO, y = (KO * 1.15)), type =  "line") %>%
    # Customize the tooltip to show the date, x-, and y-values
    hc_tooltip(pointFormat = "{point.date} <br> GOOG {point.y: .2f}% <br> KO: {point.x: .2f}%")


hchart(stock_wide_tibble_returns, hcaes(x = AMZN, y = DIS),	type = "scatter") %>%	
    hc_add_series(stock_wide_tibble_returns, hcaes(x = AMZN, y = (AMZN * .492)), type =  "line",
                  # Add the tooltip argument
                  tooltip = list(
                      # Change the header of the line tooltip
                      headerFormat = "DIS/AMZN linear relationship<br>",
                      # Customize the y value display
                      pointFormat = "{point.y: .2f}%"
                      )
                  ) %>%
    # Customize the scatter tooltip
    hc_tooltip(pointFormat = "{point.date} <br> DIS: {point.y: .2f}% <br> AMZN: {point.x: .2f}%")


# Start the hchart flow for the returns data	
hchart(commodities_returns, type = "scatter", 
       hcaes(x = gold, y = palladium, date = date), color = "pink"
       ) %>%
    # Customize the tooltip
    hc_tooltip(pointFormat = "date: {point.date} <br>palladium: {point.y:.4f} <br>gold: {point.x:.4f} ") %>%
	hc_title(text = "Palladium Versus Gold 2017")

```
  
  
  
***
  
Chapter 4 - Highcharter for tidy tibble data  
  
Tidy data:  
  
* Financial data tends to be in wide format, though the tidy format is increasingly common in the R world  
* Need to change the mapping approach, with price mapped to the y-axis  
	* etf_tidy_tibble_prices %>% filter(symbol == "SPY") %>% hchart(., hcaes(x = date, y = price), type = "line")  # the . Is how highcharter takes in data from previous step  
    * etf_tidy_tibble_prices %>% filter(symbol == "EEM") %>% hchart(., hcaes(x = date, y = price), type = "line", color = "green")  
  
Chart many ETF from a tidy tibble:  
  
* Tidy format allows for easy plotting of all series  
	* etf_tidy_tibble_prices %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line")  
    * etf_tidy_tibble_prices %>% filter(symbol != "AGG") %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line")  
* Can also customize axes  
	* etf_tidy_tibble_prices %>% filter(symbol != "AGG" & symbol ! = "EFA") %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>% hc_title(text = "Tidy Line Charts") %>% hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)  
  
Creativity with tidy data:  
  
* Tidy format allows for easy manipulation and transformation of the underlying data  
	* etf_long_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns))  
    * etf_tidy_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns)) %>% hchart(., hcaes(x = symbol, y = mean, group = symbol), type = "scatter")  
    * etf_tidy_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns)) %>% hchart(., hcaes(x = symbol, y = mean, group = symbol, size = std_dev), type = "scatter")  
* Can further display the return to risk ratio  
	* etf_tidy_returns_tibble %>% summarize(mean = mean(returns), std_dev = sd(returns), return_risk = mean/std_dev) %>% hchart(., hcaes(x = symbol, y = return_risk, group = symbol), type = "column")  
  
Tidy tooltips:  
  
* Can customize the tooltips similar to what was done with xts or wide data  
	* etf_tidy_returns_tibble %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>% hc_tooltip(pointFormat = "${point.price: .2f}")  
    * etf_tidy_prices_tibble %>% hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>% hc_tooltip(pointFormat = "{point.symbol}: ${point.price: .2f}<br>", shared = TRUE)  
* Can change labels using mutate  
	* etf_tidy_prices_tibble %>%  
    *     mutate(type = case_when(symbol == "EFA" ~ "international", symbol == "EEM" ~ "emerging", symbol == "AGG" ~ "bond", symbol == "IJS" ~ "small-cap", symbol == "SPY" ~ "market"))  
    * etf_tidy_prices_tibble %>%  
    *     mutate(type = case_when(symbol == "EFA" ~ "international", symbol == "EEM" ~ "emerging", symbol == "AGG" ~ "bond", symbol == "IJS" ~ "small-cap", symbol == "SPY" ~ "market")) %>%  
    *     hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>%  
    *     hc_tooltip(pointFormat = " {point.symbol}: ${point.price: .2f <br> fund type: {point.type}")  
* Can further run this process using summary stats (underlying tibble is grouped by symbol)  
	* etf_tidy_returns_tibble %>% summarize(mean = mean(returns), st_dev = sd(returns), max_return = max(returns), min_return = min(returns))  
    * etf_tidy_returns_tibble %>%  
    *     summarize(mean = mean(returns), st_dev = sd(returns), max_return = max(returns), min_return = min(returns)) %>%  
    *     hchart(., hcaes(x = symbol, y = mean, group = symbol), type = "column") %>%  
    *     hc_tooltip(pointFormat = "sd: {point.st_dev: .4f}% <br> max: {point.max_return: .4f}% <br> min: {point.min_return: .4f}%")  
  
Wrap up:  
  
* Tibbles and xts objects for highcharter - wide tibbles, tidy tibles, xts  
	* highchart(type="stock")  
    * hchart()  
* Customized tooltips  
  
Example code includes:  
```{r cache=TRUE}

stock_tidy_tibble_prices %>% 
    # Filter by the symbol
    filter(symbol == "KO") %>%
    # Pass the data, choose the mappings and create a line chart
    hchart(., hcaes(x = date, y = price), type = "line", color = "red")


stock_tidy_tibble_prices %>% 
    # Filter the data by symbol
    filter(symbol == "GOOG") %>%
    # Pass the data
    hchart(., hcaes(x = date, y = price), type = "line", color = "purple")


# Chart AMZN as a black line
stock_tidy_tibble_prices %>% 
    filter(symbol == "AMZN") %>%
    hchart(., hcaes(x = date, y = price), type = "line", color = "black")


stock_tidy_tibble_prices %>%
    # Pass in the data
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>%
    # Title the chart
    hc_title(text = "Daily Prices from Tidy Tibble") %>% 
    # Customize the y-axis and move the labels to the left
    hc_yAxis(title = list(text = "Prices (USD)"), labels = list(format = "${value}"), opposite = FALSE)


stock_tidy_tibble_prices %>%
    # Filter the data so it doesn't inclue JP Morgan
    filter(symbol != "JPM") %>%
    # Pass in the data and define the aesthetic mappings
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line")

stock_tidy_tibble_prices %>%
    # Filter the data so it doesn't include Disney and Coke
    filter(!(symbol %in% c("DIS", "KO"))) %>%
    # Pass in the data and define the aesthetic mappings
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line")


stock_tidy_tibble_returns <- stock_tidy_tibble_prices %>%
    arrange(symbol, date) %>%
    group_by(symbol) %>%
    mutate(returns = price / lag(price) - 1) %>%
    filter(!is.na(returns))
str(stock_tidy_tibble_returns)


stock_tidy_tibble_returns %>%
    # Calculate the standard deviation and mean of returns
    summarize(std_dev = sd(returns), mean = mean(returns)) %>%
    hchart(., hcaes(x = symbol, y = std_dev, color = symbol, size = mean), type = "scatter") %>% 
    hc_title(text = "Standard Dev and Mean Return")


stock_tidy_tibble_returns %>%
    summarize(avg_returns = mean(returns), vol_risk = sd(returns), risk_return = vol_risk/avg_returns) %>%
    # Pass the summary statistics to hchart
    hchart(., hcaes(x = symbol, y = risk_return, group = symbol), type = "column") %>% 
    hc_title(text = "Risk/Return") %>% 
    hc_subtitle(text = "lower bars are better")


stock_tidy_tibble_prices %>%
    mutate(sector = case_when(symbol == "AMZN" ~ "tech", symbol == "GOOG" ~ "tech", symbol == "DIS" ~ "fun",
                              symbol == "JPM" ~ "bank", symbol == "KO" ~ "food")) %>%
    hchart(., hcaes(x = date, y = price, group = symbol), type = "line") %>%
    # Set the tooltip display with curly braces
    hc_tooltip(pointFormat = "{point.symbol}: ${point.y: .2f}<br> sector: {point.sector}")


# Calculate the mean, sd, max and min returns
stock_tidy_tibble_returns %>% 
    summarize(mean = mean(returns), st_dev = sd(returns), 
              max_return = max(returns), min_return = min(returns)
              ) %>%
    hchart(., hcaes(x = symbol, y = st_dev, group = symbol), type = "column") %>% 
    hc_tooltip(pointFormat = "mean: {point.mean: .4f}% <br>max: {point.max_return: .4f}% <br>min: {point.min_return: .4f}%")


# Pass the tidy tibble to hchart()
hchart(commodities_returns_tidy, hcaes(x=date, y=return, group=metal, date=date), type="scatter") %>% 
    hc_title(text = "Gold, Palladium and Platinum Returns 2017") %>%
    # Customize the tooltip
    hc_tooltip(pointFormat = "date: {point.date} <br>{point.metal}: {point.return: .4f}")

```
  
  
  
***
  
### _Advanced Dimensionality Reduction in R_  
  
Chapter 1 - Introduction to Advanced Dimensionality Reduction  
  
Exploring the MNIST Dataset:  
  
* The t-SNE technique is for t-Distributed Stochastic Neighbor Embedding  
* The GLRM is the Generalized Low Rank Model  
* There are multiple benefits to running dimensionality reduction - feature selection, data compression, vizualization, etc.  
* The MNIST dataset contains 70,000 images of 28x28 pixel handwritten digits  
  
Distance Metrics:  
  
* Can use distance metrics to quantify similarity between MNIST digits  
* A distance metric is a function for points x, y, z where the output satisfies all of  
	* Triangle inequality: d(x,z) <= d(x,y) + d(y,z)  
    * Symmetric property: d(x,y) = d(y,x)  
    * Non-negativity and identity: d(x,y) >= 0 and d(x,y)=0 only if x=y  
* Euclidean distance is an example - length of the connecting line segment (square root of sum-squared distances by dimension)  
	* distances <- dist(mnist_sample[195:200 ,-1])  
    * heatmap(as.matrix(distances), Rowv = NA, symm = T, labRow = mnist_sample$label[195:200], labCol = mnist_sample$label[195:200])  
* Minkowski family of distances  
	* Sum-over-all-dimensions-of[ (absolute-value-distance-on-dimension)**p ]**(1/p)  # referred to as the Manhattan distance for p=1  
    * distances <- dist(mnist_sample[195:200 ,-1, method = "minkowski", p = 3])  
    * distances <- dist(mnist_sample[195:200 ,-1], method = "manhattan")  
* The Kullback-Leibler Divergence fails to meet all the criteria  
	* Not a metric since it does not satisfy the symmetric and triangle inequality properties  
    * Measures differences in probability distributions  
    * A divergence of 0 indicates that the two distributions are identical  
    * A common distance metric in Machine Learning (t-SNE). For example, in decision trees it is called Information Gain  
    * library(philentropy)  
    * mnist_6 <- mnist_sample[195:200, -1]  
    * mnist_6 <- mnist_6 + 1  
    * sums <- rowSums(mnist_6)  
    * distances <- distance(mnist_6/sums, method = "kullback-leibler")  
    * heatmap(as.matrix(distances), Rowv = NA, symm = T, labRow = mnist_sample$label, labCol = mnist_sample$label)  
  
PCA and t-SNE:  
  
* The "curse of dimensionality" is that distance metrics do not perform well with high-dimension datasets  
* Principal Component Analysis (PCA) is a well-known technique for dimension reduction  
	* pca_result <- prcomp(mnist[, -1])  
    * pca_result <- prcomp(mnist[, -1], rank = 2)  # get only the first two components  
    * plot(pca_result$x[,1:2], pch = as.character(mnist$label), col = mnist$label, main = "PCA output")  
* The t-SNE process (not shown) can sometimes better differentiate the underlying data  
	* plot(tsne$tsne_x, tsne$tsne_y, pch = as.character(mnist$label), col = mnist$label+1, main = "t-SNE output")  
  
Exampe code includes:  
```{r}

load("./RInputFiles/mnist-sample-200.RData")
load("./RInputFiles/fashion_mnist_500.RData")
load("./RInputFiles/creditcard.RData")

dim(mnist_sample)
dim(fashion_mnist)
str(creditcard)


# Have a look at the MNIST dataset names
names(mnist_sample)
names(fashion_mnist)

# Labels of the first 6 digits
head(mnist_sample[, 1])


# Plot the histogram of the digit labels
hist(mnist_sample$label)

# Compute the basic statistics of all records
# summary(mnist_sample)

# Compute the basic statistics of digits with label 0
# summary(mnist_sample[mnist_sample$label==0,])


# Show the labels of the first 10 records
mnist_sample$label[1:10]

# Compute the Euclidean distance of the first 10 records
distances <- dist(mnist_sample[1:10, -1], method="euclidean")

# Show the distances values
distances

# Plot the numeric matrix of the distances in a heatmap
heatmap(as.matrix(distances), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )


# Minkowski distance or order 3
distances_3 <- dist(mnist_sample[1:10, -1], method="minkowski", p=3)
distances_3
heatmap(as.matrix(distances_3), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )

# Minkowski distance of order 2
distances_2 <- dist(mnist_sample[1:10, -1], method="minkowski", p=2)
distances_2
heatmap(as.matrix(distances_2), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )


# Get the first 10 records
mnist_10 <- mnist_sample[1:10, -1]

# Add 1 to avoid NaN when rescaling
mnist_10_prep <- mnist_10 + 1

# Compute the sums per row
sums <- rowSums(mnist_10_prep)

# Compute KL divergence
distances <- philentropy::distance(mnist_10_prep/sums, method="kullback-leibler")
heatmap(as.matrix(distances), Rowv = NA, symm = TRUE, 
        labRow = mnist_sample$label[1:10], labCol = mnist_sample$label[1:10]
        )


# Get the principal components from PCA
pca_output <- prcomp(mnist_sample[, -1])

# Observe a summary of the output
summary(pca_output)

# Store the first two coordinates and the label in a data frame
pca_plot <- data.frame(pca_x = pca_output$x[, 1], pca_y = pca_output$x[, 2], 
                       label = as.factor(mnist_sample$label)
                       )

# Plot the first two principal components using the true labels as color and shape
ggplot(pca_plot, aes(x = pca_x, y = pca_y, color = label)) + 
    ggtitle("PCA of MNIST sample") + 
    geom_text(aes(label = label)) + 
    theme(legend.position = "none")


tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 2)  # modifying the default parameters


# Explore the tsne_output structure
str(tsne_output)

# Have a look at the first records from the t-SNE output
head(tsne_output$Y)

# Store the first two coordinates and the label in a data.frame
tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], 
                        label = as.factor(mnist_sample$label)
                        )

# Plot the t-SNE embedding using the true labels as color and shape
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = label)) + 
    ggtitle("T-Sne output") + 
    geom_text(aes(label = label)) + 
    theme(legend.position = "none")

```
  
  
  
***
  
Chapter 2 - Introduction to t-SNE  
  
Building a t-SNE Embedding:  
  
* The t-SNE method was published in 2008  
	* Non-linear dimensionality reduction technique  
    * Works well for most of the problems and is a very good method for visualizing high dimensional datasets  
    * Rather than keeping dissimilar points apart (like PCA) it keeps the low-dimensional representation of similar points together  
* Generally, the t-SNE method starts with PCA, then moves on to additional steps  
	* Use PCA to reduce the input dimensions into a small number  
    * Construct a probability distribution over pairs of original high dimensional records  
    * Define a similarity probability distribution of the points in the low-dimensional embedding  
    * Minimize the K-L divergence between the two distributions using gradient descent method  
* Can run t-SNE in R using the Rtsne package  
	* library(Rtsne)  
    * tsne_output <- Rtsne(mnist[, -1])  
    * tsne_output <- Rtsne(mnist[, -1], PCA = FALSE, dims = 3)  # modifying the default parameters  
    * tsne_output$itercosts  # K-L divergence cost, samples after 50 iterations  
    * head(tsne_output$costs)  # cost of each record after the final iteration  
  
Optimal Number of t-SNE Iterations:  
  
* Hyper-parameters are common in machine learning, and t-SNE has several of them  
	* Number of iterations  
    * Perplexity  
    * Learning rate  
    * Optimization criterium: K-L divergence  
* Because t-SNE is not deterministic, running with the same parameters can drive different results  
	* set.seed(1234)  
    * tsne_output_1 <- Rtsne(mnist[, -1], max_iter = 1500)   
    * set.seed(1234)  
    * tsne_output_2 <- Rtsne(mnist[, -1], max_iter = 1500)  
    * identical(tsne_output_1, tsne_output_2)  # TRUE  
* One of the important parameters for t-SNE is the number of iterations (default is 1000)  
  
Effect of Perplexity Parameter:  
  
* Perplexity is a hyper-parameter for balancing global and local criteria  
	* A guess about the number of close neighbors  
    * In a real setting is important to try different values  
    * Must be lower than the number of input records  
    * tsne_output <- Rtsne(mnist[, -1], perplexity = 50, max_iter = 1300)  
  
Classifying Digits with t-SNE:  
  
* Can classify digits (and run other tasks) using t-SNE  
	* Build a t-SNE model and calculate the centroids  
    * Classify unknown data points based on proximity to the centroid  
* Example for step 1 (building the t-SNE)  
	* tsne <- Rtsne(mnist_10k[, -1], perplexity = 5)  
    * tsne_plot <- data.frame(tsne_x= tsne_out$Y[1:5000,1], tsne_y = tsne_out$Y[1:5000,2], digit = as.factor(mnist_10k[1:5000,]$label))  
    * ggplot(tsne_plot, aes(x= tsne_x, y = tsne_y, color = digit)) + ggtitle("MNIST embedding of the first 5K digits") + geom_text(aes(label = digit)) + theme(legend.position="none")  
* Example for step 1b (calculating the centroids)  
	* centroids <- as.data.table(tsne_out$Y[1:5000,])  
    * setnames(centroids, c("X", "Y"))  
    * centroids[, label := as.factor(mnist_10k[1:5000,]$label)]  
    * centroids[, mean_X := mean(X), by = label]  
    * centroids[, mean_Y := mean(Y), by = label]  
    * centroids <- unique(centroids, by = "label")  
    * ggplot(centroids, aes(x= mean_X, y = mean_Y, color = label)) + ggtitle("Centroids coordinates") + geom_text(aes(label = label)) + theme(legend.position = "none")  
* Example for step 2 (classifying new digits)  
	* distances <- as.data.table(tsne_out$Y[5001:10000,])  
    * setnames(distances, c("X", "Y"))  
    * distances[, label := mnist_10k[5001:10000,]$label]  
    * distances <- distances[label == 4 | label == 9]  
    * distances[, dist_4:=sqrt(((X - centroids[label==4,]$mean_X) + (Y - centroids[label==4,]$mean_Y))^2)]  
    * ggplot(distances, aes(x=dist_4, fill = as.factor(label))) + geom_histogram(binwidth=5, alpha=.5, position="identity", show.legend = F)  
  
Example code includes:  
```{r}

# Compute t-SNE without doing the PCA step
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)

# Show the obtained embedding coordinates
head(tsne_output$Y)

# Store the first two coordinates and plot them 
tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], 
                        digit = as.factor(mnist_sample$label)
                        )

# Plot the coordinates
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("t-SNE of MNIST sample") + 
geom_text(aes(label = digit)) + 
theme(legend.position = "none")


# Inspect the output object's structure
str(tsne_output)

# Show the K-L divergence of each record after the final iteration
tsne_output$itercosts
tsne_output$costs

# Plot the K-L divergence of each record after the final iteration
plot(tsne_output$itercosts, type = "l")
plot(tsne_output$costs, type = "l")


# Generate a three-dimensional t-SNE embedding without PCA
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA=FALSE, dims=3)

# Generate a new t-SNE embedding with the same hyper-parameter values
tsne_output_new <- Rtsne::Rtsne(mnist_sample[, -1], PCA=FALSE, dims=3)

# Check if the two outputs are identical
identical(tsne_output, tsne_output_new)

# Generate a three-dimensional t-SNE embedding without PCA
set.seed(1234)
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)

# Generate a new t-SNE embedding with the same hyper-parameter values
set.seed(1234)
tsne_output_new <- Rtsne::Rtsne(mnist_sample[, -1], PCA = FALSE, dims = 3)

# Check if the two outputs are identical
identical(tsne_output, tsne_output_new)


# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with 2000 iterations
tsne_output <- Rtsne::Rtsne(mnist_sample[, -1], PCA=TRUE, dims=2, max_iter=2000)

# Observe the output costs 
tsne_output$itercosts

# Get the 50th iteration with the minimum K-L cost
which.min(tsne_output$itercosts)


# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with perplexity 5
tsne_output_5 <- Rtsne::Rtsne(mnist_sample[, -1], perplexity=5, max_iter=1200)

# Observe the returned K-L divergence costs at every 50th iteration
tsne_output_5$itercosts

# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with perplexity 20
tsne_output_20 <- Rtsne::Rtsne(mnist_sample[, -1], perplexity=20, max_iter=1200)

# Observe the returned K-L divergence costs at every 50th iteration
tsne_output_20$itercosts

# Set seed to ensure reproducible results
set.seed(1234)

# Execute a t-SNE with perplexity 50
tsne_output_50 <- Rtsne::Rtsne(mnist_sample[, -1], perplexity=50, max_iter=1200)

# Observe the returned K-L divergence costs at every 50th iteration
tsne_output_50$itercosts


# Observe the K-L divergence costs with perplexity 5 and 50
tsne_output_5$itercosts
tsne_output_50$itercosts

# Generate the data frame to visualize the embedding
tsne_plot_5 <- data.frame(tsne_x = tsne_output_5$Y[, 1], tsne_y = tsne_output_5$Y[, 2], digit = as.factor(mnist_sample$label))
tsne_plot_50 <- data.frame(tsne_x = tsne_output_50$Y[, 1], tsne_y = tsne_output_50$Y[, 2], digit = as.factor(mnist_sample$label))

# Plot the obtained embeddings
ggplot(tsne_plot_5, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("MNIST t-SNE with 1300 iter and Perplexity=5") + geom_text(aes(label = digit)) + 
theme(legend.position="none")
ggplot(tsne_plot_50, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("MNIST t-SNE with 1300 iter and Perplexity=50") + geom_text(aes(label = digit)) + 
theme(legend.position="none")


# Prepare the data.frame
tsne_plot <- data.frame(tsne_x = tsne_output_50$Y[1:100, 1], 
                        tsne_y = tsne_output_50$Y[1:100, 2], 
                        digit = as.factor(mnist_sample$label[1:100])
                        )

# Plot the obtained embedding
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = digit)) + 
ggtitle("MNIST embedding of the first 100 digits") + 
geom_text(aes(label = digit)) + 
theme(legend.position="none")


# Get the first 5K records and set the column names
dt_prototypes <- as.data.table(tsne_output_50$Y[1:100, ])
setnames(dt_prototypes, c("X","Y"))

# Paste the label column as factor
dt_prototypes[, label := as.factor(mnist_sample[1:100,]$label)]

# Compute the centroids per label
dt_prototypes[, mean_X := mean(X), by = label]
dt_prototypes[, mean_Y := mean(Y), by = label]

# Get the unique records per label
dt_prototypes <- unique(dt_prototypes, by = "label")
dt_prototypes


# Store the last 100 records in distances and set column names
distances <- as.data.table(tsne_output_50$Y[101:200, ])
setnames(distances, c("X", "Y"))

# Paste the true label
distances[, label := mnist_sample[101:200,]$label]

# Filter only those labels that are 1 or 0 
distances <- distances[label == 1 | label == 0]

# Compute Euclidean distance to prototype of digit 1
distances[, dist_1 := sqrt(( (X - dt_prototypes[label == 1,]$mean_X) + 
                             (Y - dt_prototypes[label == 1,]$mean_Y))^2)]


# Compute the basic statistics of distances from records of class 1
summary(distances[label == 1]$dist_1)

# Compute the basic statistics of distances from records of class 1
summary(distances[label == 0]$dist_1)

# Plot the histogram of distances of each class
ggplot(distances, aes(x = dist_1, fill = as.factor(label))) +
geom_histogram(binwidth = 5, alpha = .5, position = "identity", show.legend = FALSE) + 
ggtitle("Distribution of Euclidean distance 1 vs 0")

```
  
  
  
***
  
Chapter 3 - Using t-SNE with Predictive Models  
  
Credit Card Fraud Detection:  
  
* There are benefits of using t-SNE for feature engineering for further analysis  
	* Less correlation of input features  
    * Reduction in computation time  
* Database of European credit card farud available from 2013  
	* Released by Andrea Dal Pozzolo, et al. and available in Kaggle datasets  
    * Highly unbalanced: 492 fraud cases out of 248,807 (0.172%)  
    * Anonymized numerical features which are the result of a PCA  
    * 30 features plus the Class (1 fraud, 0 not-fraud)  
    * We only know the meaning of two features: time and amount of the transaction  
* Need to manage the class imbalances, for example by over-smapling the minority or under-sampling the majority  
	* set.seed(1234)  
    * idx <- sample(1:nrow(creditcard), nrow(creditcard)*.20)  
    * creditcard.test <- creditcard[idx]  
    * creditcard.train <- creditcard[!idx]  
    * creditcard.pos <- creditcard.train[Class==1]  
    * creditcard.neg <- creditcard.train[Class==0]  
    * creditcard.neg.bal <- creditcard.neg[sample(1:nrow(creditcard.neg), nrow(creditcard.pos))]  
    * creditcard.train <- rbind(creditcard.pos, creditcard.neg.bal)  
  
Training Random Forest Models:  
  
* The random forest model can help with classification (widely used method that does not require as much fine-tuning of parameters)  
	* randomForest the most common R package for random forests  
    * library(randomForest)  
    * train_x <- creditcard_train[, -31]  
    * train_y <- creditcard_train$Class  
    * rf_model <- randomForest(x = train_x, y = train_y, ntree = 100)  
* Can plot performance based on the number of trees  
	* plot(rf_model, main = "Error evolution vs number of trees")  
    * legend("topright", colnames(rf_model$err.rate),col=1:3,cex=0.8,fill=1:3)  
    * varImpPlot(rf_model, main = "Variable importance")  
  
Predicting Data:  
  
* Can make predictions using the random forest and evaluate the model  
	* prop.table(table(creditcard_test$Class))  
    * pred_rf <- predict(rf_model, creditcard_test, type = "prob")  
    * hist(pred_rf[, 2], main = "Histogram of predictions on the test set", xlab = "prediction value")  
* Can assess quality of predictions using AUC  
	* pred <- prediction(pred_rf[,2], creditcard_test$Class)  
    * perf <- performance(pred, measure = "auc")  
    * perf@y.values  
  
Visualizing Neural Network Layers:  
  
* Neural networks can also be used for classification, with layers of neurons producing a final output  
* There are many types of activation functions - sigmoid, tanh, relu, leaky relu, etc.  
* Visualizing the weights for each neuron in each layer can be valuable, and dimensionality reduction helps with that  
	* head(layer_128_train[, 1:7])  
    * summary(layer_128_train[, 1:4])  
    * tsne_nn_layer_train <- Rtsne(as.matrix(layer_128_train), perplexity = 50, max_iter = 400, check_duplicates = F, dims = 2, verbose = T)  
    * tsne_plot_train <- data.frame(tsne_x = tsne_nn_layer_train$Y[,1], tsne_y = tsne_nn_layer_train$Y[,2], y_col = creditcard_train$Class)  
    * ggplot(tsne_plot_train, aes(x = tsne_x, y = tsne_y, color = y_col)) + geom_point() + ggtitle("Credit card embedding 128 neurons layer") + theme(legend.position="none")  
  
Example code includes:  
```{r}

# Look at the data dimensions
dim(creditcard)

# Explore the column names
names(creditcard)

# Observe some records
str(creditcard)

# Generate a summary
# summary(creditcard)

# Plot a histogram of the transaction time
ggplot(creditcard, aes(x = Time)) + 
    geom_histogram()


# Extract positive and negative instances of fraud
creditcard_pos <- creditcard[Class == 1]
creditcard_neg <- creditcard[Class == 0]

# Fix the seed
set.seed(1234)

# Create a new negative balanced dataset by undersampling
creditcard_neg_bal <- creditcard_neg[sample(1:nrow(creditcard_neg), nrow(creditcard_pos)), ]

# Generate a balanced train set
creditcard_train <- rbind(creditcard_pos, creditcard_neg_bal)


# Fix the seed
set.seed(1234)

# Separate x and y sets
train_x <- creditcard_train[, -31]
train_y <- as.factor(creditcard_train$Class)

# Train a random forests
rf_model <- randomForest::randomForest(train_x, train_y, ntree=100)

# Plot the error evolution and variable importance
plot(rf_model)
randomForest::varImpPlot(rf_model)


# Set the seed
set.seed(1234)

# Generate the t-SNE embedding 
tsne_output <- Rtsne::Rtsne(as.matrix(creditcard_train[, -31]), check_duplicates = FALSE, PCA=FALSE)

# Generate a data frame to plot the result
tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1],
                        tsne_y = tsne_output$Y[, 2],
                        Class = creditcard_train$Class
                        )

# Plot the embedding usign ggplot and the label
ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + 
    ggtitle("t-SNE of credit card fraud train set") + 
    geom_text(aes(label = Class)) + theme(legend.position = "none")


# Fix the seed
set.seed(1234)

# Train a random forest
rf_model_tsne <- randomForest::randomForest(tsne_plot[, c("tsne_x", "tsne_y")], 
                                            as.factor(creditcard_train$Class), ntree=100
                                            )

# Plot the error evolution
plot(rf_model_tsne)

# Plot the variable importance
randomForest::varImpPlot(rf_model_tsne)


# Predict on the test set using the random forest 
# pred_rf <- predict(rf_model, creditcard_test, type = "prob")

# Plot a probability distibution of the target class
# hist(pred_rf[, 2])

# Compute the area under the curve
# pred <- prediction(pred_rf[, 2], creditcard_test$Class)
# perf <- performance(pred, measure = "auc") 
# perf@y.values


# Predict on the test set using the random forest generated with t-SNE features
# pred_rf <- predict(rf_model_tsne, test_x, type = "prob")

# Plot a probability distibution of the target class
# hist(pred_rf[, 2])

# Compute the area under the curve
# pred <- prediction(pred_rf[, 2], creditcard_test$Class)
# perf <- performance(pred, measure="auc") 
# perf@y.values


# Observe the dimensions
# dim(layer_128_train)

# Show the first six records of the last ten columns
# head(layer_128_train[, 118:128])

# Generate a summary of all columns
# summary(layer_128_train)


# Set the seed
# set.seed(1234)

# Generate the t-SNE
# tsne_output <- Rtsne(as.matrix(layer_128_train), check_duplicates=FALSE, max_iter=400, perplexity=50)

# Prepare data.frame
# tsne_plot <- data.frame(tsne_x = tsne_output$Y[, 1], tsne_y = tsne_output$Y[, 2], 
#                         Class = creditcard_train$Class
#                         )

# Plot the data 
# ggplot(tsne_plot, aes(x = tsne_x, y = tsne_y, color = Class)) + 
#     geom_point() + 
#     ggtitle("Credit card embedding of Last Neural Network Layer")

```
  
  
  
***
  
Chapter 4 - Generalized Low Rank Models  
  
Exploring Fashion MNIST dataset:  
  
* The fashion MNIST dataset contains 70,000 grayscale images at 28x28 pixels of 10 clothing categories  
	* Identical format to traditional MNIST  
    * Released by Zalando  
    * With the goal of replacing MNIST, because:  
    * MNIST is easy to predict  
    * MNIST is overused  
* Exploring and modeling with the fashion MNIST data  
	* class_names <- c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')  
    * xy_axis <- data.frame(x = expand.grid(1:28, 28:1)[,1], y = expand.grid(1:28, 28:1)[,2])  
    * plot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[1, -1]))[,1])  
    * plot_theme <- list( raster = geom_raster(hjust = 0, vjust = 0), gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = FALSE), theme = theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), panel.background = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.background = element_blank()) )  
    * ggplot(plot_data, aes(x, y, fill = fill)) + ggtitle(class_names[as.integer(fashion_mnist[1,1])+1]) + plot_theme  
  
Generalized Low Rank Models (GLRM):  
  
* There are many benefits to the GLRM approach, including  
	* Reduces the required storage  
    * Enables data visualization  
    * Removes noise  
    * Imputes missing data  
    * Simplifies data processing  
* The low-rank structure converts an mxn matrix to a combination of mxk and kxn  
	* Parallelized dimensionality reduction algorithm  
    * Categorical columns are transformed into binary columns  
* Can run GLRM in R with H2O  
	* H2O is an open source machine learning framework with R interfaces  
    * Has a good parallel implementation of GLRM  
    * Steps: (1) initialize the cluster and (2) store the input data  
    * h2o.init()  
    * fashion_mnist.hex <- as.h2o(fashion_mnist, "fashion_mnist.hex")  
    * model_glrm <- h2o.glrm(training_frame = fashion_mnist.hex, cols = 2:ncol(fashion_mnist), k = 2, max_iterations = 100)  # k is the rank (dimension) of the space  
  
Visualizing a GLRM Model:  
  
* Often helpful to extract the XY representation of the GLRM  
	* X <- as.data.table(h2o.getFrame(model_glrm@model$representation_name))  # dim will be nxk  
    * Y <- model_glrm@model$archetypes  # dim will be kxm  
    * ggplot(X, aes(x= Arch1, y = Arch2, color = fashion_mnist$label)) + ggtitle("Fashion Mnist GLRM Archetypes") + geom_text(aes(label = fashion_mnist$label)) + theme(legend.position="none")  
* Can grab the centroids by class, and use as prototypes for classification  
	* X[, label := as.numeric(fashion_mnist$label)]  
    * X[, mean_x := mean(Arch1), by = label]  
    * X[, mean_y := mean(Arch2), by = label]  
    * X_mean <- unique(X, by = "label")  
    * class_names = c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')  
    * ggplot(X_mean, aes(x = mean_x, y = mean_y, color = as.factor(X_mean$label))) + ggtitle("Fashion Mnist GLRM class centroids") + geom_text(aes(label = class_names[label])) + theme(legend.position="none")  
* Can reconstruct the original data using X*Y, with the predict method  
	* fashion_pred <- predict(model_glrm, fashion_mnist.hex)  
    * head(fashion_pred[1:2, 1:4])  
    * xy_axis <- data.frame(x = expand.grid(1:28,28:1)[,1], y = expand.grid(1:28,28:1)[,2])  
    * data_reconstructed <- cbind(xy_axis, fill = as.data.frame(t(fashion_pred[1000,]))[,1])  
    * plot_reconstructed <- ggplot(plot_data, aes(x, y, fill = fill)) + ggtitle("Reconstructed Pullover (K=2)") + plot_theme  
    * data_original <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[1000, -1]))[,1])  
    * plot_original <- ggplot(plot_data_2, aes(x, y, fill = fill)) + ggtitle("Original Pullover") + plot_theme  
    * grid.arrange(plot_reconstructed, plot_original, nrow = 1)  
* Higher values of k will typically lead to better reconstructions - assessed visually or by using the resulting error values  
  
Dealing with Missing Data and Speeding-Up Models:  
  
* Missing data can confound the analysis, and a good approach to it is required  
* Common in real-world datasets  
	* May be Intentionally not provided  
    * May be Due to an error  
    * With GLRM we can impute missing data and assign an estimation  
* Example of randomly generated missing data using the fashion_mnist dataset  
	* fashion_mnist_miss.hex <- h2o.insertMissingValues(fashion_mnist.hex[,-1], fraction=0.2, seed = 1234)  
    * summary(fashion_mnist_miss[,781:784])  
    * model_glrm <- h2o.glrm(training_frame = fashion_mnist_miss.hex, transform = "NORMALIZE", ignore_const_cols = FALSE, k = 64, max_iterations = 200, seed = 123)  
    * fashion_pred <- h2o.predict(model_glrm, fashion_mnist_miss.hex)  
    * summary(fashion_pred[,782:784])  
* Another advantage of GLRM is to speed up training models (lower volumes of data for processing)  
	* Training machine learning models is faster using a low-dimensional representation  
    * Key is to have a good compressed representation  
* Example of training a random forest  
	* Trained several h2o random forests, 4-Fold Cross-Validation  
    * Fashion MNIST (60.000) was compressed with GLRM and changing the value of K from 2 to 256  
    * We measure the accuracy and the required time  
    * perf_metrics  
  
Summary and Wrap-Up:  
  
* Can use t-SNE and GLRM to reduce dimensionality  
* Simplify data processing, enhace visualizing data, etc.  
  
Example code includes:  
```{r}

# Show the dimensions
dim(fashion_mnist)

# Create a summary of the last five columns 
summary(fashion_mnist[, 780:785])

# Table with the class distribution
table(fashion_mnist$label)


xy_axis <- data.frame(x=rep(1:28, times=28), y=rep(28:1, each=28))
plot_theme <- list( raster = geom_raster(hjust = 0, vjust = 0), 
                    gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = FALSE), 
                    theme = theme(axis.line = element_blank(), axis.text = element_blank(), 
                                  axis.ticks = element_blank(), axis.title = element_blank(),
                                  panel.background = element_blank(), panel.border = element_blank(),
                                  panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                                  plot.background = element_blank()
                                  )
                    )  
class_names <- c('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
                 )  

# Get the data from the last image
plot_data <- cbind(xy_axis, fill = as.data.frame(t(fashion_mnist[500, -1]))[,1])

# Observe the first records
head(plot_data)

# Plot the image using ggplot()
ggplot(plot_data, aes(x, y, fill = fill)) + 
    ggtitle(class_names[as.integer(fashion_mnist[500, 1])]) + 
    plot_theme 


# Start a connection with the h2o cluster
h2o::h2o.init()

# Store the data into h2o cluster
fashion_mnist.hex <- h2o::as.h2o(fashion_mnist, "fashion_mnist.hex")

# Launch a GLRM model over fashion_mnist data
model_glrm <- h2o::h2o.glrm(training_frame = fashion_mnist.hex, cols = 2:ncol(fashion_mnist), 
                            k = 2, seed = 123, max_iterations = 100
                            )

# Plotting the convergence
plot(model_glrm)


# Launch a GLRM model with normalized fashion_mnist data  
model_glrm <- h2o::h2o.glrm(training_frame = fashion_mnist.hex, transform = "NORMALIZE",
                            cols = 2:ncol(fashion_mnist), k = 2, seed = 123, max_iterations = 100
                            )

# Plotting the convergence
plot(model_glrm)


X_matrix <- as.data.table(h2o::h2o.getFrame(model_glrm@model$representation_name))

# Dimension of X_matrix
dim(X_matrix)

# First records of X_matrix
head(X_matrix)

# Plot the records in the new two dimensional space
ggplot(as.data.table(X_matrix), aes(x= Arch1, y = Arch2, color = fashion_mnist$label)) + 
    ggtitle("Fashion Mnist GLRM Archetypes") + 
	geom_text(aes(label = fashion_mnist$label)) + 
	theme(legend.position="none")


# Store the label of each record and compute the centroids
X_matrix[, label := as.numeric(fashion_mnist$label)]
X_matrix[, mean_x := mean(Arch1), by = label]
X_matrix[, mean_y := mean(Arch2), by = label]

# Get one record per label and create a vector with class names
X_mean <- unique(X_matrix, by = "label")
label_names <- c("T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", 
                 "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
                 )

# Plot the centroids
ggplot(X_mean, aes(x = mean_x, y = mean_y, color = as.factor(label))) + 
	ggtitle("Fashion Mnist GLRM class centroids") + 
	geom_text(aes(label = label_names[label])) +
	theme(legend.position="none")


makeNA <- function(x) {
    vecNA <- sort(unique(sample(1:length(x), round(0.225*length(x)), replace=TRUE)))
    x[vecNA] <- NA
    return(x)
}

fashion_mnist_miss <- fashion_mnist %>%
    select(-label) %>%
    apply(1, FUN=makeNA)


# Store the input data in h2o
fashion_mnist_miss.hex <- h2o::as.h2o(fashion_mnist_miss, "fashion_mnist_miss.hex")

# Build a GLRM model
model_glrm <- h2o::h2o.glrm(training_frame = fashion_mnist_miss.hex, transform="NORMALIZE", 
                            k=2, max_iterations=100
                            )

# Impute missing values
fashion_pred <- h2o::h2o.predict(model_glrm, fashion_mnist_miss.hex)

# Observe the statistics of the first 5 pixels
summary(fashion_pred[, 1:5])


# Get the starting timestamp
time_start <- proc.time()

# Train the random forest
rf_model <- randomForest::randomForest(x = fashion_mnist[, -1], y = fashion_mnist$label, ntree = 20)

# Get the end timestamp
time_end <- timetaken(time_start)

# Show the error and the time
rf_model$err.rate[20]
time_end


# Get the starting timestamp
# time_start <- proc.time()

# Train the random forest
# rf_model <- randomForest(x = train_x, y = train_y, ntree = 500)

# Get the end timestamp
# time_end <- timetaken(time_start)

# Show the error and the time
# rf_model$err.rate[500]
# time_end

```
  
  
  
***
  
### _Optimizing R Code with Rcpp_  
  
Chapter 1 - Introduction  
  
Introduction:  
  
* R is an interpretative language which can lead to slow run-times  
* C++ is a compiled language, making it much faster at the expense of requiring compiling (harder to learn and write)  
* The Rcpp package simplifies the process of using C++ from R  
	* Introduction - basic C++ syntax  
    * C++ functions and control flow  
    * Vector classes  
    * Case studies  
* Can use the library(microbenchmark) to help see the processing times of various code snippets  
	* library(microbenchmark)  
    * x <- rnorm(1e6)  
    * microbenchmark( slowmax(x), max(x) )  # slowmax was written in a very inefficient manner  
* Since it is a compiled language, C++ typically does not have console capbilities, though Rcpp build in some helper functions for this  
	* evalCpp( "40 + 2" )  
    * evalCpp( "exp(1.0)" )  
    * evalCpp( "std::numeric_limits<int>::max()")  
* Basic number types differ between R and C++  
	* Literal numbers are doubles in R, and require the L to cast as integers (32 is a double, 32L is an integer)  
    * Literal numbers are integers in C++, and require a trailing .0 to cast as doubles (32 is an integer, 32.0 is a double)  
* Can explicitly cast numbers between double and integer in C++  
	* y <- evalCpp( "(double)(40 + 2)" )  
    * evalCpp( "13 / 4" )  # Integer division, will result in 3  
    * evalCpp( "(double)13 / 4" )  # Casted to float division if either operand is a double, will result in 3.25  
  
Inline Functions with cppFunction:  
  
* Can define C++ functions using Rcpp, either scripted or using the R console  
	* library(Rcpp)  
    * cppFunction("int fun(){  
    *     int x = 37 ;  
    *     return x ;  
    * }" )  
    * fun()  
* There are many languages of engineering that happen automatically behind the scenes using Rcpp  
* Variables in C++ are statically typed (contrast to R which is dynamically typed), meaning they may not undergo a change of type at any time  
	* Functions must declare the types of all inputs and outputs, allowing the compiler to optimize code  
* Example functions using Rcpp  
	* cppFunction("double add( double x, double y){  
    *     double res = x + y ;  
    *     return res ;  
    * }  
    * )  
    * add( 30, 12 )  
    * See below for the equivalent R code  
    * addr <- function(x, y) {  
    *     res <- x + y  
    *     res  
    * }  
  
Debugging:  
  
* Light debugging includes print outs as the loop runs or message printing at key points  
* The Rprintf() functions in Rcpp allows for printing to the screen  
	* cppFunction( 'int fun(){  
    *     // Some values  
    *     int x = 42 ;  
    *     // Printing a message to the R console  
    *     Rprintf( "some message in the console, x=%d\\n", x ) ;  
    *     // Return some int  
    * return 76 ;  
    * }  
    * ')  
* Integer placeholders are %d while string placeholders are %s  
* Error handling in C++ allows for checking that key parameters are inside a defined key range  
	* cppFunction( 'int fun(int x){  
    *     // A simple error message  
    *     if( x < 0 ) stop( "sorry x should be positive" ) ;  
    *     // A formatted error message  
    *     if( x > 20 ) stop( "x is too big (x=%d)", x ) ;  
    *     // Return some int  
    * return x ;  
    * }')  
  
Example code includes:  
```{r eval=FALSE}

# Load microbenchmark
library(microbenchmark)
library(Rcpp)


# Define the function sum_loop
sum_loop <- function(x) {
  result <- 0
  for (i in x) result <- result + i
  result
}

x <- rnorm(100000)

# Check for equality 
all.equal(sum_loop(x), sum(x))

# Compare the performance
microbenchmark(sum_loop = sum_loop(x), R_sum = sum(x))


# Evaluate 2 + 2 in C++
x <- evalCpp("2+2")

# Evaluate 2 + 2 in R
y <- 2+2

# Storage modes of x and y
storage.mode(x)
storage.mode(y)

# Change the C++ expression so that it returns a double
z <- evalCpp("2.0 + 2")


# Evaluate 17 / 2 in C++
evalCpp("17/2")

# Cast 17 to a double and divide by 2
evalCpp("(double)17/2")

# Cast 56.3 to an int
evalCpp("(int)56.3")


# Define the function the_answer()
cppFunction('
  int the_answer() {
    return 42 ;
  }
')

# Check the_answer() returns the integer 42
the_answer() == 42L


# Define the function euclidean_distance()
cppFunction('
  double euclidean_distance(double x, double y) {
    return sqrt(x*x + y*y) ;
  }
')

# Calculate the euclidean distance
euclidean_distance(1.5, 2.5)


# Define the function add()
cppFunction('
  int add(int x, int y) {
    int res = x + y ;
    Rprintf("** %d + %d = %d\\n", x, y, res) ;
    return res ;
  }
')

# Call add() to print THE answer
add(40, 2)


cppFunction('
  // adds x and y, but only if they are positive
  int add_positive_numbers(int x, int y) {
      // if x is negative, stop
      if( x < 0 ) stop("x is negative") ;
    
      // if y is negative, stop
      if( y < 0 ) stop("y is negative") ;
     
      return x + y ;
  }
')

# Call the function with positive numbers
add_positive_numbers(2, 3)

# Call the function with a negative number
add_positive_numbers(-2, 3)

```
  
  
  
***
  
Chapter 2 - Functions and Control Flow  
  
C++ Functions Belong to C++ Files:  
  
* Can use .cpp files to save and source C++ functions  
	* sourceCpp( "code.cpp" )  
    * timesTwo( 21 )  
* To write a .cpp file, the following structure should be included  
	* #include <Rcpp.h>  
    * using namespace Rcpp ;  
    * // [[Rcpp::export]]  
    * int timesTwo( int x ){  
    *     return 2*x ;  
    * }  
* Note that the return requires a semicolon afterwards  
* Note that // [[Rcpp::export]] is a comment to C++ but is also picked up as meaningfull by Rcpp (defines that the following functions should be exported)  
  
Writing Functions in C++:  
  
* Only the exported functions by way of //[[Rcpp::export]] are available to R; the others are internal (private) to the C++ session  
* Can run single-line comments using // and multi-line comments using /* */  
* Rcpp includes a special comment that embeds R code to the C++ file  
	* /*** R <insert R code /*  
* The if-else syntax for Rcpp is very similar to the base R syntax  
	* if( condition ){  
    * // code if true  
    * } else {  
    * // code otherwise  
    * }  
* Can also have a void function that is called only for side effects such as printing  
	* // [[Rcpp::export]]  
    * void info( double x){  
    *     if( x < 0 ){  
    *         Rprintf( "x is negative" ) ;  
    *     } else if( x == 0 ){  
    *         Rprintf( "x is zero" ) ;  
    *     } else if( x > 0 ){  
    *         Rprintf( "x is positive" ) ;  
    *     } else {  
    *         Rprintf( "x is not a number" ) ;  
    *     }  
    * }  
  
For Loops:  
  
* There are four components to a C++ for loop, and the process has meaningful differences from R for loops  
	* Initialization - happens once, at the very beginning of the for loop  
    * Continue condition - Logical condition to control if the loop continues  
    * Increment - Executed at the end of each iteration (often to add 1 to an index)  
    * Body - statements to be executed at each iteration  
* Example of a very typicaly for loop using C++ (note that i++ is shorthand for "increment I by 1"  
	* for (int i=0; i<n; i++ ){  
    *     // some code using i  
    * }  
* Example function to calculate the sum of the first n integers  
	* // [[Rcpp::export]]  
    * int nfirst( int n ){  
    *     if( n < 0 ) {  
    *         stop( "n must be positive, I see n=%d", n ) ;
    *     }  
    *     int result = 0 ;  
    *     for( int i=0; i<n; i++){  
    *         if( i == 13 ){  
    *             Rprintf( "I cannot handle that, I am superstitious" ) ;  
    *             break ;  
    *         }  
    *         result = result + (i+1) ;  
    *     }  
    *     return result ;  
    * }  
* Example of the iterative approach from Newton to calculating square roots (see example code below)  
  
While Loops:  
  
* While loop is conceptually simpler, and there is only a continue condition and a body of the function  
* The for loop from above can be conceptually re-written as a while loop  
	* init  
    * while( condition ){  
    *     body  
    *     increment  
    * }  
* Can also run the do-while loop, where the condition comes after the body (particularly useful when the body must be run at least once)  
	* do {  
    *     body  
    * } while( condition ) ;  
  
Example code includes:  
```{r eval=FALSE}

# file should be included as 'script.cpp')
# file called as sourceCpp('script.cpp')


#include <Rcpp.h>
using namespace Rcpp ; 

// Export the function to R
//[[Rcpp::export]]
double twice(double x) {
    // Fix the syntax error
    return x+x;
}


// Include the Rcpp.h header
#include <Rcpp.h>

// Use the Rcpp namespace
using namespace Rcpp;

// [[Rcpp::export]]
int the_answer() {
    // Return 42
    return 42;
}

/*** R
# Call the_answer() to check you get the right result
the_answer()
*/


#include <Rcpp.h>
using namespace Rcpp; 

// Make square() accept and return a double
double square(double x) {
  // Return x times x
  return x*x ;
}

// [[Rcpp::export]]
double dist(double x, double y) {
  // Change this to use square()
  return sqrt(square(x) + square(y));
}


#include <Rcpp.h>
using namespace Rcpp; 

double square(double x) {
  return x * x ;
}

// [[Rcpp::export]]
double dist(double x, double y) {
  return sqrt(square(x) + square(y));
}

// Start the Rcpp R comment block
/*** R
# Call dist() to the point (3, 4)
dist(3, 4)
# Close the Rcpp R comment block
*/


#include <Rcpp.h>
using namespace Rcpp ;

// [[Rcpp::export]]
double absolute(double x) {
  // Test for x greater than zero
  if(x > 0) {
    // Return x
    return x; 
  // Otherwise
  } else {
    // Return negative x
    return -x;
  }
}

/*** R  
absolute(pi)
absolute(-3)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sqrt_approx(double value, int n) {
    // Initialize x to be one
    double x = 1;
    // Specify the for loop
    for(int i = 0; i < n; i++) {
        x = (x + value / x) / 2.0;
    }
    return x;
}

/*** R
sqrt_approx(2, 10)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List sqrt_approx(double value, int n, double threshold) {
    double x = 1.0;
    double previous = x;
    bool is_good_enough = false;
    int i;
    for(i = 0; i < n; i++) {
        previous = x;
        x = (x + value / x) / 2.0;
        is_good_enough = fabs(previous - x) < threshold;
        
        // If the solution is good enough, then "break"
        if(is_good_enough) break;
    }
    return List::create(_["i"] = i , _["x"] = x);
}

/*** R
sqrt_approx(2, 1000, 0.1)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sqrt_approx(double value, double threshold) {
    double x = 1.0;
    double previous = x;
    bool is_good_enough = false;
    
    // Specify the while loop
    while(is_good_enough == false) {
        previous = x;
        x = (x + value / x) / 2.0;
        is_good_enough = fabs(x - previous) < threshold;
    }
    
    return x ;
}

/*** R
sqrt_approx(2, 0.00001)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sqrt_approx(double value, double threshold) {
    double x = 1.0;
    double previous = x;
    bool is_good_enough = false;
    
    // Initiate do while loop
    do {
        previous = x;
        x = (x + value / x) / 2.0;
        is_good_enough = fabs(x - previous) < threshold;
    // Specify while condition
    } while (is_good_enough == false);
    
    return x;
}

/*** R
sqrt_approx(2, 0.00001)
*/

```
  
  
  
***
  
Chapter 3 - Vector Classes  
  
Rcpp Classes and Vectors:  
  
* Rcpp defines a number of C++ classes that greatly simplify processing and interfacing between C++ and R  
	* NumericVector to manipulate numeric vectors, e.g. c(1,2,3)  
    * IntegerVector for integer e.g. 1:3  
    * LogicalVector for logical e.g. c(TRUE, FALSE)  
    * CharacterVector for strings e.g. c("a", "b", "c")  
    * List for lists, aka vectors of arbitrary R objects  
* There is an API for vectors, which allows for running some key methods  
	* x.size() gives the number of elements of the vector x  
    * x[i] gives the element on the ith position in the vector x  
    * Indexing in C++ starts at 0. The index is an offset to the first position  
* Example pseudo-code for looping around a vector  
	* // x comes from somewhere  
    * NumericVector x = ... ;   
    * int n = x.size() ;  
    * for( int i=0; i<n; i++){  
    *     // manipulate x[i]  
    * }  
  
Creating Vectors:  
  
* Exported Rcpp classes are meant to be called from R  
* Because C++ is a type-sepcific language, Rcpp attempts to coerce types to the needed type prior to passing the data to C++  
* Example for creating a vector of a given size in Rcpp (numeric vectors are initialized to 0 and string vectors are initialized to blank strings)  
	* // [[Rcpp::export]]  
    * NumericVector ones(int n){  
    *     // create a new numeric vector of size n  
    *     NumericVector x(n) ;  
    *     // manipulate it  
    *     for( int i=0; i<n; i++){  
    *         x[i] = 1 ;  
    *     }  
    *     return x ;  
    * }  
* Can override the default initialization values for a new vector by passing an additional argument  
	* double value = 42.0 ;  
    * int n = 20 ;  
    * // create a numeric vector of size 20  
    * // with all values set to 42  
    * NumericVector x( n, value ) ;  
* Can use a static method to initialize the class (???)  
	* NumericVector x = NumericVector::create( 1, 2, 3 ) ;  
    * CharacterVector s = CharacterVector::create( "pink", "blue" ) ;  
    * NumericVector x = NumericVector::create( _["a"] = 1, _["b"] = 2, _["c"] = 3 ) ;  
    * IntegerVector y = IntegerVector::create( _["d"] = 4, 5, 6, _["f"] = 7 ) ;  
* Can also clone vectors to avoid changing the original vector (creates a "deep copy")  
	* // [[Rcpp::export]]  
    * NumericVector positives( NumericVector x ){  
    *     // clone x into y  
    *     NumericVector y = clone(x) ;  
    *     for( int i=0; i< y.size(); i++){  
    *         if( y[i] < 0 ) y[i] = 0 ;  
    *     }  
    *     return y ;  
    * }  
  
Weighted Mean:  
  
* Example for using Rcpp to run the weighted means  
	* weighted_mean_R <- function(x, w){ sum(x*w) / sum(w) }  # R code version  
* Example of very inefficient R code  
	* weighted_mean_loop <- function(x, w){  
    *     total_xw <- 0  
    *     total_w  <- 0  
    *     for( i in seq_along(x)){  
    *         total_xw <- total_xw + x[i]*w[i]  
    *         total_w  <- total_w  + w[i]  
    *     }  
    *     total_xw / total_w  
    * }  
* Translation to C++ code using Rcpp  
	* // [[Rcpp::export]]  
    * double weighted_mean_cpp( NumericVector x, NumericVector w){  
    *     double total_xw = 0.0 ;  
    *     double total_w  = 0.0 ;  
    *     int n = ___ ;  
    *     for( ___ ; ___ ; ___ ){  
    *         // accumulate into total_xw and total_w  
    *     }  
    *     return total_xw / total_w ;  
    * }  
* Missing values need to be tested based on special functions (similar to R)  
	* Each type of vector has its own special missing values  
    * bool test = NumericVector::is_na(x) ;  
    * double y = NumericVector::get_na() ;  // The representation of NA in double  
  
Vectors From the STL:  
  
* Rcpp vectors are thin wrappers around R vectors  
	* Cannot (cost effectively) change size: data copy every time  
* STL (Standard Template Library) vectors are independent of R vectors  
	* Cheap to grow and shrink: amortized copies  
* Generally, growing vectors is very expensive and should be avoided (in R or C++)  
	* Can even be more efficient to have two functions; first finds the vector size, second creates vector of that size and then fills it  
  
Example code includes:  
```{r eval=FALSE}

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double first_plus_last(NumericVector x) {
    // The size of x
    int n = x.size();
    // The first element of x
    double first = x[0];
    // The last element of x
    double last = x[n-1];
    return first + last;
}

/*** R
x <- c(6, 28, 496, 8128)
first_plus_last(x)
# Does the function give the same answer as R?
all.equal(first_plus_last(x), x[1] + x[4])
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double sum_cpp(NumericVector x) {
  // The size of x
  int n = x.size();
  // Initialize the result
  double result = 0;
  // Complete the loop specification
  for(int i = 0; i<n; i++) {
    // Add the next value
    result = result + x[i];
  }
  return result;
}

/*** R
set.seed(42)
x <- rnorm(1e6)
sum_cpp(x)
# Does the function give the same answer as R's sum() function?
all.equal(sum_cpp(x), sum(x))
*/


#include <Rcpp.h>
using namespace Rcpp;

// Set the return type to IntegerVector
// [[Rcpp::export]]
IntegerVector seq_cpp(int lo, int hi) {
  int n = hi - lo + 1;
    
  // Create a new integer vector, sequence, of size n
  IntegerVector sequence(n);
    
  for(int i = 0; i < n; i++) {
    // Set the ith element of sequence to lo plus i
    sequence[i] = lo + i;
  }
  
  return sequence;
}

/*** R
lo <- -2
hi <- 5
seq_cpp(lo, hi)
# Does it give the same answer as R's seq() function?
all.equal(seq_cpp(lo, hi), seq(lo, hi))
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List create_vectors() {
  // Create an unnamed character vector
  CharacterVector polygons = CharacterVector::create("triangle", "square", "pentagon");
  // Create a named integer vector
  IntegerVector mersenne_primes = IntegerVector::create(_["first"] = 3, _["second"] = 7, _["third"] = 31);
  // Create a named list
  List both = List::create(_["polygons"] = polygons, _["mersenne_primes"] = mersenne_primes);
  return both;
}

/*** R
create_vectors()
*/


# Unlike R, C++ uses a copy by reference system, meaning that if you copy a variable then make changes to the copy, the changes will also take place in the original.
# To prevent this behavior, you have to use the clone() function to copy the underlying data from the original variable into the new variable
# The syntax is y = clone(x). In this exercise, we have defined two functions for you:
# change_negatives_to_zero(): Takes a numeric vector, modifies by replacing negative numbers with zero, then returns both the original vector and the copy.
# change_negatives_to_zero_with_cloning(): Does the same thing as above, but clones the original vector before modifying it.

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
List change_negatives_to_zero(NumericVector the_original) {
  // Set the copy to the original
  NumericVector the_copy = the_original;
  int n = the_original.size();
  for(int i = 0; i < n; i++) {
    if(the_copy[i] < 0) the_copy[i] = 0;
  }
  return List::create(_["the_original"] = the_original, _["the_copy"] = the_copy);
}

// [[Rcpp::export]]
List change_negatives_to_zero_with_cloning(NumericVector the_original) {
  // Clone the original to make the copy
  NumericVector the_copy = clone(the_original);
  int n = the_original.size();
  for(int i = 0; i < n; i++) {
    if(the_copy[i] < 0) the_copy[i] = 0;
  }
  return List::create(_["the_original"] = the_original, _["the_copy"] = the_copy);
}

/*** R
x <- c(0, -4, 1, -2, 2, 4, -3, -1, 3)
change_negatives_to_zero_with_cloning(x)
change_negatives_to_zero(x)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double weighted_mean_cpp(NumericVector x, NumericVector w) {
  // Initialize these to zero
  double total_w = 0.0;
  double total_xw = 0.0;
  
  // Set n to the size of x
  int n = x.size();
  
  // Specify the for loop arguments
  for(int i = 0; i<n; i++) {
    // Add ith weight
    total_w += w[i];
    // Add the ith data value times the ith weight
    total_xw += w[i]*x[i];
  }
  
  // Return the total product divided by the total weight
  return total_xw / total_w;
}

/*** R 
x <- c(0, 1, 3, 6, 2, 7, 13, 20, 12, 21, 11)
w <- 1 / seq_along(x)
weighted_mean_cpp(x, w)
# Does the function give the same results as R's weighted.mean() function?
all.equal(weighted_mean_cpp(x, w), weighted.mean(x, w))
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
double weighted_mean_cpp(NumericVector x, NumericVector w) {
  double total_w = 0;
  double total_xw = 0;
  
  int n = x.size();
  
  for(int i = 0; i < n; i++) {
    // If the ith element of x or w is NA then return NA
    if (NumericVector::is_na(x[i]) | NumericVector::is_na(w[i])) return NumericVector::get_na();
    total_w += w[i];
    total_xw += x[i] * w[i];
  }
  
  return total_xw / total_w;
}

/*** R 
x <- c(0, 1, 3, 6, 2, 7, 13, NA, 12, 21, 11)
w <- 1 / seq_along(x)
weighted_mean_cpp(x, w)
*/


NumericVector bad_select_positive_values_cpp(NumericVector x) {
  NumericVector positive_x(0);
  for(int i = 0; i < x.size(); i++) {
    if(x[i] > 0) {
      positive_x.push_back(x[i]);
    }
  }
  return positive_x;
}

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector good_select_positive_values_cpp(NumericVector x) {
  int n_elements = x.size();
  int n_positive_elements = 0;
  
  // Calculate the size of the output
  for(int i = 0; i < n_elements; i++) {
    // If the ith element of x is positive
    if(x[i] > 0) {
      // Add 1 to n_positive_elements
      n_positive_elements++;
    }
  }
  
  // Allocate a vector of size n_positive_elements
  NumericVector positive_x(n_positive_elements);
  
  // Fill the vector
  int j = 0;
  for(int i = 0; i < n_elements; i++) {
    // If the ith element of x is positive
    if(x[i] > 0) {
      // Set the jth element of positive_x to the ith element of x
      positive_x[j] = x[i];
      // Add 1 to j
      j++;
    }
  }
  return positive_x;
}

/*** R
set.seed(42)
x <- rnorm(1e4)
# Does it give the same answer as R?
all.equal(good_select_positive_values_cpp(x), x[x > 0])
# Which is faster?
microbenchmark(
  bad_cpp = bad_select_positive_values_cpp(x),
  good_cpp = good_select_positive_values_cpp(x)
)
*/


# The standard template library (stl) is a C++ library containing flexible algorithms and data structures
# For example, the double vector from the stl is like a "native C++" equivalent of Rcpp's NumericVector
# The following code creates a standard double vector named x with ten elements
std::vector<double> x(10);
# Usually it makes more sense to stick with Rcpp vector types because it gives you access to many convenience methods that work like their R equivalents, including mean(), round(), and abs()
# However, the stl vectors have an advantage that they can dynamically change size without paying for data copy every time


#include <Rcpp.h>
using namespace Rcpp;

// Set the return type to a standard double vector
// [[Rcpp::export]]
std::vector<double> select_positive_values_std(NumericVector x) {
  int n = x.size();
  
  // Create positive_x, a standard double vector
  std::vector<double> positive_x(0);
    
  for(int i = 0; i < n; i++) {
    if(x[i] > 0) {
      // Append the ith element of x to positive_x
      positive_x.push_back(x[i]);
    }
  }
  return positive_x;
}

/*** R
set.seed(42)
x <- rnorm(1e6)
# Does it give the same answer as R?
all.equal(select_positive_values_std(x), x[x > 0])
# Which is faster?
microbenchmark(
  good_cpp = good_select_positive_values_cpp(x),
  std = select_positive_values_std(x)
)
*/

```
  
  
  
***
  
Chapter 4 - Case Studies  
  
Random Number Generation:  
  
* Can use functions from the R namespace for features like drawing random numbers from a distribution  
	* // one number from a N(0,1) - function only gives a single number  
    * double x = R::rnorm( 0, 1 ) ;  
* Can also use the functions of the same name available in the Rcpp namespace - Rcpp::rnorm() and the like  
* Example for creating numbers from a truncated distribution (e.g., a normal with only positive values) - rejected sampling  
	* // we generate n numbers  
    * NumericVector x(n) ;  
    * // fill the vector in a loop  
    * for( int i=0; i<n; i++){  
    *     // keep generating d until it gets positive  
    *     double d  ;  
    *     do {  
    *         d = ... ;  
    *     } while( d < 0 ) ;  
    * x[i] = d ;  
    * }  
* Example for creating numbers from a mixture of distributions  
	* int component( NumericVector weights, double total_weight ){  
    * // return the index of the selected component  
    * }  
    * NumericVector rmix( int n, NumericVector weights, NumericVector means, NumericVector sds ){  
    *     NumericVector res(n) ;  
    *     for( int i=0; i<n; i++){  
    *         // find which component to use  
    *         ...  
    *         // simulate using the mean and sd from the selected component  
    *         ...  
    *     }  
    *     return res ;  
    * }  
  
Rolling Operations:  
  
* Rolling means are vectors where the value at position j is the mean of the next n elements  
	* The process runs much faster if just the first element is deleted and then the last element is added  
* Using C++ code can help make processes like these easy to read while maintaining very strong performance  
* Example of running last observation carried forward (LOCF)  
	* # iterative version (can be converted to C++ code)  
    * na_meancf2 <- function(x){  
    *     total <- 0  
    *     n <- 0  
    *     for( i in seq_along(x) ){  
    *         if( is.na(x[i]) ){  
    *             x[i] <- total / n  
    *         } else {  
    *             total <- x[i] + total  
    *             n <- n + 1  
    *         }  
    *     }  
    * }  
  
Auto-Regressive Model:  
  
* Example for AR code in R (works well, but is inefficient)  
	* ar <- function(n, phi, sd){  
    *     x <- epsilon <- rnorm(n, sd = sd)  
    *     np <- length(phi)  
    *     for( i in seq(np+1, n)){  
    *         x[i] <- sum(x[seq(i-1, i-np)] * phi) + epsilon[i]  
    *     }  
    *     x  
    * }  
* Example skeleton for writing the same code using C++  
	* NumericVector x(n) ;  
    * // initial loop  
    * for( ___ ; __ < np ; ___ ){  
    *     x[i] = R::rnorm(___) ;  
    * }  
    * // outer loop  
    * for( ___ ; ___ ; ___ ){  
    *     double value = rnorm(___) ;  
    *     // inner loop  
    *     for( ___ ; ___ ; ___ ){  
    *         value += ___ ;  
    *     }  
    *     x[i] = value ;  
    * }  
* Similar example for the R code for an MA (moving average)  
	* ma <- function(n, theta, sd){  
    *     epsilon <- rnorm(n, sd = sd)  
    *     x <- numeric(n)  
    *     nq <- length(theta)  
    *     for( i in seq(nq+1, n)){  
    *         x[i] <- sum(epsilon[seq(i-1, i-nq)] * theta) + epsilon[i]  
    *     }  
    *     x  
    * }  
* Example of running the MA (skeleton) using C++  
	* #include <Rcpp.h>  
    * using namespace Rcpp ;  
    * // [[Rcpp::export]]  
    * NumericVector ma( int n, double mu, NumericVector theta, double sd ){  
    *     int nq = theta.size() ;  
    *     // generate the noise vector at once  
    *     // using the Rcpp::rnorm function, similar to the R function  
    *     NumericVector eps = Rcpp::rnorm(n, 0.0, sd) ;  
    *     // init the output vector of size n with all 0.0  
    *     NumericVector x(___) ;  
    *     // start filling the values at index nq + 1  
    *     for( int i=nq+1; i<n; i++){  
    *         ____  
    *     }  
    *     return x ;  
    * }  
* Can also combine AR-MA in to an ARMA model  
  
Wrap Up:  
  
* The Rcpp package combines the ease of R syntax with the speed and efficiency of C++  
* Vectorized code is basically a loop in a compiled language such as C++  
* Recap of key topics from this course - for loops in C++ are an extremely common use case  
	* evalCpp and cppFunction  
    * for loops in C++ - for( init ; condition ; increment ){ body }  
    * Vectors, including that vector indexing starts at 0  
* C++ files with Rcpp  
	* #include <Rcpp.h>  
    * using namespace Rcpp ;  
    * // [[Rcpp::export]]  
    * double add( double x, double y){  
    *     return x + y ;  
    * }  
    * // [[Rcpp::export]]  
    * double twice( double x){  
    *     return 2.0 * x;  
    * }  
  
Example code includes:  
```{r eval=FALSE}

# When you write R code, it usually makes sense to generate random numbers in a vectorized fashion
# When you are in C++ however, you are allowed (even by your guilty conscience) to use loops and process the data element by element

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector positive_rnorm(int n, double mean, double sd) {
    // Specify out as a numeric vector of size n
    NumericVector out(n);
    // This loops over the elements of out
    for(int i = 0; i < n; i++) {
        // This loop keeps trying to generate a value
        do {
            // Call Rs rnorm()
            out[i] = R::rnorm(mean, sd);
            // While the number is negative, keep trying
        } while(out[i] < 0);
    }
    return out;
}

/*** R
positive_rnorm(10, 2, 2)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
int choose_component(NumericVector weights, double total_weight) {
    // Generate a uniform random number from 0 to total_weight
    double x = R::runif(0, total_weight);
    // Remove the jth weight from x until x is small enough
    int j = 0;
    while(x >= weights[j]) {
        // Subtract jth element of weights from x
        x -= weights[j];
        j++;
    }
    return j;
}

/*** R
weights <- c(0.3, 0.7)
# Randomly choose a component 5 times
replicate(5, choose_component(weights, sum(weights)))
*/


#include <Rcpp.h>
using namespace Rcpp;

// From previous exercise; do not modify
// [[Rcpp::export]]
int choose_component(NumericVector weights, double total_weight) {
    double x = R::runif(0, total_weight);
    int j = 0;
    while(x >= weights[j]) {
        x -= weights[j];
        j++;
    }
    return j;
}

// [[Rcpp::export]]
NumericVector rmix(int n, NumericVector weights, NumericVector means, NumericVector sds) {
    // Check that weights and means have the same size
    int d = weights.size();
    if(means.size() != d) {
        stop("means size != weights size");
    }
    // Do the same for the weights and std devs
    if(sds.size() != d) {
        stop("sds size != weights size");
    }
    // Calculate the total weight
    double total_weight = 0.0;
    for (int i=0; i<d; i++) { 
        total_weight += weights[i];
    };
    // Create the output vector
    NumericVector res(n);
    // Fill the vector
    for(int i = 0; i < n; i++) {
        // Choose a component
        int j = choose_component(weights, total_weight);
        // Simulate from the chosen component
        res[i] = R::rnorm(means[j], sds[j]);
    }
    return res;
}

/*** R
weights <- c(0.3, 0.7)
means <- c(2, 4)
sds <- c(2, 4)
rmix(10, weights, means, sds)
*/


# Complete the definition of rollmean3()
rollmean3 <- function(x, window = 3) {
    # Add the first window elements of x
    initial_total <- sum(head(x, window))
    # The elements to add at each iteration
    lasts <- tail(x, - window)
    # The elements to remove
    firsts <- head(x, - window)
    # Take the initial total and add the 
    # cumulative sum of lasts minus firsts
    other_totals <- initial_total + cumsum(lasts - firsts)
    # Build the output vector
    c(rep(NA, window - 1), # leading NA
      initial_total / window, # initial mean
      other_totals / window   # other means
      )
}

# From previous step; do not modify
rollmean3 <- function(x, window = 3) {
    initial_total <- sum(head(x, window))   
    lasts <- tail(x, - window)
    firsts <- head(x, - window)
    other_totals <- initial_total + cumsum(lasts - firsts)
    c(rep(NA, window - 1), initial_total / window, other_totals / window)
}

# This checks rollmean1() and rollmean2() give the same result
all.equal(rollmean1(x), rollmean2(x))

# This checks rollmean1() and rollmean3() give the same result
all.equal(rollmean1(x), rollmean3(x))

# Benchmark the performance
microbenchmark(rollmean1(x), rollmean2(x), rollmean3(x), times = 5)


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector rollmean4(NumericVector x, int window) {
    int n = x.size();
    // Set res as a NumericVector of NAs with length n
    NumericVector res(n, NumericVector::get_na());
    // Sum the first window worth of values of x
    double total = 0.0;
    for(int i = 0; i < window; i++) {
        total += x[i];
    }
    // Treat the first case seperately
    res[window - 1] = total / window;
    // Iteratively update the total and recalculate the mean 
    for(int i = window; i < n; i++) {
        // Remove the (i - window)th case, and add the ith case
        total += - x[i-window] + x[i];
        // Calculate the mean at the ith position
        res[i] = total / window;
    }
    return res;  
}

/*** R
# Compare rollmean2, rollmean3 and rollmean4   
set.seed(42)
x <- rnorm(1e4)
microbenchmark(rollmean2(x, 4), rollmean3(x, 4), rollmean4(x, 4), times = 5)   
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector na_locf2(NumericVector x) {
    // Initialize to NA
    double current = NumericVector::get_na();
    int n = x.size();
    NumericVector res = no_init(n);
    for(int i = 0; i < n; i++) {
        // If ith value of x is NA
        if(NumericVector::is_na(x[i])) {
            // Set ith result as current
            res[i] = current
        } else {
            // Set current as ith value of x
            current = x[i];
            res[i] = x[i]
        }
    }
    return res ;
}

/*** R
library(microbenchmark)
set.seed(42)
x <- rnorm(1e5)
# Sprinkle some NA into x
x[sample(1e5, 100)] <- NA  
microbenchmark(na_locf1(x), na_locf2(x), times = 5)
*/


#include <Rcpp.h>
using namespace Rcpp; 

// [[Rcpp::export]]
NumericVector na_meancf2(NumericVector x) {
    double total_not_na = 0.0;
    double n_not_na = 0.0;
    NumericVector res = clone(x);
    int n = x.size();
    for(int i = 0; i < n; i++) {
        // If ith value of x is NA
        if(NumericVector::is_na(x[i])) {
            // Set the ith result to the total of non-missing values 
            // divided by the number of non-missing values
            res[i] = total_not_na / n_not_na;
        } else {
            // Add the ith value of x to the total of non-missing values
            total_not_na += x[i];
            // Add 1 to the number of missing values
            n_not_na ++;
        }
    }
    return res;
}

/*** R
library(microbenchmark)
set.seed(42)
x <- rnorm(1e5)
x[sample(1e5, 100)] <- NA  
microbenchmark(na_meancf1(x), na_meancf2(x), times = 5)
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector ar2(int n, double c, NumericVector phi, double eps) {
    int p = phi.size();
    NumericVector x(n);
    // Loop from p to n
    for(int i = p; i < n; i++) {
        // Generate a random number from the normal distribution
        double value = R::rnorm(c, eps);
        // Loop from zero to p
        for(int j = 0; j < p; j++) {
            // Increase by the jth element of phi times 
            // the "i minus j minus 1"th element of x
            value += phi[j] * x[i-j-1];
        }
        x[i] = value;
    }
    return x;
}

/*** R
d <- data.frame(x = 1:50, y = ar2(50, 10, c(1, -0.5), 1))
ggplot(d, aes(x, y)) + 
    geom_line()
*/


#include <Rcpp.h>
using namespace Rcpp ;

// [[Rcpp::export]]
NumericVector ma2( int n, double mu, NumericVector theta, double sd ){
    int q = theta.size(); 
    NumericVector x(n);
    // Generate the noise vector
    NumericVector eps = rnorm(n, 0.0, sd);
    // Loop from q to n
    for(int i = q; i < n; i++) {
        // Value is mean plus noise
        double value = mu + eps[i];
        // Loop from zero to q
        for(int j = 0; j < q; j++) {
            // Increase by the jth element of theta times
            // the "i minus j minus 1"th element of eps
            value += theta[j] * eps[i - j - 1];
        }
        // Set ith element of x to value
        x[i] = value;
    }
    return x ;
}

/*** R
d <- data.frame(x = 1:50, y = ma2(50, 10, c(1, -0.5), 1))
ggplot(d, aes(x, y)) + 
    geom_line()
*/


#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector arma(int n, double mu, NumericVector phi, NumericVector theta, double sd) {
    int p = phi.size();
    int q = theta.size();
    NumericVector x(n);
    // Generate the noise vector
    NumericVector eps = rnorm(n, 0.0, sd);
    // Start at the max of p and q plus 1
    int start = std::max(p, q) + 1;
    // Loop i from start to n
    for(int i = start; i < n; i++) {
        // Value is mean plus noise
        double value = mu + eps[i];
        // The MA(q) part
        for(int j = 0; j < q; j++) {
            // Increase by the jth element of theta times
            // the "i minus j minus 1"th element of eps
            value += theta[j] * eps[i - j - 1];
        }
        // The AR(p) part
        for(int j = 0; j < p; j++) {
            // Increase by the jth element of phi times
            // the "i minus j minus 1"th element of x
            value += phi[j] * x[i - j - 1];
        }
        x[i] = value;
    }
    return x;
}

/*** R
d <- data.frame(x = 1:50, y = arma(50, 10, c(1, -0.5), c(1, -0.5), 1))
ggplot(d, aes(x, y)) + 
    geom_line()
*/

```
  
  
  
***
  
### _Regression Modeling in R: Case Studies_  
  
Chapter 1 - GLMs  
  
Before Starting:  
  
* Course focuses on using GLM and Mixed Effects Models in case studies  
* First dataset is data(dragonflies)  
	* head(dragonflies)  
* How does stream velocity influence the number of dragonflies present?  
	* Response Variable: What are we trying to explain? (y-axis) - abundance  
    * Predictor: What do we think is going to influence our response variable? (x-axis) - stream_flow  
* Can use ggplot2 to investigate the data prior to starting modeling  
	* ggplot(dragonflies) + geom_histogram(aes(x = abundance))  
    * ggplot(dragonflies) + geom_point(aes(x = stream_flow, y = abundance))  
* Overview of key steps prior to starting modeling  
	* What are the data?  
    * What is the research question?  
    * What are the variables of interest?  
    * What do the raw data look like?  
  
Introduction to Generalized Linear Models (GLM):  
  
* The GLM is an ordinary linear model in its simplest form, but can be more complex when needed based on the data, distributions, and research questions  
	* glm(response ~ predictor, data, family = "gaussian")  
    * gaussian_glm <- glm(abundance ~ stream_flow, data = dragonflies, family = "gaussian")  
* Can generate predicted values and bisually assess fits and predictions  
	* pred_df <- data.frame(stream_flow = seq(from = 1, to = 5, length = 10))  
    * pred_df$predicted <- predict(gaussian_glm, pred_df)  
    * ggplot(dragonflies) + geom_point(aes(x = stream_flow, y = abundance)) + geom_line(aes(x = stream_flow, y = predicted), data = pred_df)  
* Important to check residuals - heterogeneity  
	* diag <- data.frame(residuals = resid(gaussian_glm), fitted = fitted(gaussian_glm))  
    * ggplot(diag) + geom_point(aes(x = fitted, y = residuals)  
  
Poisson GLM:  
  
* Linear models with Gaussian residuals can produce negative values (which may be inappropriate) and heterogeneity (assumption violation)  
* The Poisson GLM is a flexible extension to the GLM that can be especially valuable for count data  
	* Apply a Poisson distribution to the error structure of the model  
    * Applicable to count data  
    * Use the link function log()  
* Example code for running the Poisson GLM  
	* poisson_glm <- glm(abundance ~ stream_flow, data = dragonflies, family = "poisson")  
    * pred_df <- data.frame(stream_flow = seq(from = 1, to = 5, length = 10))  
    * pred_df$predicted <- predict(poisson_glm, pred_df, type = "response")  
    * ggplot(dragonflies) + geom_point(aes(x = stream_flow, y = abundance)) + geom_line(aes(x = stream_flow, y = predicted), data = pred_df)  
    * diag <- data.frame(residuals = resid(poisson_glm), fitted = fitted(poisson_glm))  
    * ggplot(diag) + geom_point(aes(x = fitted, y = residuals))  
* Can check for over-dispersion with the Poisson GLM model  
	* The ratio between residual deviance and degrees of freedom should be 1  
    * Overdispersion: if this value is > 1, there is more variability than can be explained by the model or the error structure  
    * dispersion(poisson_glm, modeltype = "poisson")  
  
Example code includes:  
```{r}

dragonflies <- readr::read_csv("./RInputFiles/data1.csv")
orchids <- readr::read_csv("./RInputFiles/lme_data.csv")
str(dragonflies)
str(orchids)


# Draw histogram
ggplot(dragonflies) +
    geom_histogram(aes(x = feeding_events))


# Draw scatterplot
ggplot(dragonflies) +
    geom_point(aes(x = stream_flow, y = feeding_events))


# Apply a GLM
gaussian_glm <- glm(feeding_events ~ stream_flow, data = dragonflies, family = "gaussian")


# Set up a data frame for predictions
pred_df <- data.frame(stream_flow = seq(from = 1, to = 5, length = 10))

# Generate predictions
pred_df$predicted <- predict(gaussian_glm, pred_df)

# Look at the data frame
pred_df

# Add model line to plot
ggplot(dragonflies) +
    geom_point(aes(x = stream_flow, y = feeding_events)) +
    geom_line(aes(x = stream_flow, y = predicted), data = pred_df)


# Generate data frame of residuals and fitted values
diag <- data.frame(residuals = resid(gaussian_glm), fitted = fitted(gaussian_glm))

# Visualize residuals vs fitted values
ggplot(diag) +
    geom_point(aes(x = fitted, y = residuals))


# Apply Poisson GLM
poisson_glm <- glm(feeding_events ~ stream_flow, data = dragonflies, family = "poisson")

# Set up a data frame for predictions
pred_df <- data.frame(stream_flow = seq(from = 1, to = 5, length = 10))

# Generate predictions
pred_df$predicted <- predict(poisson_glm, pred_df, type = "response")

# Add line reprsenting Poisson GLM
ggplot(dragonflies) +
    geom_point(aes(x = stream_flow, y = feeding_events)) +
    geom_line(aes(x = stream_flow, y = predicted), data = pred_df)


dispersion <- function(model, modeltype = "p"){
    A <- sum(resid(model, type = "pearson") ^ 2)
    if (modeltype %in% c("poisson", "quasipoisson")) {
        B <- length(resid(model)) - length(coef(model))
    }
    if (modeltype %in% c("nb", "negativebinomial")) {
        B <- length(resid(model)) - (length(coef(model)) + 1)
    }
    DISP <- A / B
    return(DISP)
}


# Generate data frame of residuals and fitted values
diag <- data.frame(fitted=fitted(poisson_glm), residuals=resid(poisson_glm))

# Visualize residuals vs fitted values
ggplot(diag) + 
    geom_point(aes(x=fitted, y=residuals))

# Calculate the dispersion of the model
dispersion(poisson_glm, modeltype="poisson")

```
  
  
  
***
  
Chapter 2 - Extending GLMs  
  
Adding Factors and Interactions:  
  
* Can use residuals to attempt to find additional factors for modeling (address over-dispersion)  
	* pr_fac(poisson_glm, dragonflies$season, xlabel = "season", modeltype = "poisson")  
* May want to add factors and interactions among the predictor variables  
	* Multiple predictor variables (factors) may influence the response variable  
    * Interaction: the effect of one predictor may depend on the level of the other predictor variable  
    * poisson_glm_factor <- glm(abundance ~ stream_flow * season, data = dragonflies, family = "poisson")  # The star is an interaction effect - each alone, plus the interaction of the two  
* Can then use expand.grid to help with prediction for the interaction data  
	* pred_df <- expand.grid(stream_flow = seq(from = 1, to = 5, length = 10), season = c("summer", "autumn"))  
    * pred_df$predicted <- predict(poisson_glm_factor, pred_df, type = "response")  
    * ggplot(dragonflies) + geom_point(aes(x = stream_flow, y = abundance)) + geom_line(aes(x = stream_flow,     * y = predicted, col = season), data = pred_df)  
    * diag <- data.frame(residuals = resid(poisson_glm_factor), fitted = fitted(poisson_glm_factor))  
    * ggplot(diag) + geom_point(aes(x = fitted, y = residuals))  
    * dispersion(poisson_glm_factor, modeltype = "poisson")  
  
Adding an Offset to the Model:  
  
* There are unequal areas sampled in the data, which needs to be accounted for in the count modeling  
	* May want to look at metrics on a "per square meter" or similar model  
    * Can introduce issues of non-integer data, which fails the assumptions of a count model  
* Can add an offset to use for count modeling  
	* dragonflies$logarea <- log(dragonflies$area)  
    * poisson_glm_offset <- glm(abundance ~ stream_flow * season + offset(logarea), data = dragonflies, family = "poisson")  
  
Negative Binomial Model and Model Selection:  
  
* Because the Poisson model uses a single parameter for both mean and variance, overdispersion is common  
* The negative binomial GLM adds an additional term, allowing for variance different (often larger) than mean  
	* Have an extra parameter, theta, which relaxes the assumptions of equality between the mean and the variance  
    * This improves upon the Poisson GLM by addressing the issue of overdispersion  
    * Use the same link function as Poisson GLMs  
* The MASS package has functions for running the negative binomial model  
	* neg_binom_glm <- MASS::glm.nb(abundance ~ stream_flow * season + offset(logarea), data=dragonflies)  
	* Use drop1 to test influence of each term  
    * drop1(neg_binom_glm, test = "Chisq")  
    * neg_binom_glm_small <- glm.nb(abundance ~ stream_flow + season + offset(logarea), data=dragonflies)  
    * remove the interaction term, since it has a high p-value  
    * drop1(neg_binom_glm_small, test = "Chisq")  
* Can now look at model performances and pick the best  
	* dispersion(neg_binom_glm, modeltype = "nb")  
    * dispersion(neg_binom_glm_small, modeltype = "nb")  
  
Model Selection and Visualization:  
  
* Eventually need to select and defend a model even if there are some flaws or potential improvements  
* The AIC (Akaike Information Criteria) can be helpful for comparing nested models  
	* AIC(neg_binom_glm, neg_binom_glm_small)  
    * Lower values indicate better fit  
    * A difference of three or more indicates a model is a better fit than the other  
    * Generally, prefer the less complex model when the performances are similar  
* Can then generate predicted values from the selected model  
	* pred_df <- expand.grid(stream_flow = seq(from = 1, to = 5, length = 10), season = c("summer", "autumn"), logarea = mean(dragonflies$logarea))  
    * pred_df$predicted <- predict(neg_binom_glm_small, pred_df, type = "response")  
    * ggplot(dragonflies) + geom_point(aes(x = stream_flow, y = abundance)) + geom_line(aes(x = stream_flow, y = predicted, col = season), data = pred_df)  
    * raw_fit <- predict(neg_binom_glm_small, pred_df, type = "link")  # fitted values on the link scale  
    * raw_se <- predict(neg_binom_glm_small, pred_df, type = "link", se.fit = TRUE)$se  # SE on the link scale  
    * pred_df$lower <- exp(raw_fit - 1.96 * raw_se)  
    * pred_df$upper <- exp(raw_fit + 1.96 * raw_se)  
    * ggplot(dragonflies) + geom_point(aes(x = stream_flow, y = abundance)) + geom_line(aes(x = stream_flow, y = predicted, col = season), data = pred_df) + geom_line(aes(x = stream_flow, y = upper, col = season), linetype = "dashed", data = pred_df) + geom_line(aes(x = stream_flow, y = lower, col = season), linetype = "dashed", data = pred_df)  
  
Example code includes:  
```{r}

pr_fac <- function(model, plotfactor, xlabel = "", modeltype = "lm"){
    if(modeltype %in% c("linear", "lm", "poisson", "p", "quasipossion", "qp")){
        skiprows <- unique(summary(model)$na.action)
    }
    if(modeltype %in% c("negativebinomial", "nb")){
        skiprows <- unique(summary(model)[21])
    }
    if(length(skiprows) > 0) {Factor <- plotfactor[-skiprows]}
    if(length(skiprows) == 0) {Factor <- plotfactor}
    plot1 <- data.frame(PR = resid(model, type = "pearson"),Factor)
    if(is.factor(Factor) == FALSE){
        PR.plot1 <- ggplot(plot1) + 
            geom_point(aes(y = PR, x = Factor)) + 
            geom_hline(yintercept = 0, linetype = 'dashed', col = 'red')+ 
            ylab("Residuals") + xlab(xlabel) + theme_bw(18)
    }
    if(is.factor(Factor) == TRUE){
        PR.plot1 <- ggplot(plot1) +
            geom_boxplot(aes(y = PR, x = Factor)) +
            geom_hline(yintercept = 0, linetype = 'dashed', col = 'red') + 
            ylab("Residuals") + xlab(xlabel) +
            theme_bw(18)
    }
    return(PR.plot1)
}


# Compare residuals across factor levels
pr_fac(poisson_glm, dragonflies$time, xlabel = "time", modeltype = "poisson")

# Add time as a factor, including an interaction
poisson_glm_factor <- glm(feeding_events ~ stream_flow * time, data = dragonflies, family = "poisson")

# Generate predicted values of feeding_events
pred_df <- expand.grid(stream_flow = seq(from = 1, to = 5, length = 10), time = c("day", "night"))
pred_df$predicted <- predict(poisson_glm_factor, pred_df, type = "response")

# Visualize predicted values of feeding events
ggplot(dragonflies) +
    geom_point(aes(x = stream_flow, y = feeding_events)) +
    geom_line(aes(x = stream_flow, y = predicted, col = time), data = pred_df)


# Generate data frame of residuals and fitted values
diag <- data.frame(residuals=resid(poisson_glm_factor), fitted=fitted(poisson_glm_factor))

# Visualize residuals vs fitted values
ggplot(diag) +
    geom_point(aes(x=fitted, y=residuals))

# Calculate the dispersion of the model
dispersion(poisson_glm_factor, modeltype="poisson")


# Create a column containing the natural log of area
dragonflies$logarea <- log(dragonflies$area)

# Apply Poisson GLM with interaction and offset
poisson_glm_offset <- glm(feeding_events ~ stream_flow * time + offset(logarea), 
                          data = dragonflies, family = "poisson"
                          )


# Apply Negative Binomial GLM
neg_binom_glm <- MASS::glm.nb(feeding_events ~ stream_flow*time + offset(logarea), data=dragonflies)

# Use drop1 to determine which term(s) can be dropped
drop1(neg_binom_glm, test="Chisq")

# Apply a new Negative Binomial GLM
neg_binom_glm_small <- MASS::glm.nb(feeding_events ~ stream_flow + time + offset(logarea), data=dragonflies)


# Calculate dispersion for each model
dispersion(neg_binom_glm, modeltype="nb")
dispersion(neg_binom_glm_small, modeltype="nb")


# Generate data frame of residuals and fitted values for neg_binom_glm
diag <- data.frame(fitted=fitted(neg_binom_glm), residuals=resid(neg_binom_glm))

# Visualize residuals vs fitted values for neg_binom_glm
ggplot(diag) + 
    geom_point(aes(x=fitted, y=residuals))

# Generate data frame of residuals and fitted values for neg_binom_glm
diag_small <- data.frame(residuals = resid(neg_binom_glm_small), fitted = fitted(neg_binom_glm_small))

# Visualize residuals vs fitted values for neg_binom_glm
ggplot(diag_small) +
    geom_point(aes(x = fitted, y = residuals))


# Compare AIC scores
AIC(neg_binom_glm, neg_binom_glm_small)

# View the selected model
neg_binom_glm_small


# Create data frame
pred_df <- expand.grid(stream_flow=seq(1, 5, length=5), time=c("day", "night"), logarea=log(6))

# Generate predicted values
pred_df$predicted <- predict(neg_binom_glm_small, newdata=pred_df, type="response")

# Visualize predicted values
ggplot(dragonflies) +
    geom_point(aes(x = stream_flow, y = feeding_events)) +
    geom_line(aes(x=stream_flow, y=predicted, color=time), data=pred_df)


# Extract fitted values
raw_fit <- predict(neg_binom_glm_small, pred_df, type = "link")

# Extract standard errors
raw_se <- predict(neg_binom_glm_small, pred_df, type = "link", se = TRUE)$se

# Generate predictions of upper and lower values
pred_df$upper <- exp(raw_fit + 1.96 * raw_se)
pred_df$lower <- exp(raw_fit - 1.96 * raw_se)

# Visualize the standard errors around the predicted values
ggplot(dragonflies) +
    geom_point(aes(x = stream_flow, y = feeding_events)) +
    geom_line(aes(x = stream_flow, y = predicted, col = time), data = pred_df) +
    geom_line(aes(x = stream_flow, y = lower, col = time), linetype="dashed", data = pred_df) +
    geom_line(aes(x = stream_flow, y = upper, col = time), linetype="dashed", data = pred_df)

```
  
  
  
***
  
Chapter 3 - Mixed Effects Model I  
  
Mixed Effects Models:  
  
* Mixed Effects Models are useful for modeling grouped (nested) data  
* Dataset will be about orchids - 20 different trees in 8 different sites  
	* Data belonging to the same group are correlated  
    * This violates assumptions about the independence of observations  
* Can ask the model to address using fixed effects  
	* linear_glm <- glm(richness ~ tree_age + site, data = orchids, family = "gaussian")  
    * Treating site as a fixed effect means that we are estimating parameters for each of the eight individual sites  
    * Need to have sufficient data for each group  
    * Adding parameters costs us degrees of freedom  
* Can ask the model to address using mixed effects, since site is not a predictor variable, merely a source of correlation  
	* Random effect: the model aims to estimate the distribution of the effect rather than estimate the effect itself as a constant  
    * Concerned with the wider population rather than the individuals sampled  
* The random intercept model estimates a distribution of the random effect of site on intercept  
	* nlme::lme(a ~ b | random = ~1 | site, data=)  
    * random = ~1 | site specifies a random intercept model where site will act as a random effect to influence the intercept of the linear model  
  
Model Selection and Interpretation:  
  
* May want to assess the relative qualities of the GLM and the random effects models  
	* Linear models need to be fit with GLS (generalized least squares) to compare to mixed effects models  
    * Mixed effects models need to be fit with REML (restricted maximum likelihood) to compare with linear models  
The ANOVA function can be helpful for comparing models with similar assumptions (consistencies in the underlying assumptions)  
	* anova(gls_model, random_int_model)  
* Can have a problem of "testing on the boundary" - relevant only when the p-value is marginal  
* Often want to understand the variance of the random effects variables  
    * The data under the "Random Effects:" area can be used - square the provided data  
    * VarCorr(random_int_model)  # grab data using a function instead  
  
Visualizing a Random Intercept Model:  
  
* Can begin by plotting the original data with the random effects data as a color  
    * ggplot(orchids) + geom_jitter(aes(x = tree_age, y = richness, col = site))  
* Can add the population effects predictions (these are based only on the fixed components; overall relationships)  
	* pred_df.fixed <- data.frame(tree_age = seq(from = 5, to = 20, length = 10))  
    * pred_df.fixed$predicted <- predict(random_int_model, pred_df.fixed, level = 0)  # level=0 means "population level predictions"  
    * ggplot(orchids) + geom_jitter(aes(x = tree_age, y = richness, col = site)) + geom_line(aes(x = tree_age, y = predicted), size = 2, data = pred_df.fixed)  # no color since site is not included  
* Can add the full effects predictions (fixed and random)  
	* pred_df.random <- expand.grid(tree_age = seq(from = 5, to = 20, length = 10), site = unique(orchids$site))  
    * pred_df.random$random <- predict(random_int_model, pred_df.random, level = 1)  # level=1 is the full prediction (random plus fixed)  
    * ggplot(orchids) + geom_jitter(aes(x = tree_age, y = richness, col = site)) + geom_line(aes(x = tree_age, y = predicted), size = 2, data = pred_df.fixed) + geom_line(aes(x = tree_age, y = random, col = site), data = pred_df.random)  
  
Example code includes:  
```{r}

# Create scatterplot of humidity and abundance
ggplot(orchids, aes(x=humidity, y=abundance, color=site)) +
    geom_point()


# Apply GLM
linear_glm <- glm(abundance ~ humidity + site, data = orchids, family = "gaussian")

# Look at the output to see paramters for each site
coef(linear_glm)


# Apply random intercept model
random_int_model <- nlme::lme(abundance ~ humidity, random = ~1|site, data=orchids)

# Look at model output
random_int_model


# Fit linear model using Generalized Least Squares
gls_model <- nlme::gls(abundance ~ humidity, data = orchids)

# Apply a random intercept model
random_int_model <- nlme::lme(abundance ~ humidity, random = ~1 | site, data = orchids, method = "REML")

# Apply likelihood ratio test to compare models
anova(gls_model, random_int_model)

# Print the model that fits better
random_int_model


# Calculate estimate of variance for the random intercept
calculated_value <- 3.515514**2
calculated_value

# Extract estimate of variance for the random intercept
extracted_value <- nlme::VarCorr(random_int_model)[1, 1]
extracted_value


# Create data frame for fixed component
pred_df.fixed <- data.frame(humidity = seq(from = 40, to = 75, length = 10))

# Generate population level predictions
pred_df.fixed$predicted <- predict(random_int_model, pred_df.fixed, level = 0)

# Visualize predicted values
ggplot(orchids) +
    geom_point(aes(x = humidity, y = abundance, col = site)) +
    geom_line(aes(x=humidity, y=predicted), data=pred_df.fixed, size=2)


# Create data frame for random component
pred_df.random <- expand.grid(humidity = seq(from = 40, to = 75, length = 10), site = unique(orchids$site))

# Generate within-site predictions
pred_df.random$random <- predict(random_int_model, newdata=pred_df.random, level=1)

# Visualize predicted values
ggplot(orchids) +
    geom_point(aes(x = humidity, y = abundance, col = site)) +
    geom_line(aes(x = humidity, y = predicted), size = 2, data = pred_df.fixed) +
    geom_line(aes(x=humidity, y=random, col = site), data = pred_df.random)

```
  
  
  
***
  
Chapter 4 - Mixed Effects Models II  
  
Random Intercept and Slope Models:  
  
* May want to have the random effects drive both intercept and slope - no longer parallel lines, but instead different slopes AND intercepts  
* Can add interaction term in GLM, but may require too many parameters and too great a requirement for DF  
	* linear_glm <- glm(richness ~ tree_age * site, data = orchids, family = "gaussian")  
* Can instead consider site as part of a mixed effects model  
	* random_int_slope_model <- lme(richness ~ tree_age, random = ~1 + tree_age | site, data = orchids)  
    * Random intercept model: ~1 | randomEffect  
    * Random intercept and slope model: ~1 + predictor | randomEffect  
* Can then generate population level predictions  
	* pred_df.fixed <- data.frame(tree_age = seq(from = 5, to = 20, length = 10))  
    * pred_df.fixed$predicted <- predict(random_int_slope_model, pred_df.fixed, level = 0)  # level=0 is for population level  
* Can also generate individual predictions  
	* pred_df.random <- expand.grid(tree_age = seq(from = 5, to = 20, length = 10), site = unique(orchids$site))  
    * pred_df.random$random <- predict(random_int_slope_model, pred_df.random, level = 1)  # level=1 is the full prediction  
    * ggplot(orchids) + geom_jitter(aes(x = tree_age, y = richness, col = site)) + geom_line(aes(x = tree_age, y = predicted), size = 2, data = pred_df.fixed) + geom_line(aes(x = tree_age, y = random, col = site), data = pred_df.random)  
  
Model Selection and Interpretation:  
  
* May want to assess relative performance of including random effects slope term - need to fit with REML  
	* random_int_model <- lme(richness ~ tree_age, random = ~1 | site, data = orchids, method = "REML")  
    * random_int_slope_model <- lme(richness ~ tree_age, random = ~1 + tree_age| site, data = orchids, method = "REML")  
    * anova(random_int_model, random_int_slope_model)  
* May want to correct the p-value for how ANOVA works with mixed-effects models  
	* LR <- ((-284.8980) - (-284.6478)) * -2  # -284.898 and -284.6748 are the logLik values from the regression  
    * ((1 - pchisq(LR, 1)) + (1 - pchisq(LR, 2))) * 0.5  # corrected p-values  
* May want to calculate the variances - can either calculate directly by squaring from the output  
	* There is also now a correlation variable shown as Corr which shows whether slopes are linked to intercepts  
  
Using Modeling as a Tool:  
  
* Modeling should be used as a tool for answering key research questions  
	* What are the data?  
    * What is the research question?  
    * What are the variables of interest?  
    * What do the raw data look like?  
* May want to use GLM in cases where the errors are not normally distributed  
	* Flexible generalization of an ordinary linear model  
    * Common approach for modeling count data, but broadly applicable  
* Need to clearly define the model formula and links for reporting  
	* What is my model formula?  
    * What link function is used?  
    * How do I know this model is appropriate?  
    * How did I choose this model over other models?  
    * What does this model tell me about my research question?  
* Mixed effects models aim to estimate a random categorical effect  
	* Aim to estimate the distribution of a random categorical effect  
    * Attempts to answer a question about the wider population rather than compare specific groups  
* Should answer key questions about the random effects modeling  
	* What is my model formula?  
    * How did I choose this model over other models?  
    * Why did I choose to use this model type?  
    * What does this model tell me about my research question?  
* General practices for reporting outcomes  
	* No firm rules about what to include when reporting the results of your model  
    * Do some research!  
  
Wrap Up:  

* GLM and Mixed Effects Models  
* Evaluate and defend models  
* Visualize predictions  
* Extract pertinent information  
  
Example code includes:  
```{r}

# Apply random intercept and slope model
random_int_slope_model <- nlme::lme(abundance ~ humidity, random = ~1 + humidity | site, data=orchids)

# Look at model output
random_int_slope_model


# Create data frame for fixed component
pred_df.fixed <- data.frame(humidity = seq(from = 40, to = 75, length = 10))

# Generate population level predictions
pred_df.fixed$predicted <- predict(random_int_slope_model, pred_df.fixed, level = 0)

# Create data frame for random component
pred_df.random <- expand.grid(humidity = seq(from = 40, to = 75, length = 10), site = unique(orchids$site))

# Generate within-site predictions
pred_df.random$random <- predict(random_int_slope_model, pred_df.random, level = 1)


# Visualize population level predictions and within-site predictions of abundance
ggplot(orchids) +
    geom_point(aes(x = humidity, y = abundance, col = site)) +
    geom_line(aes(x=humidity, y=predicted), size=2, data=pred_df.fixed)

# Visualize population level predictions and within-site predictions of abundance
ggplot(orchids) +
    geom_point(aes(x = humidity, y = abundance, col = site)) +
    geom_line(aes(x = humidity, y = predicted), size = 2, data = pred_df.fixed) +
    geom_line(aes(x=humidity, y=random, col = site), data = pred_df.random)


# Apply a maximum likelihood ratio test
anova(random_int_model, random_int_slope_model)


# Calculate the corrected p-value
LR <- ((-420.2667) - (-408.7254)) * -2
((1 - pchisq(LR, 1)) + (1 - pchisq(LR, 2))) * 0.5

# Print the model that has more parameters
fewer_parameters <- random_int_model
fewer_parameters

# Print the model that has the better AIC value
better_aic_value <- random_int_slope_model
better_aic_value


# View the model output
random_int_slope_model

# Calculate the estimated variance of random intercept
variance_int <- 7.3277203**2

# Calculate the estimated variance of random slope
variance_slope <- 0.1387053**2

# Print the higher estimate
variance_int


myData <- data.frame(X=1:160, 
                     y=c(7, 4, 4, 4, 4, 3, 4, 3, 3, 3, 7, 3, 3, 3, 3, 3, 4, 4, 6, 3, 2, 3, 3, 7, 8, 2, 3, 4, 9, 4, 3, 3, 4, 6, 4, 3, 3, 4, 7, 5, 2, 1, 2, 2, 2, 3, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 3, 1, 3, 1, 2, 2, 0, 2, 0, 2, 3, 1, 2, 1, 2, 1, 1, 2, 2, 10, 3, 0, 2, 2, 7, 4, 0, 2, 2, 5, 2, 1, 2, 2, 5, 4, 1, 3, 2, 2, 1, 4, 2, 2, 1, 2, 2, 2, 3, 3, 2, 3, 2, 4, 2, 2, 4, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 0, 2, 2, 2, 1, 0, 1, 3, 2, 2, 2, 3, 2, 0, 1, 3, 3, 2, 1, 2, 3, 3, 3, 0, 2, 2, 2, 2, 1), 
                     x=c(59.5, 70.4, 73.4, 53.8, 66.8, 57.9, 78.7, 81, 60.1, 73.3, 69.3, 60.1, 63.4, 48.1, 65.4, 58.2, 68.9, 75.1, 58.3, 69.4, 48.1, 51.8, 53.6, 59.4, 73.3, 43.4, 45.2, 58.5, 52.3, 81.5, 39.9, 48.1, 50.8, 49.8, 82.5, 44.3, 56.1, 57.6, 62.9, 78, 69.8, 45.1, 62, 64.8, 58.3, 68.6, 38.6, 64.5, 57, 65.9, 71.3, 33.8, 72.7, 71.1, 53.7, 73.4, 46.2, 63.4, 62.9, 53, 42.3, 60.3, 55.4, 46.3, 68.1, 45.5, 53.7, 47.1, 43, 58.7, 48, 68.9, 54.7, 58.2, 66.4, 42.5, 59.1, 50.6, 50.2, 70.8, 48.8, 65, 52.7, 43.7, 59.5, 42.1, 74.7, 61.6, 45.6, 61.2, 59.3, 72.5, 42.3, 50.1, 68.9, 44.1, 68.6, 49.6, 44.4, 61.9, 65.8, 63.5, 37.8, 68.9, 52.3, 74.1, 66, 38, 68.2, 55.7, 62.1, 57.8, 29.5, 76.6, 51.3, 70.3, 59.6, 37, 65.2, 54, 51.2, 42.1, 46.2, 60.6, 56.3, 51.1, 42.5, 38.5, 51.6, 50.4, 59.1, 33.4, 49.2, 65.7, 64.7, 57, 38.7, 49.7, 60.2, 54.7, 43.3, 63.7, 70, 58.3, 39.5, 48.5, 68.6, 68.5, 57.6, 47, 40.3, 70.8, 77.4, 59.9, 48.8, 43.5, 63, 65.1, 54.9, 40), 
                     group=as.factor(c('a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'f', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'g', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h'))
                     )

model1 <- nlme::gls(y ~ x, data = myData)
model2 <- glm(y ~ x, data = myData)
model3 <- glm(y ~ x * group, data = myData, family = "gaussian")
model4 <- MASS::glm.nb(y ~ x, data = myData)
model5 <- nlme::lme(y ~ x, random = ~1|group, data = myData)
model6 <- nlme::lme(y ~ x, random = ~1 + x|group, data = myData)


# Print the coefficients of the model
coef(model3)

# Print the model output
model4

# Print the model formula
model5$call


model1 <- nlme::gls(y ~ x, data = myData)
model2 <- glm(y ~ x, data = myData)
model3 <- glm(y ~ x * group, data = myData, family = "gaussian")
model4 <- nlme::lme(y ~ x, random = ~1|group, data = myData, method = "REML")
model5 <- nlme::lme(y ~ x, random = ~1|group, data = myData, method = "ML")


# Apply a likelihood ratio test
anova(model4, model1)

# Correct the p-value
LR <- ((-295.0109) - (-272.0328)) * -2
(1 - pchisq(LR, 1)) * 0.5


# Generate data frame of residuals and fitted values
diag <- data.frame(residuals=model2$residuals, fitted=model2$fitted)

# Visualize residuals vs fitted values
ggplot(diag, aes(x=fitted, y=residuals)) +
    geom_point()


model1 <- glm(y ~ x * group, data = myData, family = "gaussian")
model2 <- MASS::glm.nb(y ~ x * group, data = myData)
model3 <- nlme::lme(y ~ x, random = ~1 + x|group, data = myData)


# Create a data frame where x is 35 and 40 at each group
pred_df <- expand.grid(x = c(35, 40), group = unique(myData$group))

# Generate predictions using model1
predict(model1, pred_df)

# Generate predictions using model2
predict(model2, pred_df, type = "response")

# Generate predictions using model3
predict(model3, pred_df)

```
  
  
  
***
  
### _Topic Modeling in R_  
  
Chapter 1 - Introduction to the Workflow  
  
Background:  
  
* Topics give a quick description of what an article is about  
	* A topic is a label for a collection of words that often occur together. E.g., weather includes words: rain, storm, snow, winds, ice  
* Topic modeling is the process of finding a collection of topics fitted to a set of documents  
	* Increasingly popular, helps the audience know if the article is likely of interest for them  
* Course will focus on one specific implementation of topic modeling algorithms, called Latent Dirichlet Allocation (LDA)  
	* LDA takes a document-term matrix as its input - frequencies of words, but NOT word orders  
    * A collection of documents is referred to as a corpus  
* Can use LDA as a supervised clustering algorithm  
	* lda_mod <- LDA(x=d, k=2, method="Gibbs", control=list(alpha=1, delta=0.1, seed=10005, keep=1))  
    * And the result is two tables, for terms and topics.  
* Some general best practices apply to topic modeling  
	* Matrices are not a good way to present the results. We need to use charts  
    * There are choices which words to keep and which ones to exclude from a document-term matrix  
    * Documents can be constructed in multiple way: they can be based on chapters in a novel, on paragraphs, or even on a sequence of several words  
    * The LDA algorithm relies on control parameters which can impact the output  
  
Counting Words:  
  
* The task of splitting text into words is also called 'tokenization' - sequence of characters or sequence of words  
* Package tidytext has function unnest_tokens() that does the splitting  
	* unnest_tokens(data, input=text, output=word, format="text", tokens="word", drop=TRUE, to_lower=TRUE)  
    * It returns a tidy table, with one word per row  
* Can use the count() function from dplyr to count the words from unnest_tokens()  
	* book %>% unnest_tokens(input=text, output=word) %>% count(chapter, word)  
    * To get the top-n words: group words by chapter, sort/arrange by count in descending order, keep rows whose number is less than n  
    * book %>% unnest_tokens(input=text, output=word) %>% count(chapter, word) %>% group_by(chapter) %>% arrange(desc(n)) %>% filter(row_number() < 3) %>% ungroup()  
* Can cast the word counts in to a document-term matrix  
	* Casting a table means transforming it into a different format  
    * A document-term matrix (dtm) contains counts of words  
    * Each row corresponds to a document, each column - to a word  
    * In our case, each chapter is its own document  
    * Package tidytext has function cast_dtm to do this transformation  
    * Just add cast_dtm after count  
    * cast_dtm(data, document=chapter, term=word, value=n)  
    * dtm <- book %>% 
    *     unnest_tokens(input=text, output=word) %>%  
    *     count(chapter, word) %>%  
    *     cast_dtm(document=chapter, term=word, value=n)  # created as a sparse matrix  
    * as.matrix(dtm)  # converts to a regular matrix  
  
Displaying Frequencies with ggplot:  
  
* Can plot either word counts or probabilities of a document belonging to a topic  
* When we fit a topic model, the result is an LDA model object - It contains two matrices: beta and gamma  
	* beta contains probabilities of words in topics  
    * gamma contains probabilities of topics in documents  
    * lda_mod <- LDA(x=d2, k=2, method="Gibbs", control=list(alpha=1, delta=0.1, seed=10005))  
    * str(lda_mod)  # will have @beta and @gamma slots  
* Can create stacked column charts for topic membership using gamma  
	* tidy(lda_mod, matrix="gamma") %>%  
	*     ggplot(aes(x=document, y=gamma)) +  
	*     geom_col(aes(fill=as.factor(topic)))  
* Can create dodge column charts for word frequencies using beta  
	* tidy(lda_mod, matrix="beta") %>%  
	*     ggplot(aes(x=term, y=beta)) +  
	*     geom_col(aes(fill=as.factor(topic)), position=position_dodge())  
    * tidy(lda_mod, matrix="beta") %>%  
    *     mutate(topic = as.factor(topic)) %>%  
    *     ggplot(aes(x=term, y=beta)) +  
    *     geom_col(aes(fill=topic), position=position_dodge()) +  
    *     theme(axis.text.x = element_text(angle=90))  # labels rotated 90 degrees  
  
Example code includes:  
```{r}

word_topics <- matrix(data=c(0.0034, 0.0279, 0.0374, 0.0025, 0.0034, 0.0787, 0.0034, 0.0279, 0.0034, 0.0533, 0.1054, 0.0025, 0.0034, 0.0787, 0.0034, 0.1294, 0.0034, 0.1041, 0.0034, 0.0279, 0.0034, 0.0279, 0.0034, 0.0279, 0.0374, 0.0025, 0.0034, 0.0533, 0.0034, 0.0787, 0.0034, 0.1294, 0.0034, 0.0279, 0.0714, 0.0025, 0.0374, 0.0025, 0.0374, 0.0025, 0.0714, 0.0025, 0.1054, 0.0025, 0.1395, 0.0025, 0.0374, 0.0025, 0.0374, 0.0025, 0.0374, 0.0025, 0.0374, 0.0025, 0.0374, 0.0025, 0.0374, 0.0025, 0.0034, 0.0279, 0.0374, 0.0025, 0.0034, 0.0279, 0.0034, 0.0279, 0.0374, 0.0025), 
                      nrow=2, ncol=34, byrow=FALSE, 
                      dimnames=list(c(1, 2), 
                                    c('agreed', 'bad', 'bank', 'due', 'fines', 'loans', 'pay', 'the', 'to', 'are', 'face', 'if', 'late', 'off', 'will', 'you', 'your', 'a', 'downtown', 'in', 'new', 'opened', 'restaurant', 'is', 'just', 'on', 'street', 'that', 'there', 'warwick', 'for', 'how', 'need', 'want'
                                      )
                                    )
                      )


# Display the column names
colnames(word_topics)

# Display the probability
word_topics[1, "street"]


ch1 <- paste0('Two thousand five hundred and fifty-eight years ago a little fleet of galleys toiled painfully against the current up the long strait of the Hellespont, rowed across the broad Propontis, and came to anchor in the smooth waters of the first inlet which cuts into the European shore of the Bosphorus. There a long crescent-shaped creek, which after-ages were to know as the Golden Horn, strikes inland for seven miles, forming a quiet backwater from the rapid stream which runs outside. On the headland, enclosed between this inlet and the open sea, a few hundred colonists disembarked, and hastily secured themselves from the wild tribes of the inland, by running some rough sort of a stockade across the ground from beach to beach. Thus was founded the city of Byzantium. The settlers were Greeks of the Dorian race, natives of the thriving seaport-state of Megara, one of the most enterprising of all the cities of Hellas in the time of colonial and commercial expansion which was then at its height. Wherever a Greek prow had cut its way into unknown waters, there Megarian seamen were soon found following in its wake. ', 
              'One band of these venturesome traders pushed far to the West to plant colonies in Sicily, but the larger share of the attention of Megara was turned towards the sunrising, towards the mist-enshrouded entrance of the Black Sea and the fabulous lands that lay beyond. There, as legends told, was to be found the realm of the Golden Fleece, the Eldorado of the ancient world, where kings of untold wealth reigned over the tribes of Colchis: there dwelt, by the banks of the river Thermodon, the Amazons, the warlike women who had once vexed far-off Greece by their inroads: there, too, was to be found, if one could but struggle far enough up its northern shore, the land of the Hyperboreans, the blessed folk who dwell behind the North Wind and know nothing of storm and winter. To seek these fabled wonders the Greeks sailed ever North and East till they had come to the extreme limits of the sea. The riches of the Golden Fleece they did not find, nor the country of the Hyperboreans, nor the tribes of the Amazons; but they did discover many lands well worth the knowing, and grew rich on the profits which they drew from the metals of Colchis and the forests of Paphlagonia, from the rich corn lands by the banks of the Dnieper and Bug, and the fisheries of the Bosphorus and the Maeotic Lake. Presently the whole coastland of the sea, which the Greeks, on their first coming, called Axeinos--\"the Inhospitable\"--became fringed with trading settlements, and its name was changed to Euxeinos--\"the Hospitable\"--in recognition of its friendly ports. It was in a similar spirit that, two thousand years later, the seamen who led the next great impulse of exploration that rose in Europe, turned the name of the \"Cape of Storms\" into that of the \"Cape of Good Hope.\" The Megarians, almost more than any other Greeks, devoted their attention to the Euxine, and the foundation of Byzantium was but one of their many achievements. ', 
              'Already, seventeen years before Byzantium came into being, another band of Megarian colonists had established themselves at Chalcedon, on the opposite Asiatic shore of the Bosphorus. The settlers who were destined to found the greater city applied to the oracle of Delphi to give them advice as to the site of their new home, and Apollo, we are told, bade them \"build their town over against the city of the blind.\" They therefore pitched upon the headland by the Golden Horn, reasoning that the Chalcedonians were truly blind to have neglected the more eligible site on the Thracian shore, in order to found a colony on the far less inviting Bithynian side of the strait. Early Coin Of Byzantium. Late Coin Of Byzantium Showing Crescent And Star. From the first its situation marked out Byzantium as destined for a great future. Alike from the military and from the commercial point of view no city could have been better placed. Looking out from the easternmost headland of Thrace, with all Europe behind it and all Asia before, it was equally well suited to be the frontier fortress to defend the border of the one, or the basis of operations for an invasion from the other. ', 
              'As fortresses went in those early days it was almost impregnable--two sides protected by the water, the third by a strong wall not commanded by any neighbouring heights. In all its early history Byzantium never fell by storm: famine or treachery accounted for the few occasions on which it fell into the hands of an enemy. In its commercial aspect the place was even more favourably situated. It completely commanded the whole Black Sea trade: every vessel that went forth from Greece or Ionia to traffic with Scythia or Colchis, the lands by the Danube mouth or the shores of the Maeotic Lake, had to pass close under its walls, so that the prosperity of a hundred Hellenic towns on the Euxine was always at the mercy of the masters of Byzantium. The Greek loved short stages and frequent stoppages, and as a half-way house alone Byzantium would have been prosperous: but it had also a flourishing local trade of its own with the tribes of the neighbouring Thracian inland, and drew much profit from its fisheries: so much so that the city badge--its coat of arms as we should call it--comprised a tunny-fish as well as the famous ox whose form alluded to the legend of the naming of the Bosphorus. As an independent state Byzantium had a long and eventful history. ', 
              'For thirty years it was in the hands of the kings of Persia, but with that short exception it maintained its freedom during the first three hundred years that followed its foundation. Many stirring scenes took place beneath its walls: it was close to them that the great Darius threw across the Bosphorus his bridge of boats, which served as a model for the more famous structure on which his son Xerxes crossed the Hellespont. Fifteen years later, when Byzantium in common with all its neighbours made an ineffectual attempt to throw off the Persian yoke, in the rising called the \"Ionic Revolt,\" it was held for a time by the arch-rebel Histiaeus, who--as much to enrich himself as to pay his seamen--invented strait dues. He forced every ship passing up or down the Bosphorus to pay a heavy toll, and won no small unpopularity thereby for the cause of freedom which he professed to champion. Ere long Byzantium fell back again into the hands of Persia, but she was finally freed from the Oriental yoke seventeen years later, when the victorious Greeks, fresh from the triumph of Salamis and Mycale, sailed up to her walls and after a long leaguer starved out the obstinate garrison [B.C. 479]. ', 
              'The fleet wintered there, and it was at Byzantium that the first foundations of the naval empire of Athens were laid, when all the Greek states of Asia placed their ships at the disposal of the Athenian admirals Cimon and Aristeides. During the fifth century Byzantium twice declared war on Athens, now the mistress of the seas, and on each occasion fell into the hands of the enemy--once by voluntary surrender in 439 B.C., once by treachery from within, in 408 B.C. But the Athenians, except in one or two disgraceful cases, did not deal hardly with their conquered enemies, and the Byzantines escaped anything harder than the payment of a heavy war indemnity. In a few years their commercial gains repaired all the losses of war, and the state was itself again. We know comparatively little about the internal history of these early centuries of the life of Byzantium. Some odd fragments of information survive here and there: we know, for example, that they used iron instead of copper for small money, a peculiarity shared by no other ancient state save Sparta. Their alphabet rejoiced in an abnormally shaped {~GREEK CAPITAL LETTER BETA~}, which puzzled all other Greeks, for it resembled a {~GREEK CAPITAL LETTER PI~} with an extra limb. The chief gods of the city were those that we might have expected--Poseidon the ruler of the sea, whose blessing gave Byzantium its chief wealth; and Demeter, the goddess who presided over the Thracian and Scythian corn lands which formed its second source of prosperity. ', 
              'The Byzantines were, if ancient chroniclers tell us the truth, a luxurious as well as a busy race: they spent too much time in their numerous inns, where the excellent wines of Maronea and other neighbouring places offered great temptations. They were gluttons too as well as tipplers: on one occasion, we are assured, the whole civic militia struck work in the height of a siege, till their commander consented to allow restaurants to be erected at convenient distances round the ramparts. One comic writer informs us that the Byzantines were eating young tunny-fish--their favourite dish--so constantly, that their whole bodies had become well-nigh gelatinous, and it was thought they might melt if exposed to too great heat! Probably these tales are the scandals of neighbours who envied Byzantine prosperity, for it is at any rate certain that the city showed all through its history great energy and love of independence, and never shrank from war as we should have expected a nation of epicures to do. It was not till the rise of Philip of Macedon and his greater son Alexander that Byzantium fell for the fifth time into the hands of an enemy. The elder king was repulsed from the citys walls after a long siege, culminating in an attempt at an escalade by night, which was frustrated owing to the sudden appearance of a light in heaven, which revealed the advancing enemy and was taken by the Byzantines as a token of special divine aid [B.C. 339]. In commemoration of it they assumed as one of their civic badges the blazing crescent and star, which has descended to our own days and is still used as an emblem by the present owners of the city--the Ottoman Sultans. ', 
              'But after repulsing Philip the Byzantines had to submit some years later to Alexander. They formed under him part of the enormous Macedonian empire, and passed on his decease through the hands of his successors--Demetrius Poliorcetes, and Lysimachus. After the death of the latter in battle, however, they recovered a precarious freedom, and were again an independent community for a hundred years, till the power of Rome invaded the regions of Thrace and the Hellespont. Byzantium was one of the cities which took the wise course of making an early alliance with the Romans, and obtained good and easy terms in consequence. During the wars of Rome with Macedon and Antiochus the Great it proved such a faithful assistant that the Senate gave it the status of a _civitas libera et foederata_, \"a free and confederate city,\" and it was not taken under direct Roman government, but allowed complete liberty in everything save the control of its foreign relations and the payment of a tribute to Rome. It was not till the Roman Republic had long passed away, that the Emperor Vespasian stripped it of these privileges, and threw it into the province of Thrace, to exist for the future as an ordinary provincial town [A.D. 73]. ', 
              'Though deprived of a liberty which had for long years been almost nominal, Byzantium could not be deprived of its unrivalled position for commerce. It continued to flourish under the _Pax Romana_, the long-continued peace which all the inner countries of the empire enjoyed during the first two centuries of the imperial _regime_, and is mentioned again and again as one of the most important cities of the middle regions of the Roman world. But an evil time for Byzantium, as for all the other parts of the civilized world, began when the golden age of the Antonines ceased, and the epoch of the military emperors followed. In 192 A.D., Commodus, the unworthy son of the great and good Marcus Aurelius, was murdered, and ere long three military usurpers were wrangling for his blood-stained diadem. Most unhappily for itself Byzantium lay on the line of division between the eastern provinces, where Pescennius Niger had been proclaimed, and the Illyrian provinces, where Severus had assumed the imperial style. The city was seized by the army of Syria, and strengthened in haste. Presently Severus appeared from the west, after he had made himself master of Rome and Italy, and fell upon the forces of his rival Pescennius. Victory followed the arms of the Illyrian legions, the east was subdued, and the Syrian emperor put to death. But when all his other adherents had yielded, the garrison of Byzantium refused to submit. ', 
              'For more than two years they maintained the impregnable city against the lieutenants of Severus, and it was not till A.D. 196 that they were forced to yield. The emperor appeared in person to punish the long-protracted resistance of the town; not only the garrison, but the civil magistrates of Byzantium were slain before his eyes. The massive walls \"so firmly built with great square stones clamped together with bolts of iron, that the whole seemed but one block,\" were laboriously cast down. The property of the citizens was confiscated, and the town itself deprived of all municipal privileges and handed over to be governed like a dependent village by its neighbours of Perinthus. Caracalla, the son of Severus, gave back to the Byzantines the right to govern themselves, but the town had received a hard blow, and would have required a long spell of peace to recover its prosperity. Peace however it was not destined to see. All through the middle years of the third century it was vexed by the incursions of the Goths, who harried mercilessly the countries on the Black Sea whose commerce sustained its trade. Under Gallienus in A.D. 263 it was again seized by an usurping emperor, and shared the fate of his adherents. The soldiers of Gallienus sacked Byzantium from cellar to garret, and made such a slaughter of its inhabitants that it is said that the old Megarian race who had so long possessed it were absolutely exterminated. ', 
              'But the irresistible attraction of the site was too great to allow its ruins to remain desolate. Within ten years after its sack by the army of Gallienus, we find Byzantium again a populous town, and its inhabitants are specially praised by the historian Trebellius Pollio for the courage with which they repelled a Gothic raid in the reign of Claudius II. The strong Illyrian emperors, who staved off from the Roman Empire the ruin which appeared about to overwhelm it in the third quarter of the third century, gave Byzantium time and peace to recover its ancient prosperity. It profited especially from the constant neighbourhood of the imperial court, after Diocletian fixed his residence at Nicomedia, only sixty miles away, on the Bithynian side of the Propontis. But the military importance of Byzantium was always interfering with its commercial greatness. After the abdication of Diocletian the empire was for twenty years vexed by constant partitions of territory between the colleagues whom he left behind him. Byzantium after a while found itself the border fortress of Licinius, the emperor who ruled in the Balkan Peninsula, while Maximinus Daza was governing the Asiatic provinces. ', 
              'While Licinius was absent in Italy, Maximinus treacherously attacked his rivals dominions without declaration of war, and took Byzantium by surprise. But the Illyrian emperor returned in haste, defeated his grasping neighbour not far from the walls of the city, and recovered his great frontier fortress after it had been only a few months out of his hands [A.D. 314]. The town must have suffered severely by changing masters twice in the same year; it does not, however, seem to have been sacked or burnt, as was so often the case with a captured city in those dismal days. But Licinius when he had recovered the place set to work to render it impregnable. Though it was not his capital he made it the chief fortress of his realm, which, since the defeat of Maximinus, embraced the whole eastern half of the Roman world. It was accordingly at Byzantium that Licinius made his last desperate stand, when in A.D. 323 he found himself engaged in an unsuccessful war with his brother-in-law Constantine, the Emperor of the West. For many months the war stood still beneath the walls of the city; but Constantine persevered in the siege, raising great mounds which overlooked the walls, and sweeping away the defenders by a constant stream of missiles, launched from dozens of military engines which he had erected on these artificial heights. At last the city surrendered, and the cause of Licinius was lost. Constantine, the last of his rivals subdued, became the sole emperor of the Roman world, and stood a victor on the ramparts which were ever afterwards to bear his name.')

ch2 <- paste0('When the fall of Byzantium had wrecked the fortunes of Licinius, the Roman world was again united beneath the sceptre of a single master. For thirty-seven years, ever since Diocletian parcelled out the provinces with his colleagues, unity had been unknown, and emperors, whose number had sometimes risen to six and sometimes sunk to two, had administered their realms on different principles and with varying success. Constantine, whose victory over his rivals had been secured by his talents as an administrator and a diplomatist no less than by his military skill, was one of those men whose hard practical ability has stamped upon the history of the world a much deeper impress than has been left by many conquerors and legislators of infinitely greater genius. He was a man of that self-contained, self-reliant, unsympathetic type of mind which we recognize in his great predecessor Augustus, or in Frederic the Great of Prussia. Constantine the Great Though the strain of old Roman blood in his veins must have been but small, Constantine was in many ways a typical Roman; the hard, cold, steady, unwearying energy, which in earlier centuries had won the empire of the world, was once more incarnate in him. ', 
              'But if Roman in character, he was anything but Roman in his sympathies. Born by the Danube, reared in the courts and camps of Asia and Gaul, he was absolutely free from any of that superstitious reverence for the ancient glories of the city on the Tiber which had inspired so many of his predecessors. Italy was to him but a secondary province amongst his wide realms. When he distributed his dominions among his heirs, it was Gaul that he gave as the noblest share to his eldest and best-loved son: Italy was to him a younger childs portion. There had been emperors before him who had neglected Rome: the barbarian Maximinus I. had dwelt by the Rhine and the Danube; the politic Diocletian had chosen Nicomedia as his favourite residence. But no one had yet dreamed of raising up a rival to the mistress of the world, and of turning Rome into a provincial town. If preceding emperors had dwelt far afield, it was to meet the exigencies of war on the frontiers or the government of distant provinces. ', 
              'It was reserved for Constantine to erect over against Rome a rival metropolis for the civilized world, an imperial city which was to be neither a mere camp nor a mere court, but the administrative and commercial centre of the Roman world. For more than a hundred years Rome had been a most inconvenient residence for the emperors. The main problem which had been before them was the repelling of incessant barbarian inroads on the Balkan Peninsula; the troubles on the Rhine and the Euphrates, though real enough, had been but minor evils. Rome, placed half way down the long projection of Italy, handicapped by its bad harbours and separated from the rest of the empire by the passes of the Alps, was too far away from the points where the emperor was most wanted--the banks of the Danube and the walls of Sirmium and Singidunum. For the ever-recurring wars with Persia it was even more inconvenient; but these were less pressing dangers; no Persian army had yet penetrated beyond Antioch--only 200 miles from the frontier--while in the Balkan Peninsula the Goths had broken so far into the heart of the empire as to sack Athens and Thessalonica. ', 
              'Constantine, with all the Roman world at his feet, and all its responsibilities weighing on his mind, was far too able a man to overlook the great need of the day--a more conveniently placed administrative and military centre for his empire. He required a place that should be easily accessible by land and sea--which Rome had never been in spite of its wonderful roads--that should overlook the Danube lands, without being too far away from the East; that should be so strongly situated that it might prove an impregnable arsenal and citadel against barbarian attacks from the north; that should at the same time be far enough away from the turmoil of the actual frontier to afford a safe and splendid residence for the imperial court. The names of several towns are given by historians as having suggested themselves to Constantine. First was his own birth-place--Naissus (Nisch) on the Morava, in the heart of the Balkan Peninsula; but Naissus had little to recommend it: it was too close to the frontier and too far from the sea. Sardica--the modern Sofia in Bulgaria--was liable to the same objections, and had not the sole advantage of Naissus, that of being connected in sentiment with the emperors early days. Nicomedia on its long gulf at the east end of the Propontis was a more eligible situation in every way, and had already served as an imperial residence. ', 
              'But all that could be urged in favour of Nicomedia applied with double force to Byzantium, and, in addition, Constantine had no wish to choose a city in which his own memory would be eclipsed by that of his predecessor Diocletian, and whose name was associated by the Christians, the class of his subjects whom he had most favoured of late, with the persecutions of Diocletian and Galerius. For Ilium, the last place on which Constantine had cast his mind, nothing could be alleged except its ancient legendary glories, and the fact that the mythologists of Rome had always fabled that their city drew its origin from the exiled Trojans of AEneas. Though close to the sea it had no good harbour, and it was just too far from the mouth of the Hellespont to command effectually the exit of the Euxine. Byzantium, on the other hand, was thoroughly well known to Constantine. For months his camp had been pitched beneath its walls; he must have known accurately every inch of its environs, and none of its military advantages can have missed his eye. Nothing, then, could have been more natural than his selection of the old Megarian city for his new capital. Yet the Roman world was startled at the first news of his choice; Byzantium had been so long known merely as a great port of call for the Euxine trade, and as a first-class provincial fortress, that it was hard to conceive of it as a destined seat of empire. When once Constantine had determined to make Byzantium his capital, in preference to any other place in the Balkan lands, his measures were taken with his usual energy and thoroughness. The limits of the new city were at once marked out by solemn processions in the old Roman style. ', 
              'In later ages a picturesque legend was told to account for the magnificent scale on which it was planned. The emperor, we read, marched out on foot, followed by all his court, and traced with his spear the line where the new fortifications were to be drawn. As he paced on further and further westward along the shore of the Golden Horn, till he was more than two miles away from his starting-point, the gate of old Byzantium, his attendants grew more and more surprised at the vastness of his scheme. At last they ventured to observe that he had already exceeded the most ample limits that an imperial city could require. But Constantine turned to rebuke them: \"I shall go on,\" he said, \"until He, the invisible guide who marches before me, thinks fit to stop.\" Guided by his mysterious presentiment of greatness, the emperor advanced till he was three miles from the eastern angle of Byzantium, and only turned his steps when he had included in his boundary line all the seven hills which are embraced in the peninsula between the Propontis and the Golden Horn. ', 'The rising ground just outside the walls of the old city, where Constantines tent had been pitched during the siege of A.D. 323, was selected out as the market-place of the new foundation. There he erected the _Milion_, or \"golden milestone,\" from which all the distances of the eastern world were in future to be measured. This \"central point of the world\" was not a mere single stone, but a small building like a temple, its roof supported by seven pillars; within was placed the statue of the emperor, together with that of his venerated mother, the Christian Empress Helena. The south-eastern part of the old town of Byzantium was chosen by Constantine for the site of his imperial palace. The spot was cleared of all private dwellings for a space of 150 acres, to give space not only for a magnificent residence for his whole court, but for spacious gardens and pleasure-grounds. A wall, commencing at the Lighthouse, where the Bosphorus joins the Propontis, turned inland and swept along parallel to the shore for about a mile, in order to shut off the imperial precinct from the city. ', 
              'The Heart of Constantinople North-west of the palace lay the central open space in which the life of Constantinople was to find its centre. This was the \"Augustaeum,\" a splendid oblong forum, about a thousand feet long by three hundred broad. It was paved with marble and surrounded on all sides by stately public buildings. To its east, as we have already said, lay the imperial palace, but between the palace and the open space were three detached edifices connected by a colonnade. Of these, the most easterly was the Great Baths, known, from their builder, as the \"Baths of Zeuxippus.\" They were built on the same magnificent scale which the earlier emperors had used in Old Rome, though they could not, perhaps, vie in size with the enormous Baths of Caracalla. Constantine utilized and enlarged the old public bath of Byzantium, which had been rebuilt after the taking of the city by Severus. ', 
              'He adorned the frontage and courts of the edifice with statues taken from every prominent town of Greece and Asia, the old Hellenic masterpieces which had escaped the rapacious hands of twelve generations of plundering proconsuls and Caesars. There were to be seen the Athene of Lyndus, the Amphithrite of Rhodes, the Pan which had been consecrated by the Greeks after the defeat of Xerxes, and the Zeus of Dodona. Adjoining the Baths, to the north, lay the second great building, on the east side of the Augustaeum--the Senate House. Constantine had determined to endow his new city with a senate modelled on that of Old Rome, and had indeed persuaded many old senatorial families to migrate eastward by judicious gifts of pensions and houses. We know that the assembly was worthily housed, but no details survive about Constantines building, on account of its having been twice destroyed within the century. But, like the Baths of Zeuxippus, it was adorned with ancient statuary, among which the Nine Muses of Helicon are specially cited by the historian who describes the burning of the place in A.D. 404. Linked to the Senate House by a colonnade, lay on the north the Palace of the Patriarch, as the Bishop of Byzantium was ere long to be called, when raised to the same status as his brethren of Antioch and Alexandria. ', 
              'A fine building in itself, with a spacious hall of audience and a garden, the patriarchal dwelling was yet completely overshadowed by the imperial palace which rose behind it. And so it was with the patriarch himself: he lived too near his royal master to be able to gain any independent authority. Physically and morally alike he was too much overlooked by his august neighbour, and never found the least opportunity of setting up an independent spiritual authority over against the civil government, or of founding an _imperium in imperio_ like the Bishop of Rome. The Atmeidan Hippodrome And St. Sophia. All along the western side of the Augustaeum, facing the three buildings which we have already described, lay an edifice which played a very prominent part in the public life of Constantinople. This was the great Hippodrome, a splendid circus 640 cubits long and 160 broad, in which were renewed the games that Old Rome had known so well. The whole system the chariot-races between the teams that represented the \"factions\" of the Circus was reproduced at Byzantium with an energy that even surpassed the devotion of the Romans to horse racing. ', 
              'From the first foundation of the city the rivalry of the \"Blues\" and the \"Greens\" was one of the most striking features of the life of the place. It was carried far beyond the circus, and spread into all branches of life. We often hear of the \"Green\" faction identifying itself with Arianism, or of the \"Blue\" supporting a pretender to the throne. Not merely men of sporting interests, but persons of all ranks and professions, chose their colour and backed their faction. The system was a positive danger to the public peace, and constantly led to riots, culminating in the great sedition of A.D. 523, which we shall presently have to describe at length. In the Hippodrome the \"Greens\" always entered by the north-eastern gate, and sat on the east side; the \"Blues\" approached by the north-western gate and stretched along the western side. The emperors box, called the Kathisma, occupied the whole of the short northern side, and contained many hundreds of seats for the imperial retinue. The great central throne of the Kathisma was the place in which the monarch showed himself most frequently to his subjects, and around it many strange scenes were enacted. It was on this throne that the rebel Hypatius was crowned emperor by the mob, with his own wifes necklace for an impromptu diadem. Here also, two centuries later, the Emperor Justinian II. sat in state after his reconquest of Constantinople, with his rivals, Leontius and Apsimarus, bound beneath his footstool, while the populace chanted, in allusion to the names of the vanquished princes, the verse, \"Thou shalt trample on the Lion and the Asp.\" Down the centre of the Hippodrome ran the \"spina,\" or division wall, which every circus showed; it was ornamented with three most curious monuments, whose strange juxtaposition seemed almost to typify the heterogeneous materials from which the new city was built up. ', 
              'The first and oldest was an obelisk brought from Egypt, and covered with the usual hieroglyphic inscriptions; the second was the most notable, though one of the least beautiful, of the antiquities of Constantinople: it was the three-headed brazen serpent which Pausanias and the victorious Greeks had dedicated at Delphi in 479 B.C., after they had destroyed the Persian army at Plataea. The golden tripod, which was supported by the heads of the serpents, had long been wanting: the sacrilegious Phocians had stolen it six centuries before; but the dedicatory inscriptions engraved on the coils of the pedestal survived then and survive now to delight the archaeologist. The third monument on the \"spina\" was a square bronze column of more modern work, contrasting strangely with the venerable antiquity of its neighbours. By some freak of chance all three monuments have remained till our own day: the vast walls of the Hippodrome have crumbled away, but its central decorations still stand erect in the midst of an open space which the Turks call the Atmeidan, or place of horses, in dim memory of its ancient use. Along the outer eastern wall of the Hippodrome on the western edge of the Augustaeum, stood a range of small chapels and statues, the most important landmark among them being the _Milion_ or central milestone of the empire, which we have already described. ', 
              'The statues, few at first, were increased by later emperors, till they extended along the whole length of the forum. Constantines own contribution to the collection was a tall porphyry column surmounted by a bronze image which had once been the tutelary Apollo of the city of Hierapolis, but was turned into a representation of the emperor by the easy method of knocking off its head and substituting the imperial features. It was exactly the reverse of a change which can be seen at Rome, where the popes have removed the head of the Emperor Aurelius, and turned him into St. Peter, on the column in the Corso. Building A Palace (from a Byzantine MS.) North of the Hippodrome stood the great church which Constantine erected for his Christian subjects, and dedicated to the Divine Wisdom (_Hagia Sophia_). It was not the famous domed edifice which now bears that name, but an earlier and humbler building, probably of the Basilica-shape then usual. Burnt down once in the fifth and once in the sixth centuries, it has left no trace of its original character. From the west door of St. Sophia a wooden gallery, supported on arches, crossed the square, and finally ended at the \"Royal Gate\" of the palace. By this the emperor would betake himself to divine service without having to cross the street of the Chalcoprateia (brass market), which lay opposite to St. Sophia. The general effect of the gallery must have been somewhat like that of the curious passage perched aloft on arches which connects the Pitti and Uffizi palaces at Florence. The edifices which we have described formed the heart of Constantinople. Between the Palace, the Hippodrome, and the Cathedral most of the important events in the history of the city took place. But to north and west the city extended for miles, and everywhere there were buildings of note, though no other cluster could vie with that round the Augustaeum. The Church of the Holy Apostles, which Constantine destined as the burying-place of his family, was the second among the ecclesiastical edifices of the town. Of the outlying civil buildings, the public granaries along the quays, the Golden Gate, by which the great road from the west entered the walls, and the palace of the praetorian praefect, who acted as governor of the city, must all have been well worthy of notice. ', 
              'A statue of Constantine on horseback, which stood by the last-named edifice, was one of the chief shows of Constantinople down to the end of the Middle Ages, and some curious legends gathered around it. Fifteenth-Century Drawing Of The Equestrian Statue Of Constantine. It was in A.D. 328 or 329--the exact date is not easily to be fixed--that Constantine had definitely chosen Byzantium for his capital, and drawn out the plan for its development. As early as May 11, 330, the buildings were so far advanced that he was able to hold the festival which celebrated its consecration. Christian bishops blessed the partially completed palace, and held the first service in St. Sophia; for Constantine, though still unbaptized himself, had determined that the new city should be Christian from the first. Of paganism there was no trace in it, save a few of the old temples of the Byzantines, spared when the older streets were levelled to clear the ground for the palace and adjoining buildings. The statues of the gods which adorned the Baths and Senate House stood there as works of art, not as objects of worship. To fill the vast limits of his city, Constantine invited many senators of Old Rome and many rich provincial proprietors of Greece and Asia to take up their abode in it, granting them places in his new senate and sites for the dwellings they would require. The countless officers and functionaries of the imperial court, with their subordinates and slaves, must have composed a very considerable element in the new population. The artizans and handicraftsmen were enticed in thousands by the offer of special privileges. Merchants and seamen had always abounded at Byzantium, and now flocked in numbers which made the old commercial prosperity of the city seem insignificant. Most effective--though most demoralizing--of the gifts which Constantine bestowed on the new capital to attract immigrants was the old Roman privilege of free distribution of corn to the populace. The wheat-tribute of Egypt, which had previously formed part of the public provision of Rome, was transferred to the use of Constantinople, only the African corn from Carthage being for the future assigned for the subsistence of the older city. On the completion of the dedication festival in 330 A.D. an imperial edict gave the city the title of New Rome, and the record was placed on a marble tablet near the equestrian statue of the emperor, opposite the Strategion. But \"New Rome\" was a phrase destined to subsist in poetry and rhetoric alone: the world from the first very rightly gave the city the founders name only, and persisted in calling it Constantinople. ')

chapters <- tibble::tibble(chapter=1:2, text=c(ch1, ch2))
str(chapters)


# Specify the input column
word_freq <- chapters %>% 
    tidytext::unnest_tokens(output=word, input=text, token="words", format="text") %>% 
    # Obtain word frequencies
    count(chapter, word) 

# Test equality
word_freq %>% 
    filter(word == "after")


corpus <- data.frame(text=c('Due to bad loans, the bank agreed to pay the fines', 'If you are late to pay off your loans to the bank, you will face fines', 'A new restaurant opened in downtown', 'There is a new restaurant that just opened on Warwick street', 'How will you pay off the loans you will need for the restaurant you want opened?'), 
                     id=paste0("id_", 1:5), 
                     stringsAsFactors = FALSE
                     )
corpus


# The call to posterior(mod)$topics returns the probabilities of topics.
dtm <- corpus %>% 
    # Specify the input column
    tidytext::unnest_tokens(input=text, output=word, drop=TRUE) %>% 
    count(id, word) %>% 
    # Specify the token
    tidytext::cast_dtm(document=id, term=word, value=n)

mod = topicmodels::LDA(x=dtm, k=2, method="Gibbs", control=list(alpha=1, delta=0.1, seed=10005))

modeltools::posterior(mod)$topics


# Generate the document-term matrix
dtm <- corpus %>% 
    tidytext::unnest_tokens(input=text, output=word) %>% 
    count(id, word) %>% 
    tidytext::cast_dtm(document=id, term=word, value=n)

# Run the LDA for two topics
mod <- topicmodels::LDA(x=dtm, k=2, method="Gibbs",control=list(alpha=1, delta=0.1, seed=10005))

# Retrieve the probabilities of word `will` belonging to topics 1 and 2
broom::tidy(mod, matrix="beta") %>%
    filter(term == "will")

# Make a stacked column chart showing the probabilities of documents belonging to topics
broom::tidy(mod, matrix="gamma") %>% 
    mutate(topic = as.factor(topic)) %>% 
    ggplot(aes(x=document, y=gamma)) + 
    geom_col(aes(fill=topic))

```
  
  
  
***
  
Chapter 2 - Word Clouds, Stop Words, Control Arguments  
  
Random Nature of LDA Algorithm:  
  
* The LDA call has a random component - random search through the parameter space to find the best match (uses log-likelihood)  
	* Gibbs sampling - a type of Monte Carlo Markov Chain (MCMC) algorithm  
    * Tries different combinations of probabilities of topics in documents, and probabilities of words in topics: e.g. (0.5, 0.5) vs. (0.8, 0.2)  
    * The combinations are influenced by parameters alpha and delta  
    * Argument seed sets the starting point for the pseudo-random number generator  
    * Setting seeds ensures replication of results between runs  
    * Argument iter controls the number of iterations of algorithm - more iterations takes longer while maximizing the likelihood of finding the best solution  
* Intermediate results can be managed using control parameters for C code that is called by topicmodels  
	* topicmodels calls a piece of code written in C  
    * Argument thin specifies how often to return the result of search  
    * control=list(thin=1)  
    * Setting thin=1 will return result for every step, and the best one will be picked  
    * Most efficient, but slows down the execution  
* Approach for getting the most probable words in each topic  
	* LDA model object contains matrix beta with probabilities of words in topics  
    * Use function tidy to extract  
    * If we want to get top 5 words from each topic:  
    * Retrieve the matrix by calling tidy(model, matrix="beta") and sort by probabilities, filter by row number  
    * tidy(mod, matrix="beta") %>% group_by(topic) %>% arrange(desc(beta)) %>% filter(row_number() <=3) %>% ungroup() %>% arrange(topic, desc(beta))  
* Can get the top-k words (or words above a threshhold) using the function terms()  
	* Function terms from topicmodels will return either top k words or all words with probability above threshold  
    * terms(mod, k=5)  
    * terms(mod, threshold=0.05)  
  
Manipulating Vocabulary:  
  
* Often there are either words we want to keep or words we want to discard (stopwords)  
	* Stopwords are service words that are considered as noise and must be removed - they obscure word associations in topics  
* Can use the anti_join for filtering out stopwords  
	* tidytext comes with a table stop_words containing stop words from several lexicons  
    * d = data.frame(term=c("we", "went", "fishing", "slept"), count=c(2, 1, 3, 1), stringsAsFactors = F)  
    * d %>% anti_join(stop_words, by=c("term"="word"))  
* Can instead keep a targeted list of needed words using inner_join()  
	* d = data.frame(term=c("we", "went", "fishing", "slept"), count=c(2, 1, 3, 1), stringsAsFactors = F)  
    * dictionary = data.frame(term=c("fishing", "slept"), stringsAsFactors = F)  
    * d %>% inner_join(dictionary, by="term")  
  
Word Clouds:  
  
* Word clouds can be better than bar charts when there are a large number of words  
	* wordcloud will draw a cloud of text labels, with font size proportionate to frequency of the word  
    * Required arguments - a vector of words, and the vector of word frequencies  
    * No need to sort the words by frequency  
    * word_frequencies <- corpus %>% unnest_tokens(input=text, output=word) %>% count(word)  
* The wordcloud::wordcloud() function takes word frequencies and several key arguments  
	* Specify number of words shown max.words  
    * Specify the range of word frequencies, min.freq and max.freq  
    * wordcloud::wordcloud(words=word_frequencies$word, freq=word_frequencies$n, min.freq=1, max.words=20)  
* Can further add colors and rotations to the wordcloud for an improved aesthetic  
	* colors takes a vector of colors  
    * rot.per is percentage of rotated words. Default is 0.1  
    * wordcloud::wordcloud(words=word_frequencies$word, freq=word_frequencies$n, min.freq=1, colors=c("DarkOrange", "CornflowerBlue", "DarkRed"), rot.per=0.3, max.words=20)  
* Need to convert LDA data (percentages) to data formatted as expected by wordcloud (integers)  
	* wordcloud expects integer values for word frequencies  
    * LDA returns probabilities - decimal fractions  
    * Solution: multiply by a large number, truncate the fractional part  
    * mod <- LDA(x=dtm, k=2, method="Gibbs", control=list(alpha=1, thin=1, seed=10005))  
    * word_frequencies <- tidy(mod, matrix="beta") %>% mutate(n = trunc(beta * 10000)) %>% filter(topic == 1)  
    * wordcloud::wordcloud(words=word_frequencies$term, freq=word_frequencies$n, max.words=20, colors=c("DarkOrange", "CornflowerBlue", "DarkRed"), rot.per=0.3)  
  
History of the Byzantine Empire:  
  
* The Byzantine Empire (East Rome) existed from 330 CE to 1453 CE - capital was in Constantinople  
	* The text: The Byzantine Empire, by Charles Oman, printed in 1902, available from Project Guttenberg (https://www.gutenberg.org/)  
    * Twenty six chapters arranged in chronological order  
    * Package gutenbergr enables direct download of texts - dataframe with lines of text  
    * Dataframe history with two columns: text and chapter  
* Goal is to find predominant themes in each of the periods, then compare with domain expertise and plot using ggplot2  
  
Example code includes:  
```{r}

dtm <- dtm[, c("bank", "fines", "loans", "pay", "new", "opened", "restaurant")]
dtm


# Display column names
colnames(dtm)

# Fit an LDA model for 2 topics using Gibbs sampling
mod <- topicmodels::LDA(x=dtm, k=2, method="Gibbs", control=list(alpha=1, seed=10005, thin=1))

# Convert matrix beta into tidy format and filter on topic number and term
broom::tidy(mod, matrix="beta") %>%
    filter(topic==2, term=="opened")


# The call to posterior(mod)$topics returns the probabilities of topics.
dtm <- corpus %>% 
    # Specify the input column
    tidytext::unnest_tokens(input=text, output=word, drop=TRUE) %>% 
    count(id, word) %>% 
    # Specify the token
    tidytext::cast_dtm(document=id, term=word, value=n)


# Fit LDA topic model using Gibbs sampling for 2 topics
mod1 <- topicmodels::LDA(x=dtm, k=2, method="Gibbs", control=list(alpha=1, seed=10005, thin=1))

# Display the probabilities of topics in documents side by side
broom::tidy(mod1, "gamma") %>% spread(topic, gamma)

# Fit LDA topic model using Gibbs sampling for 2 topics
mod2 <- topicmodels::LDA(x=dtm, k=2, method="Gibbs", control=list(alpha=25, seed=10005, thin=1))

# Display the probabilities of topics in documents side by side
broom::tidy(mod2, "gamma") %>% spread(topic, gamma)


# Create the document-term matrix
dtm <- corpus %>%
    tidytext::unnest_tokens(output=word, input=text) %>%
    count(id, word) %>%
    tidytext::cast_dtm(document=id, term=word, value=n)

# Display dtm as a matrix
as.matrix(dtm)


# Create the document-term matrix with stop words removed
dtm <- corpus %>%
    tidytext::unnest_tokens(output=word, input=text) %>%
    anti_join(tidytext::stop_words) %>% 
    count(id, word) %>%
    tidytext::cast_dtm(document=id, term=word, value=n)

# Display the matrix
as.matrix(dtm)


dictionary <- tibble::tibble(word=c("bank", "fines", "loans", "pay", "new", "opened", "restaurant"))
dictionary

# Perform inner_join with the dictionary table
dtm <- corpus %>%
    tidytext::unnest_tokens(output=word, input=text) %>%
    inner_join(dictionary) %>%
    count(id, word) %>%
    tidytext::cast_dtm(document=id, term=word, value=n)

# Display the summary of dtm
as.matrix(dtm)


# Generate the counts of words in the corpus
word_frequencies <- corpus %>% 
    tidytext::unnest_tokens(input=text, output=word) %>%
    count(word)

# Create a wordcloud
wordcloud::wordcloud(words=word_frequencies$word, freq=word_frequencies$n, min.freq=1, max.words=10, 
                     colors=c("DarkOrange", "Blue"), random.order=FALSE, random.color=FALSE
                     )


# DO NOT HAVE FILE 'history'
# Construct a document-term matrix
# dtm <- history %>% 
#     tidytext::unnest_tokens() %>%
#     anti_join(stop_words) %>% 
#     count(chapter, word) %>% 
#     tidytext::cast_dtm(document=chapter, term=word, value=n)
# 
# # Insert the missing arguments
# mod <- topicmodels::LDA(x=dtm, k=4, method="Gibbs", control=list(alpha=1, seed=10005))
# 
# # Display top 15 words of each topic
# terms(mod, k=15)
# 
# 
# # Display the structure of the verbs dataframe
# str(verbs)
# 
# # Construct a document-term matrix
# dtm <- history %>% 
#     tidytext::unnest_tokens() %>%
#     inner_join(verbs, by=c("word"="past")) %>% 
#     count(chapter, word) %>% 
#     tidytext::cast_dtm(document=chapter, term=word, value=n)
# 
# # Fit LDA for four topics
# mod <- topicmodels::LDA(x=dtm, k=4, method="Gibbs", control=list(alpha=1, seed=10005))
# 
# # Display top 25 words from each topic
# terms(mod, k=25)
# 
# 
# # Extract matrix gamma and plot it
# broom::tidy(mod, "gamma") %>% 
#     mutate(document=as.numeric(document)) %>% 
#     ggplot(aes(x=document, y=gamma)) + 
#     geom_line(aes(color=factor(topic))) + 
#     labs(x="Chapter", y="Topic probability") +
#     scale_color_manual(values=brewer.pal(n=4, "Set1"), name="Topic")
# 
# 
# 
# # Display wordclouds one at a time
# for (j in 1:4) {
#     # Generate a table with word frequences for topic j
#     word_frequencies <- broom::tidy(mod, matrix="beta") %>% 
#         mutate(n = trunc(beta * 10000)) %>% 
#         filter(topic == j)
#     # Display word cloud
#     wordcloud::wordcloud(word = word_frequencies$term, freq = word_frequencies$n, max.words = 20,
#                          scale = c(3, 0.5), colors = c("DarkOrange", "CornflowerBlue", "DarkRed"), 
#                          rot.per = 0.3
#                          )
# }

```
  
  
  
***
  
Chapter 3 - Named Entity Recognition as Unsupervised Classification  
  
Using Topic Models as Classifiers:  
  
* Can use a topic model as a soft classifier - find probability of belonging to a specific class  
	* Named entity recognition - is an entity a geographic name or a person?  
    * "Washington crossed the Delaware river" vs. "They did a road trip across Washington"  
* The control paramters play an important role  
	* A very high alpha may drive 50-50 splits everywhere - greater than 1 puts most of the distribution in the center, less than 1 puts most of the distribution at the edges  
* Bag of M&Ms as an example for how an LDA fits a model  - multinomial distribution  
	* Several outcomes (colors of candy) repeated n times  
    * Each outcome has its own probability - fixed by the factory that filled the bag  
    * Probabilities sum up to 1  
    * What's the probability of getting 5 yellow, 2 brown, 2 blue, and 1 black when we take out 10 pieces of candy?  
    * In LDA model, topics are color, and there are two "bags of candy": one for documents, one for words  
* The Dirichlet aspect of LDA  
	* Randomized search for the best values of probabilities  
    * E.g., try 0.2, 0.5 and 0.3 for proportions of three topics 
    * Hard to do for large number of topics and words  
    * Instead, use values from Dirichlet distribution as guesses  
    * Dirichlet distribution returns a set of numbers that add up to 1 - serve as probabilities of colors for M&M candy bags  
  
From Word Windows to DTM:  
  
* An entity is a personalized noun - often capitalized  
	* If we take n words on the left and n words on the right of an entity, we get a word window  
    * E.g., attention of Megara was turned - entity is Megara and context is attention was turned  
    * Also possible to tag words to differentiate the side - attention_L1 of_L2 was_R1 turned_R2  
* Can combine the contexts of the same entity using paste to create a document  
	* docs <- df %>% group_by(entity) %>% summarise(doc_id = first(entity), text = paste(text, collapse=" "))  
* Can use regex to find entities in a document - take advantage of entities being capitalized  
	* pattern <- "[A-Z][a-z]+"  
    * m <- gregexpr(text, pattern)  
    * entities <- unlist(regmatches(text, m))  
* Can also run regular expressions using parentheses to group some patterns together  
	* Some entities include St. in them, e.g. St. Sophia 
    * p <- "(St[.] )?[A-Z][a-z]+" 
    * (St[.] ) is a group. The ? quantifier means the group is optional  
* Can also use capture groups to add a suffix  
	* t <- "the great Darius threw across"  
    * gsub("^([a-z]+) ([a-z]+)", "\\1_L1 \\2_L2", t)  # result is "the_L1 great_L2 Darius threw across"  
    * Two groups, each matches a lowercase word [a-z]+  
    * The ^ is an anchor - specifies position in the string. ^ - the start, $ - at the end  
    * The \\1 is back-reference to contents of group 1. Its contents are substituted  
  
Corpus Alignment and Classification:  
 
* Can also run models for unsupervised classification - telling the meaning of a named entity  
	* topics <- tidy(mod, matrix="gamma") %>% spread(topic, gamma)  
    * topics %>% filter(document %in% c(" Alboin"," Alexander"," Asia Minor"," Amorium"," Cappadocian"))  
* Ideally, want to use a pre-trained model, as is possible using topicmodels::posterior()  
	* model = LDA(...)  
    * result = posterior(model, new_data)  
    * result$topics  
    * new_data must be aligned with the vocabulary used in the model  
    * LDA algorithm iterates over items and their counts, does not "know" that it's dealing with words  
* Corpus alignment involves ensuring the training and test set have the same vocabularies and rules  
	* Drop words from dtm that are not part of model's vocabulary  
    * Function tidy with matrix="beta" extracts the terms and their probabilities  
    * model_vocab <- tidy(mod, matrix="beta") %>% select(term) %>% distinct()  
    * Do right-join with the model's vocabulary to keep only the words the model was trained on  
    * new_table <- new_doc %>% unnest_tokens(input=text, output=word) %>% count(doc_id, word) %>% right_join(model_vocab, by=c("word"="term"))  
* Can have challenges with NA introduced in to the process - as document name (set to first) and as counts (set to 0)  
	* new_dtm <- new_table %>% arrange(desc(doc_id)) %>% mutate(doc_id = ifelse(is.na(doc_id), first(doc_id), doc_id), n = ifelse(is.na(n), 0, n)) %>% cast_dtm(document=doc_id, term=word, value=n)  
* Hold-out data is valuable, similar to any other machine learning process  
	* Held-out data for testing  
    * Hold out a percentage of full records (same as with test datasets in ML)  
    * Hold out a percentage of terms inside a document (unique to topic modeling)  
    * Estimate quality of fit by looking at the log-likelihood  
    * held-out log-likelihood  
    * Our case: withhold full documents, no cross-validation  
  
Example code includes:  
```{r}

# The call to posterior(mod)$topics returns the probabilities of topics.
dtm <- corpus %>% 
    # Specify the input column
    tidytext::unnest_tokens(input=text, output=word, drop=TRUE) %>% 
    count(id, word) %>% 
    # Specify the token
    tidytext::cast_dtm(document=id, term=word, value=n)


# Fit a topic model using LDA with Gibbs sampling
mod = topicmodels::LDA(x=dtm, k=2, method="Gibbs", control=list(iter=500, thin=1, seed = 12345, alpha=NULL))

# Display topic prevalance in documents as a table
broom::tidy(mod, "gamma") %>% spread(topic, gamma)  


# Fit the model for delta = 0.1
mod <- topicmodels::LDA(x=dtm, k=2, method="Gibbs", control=list(iter=500, seed=12345, alpha=1, delta=0.1))

# Define which words we want to examine
my_terms = c("loans", "bank", "opened", "pay", "restaurant", "you")

# Make a tidy table
t <- broom::tidy(mod, "beta") %>% 
    filter(term %in% my_terms)

# Make a stacked column chart of word probabilities
ggplot(t, aes(x=term, y=beta)) + 
    geom_col(aes(fill=factor(topic))) +
    theme(axis.text.x=element_text(angle=90))

# Fit the model for delta = 0.5
mod <- topicmodels::LDA(x=dtm, k=2, method="Gibbs", control=list(iter=500, seed=12345, alpha=1, delta=0.5))

# Define which words we want to examine
my_terms = c("loans", "bank", "opened", "pay", "restaurant", "you")

# Make a tidy table
t <- broom::tidy(mod, "beta") %>% 
    filter(term %in% my_terms)

# Make a stacked column chart
ggplot(t, aes(x=term, y=beta)) + 
    geom_col(aes(fill=factor(topic))) +
    theme(axis.text.x=element_text(angle=90))


text <- c(ch1, ch2)

# Regex pattern for an entity and word context
p1 <- "( [a-z]+){2}( (St[.] )?[A-Z][a-z]+)+( [a-z]+){2}"

# Obtain the regex match object from gregexpr
m <- gregexpr(p1, text)

# Get the matches and flatten the list
v <- unlist(regmatches(text, m))

# Find the number of elements in the vector
length(v)

# Regex pattern for an entity and word context
p2 <- "( [a-z]+){2}( (St[.] )?[A-Z][a-z]+( (of|the) [A-Z][a-z]+)?)+( [a-z]+){2}"

# Obtain the regex match object from gregexpr
m <- gregexpr(p2, text)

# Get the matches and flatten the list
v <- unlist(regmatches(text, m))

# Find the number of elements in the vector
length(v)


entity_pattern <- "( (St[.] )?[A-Z][a-z]+( (of|the) [A-Z][a-z]+)?)+"
v <- c(' into the European shore of', ' settlers were Greeks of the', ' cities of Hellas in the', ' to the West to plant', ' attention of Megara was turned', ' of the Black Sea and the', ' behind the North Wind and know', ' wonders the Greeks sailed ever', ' of the Golden Fleece they did', ' metals of Colchis and the', ' of the Bosphorus and the', ' foundation of Byzantium was but', ' years before Byzantium came into', ' band of Megarian colonists had', ' the opposite Asiatic shore of', ' oracle of Delphi to give', ' that the Chalcedonians were truly', ' less inviting Bithynian side of', ' marked out Byzantium as destined', ' with all Europe behind it', ' early history Byzantium never fell', ' by the Danube mouth or', ' a hundred Hellenic towns on', ' house alone Byzantium would have', ' independent state Byzantium had a', ' the great Darius threw across', ' his son Xerxes crossed the', ' down the Bosphorus to pay', ' from the Oriental yoke seventeen', ' was at Byzantium that the', ' empire of Athens were laid', ' all the Greek states of', ' fifth century Byzantium twice declared', ' and the Byzantines escaped anything', ' blessing gave Byzantium its chief', ' wines of Maronea and other', ' that the Byzantines were eating', ' rise of Philip of Macedon and his', ' by the Byzantines as a', ' after repulsing Philip the Byzantines had to', ' power of Rome invaded the', ' regions of Thrace and the', ' that the Senate gave it', ' till the Roman Republic had long', ' that the Emperor Vespasian stripped it', ' for itself Byzantium lay on', ' and the Syrian emperor put', ' garrison of Byzantium refused to', ' magistrates of Byzantium were slain', ' to the Byzantines the right', ' on the Black Sea whose commerce', ' the old Megarian race who', ' we find Byzantium again a', ' the historian Trebellius Pollio for the', ' repelled a Gothic raid in', ' from the Roman Empire the ruin', ' on the Bithynian side of', ' importance of Byzantium was always', ' abdication of Diocletian the empire', ' and took Byzantium by surprise')

# Print out contents of the `entity_pattern`
entity_pattern

# Remove the named entity from text
v2 <- gsub(entity_pattern, "", v)

# Display the head of v2
head(v2)

# Remove the named entity
v2 <- gsub(entity_pattern, "", v)

# Pattern for inserting suffixes
p <- "\\1_L1 \\2_L2 \\3_R1 \\4_R2"

# Add suffixes to words
context <- gsub("([a-z]+) ([a-z]+) ([a-z]+) ([a-z]+)", p, v2)

# Extract named entity and use it as document ID
doc_id <- unlist(regmatches(v, gregexpr(entity_pattern, v)))

# Make a data frame with columns doc_id and text
corpus <- data.frame(doc_id = doc_id, text = context, stringsAsFactors = F)


# Summarize the text to produce a document for each doc_id
corpus2 <- corpus %>% 
    group_by(doc_id) %>% 
    summarise(doc = paste(text, collapse=" "))

# Make a document-term matrix
dtm <- corpus2 %>% 
    tidytext::unnest_tokens(input=doc, output=word) %>% 
    count(doc_id, word) %>%
    tidytext::cast_dtm(document=doc_id, term=word, value=n)

# Fit an LDA model for 3 topics
mod <- topicmodels::LDA(x=dtm, k=3, method="Gibbs", control=list(alpha=1, seed=12345, iter=1000, thin=1))

# Create a table with probabilities of topics in documents
topics <- broom::tidy(mod, matrix="gamma") %>% 
    spread(topic, gamma)


# Set random seed for reproducability
set.seed(12345)

# Take a sample of 20 random integers, without replacement
r <- sample.int(n=nrow(corpus2), size=20, replace=FALSE)

# Generate a document-term matrix
train_dtm <- corpus2[-r, ] %>% 
    tidytext::unnest_tokens(input=doc, output=word) %>% 
    count(doc_id, word) %>% 
    tidytext::cast_dtm(document=doc_id, term=word, value=n)

# Fit an LDA topic model for k=3
train_mod <- topicmodels::LDA(x=train_dtm, k=3, method="Gibbs", 
                              control=list(alpha=1, seed=10001, iter=1000, thin=1)
                              )


# Get the test row indices
set.seed(12345)
r <- sample.int(n=nrow(corpus2), size=20, replace=FALSE)

# Extract the vocabulary of the training model
model_vocab <- broom::tidy(train_mod, matrix="beta") %>% 
    select(term) %>% 
    distinct()

# Create a table of counts with aligned vocabularies
test_table <- corpus2[r, ] %>% 
    tidytext::unnest_tokens(input=doc, output=word) %>% 
    count(doc_id, word) %>%
    right_join(model_vocab, by=c("word"="term"))

# Prepare a document-term matrix
test_dtm <- test_table %>% 
    arrange(desc(doc_id)) %>% 
    mutate(doc_id = ifelse(is.na(doc_id), first(doc_id), doc_id), n = ifelse(is.na(n), 0, n)) %>% 
    tidytext::cast_dtm(document=doc_id, term=word, value=n)


# Obtain posterior probabilities for test documents
results <- modeltools::posterior(object=train_mod, newdata=test_dtm)

# Display the matrix with topic probabilities
results$topics

```
  
  
  
***
  
Chapter 4 - How Many Topics is Enough?  
  
Finding the Best Number of Topics:  
  
* Several approaches to picking the optimal number of topics  
	* Topic coherence - examine the words in topics, decide if they make sense (e.g. site, settlement, excavation, popsicle - low coherence)  
    * Quantitative measures  
    * Log-likelihood - how plausible model parameters are given the data  
    * Perplexity - model's "surprise" at the data  
* The log-likelihood measures the plausibility of the parameters given the data - logs allow for summing rather than multiplying  
	* Likelihood - measure of how plausible model parameters are given the data  
    * Taking a logarithm makes calculations easier  
    * All values are negative: when x<1, log(x) < 0  
    * Numerical optimization - search for the largest log-likelihood (-100 is better than -105)  
    * Function logLik returns log-likelihood of an LDA model  
* Perplexity is a measure of the model's surprise with the data - positive number, smaller is better  
	* perplexity(object=mod, newdata=dtm)  
* Can run results for multiple values of k, then plot and find optimal performance on log-likelihood and perplexity  
	* Similar to clustering, the goal is to find an elbow point  
    * Searching for best k can take a lot of time  
    * Factors: number of documents, number of terms, and number of iterations  
    * Model fitting can be resumed  
    * Function LDA accepts an LDA model as an object for initialization  
    * mod = LDA(x=dtm, method="Gibbs", k=4, control=list(alpha=0.5, seed=12345, iter=1000, keep=1))  
    * mod2 = LDA(x=dtm, model=mod, control=list(thin=1, seed=10000, iter=200))  
  
Topic Models Fitted to Novels:  
  
* A topic model can be used to analyze one long work - for example, a novel such as "Moby Dick"  
	* Documents are chunks long enough to capture an event or a scene in the plot  
    * For traditional novels - 1000+ words (roughly 3 pages of text)  
* Text chunks can be considered as chapters  
	* corpus %>% unnest_tokens(input=text, output=word) %>% count(chapter, word)  
    * With text chunks, we need to generate the "chapter number" on our own  
    * Candidate function: %/% - integer division  
    * corpus %>%  
    *     unnest_tokens(input=text, output=word) %>%  
    *     mutate(word_index = 1:n()) %>%  
    *     mutate(doc_number = word_index %/% 1000 + 1) %>%  
    *     count(doc_number, word) %>%  
    *     cast_dtm(term=word, document=doc_number, value=n)
* Picking a good chunk size is more an art than a science  
  
Locking Topics With Seed Words:  
  
* Passing seeds for random numbers helps to ensure reproducibility  
	* LDA performs randomized search through the space of parameters - Gibbs sampling  
    * Topic numbering is unstable (topic 1 need not be the same content area when re-run)  
* Can initialize the Gibbs sampler with seed words, which helps to lock in some of the topic numbers  
	* Lock topic numbers  
    * Specify weights for seed words for topics  
    * seedwords requires a matrix, k rows, N columns  
    * k is number of topics, N is vocabulary size  
    * Weights get normalized internally so they sum up to 1 (need not be passed already summing to one)  
* Example for a tiny dataset with dtm of 5x34 and k=2  
	* Matrix for seed words needs to be 2x34 - the number of rows is k and the number of columns matches the dtm  
    * seedwords = matrix(nrow=2, ncol=34, data=0)  
    * colnames(seedwords) = colnames(dtm)  
    * seedwords[1, "restaurant"] = 1  
    * seedwords[2, "loans"] = 1  
    * lda_mod = LDA(x=dtm, k=2, method="Gibbs", control=list(alpha=1, seed=1234))  
    * tidy(lda_mod, "beta") %>% spread(key=topic,value=beta) %>% filter(term %in% c("restaurant","loans"))  
    * lda_mod = LDA(x=dtm, k=2, method="Gibbs", seedwords=seedwords, control=list(alpha=1, seed=1234))  
    * tidy(lda_mod, "beta") %>% spread(key=topic,value=beta) %>% filter(term %in% c("restaurant","loans"))  
* Seed words are useful for two reasons  
	* Convenient for pre-trained models  
    * Training a model involves multiple runs of the algorithm, even for the same k  
    * Seedwords let us "lock" topic numbers  
    * Helpful input for training models  
    * Speed up algorithm convergence by providing a starting point  
  
Wrap Up:  
  
* LDA topic modeling returns a probability (soft classifier)  
* LDA requires count data as an input - can scale/transform non-integer data to counts and use successfully in LDA  
* Variable Expectation Management (VEM) can also be applied to topic modeling  
	* Can be applied to correlated topic models  
    * Topic proportions follow a multivariate normal distribution  
* Package stm by Margaret Roberts, Brandon Stewart, Dustin Lingley, and Kenneth Benoit  
	* regression modeling of topic proportions and covariates  
    * automatic corpus alignment  
    * held-out data as omitted words in documents  
    * can use result of LDA model as a seed  
* Deep learning and word embeddings (word-to-vector) predict word meanings based on word adjacencies - Word2Vec  
	* Use deep learning neural network to predict words that occur adjacent to a word, \pm n±n with n=2n=2, or 44  
    * Transform into a vector of smaller dimensions (25, 50, 100)  
    * word2vec models use very large corpora (e.g., 2 billion words)  
    * do not make accommodations for multi-word entities  
    * take a long time to train  
* Experiment with package wordVectors created by Ben Schmidt  
  
Example code includes:  
```{r eval=FALSE}

# DO NOT HAVE THE DATA FRAME df
# Split the Abstract column into tokens
dtm <- df %>% 
   tidytext::unnest_tokens(input=Abstract, output=word) %>% 
   # Remove stopwords
   anti_join(stop_words) %>% 
   # Count the number of occurrences
   count(AwardNumber, word) %>% 
   # Create a document term matrix
   tidytext::cast_dtm(document=AwardNumber, term=word, value=n)


dtm <- df %>% 
   tidytext::unnest_tokens(input=Abstract, output=word) %>% 
   anti_join(stop_words) %>% 
   # Count occurences
   count(AwardNumber, word) %>%
   # Group the data
   group_by(word) %>% 
   # Filter for document wide frequency
   filter(sum(n) >= 10) %>% 
   # Ungroup the data andreate a document term matrix
   ungroup() %>% 
   tidytext::cast_dtm(document=AwardNumber, term=word, value=n)


# Create a LDA model
mod <- topicmodels::LDA(x=dtm, method="Gibbs", k=3, control=list(alpha=0.5, seed=1234, iter=500, thin=1))

# Retrieve log-likelihood
topicmodels::logLik(mod)

# Find perplexity
topicmodels::perplexity(object=mod, newdata=dtm)


# Display names of elements in the list
names(models[[1]])

# Retrieve the values of k and perplexity, and plot perplexity vs k
x <- sapply(models, '[[', "k")
y <- sapply(models, '[[', "perplexity")
plot(x, y, xlab="number of clusters, k", ylab="perplexity score", type="o")

# Record the new perplexity scores
new_perplexity_score <- numeric(length(models))

# Run each model for 100 iterations
for (i in seq_along(models)) {
    mod2 <- topicmodels::LDA(x=dtm, model=models[[i]]$model, control=list(iter=100, seed=12345, thin=1))
    new_perplexity_score[i] <- topicmodels::perplexity(object=mod2, newdata=dtm)
}

# Specify the possible values of k and build the plot
k <- 2:10
plot(x=k, y=new_perplexity_score, xlab="number of clusers, k", ylab="perplexity score", type="o")


t <- history %>% 
    # Unnest the tokens
    tidytext::unnest_tokens(input=text, output=word) %>% 
    # Create a word index column
    mutate(word_index = row_number()) %>% 
    # Create a document number column
    mutate(document_number = word_index %/% 1000 + 1)


dtm <- t %>% 
    # Join verbs on "word" and "past"
    inner_join(verbs, by=c("word"="past")) %>% 
    # Count word
    count(document_number, word) %>%
    # Create a document-term matrix
    tidytext::cast_dtm(document=document_number, term=word, value=n)


# Store the names of documents in a vector
required_documents <- c(" Africa", " Emperor Heraclius", " Adrianople", " Daniel", " African")

# Convert table into wide format
broom::tidy(mod, matrix="gamma") %>% 
    spread(key=topic, value=gamma) %>% 
    # Keep only the rows with document names matching the required documents
    filter(document %in% required_documents)


# Set up the column names
colnames(seedwords) <- colnames(dtm)

# Set the weights
seedwords[1, "defeated_l2"] = 1
seedwords[2, "across_l2"] = 1

# Fit the topic model
mod <- topicmodels::LDA(dtm, k=3, method="Gibbs", seedwords=seedwords, 
                        control=list(alpha=1, iter=500, seed=1234)
                        )

# Examine topic assignment in the fitted model
broom::tidy(mod, "gamma") %>% spread(topic, gamma) %>% 
    filter(document %in% c(" Daniel", " Adrianople", " African"))

```
  
  
  
***
  
### _Intermediate Interactive Data Visualization with plotly in R_  
  
Chapter 1 - Introduction and Review of plotly  
  
Interactive and Dynamic Graphics:  
  
* Interactive graphics can provide insights over and above standard plots  
* The plotly library is still under development and allows for interactive and dynamic plots  
* Should consider the report's intended usage - static, interactive, dynamic  
	* Interactive - changes in response to user input, such as zoom in, see the label when hovering over a point, etc.  
    * Dynamic - change automatically (e.g., animation)  
* Example for running plotly  
	* msci %>% plot_ly(x=~Date, y=~Close) %>% add_lines()  
  
Adding Aesthetics to Represent a Variable:  
  
* Can add variables for attributes such as color or point size  
	* add_markers(color=~income)  # income is a categorical in this example  
	* add_markers(symbol=~income)  # plotly calls symbol a glyph, and will also color by symbol  
    * add_markers(color=~population)  # continuous metrics with continuous scale  
    * add_markers(color=~log10(population))  # continuous metrics  
    * add_markers(size=~population)  # continuous metrics  
* Can further polish the labels and add axis labels  
	* happy %>% plot_ly(x=~x, y=~y, hoverinfo="text", text=~paste("My A: ", A, "</br> My B: ", B) %>% add_markers(size=~A) %>% layout(xaxis=list(title="My X Label"), yaxis=list(title="My Y Label"))  
  
Moving Beyond Simple Interactivity:  
  
* Can extend on interaction to include animations and linked brushing  
* Example for creating a static bubble chart  
    * myData %>% plot_ly(x=~myX, y=~myY, hoverinfo="text", text=~myLabel) %>% add_markers(size=~mySize, color=~myColor, marker=list(opacity=0.5, sizemode="diameter", sizeref=2))  
* Can create linked brushing where clicking a bar chart can also update the points highlighted in a scatter plot  
  
Example code includes:  
```{r eval=FALSE}

# load the plotly package
library(plotly)


acwiDate <- as.Date(c('2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-09', '2017-01-10', '2017-01-11', '2017-01-12', '2017-01-13', '2017-01-17', '2017-01-18', '2017-01-19', '2017-01-20', '2017-01-23', '2017-01-24', '2017-01-25', '2017-01-26', '2017-01-27', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-02', '2017-02-03', '2017-02-06', '2017-02-07', '2017-02-08', '2017-02-09', '2017-02-10', '2017-02-13', '2017-02-14', '2017-02-15', '2017-02-16', '2017-02-17', '2017-02-21', '2017-02-22', '2017-02-23', '2017-02-24', '2017-02-27', '2017-02-28', '2017-03-01', '2017-03-02', '2017-03-03', '2017-03-06', '2017-03-07', '2017-03-08', '2017-03-09', '2017-03-10', '2017-03-13', '2017-03-14', '2017-03-15', '2017-03-16', '2017-03-17', '2017-03-20', '2017-03-21', '2017-03-22', '2017-03-23', '2017-03-24', '2017-03-27', '2017-03-28', '2017-03-29', '2017-03-30', '2017-03-31', '2017-04-03', '2017-04-04', '2017-04-05', '2017-04-06', '2017-04-07', '2017-04-10', '2017-04-11', '2017-04-12', '2017-04-13', '2017-04-17', '2017-04-18', '2017-04-19', '2017-04-20', '2017-04-21', '2017-04-24', '2017-04-25', '2017-04-26', '2017-04-27', '2017-04-28', '2017-05-01', '2017-05-02', '2017-05-03', '2017-05-04', '2017-05-05', '2017-05-08', '2017-05-09', '2017-05-10', '2017-05-11', '2017-05-12', '2017-05-15', '2017-05-16', '2017-05-17', '2017-05-18', '2017-05-19', '2017-05-22', '2017-05-23', '2017-05-24', '2017-05-25', '2017-05-26', '2017-05-30', '2017-05-31', '2017-06-01', '2017-06-02', '2017-06-05', '2017-06-06', '2017-06-07', '2017-06-08', '2017-06-09', '2017-06-12', '2017-06-13', '2017-06-14', '2017-06-15', '2017-06-16', '2017-06-19', '2017-06-20', '2017-06-21', '2017-06-22', '2017-06-23', '2017-06-26', '2017-06-27', '2017-06-28', '2017-06-29', '2017-06-30', '2017-07-03', '2017-07-05', '2017-07-06', '2017-07-07', '2017-07-10', '2017-07-11', '2017-07-12', '2017-07-13', '2017-07-14', '2017-07-17', '2017-07-18', '2017-07-19', '2017-07-20', '2017-07-21', '2017-07-24', '2017-07-25', '2017-07-26', '2017-07-27', '2017-07-28', '2017-07-31', '2017-08-01', '2017-08-02', '2017-08-03', '2017-08-04', '2017-08-07', '2017-08-08', '2017-08-09', '2017-08-10', '2017-08-11', '2017-08-14', '2017-08-15', '2017-08-16', '2017-08-17', '2017-08-18', '2017-08-21', '2017-08-22', '2017-08-23', '2017-08-24', '2017-08-25', '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31', '2017-09-01', '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08', '2017-09-11', '2017-09-12', '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-18', '2017-09-19', '2017-09-20', '2017-09-21', '2017-09-22', '2017-09-25', '2017-09-26', '2017-09-27', '2017-09-28', '2017-09-29', '2017-10-02', '2017-10-03', '2017-10-04', '2017-10-05', '2017-10-06', '2017-10-09', '2017-10-10', '2017-10-11', '2017-10-12', '2017-10-13', '2017-10-16', '2017-10-17', '2017-10-18', '2017-10-19', '2017-10-20', '2017-10-23', '2017-10-24', '2017-10-25', '2017-10-26', '2017-10-27', '2017-10-30', '2017-10-31', '2017-11-01', '2017-11-02', '2017-11-03', '2017-11-06', '2017-11-07', '2017-11-08', '2017-11-09', '2017-11-10', '2017-11-13', '2017-11-14', '2017-11-15', '2017-11-16', '2017-11-17', '2017-11-20', '2017-11-21', '2017-11-22', '2017-11-24', '2017-11-27', '2017-11-28', '2017-11-29', '2017-11-30', '2017-12-01', '2017-12-04', '2017-12-05', '2017-12-06', '2017-12-07', '2017-12-08', '2017-12-11', '2017-12-12', '2017-12-13', '2017-12-14', '2017-12-15', '2017-12-18', '2017-12-19', '2017-12-20', '2017-12-21', '2017-12-22', '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29'))
acwiOpen <- c(59.61, 59.87, 60.15, 60.35, 60.22, 60.24, 60.25, 60.34, 60.54, 60.4, 60.47, 60.43, 60.43, 60.45, 60.49, 61.08, 61.41, 61.31, 60.89, 60.85, 61.13, 60.85, 61.26, 61.08, 61.12, 61.01, 61.3, 61.58, 61.95, 62, 62.08, 62.49, 62.19, 62.58, 62.61, 62.93, 62.32, 62.48, 62.47, 62.83, 63, 62.82, 62.75, 62.64, 62.63, 62.44, 62.78, 62.93, 62.74, 62.91, 63.65, 63.62, 63.58, 63.79, 62.83, 62.94, 63.09, 62.69, 63.07, 63.15, 63.42, 63.24, 63.31, 63.09, 63.43, 63.14, 63.07, 63.1, 63.13, 63.11, 62.88, 62.83, 62.75, 62.95, 62.96, 63.07, 63.94, 64.23, 64.34, 64.39, 64.34, 64.44, 64.6, 64.47, 64.52, 64.7, 64.95, 64.91, 64.84, 64.92, 64.95, 65.23, 65.58, 65.06, 64.41, 64.91, 65.36, 65.53, 65.48, 65.78, 65.78, 65.66, 65.91, 65.87, 66.44, 66.54, 66.3, 66.36, 66.4, 66.42, 66.06, 66.38, 66.71, 65.72, 66.16, 66.5, 65.86, 65.47, 65.46, 65.5, 65.93, 65.73, 65.62, 65.86, 65.6, 65.67, 65.54, 65.32, 65.09, 65.36, 65.48, 65.93, 66.24, 66.4, 66.74, 66.62, 66.88, 67.25, 67.02, 67.05, 67.22, 67.34, 67.59, 67.03, 67.35, 67.54, 67.79, 67.46, 67.56, 67.52, 67.53, 67.11, 67.03, 66.34, 66.8, 66.94, 67.14, 66.95, 66.27, 66.32, 66.63, 66.77, 67, 67.01, 67.11, 66.6, 66.97, 67.3, 67.73, 67.48, 67.4, 67.76, 67.72, 68.04, 68.46, 68.45, 68.3, 68.5, 68.74, 68.81, 68.88, 68.82, 68.69, 68.51, 68.37, 68.33, 68.26, 68.59, 68.77, 69, 69.17, 69.25, 69.21, 69.45, 69.57, 69.7, 69.73, 70, 70.03, 69.97, 70.14, 69.82, 70.19, 70.25, 70.02, 70.04, 69.87, 69.93, 70.06, 70.21, 70.63, 70.41, 70.48, 70.57, 70.73, 70.71, 70.39, 70.37, 70.02, 70.14, 69.7, 70.21, 70.33, 70.48, 70.89, 71.15, 71.35, 71.31, 71.32, 71.66, 71.67, 71.61, 71.88, 71.88, 71.01, 70.94, 71.58, 71.66, 71.79, 72.04, 72.07, 71.88, 72.54, 72.03, 71.98, 71.83, 71.97, 72.01, 72.04, 72.3, 72.39)
acwiVolume <- 1000 * c(2576.7, 1087.3, 1717.3, 1233.8, 1471.1, 1393.6, 1508.8, 1481.4, 2432.3, 2090, 2246.2, 2137.9, 1611.1, 1991.5, 1641.1, 2840.2, 2160.1, 1003.4, 4898.2, 2894.4, 3562.9, 1657.2, 1997.7, 1144.1, 1062.4, 1950.6, 1028, 1650, 2759.1, 2269.6, 2227.5, 2579.4, 2296.1, 2893.7, 2520.1, 3967.3, 1936.7, 1179.7, 1733, 2769.6, 1960.8, 1665.1, 930.8, 1061.5, 1612.4, 1679.7, 2343.2, 1273.7, 1780.8, 3030.7, 2072.3, 1595.4, 1362.9, 2535.6, 1795.2, 3562.7, 1503.5, 1775.4, 4208, 2474.4, 1574.5, 2063.4, 2034.7, 1977.5, 2895.8, 1652.6, 952.9, 1817.8, 1235.2, 1185.6, 1809.8, 2123.6, 1750.8, 976.5, 1681.4, 2036.3, 1944.4, 2231.6, 1360.9, 1897.3, 1511.7, 1268.5, 3350.2, 938.8, 703.2, 1007.2, 869.8, 1010.1, 932.1, 2738.8, 967.7, 1258.5, 676.4, 1467.8, 1974.6, 1207, 1498.8, 1682.5, 962.7, 1089.5, 1248.6, 1226.7, 1130.4, 3266.9, 3152.7, 1609.4, 1270.5, 2328.8, 1655.5, 1555.6, 3059.1, 850.7, 1955.1, 1871, 1534.7, 1774.6, 2155.1, 1116.5, 2049.5, 1096.9, 3644.7, 1694.3, 1859.1, 1690.4, 3787.3, 1465.5, 3957.6, 2196.1, 2179.2, 761.9, 1039.2, 1509.3, 1474.7, 774.2, 1197.8, 2261.4, 1640.3, 1669.5, 881, 1580.2, 1884.3, 4450.3, 1594.1, 1183.3, 2577.7, 2403.4, 1061.5, 623.1, 1141.4, 917.6, 859.4, 606.6, 1778.9, 1512.2, 1033, 871.8, 1123.2, 1594, 1434.6, 1029.3, 1013.7, 864.3, 4048.5, 989.2, 748.9, 1259.3, 781.2, 1045.1, 1675.7, 4411.3, 1557.2, 1050.2, 610.4, 961.4, 1144, 1079.9, 358.2, 969.8, 1256.6, 1515.8, 2005, 1264.2, 951.8, 4745.4, 1015.3, 801, 3610.4, 2570.3, 3559.1, 1349.5, 1151, 846.5, 1034.1, 625.7, 1088.8, 1111.9, 1388.6, 650.2, 2709.7, 1315.7, 2027.4, 1260.3, 1314.8, 876.1, 677.4, 920, 1039.3, 960.1, 771.1, 1247.4, 1493.2, 1420.1, 1157.6, 764.1, 1731.1, 536.9, 1439.9, 551.3, 789.8, 1108.1, 1198.4, 998.6, 591.1, 889.4, 872.3, 1325.8, 700.6, 1098.1, 1015.9, 1623.8, 7719.2, 6765.1, 2408.8, 2596.6, 1169.7, 1864.7, 1167.7, 5803.3, 1731, 1855.7, 2458.4, 1700.7, 2077.2, 1458.3, 2557.6, 2240.2, 1107.5, 4644.2, 3342.1, 1402.7, 1104.4)
acwi <- tibble::tibble(Date=acwiDate, Open=acwiOpen, Volume=acwiVolume)
str(acwi)


# Create a times series plot of Open against Date
acwi %>% 
    plot_ly(x = ~Date, y = ~Open) %>% 
    add_lines()


# Create a scatterplot with Date on the x-axis 
# and Volume on the y-axis
acwi %>% 
    plot_ly(x = ~Date, y = ~Volume) %>%
    add_markers()


happyLE <- c(52.3, 69.1, 65.7, 67.5, 65.1, 72.8, 72.4, 63.1, 66.1, 62.5, 66.6, 72.1, 51.8, 60.3, 68, 58, 65.5, 66.4, 52.4, 58.6, 50.1, 44.6, 46, 69.6, 69.3, 64.1, 55.3, 50.8, 69.9, 67, 72.8, 71.5, 71.7, 63.5, 67.3, 61.6, 64.3, 67.3, 56.7, 71.7, 72.6, 57.1, 64.3, 71.1, 54.9, 71.8, 63.4, 51.2, 53.3, 63.8, 76.5, 67.2, 72.8, 59.5, 60.6, 66, 61, 71.7, 71.9, 74.1, 47, 65.8, 75.3, 64.4, 64.2, 58.7, 62.4, 65.4, 62.9, 57.9, 65.1, 68.9, 52.8, 61.5, 67, 72.2, 65.9, 57, 54.2, 49.2, 71.8, 53.3, 65.6, 68.1, 63.7, 62.3, 67.1, 65.3, 50, 57.6, 55.5, 61.3, 71.6, 71.6, 66.3, 51.3, 45.9, 71.1, 57.5, 63.1, 68, 65.4, 60.3, 69.1, 72.2, 66.9, 63, 64.1, 58, 65.7, 44.4, 75.8, 68.8, 70.9, 55.2, 74, 50, 74.5, 65.3, 73, 73.2, 71.2, 63, 56.7, 66.4, 52.3, 61.7, 65.9, 65.8, 60.4, 51.8, 63.2, 68.6, 72.1, 69.8, 68.4, 63.2, 66.2, 55, 53.8, 52.7)
happyHappy <- c(2.66, 4.64, 5.25, 6.04, 4.29, 7.26, 7.29, 5.15, 6.23, 4.31, 5.55, 6.93, 4.85, 5.65, 5.09, 3.5, 6.33, 5.1, 4.65, 4.59, 5.07, 3.48, 4.56, 6.32, 5.1, 6.16, 4.88, 4.31, 7.23, 5.34, 6.06, 6.79, 7.59, 5.61, 5.84, 3.93, 6.34, 5.94, 4.18, 7.79, 6.64, 4.78, 4.45, 7.07, 5.48, 5.15, 6.33, 4.87, 3.82, 6.02, 5.36, 6.07, 7.48, 4.05, 5.1, 4.72, 4.46, 7.06, 7.33, 6.2, 5.04, 5.89, 5.91, 4.81, 5.88, 4.48, 6.15, 6.09, 5.63, 4.62, 5.98, 5.15, 4.42, 5.65, 6.27, 7.06, 5.23, 4.08, 3.42, 4.74, 6.68, 4.68, 6.17, 6.41, 5.33, 5.33, 5.61, 5.31, 4.28, 4.15, 4.44, 4.74, 7.46, 7.33, 6.48, 4.62, 5.32, 7.58, 5.83, 4.63, 6.57, 5.71, 5.59, 6.2, 5.71, 6.09, 5.58, 6.29, 4.68, 5.12, 4.09, 6.38, 6.37, 6.17, 4.51, 5.87, 2.82, 6.23, 4.33, 7.29, 7.47, 6.36, 5.83, 3.35, 5.94, 4.36, 6.19, 4.12, 5.61, 5.23, 4, 4.31, 7.04, 7.1, 6.99, 6.34, 6.42, 5.18, 3.25, 3.93, 3.64)
happyGDP <- c(7.46, 9.37, 9.54, 9.84, 9.03, 10.71, 10.72, 9.65, 10.69, 8.16, 9.72, 10.66, 7.63, 8.83, 9.37, 9.68, 9.55, 9.82, 7.43, 8.2, 8.13, NA, 7.49, 10.04, 9.64, 9.49, 8.56, 6.63, 9.67, 10.01, 10.36, 10.39, 10.75, 9.59, 9.22, 9.26, 9, 10.29, 7.44, 10.61, 10.56, 9.72, 9.17, 10.71, 8.33, 10.12, 8.92, 7.53, 7.4, 8.4, 10.92, 10.19, 10.76, 8.77, 9.32, 9.85, 9.62, 11.07, 10.41, 10.47, 8.18, 9.03, 10.57, 9.03, 10.07, 8.01, NA, 11.11, 8.11, 8.71, 10.13, 9.49, 6.63, NA, 10.28, 11.47, 9.5, 7.25, 7, 7.61, 10.5, 8.19, 9.91, 9.74, 8.55, 9.32, 9.69, 8.92, 7.05, 8.63, 9.2, 7.8, 10.79, 10.48, 8.58, 6.83, 8.59, 11.08, 8.52, NA, 10, 9.41, 8.94, 10.21, 10.24, 10.03, 10.1, 10.81, 7.81, 9.56, 7.25, 11.32, 10.31, 10.35, 9.41, 10.49, NA, 10.45, 9.38, 10.77, 10.96, NA, 7.96, 7.9, 9.69, 7.25, 10.32, 9.29, 10.12, 9.7, 7.44, 8.97, 11.12, 10.58, 10.9, 9.92, 8.76, 8.74, NA, 8.21, 7.54)
happyRegion <- c('South Asia', 'Central and Eastern Europe', 'Middle East and North Africa', 'Latin America and Caribbean', 'Commonwealth of Independent States', 'North America and ANZ', 'Western Europe', 'Commonwealth of Independent States', 'Middle East and North Africa', 'South Asia', 'Commonwealth of Independent States', 'Western Europe', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'Central and Eastern Europe', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'Central and Eastern Europe', 'Sub-Saharan Africa', 'Southeast Asia', 'Sub-Saharan Africa', 'Sub-Saharan Africa', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'East Asia', 'Latin America and Caribbean', 'Sub-Saharan Africa', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'Central and Eastern Europe', 'Western Europe', 'Central and Eastern Europe', 'Western Europe', 'Latin America and Caribbean', 'Latin America and Caribbean', 'Middle East and North Africa', 'Latin America and Caribbean', 'Central and Eastern Europe', 'Sub-Saharan Africa', 'Western Europe', 'Western Europe', 'Sub-Saharan Africa', 'Commonwealth of Independent States', 'Western Europe', 'Sub-Saharan Africa', 'Western Europe', 'Latin America and Caribbean', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'Latin America and Caribbean', 'East Asia', 'Central and Eastern Europe', 'Western Europe', 'South Asia', 'Southeast Asia', 'Middle East and North Africa', 'Middle East and North Africa', 'Western Europe', 'Middle East and North Africa', 'Western Europe', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'East Asia', 'Middle East and North Africa', 'Commonwealth of Independent States', 'Sub-Saharan Africa', 'Central and Eastern Europe', 'Middle East and North Africa', 'Commonwealth of Independent States', 'Southeast Asia', 'Central and Eastern Europe', 'Middle East and North Africa', 'Sub-Saharan Africa', 'Middle East and North Africa', 'Central and Eastern Europe', 'Western Europe', 'Central and Eastern Europe', 'Sub-Saharan Africa', 'Sub-Saharan Africa', 'Sub-Saharan Africa', 'Western Europe', 'Sub-Saharan Africa', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'Commonwealth of Independent States', 'East Asia', 'Central and Eastern Europe', 'Middle East and North Africa', 'Sub-Saharan Africa', 'Southeast Asia', 'Sub-Saharan Africa', 'South Asia', 'Western Europe', 'North America and ANZ', 'Latin America and Caribbean', 'Sub-Saharan Africa', 'Sub-Saharan Africa', 'Western Europe', 'South Asia', 'Middle East and North Africa', 'Latin America and Caribbean', 'Latin America and Caribbean', 'Southeast Asia', 'Central and Eastern Europe', 'Western Europe', 'Central and Eastern Europe', 'Commonwealth of Independent States', 'Middle East and North Africa', 'Sub-Saharan Africa', 'Central and Eastern Europe', 'Sub-Saharan Africa', 'Southeast Asia', 'Central and Eastern Europe', 'Central and Eastern Europe', 'Sub-Saharan Africa', 'East Asia', 'Sub-Saharan Africa', 'Western Europe', 'South Asia', 'Western Europe', 'Western Europe', 'East Asia', 'Commonwealth of Independent States', 'Sub-Saharan Africa', 'Southeast Asia', 'Sub-Saharan Africa', 'Latin America and Caribbean', 'Middle East and North Africa', 'Middle East and North Africa', 'Commonwealth of Independent States', 'Sub-Saharan Africa', 'Commonwealth of Independent States', 'Middle East and North Africa', 'Western Europe', 'North America and ANZ', 'Latin America and Caribbean', 'Commonwealth of Independent States', 'Southeast Asia', 'Middle East and North Africa', 'Sub-Saharan Africa', 'Sub-Saharan Africa')
happyIncome <- factor(c('low', 'upper-middle', 'upper-middle', 'high', 'upper-middle', 'high', 'high', 'upper-middle', 'high', 'lower-middle', 'upper-middle', 'high', 'low', 'lower-middle', 'upper-middle', 'upper-middle', 'upper-middle', 'upper-middle', 'low', 'lower-middle', 'lower-middle', 'low', 'low', 'high', 'upper-middle', 'upper-middle', 'NA', 'NA', 'upper-middle', 'high', 'high', 'high', 'high', 'upper-middle', 'upper-middle', 'NA', 'lower-middle', 'high', 'low', 'high', 'high', 'upper-middle', 'lower-middle', 'high', 'lower-middle', 'high', 'upper-middle', 'low', 'low', 'lower-middle', 'NA', 'high', 'high', 'lower-middle', 'lower-middle', 'NA', 'upper-middle', 'high', 'high', 'high', 'NA', 'upper-middle', 'high', 'upper-middle', 'upper-middle', 'lower-middle', 'lower-middle', 'high', 'NA', 'NA', 'high', 'upper-middle', 'low', 'upper-middle', 'high', 'high', 'NA', 'low', 'low', 'low', 'high', 'lower-middle', 'upper-middle', 'upper-middle', 'lower-middle', 'lower-middle', 'upper-middle', 'lower-middle', 'low', 'lower-middle', 'upper-middle', 'low', 'high', 'high', 'lower-middle', 'low', 'lower-middle', 'high', 'lower-middle', 'NA', 'high', 'upper-middle', 'lower-middle', 'high', 'high', 'upper-middle', 'NA', 'high', 'low', 'upper-middle', 'low', 'high', 'NA', 'high', 'upper-middle', 'NA', 'low', 'high', 'lower-middle', 'high', 'high', 'NA', 'low', 'low', 'upper-middle', 'low', 'high', 'lower-middle', 'upper-middle', 'upper-middle', 'low', 'lower-middle', 'high', 'high', 'high', 'high', 'lower-middle', 'lower-middle', 'NA', 'lower-middle', 'low'), levels=c("low", "lower-middle", "upper-middle", "high"))
happySS <- c(0.491, 0.638, 0.807, 0.907, 0.698, 0.95, 0.906, 0.787, 0.876, 0.713, 0.9, 0.922, 0.436, 0.779, 0.775, 0.768, 0.905, 0.942, 0.785, 0.765, 0.695, 0.32, 0.661, 0.88, 0.772, 0.909, 0.655, 0.67, 0.922, 0.77, 0.819, 0.901, 0.952, 0.894, 0.849, 0.638, 0.829, 0.936, 0.734, 0.964, 0.931, 0.807, 0.59, 0.892, 0.669, 0.753, 0.826, 0.634, 0.647, 0.843, 0.831, 0.877, 0.967, 0.607, 0.796, 0.714, 0.695, 0.943, 0.916, 0.92, 0.661, 0.913, 0.882, 0.815, 0.914, 0.715, 0.792, 0.853, 0.883, 0.707, 0.895, 0.777, 0.685, 0.823, 0.926, 0.905, 0.8, 0.626, 0.555, 0.741, 0.937, 0.779, 0.91, 0.8, 0.831, 0.924, 0.881, 0.641, 0.678, 0.795, 0.828, 0.816, 0.937, 0.955, 0.838, 0.582, 0.733, 0.95, 0.69, 0.824, 0.912, 0.83, 0.851, 0.882, 0.9, 0.811, 0.896, 0.84, 0.744, 0.884, 0.652, 0.897, 0.913, 0.928, 0.87, 0.807, 0.557, 0.903, 0.823, 0.914, 0.95, 0.891, 0.663, 0.705, 0.877, 0.508, 0.916, 0.717, 0.876, 0.908, 0.74, 0.858, 0.836, 0.937, 0.921, 0.914, 0.942, NA, 0.79, 0.744, 0.754)
happyCountry <- c('Afghanistan', 'Albania', 'Algeria', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahrain', 'Bangladesh', 'Belarus', 'Belgium', 'Benin', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Cambodia', 'Cameroon', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Congo (Brazzaville)', 'Congo (Kinshasa)', 'Costa Rica', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Estonia', 'Ethiopia', 'Finland', 'France', 'Gabon', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Haiti', 'Honduras', 'Hong Kong S.A.R. of China', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Ivory Coast', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Liberia', 'Libya', 'Lithuania', 'Luxembourg', 'Macedonia', 'Madagascar', 'Malawi', 'Mali', 'Malta', 'Mauritania', 'Mauritius', 'Mexico', 'Moldova', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Pakistan', 'Palestinian Territories', 'Panama', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Russia', 'Saudi Arabia', 'Senegal', 'Serbia', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa', 'South Korea', 'South Sudan', 'Spain', 'Sri Lanka', 'Sweden', 'Switzerland', 'Taiwan Province of China', 'Tajikistan', 'Tanzania', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vietnam', 'Yemen', 'Zambia', 'Zimbabwe')
happy <- tibble::tibble(region=happyRegion, life.expectancy=happyLE, social.support=happySS, 
                        happiness=happyHappy, log.gdp=happyGDP, income=happyIncome, 
                        country=happyCountry
                        )
str(happy)


# Create a coded scatterplot of happiness vs. life.expectancy
happy %>%
    plot_ly(x=~life.expectancy, y=~happiness) %>%
    add_markers(color=~region, size=~log.gdp)


# Fill in the specified plotting symbols
happy %>%
    plot_ly(x = ~life.expectancy, y = ~happiness) %>%
    add_markers(symbol = ~income, symbols = c("circle-open", "square-open", "star-open", "x-thin-open"))


# Complete the following code to polish the plot
happy %>%
    plot_ly(x = ~social.support, y = ~happiness, hoverinfo = "text",
            text = ~paste("Country: ", country, "<br> Income: ", income,
                          "<br> Happiness: ", round(happiness, 2),
                          "<br> Social support: ", round(social.support, 2)
                          )
            ) %>%
    add_markers(symbol = ~income, symbols = c("circle-open", "square-open", "star-open", "x-thin-open")) %>%
    layout(xaxis = list(title="Social support index"), yaxis = list(title="National happiness score"))


us_economy <- readr::read_csv("./RInputFiles/state_economic_data.csv")
str(us_economy, give.attr=FALSE)
launches <- readr::read_csv("./RInputFiles/launches.csv")
str(launches, give.attr=FALSE)


# Change the sizemode so that size refers to the diameter of the points
us_economy %>%
    filter(year == 2017) %>%
    plot_ly(x = ~gdp, y = ~house_price) %>%
    add_markers(size = ~population, color = ~region, marker = list(sizemode="diameter"))


# Create a line chart of house_price over time by state
us_economy %>%
    filter(year >= 2000) %>%
    group_by(state) %>%
    plot_ly(x = ~year, y = ~house_price) %>%
    add_lines()

```
  
  
  
***
  
Chapter 2 - Animating Graphics  
  
Introduction to Animated Graphics:  
  
* Animated graphics can help show changes in data over time, especially when there are a large number of time periods such that faceting would be difficult/impossible  
* Example for the base aesthetic  
	* myDF %>% plot_ly(x=~myX, y=~myY) %>% add_markers(frame=~myAnimateVar, showlegend=FALSE) %>% layout(xaxis=list(type="log"), yaxis=list(type="log"))  # the frame= variable controls the animation  
* May want to keep the glyphs consant (not changing when there is an NA above/below them) using the ids variable  
	* myDF %>% plot_ly(x=~myX, y=~myY) %>% add_markers(frame=~myAnimateVar, ids=~myIDVar, showlegend=FALSE) %>% layout(xaxis=list(type="log"), yaxis=list(type="log"))  # the frame= variable controls the animation  
  
Polishing Animations:  
  
* Can polish aspects of the animation such as time per change or slider appearance  
* Can use the animation_opts() function to change the animation  
	* ani %>% animation_opts(frame=500, transition=frame, easing="linear", redraw=TRUE)  # assumes ani is a properly built plot_ly with animation; frame=500 means 500ms per frame (lower means faster transitions)  
    * Note that there is a pause of transition-frame between each transition  
    * Can set easing="bounce" to change right to the next frame  
* Can change the appearance of the slider  
	* ani %>% animation_slider(hide=TRUE)  # remove the slider  
    * ani %>% animation_slider(currentvalue=list(prefix=NULL, font=list(color="black", fontsize=40)))  
  
Layering in plotly:  
  
* Can layer traces, similar to the process of layering in ggplot2  
* Example of producing a text layer first (large year), then layering the rest of the plot on top  
	* myDF %>% plot_ly(x=~myX, y=~myY) %>% add_text(x=xText, y=yText, text=~year, frame=~year, textfont=list(size=150, color=toRGB("gray80"))) %>% add_markers(frame=~year, …, layout(…, showlegend=FALSE)) %>% animation_slider(hide=TRUE)  
  
Cumulative Animations:  
  
* Cumulative time series is a process where the time series seem to be growing over time  
	* Each frame must contain a path containing all the data up through that year (not just the current year)  
* Can create a cumulative data frame  
	* Basically, a frame column is create with the ID being the key year, with a year column retained that shows all the data up through that year  
* The dplyr and purrr packages have a split() function  
	* belgium %>% split(.$year) %>% accumulate(~bind_rows(.x, .y)) %>% set_names(1960:2018) %>% bind_rows(.id="frame") %>% plot_ly(x=~year, y=~income) %>% add_lines(frame=~frame, showlegend=FALSE)  
  
Example code includes:  
```{r eval=FALSE}

# Create an animated bubble chart of house_price against gdp
us_economy %>%
    plot_ly(x = ~gdp, y = ~house_price) %>%
    add_markers(size = ~population, color = ~region, frame = ~year, ids = ~state, 
                marker = list(sizemode = "diameter")
                )


# Animate a bubble chart of house_price against gdp over region
ani <- us_economy %>%
    filter(year==2017) %>%
    plot_ly(x = ~gdp, y = ~house_price) %>%
    add_markers(size = ~population, color = ~region, 
                frame = ~region, ids = ~state, marker = list(sizemode = "diameter")
                )
ani


# Adjust the frame and transition
ani %>% 
    animation_opts(frame = 2000, transition = 300)

# Change the type of transition to "elastic"
ani %>% 
    animation_opts(frame = 2000, transition = 300, easing = "elastic")

# Remove the prefix from the slider and change the font color to "red"
ani %>% 
    animation_opts(frame = 2000, transition = 300, easing = "elastic") %>%
    animation_slider(currentvalue = list(prefix = NULL, font = list(color = "red")))

# Polish the x- and y-axis titles
ani %>% 
    animation_opts(frame = 2000, transition = 300, easing = "elastic") %>% 
    animation_slider(currentvalue = list(prefix = NULL, font = list(color = "red"))) %>%
    layout(xaxis = list(title="Real GDP (millions USD)"), yaxis = list(title="Housing price index"))


# Reduce the bubble size
us_economy %>%
    plot_ly(x = ~gdp, y = ~house_price) %>%
    add_markers(size = ~population, color = ~region, frame = ~year, ids = ~state,
                marker = list(sizemode = "diameter", sizeref=3)
                ) %>%
    layout(xaxis = list(title = "Real GDP (millions USD)", type = "log"), 
           yaxis = list(title = "Housing price index")
           )

# Map state names to the hover info text
us_economy %>%
    plot_ly(x = ~gdp, y = ~house_price, hoverinfo = "text", text = ~state) %>%
    add_markers(size = ~population, color = ~region, frame = ~year, ids = ~state,
                marker = list(sizemode = "diameter", sizeref = 3)
                ) %>%
    layout(xaxis = list(title = "Real GDP (millions USD)", type = "log"),
           yaxis = list(title = "Housing price index")
           )


# Add the year as background text and remove the slider
us_economy %>%
    plot_ly(x = ~gdp, y = ~house_price, hoverinfo = "text", text = ~state) %>%
    add_text(x = 200000, y = 450, text = ~year, frame = ~year,
             textfont = list(color = toRGB("gray80"), size = 150)
             ) %>%
    add_markers(size = ~population, color = ~region, frame = ~year, ids = ~state,
                marker = list(sizemode = "diameter", sizeref = 3)
                ) %>%
    layout(xaxis = list(title = "Real GDP (millions USD)", type = "log"),
           yaxis = list(title = "Housing price index")
           ) %>%
    animation_slider(hide = TRUE)


# extract the 1997 data
us1997 <- us_economy %>%
    filter(year == 1997)

# create an animated scatterplot with baseline from 1997
us_economy %>%
    plot_ly(x = ~gdp, y = ~house_price) %>%
    add_markers(data = us1997, marker = list(color = toRGB("gray60"), opacity = 0.5)) %>%
    add_markers(frame = ~year, ids = ~state, data = us_economy, showlegend = FALSE, alpha = 0.5) %>%
    layout(xaxis = list(type = "log"))


# Find median life HPI for each region in each year
med_hpi <- us_economy %>%
    group_by(region, year) %>%
    summarize(median_hpi=median(house_price))


# Animate the cumulative time series of median HPI over time
med_hpi %>%
    split(.$year) %>%
    accumulate(~bind_rows(.x, .y)) %>%
    set_names(1997:2017) %>%
    bind_rows(.id = "frame") %>%
    plot_ly(x=~year, y=~median_hpi, color=~region) %>%
    add_lines(frame=~frame, showlegend=FALSE)

```
  
  
  
***
  
Chapter 3 - Linking Graphics  
  
Linking Two Charts:  
  
* Linking charts allows for brushing of one chart to highlight other charts  
* A similar idea applies to linked highlighting for a chart  
* The crosstalk package allows for linking plots using JavaScript (displays in the Rstudio pane)  
	* p1 <- <plotlycode>  
    * p2 <- <plotlycode>  
    * subplot(p1, p2, titleX=TRUE, titleY=TRUE) %>% hide_legend()  # creates separate views  
* Example for creating linked views  
	* library(crosstalk)  
    * shared_data <- SharedData$new(world2014)  
    * p1 <- <plotlycode updated for dataset shared_data>  
    * p2 <- <plotlycode updated for dataset shared_data>  
    * subplot(p1, p2, titleX=TRUE, titleY=TRUE) %>% hide_legend()  %>% highlight(on="plotly_selected") # creates linked views  
  
Brushing Groups:  
  
* Can run time series in a similar manner using SharedData()  
	* world_indicators %>% SharedDara$new(key=~country) %>% …  # this will highlight everything for country all at the same time  
* Can use dplyr commands within the plotly pipeline AFTER the plot_ly() command  
	* Cannot directly use the dplyr commands on the shared data object  
  
Selection Strategies:  
  
* Transient selection (default) is the process where previous selections are forgotten when new selections are made  
* By contrast, persistent selection allows for making multiple selections that are all maintained - can SHIFT-SELECT or can make persistent using code  
	* subplot(…) %>% hide_legend() %>% highlight(persistent=TRUE)  
    * subplot(…) %>% hide_legend() %>% highlight(persistent=TRUE, dynamic=TRUE)  # allows for users to select different colors for different selections  
* Can also use indirect manipulation, such as user-interaction with plotly by way of a dropdown  
	* subplot(…) %>% hide_legend() %>% highlight(selectize=TRUE)  
  
Making Shinier Charts:  
  
* Can create highly interactive charts similar to shiny using plotly  
	* shared_data <- SharedData$new(world2014)  
    * p1 <- shared_data %>% plot_ly(…) %>% …  
    * p2 <- shared_data %>% plot_ly(…) %>% …  
    * bscols(p1, p2)  # bscols creates two side-by-side html widgets (each has its own control panel)  
* Can modify the bscols() call to add filters  
	* bscols(filter_checkbox(id="four_regions", label="region", sharedData=shared_data, group=~four_regions), p1)  
    * bscols(filter_select(id="four_regions", label="region", sharedData=shared_data, column=~four_regions), p1)  # drop-downs  
    * bscols(filter_slider(id="co2", label="CO2 Concentrations", sharedData=shared_data, column=~co2), p1)  # sliders  
* Can fix the axes so they do not scale with data selections  
	* bscols(filter_slider(id="co2", label="CO2 Concentrations", sharedData=shared_data, column=~co2), p1 %>% layout(xaxis=list(range(2.5, 5)), yaxis=list(range(-1.4, 1.55))))  # sliders  
* Can put the pieces together in a single call  
	* bscols(widths=c(2, 5, 5), list(filter_checkbox(…), filter_slider(…), p1, p2)  
  
Example code includes:  
```{r eval=FALSE}

us2017 <- us_economy %>% 
    filter(year == 2017) %>% 
    group_by(state, year, gdp, home_owners, house_price, population, region, division) %>%
    summarize(employment=mean(employment)) %>%
    ungroup()
str(us2017, give.attr=FALSE)


# Create a SharedData object from us2017
shared_us <- crosstalk::SharedData$new(us2017)

# Create a scatterplot of house_price vs. home_owners
p1 <- shared_us %>%
    plot_ly(x = ~home_owners, y = ~house_price) %>%
    add_markers()
 
# Scatterplot of house_price vs. employment rate
p2 <- shared_us %>%
    plot_ly(x = ~employment/population, y = ~house_price) %>%
    add_markers()
  
# Polish the linked scatterplots
linked_plots <- subplot(p1, p2, titleX = TRUE, shareY = TRUE) %>% hide_legend()
linked_plots


# Add a highlight layer
linked_plots %>% 
    highlight()

# Enable linked brushing
linked_plots %>% 
    highlight(on = "plotly_selected")

# Enable hover highlighting
linked_plots %>% 
    highlight(on = "plotly_hover")


# Create a shared data object keyed by individual states
state_data <- us_economy %>%
    crosstalk::SharedData$new(key=~state)

# Using the shared data, plot house price vs. year
state_data %>%
    plot_ly(x=~year, y=~house_price) %>%
    # Group by state
    group_by(state) %>%
    # Add lines
    add_lines()


# Create a shared data object keyed by region
shared_region <- us_economy %>% 
    crosstalk::SharedData$new(key = ~region)

# Create a dotplot of avg house_price by region in 2017
dp_chart <- shared_region %>%
    plot_ly() %>%
    filter(year == 2017) %>%
    group_by(region) %>%
    summarize(avg.hpi = mean(house_price, na.rm = TRUE)) %>%
    add_markers(x = ~avg.hpi, y = ~region)
  
# Code for time series plot
ts_chart <- shared_region %>%
    plot_ly(x = ~year, y = ~house_price) %>%
    group_by(state) %>%
    add_lines()
  
# Link dp_chart and ts_chart
subplot(dp_chart, ts_chart)


# Create a shared data object keyed by division
shared_region <- crosstalk::SharedData$new(us2017, key = ~division)

# Create a bar chart for division
bc <- shared_region %>%
    plot_ly() %>%
    count(division) %>%
    add_bars(x = ~division, y = ~n) %>%
    layout(barmode = "overlay")
  
# Bubble chart
bubble <- shared_region %>%
    plot_ly(x = ~home_owners, y = ~house_price, hoverinfo = "text", text = ~state) %>%
    add_markers(size = ~population, marker = list(sizemode = "diameter"))

# Remove the legend
subplot(bc, bubble) %>% hide_legend()


# Enable persistent hover selection and a color selector 
linked_plots %>%
    highlight(persistent = TRUE, selectize = TRUE, dynamic = TRUE, on="plotly_hover")


# Create a shared data object keyed by state
state_data <- crosstalk::SharedData$new(us2017, key = ~state, group = "Select a state")

# Enable indirect selection by state
state_data %>%
    plot_ly(x = ~home_owners, y = ~house_price, hoverinfo = "text", text = ~state) %>%
    add_markers(size = ~population, marker = list(sizemode = "diameter")) %>%
    highlight(selectize = TRUE)

# Create a shared data object keyed by region
region_data <- crosstalk::SharedData$new(us2017, key = ~region, group = "Select a region")

# Enable indirect selection by region
region_data %>%
    plot_ly(x = ~home_owners, y = ~house_price, hoverinfo = "text", text = ~state) %>%
    add_markers(size = ~population, marker = list(sizemode = "diameter")) %>%
    highlight(selectize = TRUE)


# Create a row of subplots containing p97, p07, and p17 with widths 6, 3, 3
# crosstalk::bscols(widths=c(6, 3, 3) , p97, p07, p17)

# Specify that p07 should span 5 columns
# crosstalk::bscols(widths=c(NA, 5, NA), p97, p07, p17)

# Stack p07 and p17 in the right column
# crosstalk::bscols(p97, list(p07, p17))


# shared data object
shared_us <- crosstalk::SharedData$new(us2017, key = ~region)

# scatterplot of housing price index against homeownership
p17 <- shared_us %>%
    plot_ly(x = ~home_owners, y = ~house_price, color = ~region, height = 400) %>%
    add_markers()
  
# add a column of checkboxes for region to the left of the plot
crosstalk::bscols(widths=c(3, NA),
                  crosstalk::filter_checkbox(id = "region", label = "Region", 
                                             sharedData = shared_us, group = ~region
                                             ), p17
                  )


shared_us <- crosstalk::SharedData$new(us2017)

p17 <- shared_us %>%
    plot_ly(x = ~home_owners, y = ~house_price, color = ~region, height = 400) %>%
    add_markers() %>%
    layout(xaxis = list(title = "Home ownership (%)"), yaxis = list(title = "HPI"))
  
# add a slider filter for each axis below the scatterplot
crosstalk::bscols(list(p17,
                       crosstalk::filter_slider(id = "price", label = "HPI", 
                                                sharedData = shared_us, column = ~house_price
                                                ),
                       crosstalk::filter_slider(id = "owners", label = "Home ownership (%)", 
                                                sharedData = shared_us, column = ~home_owners
                                                )
                       )
                  )

```
  
  
  
***
  
Chapter 4 - Case Study: Space Launches  
  
Introduction to the Data:  
  
* Government and private company space launches, 5726x11 dataset launches  
* Can explore the "space race" data by government, as well as by private company  
  
Recap: Animation:  
  
* Can run accumulative animations using accumulate() and bind_rows() as per Chapter 2  
* Need to ensure a common baseline, even if data did not exist for all elements at that time  
	* complete_logs <- monthly_logs %>% complete(package, dec_date, fill=list(downloads=0))  # default for fill is NA  
  
Recap: Linked Views and Selector Widgets:  
  
* The SharedData$new(myData) command allows for sharing data across charts  
* Can use either subplot() or bscols() to create multiple charts  
	* Should use highlight() also to enable linked brushing  
* Will use the launch vehicles dataset, focused on a subset of length/diameter, payload capacity to LEO, and total thrust  
  
Wrap Up:  
  
* Animating plotly charts  
	* Fundamentals of plotly, and polishing  
    * Animating plotly charts - frame, ids, layer  
    * crosstalk::SharedData$new() for linking plotly charts  
* Can extend capabilities, including  
	* leaflet, highcharter, trelliscope, rbokeh, etc.  
    * Shiny  
    * plotly for R  
  
Example code includes:  
```{r eval=FALSE}

# table of launches by year
launches_by_year <- launches %>%
    count(launch_year)

# create a line chart of launches over time
launches_by_year %>%
    plot_ly(x=~launch_year, y=~n) %>%
    add_lines() %>%
    layout(xaxis = list(title = "Year"), yaxis = list(title = "Launches"))

# create a filled area chart of launches over time
launches_by_year %>%
    plot_ly(x=~launch_year, y=~n) %>%
    add_lines(fill="tozeroy") %>%
    layout(xaxis = list(title = "Year"), yaxis = list(title = "Launches"))

# create a bar chart of launches over time
launches_by_year %>%
    plot_ly(x=~launch_year, y=~n) %>%
    add_bars() %>%
    layout(xaxis = list(title = "Year"), yaxis = list(title = "Launches"))


# table of launches by year
state_launches <- launches %>% 
    filter(agency_type == "state") %>% 
    count(launch_year, state_code)

# create a ShareData object for plotting
shared_launches <- state_launches %>% 
    crosstalk::SharedData$new(key = ~state_code)

# Create a line chart for launches by state, with highlighting
shared_launches %>%
    plot_ly(x=~launch_year, y=~n, color=~state_code) %>%
    add_lines() %>%
    highlight()


# table of launches by year and agency type
launches_by_year <- launches %>% 
    count(launch_year, agency_type)

# create a ShareData object for plotting
shared_launches <- launches_by_year %>% 
    crosstalk::SharedData$new(key = ~agency_type)

# create a line chart displaying launches by agency type, with highlighting
shared_launches %>%
    plot_ly(x=~launch_year, y=~n, color=~agency_type) %>%
    add_lines() %>%
    highlight()


# Complete the state_launches data set
annual_launches <- launches %>% 
    filter(agency_type == "state") %>%
    count(launch_year, state_code) %>%
    tidyr::complete(state_code, launch_year, fill = list(n = 0))

# Create the cumulative data set
cumulative_launches <- annual_launches %>%
    split(f = .$launch_year) %>%
    accumulate(., ~bind_rows(.x, .y)) %>%
    bind_rows(.id = "frame")

# Create the cumulative animation
cumulative_launches %>%
    plot_ly(x = ~launch_year, y = ~n) %>%
    add_lines(color = ~state_code, frame = ~frame, ids = ~state_code)


# Complete the private_launches data set
annual_launches <- launches %>% 
    filter(agency_type == "private") %>%
    rename(year=launch_year, agency_name=agency) %>%
    count(year, agency_name) %>%
    tidyr::complete(agency_name, year, fill = list(n = 0))

# Create the cumulative data set
cumulative_launches <- annual_launches %>%
    split(f = .$year) %>%
    accumulate(., ~bind_rows(.x, .y)) %>%
    bind_rows(.id = "frame")

# Create the cumulative animation
cumulative_launches %>%
    plot_ly(x = ~year, y = ~n, color = ~agency_name) %>%
    add_lines(frame = ~frame, ids = ~agency_name)


# Create a SharedData object allowing selection by year
# shared_year <- crosstalk::SharedData$new(launches, key = ~launch_year)

# Create a bar chart of launches by year
# bar <- shared_year %>%
#     plot_ly(x = ~launch_year, y = ~n) %>%
#     count(launch_year) %>%
#     add_bars() %>%
#     layout(barmode = "overlay") %>%
#     highlight()

# Create a scatterplot of diameter vs. length
# scatter <- shared_year %>%
#     plot_ly(x = ~length, y = ~diameter) %>%
#     add_markers() %>%
#     highlight()

# Use bscols to link the two charts
# crosstalk::bscols(bar, scatter)


# Create a SharedData object allowing selection of observations
# shared_obs <- crosstalk::SharedData$new(lv2000)

# Create a scatterplot of to_thrust against leo_capacity
# p1 <- shared_obs %>%
#     plot_ly(x = ~leo_capacity, y = ~to_thrust) %>%
#     add_markers() 

# Scatterplot of diameter vs. length
# p2 <- shared_obs %>%
#     plot_ly(x = ~length, y = ~diameter) %>%
#     add_markers() 

# Link p1 and p2
# subplot(p1, p2) %>%
#     highlight(on = "plotly_selected", off = "plotly_deselect") %>%
#     hide_legend()


# SharedData object allowing selection of observations
# shared_obs <- crosstalk::SharedData$new(lv2000)

# Scatterplot of to_thrust against leo_capacity
# scatter <- shared_obs %>%
#     plot_ly(x = ~leo_capacity, y = ~to_thrust) %>%
#     add_markers()

# Create a histogram of to_thrust
# histo <- shared_obs %>%
#     plot_ly(x = ~to_thrust) %>%
    # add_histogram(name = "overall")

# Link the two plots
# subplot(scatter, histo) %>%
#     hide_legend() %>%
#     highlight(on = "plotly_selected")


# Create a SharedData object containing the number of launches by year and state
shared_launches <- launches %>% 
    filter(agency_type == "state") %>%
    count(state_code, launch_year) %>%
    crosstalk::SharedData$new()

# Create a line chart displaying the launches by state
launch_ts <- shared_launches %>%
    plot_ly(x = ~launch_year, y = ~n, color = ~state_code) %>%
    add_lines()  

# Add a slider below the chart to filter the years displayed
crosstalk::bscols(list(launch_ts, 
                       crosstalk::filter_slider(id = "time", label = "Year", 
                                                sharedData = shared_launches, column = ~launch_year
                                                )
                       )
                  )

```
  
  
  
***
  
### _Defensive R Programming_  
  
Chapter 1 - Avoiding Conflict  
  
Defensive R Programming:  
  
* Course goals are to make robust data procesing pipelines  
	* Automate to reduce problems, and learn to diagnose and fix problems that occur  
  
Avoid Reinventing the Wheel:  
  
* There are many freely available packages for R  
* Many factors in choosing a package  
	* Mature?  
    * Actively developed?  
    * Documentation and vignettes?  
    * Common usage?  
* Can look at the "CRAN Task View" page to see a human-curated summary of appropriate packages by application  
  
Packages and Namespaces:  
  
* The ls() function will reveal the contents of the current environment  
	* Can quickly lose track of what is going on when there are many objects  
* Packages use namespaces, or containers for all of the functions in the package  
	* library(myPackage) provides access to all the functions in the namespace  
    * getNamespaceExports("dplyr")  # returns all 238 exported functions  
    * dplyr::filter  # use the filter() function from the dplyr namespace  
* When a function is typed without a namespace, the first namespace containing the function is returned  
	* The global environment is always first  
    * Then, packages go in load order (most recent packages first)  
* The library("conflicted") helps make this easier by identifying and allowing for solutions to conflicts  
	* library("conflicted")  
    * library("dplyr")  
    * filter  # will pull up that there is a conflict  
    * conflict_prefer("filter", "dplyr")  # sets the default to dplyr::filter  
  
Example code includes:  
```{r eval=FALSE}

# Create a data frame of the packages where a newer version is available
old <- old.packages()

# Find how many packages need to be updated
no_of_old_pkgs <- nrow(old)


# Count the number of functions in ggplot2
no_of_functions <- length(getNamespaceExports("ggplot2"))


# Load the dplyr and conflicted packages
library("dplyr")
library("conflicted")

# Prefer the dplyr version of the lag function
conflict_prefer("lag", "dplyr")

# This should return NA, 1, 2, 3
lag(1:4)

```
  
  
***
  
Chapter 2 - Early Warning Systems  
  
Early Warning Systems:  
  
* Early warnings help with both problem avoidance and problem handling  
* Shortcuts such as T / F for TRUE / FALSE can be risky  
	* T <- 5 is not protected  
    * TRUE <- 5 cannot happen (protected against overwriting)  
* When testing for logical problems, use isTRUE() for protection against numerical values  
  
Message in a Bottle:  
  
* The message() function provides information to a user  
* Can suppress messages by wrapping calls in suppressMessages(f(…))  
	* suppressPackageStartupMessages(library(…))  
  
Warning:  
  
* The warning() function provides a message while signaling that something may have gone wrong  
* Can suppress using suppressWarnings()  
	* It is almost always better to address the underlying issues rather that suppress the warnings  
  
Stop:  
  
* The stop() function flags an error and stops the code  
	* stop("Insert message here")  
* Errors cannot be suppressed, since something has gone wrong and must be fixed  
* The try() function is a bit like suppress(), but it will also capture the error before proceeding  
	* res <- try("a" + "b", silent=TRUE)  
    * res  # list that shows problems  
    * class(res)  # will be try-error if something has gone wrong  
  
Example code includes:  
```{r eval=FALSE}

# Define F to be interpreted as TRUE
F <- TRUE

# Read in data: don't alter the line below
data_set <- read.csv("iris.csv", header = F)


suppressPackageStartupMessages(library("dplyr"))


# Suppress the standard output of the simulate() function
sim = suppressMessages(simulate(runs = 5))


# Modify the function to make it less noisy
get_distribution <- function(N, verbose = TRUE) {
  results <- numeric(N)
  for(i in 1:N) {
    results[i] <- simulate()
    # Check if verbose is TRUE
    if(isTRUE(verbose)) {
      # Show a progress report
      message("Simulation ", i, " completed")
    }
  }
  return(results)
}


# Create new variable 
x <- c(1, 1, 1)
y <- 1:3

# Suppress the warning
m <- suppressWarnings(cor(x, y))


mean_age = function(ages) {
  if(any(ages < 0)) {
    stop("You have negative ages!")
  }
  # Stop the execution if any of the ages are over 150
  if(any(ages > 150)) {
    stop("You have ages over 150!")
  }
  m = mean(ages)
  return(m)
}

```
  
  
  
***
  
Chapter 3 - Preparing Defenses  
  
Preparing Defenses:  
  
* DRY is the principle of "Do not Repeat Yourself" - avoid copy/paste, and reuse existing principle  
	* The copy and paste rule says that at most one copy/paste should be done  
    * Should typically use functions or for loops rather than multiple copy/paste  
  
Comments:  
  
* Code can be less obvious over time, even to its own author  
	* Avoid obvious comments that merely describe a common action  
    * Avoid comments that will not be updated - bad comments are worse than no comments  
    * Be consistent - have a style of commenting, and stay with it  
    * Follow the general rules of grammar, including capitalization  
    * Be very careful with jokes or other comments that could cause offsense  
  
Dotty:  
  
* The full stop in R has a very special meaning - related to S3 Object Oriented Programming  
	* summary(m) runs as summary.lm(m) if class(m) is "lm"  
    * Generally agreed rule that dots should not be used in variable names  
  
Coding Style:  
  
* Consistency is key, particularly for the same piece of code and/or collaborators  
	* Do not mix and match <- and =  
    * Consistent spacing is easy to implement and improves readability  
    * Use spaces around assignment operators  
    * Add spaces after a comma  
  
Static Code Analysis for R:  
  
* Static code analysis is the computer checking the code  
* The lintr package checks for syntax and/or semantic issues  
	* lintr::lint("myFileName.R")  
  
Example code includes:  
```{r eval=FALSE}

m <- mean(x)
s <- sd(x)
n <- length(x)
c(m - 1.96 * s/sqrt(n), m + 1.96 * s/sqrt(n))

m <- mean(y)
s <- sd(y)
n <- length(y)
c(m - 1.96 * s/sqrt(n), m + 1.96 * s/sqrt(n))

# Define a function to prevent pasting the code above
ci <- function(values) {
    n <- length(values)
    m <- mean(values) 
    s <- sd(values) 
    c(m - 1.96*s/sqrt(n), m + 1.96*s/sqrt(n))
}


# Define a function to prevent pasting the code above
ci <- function(x, plot_it = FALSE) {
    # Plot the data
    if (isTRUE(plot_it)) hist(x)
    m <- mean(x)
    s <- sd(x)
    n <- length(x)
    c(m - 1.96 * s/sqrt(n), m + 1.96 * s/sqrt(n))
}

# Generate 100 normal random numbers
sample_values <- rnorm(100)
ci(sample_values)


# Fix the code
f <- function(x, y, z) {
    x + y + z
}


# Fix the code
summarise_data <- function(values){
    c(mean(values), median(values))
}

stats <- summarise_data(runif(10))

```
  
  
  
***
  
Chapter 4 - Creating a Battle Plan  
  
Battle Plan:  
  
* Building work in small, modular chunks allows for better QC and for building on past work  
* Sensible filenames help with organizing and maintaining code  
	* Do not use spaces in filenames - "always a bad idea"  
    * Underscores have a few small problems - especially that \w treates _ as a character  
    * Generally a best practice to use dashes rather than underscores  
  
Human Readable Filenames:  
  
* URL slugs are the ending portion of a web address  
* Good filenames should be similar to good slugs - more descriptive is generally better  
* Dates should be unambiguous and nicely sortable  
	* ISO8601 is YYYY-MM-DD  
* Numbers in filenames or directories can be handy  
	* Using 01 is often better than 1, since sorting will still work for 10-99 files  
  
Organizing Projects:  
  
* Code often starts with small code, and expands (and becomes more messy) over time  
* Can mitigate issues by having every project in its own directory  
	* Can then put the same structure inside every directory  
    * Data edited only inside R  
    * Can have a directory with the boring name R, and containing the file load.R (will load what is needed from the /input/ directory)  
    * Can have standard names like clean.R, analysis.R, etc.  
  
Graphics and Output:  
  
* Can add directories output and graphics, each generated by the scripts in R  
  
Example code includes:  
```{r eval=FALSE}

# The load.R file
library("readr")
library("readxl")

# Print the current working directory
getwd()

# Change to relative paths
battles <- read_csv("input/battles.csv")
foes <- read_xlsx("input/foes.xlsx")


library("ggplot2")
library("readr")

# Show the file/directory structure
list.files(".")

# Change to relative paths and load the data
battles <- read_csv("input/battles.csv")
g <- ggplot(battles) + 
    geom_bar(aes(Location)) # Bar chart

# Change to relative paths and save the data
ggsave(filename = "graphics/locations.pdf", plot = g)


# The load.R file
library("readr")
library("readxl")

# Change to relative paths
battles <- read_csv("input/battles.csv")
foes <- read_xlsx("input/foes.xlsx")

library("ggplot2")
library("readr")
source("R/load.R")

# Create a bar chart
g <- ggplot(battles) + 
    geom_bar(aes(Location))

# Change to relative paths
ggsave(filename = "graphics/locations.pdf", plot = g)

```
  
  
  
***
  
### _Feature Engineering in R_  
  
Chapter 1 - Creating Features from Categorical Data  
  
Introduction to Feature Engineering in R:  
  
* Feature engineering is the process of modifying and combining input features to best solve the analytical problem  
* Modeling processes tend to be iterative, with a mix of exploring and the refining anf repeating  
* Categorical variables often needed to be encoded as factors (numbers) for machine interpretability  
* Can use the caret package to create dummy variables automatically  
	* caret::dummyVars("~myVar", data=myData)  
* Can run One Hot Encoding through a series of ifelse statements inside mutate  
  
Binning Encoding: Content Driven:  
  
* Can use select() and table() to find the categories and counts  
* The case-when approach is valuable when combining multiple categories in to larger groups  
	* mutate(new_var = case_when(a == "b" ~ "11", a == "c" ~ "11", a == "d" ~ "12", …, TRUE ~ as.character(a)))  # TRUE means everything not in the case_when  
  
Binning Encoding: Data Driven:  
  
* May need to bin even where there are many potential categories; need more meaningful features than One Hot Encoding  
	* prop_results <- prop.table(myTable, 1)  # the 1 means proportions of the row  
    * prop_results %>% filter(myKeyCol) %>% arrange(n)  # sort from low to high  
* Can then use the results to come up with expert cut points for creating a few bins  
  
Example code includes:  
```{r}

id <- c(3410, 9157, 2250, 2353, 4872, 2929, 4077, 1351, 9596, 4157, 3536, 5742, 5183, 2432, 8359, 3055, 6889, 2850, 6975, 7697, 694, 8580, 7161, 3183, 6188, 8429, 7552, 5562, 5455, 5354, 4126, 3246, 2033, 4444, 3875, 2926, 648, 9110, 546, 1871, 4340, 8645, 5766, 189, 6321, 3414, 5686, 2863, 5853, 9226, 655, 2966, 1176, 3817, 1070, 7258, 7871, 7581, 8098, 1123, 7106, 9532, 8566, 4229, 4285, 544, 407, 8262, 6232, 7946, 6995, 8134, 1512, 2302, 2034, 9555, 9467, 7627, 4558, 7661, 7650, 4326, 8771, 8147, 6613, 5080, 9968, 9908, 4585, 3751, 9588, 8491, 84, 6731, 3652, 5449, 4482, 587, 2620, 5655, 9915, 4243, 8141, 8934, 1908, 4480, 2320, 1612, 9315, 8035, 8970, 6039, 1202, 9223, 9879, 2456, 1633, 2798, 4435, 4774, 7706, 6378, 8537, 1235, 1663, 8698, 2397, 951, 8753, 9802, 4504, 426, 1482, 1157, 8866, 9678, 3122, 5903, 1252, 9669, 6233, 5463, 2952, 8259, 1162, 1006, 5625, 8526, 9584, 9177, 5592, 1489, 6035, 223, 62, 751, 393, 2471, 4813, 7462, 1809, 5089, 2290, 2020, 7408, 7338, 1214, 3690, 3715, 250, 2968, 6546, 2300, 8835, 5976, 2783, 3292, 7578, 5519, 3250, 8831, 2278, 9783, 4970, 2446, 5813, 6017, 8048, 4628, 5550, 4995, 636, 6238, 4767, 8013, 4601, 8460, 9929, 9592, 5148, 4215, 8516, 9725, 6753, 3952, 6038, 2682, 5886, 630, 9006, 2710, 812, 1545, 5011, 7587, 7847, 5184, 5889, 7411, 9559, 7008, 5986, 811, 7515, 3084, 929, 6359, 4879, 144, 1712, 6758, 7142, 746, 4947, 3673, 9261, 2745, 9755, 8489, 4073, 8397, 1297, 2407, 4080, 2713, 5275, 8447, 192, 5542, 1927)
grade <- c(5, 12, 3, 3, 6, 4, 5, 2, 12, 6, 5, 7, 7, 3, 11, 4, 9, 4, 9, 10, 1, 11, 9, 4, 8, 11, 10, 7, 7, 7, 6, 4, 3, 6, 5, 4, 1, 11, 1, 3, 6, 11, 8, 1, 8, 5, 7, 4, 8, 12, 1, 4, 2, 5, 2, 9, 10, 10, 10, 2, 9, 12, 11, 6, 6, 1, 1, 10, 8, 10, 9, 10, 2, 3, 3, 12, 12, 10, 6, 10, 10, 6, 11, 10, 8, 7, 12, 12, 6, 5, 12, 11, 1, 9, 5, 7, 6, 1, 4, 7, 12, 6, 10, 11, 3, 6, 3, 2, 12, 10, 11, 8, 2, 12, 12, 3, 2, 4, 6, 6, 10, 8, 11, 2, 2, 11, 3, 2, 11, 12, 6, 1, 2, 2, 11, 12, 4, 8, 2, 12, 8, 7, 4, 10, 2, 2, 7, 11, 12, 12, 7, 2, 8, 1, 1, 1, 1, 3, 6, 9, 3, 7, 3, 3, 9, 9, 2, 5, 5, 1, 4, 8, 3, 11, 8, 4, 4, 10, 7, 4, 11, 3, 12, 7, 3, 8, 8, 10, 6, 7, 7, 1, 8, 6, 10, 6, 11, 12, 12, 7, 6, 11, 12, 9, 5, 8, 4, 8, 1, 11, 4, 1, 2, 7, 10, 10, 7, 8, 9, 12, 9, 8, 1, 10, 4, 2, 8, 6, 1, 3, 9, 9, 1, 7, 5, 12, 4, 12, 11, 5, 11, 2, 3, 5, 4, 7, 11, 1, 7, 3)
gender <- factor(c('Female', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male'))
discipline <- c(0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0)
infraction <- c('academic dishonesty', 'disruptive conduct', 'failure to cooperate', 'failure to cooperate', 'alcohol', 'failure to cooperate', 'fighting', 'disruptive conduct', 'fighting', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'fighting', 'plagiarism', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'fighting', 'failure to cooperate', 'alcohol', 'fighting', 'failure to cooperate', 'failure to cooperate', 'alcohol', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'fighting', 'fighting', 'failure to cooperate', 'failure to cooperate', 'fighting', 'failure to cooperate', 'failure to cooperate', 'fighting', 'disruptive conduct', 'failure to cooperate', 'academic dishonesty', 'academic dishonesty', 'disruptive conduct', 'alcohol', 'failure to cooperate', 'disruptive conduct', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'fighting', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'fighting', 'vandalism', 'failure to cooperate', 'minor incident', 'fighting', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'fighting', 'academic dishonesty', 'disruptive conduct', 'plagiarism', 'academic dishonesty', 'fighting', 'failure to cooperate', 'minor incident', 'academic dishonesty', 'fighting', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'minor incident', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'fighting', 'fighting', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'fighting', 'fighting', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'alcohol', 'fighting', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'fighting', 'academic dishonesty', 'disruptive conduct', 'plagiarism', 'disruptive conduct', 'alcohol', 'failure to cooperate', 'fighting', 'fighting', 'disruptive conduct', 'alcohol', 'academic dishonesty', 'failure to cooperate', 'alcohol', 'fighting', 'failure to cooperate', 'failure to cooperate', 'vandalism', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'failure to cooperate', 'disruptive conduct', 'failure to cooperate', 'failure to cooperate', 'alcohol', 'academic dishonesty')
infraction <- c(infraction, 'fighting', 'failure to cooperate', 'failure to cooperate', 'minor incident', 'failure to cooperate', 'disruptive conduct', 'alcohol', 'fighting', 'academic dishonesty', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'fighting', 'fighting', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'alcohol', 'academic dishonesty', 'vandalism', 'failure to cooperate', 'fighting', 'failure to cooperate', 'disruptive conduct', 'failure to cooperate', 'alcohol', 'fighting', 'disruptive conduct', 'fighting', 'disruptive conduct', 'academic dishonesty', 'disruptive conduct', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'disruptive conduct', 'fighting', 'fighting', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'fighting', 'fighting', 'disruptive conduct', 'academic dishonesty', 'alcohol', 'disruptive conduct', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'minor incident', 'failure to cooperate', 'academic dishonesty', 'failure to cooperate', 'fighting', 'fighting', 'vandalism', 'fighting', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'failure to cooperate', 'vandalism', 'fighting', 'minor incident', 'plagiarism', 'minor incident', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'alcohol', 'alcohol', 'academic dishonesty', 'alcohol', 'fighting', 'fighting', 'alcohol', 'failure to cooperate', 'minor incident', 'alcohol', 'fighting', 'failure to cooperate', 'academic dishonesty', 'fighting', 'failure to cooperate', 'disruptive conduct', 'failure to cooperate', 'disruptive conduct', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'academic dishonesty', 'failure to cooperate', 'fighting', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'plagiarism', 'fighting', 'academic dishonesty', 'disruptive conduct', 'academic dishonesty', 'academic dishonesty', 'vandalism', 'failure to cooperate', 'fighting', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'fighting', 'failure to cooperate', 'failure to cooperate', 'alcohol', 'disruptive conduct', 'failure to cooperate', 'academic dishonesty', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'failure to cooperate', 'fighting', 'alcohol', 'academic dishonesty', 'failure to cooperate')
times <- c(1473938153, 1483110896, 1489060088, 1493899994, 1493909505, 1483614394, 1492518486, 1494502453, 1495209565, 1494590293, 1483704752, 1476705077, 1481545093, 1489416862, 1493390730, 1478088653, 1476705910, 1485259499, 1480431250, 1492614726, 1486539571, 1478700385, 1484581056, 1482755357, 1483446278, 1482247343, 1474383858, 1473682392, 1481283681, 1485173234, 1489503848, 1477492361, 1485951473, 1491562724, 1477914755, 1491835059, 1484295102, 1478011951, 1478592346, 1482493816, 1486468634, 1475075730, 1481633206, 1478765263, 1485173006, 1487245747, 1477395294, 1490617068, 1491828436, 1484238621, 1476085966, 1492778297, 1491910329, 1479739916, 1484653465, 1485788584, 1495206822, 1482762803, 1480604700, 1490701662, 1475679546, 1491923600, 1482503451, 1474974934, 1494851233, 1487058134, 1489389522, 1490191070, 1484743053, 1491922384, 1477492959, 1487602919, 1494850933, 1495717272, 1487330996, 1495726535, 1478531164, 1488290353, 1472818971, 1490193538, 1496153591, 1489582550, 1494339621, 1478530685, 1485432998, 1490715336, 1491234140, 1481122391, 1479987613, 1489577270, 1492183462, 1479223633, 1480577585, 1493810573, 1474978111, 1476282532, 1473941219, 1474360465, 1492516515, 1477999051, 1474470475, 1478780271, 1487256863, 1487079493, 1495538105, 1496156561, 1486468944, 1494246393, 1475072152, 1473090763, 1488381954, 1488540975, 1492688980, 1478012745, 1481643977, 1478085901, 1475582009, 1489578677, 1490357943, 1489579269, 1478186519, 1479470482, 1485959227, 1484049999, 1494329765, 1481296561, 1493900316, 1475654478, 1481207010, 1480430874, 1476445526, 1483949281, 1479731318, 1480592314, 1476369532, 1490193865, 1487334057, 1473941362, 1478090382, 1477666074, 1479471905, 1473421548, 1486555602, 1493304379, 1485346538, 1476952402, 1485428735, 1486566600, 1487253880, 1474554634, 1476707895, 1493987697, 1484567632, 1482305579, 1479974396, 1479369745, 1485933183, 1491392716, 1475582850, 1479393434, 1484061732, 1488541793, 1474460983, 1488540387, 1493908692, 1475074857, 1486641048, 1494417673, 1491564900, 1481096843, 1487595204, 1493644827, 1476879918, 1480086060, 1493900260, 1490095752, 1496317517, 1478791768, 1493304308, 1485528923, 1483970062, 1473768259, 1490024346, 1489145829, 1494415660, 1483360369, 1481899428, 1490195704, 1474976116, 1478012482, 1487772608, 1495097185, 1493206536, 1484575342, 1478700512, 1479987287, 1492437830, 1483458545, 1480346620, 1493639689, 1485432736, 1477924233, 1491579799, 1479395782, 1476793778, 1493120617, 1476978304, 1496143032, 1482913710, 1489592461, 1479121859, 1478678802, 1486639883, 1485172303, 1486045483, 1485961556, 1473939375, 1493985609, 1475509067, 1485441623, 1491835910, 1491393930, 1480663866, 1479134060, 1492600360, 1485329189, 1494591915, 1476185444, 1493797262, 1489061537, 1482320494, 1484925313, 1476947699, 1480595285, 1485171879, 1479998225, 1485518119, 1481210950, 1496158904, 1478520382, 1487259327, 1476445306, 1486382276, 1485776699, 1475235680, 1493641377, 1477495455, 1487775512, 1492603708, 1479815502)
samps <- c(3230, 6560, 2134, 2228, 7145, 2773, 3865, 1280, 9692, 3935, 3348, 5202, 4754, 6900, 8771, 2893, 5992, 2698, 7739, 8273, 657, 8947, 7877, 3015, 5569, 8823, 6160, 5062, 4968, 4892, 6987, 6942, 1925, 4167, 3674, 6927, 615, 9339, 516, 1772, 4083, 6423, 5221, 180, 5675, 3234, 5159, 2711, 5296, 9419, 622, 2808, 1112, 6970, 1015, 6076, 8401, 8187, 8570, 1064, 7833, 6658, 8935, 3992, 4041, 514, 386, 8695, 5602, 8466, 7755, 8595, 1431, 2181, 1926, 9661, 6643, 6177, 4253, 8242, 8234, 4072, 6459, 6316, 5911, 7177, 9974, 6753, 4272, 3553, 9685, 8869, 79, 5951, 3457, 7257, 4197, 555, 2483, 5133, 9931, 4004, 8598, 9200, 1805, 7056, 2196, 1528, 9486, 8530, 6515, 5448, 1137, 6578, 6748, 2325, 1548, 2651, 4160, 4424, 8281, 5724, 8909, 1168, 1577, 9031, 2269, 901, 9070, 9844, 4209, 404, 1403, 1094, 9147, 9757, 2956, 5339, 1185, 9750, 5603, 4975, 2795, 8692, 1099, 953, 5106, 8901, 9682, 6566, 5083, 1410, 5445, 211, 59, 713, 372, 2340, 4455, 8101, 6866, 4681, 2171, 1912, 8065, 8017, 1148, 3494, 3519, 236, 2810, 5857, 2179, 9128, 5395, 2636, 3118, 8184, 7267, 6943, 9126, 2160, 6725, 4584, 2316, 5263, 7357, 8540, 4307, 7271, 7162, 604, 5606, 7120, 8517, 4287, 8846, 9942, 9689, 4727, 3980, 8891, 9787, 7568, 3747, 5447, 6912, 5325, 598, 9252, 2567, 771, 1464, 4619, 8193, 6237, 4755, 5327, 6115, 9663, 6015, 5404, 770, 8135, 2919, 881, 5706, 4504, 137, 1622, 5958, 7861, 708, 4564, 3478, 9445, 2601, 6719, 8867, 3861, 6371, 1227, 2279, 3868, 2570, 4833, 8836, 6781, 5043, 1824)

discipline_logs <- tibble::tibble(id, grade, discipline, gender, 
                                  infraction=factor(infraction), 
                                  timestamp=as.POSIXct(times, origin="1970-01-01", tz="UTC")
                                  )
glimpse(discipline_logs)


discipline_logs <- discipline_logs %>%
    mutate(male = ifelse(gender == "Male", 1, 0), female = ifelse(gender == "Female", 1, 0))


# Create a new column with the proper string encodings
discipline_logs_new <-  discipline_logs %>%
    mutate(school_type = case_when(grade >= 1 & grade <= 5 ~ "elementary_school",
                                   grade >= 6 & grade <= 8 ~ "middle_school",
                                   grade <= 12 & grade >= 9 ~ "high_school"
                                   )
           )

# Look at a table of the new column 
discipline_logs_new %>% 
    select(school_type) %>% 
    table()


discipline_logs_new <- discipline_logs_new %>%
    mutate(elem_sch = ifelse(school_type == "elementary_school", 1, 0),
           mid_sch = ifelse(school_type == "middle_school", 1, 0),
           high_sch = ifelse(school_type == "high_school", 1, 0)
           )


# Create a table of the frequencies
discipline_table <- discipline_logs %>% 
    select(grade, discipline) %>% 
    table()

# Create a table of the proportions
prop_table <- prop.table(discipline_table, 1)


dgr_prop <- discipline_logs %>%
    group_by(grade) %>%
    summarize(proportion=mean(discipline))
dgr_prop


# Combine the proportions and discipline logs data
discipline <- inner_join(discipline_logs, dgr_prop, by = "grade")

# Display a glimpse of the new data frame
glimpse(discipline)

# Create a new column with three levels using the proportions as ranges
discipline_ed <- discipline %>%
   mutate(education_levels = case_when(proportion >= 0 & proportion <= .20 ~ "low_grade",
                                       proportion > .20 & proportion <= .25 ~ "middle_grade", 
                                       proportion > .25 & proportion <= 1 ~ "high_grade"
                                       )
          )

glimpse(discipline_ed)

```
  
  
  
***
  
Chapter 2 - Creating Features from Numeric Data  
  
Numerical Bucketing or Binning:  
  
* Bins can work better than numbers when there are non-linear relationships among the variables  
* Can bucket in to equal buckets either equal by quantile or equal by variable  
	* myDF %>% mutate(myVar_cat = cut(myVar, breaks=seq(myLow, myHigh, by=myBy)))  
    * model.matrix(myResponse ~ myVar_cat - 1, data=myDF)  # the -1 means 'less than full rank' or 'include all the categories'  
  
Binning Numerical Data Using Quantiles:  
  
* Can run quantile bucketing to bin in to equal volumes  
	* myDF %>% mutate(myQ = ntile(myVar, 5))  % will create 5 equally sized buckets numbered 1:5  
  
Date and Time Feature Extraction  
  
* The lubridate package from the tidyverse makes working with dates and times much easier  
	* mdy_hm()  
    * ymd_hm()  
    * ymd_hms()  
* Can extract information from dates using lubridate  
	* wday()  # 1 is Sunday, 7 is Saturday  
    * wday(, label=TRUE)  # label with the 3-character abbreviation  
    * lubridate::hour()  
  
Example code includes:  
```{r}

samps <- c(1115, 3501, 3729, 3314, 1269, 3992, 443, 643, 3509, 3795, 3020, 1011, 8, 696, 419, 2159, 228, 4319, 1154, 4927, 1477, 4127, 4413, 2767, 2156, 2242, 3810, 1430, 2695, 4422, 2168, 1348, 1642, 2315, 4576, 400, 4062, 4273, 848, 3838, 1162, 2721, 3860, 3873, 922, 2844, 1182, 2907, 1759, 184, 227, 3407, 3571, 4852, 3487, 4254, 2455, 430, 15, 3942, 4267, 2497, 45, 2213, 4114, 1807, 3107, 1095, 3391, 1889, 2642, 2246, 101, 4134, 3057, 3603, 2847, 3980, 3784, 195, 645, 4762, 1321, 2524, 2496, 1388, 12, 292, 2881, 3214, 216, 3982, 67, 2348, 1765, 1784, 1499, 2925, 4984, 648, 3773, 2711, 3945, 3780, 3995, 83, 801, 2684, 3604, 3271, 1101, 2264, 1487, 4541, 936, 4365, 3013, 2442, 3334, 2311, 4722, 2895, 4775, 3100, 3599, 1986, 2061, 3415, 4111, 2358, 3825, 535, 2539, 3738, 2736, 2478, 4646, 4452, 2601, 1790, 2429, 1246, 3849, 4072, 3480, 512, 2921, 4881, 3752, 2281, 1970, 382, 4368, 4855, 3001, 1010, 4492, 1433, 3806, 3238, 81, 1906, 2127, 389, 3581, 3940, 2620, 4565, 117, 4057, 2025, 4814, 364, 3794, 4545, 4399, 589, 2550, 2094, 802, 3665, 4540, 2409, 3774, 3679, 2073, 2212, 222, 970, 432, 3808, 4518, 4544, 3311, 3094, 2514, 2890, 3675, 3551, 1572, 4614, 4128, 2752, 1924, 3689, 4367, 77, 1345, 885, 974, 1241, 4272, 1068, 1096, 4233, 4819, 554, 4642, 780, 3863, 437, 1580, 935, 2649, 2481, 1869, 2380, 1169, 3809, 4900, 3931, 4437, 4213, 2657, 2146, 1191, 1896, 3984, 1252, 4094, 4670, 3036, 4948, 1220, 4137, 4721, 3939, 3983, 3432, 224)
Quantity <- c(2, 2, 3, 3, 20, 10, 3, 1, 6, 48, 1, 1, 1, 2, 6, 1, 3, 5, 1, 24, 1, 1, 4, 6, 12, 3, 1, 3, 6, 8, 2, 3, 3, 1, 1, 2, 2, 12, 4, 12, 16, 1, 1, 2, 2, 1, 12, 2, 6, 1, 31, 1, 1, 20, 3, 12, 3, 12, 1, 1, 6, 2, 12, 6, 4, 1, 2, 4, 12, 8, 6, 3, 1, 6, 6, 16, 1, 6, 1, 3, 24, 1, 3, 2, 1, 1, 3, 6, 1, 5, 48, 20, 36, 2, 9, 1, 12, 8, 5, 1, 1, 1, 12, 1, 12, 3, 2, 2, 6, 1, 1, 6, 24, 6, 1, 6, 3, 4, 10, 1, 3, 1, 1, 10, 2, 5, 6, 6, 1, 12, 12, 24, 3, 1, 2, 1, 10, 2, 3, 48, 6, 24, 12, 6, 1, 2, 1, 3, 1, 4, 2, 8, 6, 12, 2, 1, 1, 2, 24, 1, 6, 4, 1, 7, 8, 1, 3, 12, 2, 5, 36, 4, 1, 24, 4, 20, 2, 12, 3, 4, 1, 6, 2, 1, 1, 1, 24, 24, 2, 6, 3, 4, 12, 2, 12, 1, 12, 1, 2, 10, 2, 48, 2, 6, 6, 1, 48, 6, 1, 4, 48, 6, 24, 3, 1, 2, 1, 1, 8, 12, 16, 12, 3, 1, 12, 12, 6, 8, 24, 2, 2, 4, 2, 1, 2, 3, 1, 1, 2, 1, 24, 1, 2, 6, 4, 1, 3, 6, 12, 3)

online_retail <- tibble::tibble(Quantity)
glimpse(online_retail)


# Summarize the Quantity variable
online_retail %>% 
    select(Quantity) %>%
    summary()

# Create a histogram of the possible variable values
ggplot(online_retail, aes(x = Quantity)) + 
    geom_histogram(stat = "count")


# Create a sequence of numbers to capture the Quantity range
seq(1, 46, by=5)

# Use the cut function to create a variable quant_cat
online_retail <- online_retail %>%
    mutate(quant_cat = cut(Quantity, breaks = seq(1, 50, by = 5)))

# Create a table of the new column quant_cat
online_retail %>%
    select(quant_cat) %>%
    table()


# Create new columns from the quant_cat feature
head(model.matrix(~ quant_cat - 1, data = online_retail))

# Break the Quantity variable into 3 buckets
online_retail <- online_retail %>% 
    mutate(quant_q = ntile(Quantity, 3))

# Use table to look at the new variable
online_retail %>% 
    select(quant_q) %>% 
    table()


# Use table to look at the new variable
online_retail %>% 
    select(quant_q) %>% 
    table()

# Specify a full rank representation of the new column
head(model.matrix(~ quant_q, data = online_retail))


# Look at the column timestamp
discipline_logs %>% 
    select(timestamp) %>% 
    glimpse()

# Assign date format to the timestamp_date column
discipline_logs <- discipline_logs %>% 
    mutate(timestamp_date=lubridate::ymd_hms(timestamp))


# Create new column dow (day of the week) 
discipline_logs <- discipline_logs %>% 
    mutate(dow = lubridate::wday(timestamp_date, label = TRUE))

head(discipline_logs)

# Create new column hod (hour of day) 
discipline_logs <- discipline_logs %>% 
    mutate(hod = lubridate::hour(timestamp_date))

head(discipline_logs)


# Create histogram of hod 
discipline_logs %>% 
    ggplot(aes(x=hod)) + 
    geom_histogram()

```
  
  
  
***
  
Chapter 3 - Transforming Numerical Features  
  
Box and Yeo Transformations:  
  
* Transformations can be helpful to address variables on different scales, and skews  
* Power transformations can be used to convert to something resembling normality  
	* The Box-Cox transformation estimates the best power for the transform to be closer to normality (variable must always be greater than zero)  
    * The Yeo-Johnson transformation can be used for variables that are 0 or negative  
    * processed_vars <- caret::preProcess(myVar, method=c("YeoJohnson"))  
    * transformed <- predict(processed_vars, myDF)  
  
Normalization Techniques:  
  
* Rescaling to a range can be an effective technique  
* One common method of rescaling is to create (x - xmin) / (xmax - xmin)  
	* processed_vars <- caret::preProcess(myVars, method=c("range"))  
    * transformed <- predict(processed_vars, myDF)  
* Can center variables using method=c("center") in caret::preProcess()  
  
Z-score Standardization:  
  
* Z-score standardization normalizes all variables to N(0, 1)  
	* (x - mean(x)) / sd(x)  
    * processed_vars <- caret::preProcess(myVars, method=c("center", "scale"))  
    * transformed <- predict(processed_vars, myDF)  
  
Example code includes:  
```{r}

defense <- c(49, 63, 83, 43, 58, 78, 65, 80, 100, 35, 55, 50, 30, 50, 40, 40, 55, 75, 35, 60, 30, 65, 44, 69, 40, 55, 85, 110, 52, 67, 87, 40, 57, 77, 48, 73, 40, 75, 20, 45, 35, 70, 55, 70, 85, 55, 80, 50, 60, 25, 50, 35, 60, 48, 78, 35, 60, 45, 80, 40, 65, 95, 15, 30, 45, 50, 70, 80, 35, 50, 65, 35, 65, 100, 115, 130, 55, 70, 65, 110, 70, 95, 55, 45, 70, 55, 80, 50, 75, 100, 180, 30, 45, 60, 160, 45, 70, 90, 115, 50, 70, 80, 85, 95, 110, 53, 79, 75, 95, 120, 95, 120, 5, 115, 80, 70, 95, 60, 65, 55, 85, 65, 80, 35, 57, 57, 100, 95, 55, 79, 80, 48, 50, 60, 60, 60, 70, 100, 125, 90, 105, 65, 65, 100, 85, 90, 45, 65, 95, 90, 100, 65, 80, 100, 43, 58, 78, 64, 80, 100, 34, 64, 30, 50, 30, 50, 40, 70, 80, 38, 58, 15, 28, 15, 65, 85, 45, 70, 40, 55, 85, 95, 50, 80, 115, 75, 40, 50, 70, 55, 30, 55, 45, 45, 85, 60, 110, 42, 80, 60, 48, 58, 65, 90, 140, 70, 105, 200, 50, 75, 85, 100, 230, 75, 55, 50, 75, 40, 120, 40, 80, 95, 35, 75, 45, 70, 140, 30, 50, 95, 60, 120, 90, 62, 35, 35, 95, 15, 37, 37, 105, 10, 75, 85, 115, 50, 70, 110, 130, 90, 100, 35, 45, 65, 40, 60, 70, 50, 70, 90, 35, 70, 41, 61, 35, 55, 50, 55, 70, 30, 50, 70, 50, 40, 60, 30, 60, 30, 100, 25, 35, 65, 32, 62, 60, 80, 60, 80, 100, 90, 45, 45, 23, 43, 63, 30, 60, 40, 135, 45, 65, 75, 85, 100, 140, 180, 55, 75, 40, 60, 40, 50, 75, 75, 45, 53, 83, 20, 40, 35, 45, 40, 70, 140, 35, 65, 60, 45, 50, 80, 40, 60, 60, 90, 60, 60, 65, 85, 43, 73, 65, 85, 55, 105, 77, 97, 50, 100, 20, 79, 70, 70, 35, 65, 90, 130, 83, 80, 60, 48, 50, 80, 50, 70, 90, 85, 105, 105, 130, 55, 60, 100, 80, 80, 100, 130, 200, 100, 150, 90, 80, 90, 140, 90, 100, 50, 64, 85, 105, 44, 52, 71, 53, 68, 88, 30, 50, 70, 40, 60, 41, 51, 34, 49, 79, 35, 65, 40, 60, 118, 168, 45, 85, 50, 42, 102, 70, 35, 55, 45, 70, 48, 68, 66, 34, 44, 44, 84, 60, 52, 42, 64, 50, 47, 67, 86, 116, 95, 45, 5, 45, 108, 45, 65, 95, 40, 40, 70, 78, 118, 90, 110, 40, 65, 72, 56, 76, 50, 50, 75, 65, 115, 95, 130, 125, 67, 67, 95, 86, 130, 110, 125, 80, 70, 65, 145, 135, 70, 77, 130, 105, 70, 120, 100, 106, 110, 120, 120, 80, 100, 90, 100, 120, 100, 55, 75, 95, 45, 55, 65, 45, 60, 85, 39, 69, 45, 65, 90, 37, 50, 48, 63, 48, 63, 48, 63, 45, 85, 50, 62, 80, 32, 63, 85, 105, 130, 43, 55, 40, 60, 86, 55, 85, 95, 40, 55, 75, 85, 75, 70, 90, 80, 59, 99, 89, 60, 85, 50, 75, 65, 35, 45, 80, 45, 55, 67, 85, 125, 70, 115, 80, 85, 145, 103, 133, 45, 65, 62, 82, 40, 60, 40, 60, 50, 70, 95, 40, 50, 75, 50, 63, 50, 65, 85, 50, 70, 60, 45, 105)
defense <- c(defense, 45, 70, 50, 70, 80, 50, 60, 91, 131, 70, 95, 115, 40, 70, 80, 55, 75, 55, 60, 90, 60, 70, 90, 40, 80, 50, 85, 40, 84, 50, 60, 90, 50, 80, 70, 100, 95, 50, 75, 75, 105, 66, 112, 50, 70, 90, 55, 65, 129, 90, 72, 70, 70, 100, 120, 90, 90, 90, 77, 95, 65, 95, 122, 40, 58, 72, 40, 52, 67, 38, 77, 43, 55, 71, 40, 60, 50, 58, 72, 39, 47, 68, 48, 62, 62, 78, 60, 54, 76, 100, 150, 150, 60, 72, 66, 86, 53, 88, 67, 115, 60, 90, 62, 88, 33, 52, 77, 119, 50, 72, 65, 75, 57, 150, 35, 53, 70, 91, 48, 76, 70, 122, 85, 184, 35, 80, 95, 95, 121, 150, 60, 120, 55, 75, 75, 40, 50, 90, 54, 69, 74, 30, 50, 75, 30, 60, 45, 95, 90, 57, 77, 70, 40, 60, 40, 65, 20, 62, 152, 70, 100, 52, 92, 35, 90, 55, 80, 40, 60, 50, 80, 38, 48, 98, 90, 80, 90, 40, 140, 80, 110, 130, 95, 95, 100, 65, 135, 63, 80, 70, 85, 100, 65, 90, 125, 85, 75, 115, 115, 31, 131, 107, 89, 47, 139, 37, 71, 103, 131, 53, 101, 115, 80, 123, 111, 78, 120, 40, 80, 35, 70, 70, 40, 40, 40, 40, 40, 40, 50, 90, 120, 40, 75, 30, 60, 35, 60, 65, 100, 115, 130, 180, 50, 75, 80, 85, 110, 100, 120, 109, 85, 100, 70, 105, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 230, 140, 115, 90, 150, 75, 80, 110, 65, 125, 125, 230, 85, 80, 70, 100, 110, 70, 70, 70, 75, 60, 80, 130, 150, 120, 100, 90, 160, 100, 20, 160, 90, 45, 45, 105, 95, 70, 48, 68, 94, 115, 88, 105, 95, 107, 107, 107, 107, 107, 100, 75, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 126, 65, 105, 50, 50, 50, 70, 70, 70, 80, 70, 90, 90, 100, 90, 90, 95, 95, 95, 95, 67, 67, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 39, 39, 39, 39, 47, 47, 47, 47, 67, 68, 68, 68, 68, 60, 60, 60, 60, 60, 60, 60, 60, 60, 76, 50, 70, 70, 70, 122, 122, 122, 95, 71, 71, 121, 121, 110, 60, 60, 90, 70, 70, 70, 75, 130, 90, 60, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 100, 100, 100, 100, 100, 100, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 125, 115)
speed <- c(45, 60, 80, 65, 80, 100, 43, 58, 78, 45, 30, 70, 50, 35, 75, 56, 71, 101, 72, 97, 70, 100, 55, 80, 90, 110, 40, 65, 41, 56, 76, 50, 65, 85, 35, 60, 65, 100, 20, 45, 55, 90, 30, 40, 50, 25, 30, 45, 90, 95, 120, 90, 115, 55, 85, 70, 95, 60, 95, 90, 90, 70, 90, 105, 120, 35, 45, 55, 40, 55, 70, 70, 100, 20, 35, 45, 90, 105, 15, 30, 45, 70, 60, 75, 100, 45, 70, 25, 50, 40, 70, 80, 95, 110, 70, 42, 67, 50, 75, 100, 150, 40, 55, 35, 45, 87, 76, 30, 35, 60, 25, 40, 50, 60, 90, 60, 85, 63, 68, 85, 115, 90, 105, 95, 105, 93, 85, 110, 80, 81, 60, 48, 55, 65, 130, 65, 40, 35, 55, 55, 80, 130, 30, 85, 100, 90, 50, 70, 80, 130, 100, 45, 60, 80, 65, 80, 100, 43, 58, 78, 20, 90, 50, 70, 55, 85, 30, 40, 130, 67, 67, 60, 15, 15, 20, 40, 70, 95, 35, 45, 55, 50, 40, 50, 30, 70, 50, 80, 110, 85, 30, 30, 95, 15, 35, 110, 65, 91, 30, 85, 48, 33, 85, 15, 40, 45, 85, 30, 30, 45, 85, 65, 5, 85, 115, 40, 55, 20, 30, 50, 50, 35, 65, 45, 75, 70, 70, 65, 95, 85, 40, 50, 60, 85, 75, 35, 70, 65, 95, 83, 100, 55, 115, 100, 85, 41, 51, 61, 110, 90, 100, 70, 95, 120, 45, 55, 80, 40, 50, 60, 35, 70, 60, 100, 20, 15, 65, 15, 65, 30, 50, 70, 30, 60, 80, 85, 125, 85, 65, 40, 50, 80, 65, 80, 35, 70, 30, 90, 100, 40, 160, 40, 28, 48, 68, 25, 50, 20, 30, 50, 90, 50, 50, 30, 40, 50, 60, 80, 65, 105, 95, 95, 85, 85, 65, 40, 55, 65, 95, 60, 60, 35, 40, 20, 60, 80, 60, 10, 70, 100, 35, 55, 50, 80, 90, 65, 70, 70, 60, 60, 35, 55, 55, 75, 23, 43, 75, 45, 80, 81, 70, 40, 45, 65, 25, 25, 51, 65, 75, 23, 50, 80, 25, 45, 65, 32, 52, 52, 55, 97, 50, 50, 100, 30, 50, 70, 50, 50, 50, 110, 110, 90, 90, 95, 100, 150, 31, 36, 56, 61, 81, 108, 40, 50, 60, 60, 80, 100, 31, 71, 25, 65, 45, 60, 70, 55, 90, 58, 58, 30, 30, 36, 36, 66, 70, 40, 95, 85, 115, 35, 85, 34, 39, 115, 70, 80, 85, 105, 105, 71, 85, 112, 45, 74, 84, 23, 33, 10, 60, 30, 91, 35, 42, 82, 102, 5, 60, 90, 32, 47, 65, 95, 50, 85, 46, 66, 91, 50, 40, 60, 125, 60, 50, 40, 50, 95, 83, 80, 95, 95, 65, 95, 80, 90, 80, 40, 45, 110, 91, 95, 80, 115, 90, 100, 77, 100, 90, 85, 80, 100, 125, 100, 120, 100, 63, 83, 113, 45, 55, 65, 45, 60, 70, 42, 77, 55, 60, 80, 66, 106, 64, 101, 64, 101, 64, 101, 24, 29, 43, 65, 93, 76, 116, 15, 20, 25, 72, 114, 68, 88, 50, 35, 40, 45, 64, 69, 74, 45, 85, 42, 42, 92, 57, 47, 112, 66, 116, 30, 90, 98, 65, 74, 92, 50, 95, 60, 55, 45, 48, 58, 97, 30, 30, 22, 32, 70, 110, 65, 75, 65, 105, 75, 115, 45, 55, 65, 20, 30, 30, 55, 98, 44, 59, 79, 75, 95, 103, 60, 20, 15, 30, 40, 60, 65)
speed <- c(speed, 65, 108, 10, 20, 30, 50, 90, 60, 40, 50, 30, 40, 20, 55, 80, 57, 67, 97, 40, 50, 105, 25, 145, 32, 65, 105, 48, 35, 55, 60, 70, 55, 60, 80, 60, 80, 65, 109, 38, 58, 98, 60, 100, 108, 108, 108, 111, 111, 90, 90, 101, 95, 108, 90, 99, 38, 57, 64, 60, 73, 104, 71, 97, 122, 57, 78, 62, 84, 126, 35, 29, 89, 72, 106, 42, 52, 75, 52, 68, 43, 58, 102, 68, 104, 28, 35, 60, 23, 29, 49, 72, 45, 73, 50, 68, 30, 44, 44, 59, 70, 109, 48, 71, 46, 58, 60, 118, 101, 50, 40, 60, 80, 75, 38, 56, 51, 84, 28, 28, 55, 123, 99, 99, 95, 50, 70, 70, 42, 52, 70, 70, 90, 60, 40, 50, 60, 65, 75, 60, 45, 45, 46, 36, 43, 63, 43, 93, 84, 124, 60, 112, 40, 45, 35, 45, 35, 27, 42, 35, 45, 15, 30, 77, 117, 50, 60, 32, 62, 72, 100, 60, 80, 80, 40, 15, 35, 5, 59, 95, 60, 65, 36, 96, 96, 92, 36, 40, 45, 65, 85, 130, 95, 75, 85, 37, 37, 97, 97, 103, 79, 151, 83, 61, 109, 43, 79, 65, 125, 80, 100, 100, 78, 145, 121, 72, 77, 77, 90, 90, 90, 90, 90, 90, 110, 40, 65, 65, 109, 90, 110, 90, 115, 150, 20, 35, 45, 30, 25, 50, 130, 45, 45, 100, 105, 81, 150, 130, 140, 45, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 30, 75, 75, 115, 71, 145, 100, 70, 100, 20, 50, 50, 100, 135, 105, 20, 80, 70, 70, 70, 75, 115, 100, 120, 110, 110, 110, 90, 90, 115, 150, 90, 180, 36, 36, 36, 36, 85, 34, 39, 135, 92, 112, 30, 110, 86, 86, 86, 86, 86, 90, 127, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 50, 98, 55, 75, 75, 75, 95, 95, 95, 121, 101, 91, 95, 95, 108, 128, 99, 99, 99, 99, 122, 132, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 42, 42, 42, 42, 52, 52, 52, 52, 92, 75, 75, 75, 75, 102, 102, 102, 102, 102, 102, 102, 102, 102, 104, 60, 56, 46, 41, 99, 69, 54, 99, 115, 115, 95, 85, 110, 80, 45, 43, 93, 93, 93, 82, 30, 45, 117, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 60, 60, 60, 60, 60, 60, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 85, 65)
attack <- c(49, 62, 82, 52, 64, 84, 48, 63, 83, 30, 20, 45, 35, 25, 90, 45, 60, 80, 56, 81, 60, 90, 60, 95, 55, 90, 75, 100, 47, 62, 92, 57, 72, 102, 45, 70, 41, 76, 45, 70, 45, 80, 50, 65, 80, 70, 95, 55, 65, 55, 100, 45, 70, 52, 82, 80, 105, 70, 110, 50, 65, 95, 20, 35, 50, 80, 100, 130, 75, 90, 105, 40, 70, 80, 95, 120, 85, 100, 65, 75, 35, 60, 90, 85, 110, 45, 70, 80, 105, 65, 95, 35, 50, 65, 45, 48, 73, 105, 130, 30, 50, 40, 95, 50, 80, 120, 105, 55, 65, 90, 85, 130, 5, 55, 95, 40, 65, 67, 92, 45, 75, 45, 110, 50, 83, 95, 125, 100, 10, 125, 85, 48, 55, 65, 65, 130, 60, 40, 60, 80, 115, 105, 110, 85, 90, 100, 64, 84, 134, 110, 100, 49, 62, 82, 52, 64, 84, 65, 80, 105, 46, 76, 30, 50, 20, 35, 60, 90, 90, 38, 58, 40, 25, 30, 20, 40, 50, 75, 40, 55, 75, 80, 20, 50, 100, 75, 35, 45, 55, 70, 30, 75, 65, 45, 85, 65, 65, 85, 75, 60, 72, 33, 80, 65, 90, 70, 75, 85, 80, 120, 95, 130, 10, 125, 95, 80, 130, 40, 50, 50, 100, 55, 65, 105, 55, 40, 80, 60, 90, 95, 60, 120, 80, 95, 20, 35, 95, 30, 63, 75, 80, 10, 85, 115, 75, 64, 84, 134, 90, 130, 100, 45, 65, 85, 60, 85, 120, 70, 85, 110, 55, 90, 30, 70, 45, 35, 70, 35, 50, 30, 50, 70, 40, 70, 100, 55, 85, 30, 50, 25, 35, 65, 30, 60, 40, 130, 60, 80, 160, 45, 90, 90, 51, 71, 91, 60, 120, 20, 45, 45, 65, 75, 85, 70, 90, 110, 40, 60, 45, 75, 50, 40, 73, 47, 60, 43, 73, 90, 120, 70, 90, 60, 100, 85, 25, 45, 60, 100, 70, 100, 85, 115, 40, 70, 115, 100, 55, 95, 48, 78, 80, 120, 40, 70, 41, 81, 95, 125, 15, 60, 70, 90, 75, 115, 40, 70, 68, 50, 130, 23, 50, 80, 40, 60, 80, 64, 104, 84, 90, 30, 75, 95, 135, 55, 75, 135, 100, 50, 75, 80, 90, 100, 150, 150, 100, 150, 68, 89, 109, 58, 78, 104, 51, 66, 86, 55, 75, 120, 45, 85, 25, 85, 65, 85, 120, 30, 70, 125, 165, 42, 52, 29, 59, 94, 30, 80, 45, 65, 105, 35, 60, 48, 83, 100, 50, 80, 66, 76, 60, 125, 55, 82, 30, 63, 93, 24, 89, 80, 25, 5, 65, 92, 70, 90, 130, 85, 70, 110, 72, 112, 50, 90, 61, 106, 100, 49, 69, 20, 62, 92, 120, 70, 85, 140, 100, 123, 95, 50, 76, 110, 60, 95, 130, 80, 125, 55, 100, 80, 50, 75, 105, 125, 120, 120, 90, 160, 100, 70, 80, 100, 90, 100, 120, 100, 45, 60, 75, 63, 93, 123, 55, 75, 100, 55, 85, 60, 80, 110, 50, 88, 53, 98, 53, 98, 53, 98, 25, 55, 55, 77, 115, 60, 100, 75, 105, 135, 45, 57, 85, 135, 60, 80, 105, 140, 50, 65, 95, 100, 125, 53, 63, 103, 45, 55, 100, 27, 67, 35, 60, 92, 72, 82, 117, 90, 140, 86, 65, 105, 75, 90, 58, 30, 50, 78, 108, 112, 140, 50, 95, 65, 105, 50, 95, 30, 45, 55, 30, 40, 65, 44, 87, 50, 65, 95, 60, 100, 75, 75, 135, 55, 85, 40, 60, 75, 47, 77, 50, 94, 55, 80, 100, 55, 85, 115, 55, 75, 30, 40, 55, 87, 117)
attack <- c(attack, 147, 70, 130, 50, 40, 70, 66, 85, 125, 120, 74, 124, 85, 125, 110, 83, 123, 55, 65, 97, 109, 65, 85, 105, 85, 60, 90, 129, 90, 115, 115, 120, 150, 125, 130, 72, 77, 120, 61, 78, 107, 45, 59, 69, 56, 63, 95, 36, 56, 50, 73, 81, 35, 22, 52, 50, 68, 38, 45, 65, 65, 100, 82, 124, 80, 48, 48, 80, 110, 50, 52, 72, 48, 80, 54, 92, 52, 105, 60, 75, 53, 73, 38, 55, 89, 121, 59, 77, 65, 92, 58, 50, 50, 75, 100, 80, 70, 110, 66, 90, 69, 117, 30, 70, 131, 131, 100, 100, 110, 110, 55, 75, 107, 65, 85, 115, 54, 69, 74, 75, 85, 120, 70, 110, 62, 82, 70, 82, 132, 70, 45, 55, 65, 115, 20, 53, 63, 100, 125, 40, 70, 55, 105, 35, 45, 44, 64, 75, 125, 30, 40, 120, 52, 60, 120, 35, 125, 55, 75, 60, 95, 95, 60, 115, 78, 98, 90, 105, 60, 131, 55, 75, 110, 115, 85, 130, 75, 29, 29, 137, 113, 53, 139, 137, 89, 101, 181, 101, 107, 95, 125, 100, 130, 104, 103, 150, 80, 56, 71, 71, 55, 55, 55, 55, 55, 55, 85, 75, 100, 41, 67, 55, 100, 35, 60, 50, 80, 95, 120, 75, 80, 105, 65, 105, 80, 125, 155, 155, 135, 190, 150, 95, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 125, 150, 185, 90, 164, 110, 160, 150, 85, 85, 105, 140, 100, 75, 140, 120, 110, 70, 70, 70, 165, 150, 120, 145, 145, 100, 130, 150, 180, 180, 180, 70, 95, 29, 29, 79, 69, 60, 48, 83, 136, 170, 145, 132, 165, 65, 65, 65, 65, 65, 120, 103, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 60, 92, 30, 60, 60, 60, 100, 100, 100, 100, 105, 145, 120, 170, 72, 128, 120, 120, 120, 120, 95, 145, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 38, 38, 38, 38, 45, 45, 45, 45, 65, 65, 65, 65, 65, 80, 80, 80, 80, 80, 80, 80, 80, 80, 48, 150, 66, 66, 66, 85, 95, 100, 131, 100, 100, 100, 100, 160, 160, 110, 70, 70, 70, 70, 115, 140, 105, 64, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 60, 60, 60, 60, 60, 60, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 110, 95)
hp <- c(45, 60, 80, 39, 58, 78, 44, 59, 79, 45, 50, 60, 40, 45, 65, 40, 63, 83, 30, 55, 40, 65, 35, 60, 35, 60, 50, 75, 55, 70, 90, 46, 61, 81, 70, 95, 38, 73, 115, 140, 40, 75, 45, 60, 75, 35, 60, 60, 70, 10, 35, 40, 65, 50, 80, 40, 65, 55, 90, 40, 65, 90, 25, 40, 55, 70, 80, 90, 50, 65, 80, 40, 80, 40, 55, 80, 50, 65, 90, 95, 25, 50, 52, 35, 60, 65, 90, 80, 105, 30, 50, 30, 45, 60, 35, 60, 85, 30, 55, 40, 60, 60, 95, 50, 60, 50, 50, 90, 40, 65, 80, 105, 250, 65, 105, 30, 55, 45, 80, 30, 60, 40, 70, 65, 65, 65, 65, 75, 20, 95, 130, 48, 55, 130, 65, 65, 65, 35, 70, 30, 60, 80, 160, 90, 90, 90, 41, 61, 91, 106, 100, 45, 60, 80, 39, 58, 78, 50, 65, 85, 35, 85, 60, 100, 40, 55, 40, 70, 85, 75, 125, 20, 50, 90, 35, 55, 40, 65, 55, 70, 90, 75, 70, 100, 70, 90, 35, 55, 75, 55, 30, 75, 65, 55, 95, 65, 95, 60, 95, 60, 48, 190, 70, 50, 75, 100, 65, 75, 60, 90, 65, 70, 20, 80, 55, 60, 90, 40, 60, 50, 100, 65, 35, 75, 45, 85, 65, 45, 75, 75, 90, 90, 85, 73, 55, 35, 50, 45, 45, 45, 95, 255, 90, 115, 100, 50, 70, 100, 106, 106, 100, 40, 50, 70, 45, 60, 80, 50, 70, 100, 35, 70, 38, 78, 45, 50, 60, 50, 60, 40, 60, 80, 40, 70, 90, 40, 60, 40, 60, 28, 38, 68, 40, 70, 60, 60, 60, 80, 150, 31, 61, 1, 64, 84, 104, 72, 144, 50, 30, 50, 70, 50, 50, 50, 60, 70, 30, 60, 40, 70, 60, 60, 65, 65, 50, 70, 100, 45, 70, 130, 170, 60, 70, 70, 60, 80, 60, 45, 50, 80, 50, 70, 45, 75, 73, 73, 90, 90, 50, 110, 43, 63, 40, 60, 66, 86, 45, 75, 20, 95, 70, 60, 44, 64, 20, 40, 99, 75, 65, 95, 50, 80, 70, 90, 110, 35, 55, 55, 100, 43, 45, 65, 95, 40, 60, 80, 80, 80, 80, 80, 80, 100, 100, 105, 100, 50, 55, 75, 95, 44, 64, 76, 53, 64, 84, 40, 55, 85, 59, 79, 37, 77, 45, 60, 80, 40, 60, 67, 97, 30, 60, 40, 60, 70, 30, 70, 60, 55, 85, 45, 70, 76, 111, 75, 90, 150, 55, 65, 60, 100, 49, 71, 45, 63, 103, 57, 67, 50, 20, 100, 76, 50, 58, 68, 108, 135, 40, 70, 68, 108, 40, 70, 48, 83, 74, 49, 69, 45, 60, 90, 70, 70, 110, 115, 100, 75, 75, 85, 86, 65, 65, 75, 110, 85, 68, 60, 45, 70, 50, 75, 80, 75, 100, 90, 91, 110, 150, 120, 80, 100, 70, 100, 120, 100, 45, 60, 75, 65, 90, 110, 55, 75, 95, 45, 60, 45, 65, 85, 41, 64, 50, 75, 50, 75, 50, 75, 76, 116, 50, 62, 80, 45, 75, 55, 70, 85, 65, 67, 60, 110, 103, 75, 85, 105, 50, 75, 105, 120, 75, 45, 55, 75, 30, 40, 60, 40, 60, 45, 70, 70, 50, 60, 95, 70, 105, 75, 50, 70, 50, 65, 72, 38, 58, 54, 74, 55, 75, 50, 80, 40, 60, 55, 75, 45, 60)
hp <- c(hp, 70, 45, 65, 110, 62, 75, 36, 51, 71, 60, 80, 55, 50, 70, 69, 114, 55, 100, 165, 50, 70, 44, 74, 40, 60, 60, 35, 65, 85, 55, 75, 50, 60, 60, 46, 66, 76, 55, 95, 80, 50, 80, 109, 45, 65, 77, 59, 89, 45, 65, 95, 70, 100, 70, 110, 85, 58, 52, 72, 92, 55, 85, 91, 91, 91, 79, 79, 100, 100, 89, 125, 91, 100, 71, 56, 61, 88, 40, 59, 75, 41, 54, 72, 38, 85, 45, 62, 78, 38, 45, 80, 62, 86, 44, 54, 78, 66, 123, 67, 95, 75, 62, 74, 45, 59, 60, 78, 101, 62, 82, 53, 86, 42, 72, 50, 65, 50, 71, 44, 62, 58, 82, 77, 123, 95, 78, 67, 50, 45, 68, 90, 57, 43, 85, 49, 65, 55, 95, 40, 85, 126, 126, 108, 50, 80, 80, 68, 78, 78, 45, 65, 95, 50, 60, 80, 35, 55, 80, 48, 88, 47, 57, 77, 47, 97, 75, 40, 60, 45, 75, 45, 50, 50, 70, 100, 38, 68, 40, 70, 40, 60, 48, 68, 70, 120, 42, 52, 72, 51, 90, 100, 25, 75, 55, 85, 55, 95, 95, 60, 65, 60, 65, 55, 68, 78, 70, 45, 55, 75, 70, 70, 70, 70, 43, 43, 137, 137, 109, 107, 71, 83, 97, 59, 223, 97, 80, 90, 80, 78, 78, 79, 65, 83, 30, 75, 75, 35, 35, 35, 35, 35, 35, 60, 50, 75, 38, 73, 10, 35, 40, 65, 55, 40, 55, 80, 95, 80, 105, 60, 95, 60, 105, 65, 95, 80, 106, 106, 90, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 75, 70, 80, 75, 100, 70, 80, 100, 68, 50, 50, 70, 60, 70, 70, 70, 75, 70, 70, 70, 64, 65, 80, 95, 80, 80, 80, 100, 100, 105, 50, 50, 50, 40, 40, 60, 60, 70, 76, 111, 65, 108, 70, 90, 68, 50, 50, 50, 50, 50, 150, 100, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 103, 70, 105, 60, 60, 60, 80, 80, 80, 79, 79, 89, 125, 125, 91, 100, 71, 71, 71, 71, 72, 72, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 44, 44, 44, 44, 54, 54, 54, 54, 74, 78, 78, 78, 78, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 60, 44, 54, 59, 55, 75, 85, 126, 54, 54, 108, 216, 50, 80, 88, 77, 75, 75, 75, 85, 45, 70, 68, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 75, 80)
spatk <- c(65, 80, 100, 60, 80, 109, 50, 65, 85, 20, 25, 90, 20, 25, 45, 35, 50, 70, 25, 50, 31, 61, 40, 65, 50, 90, 20, 45, 40, 55, 75, 40, 55, 85, 60, 95, 50, 81, 45, 85, 30, 65, 75, 85, 110, 45, 60, 40, 90, 35, 50, 40, 65, 65, 95, 35, 60, 70, 100, 40, 50, 70, 105, 120, 135, 35, 50, 65, 70, 85, 100, 50, 80, 30, 45, 55, 65, 80, 40, 100, 95, 120, 58, 35, 60, 45, 70, 40, 65, 45, 85, 100, 115, 130, 30, 43, 73, 25, 50, 55, 80, 60, 125, 40, 50, 35, 35, 60, 60, 85, 30, 45, 35, 100, 40, 70, 95, 35, 65, 70, 100, 100, 55, 115, 95, 100, 55, 40, 15, 60, 85, 48, 45, 110, 110, 95, 85, 90, 115, 55, 65, 60, 65, 95, 125, 125, 50, 70, 100, 154, 100, 49, 63, 83, 60, 80, 109, 44, 59, 79, 35, 45, 36, 86, 40, 55, 40, 60, 70, 56, 76, 35, 45, 40, 40, 80, 70, 95, 65, 80, 115, 90, 20, 60, 30, 90, 35, 45, 55, 40, 30, 105, 75, 25, 65, 130, 60, 85, 100, 85, 72, 33, 90, 35, 60, 65, 35, 55, 40, 60, 55, 55, 10, 40, 35, 50, 75, 70, 90, 30, 60, 65, 65, 105, 65, 80, 40, 80, 110, 95, 40, 60, 105, 85, 20, 35, 35, 85, 65, 70, 40, 75, 115, 90, 90, 45, 65, 95, 90, 110, 100, 65, 85, 105, 70, 85, 110, 50, 60, 85, 30, 60, 30, 50, 20, 25, 100, 25, 50, 40, 60, 90, 30, 60, 90, 30, 75, 55, 95, 45, 65, 125, 50, 100, 40, 60, 35, 55, 95, 30, 50, 30, 51, 71, 91, 20, 40, 20, 45, 35, 55, 65, 55, 40, 50, 60, 40, 60, 65, 105, 85, 75, 47, 73, 100, 43, 73, 65, 95, 70, 90, 65, 105, 85, 70, 90, 60, 45, 50, 80, 85, 115, 40, 70, 60, 100, 95, 55, 46, 76, 50, 90, 40, 70, 61, 81, 40, 70, 10, 100, 70, 60, 63, 83, 30, 60, 72, 95, 75, 23, 50, 80, 55, 75, 95, 74, 94, 114, 45, 40, 40, 60, 110, 35, 55, 95, 50, 100, 75, 110, 130, 150, 100, 150, 100, 150, 45, 55, 75, 58, 78, 104, 61, 81, 111, 30, 40, 50, 35, 55, 25, 55, 40, 60, 95, 50, 125, 30, 65, 42, 47, 29, 79, 94, 30, 80, 45, 60, 85, 62, 87, 57, 92, 60, 60, 90, 44, 54, 105, 105, 42, 64, 65, 41, 71, 24, 79, 10, 70, 15, 92, 92, 40, 50, 80, 40, 35, 115, 38, 68, 30, 60, 61, 86, 90, 49, 69, 60, 62, 92, 45, 130, 80, 55, 110, 95, 125, 120, 116, 60, 130, 45, 70, 135, 65, 75, 65, 80, 95, 75, 105, 125, 150, 150, 130, 80, 100, 75, 80, 100, 135, 100, 120, 100, 45, 60, 75, 45, 70, 100, 63, 83, 108, 35, 60, 25, 35, 45, 50, 88, 53, 98, 53, 98, 53, 98, 67, 107, 36, 50, 65, 50, 80, 25, 50, 60, 55, 77, 30, 50, 60, 25, 40, 55, 50, 65, 85, 30, 30, 40, 50, 70, 30, 40, 55, 37, 77, 70, 110, 80, 35, 45, 65, 15, 30, 106, 35, 65, 35, 45, 103, 55, 95, 53, 83, 74, 112, 40, 60, 80, 120, 40, 65, 55, 75, 95, 105, 125, 125, 44, 87, 65, 80, 110, 40, 60, 75, 40, 60, 55, 85, 65, 85, 40, 57, 97, 24, 54, 45, 70, 70, 45, 75, 105, 85, 125, 65, 95, 145, 30, 40, 60, 60, 70, 95, 40)
spatk <- c(spatk, 100, 81, 55, 95, 60, 35, 55, 40, 60, 40, 37, 57, 45, 55, 105, 48, 45, 65, 125, 50, 135, 90, 72, 90, 125, 125, 150, 120, 115, 130, 129, 128, 120, 48, 56, 74, 62, 90, 114, 62, 83, 103, 32, 50, 40, 56, 74, 27, 27, 90, 73, 109, 61, 75, 112, 62, 97, 46, 69, 65, 63, 83, 35, 45, 50, 63, 99, 59, 85, 37, 68, 39, 54, 60, 97, 58, 120, 61, 109, 45, 69, 67, 99, 110, 74, 81, 50, 55, 83, 110, 80, 50, 65, 44, 58, 32, 44, 45, 97, 131, 131, 81, 100, 150, 130, 50, 70, 100, 60, 80, 80, 66, 91, 126, 30, 40, 75, 30, 55, 55, 55, 145, 42, 62, 98, 55, 95, 30, 55, 25, 43, 53, 45, 55, 40, 50, 50, 80, 65, 90, 71, 111, 45, 55, 30, 40, 50, 82, 90, 40, 20, 60, 70, 100, 30, 95, 95, 60, 75, 91, 40, 50, 70, 135, 86, 45, 65, 100, 95, 130, 85, 95, 29, 29, 113, 137, 127, 53, 137, 173, 107, 59, 97, 127, 130, 90, 122, 130, 159, 135, 15, 135, 25, 40, 40, 50, 50, 50, 50, 50, 50, 95, 10, 25, 50, 81, 35, 50, 50, 75, 175, 30, 45, 55, 130, 40, 65, 170, 125, 50, 60, 65, 70, 70, 154, 194, 165, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 55, 65, 40, 140, 95, 145, 130, 95, 165, 85, 55, 60, 80, 135, 110, 145, 110, 70, 70, 70, 93, 115, 120, 120, 105, 140, 160, 180, 150, 180, 180, 70, 95, 29, 29, 59, 69, 87, 57, 92, 54, 120, 140, 132, 65, 105, 105, 105, 105, 105, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 80, 80, 140, 40, 40, 40, 60, 60, 60, 110, 145, 105, 170, 120, 129, 77, 120, 120, 120, 120, 103, 153, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 61, 61, 61, 61, 75, 75, 75, 75, 125, 112, 112, 112, 112, 65, 65, 65, 65, 65, 65, 65, 65, 65, 83, 150, 44, 44, 44, 58, 58, 58, 131, 61, 61, 81, 91, 160, 170, 55, 145, 98, 98, 98, 55, 140, 80, 111, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 60, 60, 60, 60, 60, 60, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 130)
spdef <- c(65, 80, 100, 50, 65, 85, 64, 80, 105, 20, 25, 80, 20, 25, 80, 35, 50, 70, 35, 70, 31, 61, 54, 79, 50, 80, 30, 55, 40, 55, 85, 40, 55, 75, 65, 90, 65, 100, 25, 50, 40, 75, 65, 75, 90, 55, 80, 55, 75, 45, 70, 40, 65, 50, 80, 45, 70, 50, 80, 40, 50, 90, 55, 70, 95, 35, 60, 85, 30, 45, 70, 100, 120, 30, 45, 65, 65, 80, 40, 80, 55, 70, 62, 35, 60, 70, 95, 50, 100, 25, 45, 35, 55, 75, 45, 90, 115, 25, 50, 55, 80, 45, 75, 50, 80, 110, 110, 75, 45, 70, 30, 45, 105, 40, 80, 25, 45, 50, 80, 55, 85, 120, 80, 95, 85, 85, 70, 70, 20, 100, 95, 48, 65, 95, 95, 110, 75, 55, 70, 45, 70, 75, 110, 125, 90, 85, 50, 70, 100, 90, 100, 65, 80, 100, 50, 65, 85, 48, 63, 83, 45, 55, 56, 96, 80, 110, 40, 70, 80, 56, 76, 35, 55, 20, 65, 105, 45, 70, 45, 60, 90, 100, 50, 80, 65, 100, 55, 65, 95, 55, 30, 85, 45, 25, 65, 95, 130, 42, 110, 85, 48, 58, 65, 35, 60, 65, 65, 65, 40, 60, 55, 80, 230, 95, 75, 50, 75, 40, 80, 30, 60, 95, 35, 75, 45, 140, 70, 50, 80, 95, 40, 60, 95, 65, 45, 35, 110, 65, 55, 55, 70, 135, 100, 75, 115, 50, 70, 100, 154, 154, 100, 55, 65, 85, 50, 60, 70, 50, 70, 90, 30, 60, 41, 61, 30, 25, 50, 25, 90, 50, 70, 100, 30, 40, 60, 30, 50, 30, 70, 35, 55, 115, 52, 82, 60, 60, 35, 55, 65, 30, 50, 30, 23, 43, 73, 30, 60, 40, 90, 35, 55, 65, 55, 40, 50, 60, 55, 75, 40, 60, 75, 85, 85, 85, 80, 53, 83, 20, 40, 35, 45, 45, 75, 70, 80, 110, 60, 45, 50, 80, 40, 60, 75, 105, 60, 60, 85, 65, 41, 71, 35, 55, 70, 120, 87, 107, 50, 80, 55, 125, 70, 120, 33, 63, 90, 130, 87, 90, 60, 48, 50, 80, 50, 70, 90, 55, 75, 75, 65, 65, 30, 50, 80, 60, 80, 90, 100, 200, 150, 130, 110, 140, 90, 90, 100, 50, 55, 65, 85, 44, 52, 71, 56, 76, 101, 30, 40, 60, 40, 60, 41, 51, 34, 49, 79, 70, 105, 30, 50, 88, 138, 45, 105, 50, 42, 102, 90, 30, 50, 53, 78, 62, 82, 66, 44, 54, 56, 96, 105, 52, 37, 59, 50, 41, 61, 86, 116, 45, 90, 65, 42, 108, 45, 55, 85, 85, 40, 70, 42, 72, 55, 75, 40, 65, 72, 61, 86, 120, 60, 85, 85, 90, 95, 55, 50, 85, 95, 115, 56, 65, 95, 75, 60, 75, 115, 150, 135, 70, 77, 130, 105, 70, 100, 120, 106, 110, 120, 130, 80, 100, 90, 100, 120, 100, 55, 75, 95, 45, 55, 65, 45, 60, 70, 39, 69, 45, 65, 90, 37, 50, 48, 63, 48, 63, 48, 63, 55, 95, 30, 42, 55, 32, 63, 25, 40, 80, 43, 55, 45, 65, 86, 35, 50, 65, 40, 55, 75, 85, 75, 60, 80, 80, 39, 79, 69, 50, 75, 50, 75, 55, 35, 45, 70, 45, 55, 67, 35, 75, 70, 115, 80, 65, 105, 45, 65, 45, 65, 62, 82, 40, 60, 40, 60, 65, 85, 110, 50, 60, 85, 50, 63, 60, 75, 95, 50, 70, 60, 45, 105, 55, 80, 85, 105, 45, 50, 60, 86, 116, 60, 85, 85, 40, 70, 80, 55, 95, 55, 60, 90, 40, 50, 70, 40, 80, 135, 65, 60, 99, 50, 60)
spdef <- c(spdef, 90, 50, 80, 40, 70, 95, 50, 75, 65, 95, 66, 48, 50, 70, 90, 55, 105, 72, 90, 129, 80, 80, 120, 100, 80, 90, 90, 128, 95, 45, 58, 75, 60, 70, 100, 44, 56, 71, 36, 77, 38, 52, 69, 25, 30, 50, 54, 66, 79, 98, 154, 57, 81, 48, 71, 90, 60, 81, 37, 49, 150, 65, 89, 57, 75, 46, 75, 56, 86, 60, 123, 63, 89, 43, 94, 45, 59, 63, 92, 130, 63, 67, 150, 75, 113, 150, 87, 60, 82, 55, 75, 35, 46, 40, 80, 98, 98, 95, 150, 130, 90, 50, 70, 100, 40, 50, 90, 56, 81, 116, 30, 50, 75, 30, 60, 45, 75, 75, 47, 67, 70, 40, 70, 40, 65, 25, 52, 142, 55, 85, 72, 132, 35, 90, 75, 100, 40, 60, 50, 60, 38, 48, 98, 110, 110, 60, 30, 90, 45, 75, 130, 95, 95, 100, 95, 85, 73, 105, 70, 91, 90, 45, 70, 105, 75, 115, 95, 130, 31, 131, 89, 107, 131, 53, 37, 71, 101, 31, 53, 89, 115, 90, 120, 85, 115, 115, 80, 80, 35, 80, 80, 50, 50, 50, 50, 50, 50, 85, 35, 65, 65, 100, 45, 70, 40, 65, 95, 30, 45, 65, 80, 50, 100, 95, 75, 80, 100, 90, 130, 95, 100, 120, 110, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 95, 100, 105, 90, 120, 85, 80, 110, 135, 115, 95, 80, 85, 80, 65, 105, 105, 70, 70, 70, 83, 60, 80, 90, 110, 150, 120, 160, 90, 100, 20, 160, 90, 45, 45, 85, 95, 78, 62, 82, 96, 95, 70, 105, 115, 107, 107, 107, 107, 107, 100, 75, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 126, 55, 105, 50, 50, 50, 70, 70, 70, 90, 80, 80, 100, 90, 90, 77, 95, 95, 95, 95, 71, 71, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 79, 79, 79, 79, 98, 98, 98, 98, 128, 154, 154, 154, 154, 90, 90, 90, 90, 90, 90, 90, 90, 90, 81, 50, 55, 55, 55, 75, 75, 75, 98, 85, 85, 95, 95, 110, 130, 60, 75, 70, 70, 70, 75, 135, 90, 60, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 100, 100, 100, 100, 100, 100, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 105, 115)

poke_df <- data.frame(hp, attack, defense, spatk, spdef, speed)
glimpse(poke_df)


library(caret)

# Select the variables
poke_vars <- poke_df %>%
    select(defense, speed) 

# Perform a Box-Cox transformation
processed_vars <- preProcess(poke_vars, method = c("BoxCox"))

# Use predict to transform data
poke_df <- predict(processed_vars, poke_df)

# Plot transformed features
ggplot(poke_df, aes(x=defense)) + 
    geom_histogram()
ggplot(poke_df, aes(x=speed)) + 
    geom_histogram()


duration <- c(261, 151, 76, 92, 198, 139, 217, 380, 50, 55, 222, 137, 517, 71, 174, 353, 98, 38, 219, 54, 262, 164, 160, 342, 181, 172, 296, 127, 255, 348, 225, 230, 208, 226, 336, 242, 365, 1666, 577, 137, 160, 180, 22, 1492, 616, 242, 355, 225, 160, 363, 266, 253, 179, 787, 145, 174, 104, 13, 185, 1778, 138, 812, 164, 391, 357, 91, 528, 273, 158, 177, 258, 172, 154, 291, 181, 176, 211, 349, 272, 208, 193, 212, 20, 1042, 246, 529, 1467, 1389, 188, 180, 48, 213, 583, 221, 173, 426, 287, 101, 203, 197, 257, 124, 229, 55, 400, 197, 190, 21, 514, 849, 194, 144, 212, 286, 107, 247, 518, 364, 178, 98, 439, 79, 120, 127, 175, 262, 61, 78, 143, 579, 677, 345, 185, 100, 125, 193, 136, 73, 528, 541, 163, 301, 46, 204, 98, 71, 157, 243, 186, 579, 163, 610, 2033, 85, 114, 114, 57, 238, 93, 128, 107, 181, 303, 558, 270, 228, 99, 240, 673, 233, 1056, 250, 252, 138, 130, 412, 179, 19, 458, 717, 313, 683, 1077, 416, 146, 167, 315, 140, 346, 562, 172, 217, 142, 67, 291, 309, 248, 98, 256, 82, 577, 286, 477, 611, 471, 381, 42, 251, 408, 215, 287, 216, 366, 210, 288, 168, 338, 410, 177, 127, 357, 175, 300, 136, 1419, 125, 213, 27, 238, 124, 18, 730, 746, 121, 247, 40, 181, 79, 206, 389, 127, 702, 151, 117, 232, 408, 179, 39, 282, 714)
balance <- c(2143, 29, 2, 1506, 1, 231, 447, 2, 121, 593, 270, 390, 6, 71, 162, 229, 13, 52, 60, 0, 723, 779, 23, 50, 0, -372, 255, 113, -246, 265, 839, 378, 39, 0, 10635, 63, -7, -3, 506, 0, 2586, 49, 104, 529, 96, -171, -364, 0, 0, 0, 1291, -244, 0, -76, -103, 243, 424, 306, 24, 179, 0, 989, 249, 790, 154, 6530, 100, 59, 1205, 12223, 5935, 25, 282, 23, 1937, 384, 582, 91, 0, 1, 206, 164, 690, 2343, 137, 173, 45, 1270, 16, 486, 50, 152, 290, 54, -37, 101, 383, 81, 0, 229, -674, 90, 128, 179, 0, 54, 151, 61, 30, 523, 31, 79, -34, 448, 81, 144, 351, -67, 262, 0, 56, 26, 3, 41, 7, 105, 818, -16, 0, 2476, 1185, 217, 1685, 802, 0, 94, 0, 0, 517, 265, 947, 3, 42, 37, 57, 22, 8, 293, 3, 348, -19, 0, -4, 18, 139, 0, 1883, 216, 782, 904, 1705, 47, 176, 1225, 86, 82, 271, 1378, 184, 0, 0, 1357, 19, 434, 92, 1151, 41, 51, 214, 1161, 37, 787, 59, 253, 211, 235, 4384, 4080, 53, 0, 2127, 377, 73, 445, 243, 307, 155, 173, 400, 1428, 219, 7, 575, 298, 0, 5699, 176, 517, 257, 56, -390, 330, 195, 301, -41, 483, 28, 13, 965, 378, 219, 324, -69, 0, 205, 278, 1065, 34, 1033, 1467, -12, 388, 294, 1827, 627, 25, 315, 0, 66, -9, 349, 100, 0, 434, 3237, 275, 0, 207, 483, 2248)

bank_df <- data.frame(balance, duration)
glimpse(bank_df)


# Select both variables
bank_vars <- bank_df %>%
    select(balance, duration)
    
# Perform a Yeo-Johnson transformation 
processed_vars <- preProcess(bank_vars, method = c("YeoJohnson"))

# Use predict to transform data
bank_df <- predict(processed_vars, bank_df)

# Plot transformed features
ggplot(bank_df, aes(x=balance)) + 
    geom_density()
ggplot(bank_df, aes(x=duration)) + 
    geom_density()


# Create a scaled new feature scaled_hp
poke_df <- poke_df %>% 
    mutate(scaled_hp = (hp - min(hp)) / (max(hp) - min(hp)))

# Summarize both features
poke_df %>% 
    select(hp, scaled_hp) %>% 
    summary()


# Use mutate to create column attack_mc
poke_df <- poke_df %>% 
    mutate(attack_mc = attack - mean(attack))


# Select variables 
poke_vars <- poke_df %>% 
    select(attack, spatk, spdef)
    
# Use preProcess to mean center variables
processed_vars <- preProcess(poke_vars, method=c("center"))

# Use predict to include tranformed variables into data
poke_df <- predict(processed_vars, poke_df)

# Summarize the three new column scales
poke_df %>% 
    select(attack, spatk, spdef) %>% 
    summary()


# Standardize Speed
poke_df <- poke_df %>% 
    mutate(z_speed = (speed - mean(speed)) / sd(speed))

# Summarize new and original variable
poke_df %>% 
    select(speed, z_speed) %>% 
    summary()


# Select variables 
poke_vars <- poke_df %>% 
    select(attack, defense, spatk, spdef)

# Create preProcess variable list 
processed_vars <- preProcess(poke_vars, method = c("center", "scale"))

# Use predict to assign standardized variables
poke_df <- predict(processed_vars, poke_df)

# Summarize new variables
poke_df %>% 
    select(attack, defense, spatk, spdef) %>% 
    summary()

```
  
  
  
***
  
Chapter 4 - Advanced Methods  
  
Feature Crossing:  
  
* Feature crossing is combining 2+ features in to a single new feature  
* Can be useful to visualize features prior to feature crossing  
* Example for creating feature crossing  
	* dmy <- dummyVars(~ gender:infraction, data = myDF)  
    * newDF <- predict(dmy, myDF)  
  
Principal Component Analysis:  
  
* PCA is a linear combination of the original features, with each of the axes in the new space being orthogonal  
	* PCA is typically used for dimensionality reduction  
    * prcomp(myData, center=TRUE, scale=TRUE)  # run the pre-processing steps to create N(0, 1) on each underlying feature  
  
Interpreting PCA Output:  
  
* Can inspect and interpret the PCA outputs  
	* summary(myPCA)  # SD, Var. CumVar by component  
    * myPCA$sdev ** 2  # Variance  
    * autoplot(myPCA, data=myData, colour="myColorVar")  
  
Wrap Up:  
  
* Developing meaningful features from categorical data  
* Bucketing and binning for numerical variables, and date-times  
* Transformations such as Box-Cox or Yeo-Johnson and z-score  
* Principal Component Analysis (PCA)  
  
Example code includes:  
```{r}

# Copy over first 250 records
gender <- stringr::str_trim(c(c(' Male', ' Male', ' Male', ' Male', ' Female', ' Female', ' Female', ' Male', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Female', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Female', ' Female', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Female', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Female', ' Female', ' Female', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Female', ' Female', ' Male', ' Male', ' Female', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Female', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Female', ' Female', ' Male', ' Male', ' Male', ' Female', ' Female', ' Male', ' Male', ' Female', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Female', ' Male', ' Female', ' Male', ' Female', ' Male', ' Female', ' Female', ' Female', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Female', ' Female', ' Female', ' Male', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Female', ' Female', ' Female', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Female', ' Male', ' Male', ' Female', ' Female', ' Female'), c(' Male', ' Male', ' Female', ' Female', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Female', ' Female', ' Male', ' Female', ' Female', ' Male', ' Female', ' Male', ' Female', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male', ' Male', ' Male', ' Male', ' Female', ' Male', ' Male')))
occ1 <- c(' Adm-clerical', ' Exec-managerial', ' Handlers-cleaners', ' Handlers-cleaners', ' Prof-specialty', ' Exec-managerial', ' Other-service', ' Exec-managerial', ' Prof-specialty', ' Exec-managerial', ' Exec-managerial', ' Prof-specialty', ' Adm-clerical', ' Sales', ' Craft-repair', ' Transport-moving', ' Farming-fishing', ' Machine-op-inspct', ' Sales', ' Exec-managerial', ' Prof-specialty', ' Other-service', ' Farming-fishing', ' Transport-moving', ' Tech-support', ' Tech-support', ' Craft-repair', 'unknown', ' Exec-managerial', ' Craft-repair', ' Protective-serv', ' Sales', ' Exec-managerial', ' Adm-clerical', ' Other-service', ' Machine-op-inspct', ' Machine-op-inspct', ' Adm-clerical', ' Sales', ' Prof-specialty', ' Machine-op-inspct', ' Prof-specialty', ' Tech-support', ' Adm-clerical', ' Handlers-cleaners', ' Prof-specialty', ' Machine-op-inspct', ' Exec-managerial', ' Craft-repair', ' Prof-specialty', ' Exec-managerial', ' Other-service', ' Prof-specialty', ' Exec-managerial', ' Exec-managerial', ' Tech-support', ' Machine-op-inspct', ' Other-service', ' Adm-clerical', ' Machine-op-inspct', ' Sales', 'unknown', ' Transport-moving', ' Prof-specialty', ' Tech-support', ' Craft-repair', ' Adm-clerical', ' Adm-clerical', ' Exec-managerial', 'unknown', ' Prof-specialty', ' Sales', ' Sales', ' Machine-op-inspct', ' Prof-specialty', ' Other-service', ' Adm-clerical', 'unknown', ' Other-service', ' Farming-fishing', ' Sales', ' Other-service', ' Other-service', ' Sales', ' Craft-repair', ' Sales', ' Protective-serv', ' Prof-specialty', ' Sales', ' Prof-specialty', ' Prof-specialty', ' Craft-repair', ' Machine-op-inspct', ' Sales', ' Protective-serv', ' Handlers-cleaners', ' Prof-specialty', ' Sales', ' Exec-managerial', ' Other-service', ' Exec-managerial')
occ2 <- c(' Exec-managerial', ' Prof-specialty', ' Tech-support', ' Craft-repair', ' Craft-repair', 'unknown', ' Handlers-cleaners', ' Adm-clerical', ' Handlers-cleaners', ' Sales', ' Prof-specialty', ' Other-service', ' Sales', ' Machine-op-inspct', ' Handlers-cleaners', ' Sales', ' Craft-repair', ' Sales', ' Craft-repair', ' Other-service', ' Exec-managerial', ' Exec-managerial', ' Prof-specialty', ' Other-service', ' Exec-managerial', ' Adm-clerical', ' Adm-clerical', 'unknown', ' Craft-repair', ' Sales', ' Other-service', ' Craft-repair', ' Sales', ' Tech-support', ' Prof-specialty', ' Craft-repair', ' Adm-clerical', ' Sales', ' Craft-repair', ' Craft-repair', ' Sales', ' Other-service', ' Prof-specialty', ' Tech-support', ' Transport-moving', ' Other-service', ' Other-service', ' Craft-repair', 'unknown', ' Adm-clerical', ' Adm-clerical', ' Exec-managerial', ' Craft-repair', 'unknown', ' Craft-repair', ' Handlers-cleaners', ' Sales', ' Craft-repair', ' Other-service', 'unknown', ' Other-service', ' Exec-managerial', ' Exec-managerial', ' Sales', ' Other-service', ' Exec-managerial', ' Protective-serv', ' Handlers-cleaners', ' Prof-specialty', ' Other-service', ' Protective-serv', ' Sales', ' Craft-repair', ' Prof-specialty', ' Sales', ' Craft-repair', ' Handlers-cleaners', ' Other-service', ' Prof-specialty', ' Exec-managerial', ' Adm-clerical', ' Craft-repair', ' Machine-op-inspct', ' Adm-clerical', ' Adm-clerical', ' Exec-managerial')
occ3 <- c( 'unknown', ' Prof-specialty', ' Prof-specialty', ' Machine-op-inspct', ' Machine-op-inspct', ' Craft-repair', ' Tech-support', ' Tech-support', ' Transport-moving', ' Craft-repair', ' Exec-managerial', ' Prof-specialty', ' Sales', ' Prof-specialty', 'unknown', ' Exec-managerial', ' Prof-specialty', ' Adm-clerical', ' Sales', ' Other-service', ' Craft-repair', ' Sales', ' Sales', ' Transport-moving', ' Craft-repair', ' Sales', ' Craft-repair', ' Machine-op-inspct', ' Exec-managerial', ' Sales', ' Sales', ' Prof-specialty', ' Craft-repair', ' Handlers-cleaners', 'unknown', ' Other-service', ' Adm-clerical', ' Machine-op-inspct', ' Sales', 'unknown', ' Farming-fishing', ' Adm-clerical', ' Adm-clerical', ' Transport-moving', ' Sales', ' Adm-clerical', ' Craft-repair', ' Prof-specialty', ' Other-service', ' Adm-clerical', ' Exec-managerial', ' Exec-managerial', ' Transport-moving', ' Prof-specialty', ' Other-service', ' Protective-serv', 'unknown', ' Craft-repair', ' Adm-clerical', ' Adm-clerical', ' Other-service', ' Tech-support', ' Adm-clerical')
occupation <- stringr::str_trim(c(occ1, occ2, occ3))

adult_incomes <- data.frame(gender, occupation, stringsAsFactors = FALSE)
glimpse(adult_incomes)


# Group the data and create a summary of the counts
adult_incomes %>%
    count(occupation, gender) %>%
    # Create a grouped bar graph
    ggplot(., aes(x=occupation, y=n, fill=gender)) +
    geom_bar(stat="identity", position="dodge") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))


# Create a table of the variables of interest
adult_incomes %>% 
    select(gender, occupation) %>% 
    table()


# Create a feature cross between gender and occupation
dmy <- dummyVars(~ gender:occupation, data=adult_incomes)

# Create object of your resulting data frame
oh_data <- predict(dmy, adult_incomes)

# Summarize the resulting output
summary(oh_data)


# Create the df
poke_x <- poke_df %>% 
    select(hp, attack, defense, spatk, spdef, speed)

# Perform PCA 
poke_pca <- prcomp(poke_x, center=TRUE, scale=TRUE)


# Calculate the proportion of variance
prop_var <- data.frame(sdev=poke_pca$sdev)
prop_var <- prop_var %>%
    mutate(pca_comp = 1:n(), pcVar = sdev^2, propVar_ex = pcVar/sum(pcVar))


# Create a plot of the components and proportion of variance
ggplot(prop_var, aes(pca_comp, propVar_ex, group=1)) + 
    geom_line() +
    geom_point()


# Create a plot of the first two components
library(ggfortify)
autoplot(poke_pca, data = poke_df)

```
  
  
  
***
  
### _Introduction to Text Analysis in R_  
  
Chapter 1 - Wrangling Text  
  
Text as Data:  
  
* The tidyverse tools work well together and can be leveraged for text analysis  
	* library(tidyverse)  
    * review_data <- read_csv("myFile.csv")  
* Can use group_by() and summarize() to get key metrics by category  
	* review_data %>% group_by(product) %>% summarize(mean(stars, na.rm=TRUE))  
  
Counting Categorical Data:  
  
* The standard print() command for the tibble shows each column and its data type  
* Can get count data using n() or count()  
	* review_data %>% group_by(myGroup) %>% summarize(n = n())  
    * review_data %>% count(myGroup) %>% arrange(desc(n)) # default name for the column created is n  
  
Tokenizing and Cleaning:  
  
* The tidytext package provides a suite of powerful tools for structuring and analyzing text  
* Some of the NLP (natural language processing) vocabulary includes:  
	* Bag of words - assumes each word is independent  
    * Each separate body of text is a document  
    * Each unique word is a term  
    * Every occurrence of a term is known as a token  
* Can use unnest_tokens() to create tokens  
	* tidy_review <- review_data %>% unnest_tokens(word, review)  # create column word by tokenizing column review; also cleans punctuation, casing, white-space, etc.  
    * tidy_review %>% count(word) %>% arrange(desc(n))  
* Stop words are common and non-informative words - can use anti_join to eliminate these  
	* tidy_review %>% anti_join(stop_words) %>% count(word) %>% arrange(desc(n))  
  
Example code includes:  
```{r}


twitter_data <- readRDS("./RInputFiles/ch_1_twitter_data.rds")
glimpse(twitter_data)


# Print twitter_data
twitter_data

# Print just the complaints in twitter_data
twitter_data %>% 
    filter(complaint_label == "Complaint")


# Start with the data frame
twitter_data %>% 
    # Group the whether or not the tweet is a complaint
    group_by(complaint_label) %>% 
    # Compute the mean, min, and max follower counts
    summarize(avg_followers = mean(usr_followers_count),
              min_followers = min(usr_followers_count),
              max_followers = max(usr_followers_count)
              )


twitter_data %>% 
    # Filter for just the complaints
    filter(complaint_label == "Complaint") %>% 
    # Count the number of verified and non-verified users
    count(usr_verified)


twitter_data %>% 
    # Group by whether or not a user is verified
    group_by(usr_verified) %>% 
    summarize(
        # Compute the average number of followers
        avg_followers = mean(usr_followers_count),
        # Count the number of users in each category
        n = n()
        )


tidy_twitter <- twitter_data %>% 
    # Tokenize the twitter data
    tidytext::unnest_tokens(word, tweet_text) 

tidy_twitter %>% 
    # Compute word counts
    count(word) %>% 
    # Arrange the counts in descending order
    arrange(desc(n))


tidy_twitter <- twitter_data %>% 
    # Tokenize the twitter data
    tidytext::unnest_tokens(word, tweet_text) %>% 
    # Remove stop words
    anti_join(tidytext::stop_words)

tidy_twitter %>% 
    # Filter to keep complaints only
    filter(complaint_label == "Complaint") %>% 
    # Compute word counts and arrange in descending order
    count(word) %>% 
    arrange(desc(n))

```
  
  
  
***
  
Chapter 2 - Visualizing Text  
  
Plotting Word Counts:  
  
* Can get the row numbers using row_number()  
	* myData %>% mutate(id=row_number()) %>% …  
* Can generate a bar plot to look at the results  
	* word_counts <- tidy_review %>% count(word) %>% arrange(desc(n))  
    * ggplot(word_counts, aes(x=word, y=n)) + geom_col()  # many problems with this chart  
* Should run filters to get only the key words of interest  
	* Cutoffs for number of usages  
    * Flipping coordinates with coord_flip()  
    * Adding a title with ggtitle("myTitle")  
  
Improving Word Count Plots:  
  
* Often have additional custom stop words that should be added to the defaults  
* Can use tribble() to set up a tibble  
	* custom_stop <- tribble(~colName1, ~colName2, "col1Row1Data", "col2Row1Data", "col1Row2Data", "col2Row2Data", …)  
    * stop_words2 <- stop_words %>% bind_rows(custom_stop)  
* The arrange function does not impact the plots - need to reorder the levels of the factors  
	* mutate(word2 = fct_reorder(word, n))  # reorder column word by n  
  
Faceting Word Count Plots:  
  
* Can count words by produce to see where the primary words are being used  
	* tidy_review %>% count(word, product) %>% arrange(desc(n)) %>% top_n(10, n)  %>% ungroup() # keep the top 10 items based on n  
* Can use fct_reorder() as per previous and then plot using facet_wrap()  
	* ggplot(word_counts, aes(x=word2, y=n, fill=product)) + geom_col(show.legend=FALSE) + facet_wrap(~ product, scales = "free_y") + coord_flip() + ggtitle("myTitle")  
  
Plotting Word Clouds:  
  
* The wordcloud() package can be used to make word clouds  
	* library(wordcloud)  
    * word_counts <- tidy_review %>% count(word)  
    * wordcloud(words = word_counts$word, freq=word_counts$n, max.words=30, colors="blue")  
* Location of words in a cloud is random, while size of the word is based on word frequency  
* Need to be careful not to include too many words in the max.words (cloud may run off the screen)  
  
Example code includes:  
```{r}

word_counts <- tidy_twitter %>% 
    filter(complaint_label == "Complaint") %>% 
    count(word) %>% 
    # Keep words with count greater than 100
    filter(n > 100)

# Create a bar plot using word_counts
ggplot(word_counts, aes(x=word, y=n)) +
    geom_col() +
    # Flip the plot coordinates
    coord_flip()


word_counts <- tidy_twitter %>% 
    # Only keep the non-complaints
    filter(complaint_label == "Non-Complaint") %>% 
    count(word) %>% 
    filter(n > 150)

# Create a bar plot using the new word_counts
ggplot(word_counts, aes(x=word, y=n)) +
    geom_col() +
    coord_flip() +
    # Title the plot "Non-Complaint Word Counts"
    ggtitle("Non-Complaint Word Counts")


custom_stop_words <- tribble(
    # Column names should match stop_words
    ~word, ~lexicon,
    # Add http, win, and t.co as custom stop words
    "http", "CUSTOM",
    "win", "CUSTOM",
    "t.co", "CUSTOM"
)

# Bind the custom stop words to stop_words
stop_words2 <- tidytext::stop_words %>% 
    bind_rows(custom_stop_words)


word_counts <- tidy_twitter %>% 
    filter(complaint_label == "Non-Complaint") %>% 
    count(word) %>% 
    # Keep terms that occur more than 100 times
    filter(n > 100) %>% 
    # Reorder word as an ordered factor by word counts
    mutate(word2 = fct_reorder(word, n))

# Plot the new word column with type factor
ggplot(word_counts, aes(x=word2, y=n)) +
    geom_col() +
    coord_flip() +
    ggtitle("Non-Complaint Word Counts")


word_counts <- tidy_twitter %>%
    # Count words by whether or not its a complaint
    count(word, complaint_label) %>%
    # Group by whether or not its a complaint
    group_by(complaint_label) %>%
    # Keep the top 20 words
    top_n(20, n) %>%
    # Ungroup before reordering word as a factor by the count
    ungroup() %>%
    mutate(word2 = fct_reorder(word, n))


# Include a color aesthetic tied to whether or not its a complaint
ggplot(word_counts, aes(x = word2, y = n, fill = complaint_label)) +
    # Don't include the lengend for the column plot
    geom_col(show.legend = FALSE) +
    # Facet by whether or not its a complaint and make the y-axis free
    facet_wrap(~ complaint_label, scales = "free_y") +
    # Flip the coordinates and add a title: "Twitter Word Counts"
    coord_flip() +
    ggtitle("Twitter Word Counts")


# Compute word counts and assign to word_counts
word_counts <- tidy_twitter %>% 
    count(word)

wordcloud::wordcloud(
    # Assign the word column to words
    words = word_counts$word, 
    # Assign the count column to freq
    freq = word_counts$n,
    max.words = 30
)


# Compute complaint word counts and assign to word_counts
word_counts <- tidy_twitter %>% 
    filter(complaint_label=="Complaint") %>% 
    count(word)

# Create a complaint word cloud of the top 50 terms, colored red
wordcloud::wordcloud(
    words = word_counts$word, 
    freq = word_counts$n, 
    max.words = 50, 
    colors = "red"
)

```
  
  
  
***
  
Chapter 3 - Sentiment Analysis  
  
Sentiment Dictionaries:  
  
* Can use the get_sentiments() function to access sentiment dictionaries  
	* get_sentiments("bing") %>% count(sentiment)  
    * get_sentiments("afinn") %>% summarize(min(score), max(score))  
    * get_sentiments("loughran") %>% count(sentiment) %>% mutate(sentiment2 = fct_reorder(sentiment, n))  # multiple types of sentiments, and words can have more than one  
  
Appending Dictionaries:  
  
* Can append sentiment dictionaries using inner_join()  
	* tidy_review %>% inner_join(get_sentiments("loughran"))  
* May want to visualize the most common words by sentiment  
	* sentiment_review2 <- sentiment_review %>% filter(sentiment %in% c("positive", "negative"))  
    * word_counts <- sentiment_review2 %>% count(word, sentiment) %>% group_by(sentiment) %>% top(10, n) %>% ungroup() %>% mutate(word2 = fct_reorder(word, n))  
    * ggplot(word_counts, aes(x=word2, y=n, fill=sentiment)) + geom_col(show.legend=FALSE) + facet_wrap(~ sentiment, scales = "free") + coord_flip() + labs(title = "myTitle", x = "Words")  
  
Improving Sentiment Analysis:  
  
* Can use the tidyr::spread() function to convert tidy data to wide and short  
	* tidy_review %>% inner_join(get_sentiments("bing")) %>% count(stars, sentiment) %>% spread(sentiment, n)  
  
Example code includes:  
```{r}

# Count the number of words associated with each sentiment in nrc
tidytext::get_sentiments("bing") %>% 
    count(sentiment) %>% 
    # Arrange the counts in descending order
    arrange(-n)


# Pull in the nrc dictionary, count the sentiments and reorder them by count
sentiment_counts <- tidytext::get_sentiments("bing") %>% 
    count(sentiment) %>% 
    mutate(sentiment2 = fct_reorder(sentiment, n))

# Visualize sentiment_counts using the new sentiment factor column
ggplot(sentiment_counts, aes(x=sentiment2, y=n)) +
    geom_col() +
    coord_flip() +
    # Change the title to "Sentiment Counts in NRC", x-axis to "Counts", and y-axis to "Sentiment"
    labs(title = "Sentiment Counts in NRC", x = "Counts", y = "Sentiment")


# Join tidy_twitter and the NRC sentiment dictionary
sentiment_twitter <- tidy_twitter %>% 
    inner_join(tidytext::get_sentiments("bing"))

# Count the sentiments in tidy_twitter
sentiment_twitter %>% 
    count(sentiment) %>% 
    # Arrange the sentiment counts in descending order
    arrange(-n)


word_counts <- tidy_twitter %>% 
    # Append the NRC dictionary and filter for positive, fear, and trust
    inner_join(tidytext::get_sentiments("bing")) %>% 
    filter(sentiment %in% c("positive", "fear", "trust")) %>%
    # Count by word and sentiment and keep the top 10 of each
    count(word, sentiment) %>% 
    group_by(sentiment) %>% 
    top_n(10, n) %>% 
    ungroup() %>% 
    # Create a factor called word2 that has each word ordered by the count
    mutate(word2 = fct_reorder(word, n))

# Create a bar plot out of the word counts colored by sentiment
ggplot(word_counts, aes(x=word2, y=n, fill=sentiment)) +
    geom_col(show.legend = FALSE) +
    # Create a separate facet for each sentiment with free axes
    facet_wrap(~ sentiment, scales = "free") +
    coord_flip() +
    # Title the plot "Sentiment Word Counts" with "Words" for the x-axis
    labs(title = "Sentiment Word Counts", x = "Words")


tidy_twitter %>% 
    # Append the NRC sentiment dictionary
    inner_join(tidytext::get_sentiments("bing")) %>% 
    # Count by complaint label and sentiment
    count(complaint_label, sentiment) %>% 
    # Spread the sentiment and count columns
    spread(sentiment, n)


tidy_twitter %>% 
    # Append the afinn sentiment dictionary
    inner_join(tidytext::get_sentiments("afinn")) %>% 
    # Group by both complaint label and whether or not the user is verified
    group_by(complaint_label, usr_verified) %>% 
    # Summarize the data with an aggregate_score = sum(score)
    summarize(aggregate_score = sum(value)) %>% 
    # Spread the complaint_label and aggregate_score columns
    spread(complaint_label, aggregate_score) %>% 
    mutate(overall_sentiment = Complaint + `Non-Complaint`)


sentiment_twitter <- tidy_twitter %>% 
    # Append the bing sentiment dictionary
    inner_join(tidytext::get_sentiments("bing")) %>% 
    # Count by complaint label and sentiment
    count(complaint_label, sentiment) %>% 
    # Spread the sentiment and count columns
    spread(sentiment, n) %>% 
    # Compute overall_sentiment = positive - negative
    mutate(overall_sentiment = positive - negative)

# Create a bar plot out of overall sentiment by complaint level, colored by a complaint label factor
ggplot(sentiment_twitter, aes(x=complaint_label, y=overall_sentiment, fill=as.factor(complaint_label))) +
    geom_col(show.legend = FALSE) +
    coord_flip() + 
    # Title the plot "Overall Sentiment by Complaint Type," with an "Airline Twitter Data" subtitle
    labs(title = "Overall Sentiment by Complaint Type", subtitle = "Airline Twitter Data")

```
  
  
***
  
Chapter 4 - Topic Modeling  
  
Latent Dirichlet Allocation:  
  
* Can assess the underlying topics of a document using LDA (Latent Dirichlet Alloaction)  
	* Collections of documents are known as a corpus, each with its own bag of words  
    * LDA finds patterns of words that tend to appear together (unsupervised learning)  
* Comparisons between clustering and topic modeling  
	* Clusters are based on distances, which are continuous (every item is a full member of a single segment)  
    * Topic modeling is based on word counts, which are discrete (every document is a partial member of every topic)  
  
Document Term Matrices:  
  
* The DTM (document-term-matrix) is the building block for the analysis  
	* DTM are especially useful for sparse matrices (matrices where 0 is the most common element)  
    * tidy_review %>% count(word, id) %>% cast_dtm(id, word, n)  # document, word, count  
* Can use as.matrix() to view a sparse matrix - generally should subset for easier viewing  
  
Running Topic Models:  
  
* Can use the topicmodels::LDA() to run topic modeling  
	* library(topicmodels)  
    * lda_out <- LDA(dtm_review, k=2, method="Gibbs", control=list(seed=42))  # k=2 will create 2 topics (hyper-parameter for tuning)  
    * glimpse(lda_out)  
    * lda_out %>% tidy(matrix="beta") %>% arrange(-beta)  
  
Interpreting Topics:  
  
* Key is to find distinct but non-repetitive topics  
* Takes user-expertise to examine the top words by topic and assign a label  
	* Adding a topic can dramatically change the solution  
    * Can change the number of topics until the segments appear distinct and non-reptitive  
  
Wrap Up:  
  
* Tokenizing text and removing stop words  
* Visualizing word counts  
* Conducting sentiment analysis  
* Running and interpreting topic models  
  
Example code includes:  
```{r cache=TRUE}

excl_words <- c("t.co", "http", "klm", "united", "americanair", "delta", "de", "southwestair", "usairways", 
                "jetblue", "british_airways", "amp", "deltaassist", "2", "ryanair", "4", "en", 
                "aircanada", "el", "emirates", "3", "virginamerica", "alaskaair", "1", "es", "vueling", 
                "britishairways", "se", "indonesiagaruda", "airfrancefr", "nedmex", "turkishairlines", 
                "airasia", "20", "flyfrontier", "tamairlines", "5", "30", "6", "10", "taylorcaniff",
                "dm", "frontiercare", "ik"
                )

# Cast the word counts by tweet into a DTM
dtm_twitter <- tidy_twitter %>% 
    filter(!(word %in% excl_words)) %>%
    count(word, tweet_id) %>% 
    tidytext::cast_dtm(tweet_id, word, n)


# Run an LDA with 2 topics and a Gibbs sampler
lda_out2 <- topicmodels::LDA(dtm_twitter, k = 2, method = "Gibbs", control = list(seed = 42))

# Tidy the matrix of word probabilities
lda_topics <- lda_out2 %>% 
    broom::tidy(matrix = "beta")


# Print the output from LDA run
lda_topics

# Start with the topics output from the LDA run
lda_topics %>% 
    # Arrange the topics by word probabilities in descending order
    arrange(-beta)


# Produce a grouped summary of the LDA output by topic
lda_topics %>% 
    group_by(topic) %>% 
    summarize(
        # Calculate the sum of the word probabilities
        sum = sum(beta),
        # Count the number of terms
        n = n()
        )


word_probs <- lda_topics %>%
    # Keep the top 10 highest word probabilities by topic
    group_by(topic) %>% 
    top_n(10, beta) %>% 
    ungroup() %>%
    # Create term2, a factor ordered by word probability
    mutate(term2 = fct_reorder(term, beta))

# Plot term2 and the word probabilities
ggplot(word_probs, aes(x=term2, y=beta)) +
    geom_col() +
    # Facet the bar plot by topic
    facet_wrap(~topic, scales = "free") +
    coord_flip()


# Start with the tidied Twitter data
tidy_twitter %>% 
    filter(!(word %in% excl_words)) %>%
    # Count each word used in each tweet
    count(word, tweet_id) %>% 
    # Use the word counts by tweet to create a DTM
    tidytext::cast_dtm(tweet_id, word, n)


# Assign the DTM to dtm_twitter
dtm_twitter <- tidy_twitter %>% 
    filter(!(word %in% excl_words)) %>%
    count(word, tweet_id) %>% 
    # Cast the word counts by tweet into a DTM
    tidytext::cast_dtm(tweet_id, word, n)

# Coerce dtm_twitter into a matrix called matrix_twitter
matrix_twitter <- as.matrix(dtm_twitter)

# Print rows 2 through 5 and sample columns
cn <- colnames(matrix_twitter)
matrix_twitter[2:5, match(c("fabulous", "industry", "village", "mistake", "time", "volunteer", "support"), cn)]


# Run an LDA with 2 topics and a Gibbs sampler
lda_out <- topicmodels::LDA(dtm_twitter, k = 2, method = "Gibbs", control = list(seed = 42))

# Glimpse the topic model output
glimpse(lda_out)

# Tidy the matrix of word probabilities
lda_topics <- lda_out %>% 
    broom::tidy(matrix="beta")

# Arrange the topics by word probabilities in descending order
lda_topics %>% 
    arrange(-beta)


# Run an LDA with 3 topics and a Gibbs sampler
lda_out2 <- topicmodels::LDA(dtm_twitter, k = 3, method = "Gibbs", control = list(seed = 42))

# Tidy the matrix of word probabilities
lda_topics2 <- lda_out2 %>% 
    broom::tidy(matrix = "beta")

# Arrange the topics by word probabilities in descending order
lda_topics2 %>% 
    arrange(-beta)


# Select the top 15 terms by topic and reorder term
word_probs2 <- lda_topics2 %>% 
    group_by(topic) %>% 
    top_n(15, beta) %>% 
    ungroup() %>%
    mutate(term2 = fct_reorder(term, beta))

# Plot word_probs2, color and facet based on topic
ggplot(word_probs2, aes(x=term2, y=beta, fill=as.factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()


# Run an LDA with 4 topics and a Gibbs sampler
lda_out3 <- topicmodels::LDA(dtm_twitter, k = 4, method = "Gibbs", control = list(seed = 42))

# Tidy the matrix of word probabilities
lda_topics3 <- lda_out3 %>% 
    broom::tidy(matrix = "beta")

# Arrange the topics by word probabilities in descending order
lda_topics3 %>% 
    arrange(-beta)


# Select the top 15 terms by topic and reorder term
word_probs3 <- lda_topics3 %>% 
    group_by(topic) %>% 
    top_n(15, beta) %>% 
    ungroup() %>%
    mutate(term2 = fct_reorder(term, beta))

# Plot word_probs3, color and facet based on topic
ggplot(word_probs3, aes(x=term2, y=beta, fill=as.factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()

```
  
  
  
***
  
### _Data Science for Managers_  
  
Chapter 1 - Introduction to Data Science  
  
What is Data Science?  
  
* Data science is a set of methodologies for drawing meaningful conclusions from available data  
	* Describe the current state (reporting, etc.)  
    * Detect anomlaous events (fraud detection, defects, etc.)  
    * Diagnosis of causes of events and behaviors  
    * Predict future events and outcomes  
* Data science workflows typically include  
	* Data collection  
    * Exploration and visualization  
    * Experimentation and prediction  
  
Applications of Data Science:  
  
* Example of a fraud detection process - likelihood that a transaction is fake  
	* Need many examples and a label describing each transaction as fraudulent or valid  
* Begin with a well-defined questions such as "what is the probability that a specific transaction is fraudulent?"  
* Example of building a smart watch to auto-detect activities - acclerometer  
* The Internet of Things (IoT) is often combined with Data Science  
	* Smart watches  
    * Electronic tool collection  
    * Etc.  
* Image recognition (humans) for self-driving cars  
	* Deep learning uses multiple layers of neurons (mini-layers) to draw conclusions  
    * Deep learning needs large volumes of data and draws conclusions that would not be possible with many other types of models  
  
Building a Data Science Team:  
  
* Data engineers, data analysts, and machine learning scientist  
	* Data engineers build the infrastructure - SQL, Java, Scala, Python  
    * Data analysts desribe the present using data (dashboards, hypothesis testing) - SQL, Excel, Tableau/BI Tools  
    * Machine learning scientists extrapolate from what is already known (test-train, prediction, image classification, etc.) - Python, R  
* Programming languages are easier to learn than spoken languages - often can read languages even if you cannot write them  
* Three main ways to structure teams - isolated, embedded, hybrid  
	* Isolated - standalone (good for training new team members and rotating projects)  
    * Embedded - part of squads of engineers and managers (good for gaining SME on a business area)  
    * Hybrid - Embedded, but with a layer that is also specific to the data science track  
  
***
  
Chapter 2 - Data Sources and Risks  
  
Data Sources and Risks:  
  
* Many sources of data - web, transactions, logistics, customers, etc.  
* Web data can be useful - user, web page, element, time stamp, etc.  
* Personally Identifiable Information (PII) is name, e-mail, address, or anything else that could be ties back to a real human  
	* PII should be trated very carefully, perhaps through a userID rather than tracking by e-mail address  
    * Data is "pseudonymized" if PII could be attained using joins of multiple tables  
    * Destroying or deleting the users data would then fully anonymize the data  
* The GDPR is the "General Data Protection Regulation" inside the EU, and gives individuals access to their personal data  
  
Solicited Data:  
  
* Solicited data can be used to create marketing collateral - e.g., surveys, reviews, questionnaires, focus groups, etc.  
* The NPS (Net Promoter Score) is a common form of solicted data  
* Solicted data can be either quantitative or qualitative  
	* Qualitative data (small-scale) can be helpful for generating hypotheses  
    * Quantitative data (larger-scale) can be helpful for testing hypothesis and making decisions  
* Revealed and stated preferences can be different  
* Best practices are to be as specific as possible when asking questions  
	* Avoid loaded language - be as objective as possible  
    * Calibrate by comparing to competitor or company offerings  
    * Limit the number of questions ot those that will help with taking a decisive action  
  
Collecting Additional Data:  
  
* Data from external sources can include API, Public Records, Mechanical Turk  
* The API is an "Application Programming Interface", and can be a good way to request/collect data from the internet  
	* In the US, data.gov can be a valuable source  
    * In the EU, data.europa.eu can be a valuable source  
* A good image classification process requires a large training set with labels  
	* Mechanical Turk (MTurk) can be very helpful - ask humans to do some of the labelling, similar to a CAPTCHA (though could be a paid process)  
    * AWS Mturk is one resource for organizing a Mechanical Turk process  
  
Data Storage and Retrieval:  
  
* Cloud storage is paying a third party to house data - AWS, Google Cloud, etc.  
* Data can be unstructured or structured - often stored in a document database  
	* Data querying can be done in many ways - by date, by user, etc.  
    * Each type of database has its own language - NoSQL (not only SQL), SQL, etc.  
* Building a corporate database depends on picking a location, deciding on the language/design (document vs. relational), system for referencing and retrieving data  
  
***
  
Chapter 3 - Analysis and Visualization  
  
Dashboards:  
  
* A dashboard is a set of metrics that updates on a schedule - real-time, daily, weekly, etc.  
	* Can track distributions over time, distributions by category, time series trends, etc.  
* Display text may be added to dashboards for a splash of qualitative content  
* Dashboards can be built with spreadsheet tools or BI tools  
  
Ad hoc analysis:  
  
* An ad hoc request is a request for data or analysis that does not need to be repeated on a regular basis  
* Example of a one-time request to understand the impact of an advertising campaign  
	* Requires specificity, context, priority, importance, etc.  
    * Ticket systems are sometimes used to manage ad hoc requests  
  
A/B Testing:  
  
* A/B Testing is a type of experiment for de-risking potential changes to the business  
	* Example of randomly dividing the audience in to two groups and then showing each a different web page title  
    * Pick Metric -> Determine Sample Size -> Conduct Experiment -> Run Assessment  
  
***
  
Chapter 4 - Prediction  
  
Supervised Machine Learning:  
  
* Machine learning is the process of making predictions from data  
	* Supervised machine learning has both features and labels  
* Example of a subscription business and a goal of determining whether the customer will churn  
	* Historical customer data available, with labels as to whether they have churned  
* Can then evaluate the model based on a hold-out sample (test-train methodology)  
  
Clustering:  
  
* Clustering can help with pattern recognition in messy datasets  
* Example of working for an airline, and trying to segment the customers  
	* Number of flights, % international, % business class, mean days in advance booked  
  
Special Topics in Machine Learning:  
  
* Seasonality can be weekly, monthly, yearly, summer, etc.  
* NLP (Natural Language Processing) is an example of a machine learning topic  
	* Features can be word counts  
    * Word count challenges can include negation and synonyms  
  
Deep Learning and Explainable AI:  
  
* Deep learning (neural networks) is a special type of machine learning that can solve more complex problems and that requires large quantities of input data  
	* Generally lack of explainability, though with good predictive power  
    * Explainable AI (artificial intelligence) would have factors that can be explained  
* Explainable AI will include both predictions AND explanations (factors that drive the predictions)  
  
***
  

### _Introduction to R for Finance_  
  
Chapter 1 - The Basics  
  
Introduction:  
  
* Basics of R and common structures for storing data, as applied to finance  
* Variables or objects are containers for storing information, created using the assignment vector ( <- )  
* R Scripts are text holding lines of code (can be saved and re-loaded later as needed)  
  
Financial returns:  
  
* The return, r, is defined such that (1 + r) * OV = NV where OV is the original value, NV is the new value, and the time period in which OV became NV is one unit of r  
* Returns are multiplicative, so the full return for two periods will be (1 + r1) * (1 + r2) - 1  
  
Basic data types:  
  
* Numeric data - decimal data  
* Integer data - whole numbers without the decimal place (5L is the integer 5, while 5 is the numeric 5.0)  
* Character (string) data are text (entered in quotes)  
* Booleans (logicals) are TRUE, FALSE, NA  
* Can get the variable type by using the function class()  
  
Example code includes:  
```{r}

# Addition!
3 + 5

# Subtraction!
6 - 4


# Addition 
2 + 2

# Subtraction
6 - 4

# Multiplication
3 * 4

# Division
4 / 2

# Exponentiation
2^4

# Modulo
7 %% 3


# Assign 200 to savings
savings <- 200

# Print the value of savings to the console
savings


# Assign 100 to my_money
my_money <- 100

# Assign 200 to dans_money
dans_money <- 200

# Add my_money and dans_money
my_money + dans_money

# Add my_money and dans_money again, save the result to our_money
our_money <- my_money + dans_money


# Variables for starting_cash and 5% return during January
starting_cash <- 200
jan_ret <- 5
jan_mult <- 1 + (jan_ret / 100)

# How much money do you have at the end of January?
post_jan_cash <- starting_cash * jan_mult

# Print post_jan_cash
post_jan_cash

# January 10% return multiplier
jan_ret_10 <- 10
jan_mult_10 <- 1 + (jan_ret_10 / 100)

# How much money do you have at the end of January now?
post_jan_cash_10 <- starting_cash * jan_mult_10

# Print post_jan_cash_10
post_jan_cash_10


# Starting cash and returns 
starting_cash <- 200
jan_ret <- 4
feb_ret <- 5

# Multipliers
jan_mult <- 1 + jan_ret/100
feb_mult <- 1 + feb_ret/100

# Total cash at the end of the two months
total_cash <- starting_cash * jan_mult * feb_mult

# Print total_cash
total_cash


# Apple's stock price is a numeric
apple_stock <- 150.45

# Bond credit ratings are characters
credit_rating <- "AAA"

# You like the stock market. TRUE or FALSE?
my_answer <- TRUE

# Print my_answer
my_answer

```
  
  
  
***
  
Chapter 2 - Vectors and Matrices  
  
What is a vector?  
  
* One or more pieces of data of the same class stored as a single variable (or a list containing data of potentially different classes)  
* If a vector is created using different classes of input data, then all of the data will be coerced to the "highest" data type  
	* logical < integer < numeric < character  # so c(TRUE, 2) will become c(1, 2) while c(1L, "dog") will become c("1", "dog")  
  
Vector manipulation:  
  
* Calculations in R are vectorized  
* R will also recycle, using the smaller vector multiple times for the element-wise calculations  
  
Matrix - a 2D vector:  
  
* Default is to fill columns first, unless byrow=TRUE has been specified  
* A matrix must have all data of a single type; data will be coerced as needed to achieve this  
* The cbind() and rvind() functions allow for binding by columns or rows  
* Can run cor() on a matrix to get back the correlations of every column to every other column  
  
Example code includes:  
```{r}

# Another numeric vector
ibm_stock <- c(159.82, 160.02, 159.84)

# Another character vector
finance <- c("stocks", "bonds", "investments")

# A logical vector
logic <- c(TRUE, FALSE, TRUE)


# Vectors of 12 months of returns, and month names
ret <- c(5, 2, 3, 7, 8, 3, 5, 9, 1, 4, 6, 3)
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

# Add names to ret
names(ret) <- months

# Print out ret to see the new names!
ret


# Vectors of 12 months of returns, and month names
ret <- c(5, 2, 3, 7, 8, 3, 5, 9, 1, 4, 6, 3)
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

# Add names to ret
names(ret) <- months

# Print out ret to see the new names!
ret


# Weights and returns
micr_ret <- 7
sony_ret <- 9
micr_weight <- .2
sony_weight <- .8

# Portfolio return
portf_ret <- micr_ret * micr_weight + sony_ret * sony_weight


# Weights, returns, and company names
ret <- c(7, 9)
weight <- c(.2, .8)
companies <- c("Microsoft", "Sony")

# Assign company names to your vectors
names(ret) <- companies
names(weight) <- companies

# Multiply the returns and weights together 
ret_X_weight <- ret*weight

# Print ret_X_weight
ret_X_weight

# Sum to get the total portfolio return
portf_ret <- sum(ret_X_weight)

# Print portf_ret
portf_ret


# Print ret
ret

# Assign 1/3 to weight
weight <- 1/3

# Create ret_X_weight
ret_X_weight <- ret * weight

# Calculate your portfolio return
portf_ret <- sum(ret_X_weight)

# Vector of length 3 * Vector of length 2?
ret * c(.2, .6)


# Vectors of 12 months of returns, and month names
ret <- c(5, 2, 3, 7, 8, 3, 5, 9, 1, 4, 6, 3)
months <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
names(ret) <- months

# First 6 months of returns
ret[1:6]

# Just March and May
ret[c("Mar", "May")]

# Omit the first month of returns
ret[-1]


# A vector of 9 numbers
my_vector <- c(1, 2, 3, 4, 5, 6, 7, 8, 9)

# 3x3 matrix
my_matrix <- matrix(data = my_vector, nrow = 3, ncol = 3)

# Print my_matrix
my_matrix

# Filling across using byrow = TRUE
matrix(data = c(2, 3, 4, 5), nrow = 2, ncol = 2, byrow = TRUE)


prices <- c(109.49, 109.9, 109.11, 109.95, 111.03, 112.12, 113.95, 113.3, 115.19, 115.19, 115.82, 115.97, 116.64, 116.95, 117.06, 116.29, 116.52, 117.26, 116.76, 116.73, 115.82, 159.82, 160.02, 159.84, 160.35, 164.79, 165.36, 166.52, 165.5, 168.29, 168.51, 168.02, 166.73, 166.68, 167.6, 167.33, 167.06, 166.71, 167.14, 166.19, 166.6, 165.99, 59.2, 59.25, 60.22, 59.95, 61.37, 61.01, 61.97, 62.17, 62.98, 62.68, 62.58, 62.3, 63.62, 63.54, 63.54, 63.55, 63.24, 63.28, 62.99, 62.9, 62.14)
apple <- prices[1:21]
ibm <- prices[22:42]
micr <- prices[43:63]


# cbind the vectors together
cbind_stocks <- cbind(apple, ibm, micr)

# Print cbind_stocks
cbind_stocks

# rbind the vectors together
rbind_stocks <- rbind(apple, ibm, micr)

# Print rbind_stocks
rbind_stocks


apple_micr_matrix <- cbind(apple, micr)

# View the data
apple_micr_matrix

# Scatter plot of Microsoft vs Apple
plot(apple_micr_matrix)


# Correlation of Apple and IBM
cor(apple, ibm)

# stock matrix
stocks <- cbind(apple, micr, ibm)

# cor() of all three
# cor(apple, micr, ibm)
cor(stocks)


# Third row
stocks[3, ]

# Fourth and fifth row of the ibm column
stocks[4:5, "ibm"]

# apple and micr columns
stocks[, c("apple", "micr")]

```
  
  
  
***
  
Chapter 3 - Data Frames  
  
What is a data frame?  
  
* Multiple data types can be contained in the same data frame  
* Simplifies plotting, subsetting, and the like  
  
Data frame manipulation:  
  
* Can subset and slice the data, though selecting just a single element from a column or row will create a vector  
	* Can use drop=FALSE option to override and maintain as a data frame  
* Can also use the subset() function, such as subset(myDF, myCond)  
  
Present value:  
  
* Future value = (1 + r) * Present value, assuming r is the interest/discount rate for the time period of present vs. future  
  
Example code includes:  
```{r}

# Variables
company <- c("A", "A", "A", "B", "B", "B", "B")
cash_flow <- c(1000, 4000, 550, 1500, 1100, 750, 6000)
year <- c(1, 3, 4, 1, 2, 4, 5)

# Data frame
cash <- data.frame(company, cash_flow, year, stringsAsFactors = FALSE)

# Print cash
cash


# Call head() for the first 4 rows
head(cash, 4)

# Call tail() for the last 3 rows
tail(cash, 3)

# Call str()
str(cash)


# Fix your column names
names(cash) <- c("company", "cash_flow", "year")

# Print out the column names of cash
colnames(cash)


cash <- data.frame(company=c("A", "A", "A", "B", "B", "B", "B"), cash_flow=c(1000, 4000, 550, 1500, 1100, 750, 6000), year=c(1, 3, 4, 1, 2, 4, 5), stringsAsFactors=FALSE)

# Third row, second column
cash[3, 2]

# Fifth row of the "year" column
cash[5, "year"]


# Select the year column
cash$year

# Select the cash_flow column and multiply by 2
cash$cash_flow * 2

# Delete the company column
cash$company <- NULL

# Print cash again
cash


cash$company <- c("A", "A", "A", "B", "B", "B", "B")
cash

# Rows about company B
subset(cash, company == "B")

# Rows with cash flows due in 1 year
subset(cash, year == 1)


# Quarter cash flow scenario
cash$quarter_cash <- 0.25 * cash$cash_flow

# Double year scenario
cash$double_year <- cash$year * 2


# Present value of $4000, in 3 years, at 5%
present_value_4k <- 4000 * (1.05 ** -3)

# Present value of all cash flows
cash$present_value <- cash$cash_flow * (1.05 ** -cash$year)

# Print out cash
cash


# Total present value of cash
total_pv <- sum(cash$present_value)

# Company B information
cash_B <- subset(cash, company == "B")

# Total present value of cash_B
total_pv_B <- sum(cash_B$present_value)

```
  
  
  
***
  
Chapter 4 - Factors  
  
What is a factor?  
  
* Categorical variables (factors) can be useful when there are multiple groups for consideration in an analysis  
* Under the hood, factor variables are stored as integers, even though they are displayed using their specified character names  
* To create a factor, use factor(myVector)  
* The cut() function can be used to create the categorical groups  
	* buckets <- seq(0, 50, by=10)  
    * cut(myVector, breaks=buckets)  
    * By default, the left-side of each bucket is exclusive, so (0, 10] will not contain 0 but will contain 10  
  
Ordering and subsetting factors:  
  
* By default, factors are ordered alphabetically, which does not work well for ordinal data (e.g., High-Medium-Low)  
* Can create a default ordered factor using ordered() rather than factor()  
* Can create a custom ordered factor using factor(x, ordered=TRUE, levels=c())  
* By default, unused levels of a factor are not dropped during subsetting; can override using drop=TRUE  
  
Example code includes:  
```{r}

# credit_rating character vector
credit_rating <- c("BB", "AAA", "AA", "CCC", "AA", "AAA", "B", "BB")

# Create a factor from credit_rating
credit_factor <- factor(credit_rating)

# Print out your new factor
credit_factor

# Call str() on credit_rating
str(credit_rating)

# Call str() on credit_factor
str(credit_factor)


# Identify unique levels
levels(credit_factor)

# Rename the levels of credit_factor
levels(credit_factor) <- c("2A", "3A", "1B", "2B", "3C")

# Print credit_factor
credit_factor


# Summarize the character vector, credit_rating
summary(credit_rating)

# Summarize the factor, credit_factor
summary(credit_factor)


# Visualize your factor!
plot(credit_factor)


AAA_rank <- c(31, 48, 100, 53, 85, 73, 62, 74, 42, 38, 97, 61, 48, 86, 44, 9, 43, 18, 62, 38, 23, 37, 54, 80, 78, 93, 47, 100, 22, 22, 18, 26, 81, 17, 98, 4, 83, 5, 6, 52, 29, 44, 50, 2, 25, 19, 15, 42, 30, 27)

# Create 4 buckets for AAA_rank using cut()
AAA_factor <- cut(x = AAA_rank, breaks = c(0, 25, 50, 75, 100))

# Rename the levels 
levels(AAA_factor) <- c("low", "medium", "high", "very_high")

# Print AAA_factor
AAA_factor

# Plot AAA_factor
plot(AAA_factor)


# Use unique() to find unique words
unique(credit_rating)

# Create an ordered factor
credit_factor_ordered <- factor(credit_rating, ordered = TRUE, levels = c("AAA", "AA", "BB", "B", "CCC"))

# Plot credit_factor_ordered
plot(credit_factor_ordered)


# Remove the A bonds at positions 3 and 7. Don't drop the A level.
keep_level <- credit_factor[-c(3, 7)]

# Plot keep_level
plot(keep_level)

# Remove the A bonds at positions 3 and 7. Drop the A level.
drop_level <- credit_factor[-c(3, 7), drop=TRUE]

# Plot drop_level
plot(drop_level)


# Variables
credit_rating <- c("AAA", "A", "BB")
bond_owners <- c("Dan", "Tom", "Joe")

# Create the data frame of character vectors, bonds
bonds <- data.frame(credit_rating, bond_owners, stringsAsFactors=FALSE)

# Use str() on bonds
str(bonds)

# Create a factor column in bonds called credit_factor from credit_rating
bonds$credit_factor <- factor(bonds$credit_rating, ordered = TRUE, levels = c("AAA", "A", "BB"))

# Use str() on bonds again
str(bonds)

```
  
  
  
***
  
Chapter 5 - Lists  
  
What is a list?  
  
* Lists are super data structures that allow for grouping elements of varying classes, lengths, etc.  
* Can be created using list(name1=data1, name2=data2, …)  
	* The single brackets will subset the outer portion of the list, which will always return another list  
    * The double brackets will return the data inside the subsetted list  
  
List creating functions:  
  
* The split() function will create a list from a data frame - split(myDF, mySplitVar)  
	* The levels of mySplitVar will  be the names used inside the list  
* Can use unsplit() to take the names list and put it back together - unsplit(myList, mySplitVar)  
* One of the instanciations of Split-Apply-Combine  
* Attributes help to define some of the features of a data element (row.names, dim, and the like)  
  
Wrap up:  
  
* Many other courses in the sequence  
  
Example code includes:  
```{r}

# List components
name <- "Apple and IBM"
apple <- c(109.49, 109.90, 109.11, 109.95, 111.03)
ibm <- c(159.82, 160.02, 159.84, 160.35, 164.79)
cor_matrix <- cor(cbind(apple, ibm))

# Create a list
portfolio <- list(name, apple, ibm, cor_matrix)

# View your first list
portfolio


# Add names to your portfolio
names(portfolio) <- c("portfolio_name", "apple", "ibm", "correlation")

# Print portfolio
portfolio


# Second and third elements of portfolio
portfolio[c(2, 3)]

# Use $ to get the correlation data
portfolio$correlation


# Add weight: 20% Apple, 80% IBM
portfolio$weight <- c(apple = 0.2, ibm = 0.8)

# Print portfolio
portfolio

# Change the weight variable: 30% Apple, 70% IBM
portfolio$weight <- c(apple = 0.3, ibm = 0.7)

# Print portfolio to see the changes
portfolio


# Take a look at portfolio
portfolio

# Remove the microsoft stock prices from your portfolio
portfolio$microsoft <- NULL


# Define grouping from year
grouping <- cash$year

# Split cash on your new grouping
split_cash <- split(cash, grouping)

# Look at your split_cash list
split_cash

# Unsplit split_cash to get the original data back.
original_cash <- unsplit(split_cash, grouping)

# Print original_cash
original_cash


# Print split_cash
split_cash

# Print the cash_flow column of B in split_cash
split_cash$B$cash_flow

# Set the cash_flow column of company A in split_cash to 0
split_cash$A$cash_flow <- 0

# Use the grouping to unsplit split_cash
cash_no_A <- unsplit(split_cash, grouping)

# Print cash_no_A
cash_no_A


# my_matrix and my_factor
my_matrix <- matrix(c(1,2,3,4,5,6), nrow = 2, ncol = 3)
rownames(my_matrix) <- c("Row1", "Row2")
colnames(my_matrix) <- c("Col1", "Col2", "Col3")

my_factor <- factor(c("A", "A", "B"), ordered = T, levels = c("A", "B"))

# attributes of my_matrix
attributes(my_matrix)

# Just the dim attribute of my_matrix
attr(my_matrix, which="dim")

# attributes of my_factor
attributes(my_factor)

```
  
  
  
***
  
### _Intermediate R for Finance_  
  
Chapter 1 - Dates  
  
Introduction to dates in R:  
  
* Sys.Date() will return the current date, which will be of class "Date"  
	* Can convert to Date format using as.Date()  
* The POSIX class (POSIXct or POSIXlt) allows for holding date-times, while the Date class allows for holding dates  
  
Date formats and extractor functions:  
  
* The standard date formats in R include yyyy-mm-dd and yyyy/mm/dd - any other date format must be specified explicitly  
	* as.Date(myChar, format=)  
* Can use extractors such as weekdays() to get formatting date back from a date vector  
  
Example code includes:  
```{r cache=TRUE}

# What is the current date?
# Sys.Date()

# What is the current date and time?
# Sys.time()

# Create the variable today
today <- Sys.Date()

# Confirm the class of today
class(today)


# Create crash
crash <- as.Date("2008-09-29")

# Print crash
crash

# crash as a numeric
as.numeric(crash)

# Current time as a numeric
as.numeric(Sys.time())

# Incorrect date format
# as.Date("09/29/2008")


# Create dates from "2017-02-05" to "2017-02-08" inclusive.
dates <- c("2017-02-05", "2017-02-06", "2017-02-07", "2017-02-08")

# Add names to dates
names(dates) <- c("Sunday", "Monday", "Tuesday", "Wednesday")

# Subset dates to only return the date for Monday
dates["Monday"]


# "08,30,30"
as.Date("08,30,1930", format = "%m,%d,%Y")

# "Aug 30,1930"
as.Date("Aug 30,1930", format = "%b %d,%Y")

# "30aug1930"
as.Date("30aug1930", format = "%d%b%Y")


# char_dates
char_dates <- c("1jan17", "2jan17", "3jan17", "4jan17", "5jan17")

# Create dates using as.Date() and the correct format 
dates <- as.Date(char_dates, format="%d%b%y")

# Use format() to go from "2017-01-04" -> "Jan 04, 17"
format(dates, format="%b %d, %y")

# Use format() to go from "2017-01-04" -> "01,04,2017"
format(dates, format="%m,%d,%Y")


# Dates
dates <- as.Date(c("2017-01-01", "2017-01-02", "2017-01-03"))

# Create the origin
origin <- as.Date("1970-01-01")

# Use as.numeric() on dates
as.numeric(dates)

# Find the difference between dates and origin
dates-origin


# dates
dates <- as.Date(c("2017-01-02", "2017-05-03", "2017-08-04", "2017-10-17"))

# Extract the months
months(dates)

# Extract the quarters
quarters(dates)

# dates2
dates2 <- as.Date(c("2017-01-02", "2017-01-03", "2017-01-04", "2017-01-05"))

# Assign the weekdays() of dates2 as the names()
names(dates2) <- weekdays(dates2)

# Print dates2
dates2

```
  
  
  
***
  
Chapter 2 - If Statements and Operators  
  
Relational Operators:  
  
* Can use the standard operators such as <, >, <=, >=, ==, !=  
  
Logical Operators:  

* Helpful for multiple comparisons - & (intersection), | (unions), ! (negate)  
* Frequently used inside a subset() operation to extract records of interest  
  
If statements:  
  
* Can use the if-then process to direct R on the code to be run  
    * if (condition) { codeIfTRUE } else if (nextCondition) { elseCode } else { allElseCode }  
  
Example code includes:  
```{r}

# Stock prices
apple <- 48.99
micr <- 77.93

# Apple vs Microsoft
apple > micr

# Not equals
apple != micr

# Dates - today and tomorrow
today <- as.Date(Sys.Date())
tomorrow <- as.Date(Sys.Date() + 1)

# Today vs Tomorrow
tomorrow < today


stocks <- data.frame(ibm=c(171, 171, 176, 178), panera=c(217, 261, 214, 212),
                     date=as.Date(c("2017-01-20", "2017-01-23", "2017-01-24", "2017-01-25"))
                     )

# Print stocks
stocks

# IBM range
stocks$ibm_buy <- (stocks$ibm < 175)

# Panera range
stocks$panera_sell <- (stocks$panera > 213)

# IBM vs Panera
stocks$ibm_vs_panera <- (stocks$ibm > stocks$panera)

# Print stocks
stocks


# IBM buy range
stocks$ibm_buy_range <- (stocks$ibm > 171) & (stocks$ibm < 176)

# Panera spikes
stocks$panera_spike <- (stocks$panera < 213.2) | (stocks$panera > 216.5)

# Date range
stocks$good_dates <- (stocks$date > as.Date("2017-01-21")) & (stocks$date < as.Date("2017-01-25"))

# Print stocks
stocks


# IBM range
!(stocks$ibm > 176)

# Missing data
missing <- c(24.5, 25.7, NA, 28, 28.6, NA)

# Is missing?
is.na(missing)

# Not missing?
!is.na(missing)


# Panera range
subset(stocks, panera > 216)

# Specific date
subset(stocks, date == as.Date("2017-01-23"))

# IBM and Panera joint range
subset(stocks, ibm < 175 & panera < 216.5)


# View stocks
stocks

# Weekday investigation
stocks$weekday <- weekdays(stocks$date)

# View stocks again
stocks

# Remove missing data
stocks_no_NA <- subset(stocks, !is.na(apple))

# Apple and Microsoft joint range
subset(stocks_no_NA, apple > 117 | micr > 63)


# micr
micr <- 48.55

# Fill in the blanks
if( micr < 55 ) {
    print("Buy!")
}


# micr
micr <- 57.44

# Fill in the blanks
if( micr < 55 ) {
    print("Buy!")
} else {
    print("Do nothing!")
}


# micr
micr <- 105.67

# Fill in the blanks
if( micr < 55 ) {
    print("Buy!")
} else if( micr >= 55 & micr < 75 ){
    print("Do nothing!")
} else { 
    print("Sell!")
}


# micr
micr <- 105.67
shares <- 1

# Fill in the blanks
if( micr < 55 ) {
    print("Buy!")
} else if( micr >= 55 & micr < 75 ) {
    print("Do nothing!")
} else { 
    if( shares >= 1 ) {
        print("Sell!")
    } else {
        print("Not enough shares to sell!")
    }
}


stocks <- data.frame(apple=c(109.49, 109.9, 109.11, 109.95, 111.03, 112.12, 113.95, 113.3, 115.19, 115.19, 115.82, 115.97, 116.64, 116.95, 117.06, 116.29, 116.52, 117.26, 116.76, 116.73, 115.82), 
                     micr=c(59.2, 59.25, 60.22, 59.95, 61.37, 61.01, 61.97, 62.17, 62.98, 62.68, 62.58, 62.3, 63.62, 63.54, 63.54, 63.55, 63.24, 63.28, 62.99, 62.9, 62.14), 
                     date=as.Date("2016-12-01") + c(7*rep(0:2, each=5) + c(0, 1, 4, 5, 6), 21, 22, 26, 27, 28, 29)
                     )

# Microsoft test
stocks$micr_buy <- ifelse(test = (stocks$micr > 60 & stocks$micr < 62), yes = 1, no = 0)

# Apple test
stocks$apple_date <- ifelse(test = (stocks$apple > 117), yes = stocks$date, no = NA)

# Print stocks
stocks

# Change the class() of apple_date.
class(stocks$apple_date) <- "Date"

# Print stocks again
stocks

```
  
  
  
***
  
Chapter 3 - Loops  
  
Repeat loops:  
  
* The fully repeating loop (infinte loop) is called with repeat { }  
	* A break statement helps inside - repeat{ if() break }  
  
While loops:  
  
* The while loop is an example of a repeated if statement  
	* while (myCond) { myActions }  
  
For loops:  
  
* for (myInd in myObj) { doStuff }  
  
Example code includes:  
```{r}

# Stock price
stock_price <- 126.34

repeat {
    # New stock price
    stock_price <- stock_price * runif(1, .985, 1.01)
    print(stock_price)
  
    # Check
    if(stock_price < 125) {
        print("Stock price is below 125! Buy it while it's cheap!")
        break
    }
}


# Stock price
stock_price <- 67.55

repeat {
    # New stock price
    stock_price <- stock_price * .995
  
    # Check
    if(stock_price < 66) {
        print("Stock price is below 66! Buy it while it's cheap!")
        break
    }
  
    print(stock_price)
  
}


# Initial debt
debt <- 5000

# While loop to pay off your debt
while (debt > 0) {
    debt <- debt - 500
    print(paste("Debt remaining", debt))
}


debt <- 5000    # initial debt
i <- 0          # x axis counter
x_axis <- i     # x axis
y_axis <- debt  # y axis

# Initial plot
plot(x_axis, y_axis, xlim = c(0,10), ylim = c(0,5000))

# Graph your debt
while (debt > 0) {

    # Updating variables
    debt <- debt - 500
    i <- i + 1
    x_axis <- c(x_axis, i)
    y_axis <- c(y_axis, debt)
  
    # Next plot
    plot(x_axis, y_axis, xlim = c(0,10), ylim = c(0,5000))
}


# debt and cash
debt <- 5000
cash <- 4000

# Pay off your debt...if you can!
while (debt > 0) {
    debt <- debt - 500
    cash <- cash - 500
    print(paste("Debt remaining:", debt, "and Cash remaining:", cash))

    if (cash == 0) {
        print("You ran out of cash!")
        break
    }
}


# Sequence
seq <- c(1:10)

# Print loop
for (value in seq) {
    print(value)
}

# A sum variable
sum <- 0

# Sum loop
for (value in seq) {
    sum <- sum + value
    print(sum)
}


stock <- data.frame(apple=c(109.49, 109.9, 109.11, 109.95, 111.03, 112.12, 113.95, 113.3, 115.19, 115.19, 115.82, 115.97, 116.64, 116.95, 117.06, 116.29, 116.52, 117.26, 116.76, 116.73, 115.82), 
                     micr=c(59.2, 59.25, 60.22, 59.95, 61.37, 61.01, 61.97, 62.17, 62.98, 62.68, 62.58, 62.3, 63.62, 63.54, 63.54, 63.55, 63.24, 63.28, 62.99, 62.9, 62.14), 
                     date=as.Date("2016-12-01") + c(7*rep(0:2, each=5) + c(0, 1, 4, 5, 6), 21, 22, 26, 27, 28, 29)
                     )

# Loop over stock rows
for (row in 1:nrow(stock)) {
    price <- stock[row, "apple"]
    date  <- stock[row, "date"]

    if(price > 116) {
        print(paste("On", date, 
                    "the stock price was", price))
    } else {
        print(paste("The date:", date, 
                    "is not an important day!"))
    }
}


# Print out corr
corr <- cor(stock[, c("apple", "micr")])
corr

# Create a nested loop
for(row in 1:nrow(corr)) {
    for(col in 1:ncol(corr)) {
        print(paste(colnames(corr)[col], "and", rownames(corr)[row], 
                    "have a correlation of", corr[row,col]))
    }
}


# Print apple
# apple

# Loop through apple. Next if NA. Break if above 117.
# for (value in apple) {
#     if(is.na(value)) {
#         print("Skipping NA")
#         next
#     }
#     
#     if(value > 117) {
#         print("Time to sell!")
#         break
#     } else {
#         print("Nothing to do here!")
#     }
# }

```
  
  
  
***
  
Chapter 4 - Functions  
  
What are functions?  
  
* Lines of code written to do a few key things - arguments, body, returns  
* Function documentation can be fund using ?myFunc  
  
Writing functions:  
  
* myFunc <- function(myArgs) { myCode }  
* Can use return() to return the value, or let the function default to returning the last object it has created (calculation that it has made)  
  
Packages:  
  
* Packages have been written by many people through an open-source process, (typically stored on CRAN)  
* Packages can be installed using install.packages("myPackage")  
* Packages can be loaded using library(myPackage)  
  
Example code includes:  
```{r}

# subset help
# ?subset

# Sys.time help
# ?Sys.time


# Round 5.4
round(5.4)

# Round 5.4 with 1 decimal place
round(5.4, 1)

# numbers
numbers <- c(.002623, pi, 812.33345)

# Round numbers to 3 decimal places
round(numbers, 3)


apple <- c(109.49, 109.9, 109.11, 109.95, 111.03, 112.12, 113.95, 113.3, 115.19, 115.19, 115.82, 115.97, 116.64, 116.95, 117.06, 116.29, 116.52, 117.26, 116.76, 116.73, 115.82)
ibm <- c(159.82, 160.02, 159.84, 160.35, 164.79, 165.36, 166.52, 165.5, 168.29, 168.51, 168.02, 166.73, 166.68, 167.6, 167.33, 167.06, 166.71, 167.14, 166.19, 166.6, 165.99)
micr <- c(59.2, 59.25, 60.22, 59.95, 61.37, 61.01, 61.97, 62.17, 62.98, 62.68, 62.58, 62.3, 63.62, 63.54, 63.54, 63.55, 63.24, 63.28, 62.99, 62.9, 62.14)

# cbind() the stocks
stocks <- cbind(apple, ibm, micr)

# cor() to create the correlation matrix
cor(stocks)

# All at once! Nest cbind() inside of cor()
cor(cbind(apple, ibm, micr))


# Percent to decimal function
percent_to_decimal <- function(percent) { percent/100 }

# Use percent_to_decimal() on 6
percent_to_decimal(6)

# Example percentage
pct <- 8

# Use percent_to_decimal() on pct
percent_to_decimal(pct)



# Percent to decimal function
percent_to_decimal <- function(percent, digits = 2) {
    decimal <- percent / 100
    
    round(decimal, digits)
}

# percents
percents <- c(25.88, 9.045, 6.23)

# percent_to_decimal() with default digits
percent_to_decimal(percents)

# percent_to_decimal() with digits = 4
percent_to_decimal(percents, digits=4)


# Present value function
pv <- function(cash_flow, i, year) {
    
    # Discount multiplier
    mult <- 1 + percent_to_decimal(i)
    
    # Present value calculation
    cash_flow * mult ^ -year
}

# Calculate a present value
pv(1200, 7, 3)


# library(tidyquant)

# Pull Apple stock data
# apple <- tidyquant::tq_get("AAPL", get = "stock.prices", from = "2007-01-03", to = "2017-06-05")

# Take a look at what it returned
# head(apple)

# Plot the stock price over time
# plot(apple$date, apple$adjusted, type = "l")

# Calculate daily stock returns for the adjusted price
# apple <- tidyquant::tq_mutate(data = apple, select = adjusted, mutate_fun = dailyReturn)

# Sort the returns from least to greatest
# sorted_returns <- sort(apple$daily.returns)

# Plot them
# plot(sorted_returns)

```
  
  
  
***
  
Chapter 5 - Apply  
  
Why use apply?  
  
* There are many functions in the apply family - apply, lapply, eapply (applied over environment), mapply (applied to multiple lists or vectors), rapply (recursively applied to a list), tapply, sapply, vapply (strictly specify the output type)  
* The apply is generally much easier to read than a loop, and frequently runs much more quickly also  
* The general application for lapply is lapply(myList, FUN=myFunction)  
  
sapply() - simplify:  
  
* The sapply function will simplify (where possible) the output of lapply to a matrix, vector, or the like  
  
vapply() - select output type:  
  
* The FUN.VALUE option specifies the type of vector expected, as well as its length  
	* vapply(myList, FUN=class, FUN.VALUE=character(1))  # expected return is a length 1 character vector for each item in the list  
    * The output of vapply will be simplified if the FUN.VALUE condition is met  
  
Wrap up:  
  
* If, loops, functions, etc., as building blocks for additional explorations  
* Empirical Finance Task View on CRAN  
  
Example code includes:  
```{r}

stock_return <- tibble::tibble(apple=c(0.3745, -0.7188, 0.7699, 0.9823, 0.9817, 1.6322, -0.5704, 1.6681, 0, 0.5469, 0.1295, 0.5777, 0.2658, 0.0941, -0.6578, 0.1978, 0.6351, -0.4264, -0.0257, -0.7796), 
                               ibm=c(0.1251, -0.1125, 0.3191, 2.7689, 0.3459, 0.7015, -0.6125, 1.6858, 0.1307, -0.2908, -0.7678, -0.03, 0.552, -0.1611, -0.1614, -0.2095, 0.2579, -0.5684, 0.2467, -0.3661), 
                               micr=c(0.0845, 1.6371, -0.4484, 2.3686, -0.5866, 1.5735, 0.3227, 1.3029, -0.4763, -0.1595, -0.4474, 2.1188, -0.1257, 0, 0.0157, -0.4878, 0.0633, -0.4583, -0.1429, -1.2083)
                               )


# Print stock_return
stock_return

# lapply to change percents to decimal
lapply(stock_return, FUN = percent_to_decimal)


# Print stock_return
stock_return

# lapply to get the average returns
lapply(stock_return, FUN=mean)

# Sharpe ratio
sharpe <- function(returns) {
    (mean(returns) - .0003) / sd(returns)
}

# lapply to get the sharpe ratio
lapply(stock_return, FUN=sharpe)


# sharpe
sharpe <- function(returns, rf = 0.0003) {
    (mean(returns) - rf) / sd(returns)
}

# First lapply()
lapply(stock_return, FUN=sharpe, rf=0.0004)

# Second lapply()
lapply(stock_return, FUN=sharpe, rf=0.0009)


# lapply() on stock_return
lapply(stock_return, FUN=sharpe)

# sapply() on stock_return
sapply(stock_return, FUN=sharpe)

# sapply() on stock_return with optional arguments
sapply(stock_return, FUN=sharpe, simplify=FALSE, USE.NAMES=FALSE)


# Market crash with as.Date()
market_crash <- list(dow_jones_drop = 777.68, date = as.Date("2008-09-28"))
                     
# Find the classes with sapply()
sapply(market_crash, FUN=class)

# Market crash with as.POSIXct()
market_crash2 <- list(dow_jones_drop = 777.68, date = as.POSIXct("2008-09-28"))

# Find the classes with lapply()
lapply(market_crash2, FUN=class)

# Find the classes with sapply()
sapply(market_crash2, FUN=class)


# Market crash with as.POSIXct()
market_crash2 <- list(dow_jones_drop = 777.68, date = as.POSIXct("2008-09-28"))

# Find the classes with sapply()
sapply(market_crash2, FUN=class)

# Find the classes with vapply() - this has been commented out because it is designed to crash
# vapply(market_crash2, FUN=class, FUN.VALUE=character(1))


# Sharpe ratio for all stocks
vapply(stock_return, FUN=sharpe, FUN.VALUE = numeric(1))

# Summarize Apple
summary(stock_return$apple)

# Summarize all stocks
vapply(stock_return, FUN=summary, FUN.VALUE = numeric(6))


# Max and min
vapply(stock_return, FUN = function(x) { c(max(x), min(x)) }, FUN.VALUE = numeric(2))

```
  
  
  
***
  
### _Importing and Managing Financial Data in R_  
  
Chapter 1 - Introduction and Downloading Data  
  
Overview - Joshua Ulrich is the author of several R packages including TTR, xts, quantmod, quantstrat, blotter:  
  
* quantmod::getSymbols() provides a consistent interface to many sources of financial data - stores as XTS by default  
	* Can return data like a normal R function, or create an R object more like load()  
    * getSymbols("AAPL", src="yahoo", auto.assign=)  # note that "yahoo" is the default data source  
    * If the auto.assign=TRUE, then an object will be created; otherwise, the data is just output and the user needs to make an assignment  
  
Introduction to Quandl:  
  
* Many datasets available for free, while some require a subscription - API based access to quandl.com  
* General specifications are Quandl::Quandl(code="FRED/DGS10") - equivalent to quantmod::getSymbols("DGS10", src="FRED")  
* Can also add the type= argument to control the type of data returned  
  
Finding and downloading data from internet sources:  
  
* Can frequently find the symbol needed from the websites such as Yahoo Finance  
	* May want to double-check the historical data and see whether there is a download link - needed for getSymbols() to work  
    * For example, getSymbols() does not work on Google since they do not make data available for download  
* Can find currency symbols using OANDA  
	* FRED also has exchange rate data available  
  
Example code includes:  
```{r cache=TRUE}

# Import QQQ data from Yahoo! Finance
quantmod::getSymbols("QQQ", auto.assign=TRUE)

# Look at the structure of the object getSymbols created
str(QQQ)

# Look at the first few rows of QQQ
head(QQQ)


# Import QQQ data from Alpha Vantage
# quantmod::getSymbols("QQQ", src="av")

# Look at the structure of QQQ
# str(QQQ)

# Import GDP data from FRED
quantmod::getSymbols("GDP", src="FRED", auto.assign=TRUE)

# Look at the structure of GDP
str(GDP)


# There are two arguments that will make getSymbols() return the data:
# Set auto.assign = FALSE.
# Set env = NULL.
# The two methods are functionally equivalent, but I encourage you to use the first method because the auto.assign argument describes the behavior better
# Your future self will be more likely to remember what auto.assign = FALSE means than what env = NULL means


# Assign SPY data to 'spy' using auto.assign argument
spy <- quantmod::getSymbols("SPY", auto.assign=FALSE)

# Look at the structure of the 'spy' object
str(spy)

# Assign JNJ data to 'jnj' using env argument
jnj <- quantmod::getSymbols("JNJ", env=NULL, auto.assign=TRUE)

# Look at the structure of the 'jnj' object
str(jnj)


# Import GDP data from FRED
gdp <- Quandl::Quandl(code="FRED/GDP")

# Look at the structure of the object returned by Quandl
str(gdp)


# Import GDP data from FRED as xts
gdp_xts <- Quandl::Quandl(code="FRED/GDP", type="xts")

# Look at the structure of gdp_xts
str(gdp_xts)

# Import GDP data from FRED as zoo
gdp_zoo <- Quandl::Quandl(code="FRED/GDP", type="zoo")

# Look at the structure of gdp_zoo
str(gdp_zoo)


# Create an object containing the Pfizer ticker symbol
symbol <- "PFE"

# Use getSymbols to import the data
quantmod::getSymbols(symbol, auto.assign=TRUE)

# Look at the first few rows of data
head(PFE)


# Create a currency_pair object
currency_pair <- "GBP/CAD"

# Load British Pound to Canadian Dollar exchange rate data
quantmod::getSymbols(currency_pair, src="oanda", auto.assign=TRUE)

# Examine object using str()
str(GBPCAD)

# Try to load data from 190 days ago
# quantmod::getSymbols(currency_pair, from = Sys.Date() - 190, to = Sys.Date(), src = "oanda")
# Drives Warning: Oanda only provides historical data for the past 180 days. Symbol: GBP/CAD


# Create a series_name object
series_name <- "UNRATE"

# Load the data using getSymbols
quantmod::getSymbols(series_name, src="FRED", auto.assign=TRUE)

# Create a quandl_code object
quandl_code <- paste0("FRED/", series_name)

# Load the data using Quandl
unemploy_rate <- Quandl::Quandl(quandl_code)

```
  
  
  
***
  
Chapter 2 - Extracting and Transforming Data  
  
Extracting specific columns:  
  
* Aggregated data are frequently provided as OHLC (Open-High-Low-Close) for an interval, due to the data volumes and expenses associated with obtaining raw tick data  
	* Volume data is frequently provided also  
* There are many single-column extractors for OHLC data, and all are the first two letters of the column of interest, and available in the quantmod() package  
	* Op(), Hi(), Lo(), Cl(), Vo(), and Ad() for adjusted close price  
    * These functions use regex on the column names, so they may return 2+ columns depending on the column names  
    * Can convert data to OHLC using the OHLC() function - will strip off the volume components  
* Can also use the quantmod::getPrice(x, symbol, prefer) function  
	* x is the data frame  
    * symbol can be specified if there are multiple symbols in x  
    * prefer can be things like "bid" or "ask"  
  
Loading and transforming multiple instruments:  
  
* The collapse argument in Quandl allows you to take the last entry in the time period - caution that this is not always great for Open  
* Can also use the transform argument within Quandl to perform simple calculations prior to downloading  
	* none, diff (row on row change), rdiff (row on row percent change), cumul (cumulative sum)  
    * rdiff_from (latest value as % increment)  
    * normalize (scale series to start at 100)  
* Can create custom environments for loading and downloading financial data  
	* data_env <- new.env()  
    * getSymbols(c(), env=data_env, auto.assign=TRUE)  
    * Need to use a double dollars to extract from the environment, such as data_env$$SPY to get the SPY object  
* Can use lapply to loop over the environment, combining a list of functions in do.call()  
	* adjusted_list <- lapply(data_env, FUN=Ad)  
    * adjusted <- do.call(merge, adjusted_list)  
  
Example code includes:  
```{r cache=TRUE}

# Look at the head of DC
head(PFE)

# Extract the close column
pfe_close <- quantmod::Cl(PFE)

# Look at the head of dc_close
head(pfe_close)

# Extract the volume column
pfe_volume <- quantmod::Vo(PFE)

# Look at the head of dc_volume
head(pfe_volume)


# Extract the high, low, and close columns
pfe_hlc <- quantmod::HLC(PFE)

# Look at the head of dc_hlc
head(pfe_hlc)

# Extract the open, high, low, close, and volume columns
pfe_ohlcv <- quantmod::OHLCV(PFE)

# Look at the head of dc_ohlcv
head(pfe_ohlcv)


# Download CME data for CL and BZ as an xts object
# oil_data <- Quandl::Quandl(code = c("CME/CLH2016", "CME/BZH2016"), type = "xts")

# Look at the column names of the oil_data object
# colnames(oil_data)

# Extract the Open price for CLH2016
# cl_open <- quantmod::getPrice(oil_data, symbol = "CLH2016", prefer = "Open$")

# Look at January, 2016 using xts' ISO-8601 subsetting
# cl_open["2016-01"]


# CL and BZ Quandl codes
quandl_codes <- c("CME/CLH2016","CME/BZH2016")

# Download quarterly CL and BZ prices
# qtr_price <- Quandl::Quandl(quandl_codes, type="xts", collapse="quarterly")

# View the high prices for both series
# quantmod::Hi(qtr_price)

# Download quarterly CL and BZ returns
# qtr_return <- Quandl::Quandl(quandl_codes, type="xts", collapse="quarterly", transform="rdiff")

# View the settle price returns for both series
# quantmod::getPrice(qtr_return, prefer="Settle")


# One paradigm you can use in the quantmod workflow involves environments
# Store all your data in one environment
# Then you can use eapply() to call a function on each object in the environment, much like what lapply() does for each element of a list
# Also like lapply(), eapply() returns a list

# Then you can merge all the elements of the list into one object by using do.call(), which is like having R programmatically type and run a command for you
# Instead of typing merge(my_list[[1]], my_list[[2]]], ...), you can type do.call(merge, my_list)

# Call head on each object in data_env using eapply
# data_list <- eapply(data_env, FUN=head)

# Merge all the list elements into one xts object
# data_merged <- do.call(merge, data_list)

# Ensure the columns are ordered: open, high, low, close
# data_ohlc <- quantmod::OHLC(data_merged)


# Symbols
symbols <- c("AAPL", "MSFT", "IBM")

# Create new environment
data_env <- new.env()

# Load symbols into data_env
quantmod::getSymbols(symbols, env=data_env, auto.assign=TRUE)

# Extract the close column from each object and combine into one xts object
close_data <- do.call(merge, eapply(data_env, quantmod::Cl))

# View the head of close_data
head(close_data)

```
  
  
  
***
  
Chapter 3 - Managing Data from Multiple Sources  
  
Setting default arguments for getSymbols():  
  
* Can set a new default data source for getSymbols()  
	* getSymbols() is actually a wrapper to getSymbols.[source] methods  
    * Users should not directly call getSymbols.FRED(), and should stick with getSymbols(src="FRED")  
* To change the default data source for getSymbols() use setDefaults(getSymbols, src="FRED")  
	* The setDefaults requires key-value pairs as the entries, and are stored in globalOptions() - harder to reproduce, not available in other R functions  
* Can find other arguments that are available using args(getSymbols.yahoo)  
* Can see which functions have new defaults using getDefaults()  
	* Can see what options have been specified using getDefaults(myFunc)  
  
Setting per-instrument default arguments:  
  
* Can use the setSymbolLookup() function to set different defaults for various symbols  
	* setSymbolLookup(AAPL="google")  # sets all future default lookups for AAPL to be Google Finance  
    * setSymbolLookup(MSFT=list(src="google", from="2016-01-01"))  # sets all future default lookups for MSFT to be Google Finance starting January 1, 2016  
* There are options for managing and sharing the symbol-based lookups  
	* saveSymbolLookup("mySaveFile")  
    * setSymbolLookup(AAPL=NULL)  # removes any previous changes made to AAPL  
    * loadSymbolLookup(mySaveFile")  
  
Handling instrument symbols that clash or are not valid R names:  
  
* R names can contain number, letters, periods, or underscores (must also start with a letter or a dot followed by a non-number)  
	* There are R variables that violate these conditions, but must be accessed in unconventional ways  
    * The getSymbols() calls automatically remove the offending symbols from the dataset name and the column names  
* However, sometimes getSymbols() is unable to create a syntacically correct R name  
	* Can either use the invalid name inside backticks - for example, head(`000001.SS`)  
    * Can also use get with a quoted name, such as get("000001.SS")  
    * Alternately, can set auto.assign to be FALSE, allowing for a user-specified name in its place  
    * Can also use the colnames() function to reset the column names in a compliant manner  
* Can also use the setSymbolLookup() feature for a longer-lasting solution  
	* setSymbolLookup(SSE=list(name="0000001.SS"), FORD=list(name="F"))  
    * The FORD remap solves the problem that F means FALSE, while the SSE remap solves the issue that Shanghai index data is an invalid R name  
    * Need to use setSymbolLookup and getSymbolLookup for these to persist across sessions  
  
Example code includes:  
```{r eval=FALSE}

# DO NOT RUN - no API Key for data

# Set the default to pull data from Alpha Vantage
quantmod::setDefaults(getSymbols, src="av")

# Get GOOG data
quantmod::getSymbols("GOOG")

# Verify the data was actually pulled from Alpha Vantage
str(GOOG)


# Look at getSymbols.yahoo arguments
args(getSymbols.yahoo)

# Set default 'from' value for getSymbols.yahoo
quantmod::setDefaults(getSymbols.yahoo, from = "2000-01-01")

# Confirm defaults were set correctly
quantmod::getDefaults("getSymbols.yahoo")


# Changing the default source for one instrument is useful if multiple sources use the same symbol for different instruments
# For example, getSymbols("CP", src = "yahoo") would load Canadian Pacific Railway data from the New York Stock Exchange
# But getSymbols("CP", src = "FRED") would load Corporate Profits After Tax from the U.S. Bureau of Economic Analysis

# You can use setSymbolLookup() to specify the default data source for an instrument
# In this exercise, you will learn how to make getSymbols("CP") load the corporate profit data from FRED instead of the railway stock data from Yahoo Finance.

# setSymbolLookup() can take any number of name = value pairs, where name is the symbol and value is a named list of getSymbols() arguments for that one symbol

# Look at the first few rows of CP
head(CP)

# Set the source for CP to FRED
quantmod::setSymbolLookup(CP="FRED")

# Load CP data again
quantmod::getSymbols("CP")

# Look at the first few rows of CP
head(CP)


# Save symbol lookup table
quantmod::saveSymbolLookup("my_symbol_lookup.rda")

# Set default source for CP to "yahoo"
quantmod::setSymbolLookup(CP="yahoo")

# Verify the default source is "yahoo"
quantmod::getSymbolLookup("CP")

# Load symbol lookup table
quantmod::loadSymbolLookup("my_symbol_lookup.rda")

# Verify the default source is "FRED"
quantmod::getSymbolLookup("CP")


# Load BRK-A data
quantmod::getSymbols("BRK-A")

# Use backticks and head() to look at the loaded data
head(`BRK-A`)

# Use get() to assign the BRK-A data to an object named BRK.A
BRK.A <- get("BRK-A")


# Create BRK.A object
BRK.A <- quantmod::getSymbols("BRK-A", auto.assign=FALSE)

# Create col_names object with the column names of BRK.A
col_names <- colnames(BRK.A)

# Set BRK.A column names to syntactically valid names
colnames(BRK.A) <- make.names(col_names)


# Set name for BRK-A to BRK.A
quantmod::setSymbolLookup("BRK.A" = list(name="BRK-A"))

# Set name for T (AT&T) to ATT
quantmod::setSymbolLookup("ATT" = list(name="T"))

# Load BRK.A and ATT data
quantmod::getSymbols(c("BRK.A", "ATT"))

```
  
  
  
***
  
Chapter 4 - Aligning Data with Different Periodicities  
  
Making irregular data regular:  
  
* Regular data series have time observations that are equal intervals apart  
	* Can be creates using seq.Date() or seq.POSIXt()  
    * Can also be created using seq(fromDate, toDate, by="day")  
    * The start() and end() functions will grab the first and last times from an xts object  
* The zero-width xts object has an index but no columns  
	* xts(, order.by=)  
* The zero-width xts object can then be merged with the irregular data to create a regular xts object with NA where there is no observation  
	* Can carry the last observation forward using na.locf()  
    * Can also use fill=na.locf as an option in the merge  
  
Aggregating to lower frequency:  
  
* Low frequency data (such as quarterly) will often have only a single day associated to it - first day of quarter, last day of quarter, middle day of quarter, etc.  
* Can aggregate data to quarterly using apply.quarterly(myData, FUN=, na.rm=)  
* The zoo package contains two functions that help address the potential for data to not match/merge based on first day of quarter vs. last day of quarter and the like  
	* yearmon() for monthly data - as.yearmon()  
    * yearqtr() for quarterly data - as.yearqtr()  
* The second best option is to force the merge to go with the earliest date in the relevant time period  
	* na.locf(myData, fromLast=TRUE) - last observation carried backward  
    * Can then use the index of the other object to extract only the specific time desired within the time period  
  
Aggregating and combining intra-day data:  
  
* Intra-day data is impacted by time zones, as per the xts and zoo explanations  
* Internally, xts keeps its index as the number of seconds since midnight UTC on 1970-01-01  
	* The merge() data will match all the indices, and will keep the time zone of the first object passed to the merge  
* Can create smaller time series using seq(,,by="1 min")  
	* Can subset data for only certain hours using myXTS["T08:00/T18:00"]  
* To fill missing values by trading day, can use the split-lapply-rbind paradigm first introduced in the xts and zoo instructions  
	* myDay <- split(myData, "days")  
    * myFilledDay <- lapply(myDay, FUN=na.locf)  
    * myFinal <- do.call(rbind, myFilledDay)  
* Can aggregate irregular intraday data using to.period(myData, period="minutes", k=5) - this will create data by 5 minutes, taking the LAST entry in each 5-minute interval  
	* To instead align every piece of data to the nearest time interval, use align.time(myData, n=60*5) - this will create data by 5 minutes, rounded to the start of the next period  
  
Example code includes:  
```{r eval=FALSE}

# DO NOT HAVE DATA

# Extract the start date of the series
start_date <- start(irregular_xts)

# Extract the end date of the series
end_date <- end(irregular_xts)

# Create a regular date sequence
regular_index <- seq(start_date, end_date, by="day")

# Create a zero-width xts object
regular_xts <- xts(, order.by=regular_index)


# Merge irregular_xts and regular_xts
merged_xts <- merge(irregular_xts, regular_xts)

# Look at the first few rows of merged_xts
head(merged_xts)

# Use the fill argument to fill NA with their previous value
merged_filled_xts <- merge(irregular_xts, regular_xts, fill=na.locf)

# Look at the first few rows of merged_filled_xts
head(merged_filled_xts)


# Aggregate DFF to monthly
monthly_fedfunds <- apply.monthly(DFF, FUN=mean)

# Convert index to yearmon
index(monthly_fedfunds) <- as.yearmon(index(monthly_fedfunds))

# Merge FEDFUNDS with the monthly aggregate
merged_fedfunds <- merge(FEDFUNDS, monthly_fedfunds)

# Look at the first few rows of the merged object
head(merged_fedfunds)


# Look at the first few rows of merged_fedfunds
head(merged_fedfunds)

# Fill NA forward
merged_fedfunds_locf <- na.locf(merged_fedfunds)

# Extract index values containing last day of month
aligned_last_day <- merged_fedfunds_locf[index(monthly_fedfunds)]

# Fill NA backward
merged_fedfunds_locb <- na.locf(merged_fedfunds, fromLast=TRUE)

# Extract index values containing first day of month
aligned_first_day <- merged_fedfunds_locb[index(FEDFUNDS)]


# Extract index weekdays
index_weekdays <- .indexwday(DFF)

# Find locations of Wednesdays
wednesdays <- which(index_weekdays == 3)

# Create custom end points
end_points <- c(0, wednesdays, nrow(DFF))

# Calculate weekly mean using custom end points
weekly_mean <- period.apply(DFF, end_points, FUN=mean)


# Create merged object with a Europe/London timezone
tz_london <- merge(london, chicago)

# Look at tz_london structure
str(tz_london)

# Create merged object with a America/Chicago timezone
tz_chicago <- merge(chicago, london)

# Look at tz_chicago structure
str(tz_chicago)


# Create a regular date-time sequence
regular_index <- seq(as.POSIXct("2010-01-04 09:00"), as.POSIXct("2010-01-08 16:00"), by = "30 min")

# Create a zero-width xts object
regular_xts <- xts(, order.by=regular_index)

# Merge irregular_xts and regular_xts, filling NA with their previous value
merged_xts <- merge(irregular_xts, regular_xts, fill=na.locf)

# Subset to trading day (9AM - 4PM)
trade_day <- merged_xts["T09:00/T16:00"]


# Split trade_day into days
daily_list <- split(trade_day , f = "days")

# Use lapply to call na.locf for each day in daily_list
daily_filled <- lapply(daily_list, FUN = na.locf)

# Use do.call to rbind the results
filled_by_trade_day <- do.call(rbind, daily_filled)


# Convert raw prices to 5-second prices
xts_5sec <- to.period(intraday_xts, period = "seconds", k = 5)

# Convert raw prices to 10-minute prices
xts_10min <- to.period(intraday_xts, period = "minutes", k = 10)

# Convert raw prices to 1-hour prices
xts_1hour <- to.period(intraday_xts, period = "hours", k = 1)

```
  
  
  
***
  
Chapter 5 - Importing Text Data, Adjusting for Corporate Actions  
  
Importing text files:  
  
* The getSymbols() can import CSV files, provided that the CSV file matches a specific format - getSymbols(fileName, src="csv", dir=)  
	* One instrument per file  
    * Columns are date, open, high, low, close, volume, adjusted close  
    * Files are named [symbol].csv  
    * The dir= option can be specified to note that the files are in a specific directory  
* Alternately, can import text files using read.zoo(), which is a wrapper to read.table()  
	* Need to specify sep= and also flag header=TRUE (provided that your data has a header)  
    * Can then run as.xts() on the object that has been returned by read.zoo()  
* Can handle date and time in different columns as part of the read.zoo() process  
	* Specify the option index.column=c("dayColumn", "timeColumn")  
* Can handle data that has multiple instruments (such as bid and ask)  
	* split=c("Symbol", "Type") will create Symbol1.Type1, Symbol1.Type2, … , SymbolN.TypeN columns  
  
Checking for weirdness - missing values and corporate actions:  
  
* Missing values are common due to things like holidays - treasuries could be closed while stock markets could be open  
	* na.locf() - carry the last observation forward to fill NA  
    * na.approx() - fill NA using linear regression  
    * na.spline() - fill NA using spline  
* When there is a large price change, it can be helpful to cross-reference it against a second data source  
	* The adjusted close price is frequently valuable (Yahoo, but not Google), since it accounts for both splits and dividends  
    * The adjusted close price can be a more realistic way to assess the return to the investors  
  
Adjusting financial time series:  
  
* The adjustOHLC() function will use Yahoo Finance data to adjust prices for splits and dividends  
	* getSymbols("MSFT")  
    * msft_adjusted <- adjustOHLC(MSFT)  # requires that the symbol and object have the same name  
    * The symbol.name= option can be set inside adjustOHLC in the event that the ticker and symbol name are not the same  
    * The adjustment process ensures that the returns are corrected, so it is not quite as simple as Price - Dividend throughout all of history  
* The splits and dividends can be obtaines using getSplits() and getDividends()  
	* The adjRatios(splits=, dividends=, close=) function will allow for adjustments for splits and dividends data  
  
Wrap up:  
  
* Tools for wrangling real-world time series data  
  
Example code includes:  
```{r eval=FALSE}

# DO NOT RUN DUE TO NOT HAVING DATA

library(quantmod)


getSymbols("AMZN", src="yahoo")
str(AMZN)
write.zoo(AMZN, "./RInputFiles/AMZN.csv")
rm(AMZN)


# Load AMZN.csv
getSymbols("./RInputFiles/AMZN.csv", src="csv")

# Look at AMZN structure
str(AMZN)


# Import AMZN.csv using read.zoo
amzn_zoo <- read.zoo("./RInputFiles/AMZN.csv", sep = ",", header = TRUE)

# Convert to xts
amzn_xts <- as.xts(amzn_zoo)

# Look at the first few rows of amzn_xts
head(amzn_xts)


# Read data with read.csv
une_data <- read.csv("UNE.csv", nrows = 5)

# Look at the structure of une_data
str(une_data)

# Read data with read.zoo, specifying index columns
une_zoo <- read.zoo("UNE.csv", index.column = c("Date", "Time"), sep = ",", header = TRUE)

# Look at first few rows of data
head(une_zoo)


# Read data with read.csv
two_symbols_data <- read.csv("two_symbols.csv", nrows = 5)

# Look at the structure of two_symbols_data
str(two_symbols_data)

# Read data with read.zoo, specifying index columns
two_symbols_zoo <- read.zoo("two_symbols.csv", split = c("Symbol", "Type"), sep = ",", header = TRUE)

# Look at first few rows of data
head(two_symbols_zoo)


# fill NA using last observation carried forward
locf <- na.locf(DGS10)

# fill NA using linear interpolation
approx <- na.approx(DGS10)

# fill NA using spline interpolation
spline <- na.spline(DGS10)

# merge into one object
na_filled <- merge(locf, approx, spline)

# plot combined object
plot(na_filled, col = c("black", "red", "green"))


# Look at the last few rows of AAPL data
tail(AAPL)

# Plot close price
plot(Cl(AAPL))

# Plot adjusted close price
plot(Ad(AAPL))


# Look at first few rows of aapl_raw
head(aapl_raw)

# Look at first few rows of aapl_split
head(aapl_split_adjusted)

# Plot difference between adjusted close and split-adjusted close
plot(Ad(aapl_raw) - Cl(aapl_split_adjusted))

# Plot difference between volume from the raw and split-adjusted sources
plot(Vo(aapl_raw) - Vo(aapl_split_adjusted))


# Look at first few rows of AAPL
head(AAPL)

# Adjust AAPL for splits and dividends
aapl_adjusted <- adjustOHLC(AAPL)

# Look at first few rows of aapl_adjusted
head(aapl_adjusted)


# The previous exercise taught you how to adjust raw historical OHLC prices for splits and dividends using adjustOHLC()
# But adjustOHLC() only works for OHLC data
# It will not work if you only have close prices. adjustOHLC() also does not return any of the split or dividend data it uses.

# You will need the dates and values for each split and dividend if you want to adjust a non-OHLC price series, or if you simply want to analyze the raw split and dividend data.

# You can download the split and dividend data from Yahoo Finance using the quantmod functions getSplits() and getDividends(), respectively
# Note that the historical dividend data from Yahoo Finance is adjusted for splits
# If you want to download unadjusted dividend data, you need to set split.adjust = FALSE in your call to getDividends()

# Download AAPL split data
splits <- getSplits("AAPL")

# Print the splits object
splits

# Download AAPL dividend data
dividends <- getDividends("AAPL")

# Look at the first few rows of dividends
head(dividends)

# Download unadjusted AAPL dividend data
raw_dividends <- getDividends("AAPL", split.adjust=FALSE)

# Look at the first few rows of raw_dividends
head(raw_dividends)


# Remember that adjustOHLC() only works for OHLC data
# If you only have close prices, you can use the adjRatios() function to calculate the split and dividend adjustment ratios
# The adjRatios() function has three arguments: splits, dividends, and close
# It returns an xts object with two columns, "Split" and "Div", that contain the split and dividend adjustment ratios, respectively

# adjRatios() needs split data in order to calculate the split adjustment ratio
# You provide split data via the splits argument
# To calculate the dividend adjustment ratio, you need to supply raw dividends and raw prices to adjRatios(), using the dividends and close arguments, respectively

# Once you have the split and dividend adjustment ratios, calculating the adjusted price is simple
# You just have to multiply the unadjusted price by both the split and dividend adjustment ratios

# Calculate split and dividend adjustment ratios
ratios <- adjRatios(splits = splits, dividends = raw_dividends, close = Cl(AAPL))

# Calculate adjusted close for AAPL
aapl_adjusted <- Cl(AAPL) * ratios[, "Split"] * ratios[, "Div"]

# Look at first few rows of Yahoo adjusted close
head(Ad(AAPL))

# Look at first few rows of aapl_adjusted
head(aapl_adjusted)

```
  
  
  
***
  
### _Introduction to Portfolio Analysis in R_  
  
Chapter 1 - The Building Blocks  
  
Introduction:  
  
* Instructor believes that portfolio analysis can help drive improved returns  
* Goal of investing is to balance risks against returns  
* Course Overview  
	* Chapter 1 - Basic Variables (weights and returns)  
    * Chapter 2 - Measures of Risk (portfolio performance)  
    * Chapter 3 - Drivers of Performance (interactions)  
    * Chapter 4 - Portfolio Optimization (highest return for level of risk)  
  
Portfolio Weights:  
  
* Often, risk can be reduced by diversifying - holding stocks in multiple companies  
* Can compute the portfolio weights, defined as the relative weight of each investment (in dollar values) to the overall portfolio value  
* Multiple strategies for balancing the portfolio weights  
	* Betting on a single asset  
    * Equal weighting on each asset  
    * Market cap weighting on each asset  
    * Optimizing mean and variance (chapter 4)  
  
Portfolio Return:  
  
* The larger the weight of an asset in the portfolio, the greater the influence on the returns  
* Returns are typically calculated relatively, which is to say (New - Old) / Old  
	* Can be calculated for a single asset, or in aggregate for a basket of assets  
  
Performance Analytics:  
  
* The longer the history of returns, the better the information available about the portfolio returns  
* The go-to package for R Finance is PerformanceAnalytics  
	* Return.calculate(pricesXTS) will return a time series of returns, with the first row being NA  
    * Return.portfolio(returnsXTS, weights=NULL, rebalance_on=) - typically just give the initial weights and let them float, with the option to set rebalancing instead  
  
Example code includes:  
```{r}

# load("./RInputFiles/sp500.RData")
# load("./RInputFiles/aapl_msft.RData")
# load("./RInputFiles/eq_prices.RData")
prices <- readRDS("./RInputFiles/prices.rds")


# Load package PerformanceAnalytics 
library(PerformanceAnalytics)


ko <- prices$KO
ge <- prices$GE

# Define ko_pep 
ko_ge <- ko / ge

# Make a time series plot of ko_pep
zoo::plot.zoo(ko_ge)
  
# Add as a reference, a horizontal line at 1
abline(h=1)


# Define the vector values
values <- c(4000, 4000, 2000)

# Define the vector weights
weights <- values / sum(values)

# Print the resulting weights
weights


# Define marketcaps
marketcaps <- c(5, 8, 9, 20, 25, 100, 100, 500, 700, 2000)
  
# Compute the weights
weights <- marketcaps / sum(marketcaps)
  
# Inspect summary statistics
summary(weights)
  
# Create a barplot of weights
barplot(weights)


# Vector of initial value of the assets
in_values <- c(1000, 5000, 2000)
  
# Vector of final values of the assets
fin_values <- c(1100, 4500, 3000)

# Weights as the proportion of total value invested in each assets
weights <- in_values / sum(in_values)

# Vector of simple returns of the assets 
returns <- (fin_values - in_values)/in_values
  
# Compute portfolio return using the portfolio return formula
preturns <- sum(weights*returns)


# Print the first and last six rows of prices
head(prices)
tail(prices)

# Create the variable returns using Return.calculate()  
returns <- Return.calculate(prices)
  
# Print the first six rows of returns. Note that the first observation is NA, because there is no prior price.
head(returns)
  
# Remove the first row of returns
returns <- returns[-1, c("AAPL", "MSFT")]


# Create the weights
eq_weights <- c(0.5, 0.5)

# Create a portfolio using buy and hold
pf_bh <- Return.portfolio(R = returns, weights = eq_weights)

# Create a portfolio rebalancing monthly 
pf_rebal <- Return.portfolio(R = returns, weights = eq_weights, rebalance_on="months")

# Plot the time-series
par(mfrow = c(2, 1), mar = c(2, 4, 2, 2))
plot.zoo(pf_bh)
plot.zoo(pf_rebal)


# Create the weights
eq_weights <- c(0.5, 0.5)

# Create a portfolio using buy and hold
pf_bh <- Return.portfolio(returns, weights = eq_weights, verbose=TRUE)

# Create a portfolio that rebalances monthly
pf_rebal <- Return.portfolio(returns, weights = eq_weights, rebalance_on = "months", verbose=TRUE)

# Create eop_weight_bh
eop_weight_bh <- pf_bh$EOP.Weight

# Create eop_weight_rebal
eop_weight_rebal <- pf_rebal$EOP.Weight

# Plot end of period weights
par(mfrow = c(2, 1), mar=c(2, 4, 2, 2))
plot.zoo(eop_weight_bh$AAPL)
plot.zoo(eop_weight_rebal$AAPL)

```
  
  
  
***
  
Chapter 2 - Analyzing Performance  
  
Dimensions of Portfolio Performance:  
  
* There are two main dimensions of performance - risk (variance or volatility) and reward (mean return)  
	* The portfolio variance is based on the sum-squared of the "de-meaned returns" (variance)  
    * The square root of the portfolio variance (standard deviation) is volatility  
* The Geometric Mean Return is more often used, since (1+x) * (1-x) is less than 1 for all x <> 0  
	* The geometric return is calculated as [Product of (1 + r)]^(1/T) - 1  
  
Annualized Sharpe Ratio:  
  
* The standard benchmark for assessing risk-return is to use US Treasuries (considered to be risk-free)  
	* The x-axis should have the volatility and the y-axis should have the mean portfolio return  
    * Optimally, every portfolio should be on the frontier, where no point with lesser volatility has equal or better returns  
* The Sharpe ratio is defined as the excess mean (versus risk free) divided by the portfolio volatility  
	* Performances are typically annualized for financial reporting  
    * Means are typically geometric by month or arithmetic by year  
    * Volatility is often defined as monthly volatility times sqrt(12)  
  
Time-Variation in Portfolio Performance:  
  
* Changes in portfolio performance are common due to business cycles, shocks/events, psychology, and the like  
* Volatility tends to be persistent for a while - higher volatility in cycle t is typically followed by relatively high volatility in cycle t+1  
	* As such, it is common to take a rolling sample of K observations, discarding the most recent and the most distant (t-k+3 to t+2)  
    * Window length is a measure of taset, with the instructor often using 3 years  
  
Non-Normality of Return Distributions:  
  
* Portfolio returns are NOT bell-shaped or normal - in general, the tails are heavier, meaning extreme negative returns are more probable than in a normal distribution  
	* This makes the standard deviation an imperfect measure of downside risk  
* The semi-deviation is defined as the the variance calculated using only the returns that are below the mean  
* Can also look at the 5% most likely downside returns (returns in the 5th percentile)  
* Additional checks for the shape of the distribution include  
	* Skewness - is it symmetric (zero means roughly symmetric, while negative means long left tail and positive means long right tail)  
    * Excess kurtosis - are the tails fatter than expected (excess kurtosis is zero for the normal distribution)  
  
Example code includes:  
```{r}

adjClose <- c(211.3, 211.8, 226.9, 238.9, 235.5, 247.4, 250.8, 236.1, 252.9, 231.3, 244, 249.2, 242.2, 274.1, 284.2, 291.7, 288.4, 290.1, 304, 318.7, 329.8, 321.8, 251.8, 230.3, 247.1, 257.1, 267.8, 258.9, 261.3, 262.2, 273.5, 272, 261.5, 271.9, 279, 273.7, 277.7, 297.5, 288.9, 294.9, 309.6, 320.5, 318, 346.1, 351.5, 349.1, 340.4, 346, 353.4, 329.1, 331.9, 339.9, 330.8, 361.2, 358, 356.1, 322.6, 306, 304, 322.2, 330.2, 343.9, 367.1, 375.2, 375.3, 389.8, 371.2, 387.8, 395.4, 387.9, 392.5, 375.2, 417.1, 408.8, 412.7, 403.7, 415, 415.4, 408.1, 424.2, 414, 417.8, 418.7, 431.4, 435.7, 438.8, 443.4, 451.7, 440.2, 450.2, 450.5, 448.1, 463.6, 458.9, 467.8, 461.8, 466.5, 481.6, 467.1, 445.8, 450.9, 456.5, 444.3, 458.3, 475.5, 462.7, 472.4, 453.7, 459.3, 470.4, 487.4, 500.7, 514.7, 533.4, 544.8, 562.1, 561.9, 584.4, 581.5, 605.4, 615.9, 636, 640.4, 645.5, 654.2, 669.1, 670.6, 640, 652, 687.3, 705.3, 757, 740.7, 786.2, 790.8, 757.1, 801.3, 848.3, 885.1, 954.3, 899.5, 947.3, 914.6, 955.4, 970.4, 980.3, 1049.3, 1101.8, 1111.8, 1090.8, 1133.8, 1120.7, 957.3, 1017, 1098.7, 1163.6, 1229.2, 1279.6, 1238.3, 1286.4, 1335.2, 1301.8, 1372.7, 1328.7, 1320.4, 1282.7, 1362.9, 1388.9, 1469.2, 1394.5, 1366.4, 1498.6, 1452.4, 1420.6, 1454.6, 1430.8, 1517.7, 1436.5, 1429.4, 1314.9, 1320.3, 1366, 1239.9, 1160.3, 1249.5, 1255.8, 1224.4, 1211.2, 1133.6, 1040.9, 1059.8, 1139.4, 1148.1, 1130.2, 1106.7, 1147.4, 1076.9, 1067.1, 989.8, 911.6, 916.1, 815.3, 885.8, 936.3, 879.8, 855.7, 841.2, 848.2, 916.9, 963.6, 974.5, 990.3, 1008, 996, 1050.7, 1058.2, 1111.9, 1131.1, 1144.9, 1126.2, 1107.3, 1120.7, 1140.8, 1101.7, 1104.2, 1114.6, 1130.2, 1173.8, 1211.9, 1181.3, 1203.6, 1180.6, 1156.8, 1191.5, 1191.3, 1234.2, 1220.3, 1228.8, 1207, 1249.5, 1248.3, 1280.1, 1280.7, 1294.9, 1310.6, 1270.1, 1270.2, 1276.7, 1303.8, 1335.8, 1377.9, 1400.6, 1418.3, 1438.2, 1406.8, 1420.9, 1482.4, 1530.6, 1503.3, 1455.3, 1474, 1526.8, 1549.4, 1481.1, 1468.4, 1378.6, 1330.6, 1322.7, 1385.6, 1400.4, 1280, 1267.4, 1282.8, 1166.4, 968.8, 896.2, 903.2, 825.9, 735.1, 797.9, 872.8, 919.1, 919.3, 987.5, 1020.6, 1057.1, 1036.2, 1095.6, 1115.1, 1073.9, 1104.5, 1169.4, 1186.7, 1089.4, 1030.7, 1101.6, 1049.3, 1141.2, 1183.3, 1180.6, 1257.6, 1286.1, 1327.2, 1325.8, 1363.6, 1345.2, 1320.6, 1292.3, 1218.9, 1131.4, 1253.3, 1247, 1257.6, 1312.4, 1365.7, 1408.5, 1397.9, 1310.3, 1362.2, 1379.3, 1406.6, 1440.7, 1412.2, 1416.2, 1426.2, 1498.1, 1514.7, 1569.2, 1597.6, 1630.7, 1606.3, 1685.7, 1633, 1681.6, 1756.5, 1805.8, 1848.4, 1782.6, 1859.4, 1872.3, 1883.9, 1923.6, 1960.2, 1930.7, 2003.4, 1972.3, 2018.1, 2067.6, 2058.9, 1995, 2104.5, 2067.9, 2085.5, 2107.4, 2063.1, 2103.8, 1972.2, 1920, 2079.4, 2080.4, 2043.9, 1940.2, 1932.2, 2059.7, 2065.3, 2096.9, 2098.9, 2173.6, 2170.9)
monDate <- c(5843, 5845, 5877, 5905, 5934, 5964, 5996, 6025, 6056, 6088, 6117, 6150, 6178, 6210, 6241, 6269, 6299, 6329, 6360, 6390, 6423, 6452, 6482, 6514, 6543, 6577, 6605, 6634, 6668, 6696, 6726, 6756, 6787, 6818, 6850, 6879, 6909, 6942, 6971, 6999, 7032, 7060, 7091, 7123, 7152, 7183, 7214, 7244, 7274, 7306, 7336, 7364, 7396, 7425, 7456, 7487, 7517, 7551, 7578, 7609, 7641, 7671, 7701, 7729, 7760, 7790, 7823, 7851, 7882, 7915, 7943, 7974, 8005, 8036, 8068, 8096, 8126, 8156, 8187, 8217, 8250, 8279, 8309, 8341, 8370, 8404, 8432, 8460, 8491, 8523, 8552, 8582, 8614, 8644, 8674, 8705, 8735, 8768, 8797, 8825, 8859, 8887, 8917, 8947, 8978, 9009, 9041, 9070, 9100, 9133, 9162, 9190, 9223, 9251, 9282, 9314, 9343, 9374, 9405, 9435, 9465, 9497, 9527, 9556, 9587, 9617, 9650, 9678, 9709, 9742, 9770, 9801, 9832, 9863, 9895, 9923, 9952, 9982, 10014, 10043, 10074, 10106, 10135, 10168, 10196, 10228, 10259, 10287, 10317, 10347, 10378, 10408, 10441, 10470, 10500, 10532, 10561, 10595, 10623, 10651, 10682, 10714, 10743, 10773, 10805, 10835, 10865, 10896, 10926, 10959, 10988, 11017, 11050, 11078, 11109, 11141, 11170, 11201, 11232, 11262, 11292, 11324, 11354, 11382, 11414, 11443, 11474, 11505, 11535, 11569, 11596, 11627, 11659, 11689, 11719, 11747, 11778, 11808, 11841, 11869, 11900, 11933, 11961, 11992, 12023, 12054, 12086, 12114, 12143, 12173, 12205, 12234, 12265, 12297, 12326, 12359, 12387, 12419, 12450, 12478, 12509, 12541, 12570, 12600, 12632, 12662, 12692, 12723, 12753, 12786, 12815, 12843, 12874, 12905, 12935, 12965, 12996, 13027, 13059, 13088, 13118, 13151, 13180, 13208, 13241, 13269, 13300, 13332, 13361, 13392, 13423, 13453, 13483, 13516, 13545, 13573, 13605, 13634, 13665, 13696, 13726, 13760, 13787, 13818, 13850, 13880, 13910, 13941, 13970, 14000, 14032, 14061, 14092, 14124, 14153, 14186, 14214, 14246, 14277, 14305, 14335, 14365, 14396, 14426, 14459, 14488, 14518, 14550, 14579, 14613, 14641, 14669, 14700, 14732, 14761, 14791, 14823, 14853, 14883, 14914, 14944, 14977, 15006, 15034, 15065, 15096, 15126, 15156, 15187, 15218, 15250, 15279, 15309, 15342, 15371, 15400, 15432, 15461, 15492, 15523, 15553, 15587, 15614, 15645, 15677, 15707, 15737, 15765, 15796, 15826, 15859, 15887, 15918, 15951, 15979, 16010, 16041, 16072, 16104, 16132, 16161, 16191, 16223, 16252, 16283, 16315, 16344, 16377, 16405, 16437, 16468, 16496, 16526, 16556, 16587, 16617, 16650, 16679, 16709, 16741, 16770, 16804, 16832, 16861, 16892, 16923, 16953, 16983, 17014)
sp500 <- xts(adjClose, order.by=as.Date(monDate))
names(sp500) <- "AdjClose"
str(sp500)


par(mfcol=c(1, 1))
par(mfrow=c(1, 1))

# Convert the daily frequency of sp500 to monthly frequency: sp500_monthly
sp500_monthly <- to.monthly(sp500)

# Print the first six rows of sp500_monthly
head(sp500_monthly)

# Create sp500_returns using Return.calculate using the closing prices
sp500_returns <- Return.calculate(sp500_monthly[, 4])

# Time series plot
plot.zoo(sp500_returns)

# Produce the year x month table
table.CalendarReturns(sp500_returns)


sp500_returns <- sp500_returns[2:nrow(sp500_returns), ]
str(sp500_returns)

# Compute the mean monthly returns
mean(sp500_returns)

# Compute the geometric mean of monthly returns
mean.geometric(sp500_returns)

# Compute the standard deviation
sd(sp500_returns)


rfRates <- c(0.0053, 0.006, 0.0052, 0.0049, 0.0052, 0.0052, 0.0046, 0.0045, 0.0046, 0.0039, 0.0049, 0.0042, 0.0043, 0.0047, 0.0044, 0.0038, 0.0048, 0.0046, 0.0047, 0.0045, 0.006, 0.0035, 0.0039, 0.0029, 0.0046, 0.0044, 0.0046, 0.0051, 0.0049, 0.0051, 0.0059, 0.0062, 0.0061, 0.0057, 0.0063, 0.0055, 0.0061, 0.0067, 0.0067, 0.0079, 0.0071, 0.007, 0.0074, 0.0065, 0.0068, 0.0069, 0.0061, 0.0057, 0.0057, 0.0064, 0.0069, 0.0068, 0.0063, 0.0068, 0.0066, 0.006, 0.0068, 0.0057, 0.006, 0.0052, 0.0048, 0.0044, 0.0053, 0.0047, 0.0042, 0.0049, 0.0046, 0.0046, 0.0042, 0.0039, 0.0038, 0.0034, 0.0028, 0.0034, 0.0032, 0.0028, 0.0032, 0.0031, 0.0026, 0.0026, 0.0023, 0.0023, 0.0028, 0.0023, 0.0022, 0.0025, 0.0024, 0.0022, 0.0025, 0.0024, 0.0025, 0.0026, 0.0022, 0.0025, 0.0023, 0.0025, 0.0021, 0.0027, 0.0027, 0.0031, 0.0031, 0.0028, 0.0037, 0.0037, 0.0038, 0.0037, 0.0044, 0.0042, 0.004, 0.0046, 0.0044, 0.0054, 0.0047, 0.0045, 0.0047, 0.0043, 0.0047, 0.0042, 0.0049, 0.0043, 0.0039, 0.0039, 0.0046, 0.0042, 0.004, 0.0045, 0.0041, 0.0044, 0.0042, 0.0041, 0.0046, 0.0045, 0.0039, 0.0043, 0.0043, 0.0049, 0.0037, 0.0043, 0.0041, 0.0044, 0.0042, 0.0039, 0.0048, 0.0043, 0.0039, 0.0039, 0.0043, 0.004, 0.0041, 0.004, 0.0043, 0.0046, 0.0032, 0.0031, 0.0038, 0.0035, 0.0035, 0.0043, 0.0037, 0.0034, 0.004, 0.0038, 0.0039, 0.0039, 0.0039, 0.0036, 0.0044, 0.0041, 0.0043, 0.0047, 0.0046, 0.005, 0.004, 0.0048, 0.005, 0.0051, 0.0056, 0.0051, 0.005, 0.0054, 0.0038, 0.0042, 0.0039, 0.0032, 0.0028, 0.003, 0.0031, 0.0028, 0.0022, 0.0017, 0.0015, 0.0014, 0.0013, 0.0013, 0.0015, 0.0014, 0.0013, 0.0015, 0.0014, 0.0014, 0.0014, 0.0012, 0.0011, 0.001, 9e-04, 0.001, 0.001, 9e-04, 0.001, 7e-04, 7e-04, 8e-04, 7e-04, 7e-04, 8e-04, 7e-04, 6e-04, 9e-04, 8e-04, 6e-04, 8e-04, 0.001, 0.0011, 0.0011, 0.0011, 0.0015, 0.0016, 0.0016, 0.0016, 0.0021, 0.0021, 0.0024, 0.0023, 0.0024, 0.003, 0.0029, 0.0027, 0.0031, 0.0032, 0.0035, 0.0034, 0.0037, 0.0036, 0.0043, 0.004, 0.004, 0.0042, 0.0041, 0.0041, 0.0042, 0.004, 0.0044, 0.0038, 0.0043, 0.0044, 0.0041, 0.004, 0.004, 0.0042, 0.0032, 0.0032, 0.0034, 0.0027, 0.0021, 0.0013, 0.0017, 0.0018, 0.0018, 0.0017, 0.0015, 0.0013, 0.0015, 8e-04, 3e-04, 0, 0, 1e-04, 2e-04, 1e-04, 0, 1e-04, 1e-04, 1e-04, 1e-04, 0, 0, 1e-04, 0, 0, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 0, 0, 0, 0, 1e-04, 0, 0, 0, 0, 0, 0, 0, 0, 1e-04, 0, 0, 1e-04, 1e-04, 1e-04, 1e-04, 1e-04, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1e-04, 1e-04, 2e-04, 2e-04, 1e-04, 1e-04, 2e-04, 2e-04, 2e-04, 2e-04)
rf <- xts(rfRates, order.by=index(sp500_returns))
str(rf)


# Compute the annualized risk free rate
annualized_rf <- (1 + rf)^12 - 1

# Plot the annualized risk free rate
plot.zoo(annualized_rf)

# Compute the series of excess portfolio returns 
sp500_excess <- sp500_returns - rf

# Compare the mean
mean(sp500_returns)
mean(sp500_excess)

# Compute the Sharpe ratio
sp500_sharpe <- mean(sp500_excess) / sd(sp500_returns)


# Compute the annualized mean
Return.annualized(sp500_returns, scale=12)

# Compute the annualized standard deviation
StdDev.annualized(sp500_returns)

# Compute the annualized Sharpe ratio: ann_sharpe
ann_sharpe <- Return.annualized(sp500_returns, scale=12) / StdDev.annualized(sp500_returns)

# Compute all of the above at once using table.AnnualizedReturns()
table.AnnualizedReturns(sp500_returns)


# In this exercise you will also use the function SharpeRatio.annualized() to calculate the annualized Sharpe Ratio
# This function takes the arguments R, and Rf
# The R argument takes an xts, vector, matrix, data.frame, timeSeries or zoo object of asset returns
# The Rf argument is necessary in SharpeRatio.annualized(), as it takes into account the risk-free rate in the same period of your returns
# For this example you can use the object rf to fulfill the Rf argument

# The function chart.RollingPerformance() makes it easy to visualize the rolling estimates of performance in R
# Familiarize yourself first with the syntax of this function
# It requires you to specify the time series of portfolio returns (by setting the argument R), the length of the window (width) and the function used to compute the performance (argument FUN)
# To see all three plots together, PerformanceAnalytics provides a shortcut function charts.RollingPerformance()
# Note the charts instead of chart. This function creates all of the previous charts at once and does not use the argument FUN

# Calculate the mean, volatility, and sharpe ratio of sp500_returns
returns_ann <- Return.annualized(sp500_returns, scale=12)
sd_ann <- StdDev.annualized(sp500_returns)
sharpe_ann <- SharpeRatio.annualized(sp500_returns, Rf=rf)

# Plotting the 12-month rolling annualized mean
chart.RollingPerformance(R = sp500_returns, width = 12, scale=12, FUN = "Return.annualized")
abline(h = returns_ann)

# Plotting the 12-month rolling annualized standard deviation
chart.RollingPerformance(R = sp500_returns, width=12, FUN="StdDev.annualized")
abline(h = sd_ann)

# Plotting the 12-month rolling annualized Sharpe ratio
chart.RollingPerformance(R = sp500_returns, width=12, FUN="SharpeRatio.annualized", Rf=rf)
abline(h = sharpe_ann)


# Fill in window for 2008
sp500_2008 <- window(sp500_returns, start = "2008-01-01", end = "2008-12-31")

# Create window for 2014
sp500_2014 <- window(sp500_returns, start = "2014-01-01", end = "2014-12-31")

# Plotting settings
par(mfrow = c(1, 2) , mar=c(3, 2, 2, 2))
names(sp500_2008) <- "sp500_2008"
names(sp500_2014) <- "sp500_2014"

# Plot histogram of 2008
chart.Histogram(sp500_2008, methods = c("add.density", "add.normal"))

# Plot histogram of 2014
chart.Histogram(sp500_2014, methods = c("add.density", "add.normal"))


#  Compute the skewness 
# skewness(sp500_daily)
skewness(sp500_returns)

# Compute the excess kurtois 
# kurtosis(sp500_daily)
kurtosis(sp500_returns)


# Calculate the SemiDeviation
SemiDeviation(sp500_returns)

# Calculate the value at risk
VaR(sp500_returns, p=0.025)
VaR(sp500_returns, p=0.05)

# Calculate the expected shortfall
ES(sp500_returns, p=0.025)
ES(sp500_returns, p=0.05)


par(mfcol=c(1, 1))
par(mfrow=c(1, 1))

index(sp500_returns) <- as.Date(index(sp500_returns))

# Table of drawdowns
table.Drawdowns(sp500_returns)

# Plot of drawdowns
chart.Drawdown(sp500_returns)

par(mfcol=c(1, 1))
par(mfrow=c(1, 1))


```
  
  
  
***
  
Chapter 3 - Performance Drivers  
  
Drivers in the Case of Two Assets:  
  
* Portfolio returns are random variables, with historical means and volatilities helping to guide the random variable  
	* Expected Portfolio Return - weighted expected returns of the individual components  
    * Expected Portfolio Variance - (w1**2) * var(R1) + (w2**2) * var(R2) + 2 * w1 * w2 * cov(R1, R2), where cov(R1, R2) = sd(R1) * sd(R2) * cor(R1, R2)  
  
Using Matrix Notation:  
  
* Weights are generally maintained in a column-matrix of Nx1, called w  
* Returns are generally maintained in a column-matrix of Nx1, called R  
* Expected returns are generally maintained in a column-matrix of Nx1, called mu  
* Variance and covariance are maintained in an NxN matrix (covariance matrix - diagonals are variances and non-diagonals are covariances)  
* The overall portfolio can be assessed as  
	* Returns are t(w) %*% R  
    * Expected returns are t(w) %*% mu  
    * Variance is sum-of (w ** 2) * diag(mtxCov) + 2 * sum-over-m,n-of wm*wn*mtxCov[m, n] = t(w) %*% mtxCov %*% w  
  
Portfolio Risk Budget:  
  
* Assessing the amount of portfolio risk caused by each of the portfolio components  
* Capital allocations and risk allocations can be much different due to both individual volatilities and covariances with other investments  
	* RC(i) = [ w(i) * (Sigma * W)(i) ] / sqrt( t(w) %*% sigma %*% w )  
    * The sum-over-I of RC(i) should be the portfolio volatility  
    * The risk budget is the ratio of RC(i) to the overall portfolio volatility  
  
Example code includes:  
```{r}

# load("./RInputFiles/eq_prices.RData")
# load("./RInputFiles/bond_prices.RData")

retEQ <- c(0.02182, 0.027, 0.03152, 0.01989, 0.01337, 0.01504, -0.01962, 0.01159, 0.0443, 0.03392, -0.01462, -0.03131, 0.01283, 0.03871, 0.01357, -0.03873, -0.01126, -0.06046, -0.02584, -0.00894, 0.04766, 0.01512, -0.08358, -0.00899, 0.01545, -0.09417, -0.16519, -0.06961, 0.0098, -0.08211, -0.10745, 0.08331, 0.09935, 0.05845, -0.00065, 0.07461, 0.03694, 0.03546, -0.01923, 0.06161, 0.0191, -0.03634, 0.03119, 0.06088, 0.01547, -0.07945, -0.05174, 0.0683, -0.04498, 0.08955, 0.0382, 0, 0.06685, 0.0233, 0.03474, 0.00012, 0.02896, -0.01121, -0.01687, -0.02, -0.05498, -0.06942, 0.10915, -0.00406, 0.01045, 0.04637, 0.04341, 0.03216, -0.00668, -0.06006, 0.04058, 0.01183, 0.02505, 0.02535, -0.0182, 0.00566, 0.00893, 0.05119, 0.01276, 0.03797, 0.01921, 0.02361, -0.01334, 0.05168, -0.02999, 0.03165, 0.04631, 0.02964, 0.02593, -0.03525, 0.04552, 0.0083, 0.00695, 0.02321, 0.02064, -0.01344, 0.03946, -0.0138, 0.02355, 0.02747, -0.00254, -0.02963, 0.0562, -0.01571, 0.00983, 0.01286, -0.02031, 0.02259, -0.06095, -0.02552, 0.08506, 0.00366, -0.01728, -0.04979, -0.00083, 0.06727, 0.00394, 0.01701, 0.00348, 0.03647, -0.00083)
retBonds <- c(0.01591, 0.01012, 0.00686, 0.01062, -0.006, -7e-04, 0.01625, -0.00195, 0.0059, -0.00917, -0.00377, 0.01073, 0.01279, 0.00633, 0.01008, 0.01804, -9e-05, 0.02313, -0.0016, 0.0013, 0.00328, -0.01265, -0.00185, 0.00394, 0.00742, -0.0176, -0.02281, 0.03024, 0.06662, -0.01987, -0.01055, 0.01103, 0.00515, 0.0071, 0.00421, 0.01269, 0.0122, 0.01188, 0.00222, 0.01286, -0.01881, 0.01415, 0.00198, -7e-05, 0.00973, 0.01082, 0.01769, 0.00855, 0.01287, 7e-05, 0.00152, -0.00835, -0.00675, -0.00085, 0.0029, -0.00223, 0.01567, 0.0124, -0.00445, 0.01687, 0.0152, 0.00771, 0.00126, -0.00331, 0.01357, 0.00726, -0.00014, -0.00573, 0.00906, 0.01078, -2e-04, 0.01362, 0, 0.00267, -0.00047, 0.00271, -0.00245, -0.00621, 0.00591, 0.00099, 0.00969, -0.02001, -0.01565, 0.00269, -0.00826, 0.01121, 0.00833, -0.00251, -0.00557, 0.01541, 0.00376, -0.00148, 0.00822, 0.01179, -0.00058, -0.00251, 0.0115, -0.00615, 0.01064, 0.00657, 0.00148, 0.02052, -0.00895, 0.00375, -0.00323, -0.00437, -0.01077, 0.00862, -0.00336, 0.00811, 0.00069, -0.00389, -0.00192, 0.01241, 0.00887, 0.00875, 0.00255, 0.00014, 0.01935, 0.00544, -0.00233)
idxDates <- as.Date(c(13390, 13420, 13451, 13481, 13512, 13543, 13571, 13602, 13632, 13663, 13693, 13724, 13755, 13785, 13816, 13846, 13877, 13908, 13937, 13968, 13998, 14029, 14059, 14090, 14121, 14151, 14182, 14212, 14243, 14274, 14302, 14333, 14363, 14394, 14424, 14455, 14486, 14516, 14547, 14577, 14608, 14639, 14667, 14698, 14728, 14759, 14789, 14820, 14851, 14881, 14912, 14942, 14973, 15004, 15032, 15063, 15093, 15124, 15154, 15185, 15216, 15246, 15277, 15307, 15338, 15369, 15398, 15429, 15459, 15490, 15520, 15551, 15582, 15612, 15643, 15673, 15704, 15735, 15763, 15794, 15824, 15855, 15885, 15916, 15947, 15977, 16008, 16038, 16069, 16100, 16128, 16159, 16189, 16220, 16250, 16281, 16312, 16342, 16373, 16403, 16434, 16465, 16493, 16524, 16554, 16585, 16615, 16646, 16677, 16707, 16738, 16768, 16799, 16830, 16859, 16890, 16920, 16951, 16981, 17012, 17043))


returns_equities <- xts(retEQ, order.by=idxDates)
names(returns_equities) <- "equities"
str(returns_equities)
returns_bonds <- xts(retBonds, order.by=idxDates)
names(returns_bonds) <- "bonds"
str(returns_bonds)


# Create a grid
grid <- seq(from = 0, to = 1, by = 0.01)

# Initialize an empty vector for sharpe ratios
vsharpe <- rep(NA, times = length(grid))

# Create a for loop to calculate Sharpe ratios
for(i in 1:length(grid)) {
    weight <- grid[i]
    preturns <- weight * returns_equities + (1 - weight) * returns_bonds
    vsharpe[i] <- SharpeRatio.annualized(preturns)
}

# Plot weights and Sharpe ratio
plot(grid, vsharpe, xlab = "Weights", ylab= "Ann. Sharpe ratio")
abline(v = grid[vsharpe == max(vsharpe)], lty = 3)


# Create a scatter plot
chart.Scatter(returns_bonds, returns_equities)

# Find the correlation
cor(returns_equities, returns_bonds)

# Merge returns_equities and returns_bonds 
returns <- merge(returns_equities, returns_bonds)

# Find and visualize the correlation using chart.Correlation
chart.Correlation(returns)

# Visualize the rolling estimates using chart.RollingCorrelation
chart.RollingCorrelation(returns_equities, returns_bonds, width = 24)


retData <- c(0.0218, 0.027, 0.0315, 0.0199, 0.0134, 0.015, -0.0196, 0.0116, 0.0443, 0.0339, -0.0146, -0.0313, 0.0128, 0.0387, 0.0136, -0.0387, -0.0113, -0.0605, -0.0258, -0.0089, 0.0477, 0.0151, -0.0836, -0.009, 0.0155, -0.0942, -0.1652, -0.0696, 0.0098, -0.0821, -0.1074, 0.0833, 0.0993, 0.0585, -7e-04, 0.0746, 0.0369, 0.0355, -0.0192, 0.0616, 0.0191, -0.0363, 0.0312, 0.0609, 0.0155, -0.0795, -0.0517, 0.0683, -0.045, 0.0896, 0.0382, 0, 0.0669, 0.0233, 0.0347, 1e-04, 0.029, -0.0112, -0.0169, -0.02, -0.055, -0.0694, 0.1091, -0.0041, 0.0104, 0.0464, 0.0434, 0.0322, -0.0067, -0.0601, 0.0406, 0.0118, 0.0251, 0.0254, -0.0182, 0.0057, 0.0089, 0.0512, 0.0128, 0.038, 0.0192, 0.0236, -0.0133, 0.0517, -0.03, 0.0316, 0.0463, 0.0296, 0.0259, -0.0352, 0.0455, 0.0083, 0.007, 0.0232, 0.0206, -0.0134, 0.0395, -0.0138, 0.0236, 0.0275, -0.0025, -0.0296, 0.0562, -0.0157, 0.0098, 0.0129, -0.0203, 0.0226, -0.061, -0.0255, 0.0851, 0.0037, -0.0173, -0.0498, -8e-04, 0.0673, 0.0039, 0.017, 0.0035, 0.0365, -8e-04, 0.0159, 0.0101, 0.0069, 0.0106, -0.006, -7e-04, 0.0163, -0.002, 0.0059, -0.0092, -0.0038, 0.0107, 0.0128, 0.0063, 0.0101, 0.018, -1e-04, 0.0231, -0.0016, 0.0013, 0.0033, -0.0127, -0.0018, 0.0039, 0.0074, -0.0176, -0.0228, 0.0302, 0.0666, -0.0199, -0.0106, 0.011, 0.0052, 0.0071, 0.0042, 0.0127, 0.0122, 0.0119, 0.0022, 0.0129, -0.0188, 0.0141, 0.002, -1e-04, 0.0097, 0.0108, 0.0177, 0.0085, 0.0129, 1e-04, 0.0015, -0.0083, -0.0068, -9e-04, 0.0029, -0.0022, 0.0157, 0.0124, -0.0045, 0.0169, 0.0152, 0.0077, 0.0013, -0.0033, 0.0136, 0.0073, -1e-04, -0.0057, 0.0091, 0.0108, -2e-04, 0.0136, 0, 0.0027, -5e-04, 0.0027, -0.0025, -0.0062, 0.0059, 0.001, 0.0097, -0.02, -0.0157, 0.0027, -0.0083, 0.0112, 0.0083, -0.0025, -0.0056, 0.0154, 0.0038, -0.0015, 0.0082, 0.0118, -6e-04, -0.0025, 0.0115, -0.0061, 0.0106, 0.0066, 0.0015, 0.0205, -0.0089, 0.0037, -0.0032, -0.0044, -0.0108, 0.0086, -0.0034, 0.0081, 7e-04, -0.0039, -0.0019, 0.0124, 0.0089, 0.0087, 0.0025, 1e-04, 0.0194, 0.0054, -0.0023, 0.0348, 0.0197, 0.0603, 0.0478, -0.0184, 0.0878, -0.027, -0.0207, -0.0013, -0.0038, -0.0901, -0.0826, 0.0674, 0.0392, 0.021, -0.0947, -0.0541, -0.0078, -0.0318, 0.0649, 0.0638, -0.0021, -0.1062, 0.0312, 0.0237, -0.0012, -0.3173, -0.2272, 0.1671, -0.1753, -0.2053, 0.0364, 0.3068, 0.0262, -0.032, 0.1077, 0.1432, 0.0659, -0.0446, 0.0657, 0.0739, -0.0552, 0.0558, 0.1019, 0.0715, -0.0533, -0.0521, 0.0959, -0.0128, 0.0447, 0.0474, -0.0185, 0.0455, 0.0325, 0.0471, -0.0161, 0.0575, 0.0137, -0.033, 0.0156, -0.0562, -0.1084, 0.1429, -0.038, 0.0485, 0.0638, -0.0115, 0.052, 0.0286, -0.0451, 0.0552, 0.02, -1e-04, -0.0186, -0.0091, -0.0026, 0.0372, 0.0374, 0.0122, 0.0287, 0.0673, -0.0598, -0.0198, 0.009, -0.0698, 0.0348, 0.0452, -0.0525, 0.001, 0.0428, 0.0507, 0.005, 0.0329, 0.024, 0.0113, 8e-04, 0.0304, -0.0604, 0.0994, 0.02, 0.019, 0.0685, -0.0367, 0.0173, -0.0585, -0.003, -0.0467, 0.0577, -0.0629, 0.0306, 0.0576, -0.0063, 0.0183, -0.0344, -0.0036, 0.1047, -0.0235, 0.0225, 0.0693, 0.0426, 0.0022, -0.0713, -0.1074, -0.0254, 0.05, -0.0692, -0.0255, 0.0371, 0.0247, 0.0046, -0.0118, 0.0313, 0.0478, -0.0319, 0.0944, 0.06, -0.0046, 0.0597, -0.0046, 0.1126, -0.0094, 0.0819, 0.0847, 0.1032, -0.1332, -0.0748, -0.1044, -0.2967, -0.1488, -0.1118, -0.1035, -0.0612, 0.0457, -0.0155, 0.215, -0.0023, 0.0073, 0.0043, -0.0237, 0.0482, 0.0129, 0.0098, -0.0811, 0.0592, 0.0061, 0.0315, -0.136, 7e-04, 0.0576, -0.0548, 0.0803, 0.029, 0.0123, 0.0909, 0.0311, 0.0265, 0.0349, 0.0426, -0.0686, -0.0596, 0.0317, -0.0171, -0.1272, 0.1027, 0.015, -0.024, 0.024, 0.0622, -0.0304, -0.0035, -0.1278, 0.0069, 0.0588, 0.0717, -0.0214, -0.0402, 0.016, -0.0052, 0.0457, -0.051, 0.0111, -0.0489, -0.0153, -3e-04, 0.0565, 0.031, -0.0349, -0.0145, -0.0047, 0.0135, -0.0221, 0.048, -6e-04, 0.01, -0.0051, 0.0238, -0.0566, -0.0156, -0.0606, -0.0595, -0.101, -0.1375, -0.0871, 0.0594, -0.0656, 0.1092, -0.0217, -0.0024, -0.1416, 0.0028, -0.06, -6e-04, -0.0914, -0.0825, -0.0527, -0.0208, 0.0447, 0.1037, 0.0131, 0.0065, -0.1005, -0.0208)
retMtx <- matrix(retData, ncol=4, byrow=FALSE)
returns <- xts(retMtx, order.by=idxDates)
names(returns) <- c("equities", "bonds", "realestate", "commodities")
str(returns)


par(mfcol=c(1, 1))
par(mfrow=c(1, 1))

# Create a vector of returns 
means <- apply(returns, 2, "mean")
  
# Create a vector of standard deviation
sds <- apply(returns, 2, "sd")

# Create a scatter plot
plot(x=sds, y=means)
text(sds, means, labels = colnames(returns), cex = 0.7)
abline(h = 0, lty = 3)


sds <- unname(sapply(returns, 1, FUN=sd))

# Create a matrix with variances on the diagonal
diag_cov <- diag(sds**2)

# Create a covariance matrix of returns
cov_matrix <- cov(returns)

# Create a correlation matrix of returns
cor_matrix <- cor(returns)

# Verify covariances equal the product of standard deviations and correlation
all.equal(cov_matrix[1,2], cor_matrix[1,2] * sds[1] * sds[2])


weights <- c(0.4, 0.4, 0.1, 0.1)
vmeans <- c(0.0070692, 0.0040095, 0.0089495, -0.0082961)

# Create a weight matrix w
w <- as.matrix(weights)

# Create a matrix of returns
mu <- as.matrix(vmeans)

# Calculate portfolio mean monthly returns
t(w) %*% mu

# Calculate portfolio volatility (sigma is the covariance matrix)
# sqrt(t(w) %*% sigma %*% w)
sqrt(t(w) %*% cov_matrix %*% w)


# Create portfolio weights
weights <- c(0.4, 0.4, 0.1, 0.1)

# Create volatility budget
vol_budget <- StdDev(returns, portfolio_method = "component", weights = weights)

# Make a table of weights and risk contribution
weights_percrisk <- cbind(weights, vol_budget$pct_contrib_StdDev)
colnames(weights_percrisk) <- c("weights", "perc vol contrib")

# Print the table
weights_percrisk

```
  
  
  
***
  
Chapter 4 - Optimizing the Portfolio  
  
Modern Portfolio Theory of Harry Markowitz:  
  
* Optimal portfolio weights drive the maximum return given a specific level of volatility  
* Typically, there is a pairing of an objective function and a constraint  
	* Maximize expected return <----> Only non-negative weights  
    * Minimize variance <----> Weights sum to 1 (all capital must be invested)  
    * Maximize Sharpe ratio <----> Portfolio expected return must exceed a target value  
* Markowitz recommends that portfolios be optimized by minimizing the variance, subject to a pre-sepcified target return  
  
Efficient Frontier:  
  
* The efficient frontier shows the maximal expected return for any given level of risk (minimum risk for any given level of target return)  
* The risk-free rate is considered to be the zero return, since it does not make sense to take risk just to get the risk-free rate  
  
In-Sample vs. Out-Sample Evaluations:  
  
* The efficient frontier is an overfot since it is based on in-sample data  
	* "The good news is that this makes portfolio management challenging"  
    * The issue can generally be addressed using the split-sample approach - estimate up to t-1, then assess from t to the end  
  
Example code includes:  
```{r}

returns <- Return.calculate(prices)[-1, ]
str(returns)

 
# Verify the class of returns 
class(returns)

# Investigate the dimensions of returns
dim(returns)

# Create a vector of row means
ew_preturns <- rowMeans(returns)

# Cast the numeric vector back to an xts object
ew_preturns <- xts(ew_preturns, order.by = time(returns))

# Plot ew_preturns
plot.zoo(ew_preturns)


# Load tseries
library(tseries)

# Create an optimized portfolio of returns
opt <- portfolio.optim(returns)

# Create pf_weights
pf_weights <- opt$pw

# Assign asset names
names(pf_weights) <- colnames(returns)

# Select optimum weights opt_weights
opt_weights <- pf_weights[pf_weights >= 0.01]

# Barplot of opt_weights
barplot(opt_weights)

# Print expected portfolio return and volatility
opt$pm
opt$ps


# The function portfolio.optim has arguments that allow for more general specifications. The arguments are as follows:
# portfolio.optim(x, pm = mean(x), shorts = FALSE, reshigh = NULL)
# The argument pm sets the target return, the argument reshigh specifies the upper constraints on the portfolio weights, and the argument shorts is a logical statement specifying whether negative weights are forbidden or not, by default shorts = FALSE

# Create portfolio with target return of average returns 
pf_mean <- portfolio.optim(returns, pm = mean(returns))

# Create portfolio with target return 10% greater than average returns
pf_10plus <- portfolio.optim(returns, pm = 1.1 * mean(returns))

# Print the standard deviations of both portfolios
pf_mean$ps
pf_10plus$ps

# Calculate the proportion increase in standard deviation
(pf_10plus$ps - pf_mean$ps) / (pf_mean$ps)


# Create vectors of maximum weights
max_weights1 <- rep(1, ncol(returns))
max_weights2 <- rep(0.1, ncol(returns))
max_weights3 <- rep(0.05, ncol(returns))

# Create an optimum portfolio with max weights of 100%
opt1 <- portfolio.optim(returns, reshigh = max_weights1)

# Create an optimum portfolio with max weights of 10%
opt2 <- portfolio.optim(returns, reshigh = max_weights2)

# Create an optimum portfolio with max weights of 5%
opt3 <- portfolio.optim(returns, reshigh = max_weights3)

# Calculate how many assets have a weight that is greater than 1% for each portfolio
sum(opt1$pw > .01)
sum(opt2$pw > .01)
sum(opt3$pw > .01)

# Print portfolio volatilites 
opt1$ps
opt2$ps
opt3$ps


# Calculate each stocks mean returns
stockmu <- colMeans(returns)

# Create a grid of target values
grid <- seq(from = 0.01, to = max(stockmu), length.out = 50)

# Create empty vectors to store means and deviations
vpm <- vpsd <- numeric(length(grid))

# Create an empty matrix to store weights
mweights <- matrix(NA, 50, 30)

# Create your for loop
for(i in 1:length(grid)) {
    opt <- portfolio.optim(x = returns, pm = grid[i])
    vpm[i] <- opt$pm
    vpsd[i] <- opt$ps
    mweights[i, ] <- opt$pw
}


# Create weights_minvar as the portfolio with the least risk
dimnames(mweights)[[2]] <- names(prices)
weights_minvar <- mweights[vpsd == min(vpsd), ]

# Calculate the Sharpe ratio
vsr <- (vpm - 0.0075) / vpsd

# Create weights_max_sr as the portfolio with the maximum Sharpe ratio
weights_max_sr <- mweights[vsr == max(vsr), ]

# Create barplot of weights_minvar and weights_max_sr
par(mfrow = c(2, 1), mar = c(3, 2, 2, 1))
barplot(weights_minvar[weights_minvar > 0.01])
barplot(weights_max_sr[weights_max_sr > 0.01])
par(mfrow = c(1, 1))


# Create returns_estim 
returns_estim <- window(returns, start = "1991-01-01", end = "2003-12-31")

# Create returns_eval
returns_eval <- window(returns, start = "2004-01-01", end = "2015-12-31")

# Create vector of max weights
max_weights <- rep(0.1, ncol(returns))

# Create portfolio with estimation sample 
pf_estim <- portfolio.optim(returns_estim, reshigh = max_weights)

# Create portfolio with evaluation sample
pf_eval <- portfolio.optim(returns_eval, reshigh = max_weights)

# Create a scatter plot
plot(pf_estim$pw, pf_eval$pw)
abline(h = 0, b = 1, lty = 3)


# Create returns_pf_estim
returns_pf_estim <- Return.portfolio(returns_estim, pf_estim$pw, rebalance_on = "months")

# Create returns_pf_eval
returns_pf_eval <- Return.portfolio(returns_eval, pf_estim$pw, rebalance_on = "months")

# Print a table for your estimation portfolio
table.AnnualizedReturns(returns_pf_estim)

# Print a table for your evaluation portfolio
table.AnnualizedReturns(returns_pf_eval)


```
  
  
  
***
  
### _Intermediate Portfolio Analysis in R_  
  
Chapter 1 - Introduction to Portfolio Theory  
  
Introduction:  
  
* Advanced concepts in portfoli optimization - building on Markowitz Portfolio Theory  
* Example for setting up Mean-Standard Deviation  
	* library(PortfolioAnalytics)  
    * data(edhec)  
    * myData <- edhec[, 1:8]  
    * port_spec <- portfolio.spec(colnames(myData))  
    * port_spec <- add.constraint(portfolio=port_spec, type="full_investment")  # must invest all money  
    * port_spec <- add.constraint(portfolio=port_spec, type="long_only")  # no shorting  
    * port_spec <- add.objective(portfolio=port_spec, type="return", name="mean")  # return is based on mean EV  
    * port_spec <- add.objective(portfolio=port_spec, type="risk", name="StdDev")  # risk is based on standard deviation  
    * opt <- optimize.portfolio(myData, portfolio=port_spec, optimize_method="random", trace=TRUE)  
    * chart.RiskReward(opt, risk.col="StdDev", return.col="mean", chart.assets=TRUE)  
  
Challenges of portfolio optimization:  
  
* Many solvers are not specific to portfolio optimization - ideal is to have effortless switching between solvers, with evaluation of multiple approaches  
	* Closed-form solvers come to solutions quickly, have a precise solution, but require that the data and approach configure to a pre-defined model  
    * Global solvers can handle many types of data and modeling, but may not come to a precise or quick solution  
* Quadratic utility is designed to maximize t(w) %*% mu - lambda * t(w) %*% Sigma %*% w, subject to all w are greater than 0 and the sum of w are equal to 1  
	* Lambda is the risk-aversion parameter; higher values mean higher penalties for variance  
* quadprog::solve.QP() will solve for problems like this  
	* min(-t(d) %*% b + 0.5 * t(b) %*% D %*% b) subject to t(A) %*% b >= b0  
    * Not very portable, not see easy to visualize, solves quickly and precisely  
  
Introduction to PortfolioAnalytics:  
  
* Provides numerical solutions and visualizations for portfolio optimization problems with complex constraints and objectives  
	* Supports multiple and modular constraint and objective types  
    * Objective function can be any valid R function - not limited to anything specific, can user define  
    * User-defined moment functions  
    * Visualizations  
    * Choices of solver, and ability to run various solvers without any changes to anything else  
* Inputs (Assets, Constraints, Objectives, Asset Return Moments) ----> Optimization (Engine) ----> Output (Optimal Portfolio)  
  
Example code includes:  
```{r}

# Load the package
library(PortfolioAnalytics)
library(ROI)


# Load the data
data(indexes)

# Subset the data
index_returns <- indexes[, 1:4]

# Print the head of the data
head(index_returns)


# The portfolio problem is to form a minimum variance portfolio subject to full investment and long only constraints
# The objective is to minimize portfolio variance
# There are two constraints in this problem: the full investment constraint means that the weights must sum to 1, and the long only constraint means that all weights must be greater than or equal to 0 (i.e. no short positions are allowed).

# Create the portfolio specification
port_spec <- portfolio.spec(colnames(index_returns))

# Add a full investment constraint such that the weights sum to 1
port_spec <- add.constraint(portfolio = port_spec, type = "full_investment")

# Add a long only constraint such that the weight of an asset is between 0 and 1
port_spec <- add.constraint(portfolio = port_spec, type = "long_only")

# Add an objective to minimize portfolio standard deviation
port_spec <- add.objective(portfolio = port_spec, type = "risk", name = "StdDev")

# Solve the optimization problem
opt <- optimize.portfolio(index_returns, portfolio = port_spec, optimize_method = "ROI")


# Print the results of the optimization
opt

# Extract the optimal weights
extractWeights(opt)

# Chart the optimal weights
chart.Weights(opt)


# Create the portfolio specification
port_spec <- portfolio.spec(assets = colnames(index_returns))

# Add a full investment constraint such that the weights sum to 1
port_spec <- add.constraint(portfolio = port_spec, type = "full_investment")

# Add a long only constraint such that the weight of an asset is between 0 and 1
port_spec <- add.constraint(portfolio = port_spec, type = "long_only")

# Add an objective to maximize portfolio mean return
port_spec <- add.objective(portfolio = port_spec, type = "return", name = "mean")

# Add an objective to minimize portfolio variance
port_spec <- add.objective(portfolio = port_spec, type = "risk", name = "var", risk_aversion = 10)

# Solve the optimization problem
opt <- optimize.portfolio(R = index_returns, portfolio = port_spec, optimize_method = "ROI")

```
  
  
  
***
  
Chapter 2 - Portfolio Optimization Workflow  
  
Portfolio specification, constraints, and objectives:  
  
* Portfolio specifications are created using portfolio.spec(assets=, …)  # this is an S3 object, and the assets should be either the asset names as a vector, a named vector of asset weights, or an integer for the number of assets  
* Portfolio constraints are added using add.constraints(portfolio, type=c("weight_sum", "box", "full_investment", …))  # note that full_investment is a special case of weight_sum with min_sum=1 and max_sum=1  
* Portfolio objectives are added using add.objective(portfolio, type=c("return", "risk", …), name, arguments=NULL, …)  # the most common objective types are return (maximize) and risk (minimize)  
	* The name= parameter should be a quoted R function such as mean or ES or the like  
    * The arguments= parameter passes argument to the function defined in name=  
    * The objectives are modular, and each occupies a slot in the S3 object  
  
Running optimizations:  
  
* Single period optimization can be run with optimize.portfolio(R=, portfolio=, optimize_method=)  # R is the returns data, portfolio is the portfolio.spec() object, optimize method can be "ROI"  
	* Backtesting can be run using optimize.portfolio.rebalancing(R=, portfolio=, optimize_method=, rebalance_on=, training_period=, rolling_window=)  # rebalance_on="years", training_period=60, rolling_window=60  
* Supports both Global Solvers and LP/QP Solvers  
  
Analyzing optimization results:  
  
* There are both visualization and data extraction capabilities  
	* Visualization - plot(), chart.Concentration(), chart.EfficientFrontier(), chart.RiskReward(), chart.RiskBudget(), chart.Weights()  
    * Data Extraction - extractObjectMeasures(), extractStats(), extractWeights(), print(), summary()  
  
Example code includes:  
```{r}

retData <- c(0.0119, 0.0123, 0.0078, 0.0086, 0.0156, 0.0212, 0.0193, 0.0134, 0.0122, 0.01, 0, 0.0068, 0.0145, 0.0146, 0.0144, 0.0126, 0.0056, -6e-04, 0.006, -0.0319, -0.0196, -0.0214, 0.0269, 0.0113, 0.0219, 0.0082, 0.0136, 0.0243, 0.0166, 0.0102, 0.0101, 0.0048, 0.0096, 0.0045, 0.0124, 0.014, 0.0227, 0.0267, 0.0243, 0.0223, 0.0149, 0.0179, 0.0093, 0.0162, 0.0141, 0.0052, -0.0081, -2e-04, 0.0344, 0.0182, 0.0162, 0.0157, 0.0033, 0.0012, 0.0091, 0.0142, 0.0078, 0.0117, 0.008, -0.0094, 0.0148, -0.0049, 0.0053, 0.0096, 0.0033, 4e-04, -0.0159, 0.005, 0.0146, 0.0104, 0.0251, 0.0157, 0.0283, 0.0133, 0.0089, 0.015, 0.0136, -0.0058, -0.0072, -0.0087, 0.0171, 0.0146, 0.0092, 0.0054, 0.0119, 0.0017, 0.0061, 0.002, -0.0128, -0.0106, 0.0013, 0.004, -0.0017, -0.0044, 0.0081, 0.0056, -0.0096, -0.0058, -0.014, -0.0316, -0.0133, 0.0107, 0.0164, 0.0066, 0.0142, -0.0015, 4e-04, 0.0092, 0.025, 0.0116, 0.0107, 0.0064, 0.0091, 0.0012, 0.0066, 0.0098, 0.0093, 0.0054, 0.0092, 0.0127, 0.013, 0.0117, 0.006, 0.0026, 0.011, 0.0011, -0.0053, -0.0145, 0.0161, 0.0177, -0.0131, -0.0077, -9e-04, -0.0083, -0.0317, 0.0076, 0.0107, -0.0081, -0.0188, -0.0066, -0.1027, -0.1237, -0.0276, 0.0177, 0.0491, 0.0164, 0.0235, 0.05, 0.0578, 0.0241, 0.0611, 0.0315, 0.0393, 0.0298, -0.0021, -0.017, -0.0015, 0.0085, 0.0591, -0.0473, 0.0198, -0.0098, 0.0133, 0.0286, 0.0104, -0.0065, 0.0122, -0.0296, 0.0193, 0.0051, -0.001, 0.0691, 0.0454, 4e-04, -0.0089, 0.0221, -0.0167, 0.0197, -0.0065, 0.021, -0.015, 0.0234, -0.0051, -0.0027, 0.0064, -0.0354, 0.0166, 0.0142, 0.0128, -0.0022, -0.0138, -0.0241, 0.0114, -0.0124, -0.0131, 0.0189, -0.0208, 0.0075, 0.0425, 0.0682, 0.0025, -0.0016, 0.0438, -0.0362, 0.0081, -0.0077, -0.004, 0.0153, 0.0246, 0.0336, -0.0543, 0.0148, -0.0072, -0.0202, 9e-04, -0.0104, 0.027, 0.0655, 0.0413, 0.022, 0.0284, -0.0376, -0.0164, 0.0489, 0.0441, 0.0402, -0.0445, 0.0065, 0.049, -0.0192, -0.0171, 0.0078, -0.0019, 0.0104, 0.0018, 0.0381, 0.0199, 0.0529, -0.0051, -0.0532, -0.0118, -0.0316, -0.0119, -0.0084, 0.022, 0.0358, 0.0475, 0, -0.0438, 5e-04, -6e-04, -0.0354, 0.0232, 0.026, -0.0013, 0.01, 0.0079, -0.0092, 0.0379, -0.0153, 0.0174, -0.0186, 0.0284, 0.0387, -0.0146, -0.0142, -0.0216, 0.002, -0.0055, 0.0102, 0.0226, 0.0146, 0.0113, -0.0144, -0.0141, 0.0241, 0.023, 0.0229, -0.0122, -0.028, 0.0469, 0.028)
retData <- c(retData, -0.0016, 0.0117, 0.0255, 0.062, -0.0056, -0.0078, 0.0162, 0.033, -0.0333, -0.0114, 0.001, 0.0345, 0.0214, 0.014, -0.0016, -0.0031, -0.018, -0.014, 0.0213, -0.0147, -0.0012, 0.0054, 0.0178, 0.0122, -0.0012, 0.003, 0.0233, 0.0217, 0.0234, 0.0147, 0.035, -0.0064, 0.0054, 0.0073, 0.0095, 0.0227, 0.0252, 0.0165, 6e-04, -0.0047, -0.0069, -0.0836, -0.0215, -0.0029, 0.0164, 0.0108, 0.0181, -0.0021, 0.0159, 0.0418, 0.0207, 0.0273, 0.0084, 0.002, -0.0041, 0.0027, 0.022, 0.03, 0.0088, 0.0421, 0.0103, -0.0101, -0.0132, 0.0203, 0.0064, 0.014, -0.0019, -0.0073, -0.0209, 1e-04, 0.0308, 0.01, -0.0037, 0.0048, 0.0235, 0.036, 0.0073, 0.0106, -0.0014, 0.0103, 0.0086, 0.0015, 0.0186, -0.0033, 0.0052, 0.0139, 0.0091, -0.0117, -0.0133, 9e-04, -0.0044, -0.0031, 0.0239, 0.0222, 0.0243, 0.0092, 0.0113, 0.0345, 0.027, 0.0267, 0.0117, 0.0137, 0.0242, 0.0267, 0.0154, 0.0198, 0.0301, 0.0075, 0.0046, 0.0093, -0.001, 0.0202, 0.0019, 0.0088, 0.0104, 0.0143, 0.0337, 0.0266, 0.0037, 0.0134, 0.0032, -0.0052, 6e-04, 0.0133, 0.0173, 0.0124, 0.0112, -0.0032, 0.01, 0.0122, 0.0253, 0.0065, 0.0172, 0.0193, 0.0086, -0.0015, 9e-04, 0.0099, 0.0033, 0.0194, 0.0179, 0.0165, 0.015, 0.0145, 0.0108, 0.0164, 0.018, 0.0027, -0.0056, -0.0118, 0.0095, 0.0175, -0.0169, 2e-04, -0.0233, 0.0014, -0.0126, 0.0088, 0.0137, -0.0031, -0.0182, -0.0072, -0.0518, -0.0775, -0.0435, -0.0197, 0.0082, -0.0122, 0.0022, 0.0387, 0.0504, 0.0198, 0.0311, 0.0244, 0.0791, 0.0525, -0.012, 0.0119, 0.0315, 0.0581, 0.056, -0.0066, 0.0229, -0.0572, -0.0378, 0.016, -0.0429, 0.0339, 0.0318, 0.0041, -0.0825, -0.0422, 0.0019, -0.1922, -0.0395, 0.014, 0.043, -0.0098, -0.012, 0.0102, 0.0585, 0.063, 0.0061, 0.0654, -0.0061, -0.0147, -0.0069, 0.0288, 0.0692, 0.123, 0.0077, 0.0528, 0.0318, -0.0541, -0.0433, 0.0334, 0.0025, 0.0368, -0.0462, -0.0256, -0.0385, 0.0116, 0.0586, -0.0221, -0.0175, 0.0114, 0.0278, 0.016, -0.0286, 0.003, -0.0425, 0.0278, 0.0483, 0.0421, 0.0273, 0.0181, 0.0331, 0.0144, 1e-04, -0.0292, -0.0309, 0.0119, -0.0252, 0.0154, 0.019, 0.0048, 0.0012, 0.0084, 0.0019, 0.045, 0.0433, 0.0268, 0.0104, 0.0374, 0.0264, 0.0259, 0.0096, 0.0403, 0.0251, 0.0253, 0.0172, -0.0252, -0.0181, 0.002, -0.0027, 0.0133, 0.028, 0.0185, 0.0328, 0.0201, 0.0143, 0.0346, -0.0197, -0.0049, 0.0072, 0.016, 0.0257, 0.0152, 0.0402, -0.023, 0.0279, 0.0284, 0.0526, 0.0161, 0.0122, 0.0365, -0.0389, -0.0097, 0.0067, 0.0133, 0.0011, 0.0257, 0.0323, 0.0291, 0.0079, 0.01, 0.0185, 0.0255, 0.027, 0.0236, 0.0275, -0.0274, 0.0428, 0.0485, -0.0237, 0.013, -0.0503, 0.028, -0.0379, 0.019)
retData <- c(retData, 0.0163, -0.0274, -0.033, -0.0336, -0.0982, -0.1331, -0.0391, -0.001, -0.0112, -0.0133, 0.035, 0.0663, 0.0884, 0.0013, 0.0451, 0.0166, 0.0189, 0.0101, 0.0016, 0.0119, 0.0189, 0.0165, 0.0247, 0.0017, 0.0202, 0.0095, 0.0041, 0.0066, 0.006, 0.0135, 0.0179, 0.0067, 0.008, 0.0108, 0.0012, -0.0107, 0.0061, 0.0052, 0.0158, 0.0209, 0.0101, 0.0023, 0.0033, 0.0107, 0.0089, 0.0168, 0.0135, 0.0095, 0.0095, 0.0066, 0.0133, 0.0198, 0.0075, 0.0253, 0.0134, 0.0168, 0.0062, 0.0171, 0.0063, 0.021, 0.0058, 0.004, 0.0045, 0.016, 0.0075, 0.012, 0.0108, 0.0075, 0.0077, 0.0017, 0.0031, 0.0094, 0.0023, 0.0058, 0.0055, 0.0056, 0.0065, -7e-04, 0.0047, 0.0076, 0.0053, 0.0022, -0.0013, 0.0069, 0.0015, 0.0016, 0.0025, 0.0094, 0.0083, 0.0024, 0.0015, 0.0031, 0.0107, 0.0034, -6e-04, 0.0031, 0.0078, 0.0115, 0.0046, 0.0054, 0.0109, 0.0063, 0.0032, -0.0082, 0.0024, 0.0042, 6e-04, -9e-04, 0.0085, -5e-04, 0.014, 0.0058, 0.0081, 0.008, 0.0019, -0.003, 0.0047, 0.0081, 0.0078, 0.0062, 0.0087, 1e-04, 0.0061, 0.0068, 0.0115, 0.0046, 0.0098, 0.0102, 2e-04, 0.0063, 0.0051, -9e-04, 9e-04, 0.0065, 0.0075, 0.0107, 0.0083, 0.0051, 0.0101, 0.0089, 0.0121, 0.0077, 0.0051, -0.0094, 0.0123, 0.0168, -0.0018, 0.0054, -0.0112, 0.012, -0.0049, 0.0059, 0.0126, 0.0156, -0.01, -0.0135, -0.0285, -0.0044, -0.0587, 5e-04, 0.0079, -0.0046, 0.0021, -0.0012, 0.0146, 0.0036, 0.0042, 0.007, 0.0213, 0.0084, -0.0023, -5e-04, 0.0346, 0.0258, 0.0307, 0.0071, 0.0329, 0.0061, 0.0134, 0.0154, 0.0055, 0.0294, 0.0263, 0.0104, -0.0083, 2e-04, -0.0037, -0.0886, -0.011, 0.0091, 0.0244, 0.0219, 0.0201, -0.0042, 0.0193, 0.0429, 0.0215, 0.0297, 0.0096, -0.0027, 0.009, 0.0054, 0.0284, 0.0286, 0.0088, 0.0346, 0.0069, -0.0059, -0.0034, 0.0268, 0.0057, 0.0173, 0.0048, -0.0068, -0.0136, 0.0127, 0.0298, 0.0045, -0.0042, 0.011, 0.0185, 0.0063, 0.0049, 0.009, -0.0254, 0.0148, 0.0105, 0.0107, 0.0078, -0.0071, 0.0153, 0.0046, 1e-04, -0.0283, -0.03, 0.006, -0.007, 0.0031, 0.0216, 0.0044, 0.0154, 0.0026, 0.0083, 0.0272, 0.0301, 0.0181, 0.0119, 0.0133, 0.0133, 0.0191, 0.0116, 0.0172, 0.0234, 0.0113, 0.0016, 2e-04, -0.0023, 0.0113, -0.0082, 0.0035, 0.0103, 0.0124, 0.0306, 0.0244, 4e-04, 0.0144, -4e-04, -0.0128, 0.0065, 0.0133, 0.0215, 0.0092, 0.01, -0.0173, 0.0125, 0.0142, 0.0341, 0.0051, 0.0185, 0.0164, 8e-04, 0.0012, -0.0011, 0.0112, 0.0035, 0.0206, 0.0182, 0.0168, 0.0201, 0.0207, 0.0146, 0.0197, 0.0213, -7e-04, -0.0032, -0.0144, 0.0134, 0.0214, -0.0202, 7e-04, -0.0271, 0.0084, -0.0168, 0.0118, 0.0176, -0.0113, -0.0166, -0.0025, -0.0627, -0.0625, -0.0301, -0.0071, 0.0132, -0.0091, 0.0117, 0.0337, 0.0442, 0.0123, 0.0291, 0.0207, 0.0191, 0.0122, 0.0109, 0.013, 0.0118, 0.0108, 0.0095, 0.0087, 0.0119, -0.0032, 0.0053, 0.0079, -0.0026, 0.0098)
retData <- c(retData, 0.0128, 0.0075, 0.004, -0.008, 0.0106, -0.0143, -0.0362, -0.0801, 0.0052, 0.012, 0.0158, 0.0208, 0.016, 0.0106, 0.0072, 0.0088, 0.0051, -0.0028, 0.0092, 0.0087, 0.0106, 0.0097, 0.0041, 0.0097, -0.0061, -6e-04, 0.0107, 0.0058, 0.0018, 0.0107, 0.0076, 6e-04, 0.0066, 0.0048, 0.0163, 0.0054, 0.0051, 0.0094, 0.0068, 0.0017, 0.0054, 0.0105, -0.0013, 0.0134, -0.0024, 0.0053, 0.0086, 0.0056, 0.0045, 0.0113, 0.0099, 0.0069, 0.0057, 0.0097, -0.0033, -0.0063, 0.0054, 0.0153, 0.0106, 0.0079, 0.0019, 0.0091, 0.0207, 0.0044, -0.0092, 0.0043, 0.0105, 0.0035, 0.0069, 0.0101, 0.0092, 0.0084, 3e-04, 0.0062, 0.004, 0.0055, 0.0062, 0.0036, 0.0012, 0.0028, 0.0075, 0.006, 0.0044, 0.0085, 0.0024, -3e-04, -0.001, 0.001, 0.0081, 0.0036, 0.0062, 0.0057, 0.0015, 0.0054, 0.0093, 0.0041, 0.0055, 0.0121, 0.0059, 0.0036, 0.0064, 0.0037, 0.0014, 0.0067, 0.006, 0.0072, 0.0069, 0.0106, 0.006, 0.0071, 0.0055, 0.0048, 7e-04, -0.0048, 0.0164, 0.0114, -0.0094, 0.0036, -0.0012, -0.0049, -0.0306, 0.0187, 0.0103, -0.0027, -0.0023, -3e-04, -0.0506, -0.0867, -0.0308, -0.0035, 0.0112, 0.0065, 0.0057, 0.0221, 0.0365, 0.0126, 0.0322, 0.0202, 0.0573, 0.0175, -0.0119, 0.0172, 0.0108, 0.0218, 0.0738, -0.018, 0.029, -0.0142, 0.0106, 0.0264, -0.005, 0.0128, 0.057, 0.0034, 0.0095, 0.012, 0.0058, -0.0263, -0.0059, -0.0223, 0.0194, 0.0233, 0.0086, -0.0111, 0.0024, 0.0329, -0.0055, 0.0214, -0.0018, -0.0061, -2e-04, 0.0073, 0.0405, 0.0612, 0.0021, 0.0408, -0.0104, -0.0304, -0.007, 0.0154, 0.0037, 0.0248, -0.0149, -0.0024, 0.0125, 0.0472, 0.0214, -0.0072, 0.0038, 0.0049, 0.0032, 0.0017, -0.004, 6e-04, -0.007, 0.0208, 0.0021, 0.0138, 0.0069, -0.0035, 0.0064, 0.0098, 0.0123, -0.0022, -0.0078, 0.0063, 0.0054, -0.0086, 0.0047, 0.0192, 0.0182, 0.0166, -0.0122, 0.0117, 0.0397, 0.0056, -0.0035, 0.0202, 0.0215, 0.0111, 0.0031, 0.0293, 0.0117, 0.015, 0.0064, -0.0178, -0.0081, -0.0019, -0.0014, -0.0039, 8e-04, 0.0138, 0.028, 0.0033, -0.0047, 0.0171, -0.0027, -0.008, 0.0088, 0.0116, 0.0119, 0.0083, 0.0269, -0.0074, 0.0164, 0.0135, 0.0258, 2e-04, 0.0094, 0.0238, -0.0155, -0.0015, 6e-04, -0.0039, -0.0067, 0.0097, 0.0199, 0.0116, 0.0061, 0.0018, 0.0027, 0.0152, 0.0192, 0.0107, 0.0116, -0.0116, 0.033, 0.0304, -0.0063, 0.0104, -0.001, 0.0312)
retData <- c(retData, -0.0169, 0.0078, 0.0114, 0.003, -0.0213, -0.0133, -0.0313, -0.0157, 0.0033, 0.0118, 0.0029, -0.0055, 0.0048, 0.0127, 0.0348, -0.0076, 0.0166, 0.005, 0.0281, -6e-04, -0.0084, 0.0084, 0.0394, 0.0223, 0.0454, 0.0107, 0.0429, 0.001, -0.0026, 0.0104, 0.0013, 0.0342, 0.0336, 0.012, -0.0087, 0.0167, -6e-04, -0.0552, 0.0206, 0.0169, 0.0291, 0.0408, 0.0258, -0.0169, 0.0229, 0.0312, 0.0095, 0.0315, 0.0177, 0.0022, 0.0113, 0.0212, 0.0481, 0.0745, 0.0075, 0.0699, 6e-04, -0.0201, -0.0097, 0.0349, 6e-04, 0.0345, -0.0016, -0.0084, -0.0153, 0.0248, 0.0165, -0.0264, -0.0199, 0.0246, 0.0043, 0.0019, -0.0144, -0.0096, -0.0348, 0.0099, 0.02, 0.018, -0.0037, -0.0123, 0.0155, -0.0042, -0.0034, -0.0249, -0.0389, 0.0041, -0.016, 0.0123, 0.0224, -0.0149, 5e-04, -0.0037, 0.002, 0.0298, 0.0362, 0.0128, 0.0118, 0.0179, 0.0094, 0.0299, 0.013, 0.0191, 0.0192, 0.0123, 0.0041, -0.0165, -0.0035, 0.0091, -0.0154, -0.0022, 0.021, 0.0074, 0.0308, 0.0178, -0.0017, 0.021, -0.0096, -0.0184, 0.0115, 0.0195, 0.0265, 0.0097, 0.0222, -0.0174, 0.0211, 0.0249, 0.0381, 0.0016, 0.0238, 0.0172, -0.0248, -0.0062, -0.0031, 0.0114, 5e-04, 0.0194, 0.02, 0.0153, 0.0121, 0.0082, 0.0115, 0.0198, 0.0224, 0.0077, 9e-04, -0.016, 0.0256, 0.0281, -0.0225, 0.0043, -0.04, 0.014, -0.0236, 0.0223, 0.0227, -0.0164, -0.0261, -0.0146, -0.0675, -0.0629, -0.0188, 0.0081, -0.0017, -0.0161, 0.0188, 0.0375, 0.0516, 9e-04, 0.0277, 0.0157, 0.015, 0.0034, 0.006, -1e-04, 0.0197, 0.0231, 0.02, 0.0079, 0.0197, 0.0094, 0.0223, 0.0158, 0.0055, 0.0212, 0.0164, 0.0139, -9e-04, 0.0072, 7e-04, -0.0544, 0.0076, 0.0159, 0.022, 0.0224, 0.0112, 0.0036, 0.0133, 0.0218, 0.021, 0.0222, 0.0147, 0.005, 0.0116, 0.0096, 0.0237, 0.009, 0.0143, 0.0239, 0.0131, 0.0188, 0.0146, 0.0167, 0.0116, 0.0157, 0.0137, 0.0026, 0.0102, 0.0125, 0.0111, 0.0054, -0.0061, 0.0058, 0.0161, -0.0087, 0.0079, 0.0099, -0.0267, 0.0085, 0.0014, 0.0045, 0.0077, -0.0044, 0.0073, -0.0013, 0, -0.017, -0.0174, 0.0061, -0.0028, 0.0032, 0.0054, 0.0046, 0.004, 0.0018, -7e-04, 0.0099, 0.0154, 0.0048, 0.0053, 0.007, 0.0077, 0.0111, 0.0044, 0.0098, 0.0097, 0.0051, 0.0017, -0.0039, 0, 0.0017, -0.0092, 0.0011, 0.0042, 0.0074, 0.0164, 0.0133, 0, 0.0065, 0.0032, -0.0105, 0.0095, 0.0085, 0.0115, 0.0061, 0.0035, -0.0145, 0.0112, 0.0138, 0.0272, 0.0104, 0.0144, 0.0119, 9e-04, 0.0087, 0.0058, 0.0053, 0.0041, 0.0132, 0.0142, 0.0133, 0.0191, 0.0255, 0.0063, 0.016, 0.0171, -0.0053, -0.0054, 1e-04, 0.0131, 0.0191, -0.0149, -0.0025, -0.0126, 0.006, -0.0045, 0.0149, 0.0136, -0.0109, 0.0011, 0.0051, -0.0276, -0.0245, 6e-04, 0.0162, 0.0056, 6e-04, 0.0125, 0.0081, 0.0107, 0.0104, 0.0068, 0.0102, 0.018, 0.0118, 0.001, 0.0122, 0.0173, 0.0198, 0.0181, 0.0103, 0.0183, 0.0079, 0.0111, 0.0082, 0.0132, 0.013)
retData <- c(retData, 0.0145, 0.0145, 0.0053, 0.0026, 0.0011, -0.0341, 5e-04, -0.014, 0.0198, 0.0164, 0.0195, 0.0085, 0.0116, 0.0238, 0.0146, 0.0148, 0.011, 0.0062, 0.0105, 0.007, 0.0137, 0.0183, 0.0173, 0.0185, 0.0163, 0.0092, 0.008, 0.0176, 0.0084, 0.0157, 0.0075, -4e-04, 6e-04, 0.0075, 0.0333, 0.003, -0.0011, 0.0174, 0.0141, 0.0019, 0.001, -0.0031, -0.0221, 0.0164, 0.0136, 0.0097, 0.0097, -0.0011, 0.0145, 0.007, 0.0031, -0.0107, -0.0185, 0.0058, -0.011, 0.0084, 0.0185, 0.0023, 0.0067, -4e-04, 0.0049, 0.0186, 0.0212, 0.0071, 0.0041, 0.0058, 0.0086, 0.0159, 0.0102, 0.0127, 0.0146, 0.0057, 0.0038, -0.0045, -0.0037, 0.0022, 7e-04, 0.0031, 0.0052, 0.004, 0.0149, 0.0099, 0.0012, 0.0081, -0.0042, -0.0108, -2e-04, 0.0095, 0.0149, 0.0053, 0.0122, -0.0038, 0.0067, 0.0126, 0.0238, 0.0073, 0.0157, 0.0126, -0.0025, 0.0021, 0.0017, 0.0092, 0.004, 0.0132, 0.0129, 0.0128, 0.0135, 0.0114, 0.0081, 0.0134, 0.0156, 0.01, 4e-04, -0.0077, 0.0153, 0.02, -0.0112, 0.0022, -0.0118, 0.0064, -0.0162, 0.013, 0.0159, -0.0084, -0.0125, -0.0023, -0.0538, -0.0692, -0.0209, 0.0031, 0.01, -0.0016, 0.01, 0.0342, 0.0392, 0.0101, 0.026, 0.0162, -0.0166, 0.0426, 0.0778, -0.0129, -0.0737, -0.0065, -0.0429, -0.0072, -0.0155, 0.0572, 0.0217, 0.0161, 0.0014, 0.0155, 0.0637, 0.0657, 0.1437, -0.0053, 0.0343, 0.2463, -0.0376, -0.1077, -0.0756, -0.0531, -0.0665, 0.0833, -0.0154, -0.0375, 9e-04, -0.0412, 0.0092, 0.0468, 0.0401, -0.013, -0.1239, -0.1137, 0.0427, -0.134, -0.023, 0.1028, 0.0704, -0.1107, 0.0553, -0.1135, 0.1204, 0.0784, 0.1657, 0.0063, -0.0271, 0.1021, 0.062, -0.0991, -0.013, 0.011, 0.0353, 0.0752, 0.0941, -0.0298, -0.0655, -0.0251, 0.0343, 0.039, -0.0446, 0.0483, 0.0346, 0.0548, 0.0644, 0.0015, 0.0731, -0.0405, -0.0547, 0.0443, 0.0162, 0.013, -0.0075, -0.0656, -0.0499, -0.0162, -0.0361, -0.0354, 0.0136, -0.0656, -0.0136, -0.0178, -0.009, 0.0018, -0.0148, 0.0384, -0.0024, -0.0051, 0.0638, 0.0126, -0.0216, -0.0092, -0.0574, -0.0391, 0.0387, 0.0118, 0.0244, 0.0393, -0.0475, -0.0032, -0.0242, 0.0259, 0.0198, 0.0233, -0.03, -0.0035, -0.0288, 0.0064, -0.0139, -0.0012, 0.0246, 0.0118, 0.0173, -0.0156, -0.0236, -0.038, -0.0268)
retData <- c(retData, 0.0039, -0.0107, 0.0028, -0.0051, -0.0265, -0.0199, 0.0236, 0.0486, 0.0092, -0.0207, -0.0026, 0.0719, 0.0056, 0.0556, 0.03, 0.0192, -0.0461, -0.0142, 0.0751, 0.0072, -0.0215, 0.0378, 0.117, 0.0428, -0.0146, 0.0282, 0.0328, -0.0462, -0.082, 8e-04, -0.0094, -0.0596, -0.0165, 0.0317, 0.0106, -0.0077, 9e-04, 0.0275, 0.0225, 0.0435, 0.0051, 0.0334, -0.0099, -0.0034, 0.0089, -0.0036, 0.0256, 0.0373, 0.0125, -0.0072, 0.0021, -7e-04, -0.0616, -0.0037, -2e-04, 0.022, 0.0222, 0.0202, -0.0063, 0.0213, 0.04, 0.0119, 0.0282, 0.0088, 0.0028, 0.0052, 0.013, 0.0483, 0.0622, 0.0169, 0.0666, 0.0039, -0.0269, -0.0122, 0.0311, -0.0022, 0.0267, -0.0069, -0.0104, -0.0205, 0.0133, 0.0223, -0.0089, -0.0068, 0.0104, 0.008, 0.0013, -0.004, 0.0019, -0.0142, 0.0095, 0.0058, 0.0099, 0.003, -0.0015, 0.009, 0.0052, 0.005, -0.0095, -0.014, 0.0037, -0.0033, -0.0031, 0.0106, 0.0077, 0.0072, 0.0031, -4e-04, 0.0134, 0.0205, 0.0068, 0.0025, 0.0078, 0.0121, 0.0152, 0.007, 0.0139, 0.0156, 0.0111, 0.0043, -0.0068, -0.0082, 0.0034, -0.0049, -0.001, 0.0099, 0.0068, 0.0244, 0.0145, 6e-04, 0.0136, -0.0044, -0.0141, 0.0018, 0.0131, 0.0134, 0.0079, 0.0147, -0.0149, 0.016, 0.0191, 0.0286, 0.0037, 0.0164, 0.0171, -0.0133, -0.0028, -5e-04, 0.0066, -3e-04, 0.0163, 0.0185, 0.0175, 0.0121, 0.0096, 0.0096, 0.0163, 0.0204, 0.0082, 0.0041, -0.0222, 0.0199, 0.0303, -0.0148, 0.004, -0.0272, 0.0142, -0.0262, 0.0097, 0.0172, -0.0068, -0.0264, -0.0156, -0.0618, -0.06, -0.0192, -0.0119, 0.006, -0.0037, 8e-04, 0.0092, 0.0312, 0.0024, 0.0153, 0.0113)
idxData <- as.Date(c(9892, 9920, 9951, 9981, 10012, 10042, 10073, 10104, 10134, 10165, 10195, 10226, 10257, 10285, 10316, 10346, 10377, 10407, 10438, 10469, 10499, 10530, 10560, 10591, 10622, 10650, 10681, 10711, 10742, 10772, 10803, 10834, 10864, 10895, 10925, 10956, 10987, 11016, 11047, 11077, 11108, 11138, 11169, 11200, 11230, 11261, 11291, 11322, 11353, 11381, 11412, 11442, 11473, 11503, 11534, 11565, 11595, 11626, 11656, 11687, 11718, 11746, 11777, 11807, 11838, 11868, 11899, 11930, 11960, 11991, 12021, 12052, 12083, 12111, 12142, 12172, 12203, 12233, 12264, 12295, 12325, 12356, 12386, 12417, 12448, 12477, 12508, 12538, 12569, 12599, 12630, 12661, 12691, 12722, 12752, 12783, 12814, 12842, 12873, 12903, 12934, 12964, 12995, 13026, 13056, 13087, 13117, 13148, 13179, 13207, 13238, 13268, 13299, 13329, 13360, 13391, 13421, 13452, 13482, 13513, 13544, 13572, 13603, 13633, 13664, 13694, 13725, 13756, 13786, 13817, 13847, 13878, 13909, 13938, 13969, 13999, 14030, 14060, 14091, 14122, 14152, 14183, 14213, 14244, 14275, 14303, 14334, 14364, 14395, 14425, 14456, 14487))
nameData <- c('Convertible Arbitrage', 'CTA Global', 'Distressed Securities', 'Emerging Markets', 'Equity Market Neutral', 'Event Driven', 'Fixed Income Arbitrage', 'Global Macro', 'Long/Short Equity', 'Merger Arbitrage', 'Relative Value', 'Short Selling', 'Funds of Funds')

asset_returns <- xts(matrix(retData, ncol=13), order.by=idxData)
names(asset_returns) <- nameData
str(asset_returns)



# Get the column names of the returns data
asset_names <- colnames(asset_returns)

# Create a portfolio specification object using asset_names
port_spec <- portfolio.spec(assets=asset_names)

# Get the class of the portfolio specification object
class(port_spec)

# Print the portfolio specification object
port_spec


# Constraints are added to the portfolio specification object with the add.constraint() function
# Each constraint added is a separate object and stored in the constraints slot in the portfolio object
# In this way, the constraints are modular and one can easily add, remove, or modify the constraints in the portfolio object
# The required arguments for add.constraint() are the portfolio the constraint is added to, the constraint type, and named arguments passed via ... to the constructor of the constraint type.

# Specify the constraint on the sum of the weights
# weight_sum, weight, leverage
# full_investment is a special case that sets min_sum = max_sum = 1
# dollar_neutral is a special case that sets min_sum = max_sum = 0

# Specify constraints for the individual asset weights
# box
# long_only is a special case that sets min = 0 and max = 1

# Specify the constraint for the sum of weights of assets by group (sector, region, asset class, etc.)
# group

# Specify a constraint on the target mean return
# return

# Add the weight sum constraint
port_spec <- add.constraint(portfolio = port_spec, type = "weight_sum", min_sum = 1, max_sum = 1)

# Add the box constraint
port_spec <- add.constraint(portfolio = port_spec, type = "box", min = c(rep(0.1, 5), rep(0.05, 8)), max = 0.4)

# Add the group constraint
port_spec <- add.constraint(portfolio = port_spec, type = "group", groups = list(c(1, 5, 7, 9, 10, 11), c(2, 3, 4, 6, 8, 12)), group_min = 0.4, group_max = 0.6)

# Print the portfolio specification object
print(port_spec)


# Objectives are added to the portfolio object with the add.objective() function
# Each objective added is a separate object and stored in the objectives slot in the portfolio specification object
# In this way, the objectives are modular and one can easily add, remove, or modify the objective objects
# The name argument must be a valid R function
# Several functions are available in the PerformanceAnalytics package, but user defined functions can also be used as objective functions

# return: This objective type seeks to maximize the objective.
# risk: This objective type seeks to minimize the objective.
# risk_budget: This objective type seeks to minimize risk concentration or penalize contribution to risk that exceeds the minimum or maximum allowable percentage contribution to risk.

# Add a return objective to maximize mean return
port_spec <- add.objective(portfolio = port_spec, type = "return", name = "mean")

# Add a risk objective to minimize portfolio standard deviation
port_spec <- add.objective(portfolio = port_spec, type = "risk", name = "StdDev")

# Add a risk budget objective
port_spec <- add.objective(portfolio = port_spec, type = "risk_budget", name = "StdDev", min_prisk = 0.05, max_prisk = 0.1)

# Print the portfolio specification object
print(port_spec)


# There are two functions for running the optimization, optimize.portfolio() and optimize.portfolio.rebalancing()
# This exercise will focus on single period optimization and the next exercise will use optimize.portfolio.rebalancing() for optimization with periodic rebalancing. optimize.portfolio() supports single-period optimization
# Key arguments include R for the asset returns, portfolio for the portfolio specification object, and optimize_method to specify the optimization method used to solve the problem
# In many cases, it is useful to specify trace = TRUE to store additional information for each iteration/trial of the optimization.

# The following optimization methods are supported:
# DEoptim: Differential evolution
# random: Random portfolios
# GenSA: Generalized Simulated Annealing
# pso: Particle swarm optimization
# ROI: R Optimization Infrastructure for linear and quadratic programming solvers

# The optimization method you choose should be based on the type of problem you are solving
# For example, a problem that can be formulated as a quadratic programming problem should be solved using a quadratic programming solver, whereas a non-convex problem should be solved using a global solver such as DEoptim.

# In this exercise, we will define the portfolio optimization problem to maximize mean return and minimize portfolio standard deviation with a standard deviation risk budget where the minimum percentage risk is 5% and the maximum percentage risk is 10%, subject to full investment and long only constraints
# The risk budget objective requires a global solver so we will solve the problem using random portfolios
# The set of random portfolios, rp, is generated using 500 permutations for this exercise

rpData <- c(0.0769, 0, 0, 0, 0, 0.01, 0.012, 0.326, 0, 0.006, 0.058, 0.112, 0.002, 0.028, 0.002, 0.016, 0.056, 0.004, 0.056, 0, 0.178, 0, 0.002, 0.34, 0, 0.008, 0.03, 0.078, 0.014, 0.088, 0.066, 0.006, 0.018, 0.072, 0.106, 0.212, 0.08, 0.032, 0, 0, 0.07, 0.204, 0.01, 0.006, 0.018, 0, 0.022, 0.036, 0.022, 0.08, 0.174, 0.22, 0.012, 0.024, 0.134, 0.004, 0.094, 0.162, 0.048, 0, 0.002, 0.01, 0, 0.046, 0.296, 0.026, 0.01, 0.01, 0, 0.004, 0.004, 0.024, 0, 0.25, 0.112, 0.082, 0, 0.002, 0.118, 0.012, 0.31, 0.048, 0.324, 0.014, 0, 0.376, 0, 0.034, 0.018, 0.002, 0.038, 0, 0.184, 0.004, 0.074, 0.038, 0.008, 0.214, 0.218, 0, 0.06, 0, 0.326, 0, 0.006, 0.108, 0, 0.022, 0.036, 0.09, 0.496, 0, 0, 0, 0.106, 0.044, 0.134, 0, 0, 0.014, 0.004, 0.008, 0.106, 0.292, 0.178, 0, 0.004, 0, 0, 0.182, 0.224, 0, 0.042, 0, 0.374, 0.032, 0.274, 0.186, 0.092, 0.008, 0, 0, 0.306, 0.146, 0, 0.024, 0.126, 0.16, 0.016, 0.058, 0.068, 0.01, 0.138, 0.302, 0.054, 0.042, 0.002, 0, 0, 0, 0.026, 0.004, 0, 0.018, 0.09, 0, 0, 0, 0.046, 0.008, 0.212, 0.016, 0, 0.014, 0, 0.152, 0.006, 0, 0.298, 0, 0, 0.018, 0.058, 0.13, 0.678, 0, 0.424, 0.012, 0.014, 0.032, 0.036, 0, 0.482, 0, 0.004, 0.38, 0.002, 0, 0.1, 0.002, 0.002, 0.084, 0.158, 0.026, 0.034, 0.094, 0, 0, 0.006, 0.51, 0.01, 0.106, 0.078, 0, 0.35, 0, 0.046, 0.024, 0.1, 0.34, 0, 0, 0.048, 0.24, 0.038, 0.008, 0.04, 0, 0.004, 0.024, 0.004, 0, 0.038, 0.262, 0.046, 0.006, 0.346, 0.004, 0, 0.578, 0.724, 0, 0.308, 0.008, 0, 0, 0.046, 0.048, 0.004, 0.212, 0, 0.038, 0.026, 0.114, 0.008, 0.44, 0.088, 0, 0.65, 0.202, 0.174, 0.164, 0.052, 0.486, 0.01, 0, 0.034, 0.124, 0.006, 0.012, 0, 0.66, 0.164, 0, 0.428, 0.478, 0.516, 0.096, 0, 0.33, 0, 0.004, 0.08, 0, 0.092, 0, 0.024, 0.06, 0, 0.002, 0.048, 0.04, 0.002, 0.172, 0, 0.018, 0.046, 0.094, 0.028, 0.046, 0.062, 0.036, 0.01, 0.694, 0.032, 0.01, 0.02, 0.022, 0.036, 0.012, 0, 0.356, 0, 0.008, 0, 0.128, 0.412, 0.154, 0.11, 0.176, 0, 0.02, 0.418, 0.016, 0, 0.034, 0.068, 0.12, 0.058, 0.006, 0.002, 0.002, 0.03, 0.208, 0, 0.034, 0.054, 0.1, 0.73, 0.056, 0, 0.414, 0.006, 0.008, 0.012, 0.03, 0.01, 0.182, 0.156, 0.052, 0, 0.034, 0.01, 0.112, 0, 0.358, 0.016, 0.02, 0, 0.062, 0.068, 0.006, 0.01, 0.052, 0.006, 0.03, 0.122, 0.004, 0, 0.002, 0, 0.014, 0.004, 0, 0, 0.048, 0.004, 0, 0.464, 0.018, 0.022, 0.034, 0, 0.024, 0.186, 0, 0.134, 0.064, 0, 0.094, 0, 0, 0.122, 0.09, 0.01, 0.744, 0.002, 0.022, 0.36, 0.166, 0.042, 0.14, 0.206, 0.242, 0, 0.006, 0.002, 0.022, 0.026, 0.196, 0, 0, 0.004, 0.034, 0.228, 0.172, 0.002, 0.014, 0.052, 0.012, 0, 0.162, 0.046, 0.044, 0.494, 0.128, 0.202, 0.002, 0, 0.096, 0.002, 0.156, 0.06, 0, 0.126, 0, 0.062, 0.1, 0.004, 0.046, 0.02, 0.098, 0, 0, 0.084, 0.028, 0.012, 0.114, 0.116, 0.018, 0.074, 0.018, 0.012, 0.16, 0.036, 0.046, 0.024, 0.098, 0.014, 0, 0.002, 0.054, 0.038)
rpData <- c(rpData, 0.092, 0.02, 0.124, 0.108, 0.056, 0.1, 0.242, 0, 0.066, 0.144, 0.082, 0.006, 0.018, 0.082, 0.088, 0.098, 0.088, 0, 0.18, 0.04, 0.008, 0.012, 0, 0.164, 0.016, 0, 0.254, 0.192, 0.044, 0, 0.17, 0.192, 0.072, 0.02, 0, 0.184, 0.0769, 0.118, 0.058, 0, 0.066, 0.086, 0, 0.002, 0.046, 0.016, 0, 0.004, 0, 0, 0.174, 0.296, 0.144, 0.092, 0.028, 0.2, 0.04, 0.004, 0.236, 0.03, 0, 0.23, 0.192, 0.016, 0.152, 0.032, 0.008, 0, 0.27, 0.01, 0.598, 0.078, 0, 0.078, 0.142, 0.022, 0, 0, 0, 0.032, 0.526, 0.2, 0, 0.024, 0, 0.016, 0, 0, 0.002, 0.202, 0.008, 0.302, 0, 0.232, 0.178, 0.026, 0.622, 0.024, 0, 0.15, 0.098, 0, 0.026, 0.23, 0.006, 0.004, 0, 0.004, 0.3, 0.004, 0, 0, 0, 0, 0.006, 0, 0.022, 0.062, 0.122, 0.004, 0.618, 0.174, 0, 0.032, 0.562, 0.006, 0.2, 0.01, 0.002, 0.048, 0, 0, 0.04, 0.194, 0.026, 0, 0.08, 0.272, 0.076, 0.032, 0.306, 0.15, 0.004, 0.04, 0.062, 0.014, 0.008, 0, 0.006, 0.002, 0.07, 0, 0.008, 0.002, 0, 0, 0.038, 0.252, 0, 0.078, 0.016, 0.008, 0.426, 0.17, 0, 0, 0.142, 0, 0, 0.002, 0.006, 0.054, 0.144, 0.002, 0, 0.134, 0.01, 0.044, 0.088, 0.006, 0, 0.004, 0.076, 0.07, 0.068, 0.218, 0, 0.028, 0.102, 0, 0.246, 0.028, 0.022, 0.008, 0.206, 0.002, 0.088, 0.038, 0.54, 0, 0.012, 0.04, 0.07, 0.022, 0.036, 0, 0.488, 0, 0, 0.104, 0.032, 0.002, 0.008, 0, 0.008, 0.024, 0.23, 0, 0.15, 0.01, 0.064, 0.002, 0, 0.154, 0.004, 0.146, 0.052, 0.004, 0.17, 0.232, 0.016, 0, 0, 0.012, 0.158, 0.028, 0.054, 0.36, 0, 0.002, 0.106, 0.008, 0.106, 0.132, 0, 0, 0.236, 0.048, 0, 0.322, 0.036, 0, 0.274, 0.066, 0, 0.018, 0, 0.064, 0.006, 0.076, 0.03, 0.004, 0.038, 0.026, 0.218, 0, 0.002, 0.024, 0.008, 0.02, 0, 0.128, 0.23, 0.002, 0.002, 0.062, 0.014, 0.01, 0.02, 0.01, 0, 0.004, 0, 0.092, 0.052, 0, 0.096, 0, 0.038, 0.544, 0, 0.02, 0.018, 0.058, 0.062, 0.034, 0.002, 0.004, 0, 0, 0.222, 0.068, 0, 0.004, 0.588, 0.066, 0, 0.018, 0.038, 0.002, 0.01, 0, 0, 0.268, 0.006, 0, 0.034, 0.27, 0.074, 0, 0, 0.088, 0.002, 0.004, 0, 0.004, 0.106, 0.06, 0, 0, 0.01, 0, 0.01, 0, 0, 0.03, 0, 0.002, 0.016, 0, 0.094, 0.014, 0.15, 0, 0.026, 0.002, 0.002, 0.418, 0.512, 0.056, 0.338, 0, 0.006, 0.114, 0.01, 0, 0.018, 0.056, 0, 0.022, 0, 0.054, 0.002, 0.586, 0, 0.202, 0.002, 0.022, 0.47, 0.026, 0.046, 0.32, 0.158, 0.006, 0, 0.176, 0.01, 0.01, 0, 0, 0.166, 0.034, 0.01, 0, 0.002, 0.096, 0.012, 0.416, 0, 0.034, 0.236, 0.032, 0.016, 0.03, 0.026, 0, 0, 0.048, 0.102, 0.038, 0.08, 0.202, 0.016, 0.638, 0.07, 0.134, 0, 0, 0, 0, 0.4, 0.02, 0.022, 0.002, 0.01, 0, 0.262, 0.024, 0.266, 0.004, 0, 0.022, 0.04, 0.028, 0, 0.176, 0.308, 0.172, 0.01, 0.008, 0.016, 0, 0.092, 0.04, 0.162, 0.112, 0.07, 0, 0.018, 0, 0.032, 0, 0.028, 0, 0.014, 0.01, 0.334, 0.006, 0.102, 0, 0.24, 0.074, 0, 0.486, 0.006, 0, 0.042, 0.004, 0.056, 0.028, 0, 0.152, 0.198, 0.094, 0.008, 0.028)
rpData <- c(rpData, 0.046, 0.014, 0, 0, 0.156, 0.018, 0.072, 0.048, 0, 0, 0.008, 0.038, 0.572, 0.01, 0.03, 0.004, 0.104, 0.158, 0.01, 0.002, 0.004, 0, 0.106, 0.188, 0.114, 0, 0.012, 0.01, 0.02, 0, 0, 0.114, 0, 0.002, 0.016, 0, 0.18, 0, 0.022, 0, 0.394, 0.008, 0.016, 0.146, 0.066, 0.364, 0.028, 0.014, 0.526, 0.08, 0.006, 0.044, 0.048, 0, 0.018, 0.506, 0.006, 0.028, 0, 0.19, 0.008, 0, 0.022, 0, 0.024, 0.002, 0.122, 0, 0.092, 0.0769, 0, 0, 0.004, 0.094, 0.094, 0.304, 0.004, 0.006, 0.604, 0.12, 0.008, 0.006, 0.38, 0.03, 0, 0.112, 0, 0.156, 0.006, 0, 0.022, 0, 0.004, 0, 0.014, 0.18, 0, 0, 0, 0.006, 0.062, 0.024, 0.012, 0, 0.022, 0, 0, 0, 0, 0.012, 0.048, 0.144, 0.066, 0.002, 0.458, 0.088, 0.004, 0.374, 0, 0.198, 0.056, 0.006, 0.01, 0.18, 0.036, 0.002, 0.006, 0.026, 0.292, 0.008, 0.042, 0.122, 0.016, 0.006, 0.026, 0.066, 0.032, 0.002, 0.132, 0, 0.004, 0, 0.226, 0.11, 0.232, 0.002, 0.002, 0.002, 0, 0, 0.006, 0, 0.116, 0, 0.09, 0.186, 0.02, 0.01, 0, 0.112, 0.01, 0, 0.012, 0, 0.006, 0, 0.184, 0.062, 0.096, 0.034, 0.468, 0.018, 0, 0.006, 0.004, 0, 0.028, 0.046, 0.002, 0, 0.758, 0.174, 0.07, 0, 0.002, 0.02, 0.002, 0, 0, 0.03, 0.002, 0.092, 0.01, 0.034, 0.066, 0.004, 0, 0.216, 0.024, 0.014, 0.014, 0.376, 0.01, 0.06, 0, 0, 0.022, 0.004, 0.038, 0.312, 0.008, 0.104, 0.002, 0.01, 0, 0.114, 0.2, 0.152, 0.104, 0.016, 0.036, 0.132, 0.008, 0.004, 0.598, 0.048, 0.004, 0.022, 0, 0.232, 0.004, 0.014, 0.042, 0.13, 0.006, 0.002, 0, 0.1, 0.182, 0.038, 0, 0, 0.022, 0.08, 0.154, 0, 0, 0, 0.156, 0.256, 0.394, 0.008, 0, 0.004, 0.002, 0.192, 0.034, 0, 0.046, 0.294, 0.154, 0.038, 0.004, 0.046, 0.06, 0.006, 0.116, 0.044, 0.014, 0.022, 0, 0.47, 0.05, 0.006, 0, 0.008, 0.066, 0.004, 0.03, 0, 0.034, 0, 0.216, 0.21, 0.038, 0.078, 0.05, 0.602, 0.004, 0.014, 0.096, 0, 0.2, 0.158, 0.67, 0.148, 0, 0.01, 0, 0.134, 0.002, 0.45, 0.01, 0, 0.186, 0.01, 0.044, 0, 0.008, 0.002, 0.214, 0.004, 0.028, 0, 0.026, 0.138, 0.018, 0.112, 0.01, 0.174, 0.05, 0.03, 0.072, 0, 0.162, 0, 0.002, 0.054, 0, 0.024, 0.022, 0.002, 0.294, 0.024, 0.02, 0.05, 0.008, 0.054, 0.06, 0, 0.036, 0.004, 0.016, 0.036, 0.012, 0, 0.026, 0.024, 0, 0, 0.03, 0.104, 0.006, 0.016, 0.006, 0, 0, 0.02, 0.04, 0.194, 0.016, 0.178, 0.394, 0.258, 0.03, 0.012, 0, 0.02, 0.006, 0.558, 0, 0.736, 0.006, 0.634, 0.412, 0.012, 0.016, 0.318, 0, 0.036, 0, 0, 0.002, 0, 0.066, 0, 0.006, 0.004, 0.142, 0.156, 0, 0.202, 0.428, 0, 0.708, 0, 0.002, 0, 0.034, 0.14, 0.002, 0.032, 0.002, 0, 0.004, 0.04, 0, 0.014, 0.09, 0, 0, 0.012, 0, 0.048, 0.002, 0.648, 0, 0.244, 0.012, 0.04, 0.012, 0.008, 0.014, 0.002, 0.012, 0.004, 0, 0, 0.238, 0, 0, 0.03, 0.08, 0.004, 0.222, 0, 0, 0.002, 0.014, 0, 0.01, 0.036, 0.054, 0.05, 0.018, 0.022, 0.14, 0, 0.058, 0.088, 0.114, 0.008, 0.012, 0, 0.558, 0, 0.066, 0, 0, 0, 0.062, 0.032)
rpData <- c(rpData, 0.06, 0.464, 0, 0.096, 0, 0.02, 0.078, 0.024, 0.028, 0.128, 0.016, 0.044, 0.018, 0, 0.016, 0.16, 0, 0, 0, 0, 0.002, 0.004, 0.364, 0, 0, 0.304, 0, 0.004, 0.362, 0.01, 0.292, 0, 0.058, 0.028, 0.004, 0, 0, 0.088, 0.02, 0.01, 0, 0.014, 0.046, 0, 0.018, 0.134, 0, 0.074, 0, 0.002, 0.002, 0, 0, 0.004, 0.008, 0.104, 0.026, 0.016, 0, 0.046, 0.084, 0.096, 0.06, 0, 0.11, 0.222, 0, 0.012, 0.016, 0, 0.074, 0.068, 0, 0, 0, 0.07, 0, 0, 0.344, 0.35, 0, 0.166, 0.152, 0.002, 0.456, 0, 0, 0.034, 0.204, 0, 0, 0.038, 0, 0.026, 0, 0.006, 0.004, 0.002, 0.002, 0, 0, 0.012, 0, 0.054, 0.002, 0.132, 0.0769, 0.234, 0.004, 0.096, 0.008, 0.076, 0, 0.172, 0.012, 0, 0.038, 0.03, 0.176, 0, 0.022, 0, 0.108, 0.054, 0.114, 0, 0.264, 0, 0.074, 0, 0.05, 0.298, 0, 0, 0.346, 0, 0.056, 0, 0, 0.228, 0.002, 0, 0.006, 0, 0, 0, 0, 0.002, 0, 0.002, 0.004, 0.004, 0.002, 0.226, 0.002, 0.094, 0.528, 0, 0.004, 0.01, 0.026, 0.148, 0.074, 0, 0.214, 0.004, 0.026, 0.022, 0, 0.572, 0.08, 0.016, 0.004, 0.07, 0.188, 0.012, 0.026, 0, 0.024, 0, 0.426, 0, 0.488, 0.166, 0.134, 0.062, 0.086, 0.026, 0.002, 0.078, 0.166, 0, 0, 0.196, 0.004, 0, 0.056, 0.028, 0, 0.288, 0.096, 0.034, 0.05, 0.02, 0.254, 0.01, 0, 0.186, 0.138, 0, 0, 0.16, 0.006, 0, 0.296, 0.072, 0, 0.008, 0.256, 0, 0.134, 0.044, 0.146, 0.932, 0.064, 0, 0.004, 0.06, 0, 0.006, 0.094, 0.104, 0.006, 0.072, 0, 0.022, 0.058, 0.04, 0, 0.006, 0, 0.054, 0.08, 0.044, 0.014, 0.06, 0.004, 0, 0.088, 0.322, 0, 0.156, 0.01, 0, 0.554, 0.002, 0.156, 0.246, 0, 0.09, 0.086, 0, 0, 0.022, 0, 0.014, 0.25, 0.004, 0.296, 0.402, 0, 0.174, 0.006, 0.012, 0.18, 0.23, 0, 0.028, 0.106, 0, 0.12, 0.076, 0, 0, 0.05, 0.034, 0, 0.014, 0.024, 0.004, 0, 0.226, 0, 0.078, 0.024, 0.16, 0.042, 0.392, 0, 0.54, 0.204, 0.024, 0, 0.048, 0.01, 0, 0.112, 0.03, 0.006, 0.236, 0.094, 0.002, 0.004, 0.02, 0, 0.276, 0.482, 0.024, 0, 0, 0.026, 0, 0, 0.016, 0, 0.03, 0.012, 0.026, 0, 0.002, 0.036, 0.19, 0.008, 0, 0.546, 0.036, 0.002, 0, 0.002, 0.042, 0.104, 0.014, 0.298, 0, 0, 0, 0.004, 0.028, 0.006, 0.196, 0.388, 0, 0, 0.02, 0.456, 0.002, 0.016, 0.13, 0.16, 0.042, 0.098, 0.024, 0.294, 0, 0.054, 0.636, 0.008, 0.048, 0, 0.004, 0.044, 0.022, 0.022, 0.04, 0, 0.092, 0.028, 0.012, 0.006, 0.048, 0.022, 0.228, 0, 0.006, 0.01, 0.012, 0.35, 0.3, 0, 0.016, 0.03, 0, 0, 0.014, 0.026, 0.228, 0.008, 0.014, 0, 0.002, 0.132, 0, 0.142, 0.018, 0, 0.004, 0, 0.006, 0.068, 0, 0.012, 0, 0.018, 0.008, 0.106, 0.024, 0.612, 0, 0, 0.176, 0.006, 0.086, 0.086, 0, 0.002, 0, 0.006, 0.14, 0.002, 0.004, 0.002, 0.002, 0.212, 0.002, 0, 0.024, 0.072, 0.478, 0.024, 0.04, 0.134, 0.026, 0.002, 0, 0, 0.002, 0.02, 0, 0, 0.004, 0.116, 0.008, 0.05, 0.19, 0.05, 0.06, 0, 0.05, 0.128, 0, 0, 0.012, 0, 0.226, 0, 0.054, 0.042, 0, 0, 0.018, 0.12, 0.122, 0.03, 0, 0.016, 0.094, 0.024, 0.02, 0, 0.004, 0.064)
rpData <- c(rpData, 0.034, 0.146, 0.014, 0.178, 0.066, 0.082, 0.046, 0, 0.002, 0, 0.01, 0.032, 0.042, 0, 0, 0.366, 0.044, 0.102, 0.02, 0.034, 0.128, 0.062, 0.052, 0, 0.064, 0.004, 0, 0.018, 0.152, 0.128, 0.034, 0, 0.02, 0.04, 0.008, 0, 0.03, 0.54, 0.002, 0.002, 0.008, 0, 0.054, 0.232, 0, 0, 0.01, 0, 0.024, 0, 0.002, 0.036, 0.004, 0.12, 0.008, 0.016, 0.014, 0, 0, 0.276, 0.11, 0.004, 0.484, 0.008, 0.004, 0, 0.122, 0.096, 0.002, 0.028, 0.442, 0.334, 0.012, 0.012, 0.014, 0.036, 0.524, 0.662, 0.092, 0.072, 0, 0, 0.002, 0, 0.018, 0, 0.108, 0.026, 0.11, 0.042, 0.146, 0.018, 0.078, 0.142, 0.02, 0.022, 0, 0.018, 0.008, 0.072, 0, 0.012, 0, 0.068, 0.044, 0, 0.044, 0.04, 0.02, 0.004, 0, 0.16, 0.02, 0, 0.006, 0.37, 0.014, 0, 0, 0, 0.116, 0.004, 0.01, 0.002, 0.0769, 0.568, 0.006, 0, 0.072, 0.392, 0.188, 0.028, 0.03, 0, 0.03, 0.104, 0.006, 0.016, 0.088, 0.044, 0.006, 0, 0, 0.02, 0, 0.28, 0, 0.012, 0, 0.002, 0, 0.262, 0.06, 0, 0.148, 0.008, 0.122, 0, 0.02, 0, 0.052, 0, 0, 0.206, 0.002, 0.058, 0.274, 0, 0.014, 0, 0.002, 0.416, 0.044, 0.25, 0, 0, 0.442, 0.08, 0.148, 0.134, 0.06, 0, 0.286, 0, 0, 0.362, 0, 0.048, 0.008, 0.202, 0.092, 0.008, 0.004, 0.18, 0, 0.066, 0.234, 0, 0, 0, 0.052, 0, 0.072, 0, 0, 0.476, 0.002, 0.01, 0.122, 0.02, 0.234, 0.454, 0.08, 0, 0.15, 0.02, 0.05, 0.322, 0.078, 0.136, 0, 0.124, 0.126, 0.042, 0.012, 0, 0.084, 0.102, 0, 0.004, 0.01, 0.01, 0.04, 0.008, 0, 0.014, 0.026, 0.024, 0.534, 0.252, 0.012, 0.006, 0.044, 0, 0.422, 0.078, 0.046, 0.068, 0.236, 0.068, 0.068, 0.204, 0.018, 0, 0.09, 0, 0.004, 0.02, 0, 0, 0.246, 0, 0.01, 0.056, 0.088, 0, 0.052, 0.026, 0, 0, 0.102, 0.004, 0.116, 0.014, 0.01, 0.2, 0.102, 0, 0, 0, 0.216, 0, 0.01, 0.024, 0, 0.022, 0, 0, 0.002, 0.002, 0, 0, 0.22, 0.11, 0, 0, 0.316, 0, 0.042, 0.09, 0, 0.014, 0.182, 0, 0, 0.324, 0.002, 0, 0.108, 0.202, 0, 0.012, 0.092, 0.132, 0.02, 0.056, 0.014, 0.012, 0, 0.036, 0.028, 0.062, 0.264, 0.084, 0.194, 0, 0, 0.07, 0.004, 0.03, 0.124, 0.004, 0, 0.022, 0.006, 0.068, 0.03, 0.17, 0.008, 0.34, 0.056, 0.166, 0, 0.166, 0.004, 0.036, 0.148, 0.116, 0.028, 0, 0.01, 0, 0.062, 0.122, 0, 0, 0.004, 0.32, 0.168, 0.008, 0, 0, 0.064, 0, 0.002, 0.046, 0.008, 0.018, 0, 0, 0, 0.028, 0.004, 0.02, 0.084, 0.002, 0, 0, 0, 0.154, 0.084, 0.15, 0.002, 0, 0, 0.024, 0.448, 0, 0.164, 0.062, 0.022, 0.156, 0, 0.318, 0.074, 0.048, 0.196, 0.066, 0.062, 0.026, 0, 0.01, 0.09, 0.048, 0.112, 0.024, 0.024, 0.056, 0, 0.08, 0, 0, 0.016, 0, 0.01, 0.04, 0, 0, 0.026, 0.066, 0.01, 0.002, 0.006, 0, 0.032, 0.478, 0.006, 0, 0, 0.018, 0.02, 0.654, 0, 0.08, 0, 0.042, 0.004, 0.018, 0, 0.142, 0.07, 0, 0.042, 0.058, 0.222, 0, 0.002, 0.012, 0.022, 0.002, 0.232, 0, 0, 0.016, 0.21, 0.026, 0, 0.022, 0.004, 0.11, 0.09, 0.024, 0, 0.02, 0.114, 0.002, 0.004, 0.352, 0.034, 0.032, 0.05, 0.186, 0.034, 0.13, 0.06)
rpData <- c(rpData, 0.048, 0.234, 0.018, 0.002, 0.014, 0.006, 0, 0, 0.058, 0, 0, 0.01, 0.094, 0.024, 0.018, 0, 0.048, 0.104, 0.26, 0.776, 0, 0.116, 0.006, 0, 0.308, 0, 0.008, 0, 0.048, 0, 0.238, 0.032, 0.07, 0.002, 0.126, 0, 0.084, 0, 0.088, 0, 0.172, 0, 0.288, 0.074, 0, 0.032, 0, 0.402, 0.246, 0.166, 0.364, 0.15, 0.02, 0.146, 0.058, 0.192, 0, 0.058, 0, 0.268, 0.306, 0.136, 0.002, 0.01, 0.042, 0, 0.34, 0.006, 0, 0, 0, 0, 0.138, 0.028, 0.068, 0.048, 0.192, 0.018, 0.442, 0, 0, 0.466, 0.088, 0.004, 0.176, 0.012, 0.006, 0, 0.016, 0.06, 0.014, 0.246, 0, 0.024, 0.07, 0.002, 0, 0.234, 0.866, 0.014, 0.028, 0.004, 0.206, 0, 0.744, 0, 0, 0.004, 0, 0, 0.3, 0.194, 0.024, 0.058, 0.524, 0.002, 0, 0, 0, 0, 0.036, 0.138, 0, 0.046, 0.124, 0.04, 0.01, 0, 0.002, 0, 0.004, 0.46, 0.086, 0.104, 0.43, 0.094, 0.068, 0.08, 0.114, 0.112, 0.02, 0.212, 0.002, 0.064, 0.042, 0.046, 0.71, 0.028, 0.0769, 0, 0, 0, 0.012, 0.022, 0.004, 0.016, 0.038, 0, 0.01, 0.002, 0.546, 0.378, 0.036, 0.004, 0.07, 0.158, 0.09, 0, 0, 0.05, 0.004, 0, 0.14, 0.142, 0, 0.004, 0.004, 0.02, 0.05, 0.664, 0, 0.056, 0.046, 0.044, 0.004, 0.002, 0, 0.022, 0, 0, 0.016, 0, 0.008, 0.048, 0, 0.138, 0.06, 0.002, 0.02, 0.028, 0.21, 0.018, 0.084, 0.028, 0.034, 0, 0, 0.068, 0.174, 0.054, 0, 0.078, 0, 0.178, 0.03, 0.006, 0.06, 0.022, 0, 0, 0.068, 0, 0.002, 0, 0.004, 0, 0.01, 0.482, 0.002, 0.022, 0.052, 0.024, 0, 0.018, 0, 0.018, 0, 0.338, 0.122, 0.008, 0.008, 0.05, 0.002, 0, 0.012, 0.034, 0.164, 0.216, 0, 0.054, 0.028, 0.15, 0.608, 0.04, 0, 0.738, 0.002, 0.084, 0.034, 0.012, 0, 0.002, 0.01, 0, 0, 0, 0.184, 0.002, 0.01, 0.038, 0, 0.368, 0.01, 0.092, 0.23, 0.028, 0.002, 0.01, 0.042, 0.306, 0.01, 0, 0, 0.078, 0.022, 0.264, 0, 0.03, 0.018, 0.016, 0.042, 0.024, 0, 0.038, 0.16, 0.084, 0.012, 0.014, 0, 0.196, 0, 0.014, 0.004, 0.034, 0.08, 0.104, 0.002, 0, 0, 0.094, 0.056, 0.04, 0.208, 0.15, 0.122, 0.004, 0.014, 0.004, 0.014, 0.228, 0.038, 0.724, 0, 0, 0.486, 0, 0.26, 0, 0.01, 0.02, 0.008, 0.588, 0, 0.006, 0.026, 0, 0.372, 0.112, 0.014, 0, 0.028, 0.002, 0.006, 0.006, 0.002, 0.006, 0.006, 0.006, 0, 0, 0.09, 0.078, 0.026, 0.012, 0.044, 0.26, 0.082, 0, 0.102, 0.16, 0, 0, 0, 0.02, 0.032, 0.002, 0.024, 0.076, 0.05, 0.04, 0.018, 0.004, 0.374, 0, 0.062, 0.678, 0, 0.042, 0, 0, 0.02, 0.064, 0.048, 0.066, 0, 0.266, 0, 0.014, 0.008, 0.536, 0, 0.084, 0.068, 0.106, 0.09, 0.13, 0.012, 0, 0.06, 0.026, 0.048, 0.004, 0, 0, 0.046, 0.158, 0.008, 0.026, 0.184, 0.256, 0.384, 0.046, 0.172, 0.004, 0.024, 0.016, 0.002, 0.08, 0, 0.126, 0.004, 0.53, 0.004, 0, 0.244, 0.004, 0.002, 0, 0, 0.236, 0, 0.038, 0, 0, 0.168, 0.094, 0, 0.002, 0.098, 0, 0.124, 0.05, 0, 0.02, 0, 0.046, 0.138, 0, 0.082, 0, 0.002, 0.034, 0.05, 0.084, 0.146, 0, 0, 0.676, 0, 0.008, 0, 0, 0, 0.004, 0.118, 0.062, 0.06, 0.002, 0, 0.008, 0.032, 0.016)
rpData <- c(rpData, 0.724, 0, 0.1, 0, 0.008, 0.174, 0.008, 0.014, 0, 0.004, 0, 0.024, 0.17, 0, 0.002, 0.008, 0, 0.024, 0.002, 0, 0.006, 0.022, 0.008, 0.012, 0, 0.03, 0.006, 0.142, 0.186, 0, 0.002, 0.002, 0.016, 0.05, 0.012, 0.002, 0.23, 0.402, 0.046, 0, 0.018, 0.002, 0, 0.098, 0.004, 0, 0, 0.122, 0, 0.024, 0.06, 0.024, 0, 0, 0, 0.034, 0.112, 0.012, 0.048, 0.008, 0.008, 0.028, 0.064, 0.004, 0, 0.014, 0, 0.12, 0.11, 0.174, 0.014, 0, 0, 0, 0, 0.016, 0.24, 0.16, 0.04, 0.012, 0.132, 0, 0, 0.006, 0.268, 0, 0, 0.002, 0, 0.028, 0, 0.01, 0, 0, 0, 0, 0.018, 0.034, 0.17, 0.038, 0.03, 0.11, 0.088, 0.498, 0, 0, 0, 0.066, 0.1, 0.066, 0.09, 0, 0.012, 0.008, 0.408, 0.024, 0.802, 0.004, 0, 0.006, 0.006, 0.164, 0.03, 0.16, 0.078, 0, 0.004, 0.094, 0.022, 0.082, 0.002, 0.012, 0.008, 0.28, 0.008, 0.008, 0.02, 0.006, 0.37, 0.106, 0.012, 0.1, 0.004, 0.572, 0.016, 0, 0.002, 0.08, 0.01, 0.046, 0.002, 0.088, 0.026, 0.052, 0.198, 0.03, 0.03, 0.172, 0.006, 0, 0.154, 0.008, 0.236, 0.01, 0.072, 0.01, 0.088, 0, 0.51, 0.002, 0.124, 0.206, 0.04, 0.07, 0.258, 0.0769, 0, 0, 0.44, 0.076, 0.002, 0, 0.026, 0.034, 0, 0, 0.002, 0.004, 0.06, 0.502, 0.01, 0.152, 0, 0.174, 0, 0.026, 0.04, 0.556, 0.006, 0.076, 0.018, 0, 0.298, 0, 0.02, 0.002, 0.028, 0.086, 0, 0.004, 0, 0.026, 0, 0, 0.002, 0, 0, 0.002, 0.038, 0.23, 0.218, 0.166, 0.028, 0.224, 0.192, 0.032, 0, 0, 0.326, 0.008, 0.068, 0, 0.002, 0.1, 0.002, 0, 0.056, 0.056, 0.004, 0.146, 0.292, 0.002, 0.004, 0.096, 0.096, 0.4, 0, 0, 0, 0.184, 0.366, 0.17, 0.458, 0.446, 0, 0.014, 0.014, 0.096, 0.07, 0, 0.086, 0.092, 0.008, 0.042, 0, 0.006, 0.354, 0.058, 0.03, 0.746, 0.06, 0.158, 0.026, 0, 0.304, 0.012, 0.014, 0.09, 0.02, 0, 0.32, 0, 0.03, 0.05, 0.238, 0, 0.006, 0.012, 0, 0, 0, 0.116, 0, 0.008, 0, 0, 0.03, 0.232, 0.058, 0, 0.114, 0.024, 0.37, 0, 0, 0, 0.002, 0.378, 0.144, 0, 0.022, 0.068, 0, 0.672, 0.092, 0.054, 0.644, 0.196, 0.066, 0, 0.006, 0.032, 0.028, 0.006, 0.012, 0.002, 0.002, 0.002, 0, 0.002, 0, 0.028, 0.244, 0.692, 0, 0.016, 0.354, 0.032, 0, 0.316, 0.16, 0.126, 0.03, 0.086, 0.064, 0.052, 0.176, 0, 0.09, 0.688, 0.004, 0.01, 0.004, 0, 0.038, 0, 0.054, 0.466, 0.182, 0.11, 0.01, 0.19, 0.024, 0.032, 0.094, 0, 0.028, 0.006, 0, 0, 0.046, 0.41, 0.278, 0.156, 0, 0.04, 0, 0.012, 0.014, 0.088, 0.028, 0.402, 0.098, 0, 0, 0.012, 0.04, 0.442, 0.02, 0.13, 0.002, 0.132, 0.116, 0.002, 0.04, 0.018, 0.16, 0.264, 0.048, 0.086, 0, 0.106, 0, 0.038, 0.002, 0.006, 0, 0.02, 0, 0.016, 0.08, 0.074, 0.17, 0.062, 0.012, 0.002, 0.014, 0, 0, 0.002, 0.004, 0.116, 0.016, 0, 0.494, 0.004, 0.158, 0, 0.006, 0, 0.004, 0.004, 0.054, 0.002, 0.02, 0.11, 0.054, 0, 0, 0.044, 0.28, 0.002, 0, 0.018, 0.072, 0.134, 0.008, 0.01, 0.012, 0.1, 0, 0, 0.06, 0.358, 0, 0, 0, 0.002, 0.15, 0.472, 0.208, 0.058, 0.002, 0.068, 0.01, 0.076, 0.002, 0.608, 0.084, 0)
rpData <- c(rpData, 0.728, 0.084, 0.002, 0.106, 0.608, 0.01, 0.008, 0.04, 0.226, 0.108, 0.16, 0.03, 0, 0.018, 0.044, 0, 0.018, 0.002, 0.114, 0, 0.078, 0, 0.276, 0.024, 0, 0.028, 0.048, 0.07, 0.012, 0, 0, 0.244, 0.078, 0.03, 0.072, 0, 0.064, 0.206, 0.066, 0, 0.26, 0.096, 0.02, 0.086, 0.022, 0.414, 0.004, 0, 0.02, 0.018, 0.39, 0.018, 0.13, 0.008, 0.174, 0, 0.012, 0.124, 0.1, 0.144, 0.138, 0.004, 0.008, 0.014, 0.008, 0.566, 0.092, 0.014, 0.078, 0, 0.152, 0.006, 0.014, 0.086, 0, 0.08, 0.006, 0.008, 0, 0, 0.008, 0.034, 0.01, 0, 0.018, 0.148, 0.008, 0.33, 0.628, 0.478, 0, 0, 0.102, 0.626, 0.44, 0, 0, 0.058, 0.026, 0.056, 0, 0.012, 0.07, 0, 0.192, 0.014, 0, 0.042, 0.036, 0.074, 0, 0.042, 0.006, 0.16, 0.29, 0.264, 0.184, 0.012, 0.272, 0.302, 0.048, 0.026, 0.004, 0.014, 0, 0.05, 0.022, 0.02, 0.006, 0.006, 0, 0, 0.08, 0.014, 0.078, 0.096, 0, 0.226, 0.182, 0.004, 0.1, 0.204, 0, 0.026, 0, 0.026, 0.04, 0, 0, 0.8, 0.006, 0.056, 0.324, 0.218, 0.014, 0.028, 0, 0, 0.014, 0.006, 0.014, 0.144, 0.038, 0.426, 0.06, 0.028, 0.002, 0.338, 0, 0.22, 0.008, 0.092, 0, 0, 0.688, 0.006, 0.02, 0.002, 0.058, 0, 0.112, 0.09, 0.03, 0, 0.064, 0.028, 0.032, 0.052, 0.182, 0.068, 0.038, 0.006, 0.046, 0.008, 0, 0, 0.01, 0.006, 0, 0.014, 0.21, 0.07, 0.002, 0.146, 0.0769, 0.038, 0, 0, 0.052, 0.04, 0, 0.14, 0.082, 0.378, 0.004, 0.03, 0.01, 0, 0.052, 0.336, 0.054, 0.302, 0.018, 0, 0.092, 0, 0.014, 0.376, 0.004, 0.042, 0.226, 0, 0, 0.772, 0.2, 0.002, 0.136, 0.212, 0.132, 0.582, 0, 0.102, 0.422, 0.754, 0, 0.04, 0.022, 0.712, 0.014, 0.004, 0.062, 0.002, 0.048, 0.164, 0.05, 0.24, 0, 0.052, 0, 0.01, 0.26, 0.002, 0.006, 0.008, 0.01, 0.212, 0, 0.002, 0, 0.218, 0.012, 0.138, 0, 0.014, 0.254, 0.004, 0.27, 0.264, 0, 0.066, 0.094, 0.014, 0.086, 0.022, 0.018, 0.228, 0, 0.006, 0.018, 0.006, 0, 0.084, 0, 0.01, 0.038, 0, 0.006, 0.014, 0, 0.21, 0.04, 0.012, 0.114, 0.316, 0.734, 0, 0.114, 0.008, 0.002, 0.082, 0, 0, 0.024, 0.218, 0, 0, 0.37, 0.38, 0.036, 0.014, 0.15, 0.006, 0.022, 0, 0.066, 0.014, 0, 0.002, 0.038, 0, 0.02, 0.022, 0.046, 0, 0.028, 0.002, 0, 0.608, 0, 0.744, 0.004, 0.068, 0, 0.31, 0.48, 0.008, 0.008, 0.014, 0.098, 0.02, 0, 0.18, 0, 0, 0.004, 0.014, 0.092, 0.316, 0, 0.024, 0.324, 0.56, 0, 0, 0, 0, 0, 0, 0.032, 0.088, 0.002, 0.014, 0.092, 0.06, 0.002, 0.042, 0.012, 0, 0.002, 0, 0.45, 0.026, 0.064, 0.004, 0, 0.016, 0.046, 0, 0.018, 0, 0.14, 0, 0, 0.03, 0.124, 0, 0.006, 0.006, 0.054, 0.224, 0.024, 0.082, 0.006, 0.064, 0.284, 0.322, 0.012, 0, 0.252, 0.022, 0.272, 0.028, 0.414, 0, 0.004, 0.002, 0, 0.002, 0, 0.404, 0.016, 0.068, 0.04, 0.006, 0.118, 0.02, 0.13, 0.04, 0.136, 0, 0.168, 0, 0, 0.07, 0.016, 0, 0.042, 0.148, 0.12, 0.006, 0, 0, 0, 0.016, 0.1, 0.016, 0.012, 0, 0.002, 0.05, 0.188, 0.05, 0.002, 0, 0.016, 0.196, 0, 0, 0.074, 0.134, 0.35, 0.084, 0.062, 0, 0.04)
rpData <- c(rpData, 0.09, 0.062, 0, 0.004, 0.008, 0.02, 0.14, 0.004, 0.23, 0, 0, 0.12, 0.014, 0, 0, 0, 0.02, 0, 0, 0, 0.074, 0.012, 0.708, 0.014, 0.072, 0.138, 0.084, 0.364, 0.052, 0.052, 0.224, 0.004, 0.06, 0.048, 0.02, 0.116, 0, 0.034, 0.11, 0, 0, 0.056, 0, 0.008, 0, 0.048, 0.062, 0.076, 0.07, 0.026, 0.032, 0.47, 0, 0.556, 0.002, 0.224, 0, 0.092, 0.164, 0.018, 0.01, 0.01, 0.032, 0.012, 0.142, 0, 0.068, 0.638, 0.006, 0.062, 0.1, 0.212, 0.204, 0.002, 0.004, 0.084, 0.016, 0.004, 0.054, 0.004, 0.168, 0, 0.054, 0.008, 0, 0.012, 0.078, 0.006, 0.086, 0, 0.116, 0.184, 0.02, 0.456, 0.026, 0.022, 0, 0.236, 0, 0.09, 0, 0.3, 0.014, 0.056, 0.026, 0.096, 0.102, 0.022, 0.002, 0, 0.144, 0.004, 0, 0.002, 0, 0.048, 0.054, 0.052, 0.042, 0.1, 0.012, 0.018, 0.048, 0.254, 0.096, 0, 0.002, 0.068, 0, 0.084, 0.008, 0.024, 0.03, 0.038, 0.048, 0.024, 0.092, 0, 0.012, 0.124, 0.088, 0.006, 0.226, 0.154, 0.244, 0.052, 0.028, 0.016, 0.124, 0, 0.352, 0.168, 0, 0.056, 0.006, 0.05, 0, 0.012, 0.008, 0, 0, 0.088, 0.018, 0, 0.062, 0.006, 0.012, 0, 0.026, 0, 0.044, 0, 0.1, 0.186, 0.234, 0.004, 0, 0.236, 0.006, 0, 0.038, 0.004, 0, 0.012, 0, 0.182, 0.076, 0, 0, 0.004, 0, 0.088, 0.04, 0.276, 0, 0.006, 0.034, 0.046, 0.078, 0.55, 0.004, 0, 0.004, 0, 0, 0.256, 0.132, 0.028, 0, 0.072, 0.268, 0.008, 0, 0, 0, 0, 0.042, 0.03, 0.216, 0.042, 0.004, 0.046, 0, 0.036, 0, 0.134, 0, 0.256, 0.05, 0, 0.486, 0, 0.008, 0.026, 0, 0.034, 0, 0.018, 0.0769, 0.008, 0.002, 0, 0.014, 0.004, 0, 0.058, 0.022, 0, 0, 0.16, 0.006, 0, 0.096, 0.004, 0.012, 0, 0.004, 0.014, 0, 0.004, 0.012, 0.122, 0.062, 0.006, 0.324, 0.294, 0, 0, 0.038, 0.002, 0.002, 0.016, 0, 0.006, 0.03, 0.002, 0, 0, 0.016, 0.432, 0.006, 0, 0.124, 0, 0.032, 0.024, 0.002, 0.056, 0, 0.216, 0.034, 0.198, 0.02, 0.002, 0.036, 0.02, 0.014, 0.004, 0.08, 0.032, 0, 0.014, 0, 0.002, 0, 0.05, 0.198, 0.224, 0.008, 0, 0.082, 0, 0.112, 0, 0, 0.088, 0.06, 0.046, 0.146, 0, 0.048, 0.002, 0, 0.012, 0, 0.034, 0.02, 0.562, 0.018, 0.086, 0, 0.098, 0, 0.016, 0.042, 0.148, 0.03, 0, 0.002, 0, 0.06, 0.082, 0.028, 0, 0.016, 0.062, 0.056, 0.006, 0, 0, 0.002, 0, 0.008, 0.128, 0.248, 0.002, 0.008, 0.316, 0, 0.324, 0.096, 0.07, 0.106, 0.174, 0.014, 0.018, 0.156, 0, 0.002, 0.082, 0.006, 0.162, 0.034, 0, 0.038, 0.004, 0.194, 0.042, 0.002, 0.142, 0.01, 0.02, 0.048, 0.016, 0.24, 0.014, 0.038, 0.332, 0.02, 0.178, 0, 0.104, 0.02, 0.002, 0.192, 0.004, 0, 0, 0.262, 0, 0.026, 0, 0.002, 0, 0.002, 0.35, 0.182, 0.162, 0.002, 0, 0.016, 0.018, 0.028, 0.352, 0.024, 0, 0, 0.004, 0, 0, 0.032, 0.004, 0, 0.108, 0, 0.046, 0.366, 0.044, 0.314, 0, 0, 0.086, 0, 0.148, 0, 0, 0.014, 0, 0.102, 0, 0, 0.16, 0.024, 0.014, 0, 0.086, 0, 0.076, 0, 0.018, 0.004, 0, 0.218, 0.008, 0, 0.01, 0.21, 0.002, 0.13, 0.094, 0.022, 0.006, 0.002, 0.07, 0.022, 0, 0, 0.05, 0.81, 0.008)
rpData <- c(rpData, 0.06, 0, 0.032, 0.024, 0.03, 0.006, 0.062, 0.006, 0, 0.076, 0, 0.018, 0, 0.01, 0.046, 0.03, 0.094, 0.232, 0.058, 0.02, 0.238, 0.096, 0.008, 0, 0.004, 0, 0.002, 0.038, 0.036, 0.088, 0.06, 0.022, 0.048, 0.008, 0.008, 0.016, 0.102, 0, 0.006, 0.016, 0.094, 0.056, 0.044, 0, 0.026, 0.486, 0.254, 0, 0.006, 0.004, 0.004, 0.002, 0, 0, 0.346, 0.416, 0.004, 0.314, 0.21, 0.048, 0.006, 0.096, 0.228, 0.058, 0.014, 0.006, 0, 0, 0.02, 0.046, 0.006, 0, 0.01, 0.302, 0.11, 0.052, 0, 0.006, 0.322, 0.028, 0, 0.612, 0, 0.008, 0, 0, 0.012, 0.452, 0.002, 0.582, 0.168, 0.052, 0.238, 0.02, 0.052, 0.002, 0.006, 0.006, 0, 0.08, 0.018, 0.012, 0, 0.002, 0.088, 0.452, 0, 0.05, 0.402, 0.016, 0.726, 0.122, 0.232, 0.218, 0.026, 0.01, 0.01, 0.126, 0, 0.004, 0.018, 0.024, 0.052, 0, 0, 0.002, 0.428, 0.014, 0, 0.092, 0, 0.048, 0.038, 0, 0.064, 0.002, 0.2, 0.146, 0.002, 0.58, 0.624, 0.62, 0.032, 0.352, 0.01, 0.178, 0, 0.026, 0.044, 0.03, 0.008, 0.136, 0, 0.002, 0.002, 0.096, 0.146, 0.042, 0, 0.012, 0.288, 0.256, 0.06, 0.002, 0.018, 0, 0, 0, 0.072, 0.004, 0.044, 0.028, 0.08, 0.028, 0.046, 0.044, 0.016, 0.002, 0.092, 0.02, 0.002, 0.498, 0.054, 0.004, 0.83, 0, 0.06, 0.03, 0, 0.05, 0, 0.012, 0.046, 0.006, 0.058, 0, 0.066, 0.004, 0.158, 0.596, 0, 0, 0, 0.004, 0.008, 0, 0.008, 0.28, 0, 0.048, 0.006, 0.012, 0, 0.196, 0, 0.064, 0, 0.05, 0.026, 0.154, 0, 0.016, 0.026, 0.014, 0.8, 0.1, 0.03, 0, 0.004, 0.058, 0.012, 0.06, 0.14, 0, 0.276, 0.004, 0.006, 0, 0, 0, 0.052, 0.208, 0, 0.06, 0.198, 0, 0.08, 0.082, 0.016, 0.008, 0.002, 0.036, 0.036, 0, 0.016, 0.696, 0.064, 0, 0.07, 0, 0.244, 0.43, 0.042, 0.038, 0.02, 0, 0.004, 0.0769, 0.03, 0, 0.092, 0.018, 0.126, 0, 0.018, 0.164, 0, 0.024, 0.002, 0.022, 0, 0.002, 0.03, 0.058, 0.052, 0.138, 0.114, 0.108, 0.004, 0, 0.006, 0.554, 0.15, 0.002, 0, 0.244, 0.006, 0.408, 0, 0.01, 0.002, 0.01, 0, 0.362, 0.11, 0.056, 0, 0.024, 0, 0.48, 0, 0.01, 0, 0.276, 0, 0.094, 0.124, 0, 0, 0.03, 0.04, 0.036, 0.248, 0.18, 0.126, 0, 0.068, 0.078, 0, 0, 0.022, 0.252, 0.002, 0.038, 0.3, 0.012, 0.14, 0.304, 0, 0, 0, 0.004, 0.024, 0.002, 0, 0.032, 0.002, 0, 0.044, 0.042, 0.092, 0.016, 0.122, 0.452, 0.108, 0.084, 0.03, 0.012, 0, 0, 0.036, 0, 0.236, 0.008, 0.022, 0.006, 0.004, 0.008, 0, 0.016, 0, 0.002, 0.016, 0, 0.042, 0.284, 0.034, 0, 0.134, 0.034, 0, 0.04, 0.136, 0.004, 0.004, 0.26, 0.184, 0.014, 0.018, 0.31, 0.004, 0.21, 0.07, 0.042, 0, 0.248, 0.676, 0.004, 0.008, 0, 0, 0.048, 0.004, 0.022, 0.03, 0.004, 0, 0.006, 0.002, 0.002, 0.178, 0.07, 0.044, 0.024, 0.154, 0.036, 0.168, 0, 0.074, 0.114, 0.054, 0.03, 0.252, 0.016, 0, 0, 0.904, 0.008, 0.014, 0.03, 0.442, 0.028, 0.006, 0, 0, 0.008, 0.022, 0.17, 0, 0.416, 0, 0, 0.038, 0, 0.132, 0, 0.728, 0.066, 0.014, 0.062, 0.018, 0, 0.006, 0.02, 0.382, 0.046, 0.168, 0.06, 0.37, 0.22, 0.016, 0, 0.044)
rpData <- c(rpData, 0, 0, 0.016, 0, 0.006, 0, 0.06, 0.002, 0.114, 0.042, 0.018, 0.006, 0.296, 0, 0, 0.158, 0, 0.018, 0, 0.132, 0, 0.124, 0.014, 0.272, 0.502, 0.408, 0.044, 0.054, 0.004, 0.004, 0.062, 0.014, 0.012, 0.482, 0.008, 0, 0.012, 0, 0.17, 0, 0.012, 0.514, 0, 0.176, 0, 0.014, 0.63, 0, 0, 0.244, 0.378, 0.004, 0.198, 0.008, 0.054, 0.27, 0.026, 0.006, 0, 0, 0.002, 0.216, 0.006, 0, 0.008, 0.188, 0, 0.032, 0.024, 0.094, 0, 0.002, 0.002, 0.004, 0.07, 0.022, 0.258, 0.146, 0, 0.258, 0, 0.004, 0.096, 0, 0, 0.038, 0, 0.012, 0.35, 0.22, 0, 0, 0.04, 0.004, 0.034, 0.004, 0.016, 0.116, 0.238, 0, 0, 0.004, 0.104, 0, 0.164, 0, 0.002, 0, 0.018, 0.002, 0.238, 0.178, 0.026, 0, 0.008, 0, 0.006, 0, 0.004, 0.014, 0, 0, 0.15, 0, 0.358, 0.132, 0, 0.144, 0, 0, 0.078, 0.006, 0.26, 0.136, 0.004, 0.002, 0, 0.022, 0.392, 0.084, 0.02, 0.03, 0.002, 0.128, 0.044, 0, 0.002, 0, 0.006, 0.092, 0.02, 0.136, 0.016, 0.106, 0.46, 0.148, 0.006, 0, 0, 0.002, 0, 0.032, 0, 0, 0, 0, 0.008, 0.424, 0.002, 0.116, 0, 0.092, 0.142, 0.03, 0, 0.028, 0, 0, 0.03, 0.038, 0.57, 0, 0.014, 0.002, 0.024, 0.146, 0, 0.066, 0, 0.026, 0.002, 0.012, 0.072, 0.016, 0.014, 0.102, 0.026, 0.05, 0.132, 0.086, 0.092, 0.006, 0, 0.064, 0.062, 0.004, 0.008, 0.168, 0.006, 0.154, 0.248, 0, 0.18, 0.118, 0.002, 0.11, 0.034, 0.018, 0.096, 0.046, 0, 0.066, 0.02, 0.632, 0, 0.174, 0.012, 0.178, 0.156, 0.002, 0.048, 0.004, 0.046, 0.098, 0.016, 0.008, 0.03, 0, 0.11, 0.304, 0.118, 0.074, 0, 0.526, 0.008, 0.036, 0.026, 0.052, 0.024, 0.012, 0.006, 0.07, 0.218, 0.016, 0.066, 0, 0.002, 0.046, 0.09, 0.058, 0.002, 0.004, 0.102, 0.1, 0, 0.294, 0, 0.04, 0.036, 0, 0.002, 0.152, 0, 0.012, 0.334, 0.024, 0.074, 0.18, 0.024, 0.006, 0.05, 0, 0.004, 0.068, 0, 0.146, 0.002, 0.09, 0.082, 0.008, 0.03, 0, 0.03, 0.002, 0.004, 0, 0, 0.036, 0.01, 0, 0.098, 0.192, 0, 0.0769, 0, 0, 0, 0.032, 0.056, 0, 0.038, 0.026, 0, 0.592, 0.414, 0.214, 0.094, 0, 0.214, 0.018, 0.028, 0.032, 0.068, 0, 0, 0.088, 0.004, 0.01, 0.056, 0.046, 0.002, 0, 0.002, 0.002, 0.158, 0, 0, 0.026, 0.052, 0.212, 0.038, 0, 0, 0.676, 0, 0.004, 0, 0, 0.06, 0.006, 0.002, 0.002, 0, 0.002, 0, 0, 0.006, 0.28, 0.002, 0.046, 0, 0.004, 0.43, 0, 0.154, 0, 0.002, 0.016, 0.018, 0.008, 0.082, 0.182, 0.138, 0, 0.888, 0.012, 0.206, 0.048, 0, 0.152, 0.008, 0, 0.004, 0.002, 0.008, 0.194, 0.134, 0.048, 0.026, 0, 0, 0.028, 0.01, 0.214, 0.356, 0, 0.02, 0.008, 0.064, 0.648, 0.002, 0, 0.006, 0.006, 0, 0.036, 0.018, 0.002, 0.068, 0.962, 0, 0.018, 0.108, 0, 0, 0.03, 0.452, 0.044, 0.346, 0.024, 0, 0.016, 0, 0, 0, 0.098, 0.022, 0.038, 0.104, 0.112, 0.028, 0, 0.06, 0, 0.026, 0.01, 0.042, 0.46, 0.006, 0.088, 0.094, 0, 0.004, 0.012, 0.068, 0.036, 0.178, 0.002, 0.172, 0.054, 0.06, 0.002, 0, 0.016, 0, 0.032, 0.108, 0.322, 0, 0.004, 0.004, 0, 0.026, 0.05, 0.048, 0.008, 0.034)
rpData <- c(rpData, 0.028, 0.008, 0.114, 0.08, 0.004, 0.026, 0.004, 0.468, 0.042, 0.008, 0.01, 0.022, 0.004, 0, 0.004, 0.01, 0.406, 0.026, 0.106, 0.014, 0, 0.252, 0, 0.052, 0, 0, 0.014, 0, 0.04, 0.004, 0.362, 0.016, 0.004, 0.146, 0.128, 0.014, 0.156, 0.032, 0.12, 0.01, 0.028, 0.002, 0, 0, 0.152, 0.076, 0.144, 0.324, 0.404, 0.044, 0.008, 0, 0.36, 0.186, 0.002, 0.002, 0.002, 0.05, 0.272, 0.09, 0.028, 0, 0.022, 0.236, 0, 0.168, 0, 0.162, 0.052, 0.006, 0, 0.436, 0, 0, 0.006, 0, 0.14, 0.006, 0, 0.396, 0.022, 0.174, 0, 0.044, 0, 0.016, 0.06, 0.066, 0.428, 0.002, 0.286, 0, 0.002, 0.068, 0, 0.052, 0.12, 0.006, 0, 0.108, 0, 0.398, 0.454, 0.014, 0, 0.002, 0, 0, 0.004, 0.004, 0.266, 0, 0.212, 0, 0, 0.19, 0.164, 0.02, 0.004, 0, 0, 0, 0, 0.03, 0.006, 0, 0.162, 0.002, 0.058, 0.002, 0.044, 0.062, 0.41, 0.114, 0.574, 0.018, 0.048, 0.01, 0.006, 0.038, 0.004, 0.004, 0.01, 0, 0.388, 0.044, 0.002, 0.006, 0.004, 0.006, 0.046, 0.378, 0, 0, 0.278, 0, 0, 0.002, 0.012, 0.002, 0, 0.004, 0.022, 0.02, 0, 0.01, 0.502, 0.004, 0, 0.014, 0.002, 0.112, 0.032, 0.026, 0.06, 0.334, 0, 0.024, 0.2, 0.188, 0.19, 0.114, 0.048, 0.08, 0.068, 0.128, 0.026, 0.006, 0.024, 0.026, 0, 0.02, 0.116, 0.592, 0, 0.572, 0.004, 0.042, 0.186, 0, 0.004, 0, 0.004, 0.008, 0.006, 0.036, 0, 0, 0.004, 0, 0, 0.038, 0.072, 0, 0.026, 0.082, 0.052, 0.176, 0.29, 0, 0.05, 0.08, 0, 0.452, 0, 0.11, 0.018, 0, 0.14, 0, 0.002, 0, 0.008, 0.002, 0, 0.008, 0.122, 0.13, 0.288, 0.01, 0.004, 0, 0.356, 0, 0, 0, 0.008, 0, 0.008, 0.026, 0, 0.002, 0.078, 0, 0.292, 0, 0, 0.028, 0.004, 0.002, 0.25, 0.004, 0.052, 0.002, 0.006, 0.016, 0.562, 0.006, 0.09, 0.542, 0.012, 0, 0.002, 0.256, 0, 0.004, 0, 0.522, 0, 0.586, 0, 0.034, 0.098, 0, 0, 0, 0.668, 0.016, 0, 0.004, 0.004, 0.002, 0, 0.174, 0.022, 0, 0.08, 0.024, 0.004, 0, 0.246, 0.032, 0.002, 0, 0.552, 0.004, 0.106, 0.008, 0.102, 0.14, 0.598, 0.026, 0, 0.208, 0, 0.19, 0.002, 0.012, 0.078, 0, 0.02, 0, 0, 0.082, 0.296, 0, 0.102, 0.118, 0, 0.346, 0.058, 0.32, 0.078, 0.024, 0.074, 0.0769, 0.004, 0.026, 0.362, 0.552, 0.008, 0.488, 0.136, 0.102, 0.004, 0.134, 0.086, 0, 0, 0, 0.048, 0.198, 0.024, 0.006, 0, 0.294, 0.592, 0.022, 0.08, 0.096, 0.006, 0, 0.026, 0.012, 0, 0.008, 0.064, 0, 0.358, 0.008, 0.008, 0.206, 0.038, 0.354, 0, 0.152, 0.032, 0.038, 0.146, 0.06, 0, 0.268, 0.032, 0, 0, 0, 0.212, 0.264, 0.018, 0, 0.016, 0.206, 0.168, 0.116, 0.094, 0, 0.008, 0.822, 0.044, 0.002, 0.02, 0.644, 0.03, 0.094, 0.004, 0.01, 0, 0, 0.06, 0, 0.092, 0.004, 0.224, 0.024, 0.36, 0.396, 0, 0.02, 0.444, 0, 0.064, 0.028, 0.002, 0.132, 0.036, 0.006, 0.132, 0, 0.002, 0, 0.094, 0.004, 0.004, 0.002, 0, 0.048, 0, 0.002, 0, 0.036, 0.052, 0.002, 0.016, 0.094, 0.024, 0.292, 0, 0.042, 0.064, 0.01, 0.042, 0.038, 0.002, 0.398, 0.178, 0.02, 0.122, 0.004, 0.014, 0.05, 0.122)
rpData <- c(rpData, 0.046, 0.086, 0, 0.014, 0.398, 0.504, 0, 0, 0, 0, 0.006, 0, 0, 0.178, 0.014, 0, 0.028, 0.02, 0.142, 0.3, 0.062, 0.036, 0.006, 0.07, 0.578, 0.006, 0.228, 0.002, 0, 0.024, 0.06, 0.038, 0.072, 0, 0.002, 0.4, 0, 0, 0, 0.35, 0.244, 0.47, 0.014, 0.128, 0.018, 0.026, 0.012, 0, 0, 0.098, 0.016, 0.73, 0.036, 0, 0.004, 0.07, 0.038, 0.034, 0.01, 0.032, 0, 0.196, 0.044, 0.024, 0.02, 0.004, 0.002, 0.062, 0, 0.026, 0.528, 0.052, 0.07, 0.342, 0, 0.138, 0.006, 0.31, 0.14, 0.738, 0.008, 0.15, 0.052, 0, 0, 0.012, 0, 0.18, 0.012, 0.062, 0.006, 0.164, 0.004, 0.048, 0.002, 0.004, 0.042, 0.028, 0.058, 0.046, 0.23, 0.038, 0.032, 0.01, 0.018, 0.166, 0.154, 0.076, 0, 0.002, 0, 0, 0.806, 0.042, 0, 0.048, 0, 0.178, 0.522, 0, 0, 0, 0.052, 0, 0.108, 0.04, 0.01, 0.044, 0.008, 0.046, 0.096, 0.024, 0.026, 0, 0, 0.014, 0, 0.004, 0.072, 0, 0.074, 0.114, 0.124, 0.008, 0.014, 0.002, 0.04, 0.016, 0.002, 0, 0, 0, 0.018, 0.414, 0.112, 0, 0, 0, 0.032, 0.332, 0.01, 0.282, 0.452, 0.342, 0, 0.004, 0.002, 0.02, 0.004, 0, 0.004, 0.662, 0, 0.002, 0.026, 0.018, 0.054, 0, 0, 0, 0.096, 0, 0, 0, 0, 0.096, 0, 0, 0.044, 0.088, 0, 0.346, 0.198, 0, 0.004, 0.006, 0.088, 0.25, 0, 0.036, 0, 0.002, 0, 0.14, 0, 0.17, 0, 0.162, 0.002, 0.014, 0, 0, 0.092, 0, 0, 0, 0, 0.262, 0.09, 0.058, 0.094, 0.002, 0, 0.006, 0.032, 0.008, 0, 0.004, 0.088, 0.336, 0, 0, 0.218, 0, 0, 0.308, 0.292, 0.158, 0.084, 0.092, 0.654, 0, 0.002, 0.002, 0.104, 0.022, 0.188, 0.284, 0.368, 0.032, 0, 0.602, 0.234, 0.614, 0.054, 0.158, 0.006, 0, 0.018, 0.006, 0.724, 0.03, 0, 0, 0.078, 0.258, 0.064, 0, 0.006, 0, 0.026, 0.708, 0.012, 0.024, 0, 0.074, 0, 0.088, 0.072, 0.468, 0.01, 0.302, 0.03, 0, 0.034, 0, 0, 0.056, 0.002, 0.208, 0.012, 0.028, 0.002, 0.312, 0, 0.148, 0, 0.102, 0, 0.102, 0.036, 0.008, 0.402, 0.046, 0.008, 0.066, 0, 0, 0, 0, 0.152, 0, 0.094, 0.102, 0, 0.018, 0.05, 0, 0, 0.05, 0.014, 0.004, 0.006, 0, 0.002, 0.072, 0.036, 0.062, 0.284, 0.052, 0.002, 0.006, 0, 0.668, 0.132, 0, 0, 0.45, 0.012, 0.076, 0, 0, 0, 0, 0.07, 0, 0.008, 0.016, 0.03, 0.034, 0.46, 0.014, 0, 0.06, 0.668, 0.036, 0.008, 0.002, 0.006, 0, 0.012, 0.034, 0.182, 0.132, 0.004, 0.118, 0, 0.016, 0.43, 0.002, 0.232, 0, 0.012, 0.0769, 0, 0.9, 0, 0.004, 0.084, 0.004, 0.038, 0.444, 0, 0, 0.054, 0.016, 0.046, 0, 0.002, 0.022, 0.276, 0.192, 0.576, 0, 0.006, 0, 0.022, 0.01, 0.02, 0.004, 0.028, 0.17, 0.068, 0, 0.01, 0.326, 0.026, 0.04, 0, 0.024, 0.594, 0.03, 0, 0.054, 0.188, 0.002, 0, 0, 0, 0.07, 0.064, 0.124, 0.016, 0, 0.02, 0, 0.024, 0.07, 0.012, 0, 0.276, 0.002, 0, 0, 0.03, 0, 0.008, 0.092, 0.002, 0.074, 0.038, 0.166, 0.034, 0, 0, 0, 0, 0, 0.132, 0.032, 0.03, 0.014, 0, 0, 0.068, 0.09, 0.002, 0.01, 0.016, 0, 0.016, 0.022, 0, 0.026, 0, 0.686, 0.066, 0, 0.11, 0, 0.02, 0.002, 0.016, 0, 0, 0.008, 0.59, 0.006, 0.002, 0, 0.02, 0, 0.112)
rpData <- c(rpData, 0.176, 0.076, 0.048, 0, 0.01, 0.002, 0.108, 0.052, 0, 0.306, 0.392, 0.052, 0.01, 0.004, 0, 0.07, 0.004, 0.002, 0.314, 0.002, 0.006, 0.022, 0.18, 0, 0.016, 0, 0, 0.286, 0.012, 0.052, 0.002, 0.068, 0.05, 0, 0.64, 0.214, 0, 0.018, 0, 0, 0.13, 0, 0.056, 0.006, 0.232, 0.002, 0.004, 0.02, 0, 0.032, 0.06, 0.018, 0.002, 0.022, 0.15, 0.016, 0.322, 0.01, 0.018, 0, 0, 0.012, 0.05, 0.018, 0, 0.012, 0, 0.098, 0.102, 0, 0.034, 0.04, 0, 0.006, 0.002, 0.164, 0, 0.002, 0, 0.004, 0.002, 0, 0, 0.046, 0.314, 0, 0, 0.192, 0.036, 0.454, 0.02, 0.03, 0.074, 0.038, 0.086, 0.004, 0.01, 0.158, 0.002, 0.01, 0, 0.004, 0.048, 0.036, 0, 0, 0, 0.006, 0.006, 0.004, 0.158, 0, 0.006, 0.086, 0.024, 0.004, 0.082, 0, 0.072, 0, 0, 0.638, 0.13, 0.056, 0.3, 0.04, 0, 0, 0, 0.09, 0, 0.002, 0.014, 0.068, 0, 0.378, 0, 0.514, 0.008, 0.01, 0.272, 0.002, 0.002, 0.078, 0.524, 0.018, 0.004, 0.19, 0.08, 0, 0.292, 0.044, 0, 0, 0.172, 0.034, 0.288, 0.36, 0.106, 0.064, 0.664, 0.058, 0.06, 0.1, 0.036, 0.004, 0, 0.024, 0.136, 0, 0.218, 0, 0.692, 0.022, 0, 0, 0.244, 0.012, 0, 0, 0.002, 0.554, 0, 0.012, 0.008, 0, 0.16, 0.054, 0, 0.186, 0.006, 0.386, 0, 0, 0.04, 0, 0.11, 0, 0, 0.038, 0, 0, 0, 0, 0.008, 0.002, 0.086, 0.046, 0.026, 0.008, 0.188, 0.004, 0, 0.006, 0.004, 0.006, 0, 0.112, 0, 0.182, 0, 0.038, 0.004, 0.228, 0.428, 0.014, 0.176, 0.326, 0.02, 0.07, 0.002, 0.348, 0.038, 0, 0.066, 0.01, 0, 0, 0.414, 0.004, 0.366, 0.008, 0.082, 0.654, 0.008, 0.058, 0.81, 0.032, 0.074, 0, 0, 0, 0, 0.004, 0.554, 0.016, 0, 0, 0.352, 0.286, 0.024, 0.052, 0.01, 0, 0, 0.07, 0.07, 0, 0.032, 0.002, 0.006, 0.004, 0, 0.004, 0, 0.046, 0, 0.028, 0.09, 0.014, 0.142, 0.04, 0.026, 0.004, 0.084, 0.002, 0.53, 0, 0.054, 0.024, 0.066, 0.006, 0.046, 0, 0.128, 0, 0.03, 0.564, 0.038, 0.246, 0, 0, 0, 0.062, 0.006, 0.014, 0, 0.016, 0.01, 0, 0.882, 0, 0.592, 0.118, 0, 0.314, 0.2, 0.096, 0.33, 0.15, 0, 0, 0.026, 0.012, 0.028, 0, 0.348, 0.012, 0.092, 0.002, 0.008, 0.032, 0.008, 0.338, 0, 0, 0.166, 0, 0, 0, 0.05, 0.094, 0.03, 0.008, 0.002, 0.004, 0.006, 0.144, 0.11, 0.074, 0.176, 0.028, 0.022, 0.03, 0.138, 0, 0.062, 0.002, 0.054, 0.026, 0, 0.142, 0.07, 0.122, 0, 0.01, 0.02, 0.082, 0, 0.078, 0.164, 0.106, 0.228, 0.584, 0.414, 0.002, 0.114, 0.008, 0.014, 0.24, 0.058, 0.198, 0.004, 0, 0, 0, 0.186, 0, 0.054)

rp <- matrix(rpData, ncol=13)
dimnames(rp) <- list(NULL, nameData)
str(rp)


# Create a portfolio specification object using asset_names
port_spec <- portfolio.spec(assets=asset_names)
port_spec <- add.constraint(portfolio = port_spec, type = "weight_sum", min_sum = 0.99, max_sum = 1.01)
port_spec <- add.constraint(portfolio = port_spec, type = "long_only")
port_spec <- add.objective(portfolio = port_spec, type = "return", name = "mean")
port_spec <- add.objective(portfolio = port_spec, type = "risk", name = "StdDev")
port_spec <- add.objective(portfolio = port_spec, type = "risk_budget", name = "StdDev", min_prisk = 0.05, max_prisk = 0.1)


# Run a single period optimization using random portfolios as the optimization method
opt <- optimize.portfolio(R = asset_returns, portfolio = port_spec, optimize_method = "random", rp = rp, trace = TRUE)

# Print the output of the single-period optimization
print(opt)


# Running the optimization with periodic rebalancing and analyzing the out-of-sample results of the backtest is an important step to better understand and potentially refine the constraints and objectives
# optimize.portfolio.rebalancing() supports optimization with periodic rebalancing (backtesting) to examine out of sample performance
# In addition to the arguments for optimize.portfolio(), a periodic rebalancing frequency must be specified with rebalance_on, training_period to specify the number of periods to use as the training data for the initial optimization, and rolling_window to specify the number of periods for the window width of the optimization
# If rolling_window is set to NULL each optimization will use all data available at the given period the optimization is run.

# To reduce computation time for this exercise, the set of random portfolios, rp, is generated using 50 permutations, and search_size, how many portfolios to test, is set to 1000
# If you are actually optimizing portfolios yourself, you'll probably want to test more portfolios (the default value for search_size is 20,000)!


newRPData <- c(0.07692, 0.018, 0, 0.044, 0, 0.308, 0.362, 0, 0.014, 0, 0.268, 0, 0.156, 0, 0.004, 0.01, 0, 0.182, 0.004, 0.076, 0.142, 0.456, 0.018, 0.02, 0, 0.034, 0.12, 0.004, 0.046, 0.01, 0.002, 0.006, 0, 0.002, 0.34, 0.022, 0.052, 0.002, 0.088, 0, 0, 0, 0.436, 0, 0.002, 0, 0.016, 0.002, 0.166, 0.07692, 0.17, 0, 0, 0.002, 0.018, 0, 0.298, 0, 0.088, 0.066, 0.006, 0.238, 0, 0.004, 0.008, 0.12, 0.038, 0.08, 0.004, 0.03, 0.004, 0.342, 0.278, 0.118, 0.306, 0, 0.436, 0.042, 0.058, 0, 0.086, 0.02, 0.008, 0.022, 0.006, 0.192, 0.004, 0.308, 0.004, 0, 0.066, 0.066, 0, 0.004, 0, 0.004, 0, 0.068, 0.07692, 0.186, 0, 0, 0.176, 0.002, 0.006, 0.19, 0.038, 0, 0.016, 0, 0.074, 0, 0.024, 0.69, 0, 0.238, 0.136, 0.062, 0, 0.026, 0, 0.018, 0, 0.06, 0.084, 0.002, 0.036, 0.05, 0.018, 0.344, 0.046, 0.076, 0.38, 0.002, 0.114, 0.396, 0, 0, 0, 0, 0.166, 0, 0, 0, 0.026, 0.046, 0.11, 0.07692, 0.212, 0.074, 0.062, 0.096, 0.058, 0, 0.058, 0.012, 0.528, 0, 0.244, 0.044, 0.592, 0.022, 0, 0.002, 0.194, 0.002, 0, 0.076, 0, 0.016, 0, 0.404, 0.068, 0.206, 0.016, 0.1, 0.012, 0, 0.006, 0.006, 0.026, 0.024, 0.02, 0.284, 0, 0, 0.13, 0, 0.016, 0, 0.002, 0.002, 0, 0, 0.01, 0.002, 0.07692, 0.03, 0.572, 0.176, 0.024, 0.006, 0.004, 0, 0.138, 0, 0.01, 0.186, 0.136, 0, 0.158, 0.014, 0.004, 0.134, 0, 0.014, 0.02, 0.166, 0.072, 0, 0.014, 0.004, 0.088, 0.028, 0.02, 0.144, 0, 0.09, 0.67, 0.102, 0.06, 0.014, 0.242, 0.012, 0.174, 0.234, 0, 0.458, 0.058, 0.432, 0, 0.102, 0.028, 0.004, 0, 0.07692, 0.194, 0, 0.014, 0, 0.018, 0.118, 0.04, 0, 0, 0.046, 0.072, 0, 0, 0.264, 0.104, 0.756, 0, 0.026, 0.006, 0.394, 0.026, 0, 0.056, 0.212, 0.13, 0.052, 0.094, 0.328, 0.134, 0.114, 0.21, 0.05, 0.006, 0, 0.012, 0, 0.266, 0.014, 0, 0.402, 0.024, 0.166, 0.016, 0.804, 0.024, 0.1, 0.002, 0.058, 0.07692, 0.04, 0.162, 0, 0.008, 0.016, 0.028, 0, 0.01, 0.334, 0.16, 0.006, 0.082, 0.02, 0.038, 0, 0.006, 0.018, 0.264, 0, 0.224, 0.028, 0.548, 0.034, 0.034, 0.012, 0.082, 0.29, 0.216, 0.144, 0.312, 0.012, 0, 0.294, 0.148, 0.014, 0, 0.038, 0.03, 0, 0, 0.004, 0.026, 0.022, 0, 0, 0.532, 0.078, 0.064, 0.07692, 0.022, 0.026, 0.002, 0, 0.232, 0.002, 0.33, 0.152, 0.024, 0, 0.112, 0.138, 0, 0.06, 0.046, 0.006, 0.04, 0.016, 0.236, 0.042, 0, 0, 0.584, 0.012, 0.008, 0.052, 0, 0.008, 0.018, 0.114, 0.042, 0.08, 0.002, 0.002, 0.098, 0, 0.082, 0.22, 0.156, 0.004, 0, 0.004, 0, 0.058, 0.846, 0.062, 0.574, 0.332, 0.07692, 0.032, 0.134, 0.024, 0.064, 0.054, 0, 0.04, 0.516, 0, 0.374, 0.044, 0.02, 0, 0.172, 0, 0.002, 0, 0.016, 0.012, 0.022, 0.004, 0, 0, 0.04, 0.078, 0.042, 0.014, 0.006, 0.034, 0.002, 0, 0.022, 0.294, 0.006, 0.248, 0, 0.002, 0.004, 0.034, 0.004, 0.016, 0.044, 0.354, 0.016, 0.01, 0.018, 0, 0.084, 0.07692, 0.016, 0, 0.68, 0.068, 0.1, 0.376, 0.006, 0.018, 0.012, 0.002, 0.002, 0.106, 0.306, 0.012, 0.016, 0, 0.01, 0.012, 0.004, 0, 0.072, 0, 0, 0.014, 0.006, 0.106, 0.006, 0.04, 0.222, 0, 0.034, 0, 0.122, 0, 0.038, 0.004, 0.194, 0.04, 0.408, 0.594, 0.166, 0, 0.016, 0, 0, 0.042, 0.016, 0.002, 0.07692, 0.01, 0, 0, 0, 0.018, 0.03, 0.016, 0.01, 0, 0.008, 0.022, 0, 0, 0.194, 0.068, 0.032, 0.024, 0.406, 0.548, 0.04, 0.004, 0, 0, 0.118, 0.114, 0, 0.036, 0.132, 0.006, 0.004, 0.16, 0.012, 0, 0.02, 0.38, 0.112, 0, 0.086, 0.024, 0, 0.148, 0.026, 0.038, 0, 0.024, 0.014, 0.06, 0.1, 0.07692, 0.038, 0.026, 0, 0, 0.178, 0.01, 0.002, 0.084, 0, 0.03, 0.282, 0.006, 0.09, 0.008, 0.028, 0.004, 0, 0.01, 0.008, 0.008, 0.21, 0.008, 0, 0.032, 0, 0.174, 0.036, 0.008, 0.016, 0.416, 0.008, 0.09, 0.064, 0.002, 0.116, 0, 0, 0, 0, 0, 0.002, 0.002, 0.002, 0, 0, 0.152, 0.038, 0, 0.07692, 0.034, 0.01, 0.004, 0.566, 0, 0.062, 0.016, 0, 0.006, 0.012, 0.024, 0, 0, 0.036, 0.006, 0.07, 0.12, 0.038, 0.022, 0.01, 0, 0, 0.004, 0.004, 0.184, 0, 0.046, 0.024, 0.154, 0.024, 0, 0.014, 0, 0, 0.038, 0, 0, 0.026, 0, 0, 0.098, 0, 0.11, 0.124, 0.004, 0, 0.172, 0.022)

rp <- matrix(newRPData, ncol=13)
rp <- t(apply(rp, 1, FUN=function(x) { x/sum(x) }))
dimnames(rp) <- list(NULL, nameData)
str(rp)


# Run the optimization backtest with quarterly rebalancing
opt_rebal <- optimize.portfolio.rebalancing(R = asset_returns, portfolio = port_spec, optimize_method = "random", rp = rp, trace = TRUE, search_size = 1000, rebalance_on = "quarters", training_period = 60, rolling_window = 60)

# Print the output of the optimization backtest
print(opt_rebal)


# Extract the objective measures for the single period optimization
extractObjectiveMeasures(opt)

# Extract the objective measures for the optimization backtest
extractObjectiveMeasures(opt_rebal)


# Extract the optimal weights for the single period optimization
extractWeights(opt)

# Chart the weights for the single period optimization
chart.Weights(opt)

# Extract the optimal weights for the optimization backtest
extractWeights(opt_rebal)

# Chart the weights for the optimization backtest
chart.Weights(opt_rebal)

```
  
  
  
***
  
Chapter 3 - Objective Functions and Moment Estimation  
  
Introduction to Moments:  
  
* Typically, there are fairly hard constraints on the assets, constraints, and objectives for the portfolio optimization  
* However, the moments of asset returns needs to be estimated, which induces estimation error  
	* First moment - expected returns  
    * Second moment - variance-covariance  
    * Third moment - coskewness  
    * Fourth moment - cokurtosis  
* Asset Return Moment Estimates can be based on any of the following - risk of error increase with the number of estimates required to be made  
	* Sample  
    * Shrinkage Estimators  
    * Factor Model  
    * Expressing Views  
    * Robust Statistics  
* The set.portfolio.moments(R=, portfolio=, method=) will set the portfolio moments approach - R is returns, portfolio is the portfolio.spec(), method can be one of c("sample", "boudt", "black_litterman", "meucci")  
  
Custom Moment Functions:  
  
* Needs to accept the arguments R for returns and portfolio for the portfolio specification object  
* Needs to return a named list with elements $mu, $sigma, $m3, and $m4  
	* Can use MASS package for mcd (minimum covariance determinant) and mve (minimum volume elipsoid)  
    * custom_fun <- function(R, portfolio, rob_method="mcd") { out <- list() ; out$sigma <- cov.rob(R, method=rob_method); return(out) }  
    * optimize.portfolio(R, portfolio, momentFUN="custom_fun", rob_method="mcd")  # for MCD  
    * optimize.portfolio(R, portfolio, momentFUN="custom_fun", rob_method="mve")  # for MVE  
  
Objective Functions:  
  
* The objective function in PortfolioAnalytics can be any valid R function  
	* StdDev() and ES() are two of the most cmmonly used - standard deviation and expected shortfall  
    * Can also use benchmark-relative functions for optimizations  
* The custom objective function needs to include R, weights, mu, sigma, m3, m4 as arguments, and should return a single value  
  
Example code includes:  
```{r}

# Add a return objective with "mean" as the objective name
port_spec <- add.objective(portfolio = port_spec, type = "return", name = "mean")

# Calculate the sample moments
moments <- set.portfolio.moments(R = asset_returns, portfolio = port_spec)

# Check if moments$mu is equal to the sample estimate of mean returns
moments$mu == colMeans(asset_returns)

# Add a risk objective with "StdDev" as the objective name
port_spec <- add.objective(portfolio = port_spec, type = "risk", name = "StdDev")

# Calculate the sample moments using set.portfolio.moments. Assign to a variable named moments.
moments <- set.portfolio.moments(R = asset_returns, portfolio = port_spec)

# Check if moments$sigma is equal to the sample estimate of the variance-covariance matrix
all.equal(moments$sigma, cov(asset_returns))


# Print the portfolio specification object
print(port_spec)

# Fit a statistical factor model to the asset returns
fit <- statistical.factor.model(R = asset_returns, k = 3)

# Estimate the portfolio moments using the "boudt" method with 3 factors
moments_boudt <- set.portfolio.moments(R = asset_returns, portfolio = port_spec, method = "boudt", k = 3)

# Check if the covariance matrix extracted from the model fit is equal to the estimate in `moments_boudt`
all.equal(moments_boudt$sigma, extractCovariance(fit))


# Define custom moment function
moments_robust <- function(R, portfolio, seed=NULL){
    out <- list()
    if (is.null(seed)) {
        out$sigma <- MASS::cov.rob(R, method = "mcd")$cov
    } else {
        out$sigma <- MASS::cov.rob(R, method = "mcd", seed=seed)$cov
    }
    out
}

# Estimate the portfolio moments using the function you just defined 
moments <- moments_robust(R = asset_returns, portfolio = port_spec)

# Check the moment estimate (will be differences due to RNG)
all.equal(MASS::cov.rob(asset_returns, method = "mcd")$cov, moments$sigma)


# Estimate the portfolio moments using the function you just defined 
set.seed(19101508)
moments <- moments_robust(R = asset_returns, portfolio = port_spec)

# Check the moment estimate (no more differences due to same seed)
set.seed(19101508)
all.equal(MASS::cov.rob(asset_returns, method = "mcd")$cov, moments$sigma)


# Create a portfolio specification object using asset_names
port_spec <- portfolio.spec(assets=asset_names)
port_spec <- add.constraint(portfolio = port_spec, type = "full_investment")
port_spec <- add.constraint(portfolio = port_spec, type = "long_only")
port_spec <- add.objective(portfolio = port_spec, type = "risk", name = "StdDev")
port_spec

rpData <- c(0.0769, 0.004, 0.014, 0.02, 0.138, 0.004, 0.008, 0, 0.012, 0.018, 0.062, 0.066, 0.176, 0.25, 0, 0.032, 0.1, 0.002, 0.194, 0.326, 0.004, 0.126, 0, 0.036, 0.304, 0.004, 0.002, 0, 0.036, 0, 0, 0.004, 0.146, 0.04, 0.01, 0.192, 0.272, 0.086, 0.074, 0.026, 0.092, 0.024, 0.102, 0, 0.098, 0.102, 0.14, 0.0769, 0.006, 0.046, 0.016, 0, 0.096, 0.146, 0.594, 0.17, 0.16, 0.324, 0.022, 0, 0.054, 0.01, 0.024, 0, 0.002, 0.168, 0.106, 0, 0.002, 0.012, 0.07, 0, 0.388, 0.114, 0.046, 0, 0.406, 0.046, 0.244, 0.254, 0.114, 0.118, 0.07, 0.012, 0, 0.12, 0.002, 0, 0.014, 0, 0.008, 0.048, 0.072, 0.07, 0.0769, 0.02, 0.162, 0.01, 0.014, 0.066, 0, 0.032, 0.002, 0.01, 0.092, 0.026, 0.016, 0, 0.074, 0, 0.012, 0.598, 0.032, 0, 0.042, 0.048, 0.424, 0, 0, 0, 0.622, 0.002, 0.006, 0, 0.036, 0.034, 0, 0.088, 0.002, 0.126, 0.034, 0.07, 0.01, 0.002, 0.17, 0.54, 0.108, 0.002, 0.008, 0.016, 0.124, 0.0769, 0.03, 0.138, 0, 0, 0.096, 0.004, 0, 0, 0, 0.008, 0.05, 0, 0, 0.002, 0.022, 0.018, 0.004, 0.052, 0.02, 0.032, 0.004, 0, 0, 0, 0.276, 0.012, 0.028, 0, 0.004, 0, 0.052, 0, 0, 0.006, 0.034, 0.158, 0, 0.002, 0.004, 0.028, 0.174, 0, 0.178, 0.13, 0.066, 0.054, 0.0769, 0.02, 0.012, 0.014, 0.242, 0.072, 0.54, 0.022, 0.032, 0.07, 0.006, 0, 0.002, 0.02, 0, 0.014, 0.132, 0, 0.05, 0, 0.002, 0.074, 0.004, 0.082, 0, 0.024, 0.008, 0.02, 0.088, 0.168, 0.24, 0.158, 0.116, 0.132, 0.002, 0.022, 0.006, 0.004, 0.008, 0.008, 0.054, 0.01, 0, 0, 0.118, 0.448, 0.18, 0.0769, 0.004, 0.01, 0.116, 0, 0.248, 0.036, 0.208, 0, 0.178, 0.008, 0.16, 0, 0.108, 0, 0.028, 0.048, 0, 0.052, 0, 0.068, 0, 0.25, 0.114, 0.214, 0.044, 0.006, 0.082, 0.76, 0.144, 0.282, 0.028, 0.074, 0.044, 0.598, 0.164, 0.226, 0, 0.264, 0.052, 0.152, 0.002, 0.054, 0.618, 0.552, 0.014, 0, 0.0769, 0, 0.614, 0.484, 0, 0.006, 0, 0.002, 0, 0.07, 0, 0.196, 0.056, 0.004, 0, 0.47, 0, 0, 0.076, 0, 0.18, 0.008, 0.07, 0, 0, 0, 0.04, 0.002, 0, 0.028, 0.016, 0.006, 0.032, 0.002, 0, 0.174, 0.034, 0, 0.04, 0.006, 0, 0.034, 0.014, 0.092, 0.012, 0.112, 0.114, 0.0769, 0.752, 0, 0.116, 0.494, 0.142, 0, 0.138, 0.54, 0.004, 0.004, 0.072, 0, 0.43, 0.212, 0.058, 0.36, 0.014, 0.002, 0, 0.012, 0.308, 0.066, 0.47, 0, 0.002, 0.004, 0.32, 0.004, 0.032, 0.008, 0.036, 0.002, 0.012, 0.008, 0.056, 0.186, 0, 0.286, 0.02, 0, 0.018, 0.02, 0.028, 0, 0.018, 0.008, 0.0769, 0, 0.004, 0.026, 0, 0.086, 0.14, 0, 0.006, 0, 0.266, 0.004, 0.126, 0.02, 0.066, 0.136, 0, 0, 0.006, 0.08, 0.53, 0.006, 0.024, 0, 0.336, 0.002, 0.018, 0.174, 0, 0.032, 0, 0.09, 0.018, 0.022, 0.04, 0.038, 0.006, 0.508, 0, 0.008, 0.076, 0.01, 0.084, 0.012, 0.008, 0.026, 0.012, 0.0769, 0, 0, 0, 0.012, 0.016, 0.026, 0, 0.012, 0.132, 0, 0.008, 0.196, 0, 0, 0, 0.01, 0, 0.002, 0, 0.084, 0.042, 0.01, 0.018, 0.106, 0.24, 0.032, 0.018, 0.008, 0.088, 0.01, 0.088, 0, 0.014, 0.016, 0.048, 0.026, 0.124, 0.006, 0.13, 0.184, 0.038, 0.256, 0.03, 0.024, 0.028, 0.002, 0.0769, 0.004, 0, 0.186, 0, 0.006, 0.036, 0, 0.054, 0, 0.032, 0.254, 0, 0.112, 0.602, 0.03, 0.01, 0.078, 0.038, 0.454, 0.012, 0.034, 0.07, 0.21, 0, 0.01, 0.006, 0.282, 0, 0.068, 0.002, 0.236, 0.038, 0.044, 0.018, 0.072, 0.026, 0.028, 0.078, 0.134, 0.16, 0.13, 0.064, 0, 0, 0.004, 0.05, 0.0769, 0.002, 0, 0.01, 0, 0.154, 0.056, 0.004, 0.05, 0.204, 0.022, 0.12, 0, 0.002, 0.034, 0.154, 0.016, 0.288, 0.238, 0.004, 0.02, 0.284, 0.056, 0, 0, 0.008, 0.114, 0.004, 0.086, 0.012, 0.102, 0.02, 0.314, 0.092, 0.182, 0.002, 0.012, 0, 0.106, 0.258, 0.032, 0, 0.298, 0.032, 0, 0.08, 0.098, 0.0769, 0.158, 0, 0.002, 0.1, 0.008, 0.008, 0, 0.122, 0.154, 0.176, 0.022, 0.428, 0, 0, 0.032, 0.294, 0.014, 0.09, 0.01, 0.014, 0.064, 0.014, 0, 0.04, 0.002, 0.022, 0.022, 0.012, 0.018, 0.258, 0.004, 0.006, 0.396, 0, 0.002, 0.002, 0.18, 0.006, 0.35, 0.052, 0.006, 0, 0, 0.002, 0.014, 0.148)

rp <- matrix(rpData, ncol=13)
rp <- t(apply(rp, 1, FUN=function(x) { x/sum(x) }))
dimnames(rp) <- list(NULL, nameData)
str(rp)


# Run the optimization with custom moment estimates
opt_custom <- optimize.portfolio(R = asset_returns, portfolio = port_spec, 
                                 optimize_method = "random", rp = rp, momentFUN = "moments_robust"
                                 )

# Print the results of the optimization with custom moment estimates
print(opt_custom)

# Run the optimization with sample moment estimates
opt_sample <- optimize.portfolio(R = asset_returns, portfolio = port_spec, 
                                 optimize_method = "random", rp = rp
                                 )

# Print the results of the optimization with sample moment estimates
print(opt_sample)


# Custom annualized portfolio standard deviation
pasd <- function(R, weights, sigma, scale = 12){
    sqrt(as.numeric(t(weights) %*% sigma %*% weights)) * sqrt(scale)
}

# Add custom objective to portfolio specification
port_spec <- add.objective(portfolio = port_spec, type = "risk", name = "pasd")

# Print the portfolio specificaton object
port_spec

set_sigma <- function(R){
  out <- list()
  out$sigma <- cov(R)
  out
}

opt <- optimize.portfolio(R = asset_returns, portfolio = port_spec, momentFUN = set_sigma, 
                          optimize_method = "random", rp = rp
                          )

# Print the results of the optimization
print(opt)

```
  
  
  
***
  
Chapter 4 - Application  
  
Applications:  
  
* Attempt to construct and solve portfolios with different styles and strategies  
	* Monthly returns from EDHEC will be available for January 1997 - March 2016  
* Need to choose an appropriate benchmark, such as S&P 500, or a blend of available indices  
* Typically, the starting point is to assume an even-weight index, then run a base (not constrained) optimization  
	* Constraints and more sophisticated inputs are typically follow-on to the basic methodology  
  
Optimization Backtest:  
  
* Run with quarterly rebalancing and 5 years (60 months) of data  
* Extract optimal weights at each balancing period  
* Pass weights to see historical returns  
* Minimize concentration risk (diversify) with elements such as a box constraint - for example, requiring all assets to be 5% to 40%  
  
Wrap up:  
  
* Portfolio optimization and the PortfolioAnalytics package  
* Take on larger and more advanced portfolio optimization problems  
  
Example code includes:  
```{r}

par(mfrow=c(1, 1))
par(mfcol=c(1, 1))
par(mar=c(5, 4, 4, 2) + 0.1)


library(PortfolioAnalytics)

# Load the data
data(edhec)
str(edhec)

# Assign the data to a variable
asset_returns <- edhec

# Create a vector of equal weights
equal_weights <- rep(1 / ncol(asset_returns), ncol(asset_returns))

# Compute the benchmark returns
r_benchmark <- Return.portfolio(R = asset_returns, weights = equal_weights, rebalance_on = "quarters")
colnames(r_benchmark) <- "benchmark"

# Plot the benchmark returns
plot(r_benchmark)


# Create the portfolio specification
port_spec <- portfolio.spec(assets=colnames(asset_returns))

# Add a full investment constraint such that the weights sum to 1
port_spec <- add.constraint(portfolio=port_spec, type="full_investment")

# Add a long only constraint such that the weight of an asset is between 0 and 1
port_spec <- add.constraint(portfolio=port_spec, type="long_only")

# Add an objective to minimize portfolio standard deviation
port_spec <- add.objective(portfolio=port_spec, type="risk", name="StdDev")

# Print the portfolio specification
port_spec


# Run the optimization
opt_rebal_base <- optimize.portfolio.rebalancing(R = asset_returns, portfolio = port_spec, 
                                                 optimize_method = "ROI", rebalance_on = "quarters",
                                                 training_period = 60, rolling_window = 60
                                                 )

# Print the results
print(opt_rebal_base)

# Chart the weights
chart.Weights(opt_rebal_base)

# Compute the portfolio returns
returns_base <- Return.portfolio(R = asset_returns, weights = extractWeights(opt_rebal_base))
colnames(returns_base) <- "base"


# Add a risk budge objective
port_spec <- add.objective(portfolio = port_spec, type = "risk_budget", name = "StdDev", 
                           min_prisk = 0.05, max_prisk = 0.1
                           )

# Run the optimization
opt_rebal_rb <- optimize.portfolio.rebalancing(R = asset_returns, portfolio = port_spec, 
                                               optimize_method = "random", rp =rp, trace = TRUE,
                                               rebalance_on = "quarters", training_period = 60,
                                               rolling_window = 60
                                               )

# Chart the weights
chart.Weights(opt_rebal_rb)

# Chart the percentage contribution to risk
chart.RiskBudget(opt_rebal_rb, match.col = "StdDev", risk.type = "percentage")

# Compute the portfolio returns
returns_rb <- Return.portfolio(R = asset_returns, weights = extractWeights(opt_rebal_rb))
colnames(returns_rb) <- "risk_budget"


# Run the optimization
opt_rebal_rb_robust <- optimize.portfolio.rebalancing(R = asset_returns, momentFUN = "moments_robust",
                                                      portfolio = port_spec, optimize_method = "random", 
                                                      rp = rp, trace = TRUE, rebalance_on = "quarters", 
                                                      training_period = 60, rolling_window = 60
                                                      )

# Chart the weights
chart.Weights(opt_rebal_rb_robust)

# Chart the percentage contribution to risk
chart.RiskBudget(opt_rebal_rb_robust, match.col = "StdDev", risk.type = "percentage")

# Compute the portfolio returns
returns_rb_robust <- Return.portfolio(R = asset_returns, weights = extractWeights(opt_rebal_rb_robust))
colnames(returns_rb_robust) <- "rb_robust"


# Combine the returns
ret <- cbind(r_benchmark, returns_base, returns_rb, returns_rb_robust)

# Compute annualized returns
table.AnnualizedReturns(R = ret)

# Chart the performance summary
charts.PerformanceSummary(R = ret)

```
  
  
  
***
  
### _Bond Valuation and Analysis in R_  
  
Chapter 1 - Introduction and Plain Vanilla Bond Valuation  
  
Introduction - Clifford Ang, author of books in the area:  
  
* Bonds require payment of both principal and interest  
* Characteristics of bonds include  
	* Issuer - government, corporation, etc.  
    * Principal (aka par value or face value) - amount borrowed  
    * Coupon rate - interest percents and payments (can be fixed or floating)  
    * Maturity - when the bond expires and the principal repayment is made (in a few bonds, there is no maturity)  
    * Embedded Options - for example, callable bonds (beyond the scope of this course)  
* Bond for use during this course - fixed maturity date, fixed coupon, no embedded options  
	* Price - amount paid to acquire the asset  
    * Value - how much the asset is worth  
    * For actively traded assets, price and value are generally very close  
  
Time Value of Money (TVM) - generally, a dollar today is worth more than a dollar tomorrow:  
  
* Future value - how much is $1 some time in the future worth today - determined by the interest rate, r  
	* FV(n-period) = PV * (1 + r)**n  
    * PV = FV(n) / (1 + r)**n  
* TMV concepts can be applied to bonds  
	* FV = 100, coupon = 5%, price today is 100, matures in 5 years  
    * Cash flows would be 5, 5, 5, 5, 105 (interest plus par)  
    * Use interest rate, r, to calculate the present value, and compare to the 100 price  
  
Bond Valuation:  
  
* Value of an asset is typically considered to be the present value of the expected future cash flows  
	* Each of the coupon payments, discounted by the interest rate  
    * The payment of the par value at maturity, discounted by the interets rate  
    * Can be managed using a cash flow vector in R, where the last element of the vector is the sum of the final coupon payment and the par value  
  
Convert Code to Functions:  
  
* Can create a function - such as bondprc(p, r, ttm, y) - to better automate and repeat the valuation process  
	* p - par, r - rate, ttm - time to maturity, y - yield  
  
Example code includes:  
```{r}

# Create pv
pv <- 100

# Create r
r <- 0.1

# Calculate fv1
fv1 <- pv * (1 + r)

# Calculate fv2
fv2 <- pv * (1+r)**2


# Calculate pv1
pv1 <- fv1 / (1 + r)

# Calculate pv2
pv2 <- fv2 / (1 + r)^2

# Print pv1 and pv2
print(pv1)
print(pv2)


# Create vector of cash flows
cf <- c(5, 5, 5, 5, 105)

# Convert to data frame
cf <- data.frame(cf)
cf


# Add column t to cf
cf$t <- as.numeric(rownames(cf))

# Calculate pv_factor
cf$pv_factor <- 1 / (1 + 0.06)^cf$t

# Calculate pv
cf$pv <- cf$cf * cf$pv_factor

# Calculate the bond price
sum(cf$pv)


# Create function
bondprc <- function(p, r, ttm, y) {
    cf <- c(rep(p * r, ttm - 1), p * (1 + r))
    cf <- data.frame(cf)
    cf$t <- as.numeric(rownames(cf))
    cf$pv_factor <- 1 / (1 + y)^cf$t
    cf$pv <- cf$cf * cf$pv_factor
    sum(cf$pv)
}

# Verify prior result
bondprc(100, 0.05, 5, 0.06)

```
  
  
  
***
  
Chapter 2 - Yield to Maturity  
  
Price-Yield Relationship:  
  
* There is an inverse relationship between bond yields and bond prices  
* Bond yields are typically related to the risk of the bond (often published by rating agencies)  
	* Investment grade is BBB and better  
    * High-yield is BB and lower (sometimes known as junk bonds)  
* Sometimes, the desired yield is considered to be driven by the average of other bonds with the same credit rating  
	* Can use Quandl() to get this data  
  
Components of Yield:  
  
* Bonds with zero default risk generally earn the risk-free yield (assumed to be a US Treasury of similar duration)  
* FRED Data can be loaded to R Workspace - t10yr <- getSymbols("DGS10", src="FRED", auto.assign=FALSE)  # puts the 10-year Treasury yield in t10yr  
* Spread is another component of bond yields - basically, the amount over and above the risk-free yield to account for the risks of holding the specific bond  
	* Credit risk is a primary driver of spread, though there is also inflation risk, call risk, liquidity risk (common issue with bonds that trade infrequently), and the like  
* Nervous markets tend to require higher spreads - can see with the difference between AAA and BBB yields  
  
Estimating Yield of a Bond:  
  
* Can estimate the yield based on the bond price using trial and error for matching up yields and prices  
* Automation is needed to speed up the process - uniroot() helps with the automation  
	* ytm <- function(cf) { uniroot(bval, c(0, 1), cf=cf)$root }  # the c(0, 1) limits the search to the space containing (0, 1)  
    * The cf is the cash flow vector - price as a negative, coupons and par as positive  
  
Example code includes:  
```{r cache=TRUE}

# Load Quandl package
library(Quandl)

# Obtain Moody's Baa index data
baa <- Quandl("FED/RIMLPBAAR_N_M")

# Identify 9/30/16 yield
baa_yield <- subset(baa, baa$Date == "2016-09-30")

# Convert yield to decimals and view
baa_yield <- baa_yield$Value/100
baa_yield


bondprc(p = 100, r = 0.05, ttm = 5, y = baa_yield)


# Generate prc_yld
prc_yld <- seq(0.02, 0.4, by=0.01)

# Convert prc_yld to data frame
prc_yld <- data.frame(prc_yld)

# Calculate bond price given different yields
for (i in 1:nrow(prc_yld)) {
     prc_yld$price[i] <- bondprc(100, 0.10, 20, prc_yld$prc_yld[i])  
}

# Plot P/YTM relationship
plot(prc_yld, type = "l", col = "blue", main = "Price/YTM Relationship")


# Generate prc_yld
prc_yld <- seq(0.02, 0.4, by=0.01)

# Convert prc_yld to data frame
prc_yld <- data.frame(prc_yld)

# Calculate bond price given different yields
for (i in 1:nrow(prc_yld)) {
     prc_yld$price[i] <- bondprc(100, 0.10, 20, prc_yld$prc_yld[i])  
}

# Plot P/YTM relationship
plot(prc_yld, type = "l", col = "blue", main = "Price/YTM Relationship")


# Examine first and last six elements in spread
# head(spread)
# tail(spread)

# Calculate spread$diff
# spread$diff <- 100 * (spread$baa - spread$aaa)

# Plot spread
# plot(x = spread$date, y = spread$diff, type = "l", xlab = "Date", ylab = "Spread (bps)",
#      col = "red", main = "Baa - Aaa Spread"
#      )


# Value bond using 5% yield
bondprc(p = 100, r = 0.05, ttm = 5, y = 0.05)

# Value bond using 7% yield
bondprc(p = 100, r = 0.05, ttm = 5, y = 0.07)

# Value bond using 6% yield
bondprc(p = 100, r = 0.05, ttm = 5, y = 0.06)


# Create cash flow vector
cf <- c(-95.79, 5, 5, 5, 5, 105)

# Create bond valuation function
bval <- function(i, cf, t=seq(along = cf)) sum(cf / (1 + i)^t)

# Create ytm() function using uniroot
ytm <- function(cf) {
    uniroot(bval, c(0, 1), cf = cf)$root
}

# Use ytm() function to find yield
ytm(cf)

```
  
  
  
***
  
Chapter 3 - Duration and Convexity  
  
Bond price volatility and Price Value of Basis Point:  
  
* Bond price volatility varies with size of yield change, coupon rate, and time to maturity  
	* For small changes in yield (positive or negative), changes in price are symmetric  
    * For larger changes in yield, large decreases in yield drive higher changes in price than large decreases in yield  
    * The lower the coupon rate, the more the bond is impacted by a change in yield  
    * The lower the time to maturity, the less the bond price is impacted by a change in yield  
* Price Value of Basis Point - impact of change in yield of 0.01%  
	* The value is often called pv01  
    * Typically, abs() is used so that the value is positive  
  
Duration - estimated price change for a 100 basis point change in yield:  
  
* Matching duration across assets and liabilities reduces risk of loss in an environment with changing yields  
* Approximate duration is (Price-down - Price-up) / (2 * Price * Delta-yield)  
	* Delta-P / P = -Duration * Delta-Yield  
* The approximate duration is more or less the tangent to the yield curve - works well for small changes in yield  
	* The convexity measure can help make the approximations better for even the larger changes in yield  
* Absent any coupons, the duration is roughly equal to the time to maturity  
  
Convexity - curvature is not well captured by duration:  
  
* Convexity Measure, referred to as C, can be approximated as [ P(down) + P(up) - 2*P ] / [ P * (delta-Y)**2 ]  
    * Delta-P / P = 0.5 * C * (delta-Y)**2  
    * The convesity measure is invariant to the direction of the yield-change  
    * Can then add together both the duration estimate AND the convexity estimate to get the total change  
* Convexity meaningfully improves the estimate of bond price, though still not a perfect fit (especially for very large changes in yield)  
  
Example code includes:  
```{r}

# Calculate the PV01
PV01 <- abs( bondprc(p=100, r=0.1, ttm=20, y=0.10) - bondprc(p=100, r=0.1, ttm=20, y=0.1001) )


# Calculate bond price today
px <- bondprc(p = 100, r = 0.1, ttm = 20, y = 0.1)
px

# Calculate bond price if yields increase by 1%
px_up <- bondprc(p = 100, r = 0.1, ttm = 20, y = 0.11)
px_up

# Calculate bond price if yields decrease by 1%
px_down <- bondprc(p = 100, r = 0.1, ttm = 20, y = 0.09)
px_down

# Calculate approximate duration
duration <- (px_down - px_up) / (2 * px * 0.01)
duration


# Estimate percentage change
duration_pct_change <- -duration * -0.01
duration_pct_change

# Estimate dollar change
duration_dollar_change <- duration_pct_change * px
duration_dollar_change


# Calculate approximate convexity
convexity <- (px_up + px_down - 2 * px) / (px * (0.01)^2)
convexity


# Estimate percentage change
convexity_pct_change <- 0.5 * convexity * (0.01)^2
convexity_pct_change

# Estimate dollar change
convexity_dollar_change <- convexity_pct_change * px
convexity_dollar_change


# Estimate change in price
price_change <- duration_dollar_change + convexity_dollar_change

# Estimate price
price <- px + duration_dollar_change + convexity_dollar_change

```
  
  
  
***
  
Chapter 4 - Comprehensive Example  
  
Summarizing main lessons:  
  
* Bond valuation, even for bonds that are not traded frequently (limited if any price discovery)  
	* Examples for this course are annual coupons at a fixed rate, with a fixed time to maturity, and with no options such as callability  
    * Real-world bonds are heterogeneous and may differ significantly from this course examples  
* Yields consist of both the risk-free yield (US treausry) plus the spread (many types of risk - default, callability, lack of liquidity, etc.)  
  
Duration and convexity:  
  
* Bond price volatility is driven by size of yield change, coupon rate, and time to maturity  
* Duration can be a poor measure of price vs. yield for larger yield changes, and the convexity   approximation can improve these estimations  
  
Wrap up:  
  
* Fundamental concepts of bond price valuation  
* "Analyzing Financial Data and Implementing Financial Models Using R" by Ang - wide range of fixed income topics  
  
Example code includes:  
```{r cache=TRUE}

# Load Quandl package
library(Quandl)

# Obtain Moody's Aaa yield
aaa <- Quandl("FED/RIMLPAAAR_N_M")

# identify yield on September 30, 2016
aaa_yield <- subset(aaa, aaa$Date == "2016-09-30")

# Convert yield into decimals
aaa_yield <- as.numeric(aaa_yield$Value) / 100
aaa_yield


# Layout the bond's cash flows
cf <- c(3, 3, 3, 3, 3, 3, 3, 103)

# Convert to data.frame
cf <- data.frame(cf)

# Add time indicator
cf$t <- seq(1, 8, by=1)

# Calculate PV factor
cf$pv_factor <- 1 / (1 + aaa_yield)^cf$t

# Calculate PV
cf$pv <- cf$cf * cf$pv_factor

# Price bond
sum(cf$pv)


# Code cash flow function
alt_cf <- function(r, p, ttm) {
    c(rep(p * r, ttm - 1), p * (1 + r))
}

# Generate cf vector
alt_cf(r = 0.03, p = 100, ttm = 8)


# Calculate bond price when yield increases
px_up <- bondprc(p = 100, r = 0.03, ttm = 8, y = aaa_yield+0.01)

# Calculate bond price when yield decreases
px_down <- bondprc(p = 100, r = 0.03, ttm = 8, y = aaa_yield-0.01)

# Calculate duration
duration <- (px_down - px_up) / (2 * px * 0.01)

# Calculate percentage effect of duration on price
duration_pct_change <- -duration * 0.01
duration_pct_change

# Calculate dollar effect of duration on price
duration_dollar_change <- duration_pct_change * px
duration_dollar_change


# Calculate convexity measure
convexity <- (px_up + px_down - 2*px) / (px * 0.01^2)

# Calculate percentage effect of convexity on price
convexity_pct_change <- 0.5 * convexity * 0.01^2
convexity_pct_change

# Calculate dollar effect of convexity on price
convexity_dollar_change <- convexity_pct_change * px
convexity_dollar_change


# Estimate price_change
price_change <- duration_dollar_change + convexity_dollar_change
price_change

# Estimate new_price
new_price <- px + duration_dollar_change + convexity_dollar_change
new_price

```
  
  
  
### _Equity Valuation in R_  
  
Chapter 1 - Present Value Apparoaches  
  
Course introduction and fundamental valuation:  
  
* DCF (Discounted Cash Flow) analysis relies on the concept of time value of money  
* The discount rate determines the amount that money tomorrow needs to be deflated to be equivalent to an amount of money today  
	* FV / (1 + r) = PV, provided that r is the discount rate for the time period between FV and PV  
    * FV / (1 + r)**n = PV, provided that PV and FV have n time-periods apart where r is the discount rate for 1 time-period  
* Two methodologies for valuing an equity  
	* Free Cash Flow to Equity (FCFE) - direct valuation of the equity component, discounted by CAPM (cost of equity)  
    * Free Cash Flow to Firm (FCFF) - Assets minus Debts to get Equity, discounted at the Weighted Average Cost of Capital  
    * This course will focus on FCFE; in theory, both approaches should lead to the same valuations  
  
Free cash flow to equity model (FCFE):  
  
* FCFE is cash flow after paying off all suppliers, employees, lenders, and governments; setting aside money for capital needs; and being net of debt issues and repayments  
	* For this course, the assumption will be the net debt issues and repayments are $0  
* The first step of FCFE is to determine the Gross Profit (Revenus less COGS), less Operating Expenses (becomes EBIT), less Interest Expenses (becomes Pre-Tax Income), less Taxes (becomes After-Tax Income or Net Income)  
* Several adjustments to After-Tax Income are required since accrual and cash do not always occur at the same time - FCFE is After-Tax Income changed by:  
	* Add back Depreciation and Amortization (cash was spent when purchased)  
    * Less Capital Expenditures (cash goes out at time of purchase)  
    * Less Increases in Working Capital (cash tied up in company operations)  
* Terminal value is cash flow beyond the 5-10 year horizon for corporate financial planning  
	* Perpetuity Growth Model - TV = FCFE(T) * (1 + g) / (Ke - g) where Ke is the cost of equity and g is the growth rate  
  
Calculating equity value:  
  
* Equity value is the present value of FCFE during the project period, plus the present value of the FCFE of the terminal period  
	* Assume (1 + ke) as the discount rate, where ke is the cost of equity  
    * Terminal value is discounted by (1 + ke)**n, where n is when the terminal volume happens  
* Equity value is typically divided by shares outstanding, leading to an overall equity value per share  
  
Example code includes:  
```{r}

fv <- 100
r <-0.05

# Calculate PV of $100 one year from now
pv_1 <- fv / (1+r)
pv_1

# Calculate PV of $100 two years from now
pv_2 <- fv / (1+r)**2
pv_2


# After-tax income differs from free cash flow to equity (FCFE) because it includes non-cash items and does not exclude any additional investments necessary to maintain the firm's operations and grow the firm based on its projections

after_tax_income <- c(22.8, 24, 30.6, 38.4, 43.2)
capex <- c(11, 11, 12, 14, 15)
depn_amort <- c(11, 12, 12, 14, 15)
incr_wc <- c(16, 16, 14, 14, 14)

# Calculate FCFE
fcfe <- after_tax_income - capex + depn_amort - incr_wc
fcfe


g <- 0.034
ke <- 0.105

# Calculate the terminal value as of 2021
tv_2021 <- fcfe[length(fcfe)] * (1 + g)/ (ke - g)
tv_2021


fcfe <- as.data.frame(fcfe)
rownames(fcfe) <- 2017:2021

# Add discount periods
fcfe$periods <- 1:5
fcfe

# Calculate Present Value Factor
fcfe$pv_factor <- (1+ke)**(-fcfe$periods)
fcfe

# Calculate Present Value of each Cash Flow
fcfe$pv <- fcfe$fcfe * fcfe$pv_factor
fcfe

# Total Present Value
pv_fcfe <- sum(fcfe$pv)
pv_fcfe


# Calculate Present Value
pv_tv <- tv_2021 * (1 + ke)**-5
pv_tv


# Calculate Equity Value
eq_val <- pv_fcfe + pv_tv
eq_val

shout <- 10

# Calculate Equity Value Per Share
eq_val_per_share <- eq_val / shout
eq_val_per_share

```
  
  
  
***
  
Chapter 2 - Perpetuity Growth Rate, Analyzing Projections, Dividend Discount Model  
  
Analyzing the projections:  
  
* Need to carefully check all of the inputs (historical and projections), since GIGO is a real risk  
* Can either plot or run a trend analysis with dummy variables - 1 for projection period, 0 for historical period  
    * Assess level of significance between historical and projected revenues  
  
Perpetuity growth rate (PGR) - "one of the most commonly abused inputs in the model":  
  
* Many analysts do not link the PGR to the financing needed in the terminal period; significantly over-values terminal period value  
* The PGR should be bounded by the relationship PGR = Reinvestment Rate * Return on Equity (this is the maximum PGR)  
	* Reinvestment Rate = (CapEx + Increase in Working Capital - Depreciation/Amortization) / After-Tax-Income  
    * Return on Equity should be Cost of Equity in steady state  
* Higher reinvestment rates and higher returns on equity should drive higher PGR  
  
Dividend discount model:  
  
* Preferred stock has seniority over common stocks, and receive dividends that can be deferred without the company defaulting  
* Generally, common stock dividends may only be paid after all preferred stock dividends have been paid (preferred stock is therefore a safer investment)  
	* Price Estimate = Dividend / (Cost of Capital - Constant Dividend Growth Rate)  
    * This model is known as a "dividend discount model"  
* Can use a two-stage model for a firm that does not pay dividends now, but expects to start (and continue) paying dividends at time period T+1  
	* V = 0 + [ Dividend(T+1) / (k - g) ] * (1 + k)**-T  
  
Example code includes:  
```{r}

hist_rev <- c(86.8, 89, 93, 128.6, 176.4, 171.4, 214.2, 236, 0, 0, 0, 0, 0)
rev_proj <- c(0, 0, 0, 0, 0, 0, 0, 0, 193.2, 212.9, 225, 279.2, 295.9)


# Combine hist_rev and rev_proj
rev_split <- rbind(hist_rev, rev_proj)

# Rename the column headers
colnames(rev_split) <- seq(2009, 2021, 1)

# Create a bar plot of the data
barplot(rev_split, col = c("red", "blue"), main = "Historical vs. Projected Revenues")
legend("topleft", legend = c("Historical", "Projected"), fill = c("red", "blue"))


rev_all <- data.frame(rev_proj = pmax(hist_rev, rev_proj))

# Create a trend variable
rev_all$trend <- 1:13

# Create shift variable
rev_all$shift <- c(rep(0, 8), rep(1, 5))
str(rev_all)

# Run regression
reg <- lm(rev_proj ~ trend + shift, data=rev_all)

# Print regression summary
summary(reg)


# Calculate reinvestment amount
reinvestment <- (capex + incr_wc - depn_amort)[5]
reinvestment

# Calculate retention ratio
retention_ratio <- reinvestment / after_tax_income[5]
retention_ratio


# Calculate expected growth rate
exp_growth_rate <- retention_ratio * ke
exp_growth_rate


stated_value <- 25
div_rate <- 0.05
kp <- 0.1

# Calculate dividend of preferred stock
div <- stated_value * div_rate
div

# Calculate value of preferred stock
pref_value <- div / kp
pref_value


# Value of Preferred if dividends start five years from now
pref_value_yr5 <- (stated_value * div_rate) / kp
pref_value_yr5

# Value discounted to present
pref_value <- pref_value_yr5 / (1 + kp)**5
pref_value


# Preferred dividend in Years 1 to 5
high_div <- 2.5

# Create vector of Year 1-5 dividends
pref_cf <- rep(high_div, 5)

# Convert to data frame
pref_df <- data.frame(pref_cf)

# Add discount periods
pref_df$periods <- 1:5

# Calculate discount factors
pref_df$pv_factor <- (1+kp)**(-pref_df$periods)

# Calculate PV of dividends
pref_df$pv_cf <- pref_df$pref_cf * pref_df$pv_factor

# Calculate value during high stage
pref_value_high <- sum(pref_df$pv_cf)

# Calculate value of the preferred stock
pref_value_high + pref_value

```
  
  
  
***
  
Chapter 3 - Discount Rate and Cost of Capital  
  
What is a discount rate?  
  
* Capital Asset Pricing Model (CAPM) is the most common model used  
	* CAPM = Risk-Free-Rate + Beta * (Market-Return - Risk-Free-Rate)  
    * Market-Return minus Risk-Free Rate is the Equity Risk Premium (ERP)  
* There are firm-specific risks, and these are not incorporated in the beta  
	* Investing in a portfolio can diversify away these risks  
    * Therefore, only the systematic risk (cannot be diversified away) is included in the CAPM  
* The Market Model looks at stock returns regressed against market returns  
	* Stock-Return = alpha + beta * Market-Return  
  
Unlevering betas:  
  
* Beta is often estimated using the average of the betas for the firm's peer group  
* However, debt levels vary by firm, meaning that risk can be very different for otherwise similar firms - Hamada formula  
	* Beta-Unlevered = Beta-Levered / (1 + (1 - Corporate-Tax-Rate) * Debt-Equity-Ratio)  
    * Book value for debt is often used, since for healthy companies the book value and the market value of debt are frequently similar  
    * Can re-lever a firms beta assuming the median and/or target level of debt-to-equity for the industry  
* The Fernandez formula takes in to account that there is some risk assumed by the debt-holders (therefore not all of the risk is assumed by the equity-holders)  
	* Beta-Unlevered = [Beta-Levered + Beta-D * (1 - Corporate-Tax-Rate) * Debt-Equity-Ratio] / (1 + (1 - Corporate-Tax-Rate) * Debt-Equity-Ratio)  
    * Hamada formula equals Fernandez formula when Beta-D (the debt beta) is equal to zero  
    * Beta-D is often estimate using a regression of prices of bonds of the same caliber  
* The peer company beta approach has the advantage of mitigating firm-specific risks  
	* The first specific beta has the advantage of being a second data point, which can be compared against the peer company findings  
  
Risk-free rate and Equity Risk Premium:  
  
* The risk-free rate (no systematic risk) is based on a security with zero beta - based on US Treasuries of 10y, 20y, or 30y  
* The equity risk premium measures the excess return demanded for investing in stocks rather than US Treasuries  
	* Generally, should match the term (duration) of US Treasuries with the term used for measuring the equity risk premium  
    * ERP could be a look at 35 years of the typical return on stocks minus the typical return on treasuries - often 5% to 8%  
  
Example code includes:  
```{r cache=TRUE}

priceData <- c(21.46, 20.75, 23.44, 23.45, 21.69, 21.67, 21.37, 23.03, 23.57, 24.37, 25.34, 27.18, 27.45, 28.27, 29.61, 28.96, 29.11, 30.48, 31.03, 33.56, 35.34, 38.17, 37.87, 44.13, 43.4, 45.41, 55.57, 48.83, 50.78, 49.84, 51.56, 49.37, 48.6, 45.49, 53.55, 58.61, 56.37, 53.15, 57.33, 59.35, 72.26, 72.63, 67.86, 55.99, 49.59, 40.26, 44.09, 51.3, 54.07, 52.69, 45.07, 46.35, 41.71, 43.34, 43.24, 46.79, 42.36, 38.12, 36.5, 36.61, 38.15, 125.5, 131.32, 137.02, 140.81, 139.87, 131.47, 136.1, 137.71, 141.16, 143.97, 141.35, 142.15, 142.41, 149.7, 151.61, 156.67, 159.68, 163.45, 160.42, 168.71, 163.65, 168.01, 175.79, 181, 184.69, 178.18, 186.29, 187.01, 188.31, 192.68, 195.72, 193.09, 200.71, 197.02, 201.66, 207.2, 205.54, 199.45, 210.66, 206.43, 208.46, 211.14, 205.85, 210.5, 197.67, 191.63, 207.93, 208.69, 203.87, 193.72, 193.56, 205.52, 206.33, 209.84, 209.48, 217.12, 217.38, 216.3, 212.55, 220.38, 223.53)
idxDates <- c(15339, 15370, 15399, 15430, 15460, 15491, 15521, 15552, 15583, 15613, 15644, 15674, 15705, 15736, 15764, 15795, 15825, 15856, 15886, 15917, 15948, 15978, 16009, 16039, 16070, 16101, 16129, 16160, 16190, 16221, 16251, 16282, 16313, 16343, 16374, 16404, 16435, 16466, 16494, 16525, 16555, 16586, 16616, 16647, 16678, 16708, 16739, 16769, 16800, 16831, 16860, 16891, 16921, 16952, 16982, 17013, 17044, 17074, 17105, 17135, 17166)
prices = xts(matrix(priceData, ncol=2, byrow=FALSE), order.by=as.Date(idxDates))
names(prices) <- c("myl_prc", "spy_prc")
str(prices)


# Show first six observations of prices
head(prices)

# Calculate MYL monthly return
rets <- quantmod::Delt(prices$myl_prc)

# Calculate SPY monthly return
rets$spy <- quantmod::Delt(prices$spy_prc)

# Change label of first variable
names(rets)[1] <- "myl"

# Remove first observation - NA
rets <- rets[-1, ]


# Run regression
reg <- lm(myl ~ spy, data=rets)

# Save beta
myl_beta <- coef(reg)[2]
myl_beta


debt_beta <- 0.08
myl_debt_eq <- 1.68

# Calculate the Mylan Unlevered Beta
myl_unl_beta <- (myl_beta + debt_beta * (1 - 0.4) * myl_debt_eq) / (1 + (1 - 0.4) * myl_debt_eq)
myl_unl_beta


med_beta <- 0.777
debt_eq <- 1.5

# Calculate levered beta
beta <- med_beta * (1 + (1 - 0.4) * debt_eq) - debt_beta * (1 - 0.4) * debt_eq
beta


treas <- Quandl::Quandl(code="FRED/DGS10")

# Review treas
head(treas)

# Extract 2016-12-30 yield 
rf <- treas[treas$Date == "2016-12-30", ]
rf

# Keep only the observation in the second column
rf_yield <- rf$Value
rf_yield

# Convert yield to decimal terms
rf_yield_dec <- rf_yield / 100
rf_yield_dec


year <- 1928:2016
sp500 <- c(0.4381, -0.083, -0.2512, -0.4384, -0.0864, 0.4998, -0.0119, 0.4674, 0.3194, -0.3534, 0.2928, -0.011, -0.1067, -0.1277, 0.1917, 0.2506, 0.1903, 0.3582, -0.0843, 0.052, 0.057, 0.183, 0.3081, 0.2368, 0.1815, -0.0121, 0.5256, 0.326, 0.0744, -0.1046, 0.4372, 0.1206, 0.0034, 0.2664, -0.0881, 0.2261, 0.1642, 0.124, -0.0997, 0.238, 0.1081, -0.0824, 0.0356, 0.1422, 0.1876, -0.1431, -0.259, 0.37, 0.2383, -0.0698, 0.0651, 0.1852, 0.3174, -0.047, 0.2042, 0.2234, 0.0615, 0.3124, 0.1849, 0.0581, 0.1654, 0.3148, -0.0306, 0.3023, 0.0749, 0.0997, 0.0133, 0.372, 0.2268, 0.331, 0.2834, 0.2089, -0.0903, -0.1185, -0.2197, 0.2836, 0.1074, 0.0483, 0.1561, 0.0548, -0.3655, 0.2594, 0.1482, 0.021, 0.1589, 0.3215, 0.1352, 0.0136, 0.1174)
tbond <- c(0.0084, 0.042, 0.0454, -0.0256, 0.0879, 0.0186, 0.0796, 0.0447, 0.0502, 0.0138, 0.0421, 0.0441, 0.054, -0.0202, 0.0229, 0.0249, 0.0258, 0.038, 0.0313, 0.0092, 0.0195, 0.0466, 0.0043, -0.003, 0.0227, 0.0414, 0.0329, -0.0134, -0.0226, 0.068, -0.021, -0.0265, 0.1164, 0.0206, 0.0569, 0.0168, 0.0373, 0.0072, 0.0291, -0.0158, 0.0327, -0.0501, 0.1675, 0.0979, 0.0282, 0.0366, 0.0199, 0.0361, 0.1598, 0.0129, -0.0078, 0.0067, -0.0299, 0.082, 0.3281, 0.032, 0.1373, 0.2571, 0.2428, -0.0496, 0.0822, 0.1769, 0.0624, 0.15, 0.0936, 0.1421, -0.0804, 0.2348, 0.0143, 0.0994, 0.1492, -0.0825, 0.1666, 0.0557, 0.1512, 0.0038, 0.0449, 0.0287, 0.0196, 0.1021, 0.201, -0.1112, 0.0846, 0.1604, 0.0297, -0.091, 0.1075, 0.0128, 0.0069)
damodaran=data.frame(year=year, sp_500=sp500, tbond_10yr=tbond)
str(damodaran)


# Review the first six rows of damodaran
head(damodaran)

# Calculate annual difference between stocks and bonds
diff <- damodaran$sp_500 - damodaran$tbond_10yr

# Calculate ERP
erp <- mean(diff)
erp


relevered_beta <- 1.404572
rf <- 0.0245
erp <- 0.0623

capm_coe <- rf + relevered_beta * erp
capm_coe

```
  
  
  
***
  
Chapter 4 - Relative Valuation  
  
Relative Valuation:  
  
* Valuation multiples from comparable firms to estimate the proper valuation for the target firm  
* The "Law of One Price" says that if two items are the same, they should have the same price  
* One of the keys to the process is to identify comparable firms - "more an art than a science"  
	* Industry classifications  
    * Competitors (though this can be tricky - not always comparable, such as WalMart vs mom-and-pop)  
  
Valuation Multiples:  
  
* Price-to-Earnings (PE) is one of the most common rations used - how much investors are willing to pay for firm earnings  
	* Historical EPS is based on the annual statement  
    * Forecasted EPS is typically based on analyst estimated  
    * Need to be consistent in the metric used - all historical or all projected or the like  
* Price-to-Book (PB) is another common ration used - how much investors are willing to pay for book equity  
	* May also be historical (annual report) or projected (analysts)  
    * Negative PB ratios are generally not meaningful  
    * Typically, a basket of comparable companies is included  
* If the comparable companies are over-values, then the target company will also be over-valued  
  
Analyzing Determinants of Multiples:  
  
* Key to good metrics is the degree of comparability of the firms - average or median may be appropriate  
	* May require adjustments to the mean/median if there are differences in risk, growth, etc.  
    * Firms with higher risk, lower growth, and lower profitability will often have lower multiples  
* Regression-based approaches can help with finding good multiples  
	* P/B vs ROE, P/E vs. 5-year growth, multiple regression, etc.  
    * Can be less subjective since the data are guiding the adjustments  
* Need to make sure that negative values are excluded - generall, set to NA  
  
Example code includes:  
```{r}

load("./RInputFiles/midcap400.rda")
str(midcap400)


# Review the first six rows of midcap400
head(midcap400)

# Subset Pharmaceuticals firms
pharma <- subset(midcap400, gics_subindustry=="Pharmaceuticals")
pharma


# Calculate P/LTM EPS
pharma$ltm_p_e <- ifelse(pharma$ltm_eps > 0, pharma$price / pharma$ltm_eps, NA)

# Calculate P/NTM EPS
pharma$ntm_p_e <- ifelse(pharma$ntm_eps > 0, pharma$price / pharma$ntm_eps, NA)

# Calculate P/BVPS
pharma$p_bv <- ifelse(pharma$bvps > 0, pharma$price / pharma$bvps, NA)
pharma


# Calculate average multiples
multiples <- colMeans(pharma[, c("ltm_p_e", "ntm_p_e", "p_bv")], na.rm=TRUE)
multiples


# Vector of metrics
metrics <- c(1, 2, 8)

# Calculate implied values
implied_val <- multiples*metrics
implied_val


# Subset Pharmaceuticals firms
cons_disc <- subset(midcap400, gics_sector=="Consumer Discretionary")
cons_disc


# Calculate ROE
cons_disc$roe <- cons_disc$ltm_eps / cons_disc$bvps

# Calculate Price to Book ratio
cons_disc$p_bv <- ifelse(cons_disc$bvps <= 0, NA, cons_disc$price / cons_disc$bvps)

# Remove NA
cons_disc_no_na <- cons_disc[complete.cases(cons_disc), ]
head(cons_disc_no_na)


# Set x-axis range
x.range <- c(min(cons_disc_no_na$roe), max(cons_disc_no_na$roe))

# Set y-axis range
y.range <- c(min(cons_disc_no_na$p_bv), max(cons_disc_no_na$p_bv))

# Plot data
plot(y = cons_disc_no_na$p_bv, x = cons_disc_no_na$roe, 
     xlab = "Return on Equity", ylab = "Price-to-Book",
     xlim = x.range, ylim = y.range, col = "blue", 
     main = "Price-to-Book Value and Return on Equity Of Mid-Cap Consumer Discretionary Firms"
     )

# Regress roe on p_bv
reg <- lm(p_bv ~ roe, data = cons_disc_no_na)

# Add trend line in red
abline(reg, col = "red")


# Regression summary
summary_reg <- summary(reg)
summary_reg

# Store intercept
a <- coef(reg)[1]
a

# Store beta
b <- coef(reg)[2]
b


# Calculate implied P/B
implied_p_b <- (a + b*0.2)
implied_p_b

# Calculate implied price
implied_price <- 8 * implied_p_b
implied_price

```
  
  
  
***
  
Chapter 5 - Comprehensive Exercise  
  
Fundamental Valuation: Analyzing Projections:  
  
* Projections are critical inputs in cash flow analysis - must be reliable to avoid GIGO results  
* Approaches to analyzing projections - visual inspection, regression analysis, etc.  
	* Make sure that maintaining the trend is a reasonable assumption  
  
Fundamental Valuation: Implementation:  
  
* Free Cash Flow to Equity (FCFE)  
	* PV of free cash flows to equity, discounted by the cost of equity  
    * FCFE over the t-period horizon, PLUS  
    * Terminal period value - FCFE(T) * (1 + g) / (ke - g) / (1 + ke)**T  
* Dividend Discount Model (DM)  
	* PV of value of dividends, discounted by the cost of equity  
    * D * (1 + g) / (ke - g) / (1 + ke)**T - perpetuity with growth formula, assuming dividend payments do not start for T years  
  
Relative Valuation:  
  
* Valuation is based on "Law of One Price" - two stocks that look the same should have the same price  
	* Identifying comparable companies is vital  
* The PE and PB ratios are the most commonly used, and can be based on either historical earnings or projected earnings  
	* The resulting valuation is implied rather than explicit  
  
Wrap up:  
  
* Understanding of funcamental concepts of equity valuation  
* Ang book - "Analyzing Financial Data and Implementing Financial Models Using R"  
  
Example code includes:  
```{r}

revenue <- matrix(data=c(81.87, 0, 87.82, 0, 85.95, 0, 89.02, 0, 97.43, 0, 96.78, 0, 96.7, 0, 0, 108.45, 0, 112.67, 0, 120, 0, 127.6, 0, 126.06), 
                  ncol=12, byrow=FALSE, dimnames=list(c("hist_rev", "proj_rev"), 2010:2021)
                  )
str(revenue)

# Create a bar chart
barplot(revenue, col = c("red", "blue"), main = "Historical vs. Projected Revenues")

# Add legend
legend("topleft", legend = c("Historical", "Projected"), fill = c("red", "blue"))


# Create a data frame of single series
rev_all <- apply(revenue, 2, FUN=sum)
rev_all_df <- data.frame(rev_all)

# Create Trend Variable
rev_all_df$trend <- 1:length(rev_all)

# Create Shift Variable
rev_all_df$shift <- ifelse(rev_all_df$trend <= 7, 0, 1)

# Run regression
reg <- lm(rev_all ~ trend + shift, data = rev_all_df)
summary(reg)


# Subset Treasury data to 12/30/16
rf <- subset(treas, Date==as.Date("2016-12-30"))

# Keep 2nd column
rf_yield <- rf$Value

# Convert to decimal terms
rf_yield_dec <- rf_yield/100
rf_yield_dec

# Calcualte difference between S&P 500 Return and Treasury Return
diff <- damodaran$sp_500 - damodaran$tbond_10yr

# Calculate average difference
erp <- mean(diff)
erp

beta <- 1.4594
# Calculate CAPM Cost of Equity
ke <- rf_yield_dec + beta*erp
ke


fcfe <- data.frame(fcfe=c(14, 15, 16, 17, 17.5))
rownames(fcfe) <- 2017:2021
fcfe

# Calculate Discount Periods to 12/31/2016
fcfe$disc_periods <- seq_len(nrow(fcfe))

# Calculate discount factor
fcfe$disc_factor <- (1 + ke)**(-fcfe$disc_periods)

# Calculate PV of each period's total free cash flow
fcfe$pv <- fcfe$fcfe * fcfe$disc_factor

# Calculate Projection Period Value
pv_proj_period <- sum(fcfe$pv)
pv_proj_period


pgr <- 0.03

# Extract 2021 FCFE
fcfe_2021 <- fcfe[nrow(fcfe), "fcfe"]

# Use perpetuity with growth formula to calculate terminal value
tv_2021 <- fcfe_2021 * (1 + pgr) / (ke - pgr)
tv_2021

# Calculate PV of Terminal Value
pv_terminal <- tv_2021 * (1 + ke)**(-nrow(fcfe))
pv_terminal


# Calculate agggregate equity value
equity_value_fcfe <- pv_proj_period + pv_terminal
equity_value_fcfe

shout <- 15

# Calculate equity value per share
equity_value_fcfe_per_share <- equity_value_fcfe / shout
equity_value_fcfe_per_share


div <- 1.25

# Use DDM to Calculate Equity Value
equity_value_ddm <- div * (1 + pgr) / (ke - pgr)
equity_value_ddm

# Equity Value Per Share
equity_value_ddm_per_share <- equity_value_ddm / shout
equity_value_ddm_per_share


p_eps_multiple <- 8
eps <- 1.39

# Calculate Implied Equity Value
equity_value_p_e <- p_eps_multiple * eps
equity_value_p_e

# Calculate Equity Value Per Share
equity_value_p_e_per_share <- equity_value_p_e / shout
equity_value_p_e_per_share


eq_val_fcfe_per_share <- 11.95
eq_val_ddm_per_share <- 11.7
eq_val_p_e_per_share <- 11.12

# Combine equity values
# eq_val <- c(eq_val_fcfe, eq_val_ddm, eq_val_p_e)

# Combine equity values per share
eq_val_per_share <- c(eq_val_fcfe_per_share, eq_val_ddm_per_share, eq_val_p_e_per_share)

# Combine into a summary table
# summary <- rbind(eq_val, eq_val_per_share)

# Rename column headers
# colnames(summary) <- c("DCF", "DDM", "P/E")
# summary

```
  
  
  
***
  
### _Credit Risk Modeling in R_  
  
Chapter 1 - Introduction and Data Pre-Processing  
  
Introduction - course is primarily about risks of loan default:  
  
* Expected Loss includes 1) probability of default (PD), 2) exposure at default (EAD), and 3) loss given default (LGD)  
	* PD * EAD * LGD is the expected value of the loss  
* Banks have default histories for previous customers - application information (income, marital, etc.) and behavioral information (e.g., payment history)  
  
Histograms and Outliers:  
  
* Can use the base hist(x=, xlab=, main=) to get a basic histogram of the underlying data  
	* The breaks= argument can be either sqrt(nrow) or providing the blanks explicitly  
* Can delete outliers based either on expert judgment or a hard-and-fast rule such as Q1/Q3 +/- 1.5*IQR
	* Can be useful to temporarily delete while visualizing the data  
  
Missing data and course classification:  
  
* There are NA values in the data for certain variables such as employment and interest rate  
	* The summary() call will give the number of NA for each variable, as well as the statistical summary of the non-NA data  
    * Can delete the observations if there are just a few NA, and might delete a variable if there are many NA for that variable  
    * Can kill a variable using myData$killVar <- NULL  
    * Alternately, can use a median imputation, or even keep the NA data since this can be valuable information  
* Can use "coarse classification", which is broadly the binning of the data  
	* The NA can then be kept as one of the categorical variables, allowing it to be used in other statistical models  
    * A missing factor variable can be "median imputed" to the most common of the extant categorical variables  
  
Data splitting and confusion matrices:  
  
* Can split the data in to train and test components, with the test set being used only to estimate the out-of-sample error rate  
* Cross-validation can also help; for example, rather than a single 67-33 train-test, can run a 3-fold CV  
* The confusion matrix can assess the predictions vs. the actual  
	* Actual status is the y-axis and predicted status is the x-axis (no/no is top-left and yes/yes is bottom-right)  
    * False Positive (FP) is the upper-right - model predicts yes, actual is no  
    * False Negative (FN) is the lower-left - model predicts no, actual is yes  
* Several metrics are available  
	* Accuracy = (TP + TN) / n  
    * Sensitivity = TP / (TP + FN) = TP / nP  
    * Specificity = TN / (TN + FP) = TN / nN  
  
Example code includes:  
```{r}

loan_ch1 <- readRDS("./RInputFiles/loan_data_ch1.rds")
loan_ch2 <- readRDS("./RInputFiles/loan_data_ch2.rds")
str(loan_ch1)
str(loan_ch2)


# View the structure of loan_data
loan_data <- loan_ch1
str(loan_data)

# Call CrossTable() on loan_status
gmodels::CrossTable(loan_data$loan_status)

# Call CrossTable() on grade and loan_status
gmodels::CrossTable(x=loan_data$grade, y=loan_data$loan_status, 
                    prop.r=TRUE, prop.c=FALSE, prop.t=FALSE, prop.chisq=FALSE
                    )


# Create histogram of loan_amnt: hist_1
hist_1 <- hist(loan_data$loan_amnt)

# Print locations of the breaks in hist_1
hist_1$breaks

# Change number of breaks and add labels: hist_2
hist_2 <- hist(loan_data$loan_amnt, breaks = 200, xlab = "Loan amount", 
               main = "Histogram of the loan amount"
               )


# Plot the age variable
plot(loan_data$age, ylab="Age")

# Save the outlier's index to index_highage
index_highage <- which(loan_data$age > 122)

# Create data set new_data with outlier deleted
new_data <- loan_data[-index_highage, ]

# Make bivariate scatterplot of age and annual income
plot(loan_data$age, loan_data$annual_inc, xlab = "Age", ylab = "Annual Income")


# Look at summary of loan_data
summary(loan_data$int_rate)

# Get indices of missing interest rates: na_index
na_index <- which(is.na(loan_data$int_rate))

# Remove observations with missing interest rates: loan_data_delrow_na
loan_data_delrow_na <- loan_data[-na_index, ]

# Make copy of loan_data
loan_data_delcol_na <- loan_data

# Delete interest rate column from loan_data_delcol_na
loan_data_delcol_na$int_rate <- NULL


# Compute the median of int_rate
median_ir <- median(loan_data$int_rate, na.rm=TRUE)

# Make copy of loan_data
loan_data_replace <- loan_data

# Replace missing interest rates with median
loan_data_replace$int_rate[na_index] <- median_ir

# Check if the NAs are gone
summary(loan_data_replace$int_rate)


# Make the necessary replacements in the coarse classification example below 
loan_data$ir_cat <- rep(NA, length(loan_data$int_rate))

loan_data$ir_cat[which(loan_data$int_rate <= 8)] <- "0-8"
loan_data$ir_cat[which(loan_data$int_rate > 8 & loan_data$int_rate <= 11)] <- "8-11"
loan_data$ir_cat[which(loan_data$int_rate > 11 & loan_data$int_rate <= 13.5)] <- "11-13.5"
loan_data$ir_cat[which(loan_data$int_rate > 13.5)] <- "13.5+"
loan_data$ir_cat[which(is.na(loan_data$int_rate))] <- "Missing"

loan_data$ir_cat <- as.factor(loan_data$ir_cat)

# Look at your new variable using plot()
plot(loan_data$ir_cat)


# Set seed of 567
set.seed(567)

# Store row numbers for training set: index_train
index_train <- sample(1:nrow(loan_data), (2/3)*nrow(loan_data))

# Create training set: training_set
training_set <- loan_data[index_train, ]

# Create test set: test_set
test_set <- loan_data[-index_train, ]


# Create confusion matrix
# conf_matrix <- table(test_set$loan_status, model_pred)

# Compute classification accuracy
# sum(diag(conf_matrix)) / sum(conf_matrix)

# Compute sensitivity
# conf_matrix[2, 2] / sum(conf_matrix[2, ])

```
  
  
  
***
  
Chapter 2 - Logistic Regression  
  
Logistic Regression Introduction:  
  
* Logistic regression is similar to linear regression, but with outputs that are constrained to be between 0 and 1  
* The logit is run in R using glm(y ~ x, data=, family="binomial")  
* The linear component is the log of the odds  
	* If variable x1 increases by 1, then odds will be multiplied by exp(B1)  
  
Logistic Regression Predictions:  
  
* Could directly make predictions using the reciprocal of 1 plus the exponential of the coefficients times the variables  
* Alternately, can use predict(myModel, newdata=, type="response")  # the default type would just give back the coefficients multiplied by the variables  
  
Evaluating regression results:  
  
* The confusion matrix can then compare the model predictions (using a cut-off for yes/no) against the known results in the test-set  
	* A common cutoff value is 0.5, though for rare events, even the highest predicted probabilities may be less than 0.5  
    * The cutoff value is often changed (lowered) to get a better sensitivity, at the expense of some loss of specificity or even overall accuracy  
  
Wrap up:  
  
* Accuracy with unbalanced groups often has a uniformly increasing (decreasing) accuracy with regards to the cut-off  
	* This is basically useless, since the model just predicts whatever is most common in the training set  
    * Better method is to look at both sensitivity and specificity, finding a cut-off that drives a nice trade-off between these  
* The logistic regresssion is known as the "logit" (default in R - can also write family="binomial"(link=logit), though this is redundant)  
    * Alternatives include probit and cloglog links, which make different assumptions about the link between the linear component and the predicted probability  
  
Example code includes:  
```{r}

loan_data <- loan_ch2
str(loan_data)

set.seed(1911031341)
idxTrain <- sample(1:nrow(loan_data), round((2/3)*nrow(loan_data)), replace=FALSE)
training_set <- loan_data[idxTrain, !(names(loan_data) %in% c("int_rate"))]
test_set <- loan_data[-idxTrain, !(names(loan_data) %in% c("int_rate"))]
str(training_set)
str(test_set)


# Build a glm model with variable ir_cat as a predictor
log_model_cat <- glm(loan_status ~ ir_cat, data=training_set, family="binomial")

# Print the parameter estimates 
log_model_cat

# Look at the different categories in ir_cat using table()
table(loan_data$ir_cat)


# Build the logistic regression model
log_model_multi <- glm(loan_status ~ age + ir_cat + grade + loan_amnt + annual_inc, 
                       data=training_set, family="binomial"
                       )

# Obtain significance levels using summary()
summary(log_model_multi)


# Build the logistic regression model
log_model_small <- glm(formula = loan_status ~ age + ir_cat, family = "binomial", data = training_set)
predictions_all_small <- predict(log_model_small, newdata = test_set, type = "response")

# Look at the range of the object "predictions_all_small"
range(predictions_all_small)


# Change the code below to construct a logistic regression model using all available predictors in the data set
log_model_full <- glm(loan_status ~ ., family = "binomial", data = training_set)

# Make PD-predictions for all test set elements using the the full logistic regression model
predictions_all_full <- predict(log_model_full, newdata=test_set, type="response")

# Look at the predictions range
range(predictions_all_full)


# Make a binary predictions-vector using a cut-off of 15%
pred_cutoff_15 <- ifelse(predictions_all_full > .15, 1, 0)

# Construct a confusion matrix
table(test_set$loan_status, pred_cutoff_15)


# Fit the logit, probit and cloglog-link logistic regression models
log_model_logit <- glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
                       family = binomial(link = logit), data = training_set
                       )
log_model_probit <- glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
                       family = binomial(link = probit), data = training_set
                       )
log_model_cloglog <-  glm(loan_status ~ age + emp_cat + ir_cat + loan_amnt,
                       family = binomial(link = cloglog), data = training_set
                       )
                       
# Make predictions for all models using the test set
predictions_logit <- predict(log_model_logit, newdata = test_set, type = "response")
predictions_probit <- predict(log_model_probit, newdata = test_set, type = "response")
predictions_cloglog <- predict(log_model_cloglog, newdata = test_set, type = "response")

# Use a cut-off of 14% to make binary predictions-vectors
cutoff <- 0.14
class_pred_logit <- ifelse(predictions_logit > cutoff, 1, 0)
class_pred_probit <- ifelse(predictions_probit > cutoff, 1, 0)
class_pred_cloglog <- ifelse(predictions_cloglog > cutoff, 1, 0)

# Make a confusion matrix for the three models
true_val <- test_set$loan_status

tab_class_logit <- table(true_val,class_pred_logit)
tab_class_probit <- table(true_val,class_pred_probit) 
tab_class_cloglog <- table(true_val,class_pred_cloglog)

# Compute the classification accuracy for all three models
acc_logit <- sum(diag(tab_class_logit)) / nrow(test_set)
acc_probit <- sum(diag(tab_class_probit)) / nrow(test_set)
acc_cloglog <- sum(diag(tab_class_cloglog)) / nrow(test_set)

```
  
  
  
***
  
Chapter 3 - Decision Trees  
  
Background - decision trees tend to be popular due to the ease of interpretation:  
  
* For each node of the tree, there are two possible splitting decisions, and grouping needs to be found  
* Tree nodes are defined based on "impurity", commonly calculated using the Gini measure  
	* Gini = 2 * P(yes) * P(no) within the node  
    * Purity gain is often defined as the change (weighted) in purity in moving down the split  
    * Algorithms frequently look for the greatest gain in purity to decide on the splitting variable and cutoffs  
  
Building decision trees using rpart:  
  
* The rpart::rpart() function will build decision trees automatically, though this is not always great for unbalanced data if using the default values  
* There are three potential techniques for overcoming the imbalances  
	* Undersampling (of common events) or oversampling (of rare events) in the training set can help  
    * Changing the prior probabilities (making the rare event look more common) can also be helpful  
    * The rpart::rpart() will allow for a loss matrix, where a greater penalty can be given to missing on the rare event  
* The validation (test) set is vital for any of these techniques, since there is a high risk that there will be training set artifacts  
  
Pruning decision trees:  
  
* Large decision trees can be both hard to interpret and subject to significant over-fitting  
* The R packages have both printcp() and plotcp() to help drive better pruning  
	* The printcp() can show the cross-validated errors in a column xerror  
    * The plotcp() can show the cross-validation error as a function of the complexity parameter, cp  
* The function prunt(myTree, cp=) will prune a tree based on the provided parameter  
	* plot(myPruneTree, uniform=TRUE)  
    * text(myPruneTree, use.n=TRUE)  # the use.n means the resulting categories for the leaf are shown  
* More intuitive plots are available using rpart.plot::prp()  
	* prp(myPruneTree, extra=1)  # the extra=1 asks for detail about membership by leaf to be shown  
  
Other tree options and confusion matrices:  
  
* One argument in rpart is weights, allowing for case weights to counter imbalance  
* The rpart.control() function has many parameters that can be adjusted, including  
	* cp - the complexity parameter  
    * minsplit - the minimum number of elements in a node to consider splitting (default is 20)  
    * minbucket - minimum number of elements in a leaf node  
* Can use predict(myPruneTree, newdata=, type="class") to make predictions using the final tree  
	* Omitting the type="class" will instead provide probabilities based on the proportions inside the leaf values  
  
Example code includes:  
```{r}

# As a small reminder, remember that Gini of a certain node = 2 * proportion of defaults in this node * proportion of non-defaults in this node. Have a look at the code for a refresher.
# gini_root <- 2 * (89 / 500) * (411 / 500)

# Look at the following code to get an idea of how you can use the gini measures you created to calculate the gain of a node.
# Gain = gini_root - (prop(cases left leaf) * gini_left) - (prop(cases right leaf * gini_right))


ssStatus <- c(0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0)
ssAge <- c(48, 36, 35, 33, 46, 24, 35, 33, 41, 49, 47, 44, 54, 42, 26, 50, 41, 30, 17, 34, 37, 41, 41, 33, 39, 40, 40, 55, 45, 45, 45, 49, 47, 29, 42, 42, 60, 40, 52, 50, 45, 39, 38, 41, 37, 43, 49, 54, 50, 55, 18, 44, 43, 36, 49, 30, 51, 38, 42, 38, 47, 39, 57, 56, 33, 51, 45, 53, 48, 37, 40, 51, 23, 22, 52, 57, 41, 33, 42, 43, 45, 26, 57, 40, 37, 40, 54, 40, 43, 45, 43, 45, 34, 57, 53, 46, 52, 35, 27, 39, 47, 23, 50, 43, 40, 56, 40, 44, 44, 42, 41, 43, 31, 35, 48, 50, 43, 29, 36, 35, 53, 37, 51, 39, 43, 60, 51, 28, 25, 50, 46, 37, 55, 43, 28, 49, 29, 53, 48, 51, 33, 35, 41, 27, 39, 47, 33, 53, 42, 56, 46, 46, 43, 52, 48, 33, 47, 55, 36, 56, 33, 44, 29, 47, 43, 54, 47, 54, 29, 22, 32, 34, 35, 36, 52, 32, 39, 57, 47, 40, 29, 51, 54, 48, 34, 35, 51, 38, 38, 31, 47, 41, 60, 42, 43, 49, 41, 35, 44, 49, 36, 46, 45, 43, 43, 43, 48, 37, 40, 48, 48, 48, 54, 37, 39, 41, 49, 47, 42, 42, 56, 49, 48, 51, 16, 43, 38, 48, 54, 47, 39, 51, 44, 46, 48, 37, 41, 33, 45, 43, 31, 44, 36, 44, 36, 12, 48, 40, 47, 38, 36, 52, 41, 36, 33, 45, 58, 44, 54, 37, 56, 29, 34, 44, 46, 46, 47, 40, 52, 36, 44, 49, 61, 55, 28, 39, 45, 40, 57, 41, 53, 39, 45, 33, 43, 38, 59, 42, 22, 47, 40, 44, 31, 13, 58, 43, 53, 45, 45, 51, 41, 51, 47, 45, 12, 46, 55, 31, 57, 41, 22, 33, 56, 36, 42, 38, 41, 49, 46, 41, 39, 51, 45, 48, 44, 47, 43, 24, 36, 51, 45, 40, 32, 54, 41, 37, 41, 40, 38, 28, 33, 47, 42, 45, 48, 52, 40, 45, 40, 30, 44, 49, 50, 32, 46, 40, 54, 50, 44, 48, 26, 41, 43, 47, 52, 47, 48, 53, 42, 50, 47, 43, 36, 41, 50, 41, 41, 35, 46, 45, 59, 15, 37, 41, 47, 56, 36, 41, 21, 27, 25, 53, 48, 50, 47, 53, 49, 57, 40, 43, 47, 52, 52, 44, 52, 44, 37, 61, 42, 37, 41, 50, 31, 40, 50, 43, 39, 44, 49, 49, 43, 54, 41, 43, 44, 47, 33, 47, 52, 66, 54, 53, 57, 38, 44, 45, 23, 52, 38, 36, 49, 38, 53, 45, 60, 45, 46, 41, 23, 41, 38, 26, 46, 37, 42, 42, 36, 46, 55, 50, 47, 41, 54, 44, 49, 35, 38, 43, 33, 39, 37, 41, 45, 38, 45, 25, 33, 46, 43, 38, 43, 45, 34, 42, 37, 55, 44, 50, 33, 40, 61, 18, 46, 41, 43, 38, 21, 35, 45, 55)
small_set <- data.frame(status=ssStatus, age=ssAge)
str(small_set)

small_tree <- rpart::rpart(formula = status ~ age, data = small_set, method = "class", 
                           control = rpart::rpart.control(minsplit = 5, cp = 0.001, maxdepth = 1)
                           )


# The Gini-measure of the root node is given below
gini_root <- 2 * 89 / 500 * 411 / 500

# Compute the Gini measure for the left leaf node
gini_ll <- 2 * (401/446) * (45/446)

# Compute the Gini measure for the right leaf node
gini_rl <- 2 * (10/54) * (44/54)

# Compute the gain
gain <- gini_root - 446 / 500 * gini_ll - 54 / 500 * gini_rl

# compare the gain-column in small_tree$splits with our computed gain, multiplied by 500, and assure they are the same
small_tree$splits
improve <- gain * 500


set.seed(1911041357)
usDef <- training_set %>%
    filter(loan_status==1)
usOK <- training_set %>%
    filter(loan_status==0) %>%
    sample_n(size=2*nrow(usDef))
undersampled_training_set <- rbind(usDef, usOK)
str(undersampled_training_set)


# Change the code provided in the video such that a decision tree is constructed using the undersampled training set. Include rpart.control to relax the complexity parameter to 0.001.
tree_undersample <- rpart::rpart(loan_status ~ ., method = "class", data = undersampled_training_set, 
                                 control = rpart::rpart.control(cp = 0.001)
                                 )

# Plot the decision tree
plot(tree_undersample, uniform=TRUE)

# Add labels to the decision tree
text(tree_undersample)


# Change the code below such that a tree is constructed with adjusted prior probabilities.
tree_prior <- rpart::rpart(loan_status ~ ., method = "class", data = training_set, 
                           parms = list(prior=c(0.7, 0.3)), control = rpart::rpart.control(cp = 0.001)
                           )

# Plot the decision tree
plot(tree_prior, uniform=TRUE)

# Add labels to the decision tree
text(tree_prior)


# Change the code below such that a decision tree is constructed using a loss matrix penalizing 10 times more heavily for misclassified defaults.
tree_loss_matrix <- rpart::rpart(loan_status ~ ., method = "class", data =  training_set, 
                                 parms = list(loss = matrix(c(0, 10, 1, 0), ncol=2)), 
                                 control = rpart::rpart.control(cp = 0.001)
                                 )

# Plot the decision tree
plot(tree_loss_matrix, uniform=TRUE)

# Add labels to the decision tree
text(tree_loss_matrix)


# Plot the cross-validated error rate as a function of the complexity parameter
rpart::plotcp(tree_prior)

# Use printcp() to identify for which complexity parameter the cross-validated error rate is minimized.
rpart::printcp(tree_prior)

# Create an index for of the row with the minimum xerror
index <- which.min(tree_prior$cptable[ , "xerror"])

# Create tree_min
tree_min <- tree_prior$cptable[index, "CP"]

#  Prune the tree using tree_min
ptree_prior <- rpart::prune(tree_prior, cp = tree_min)

# Use prp() to plot the pruned tree
rpart.plot::prp(ptree_prior)


# set a seed and run the code to construct the tree with the loss matrix again
set.seed(345)
tree_loss_matrix  <- rpart::rpart(loan_status ~ ., method = "class", data = training_set,
                                  parms = list(loss=matrix(c(0, 10, 1, 0), ncol = 2)),
                                  control = rpart::rpart.control(cp = 0.001)
                                  )

# Plot the cross-validated error rate as a function of the complexity parameter
rpart::plotcp(tree_loss_matrix)

# Prune the tree using cp = 0.0012788
ptree_loss_matrix <- rpart::prune(tree_loss_matrix, cp = 0.0012788)

# Use prp() and argument extra = 1 to plot the pruned tree
rpart.plot::prp(ptree_loss_matrix, extra=1)


# set a seed and run the code to obtain a tree using weights, minsplit and minbucket
set.seed(345)
case_weights <- ifelse(training_set$loan_status==0, 1, 3)

tree_weights <- rpart::rpart(loan_status ~ ., method = "class", 
                             data = training_set, weights = case_weights,
                             control = rpart::rpart.control(minsplit = 5, minbucket = 2, cp = 0.001)
                             )

# Plot the cross-validated error rate for a changing cp
rpart::plotcp(tree_weights)

# Create an index for of the row with the minimum xerror
# index <- which.min(tree_weights$cp[ , "xerror"])
index <- 1 + tree_weights$cp %>% 
    as.data.frame() %>% 
    slice(2:nrow(.)) %>% 
    pull(xerror) %>% 
    which.min()

# Create tree_min
tree_min <- tree_weights$cp[index, "CP"]

# Prune the tree using tree_min
ptree_weights <- rpart::prune(tree_weights, cp=tree_min)

# Plot the pruned tree using the rpart.plot()-package
rpart.plot::prp(ptree_weights, extra=1)


ptree_undersample <- rpart::rpart(formula = loan_status ~ ., data = undersampled_training_set, 
                                  method = "class", control = rpart::rpart.control(cp = 0.001)
                                  )

# Make predictions for each of the pruned trees using the test set.
pred_undersample <- predict(ptree_undersample, newdata = test_set,  type = "class")
pred_prior <- predict(ptree_prior, newdata = test_set,  type = "class")
pred_loss_matrix <- predict(ptree_loss_matrix, newdata = test_set,  type = "class")
pred_weights <- predict(ptree_weights, newdata = test_set,  type = "class")

# construct confusion matrices using the predictions.
confmat_undersample <- table(test_set$loan_status, pred_undersample)
confmat_prior <- table(test_set$loan_status, pred_prior)
confmat_loss_matrix <- table(test_set$loan_status, pred_loss_matrix)
confmat_weights <- table(test_set$loan_status, pred_weights)

# Compute the accuracies
(acc_undersample <- sum(diag(confmat_undersample)) / nrow(test_set))
(acc_prior <- sum(diag(confmat_prior)) / nrow(test_set))
(acc_loss_matrix <- sum(diag(confmat_loss_matrix)) / nrow(test_set))
(acc_weights <- sum(diag(confmat_weights)) / nrow(test_set))

```
  
  
  
***
  
Chapter 4 - Evaluating Credit Risk Models  
  
Right cut-offs (strategy curves):  
  
* Can use metrics such as an exceedance rate or acceptance rate to establish reasonable values for default risk (and associated cutoff rates)  
	* The "bad rate" is the percentage of accepted loans that default  
    * The strategy table shows the acceptance rate, cutoff, and associated "bad rate"  
  
ROC curves:  
  
* Combinations of specificity and sensitivity can be assessed using the ROC (receiver operator characteristic) curve  
	* Sensitivity is on the y-axis and 1-Specificity is on the x-axis  
* Typical comparison method is AUC (area under the curve) - 0.50 is the y=x line, while 1.00 is perfect and 0.00 is perfectly wrong  
  
Input selection based on AUC:  
  
* The AUC can be used for variable selection - how many and which variables best predict default  
* Can also run AUC-based pruning  
	* Start with all variables in the tree and calculate AUC  
    * Exclude all possible n-1 variables and calculate AUC for each; keep the model with the best AUC  
    * Exclude all possible n-2 variables from the model above and calculate AUC for each; keep the model with the best AUC  
    * Stop the iterations when the AUC starts going down  
  
Wrap up:  
  
* Additional classification methods, such as discriminant analysis, random forests, neural networks, SVM  
* The classification-models neglect the timing of defaults, which can be a drawback of this approach  
	* Survival analysis and survival curves can help to overcome this  
  
Example code includes:  
```{r}

# Make predictions for the probability of default using the pruned tree and the test set.
prob_default_prior <- predict(ptree_prior, newdata = test_set)[ ,2]

# Obtain the cutoff for acceptance rate 80%
cutoff_prior <- quantile(prob_default_prior, 0.8)

# Obtain the binary predictions.
bin_pred_prior_80 <- ifelse(prob_default_prior > cutoff_prior, 1, 0)

# Obtain the actual default status for the accepted loans
accepted_status_prior_80 <- test_set$loan_status[bin_pred_prior_80 == 0]

# Obtain the bad rate for the accepted loans
mean(accepted_status_prior_80)


# Have a look at the function strategy_bank
strategy_bank <- function(prob_of_def) {
    cutoff <- rep(NA, 21)
    bad_rate <- rep(NA, 21)
    accept_rate <- seq(1,0,by=-0.05)
    for (i in 1:21) {
        cutoff[i] <- quantile(prob_of_def,accept_rate[i])
        pred_i <- ifelse(prob_of_def> cutoff[i], 1, 0)
        pred_as_good <- test_set$loan_status[pred_i==0]
        bad_rate[i] <- sum(pred_as_good) / length(pred_as_good)
    }
    table <- cbind(accept_rate, cutoff=round(cutoff,4), bad_rate=round(bad_rate,4))
    return(list(table=table, bad_rate=bad_rate, accept_rate=accept_rate, cutoff=cutoff))
}


predictions_loss_matrix <- predict(ptree_loss_matrix, newdata=test_set, type = "prob")[, 2]

# Apply the function strategy_bank to both predictions_cloglog and predictions_loss_matrix
strategy_cloglog <- strategy_bank(predictions_cloglog)
strategy_loss_matrix <- strategy_bank(predictions_loss_matrix)

# Obtain the strategy tables for both prediction-vectors
strategy_cloglog$table
strategy_loss_matrix$table

# Plot the strategy functions
par(mfrow = c(1,2))
plot(strategy_cloglog$accept_rate, strategy_cloglog$bad_rate, 
     type = "l", xlab = "Acceptance rate", ylab = "Bad rate", 
     lwd = 2, main = "logistic regression", ylim=c(0, 0.15)
     )

plot(strategy_loss_matrix$accept_rate, strategy_loss_matrix$bad_rate, 
     type = "l", xlab = "Acceptance rate", 
     ylab = "Bad rate", lwd = 2, main = "tree", ylim=c(0, 0.15)
     )
par(mfrow = c(1,1))


library(pROC)

# Construct the objects containing ROC-information
ROC_logit <- roc(test_set$loan_status, predictions_logit)
ROC_probit <- roc(test_set$loan_status, predictions_probit)
ROC_cloglog <- roc(test_set$loan_status, predictions_cloglog)
ROC_all_full <- roc(test_set$loan_status, predictions_all_full)

# Draw all ROCs on one plot
plot.roc(ROC_logit)
lines(ROC_probit, col="blue")
lines(ROC_cloglog, col="red")
lines(ROC_all_full, col="green")

# Compute the AUCs
pROC::auc(ROC_logit)
pROC::auc(ROC_probit)
pROC::auc(ROC_cloglog)
pROC::auc(ROC_all_full)


predictions_undersample <- predict(ptree_undersample, newdata=test_set, type = "prob")[, 2]
predictions_prior <- predict(ptree_prior, newdata=test_set, type = "prob")[, 2]
predictions_loss_matrix <- predict(ptree_loss_matrix, newdata=test_set, type = "prob")[, 2]
predictions_weights <- predict(ptree_weights, newdata=test_set, type = "prob")[, 2]


# Construct the objects containing ROC-information
ROC_undersample <- roc(test_set$loan_status, predictions_undersample)
ROC_prior <- roc(test_set$loan_status, predictions_prior)
ROC_loss_matrix <- roc(test_set$loan_status, predictions_loss_matrix)
ROC_weights <- roc(test_set$loan_status, predictions_weights)

# Draw the ROC-curves in one plot
plot.roc(ROC_undersample)
lines(ROC_prior, col="blue")
lines(ROC_loss_matrix, col="red")
lines(ROC_weights, col="green")

# Compute the AUCs
auc(ROC_undersample)
auc(ROC_prior)
auc(ROC_loss_matrix)
auc(ROC_weights)


# Build four models each time deleting one variable in log_3_remove_ir
log_4_remove_amnt <- glm(loan_status ~ grade + annual_inc + emp_cat, 
                         family = binomial, data = training_set
                         )
log_4_remove_grade <- glm(loan_status ~ loan_amnt + annual_inc + emp_cat, 
                          family = binomial, data = training_set
                          )
log_4_remove_inc <- glm(loan_status ~ loan_amnt + grade + emp_cat, 
                        family = binomial, data = training_set
                        )
log_4_remove_emp <- glm(loan_status ~ loan_amnt + grade + annual_inc, 
                        family = binomial, data = training_set
                        )

# Make PD-predictions for each of the models
pred_4_remove_amnt <- predict(log_4_remove_amnt, newdata = test_set, type = "response")
pred_4_remove_grade <- predict(log_4_remove_grade, newdata = test_set, type = "response")
pred_4_remove_inc <- predict(log_4_remove_inc, newdata = test_set, type = "response")
pred_4_remove_emp <- predict(log_4_remove_emp, newdata = test_set, type = "response")

# Compute the AUCs
auc(test_set$loan_status, pred_4_remove_amnt)
auc(test_set$loan_status, pred_4_remove_grade)  
auc(test_set$loan_status, pred_4_remove_inc)
auc(test_set$loan_status, pred_4_remove_emp)


# Build three models each time deleting one variable in log_4_remove_amnt
log_5_remove_grade <- glm(loan_status ~ annual_inc + emp_cat, family = binomial, data = training_set) 
log_5_remove_inc <- glm(loan_status ~ grade + emp_cat, family = binomial, data = training_set) 
log_5_remove_emp <- glm(loan_status ~ grade + annual_inc, family = binomial, data = training_set) 

# Make PD-predictions for each of the models
pred_5_remove_grade <- predict(log_5_remove_grade, newdata = test_set, type = "response")
pred_5_remove_inc <- predict(log_5_remove_inc, newdata = test_set, type = "response")
pred_5_remove_emp <- predict(log_5_remove_emp, newdata = test_set, type = "response")

# Compute the AUCs
auc(test_set$loan_status, pred_5_remove_grade)
auc(test_set$loan_status, pred_5_remove_inc)
auc(test_set$loan_status, pred_5_remove_emp)

# Plot the ROC-curve for the best model here
plot.roc(roc(test_set$loan_status, pred_4_remove_amnt))

```
  
  
  
***
  
### _Financial Trading in R_  
  
Chapter 1 - Trading Basics  
  
Rationales for trading: 
  
* Trading is the act of buying or selling a security - stock, bond, physical asset, commodity, etc.  
* Trading may be done to create a profit, hedge against a risk, predict from price movements  
* Systematic trading strategy attempts to find good places on the risk/reward spectrum  
* Two types of trades - divergence (momentum) and convergence (mean reversion, contrarian)  
  
Pitfalls of trading systems:  
  
* Market data is a mix of fear, greed, noise, etc.  
* Overfit on past (in-sample) data often performs very poorly on out-of-sample data  
* Objectives are to have stability, often driven by not too many components to the model  
* Hypothesis tests can help identify trends between indicators and future returns  
  
Getting financial data:  
  
* The quantmod library can be used to obtain financial data  
	* This course will look at ETF, basket of investments  
    * LQD - corporate debt and SPY - stock index  
* US markets are open from 09h30 to 16h00 EDT (New York time)  
	* The Ad() function will pull out the adjusted close, meaning adjusted for dividends and splits  
  
Adding indicators to financial data:  
  
* The TTR library contains libraries of classical trading indicators - moving average and the like using SMA()  
* When the current price is above the moving average, the general thought is that the price will continue to rise  
  
Example code includes:  
```{r eval=FALSE}

# Get SPY from yahoo
quantmod::getSymbols("SPY", from = "2000-01-01", to = "2016-06-30", 
                     src =  "yahoo", adjust =  TRUE, auto.assign=TRUE
                     )

# Plot the closing price of SPY
plot(quantmod::Cl(SPY))

# Add a 200-day SMA using lines()
lines(TTR::SMA(quantmod::Cl(SPY), n = 200), col = "red")

```
  
  
  
***
  
Chapter 2 - Boilerplate quantstrat strategies  
  
Setting up a strategy - I:  
  
* Three important dates - initialization, from date, and to date - and a timezone with currency  
* Need to have the financial data in the working environment  
* The stock() function will set up an investment for testing  
	* stock("LQD", currency="USD", multiplier=1)  
  
Setting up a strategy - II:  
  
* Trade size and initial equity are important inputs for scale purposes  
* An account may contain 1 or more portfolios, and each of the portfolios may contain 1 or more strategies  
	* Can remove a strategy from the environment using rm.strat()  
    * Portfolios can be initialized using initPortf(portfolio.st, symbols=, initDate=, currency=)  
* Initializing accounts can be run using initAcct(account.st, portfolios=portfolio.st, initDate=, currency=, initEq=)  
* Initializing orders can be run using initOrders(portfolio.st, initDate=)  
* Initializing strategy only needs a strategy name such as strategy(strategy.st, store=TRUE)  
  
Example code includes:  
```{r eval=FALSE}

# DO Not RUN - do not have package 'quantstrat'
# Load the quantstrat package (not available on CRA)
library(quantstrat)

# Create initdate, from, and to strings
initdate <- "1999-01-01"
from <- "2003-01-01"
to <- "2015-12-31"

# Set the timezone to UTC (do not want to mess up my machine with this . . . )
# Sys.setenv(TZ="UTC")

# Set the currency to USD 
currency("USD")


# Load the quantmod package
library(quantmod)

# Retrieve SPY from yahoo
getSymbols("SPY", src="yahoo", from=from, to=to, adjust=TRUE)

# Use stock() to initialize SPY and set currency to USD
stock("SPY", currency="USD")


# Define your trade size and initial equity
tradesize <- 100000L
initeq <- 100000L

# Define the names of your strategy, portfolio and account
strategy.st <- "firststrat"
portfolio.st <- "firststrat"
account.st <- "firststrat"

# Remove the existing strategy if it exists
rm.strat(strategy.st)


# Initialize the portfolio
initPortf(portfolio.st, symbols = "SPY", initDate = initdate, currency = "USD")

# Initialize the account
initAcct(account.st, portfolios = portfolio.st, initDate = initdate, currency = "USD", initEq = initeq)

# Initialize the orders
initOrders(portfolio.st, initDate = initdate)

# Store the strategy
strategy(strategy.st, store = TRUE)

```
  
  
  
***
  
Chapter 3 - Indicators  
  
Introduction to indicators:  
  
* Market data tends to be noisy, so indicators are often employed for interpretation and trading strategy  
	* Tend to be lagging in nature; attempt to be smooth without losing much signal  
    * The 200-day moving-average (or 50-day moving average) is a common indicator  
    * The oscillation (e.g., relative strength indiex) indicators take values between -1 and 1 (purpose is to do a short lookback at when prices have taken a decrease)  
* This course will lake at both crossovers and oscillations - developed using the R libraries rather than being pre-canned  
  
Indicator mechanic:  
  
* Starts with the add.indicator() function  
* Supply the strategy name, such as strategy.st  
* Name the function for calculating the indicator - somewhat different from standard R functions (assume time series, produce ordered output data, etc.)  
* Supply the inputs for the function as a list  
* Provide a label to the indicator so that other quanstrat functions can call it  
* Example is add.indicator(strategy=strategy.st, name="SMA", arguments=list(x=quote(Cl(mktdata)), n=200), label="SMA200")  
	* This indicator is in some ways similar to using apply for a data frame  
  
Indicator structure review:  
  
* Good practice is to use fairly descriptive names for indicators - such as "SMA200" or "RSI_3"  
* The applyIndicators() function allows for checking whether the indicator has been properly created  
	* applyIndicators(strategy=strategy.st, mktdata=OHLC(SPY))  # will create varOrig.eachIndicator  
* Can use the time series subsetting methods with ["T1/T2"]  
  
Example code includes:  
```{r eval=FALSE}

# DO Not RUN - do not have package 'quantstrat'

# Create a 200-day SMA
spy_sma <- TTR::SMA(x=quantmod::Cl(SPY), n=200)

# Create an RSI with a 3-day lookback period
spy_rsi <- TTR::RSI(price=quantmod::Cl(SPY), n=3)


# Plot the closing prices of SPY
plot(quantmod::Cl(SPY))

# Overlay a 200-day SMA
lines(TTR::SMA(quantmod::Cl(SPY), n = 200), col = "red")

# What kind of indicator?
"trend"


# Plot the closing price of SPY
plot(quantmod::Cl(SPY))

# Plot the RSI 2
plot(TTR::RSI(quantmod::Cl(SPY), n = 2))

# What kind of indicator?
"reversion"


# Add a 200-day SMA indicator to strategy.st
add.indicator(strategy = strategy.st, name = "SMA", 
              arguments = list(x=quote(quantmod::Cl(mktdata)), n=200), label = "SMA200"
              )


# Add a 50-day SMA indicator to strategy.st
add.indicator(strategy = strategy.st, name = "SMA", 
              arguments = list(x=quote(qunatmod::Cl(mktdata)), n=50), label = "SMA50"
              )


# Add an RSI 3 indicator to strategy.st
add.indicator(strategy = strategy.st, name = "RSI", 
              arguments = list(x=quote(quantmod::Cl(mktdata)), n=3), label = "RSI_3"
              )


# Write the RSI_avg function
RSI_avg <- function(price, n1, n2) {

    # RSI 1 takes an input of the price and n1
    rsi_1 <- RSI(price = price, n = n1)
    # RSI 2 takes an input of the price and n2
    rsi_2 <- RSI(price = price, n = n2)
    # RSI_avg is the average of rsi_1 and rsi_2
    RSI_avg <- (rsi_1 + rsi_2)/2
  
    # Your output of RSI_avg needs a column name of RSI_avg
    colnames(RSI_avg) <- "RSI_avg"
    return(RSI_avg)
}

# Add this function as RSI_3_4 to your strategy with n1 = 3 and n2 = 4
add.indicator(strategy.st, name = "RSI_avg", 
              arguments = list(price = quote(Cl(mktdata)), n1 = 3, n2 = 4), label = "RSI_3_4"
              )


# While the RSI is decent, it is somewhat outdated as far as indicators go
# In this exercise, you will code another indicator from scratch
# The indicator is called the David Varadi Oscillator (DVO), originated by David Varadi, a quantitative research director
# The version you will implement is a simplified version.

# The purpose of this oscillator is similar to something like the RSI in that it attempts to find opportunities to buy a temporary dip and sell in a temporary uptrend
# In addition to obligatory market data, an oscillator function takes in two lookback periods

# First, the function computes a ratio between the closing price and average of high and low prices
# Next, it applies an SMA to that quantity to smooth out noise, usually on a very small time frame, such as two days
# Finally, it uses the runPercentRank() function to take a running percentage rank of this average ratio, and multiplies it by 100 to convert it to a 0-100 quantity

# Declare the DVO function
DVO <- function(HLC, navg = 2, percentlookback = 126) {
  
    # Compute the ratio between closing prices to the average of high and low
    ratio <- Cl(HLC)/((Hi(HLC) + Lo(HLC))/2)
  
    # Smooth out the ratio outputs using a moving average
    avgratio <- SMA(ratio, n = navg)
  
    # Convert ratio into a 0-100 value using runPercentRank()
    out <- runPercentRank(avgratio, n = percentlookback, exact.multiplier = 1) * 100
    colnames(out) <- "DVO"
    return(out)
}


# Add the DVO indicator to your strategy
add.indicator(strategy = strategy.st, name = "DVO", 
              arguments = list(HLC = quote(HLC(mktdata)), navg = 2, percentlookback = 126),
              label = "DVO_2_126"
              )

# Use applyIndicators to test out your indicators
test <- applyIndicators(strategy = strategy.st, mktdata = OHLC(SPY))

# Subset your data between Sep. 1 and Sep. 5 of 2013
test_subset <- test["2013-09-01/2013-09-05"]

```
  
  
  
***
  
Chapter 4 - Signals  
  
Introduction to signals:  
  
* Signals are combinations of indications and market data that signal a specific trading behavior  
	* The signal is generally considered to be necessary but not sufficient to trade  
* Can use the add.signal(strategy=, name=, arguments=list(), label=) function to add a signal to the trading strategy - name is the quoted function name, label is the quoted name for further use  
	* Again similar to the apply methodology - can then use the signal in later coding  
    * sigComparison - relationship between two indicators, returns 1 if TRUE  
    * sigCrossover - returns 1 on the FIRST occurrence of sigComparison being TRUE (or when one signal crosses another)  
    * sigThreshold - compares range-bound indicator to a static quantity  
    * sigFormula - creation of more complex signal functions  
  
sigComparison and sigCrossover:  
  
* Both compare two variables against each other, such as SMA50 and SMA200  
	* The name argument for add.signal() is "sigComparison" or "sigCrossover"  
    * The arguments list includes list(columns=c("col1", "col2"), relationship=)  # where "lt" is less than, "gt" is greater than, "eq" is equals, "lte" is less than or equals, "gte" is greater than or equals  
* The sigCrossover is usually the signal for a trade, while the sigComparison is the signal that a certain condition of interest currently exists  
  
sigThreshold:  
  
* With oscillating indicators, may want to take a specific threshold and use that for an action trigger  
	* add.signal(strategy=, name="sigThreshold", arguments=list(column="col1", threshold=20, cross=TRUE, relationship="lt"), label=)  # cross=TRUE mimics sigCrossover while cross=FALSE minics sigComparison  
  
sigFormula:  
  
* Catch-all formulae for making ensembles of signals  
* add.signal(strategy=, name="sigFormula", arguments=list(formula="regular logical statement inside an if statement", cross=TRUE), label=)  
    * Basically, if you would have written "if (myCond & myCond2 & !myCond3)" then formula="myCond & myCond2 & !myCond3"  
  
Example code includes:  
```{r eval=FALSE}

# DO Not RUN - do not have package 'quantstrat'

# Add a sigComparison which specifies that SMA50 must be greater than SMA200, call it longfilter
add.signal(strategy.st, name = "sigComparison", 
           arguments = list(columns = c("SMA50", "SMA200"), relationship = "gt"), label = "longfilter"
           )


# Add a sigCrossover which specifies that the SMA50 is less than the SMA200 and label it filterexit
add.signal(strategy.st, name = "sigCrossover",
           arguments = list(columns = c("SMA50", "SMA200"), relationship = "lt"), label = "filterexit"
           )


# Implement a sigThreshold which specifies that DVO_2_126 must be less than 20, label it longthreshold
add.signal(strategy.st, name = "sigThreshold", 
           arguments = list(column = "DVO_2_126", threshold = 20, relationship = "lt", cross = FALSE), 
           label = "longthreshold"
           )


# Add a sigThreshold signal to your strategy that specifies that DVO_2_126 must cross above 80 and label it thresholdexit
add.signal(strategy.st, name = "sigThreshold", 
           arguments = list(column = "DVO_2_126", threshold = 80, relationship = "gt", cross = TRUE), 
           label = "thresholdexit"
           )


# Create your dataset: test
test_init <- applyIndicators(strategy.st, mktdata = OHLC(SPY))
test <- applySignals(strategy = strategy.st, mktdata = test_init)


# Add a sigFormula signal to your code specifying that both longfilter and longthreshold must be TRUE, label it longentry
add.signal(strategy.st, name = "sigFormula",
           arguments = list(formula = "longfilter & longthreshold", cross = TRUE), label = "longentry"
           )

```
  
  
  
***
  
Chapter 5 - Rules  
  
Introduction to rules:  
  
* Rules specify how a transaction will be enacted given a signal - the most involved component for quantstrat (significant customization is available)  
	* add.rule(strategy.st, name="ruleSignal", arguments=list(sigcol="filterexit", sigval=TRUE, …), type="exit")  # example of rule  
    * Entry rules take cash and purchase an asset while exit rules sell an asset and return the proceeds to cash  
    * The catch-all function for rules tends to be ruleSignal  
    * The sigval=TRUE (almost always the desired argument) means that the sigcol will be 1 when that signal is true  
  
More rule mechanics:  
  
* The orderqty specifies how much to buy or sell or short - for exits, orderqty="all" will fully close out the trade  
* The ordertype of "market" is a market order while "limit" and "stoplimit" have strike prices for buying or selling  
* The orderside of "long" is for a long trade and of "short" is for a short trade  
  
More rule mechanics - II:  
  
* The replace argument is TRUE if all other rules should be canceled for the day (typically should be set to FALSE)  
* The prefer argument of "Open" means prefer to buy at the Open (next available opportunity)  
  
Order sizing functions:  
  
* An order sizing function allows for variable position sizes (this deprecates the orderqty= argument)  
	* osFUN=, tradeSize=, maxSize=  
* Order sizing functions are somewhat similar to apply  
  
Example code includes:  
```{r eval=FALSE}

# DO Not RUN - do not have package 'quantstrat'

# Fill in the rule's type as exit
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "filterexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Fill in the sigcol argument in add.rule()
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "filterexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Fill in the sigval argument in add.rule()
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "filterexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Fill in the orderqty argument in add.rule()
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "filterexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Fill in the ordertype argument in add.rule()
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "filterexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Fill in the orderside argument in add.rule()
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "filterexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Fill in the replace argument in add.rule()
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "thresholdexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Fill in the prefer argument in add.rule()
add.rule(strategy.st, name = "ruleSignal", 
         arguments = list(sigcol = "thresholdexit", sigval = TRUE, orderqty = "all", 
                          ordertype = "market", orderside = "long", replace = FALSE, prefer = "Open"
                          ), 
         type = "exit"
         )


# Create an entry rule of 1 share when all conditions line up to enter into a position
add.rule(strategy.st, name = "ruleSignal", 
         arguments=list(sigcol = "longentry", sigval = TRUE, orderqty = 1, ordertype = "market",
                        orderside = "long", replace = FALSE, prefer = "Open"
                        ),
         type = "enter"
         )


# Add a rule that uses an osFUN to size an entry position
add.rule(strategy = strategy.st, name = "ruleSignal",
         arguments = list(sigcol = "longentry", sigval = TRUE, ordertype = "market",
                          orderside = "long", replace = FALSE, prefer = "Open",
                          osFUN = osMaxDollar, tradeSize = tradesize, maxSize = tradesize
                          ),
         type = "enter"
         )

```
  
  
  
***
  
Chapter 6 - Analyzing Results  
  
Analyzing strategy:  
  
* Buy when 50-day is above 200-day AND dvo < 20  
* Sell when 50-day cross under 200-day  
* Can run the strategy to see what happens  
	* applyStrategy(strategy=strategy.st, portfolios=portfolio.st)  
    * updatePortf(portfolio.st)  
    * daterange <- time(getPortfolio(portfolio.st)$summary)[-1]  
    * updateAcct(account.st, daterange)  
    * updateEndEq(account.st)  
    * tStats <- tradeStats(Portfolios=portfolio.st)  # Num.Trades, Percent.Positive, etc.  
  
Visualizing strategy:  
  
* chart.Posn(portfolio=portfolio.st, Symbol=)  
* add_TA(myIndicator, on=, col=)  # myInidcator needs to be calculated outside the portfolio strategy (on=1 will put the indicator on the same window)  
* Can use zoom_Chart("YYYY-MM/YYYY-MM")  
  
Additional analytics:  
  
* Generate P&L series for the strategy  
	* Sharpe ratio can be obtained from the P&L - reward vs risk  
    * SharpeRatio.annualized(portPL, geometric=FALSE)  
* Can get the portfolio returns from PortfReturns(account.st)  
  
Example code includes:  
```{r eval=FALSE}

# DO Not RUN - do not have package 'quantstrat'

# Use applyStrategy() to apply your strategy. Save this to out
out <- applyStrategy(strategy = strategy.st, portfolios = portfolio.st)

# Update your portfolio (portfolio.st)
updatePortf(portfolio.st)
daterange <- time(getPortfolio(portfolio.st)$summary)[-1]

# Update your account (account.st)
updateAcct(account.st, daterange)
updateEndEq(account.st)

# What is the date of the last trade?
"2013-12-23"


# Get the tradeStats for your portfolio
tstats <- tradeStats(Portfolios = portfolio.st)

# Print the profit factor
tstats$Profit.Factor


# Use chart.Posn to view your system's performance on SPY
chart.Posn(Portfolio = portfolio.st, Symbol = "SPY")


# Compute the SMA50
sma50 <- SMA(x = Cl(SPY), n = 50)

# Compute the SMA200
sma200 <- SMA(x = Cl(SPY), n = 200)

# Compute the DVO_2_126 with an navg of 2 and a percentlookback of 126
dvo <- DVO(HLC = HLC(SPY), navg = 2, percentlookback = 126)

# Recreate the chart.Posn of the strategy from the previous exercise
chart.Posn(Portfolio = portfolio.st, Symbol = "SPY")

# Overlay the SMA50 on your plot as a blue line
add_TA(sma50, on = 1, col = "blue")

# Overlay the SMA200 on your plot as a red line
add_TA(sma200, on = 1, col = "red")

# Add the DVO_2_126 to the plot in a new window
add_TA(dvo)


portpl <- .blotter$portfolio.firststrat$summary$Net.Trading.PL
SharpeRatio.annualized(portpl, geometric=FALSE)


# Get instrument returns
instrets <- PortfReturns(portfolio.st)

# Compute Sharpe ratio from returns
SharpeRatio.annualized(instrets, geometric = FALSE)

```
  
  
  
***
  
### _Valuation of Insurance Products in R_  
  
Chapter 1 - Valuation of Cash Flows  
  
Cash flows and discounting - suppose you have a unit of capital and a unit of time (t=0 is now and t=K is time in the future and Ck is cash paid at time K):  
  
* Cash-flow vectors for payments from time 0 through time n will be of length n+1  
* The time value of money (TVM) suggests that the value of money now is greater than the value of money in the future; can assume interest rate to be i  
	* if r=I for 1 time period, then the discount rate for k time periods is 1 / (1 + i)**K  
* The discounted value of the cash flows to t=0 is called the present value (PV) of the cash flows  
  
Valuation - consider v(s, t) to be the value at time s of 1 Euro paid at time t:  
  
* When s < t, this is a discounting factor and when s > t then this is an accumulation factor  
* This provides a more general mechanism for valuing a cash flow vector - can use v(s, t) for each of the entries to calculate either present value (value for t=0) or accumulated value (value for t=N)  
	* Can create the function discount <- function(s, t, i = 0.03) { (1 + i) ^ - (t - s) }  
  
Actuarial equivalence:  
  
* Goal is often to establish an equivalence between two cash flows - example being a loan (money out from lender, repayments in to lender) or an insurance product (premiums vs. benefits)  
	* Cash flows would typically be discounted by the interest rate, which might vary by transaction  
  
Change of period and term structure:  
  
* Suppose that you have an interest rate, r, that applies over a time period of 1 unit (perhaps a year)  
	* If instead the rate is desired over a time period of k units, then the interest rate would be (1+r)**k - 1 over that time period  
    * For example, to get a monthly interest rate from a yearly interest rate, use k=1/12 and get (1+r)**(1/12) - 1  
* Interest rates can often change over time - "term structure of interest rates"  
	* Basically, use a different discount factor for each of the time periods  
    * The cumprod() function can be very helpful for getting all the relevant products of the discount factors  
  
Example code includes:  
```{r}

# Define the cash flows
cash_flows <- c(rep(30, 4), 130, 230)
  
# Define i and v
i <- 0.02
v <- 1 / (1 + i)
  
# Define the discount factors
discount_factors <- v ^ (0:5)
  
# Calculate the present value
present_value <- sum(discount_factors * cash_flows)
present_value


# Define the cash flows
cash_flows <- c(0, rep(3000, 3), rep(1000, 2))
  
# Define the discount factors
discount_factors <- (1 + 0.05) ^ -(0:5)
  
# Calculate the net present value
net_present_value <- sum(discount_factors * cash_flows) - 10000
net_present_value


# Define the discount function v
discount <- function(s, t, i = 0.02) {
    (1 + i) ^ -(t - s)
}

# Calculate the present value
present_value <- sum(cash_flows * discount(0, 0:5))
present_value

# Calculate the value at time 6
sum(cash_flows * discount(6, 0:5))

# Calculate the value at time 6, starting from present_value
present_value * discount(6, 0)


# Calculate the present value of the deposits
# PV_deposit <- sum(deposits * discount_factors) 

# Calculate the present value of the payments
# PV_payment <- sum(payments * discount_factors) 

# Calculate the yearly deposit K in the first 4 years
# K <- PV_payment / PV_deposit
# K


# Interest rates
interest <- c(rep(0.05, 3), rep(0.06, 3), rep(0.07, 5))

# Define the yearly discount factors
yearly_discount_factors <- (1+interest) ^ ( - 1)

# Define the discount factors
discount_factors <- c(1 , cumprod(yearly_discount_factors))

# Define the cash flow vector
cash_flow <- c(1000, 5000, rep(-816.86, 10))

# Calculate the PV
PV <- sum(cash_flow * discount_factors)
PV


# Define the number of payments
number_payments <- 20*12

# Define the yearly interest rate
i <- 0.0304
  
# Calculate the monthly interest rate
monthly_interest <- (1 + i) ^ (1/12) - 1
monthly_interest


# Define the discount factors
discount_factors <- (1+monthly_interest) ^ - (1:number_payments)

# Define the payment pattern
payments <- rep(1, number_payments)

# Calculate the monthly loan payment K
K <- 125000 / sum(payments * discount_factors)
K

```
  
  
  
***
  
Chapter 2 - Life Tables  
  
Random future lifetime - often used to express future life/mortality for a policyholder of age X:  
  
* The random variable T(x) is used to express the future lifetime of X, with X + T(X) being the distribution of the possible ages at death  
* The Human Mortality Data at www.mortality.org have life tables for a number of countries  
	* The variable q(x) is used to define the mortality rate at age x; p(x) = 1 - q(x) is defined as the survival probability at age x  
    * The column qx runs from age 0 to age n, so element t reflects mortality at age t-1  
    * The life expectancy at age X, E[T(X)], is stored in the variable ex -- this is future years of life remaining  
  
Binomial experiments - q(x) and p(x):  
  
* The lx variable in life_table assumes that 100000 females were born in 1999 and looks at how many of them would still be alive at various ages  
	* sims <- rbinom(n=length(lx), size=lx, prob=px)  
    * The kPx variable can be derived as l(x+k) / l(x)  
  
Calculating probabilities:  
  
* Can multiply probabilities over smaller time periods to get the overall probability (assuming the underlying probabilities are independent)  
	* prod(px[t1:t2]) or cumprod(px[t1:t2])  
* The "deferred mortality probability" is the likelihood that someone survives for k years and then dies in the next year  
	* # 5-year deferred mortality probability of (65)  
    * prod(px[(65 + 1):(69 + 1)]) * qx[70 + 1] # results will be 0.02086664  
  
Calculating life expectancies:  
  
* The "curtate future lifetime" is the number of whole years lived in to the future - sum from k=1 to infinite of kPx  
	* # one-year survival probabilities  
    * head(px[(35 + 1):length(px)]) # 0.99883 0.99896 0.99902 0.99879 0.99887 0.99824  
    * # k-year survival probabilities of (35)  
    * kp35 <- cumprod(px[(35 + 1):length(px)])  
    * head(kp35) # 0.9988300 0.9977912 0.9968134 0.9956072 0.9944822 0.9927319  
    * # curtate expected future lifetime of (35)  
    * sum(kp35) # 43.53192  
* As a rule of thumb, total life expectancy is curtate life expectancy plus half a year (on average, will die middle of a year) - curtate is only the expected whole number of years (floor) lived from today  
  
Dynamics - mortality has been changing over time, and tables are published regularly:  
  
* Can have mortality rates as a matrix, with ages as the rows and years as the columns  
	* Can use the vertical columns to calculate the cumulative mortality  
    * Alternately, can use the diagonals, to reflect that as people get older by a year, the calendar year also advances by a year  
    * The diagnonal approach is more accurate but also requires estimates for future mortality rates (complicated and subject to much research)  
  
Example code includes:  
```{r}

# The mortality rates have been obtained from the Human Mortality Database and are stored in the data set life_table

life_table_1841_2015 <- read_csv2("./RInputFiles/life_table_females.csv")
life_table_1999 <- read_csv2("./RInputFiles/life_table_females_1999.csv")
str(life_table_1841_2015)
str(life_table_1999)

life_table <- life_table_1999

# Inspect life_table using str(), head() and tail()
str(life_table)
head(life_table)
tail(life_table)

# Define age, qx and ex
age <- life_table$age
qx <- life_table$qx
ex <- life_table$ex
lx <- life_table$lx
dx <- life_table$dx
px <- 1 - qx

# The probability that (18) dies before turning 19
qx[age==18]

# The expected future lifetime of (18)
ex[age==18]


par(mfrow=c(1, 1))
par(mfcol=c(1, 1))

# Plot the female mortality rates in the year 1999
plot(age, qx, main = "Mortality rates (Belgium, females, 1999)", 
     xlab = "Age x", ylab = expression(paste("Mortality rate ", q[x])), type = "l"
     )

# Plot the logarithm of the female mortality rates in the year 1999
plot(age, log(qx), main = "Log mortality rates (Belgium, females, 1999)", 
    xlab = "Age x", ylab = expression(paste("Log mortality rate ", log(q[x]))), type = "l"
    )


# Compute the probabilty for (0) to reach the age 100
lx[101] / lx[1]

# Compute the probabilty for (18) to reach the age 100
lx[101] / lx[19]

# Plot the survival probabilties for (18) up to age 100
k <- 0:82
plot(k, lx[19+k]/lx[19], pch = 20, xlab = "k", 
     ylab = expression(paste(""[k], "p"[18])), main = "Survival probabilities for (18)"
     )


# Plot the number of deaths dx by age
plot(age, dx, type = "h", pch = 20, xlab = "Age x", 
     ylab = expression("d"[x]), main = "Number of deaths (Belgium, females, 1999)"
     )

# Simulate the number of deaths using a binomial distribution
sims <- rbinom(n = length(lx), size = lx, prob = qx)
  
# Plot the simulated number of deaths on top of the previous graph
points(age, sims, pch = 4, col = "red")


# Calculate the probability that (18) survives 5 more years
prod(px[(18+1):(18+5)])

# Compute the survival probabilities of (18) until the age of 100
kpx <- cumprod(px[(18+1):(100)])

# Extract the probability that (18) survives until the age of 100
kpx[length(kpx)]

# Plot the probabilties for (18) to reach the age of 19, 20, ..., 100
plot(1:length(kpx), kpx, pch = 20, xlab = "k", 
     ylab = expression(paste(""[k], "p"[18])), main = "Survival probabilities for (18)"
     )


# Compute the survival probabilities of (18)
kpx <- c(1, cumprod(px[(18+1):(length(px) - 1)]))

# Compute the deferred mortality probabilities of (18)
kqx <- kpx * qx[(18+1):length(px)]

# Print the sum of kqx
sum(kqx)

# Plot the deferred mortality probabilities of (18)
plot(0:(length(kqx)-1), kqx, pch = 20, xlab = "k", 
     ylab = expression(paste(""['k|'], "q"[18])), main = "Deferred mortality probabilities of (18)"
     )


# Survival probabilities and curtate expected future lifetime of (0)
kp0 <- cumprod(px)
sum(kp0)

# Survival probabilities and curtate expected future lifetime of (18)
kp18 <- cumprod(px[(18+1):length(px)])
sum(kp18)

# Complete expected future lifetime of (0) and (18)
ex[c(1, 19)]


# Function to compute the curtate expected future lifetime for a given age and life table
curtate_future_lifetime <- function(age, life_table) {
    px <- 1 - life_table$qx
    kpx <- cumprod(px[(age+1):length(px)])
    sum(kpx)
}

# Vector of ages
ages <- life_table$age

# Curtate future lifetimes for all ages
future_lifetimes <- sapply(ages, FUN=curtate_future_lifetime, life_table)

# Future lifetime by age
plot(ages, future_lifetimes, type = 'l', lwd = 2, col = "green", 
     xlab = "Age x", ylab = "Future lifetime", main = "Future lifetime by age"
     )


# Explore life_table
life_table <- life_table_1841_2015
str(life_table)
head(life_table)
range(life_table$year)

# Plot the logarithm of the female mortality rates for (18) by year
with(subset(life_table, age == 18), 
     plot(year, log(qx), type = "l", main = "Log mortality rates (Belgium, females, 18-year-old)", 
          xlab = "Year t", ylab = expression(paste("Log mortality rate ", log(q[18])))
          )
     )

# Plot the logarithm of the female mortality rates in the year 1950 by age
with(subset(life_table, year==1950), 
     plot(age, log(qx), type = "l", main = "Log mortality rates (Belgium, females, 1950)",
          xlab = "Age x", ylab = expression(paste("Log mortality rate ", log(q[x])))
          )
     )


# Construct and print the cohort life table for birth year 1981
life_table_1981 <- subset(life_table, year-age == 1981)
life_table_1981

# 1981 cohort one-year survival probabilities
px <- 1 - life_table_1981$qx

# 1981 cohort survival probability that (18) survives 5 more years
prod(px[(18+1):(18+5)])

# 1881 cohort survival probability that (18) survives 5 more years
with(subset(life_table, year - age == 1881), prod(1 - qx[(18 + 1):(22 + 1)]))

```
  
  
  
***
  
Chapter 3 - Life Annuities  
  
The basics - simple life annuities:  
  
* An "annuity certain" is a payment that should be made on a specific schedule under any situation  
* A "life annuity" is contingent on the individual being alive, and is guaranteed only for that variable amount of time (could be greater or lesser)  
	* The number of payments made is stochastic  
    * The "pure endowment" annuity has only a single payment, and that payment is a 1/0 depending on whether the individual is alove at time K - depends on both the discount rate and likelihood of living  
  
Whole, temporary, and deferred life annuities:  
  
* Can have many payments at different times, each multiplied by the appropriate discount factor and likelihood of surviving to the time of the payment  
* With "whole life annuities", payments are made at the beginning of the year (so the first payment is theoretically at time zero) - a(x) with a double-dot  
* With "whole life immediate annuities", payments are made at the end of the year (so the first payment is made at time 1, but only if the annuitant survives a year) - a(x)  
* With "temporary life annuities", payments are made for a maximum of n years (from 0 to n-1)  
* With "deferred whole life annuities", no payment is made until the end of year u  
  
Guaranteed payments:  
  
* Sometimes, an annuity contains both a guarantee (first n years) and then a life annuity (paid only if the annuitant is alive each year after n)  
* Can combine the approaches, using the guaranteed values for the first n years, and the probability of being alive times the annual payment for all future years  
  
On premium payments and retirement plans:  
  
* Premiums plus interest earnings should match benefits promised (all assuming appropriate discounting)  
	* Since premiums stop at death, set up actuarial equivalence between the expected premium/interest stream, and the expected benefits stream  
    * The premium payment vector is often called rho  
  
Example code includes:  
```{r}

life_table <- life_table_1999
str(life_table)

# Define age, qx and ex
age <- life_table$age
qx <- life_table$qx
ex <- life_table$ex
lx <- life_table$lx
dx <- life_table$dx
px <- 1 - qx


# PV of guaranteed payment of 10,000 in 5 years
PV <- 10000 * (1+0.02) ^ -5
PV

# 5 year survival probabilities of (20)
kpx <- prod(px[(20+1):(20+5)])

# EPV of pure endowment of 10,000 in 5 years for (20)
PV * kpx


# PV of guaranteed payments of 10,000 in 5, 10 and 30 years
PV <- 10000 * (1+0.02) ^ -c(5, 10, 30)
PV

# Survival probabilities of (20)
kpx <- cumprod(px[(20+1):length(px)])

# EPV of pure endowments of 10,000 in 5, 10 and 30 years for (20)
PV * kpx[c(5, 10, 30)]


# Function to compute the EPV of a whole life annuity due for a given age, interest rate i and life table
life_annuity_due <- function(age, i, life_table) {
    px <- 1 - life_table$qx
    kpx <- c(1, cumprod(px[(age+1):length(px)]))
    discount_factors <- (1+i) ^ -(0:(length(kpx)-1))
    sum(discount_factors * kpx)
}

# EPV of a whole life annuity due for (20) at interest rate 2% using life_table
life_annuity_due(age=20, i=0.02, life_table=life_table)

# EPV of a whole life annuity due for (20) at interest rate 5% and for (65) at interest rate 2% using life_table
life_annuity_due(age=20, i=0.05, life_table=life_table)
life_annuity_due(age=65, i=0.02, life_table=life_table)


# EPV of a whole life annuity due for (20) at interest rate 2% using life_table
life_annuity_due(20, 0.02, life_table)

# Function to compute the EPV of a whole life immediate annuity for a given age, interest rate i and life table
life_immediate_annuity <- function(age, i, life_table) {
    px <- 1 - life_table$qx
    kpx <- cumprod(px[(age + 1):length(px)])
    discount_factors <- (1+i) ^ -(1:(length(kpx)))
    sum(discount_factors * kpx)
}

# EPV of a whole life immediate annuity for (20) at interest rate 2% using life_table
life_immediate_annuity(20, 0.02, life_table)


# EPV of a whole life annuity due for (20) at interest rate 2% using life_table
life_annuity_due(20, 0.02, life_table)

# Function to compute the EPV of a temporary life annuity due for a given age, period of n years, interest rate i and life table
temporary_life_annuity_due <- function(age, n, i, life_table) {
    px <- 1 - life_table$qx
    kpx <- c(1, cumprod(px[(age+1):(age+n-1)]))
    discount_factors <- (1 + i) ^ - (0:(n-1))
    sum(discount_factors*kpx)
}

# EPV of a temporary life annuity due for (20) over 10 years at interest rate 2% using life_table
temporary_life_annuity_due(age=20, n=10, i=0.02, life_table)


# Pension benefits
benefits <- 20000 * (1+0.02) ^ (0:35)

# Discount factors (to age 65)
discount_factors <- (1+0.04) ^ -(0:35)

# PV of pension at age 65
PV_65 <- sum(discount_factors * benefits)
PV_65

# PV of pension at age 20
PV_20 <- PV_65 * (1+0.03) ^ -45
PV_20 


# Survival probabilities of (65) up to age 100
kpx <- c(1, cumprod(px[(65+1):(100)]))

# EPV of pension at age 65
EPV_65 <- sum(discount_factors * kpx * benefits)
cbind(PV_65, EPV_65)

# EPV of pension at age 20
EPV_20 <- EPV_65 * (1.03 ^ -45 * prod(px[(20+1):(65)]))
cbind(PV_20, EPV_20)


# Survival probabilities of (40)
kpx <- c(1, cumprod(px[(40+1):length(px)])) 

# Discount factors (to age 40)
discount_factors <- (1 + 0.03) ^ -(0:(length(kpx)-1))

# Pension benefits
benefits <- c(rep(0, 25), rep(18000, length(kpx) - 25))

# The single premium
single_premium <- sum(benefits * discount_factors * kpx)
single_premium


# Premium pattern rho
rho <- c(rep(1, 15), rep(0.5, 10), rep(0, length(kpx)-25))

# The initial premium
initial_premium <- single_premium / sum(rho * discount_factors * kpx)
initial_premium

# The annual premiums 
initial_premium * rho

# Sum of the annual premiums (no actuarial discounting)
sum(initial_premium * rho)


# Curtate life expectancy of (40)
sum(kpx[-1])

# Present value of annuity benefits when (40) lives until age 75
subset1 <- 1:36
sum(benefits[subset1] * discount_factors[subset1])

# Present value of annuity benefits when (40) lives until age 95
subset2 <- 1:56
sum(benefits[subset2] * discount_factors[subset2])

```
  
  
  
***
  
Chapter 4 - Life Insurances  
  
Basics - life insurance is somewhat the inverse of the life annuity (pays benefits upon death):  
  
* Simple life insurance has a single payment at time 0, and will pay out in the year when the policyholder dies  
* Often need to calculate survival probability through age k-1 multiplied by death probability at age k  
  
Whole, temporary, and deferred:  
  
* The death benefit vector can be created as a series of simple life insurance vectors  
	* Whole life insurance is in force forever  
    * Term life insurance is in force only over a specific term  
    * Deferred whole life insurance is in force forever, but only after they have survived through a specific deferral period  
* Calculation of whole life insurance for a 35-year old with interest rate i=3%  
	* # Whole-life insurance of (35)  
    * kpx <- c(1, cumprod(px[(35 + 1):(length(px) - 1)]))  
    * kqx <- kpx * qx[(35 + 1):length(qx)]  
    * discount_factors <- (1 + 0.03) ^ - (1:length(kqx))  
    * benefits <- rep(1, length(kqx))  
    * sum(benefits * discount_factors * kqx) # 0.2880872  
* Calculation of a 20-year deferred whole life insurance for a 35-year old with interest rate i=3% (only the benefits vector is modified)  
	* # Whole-life insurance of (35)  
    * kpx <- c(1, cumprod(px[(35 + 1):(length(px) - 1)]))  
    * kqx <- kpx * qx[(35 + 1):length(qx)]  
    * discount_factors <- (1 + 0.03) ^ - (1:length(kqx))  
    * benefits <- c(rep(0, 20), rep(1, length(kqx) - 20))  
    * sum(benefits * discount_factors * kqx)  
  
Combined benefits:  
  
* Endowment insurance includes both a death benefit and a savings component (payment made at a certain date, assuming the person is alive at that date)  
	* Can calculate the EV of the death benefit first, then calculate the EV of the savings component  
  
Wrap up:  
  
* Valuation of cash flows, including discount factors  
* Life tables, and probabilistic models for human mortality  
* Life insurance products for life-contingent risks - annuities (including pensions) and insurance (including endowments)  
* Many textbooks are available for further study, including relevant R code  
* More advanced courses cover topics like reserves and insuring multiple lives and investment strategies  
* Basically, the actuary is the data scientist for insurance  
  
Example code includes:  
```{r}

# 10-year survival probability of (20) 
kpx <- prod(px[(20+1):(30)])
kpx

# 10-year deferred mortality probability of (20) 
kqx <- kpx * qx[30+1]
kqx

# Discount factor
discount_factor <- (1 + 0.01) ^ -11
discount_factor

# EPV of the simple life insurance
10000 * discount_factor * kqx


plot_by_age <- function() {
    ages <- 0:100
    EPV <- sapply(ages, whole_life_insurance, i = 0.03, life_table = life_table)
    plot(ages, EPV, type = 'l', col = "red", ylim = c(0, 1),
         main = "Whole life insurance (interest rate i = 3%)", xlab = "Age x", ylab = "EPV"
         )
}

plot_by_interest_rate <- function() {
    interest_rates <- seq(0.001, 0.10, by = 0.001)
    EPV <- sapply(interest_rates, whole_life_insurance, age = 20, life_table = life_table)
    plot(interest_rates, EPV, type = 'l', col = "red", ylim = c(0, 1),
         main = "Whole life insurance (age 20)", xlab = "Interest rate i", ylab = "EPV"
         )
}

# Function to compute the EPV of a whole life insurance
whole_life_insurance <- function(age, i, life_table) {
    qx <- life_table$qx
    px <- 1 - qx
    kpx <- c(1, cumprod(px[(age+1):(length(px) - 1)]))
    kqx <- kpx * qx[(age+1):length(qx)]
    discount_factors <- (1 + i) ^ - (1:length(kqx))
    sum(discount_factors * kqx)
}

# Plot the EPV of a whole life insurance for a range of ages at interest rate 3% using life_table
plot_by_age()

# Plot the EPV of a whole life insurance for (20) for a range of interest rates using life_table
plot_by_interest_rate()


# EPV of a whole life insurance for (20) at interest rate 2% using life_table
whole_life_insurance(20, 0.02, life_table)

# Function to compute the EPV of a temporary life insurance
temporary_life_insurance <- function(age, n, i, life_table) {
    qx <- life_table$qx
    px <- 1 - qx
    kpx <- c(1, cumprod(px[(age+1):(age+n-1)]))
    kqx <- kpx * qx[(age+1):(age+n)]
    discount_factors <- (1 + i) ^ -(1:length(kqx))
    sum(discount_factors * kqx)
}

# EPV of a temporary life insurance for (20) over a period of 45 years at interest rate 2% using life_table
temporary_life_insurance(age=20, n=45, i=0.02, life_table)


# EPV of a whole life insurance for (20) at interest rate 2% using life_table
whole_life_insurance(20, 0.02, life_table)

# Function to compute the EPV of a deferred whole life insurance
deferred_life_insurance <- function(age, u, i, life_table) {
    qx <- life_table$qx;  px <- 1 - qx
    kpx <- c(1, cumprod(px[(age + 1):(length(px) - 1)]))
    kqx <- kpx * qx[(age + 1):length(qx)]
    discount_factors <- (1 + i) ^ - (1:length(kqx))
    benefits <- c(rep(0, u), rep(1, length(kpx) - u))
    sum(benefits * discount_factors * kqx)
}

# EPV of a deferred life insurance for (20) deferred over 45 years at interest rate 2% using life_table
deferred_life_insurance(age=20, u=45, i=0.02, life_table)


i <- 0.05

# Deferred mortality probabilites of (48)
kqx <- c(1, cumprod(px[(48+1):(48+27-1)])) * qx[(48+1):(48+27)]

# Discount factors
discount_factors <- (1 + i) ^ -(1:length(kqx))

# Death benefits
benefits <- c(rep(0, 7), rep(40000, length(kqx) - 7))

# EPV of the death benefits                      
EPV_death_benefits <- sum(benefits * discount_factors * kqx)
EPV_death_benefits


# Pure endowment
EPV_pure_endowment <- 80000 * (1 + i) ^  -27 * prod(px[(48+1):(48+27)])
EPV_pure_endowment

# Premium pattern
kpx <- c(1, cumprod(px[(48+1):(48+27-1)]))
discount_factors <- (1+i) ^ - (0:(length(kpx) - 1)) 
rho <- rep(1, length(kpx))
EPV_rho <- sum(kpx * discount_factors * rho)
EPV_rho

# Premium level
(EPV_pure_endowment + EPV_death_benefits) / EPV_rho

```
  
  
  
***
  
### _Data Manipulation with dplyr in R_  
  
Chapter 1 - Transforming Data with dplyr  
  
Counties Dataset:  
  
* Four main dplyr verbs are select, filter, arrange, mutate  
* The dataset will be US census data at the county level  
	* dplyr::glimpse(counties)  
    * counties_selected <- counties %>% select(state, county, population, unemployment)  
  
Filter and Arrange Verbs:  
  
* Can use arrange() to sort the data by the given field(s) - can wrap the variable in desc() to sort descending  
* can use filter() to filter based on a condition that returns a boolean  
  
Mutate:  
  
* The mutate() verb allows for creating new variables from the existing variables  
  
Example code includes:  
```{r}

counties <- readRDS("./RInputFiles/counties.rds")
babynames <- readRDS("./RInputFiles/babynames.rds")
str(counties)
str(babynames)


# Select the columns 
counties %>%
    select(state, county, population, poverty)


counties_selected <- counties %>%
    select(state, county, population, private_work, public_work, self_employed)

# Add a verb to sort in descending order of public_work
counties_selected %>%
    arrange(desc(public_work))


counties_selected <- counties %>%
    select(state, county, population)

# Filter for counties in the state of California that have a population above 1000000
counties_selected %>%
    filter(state=="California", population > 1000000)


counties_selected <- counties %>%
    select(state, county, population, private_work, public_work, self_employed)

# Filter for Texas and more than 10000 people; sort in descending order of private_work
counties_selected %>%
    filter(state=="Texas", population > 10000) %>%
    arrange(desc(private_work))


counties_selected <- counties %>%
    select(state, county, population, public_work)

# Sort in descending order of the public_workers column
counties_selected %>%
    mutate(public_workers = public_work * population / 100) %>%
    arrange(desc(public_workers))


# Select the columns state, county, population, men, and women
counties_selected <- counties %>%
    select(state, county, population, men, women)

# Calculate proportion_women as the fraction of the population made up of women
counties_selected %>%
    mutate(proportion_women = women / population)


counties %>%
    # Select the five columns 
    select(state, county, population, men, women) %>%
    # Add the proportion_men variable
    mutate(proportion_men = men/population) %>%
    # Filter for population of at least 10,000
    filter(population >= 10000) %>%
    # Arrange proportion of men in descending order 
    arrange(desc(proportion_men))

```
  
  
  
***
  
Chapter 2 - Aggregating Data  
  
Count Verb:  
  
* The count() verb will give a count by specific grouping of values  
	* count() will give a total count of the rows in the data  
    * count(state) will give total count of rows by state  
* Can add a sort capability to get descending (most common first)  
	* count(state, sort=TRUE)  
* Can add a weighting capability to the count()  
	* count(state, wt=population, sort=TRUE)  # wt=population means that population will be summed by state, rather than returning a count of rows  
  
Group By, Summarize, and Ungroup:  
  
* Can use summarize() to create aggregate statistics such as median(), sum(), max(), n() and the like  
* The group_by(var) command means that any summarize or other action will act on each grouping of var, rather than on the aggregate dataset  
* Can then add an ungroup() to remove the most recent group_by() and revert to regular processing of the data  
  
The top_n verb:  
  
* The top_n() function is useful for keeping the most extreme observations from each group  
	* top_n(1, population) will pull the largest population for each group_by level  
  
Example code includes:  
```{r}

# Use count to find the number of counties in each region
counties %>%
    count(region, sort=TRUE)


# Find number of counties per state, weighted by citizens
counties %>%
    count(state, wt=citizens, sort=TRUE)


counties %>%
    # Add population_walk containing the total number of people who walk to work 
    mutate(population_walk = walk * population / 100) %>%
    # Count weighted by the new column
    count(state, wt=population_walk, sort=TRUE)


# Summarize to find minimum population, maximum unexployment, and average income
counties %>%
    summarize(min_population=min(population), 
              max_unemployment=max(unemployment), 
              average_income=mean(income)
              )


# Add a density column, then sort in descending order
counties %>%
    group_by(state) %>%
    summarize(total_area = sum(land_area), total_population = sum(population)) %>%
    mutate(density = total_population / total_area) %>%
    arrange(desc(density))


# Calculate the average_pop and median_pop columns 
counties %>%
    group_by(region, state) %>%
    summarize(total_pop = sum(population)) %>%
    summarize(average_pop = mean(total_pop), median_pop=median(total_pop))


# Group by region and find the greatest number of citizens who walk to work
counties %>%
    group_by(region) %>%
    top_n(1, walk) %>%
    select(state, county, region, metro, population, walk, citizens)


counties %>%
    group_by(region, state) %>%
    # Calculate average income
    summarize(average_income=mean(income)) %>%
    # Find the highest income state in each region
    top_n(1, average_income)


# Count the states with more people in Metro or Nonmetro areas
counties %>%
    group_by(state, metro) %>%
    summarize(total_pop = sum(population)) %>%
    top_n(1, total_pop) %>%
    ungroup() %>%
    count(metro)

```
  
  
  
***
  
Chapter 3 - Selecting and Transforming Data  
  
Selecting:  
  
* Can use the colon operator(:) in select to select everything from column a to column b inclusive  
	* select(a, b:d)  
* Can use contains or starts_with to select only columns whose name contains a key phrase  
	* select(contains("work"))  
    * select(starts_with("income"))  
* Can find the select helpers using ?select_helpers  
	* last_col() will grab the last column  
* Can use the minus operator(-) to mean 'do not select this column'  
  
Renaming:  
  
* Renaming the columns can be done using rename()  
	* rename(newName=oldName)  
* Can also run renaming inside a select statement  
	* select(a, b, newName=oldName)  
  
Transmuting:  
  
* Transmute is a combination of select and mutate  
	* transmute(a, b, c = d / e)  # final data will have only a, b, c  
  
Example code includes:  
```{r}

# Glimpse the counties table
glimpse(counties)

counties %>%
    # Select state, county, population, and industry-related columns
    select(state, county, population, professional:production) %>%
    # Arrange service in descending order 
    arrange(desc(service))


counties %>%
    # Select the state, county, population, and those ending with "work"
    select(state, county, population, ends_with("work")) %>%
    # Filter for counties that have at least 50% of people engaged in public work
    filter(public_work >= 50)


# Rename the n column to num_counties
counties %>%
    count(state) %>%
    rename(num_counties=n)


# Select state, county, and poverty as poverty_rate
counties %>%
    select(state, county, poverty_rate = poverty)


counties %>%
    # Keep the state, county, and populations columns, and add a density column
    transmute(state, county, population, density = population / land_area) %>%
    # Filter for counties with a population greater than one million 
    filter(population > 1000000) %>%
    # Sort density in ascending order 
    arrange(density)


# Change the name of the unemployment column
counties %>%
    rename(unemployment_rate = unemployment)

# Keep the state and county columns, and the columns containing poverty
counties %>%
    select(state, county, contains("poverty"))

# Calculate the fraction_women column without dropping the other columns
counties %>%
    mutate(fraction_women = women / population)

# Keep only the state, county, and employment_rate columns
counties %>%
    transmute(state, county, employment_rate = employed / population)

```
  
  
  
***
  
Chapter 4 - Case Study  
  
The babynames dataset:  
  
* The babynames data includes the names of babies born in the US by year  
	* Columns are year-name-number  
* Can use ggplot2 to make line plots of given names  
	* babynames %>% filter(name="myName") %>% ggplot(aes(x=year, y=number)) + geom_line()  
  
Grouped Mutates:  
  
* Can run a grouped mutate, which will create the same calculation for every member of the group  
	* myDF %>% group_by(a) %>% mutate(d_total=sum(d), d_pct=d/d_total) %>% ungroup()  
  
Window Functions:  
  
* Window functions include lag() or lead()  
	* v <- c(1, 3, 6, 14)  
    * lag(v) # c(NA, 1, 3, 6)  
    * v - lag(v) # c(NA, 2, 3, 8)  
  
Wrap Up:  
  
* Transforming data using dplyr - select, filter, mutate, arrange  
* Summarizing with count() and group_by()  
* Renaming with rename() and transmute()  
* Plotting data and running window functions  
  
Example code includes:  
```{r}

babynames %>%
    # Filter for the year 1990
    filter(year==1990) %>%
    # Sort the number column in descending order 
    arrange(desc(number))


# Find the most common name in each year
babynames %>%
    group_by(year) %>%
    top_n(1, number)


# Filter for the names Steven, Thomas, and Matthew 
selected_names <- babynames %>%
    filter(name %in% c("Steven", "Thomas", "Matthew"))

# Plot the names using a different color for each name
ggplot(selected_names, aes(x = year, y = number, color = name)) +
    geom_line()


# Find the year each name is most common 
babynames %>%
    group_by(year) %>%
    mutate(year_total=sum(number)) %>%
    ungroup() %>%
    mutate(fraction = number / year_total) %>%
    group_by(name) %>%
    top_n(1, fraction)


names_normalized <- babynames %>%
    group_by(name) %>%
    mutate(name_total = sum(number), name_max = max(number)) %>%
    # Ungroup the table 
    ungroup() %>%
    # Add the fraction_max column containing the number by the name maximum 
    mutate(fraction_max = number / name_max)
names_normalized


# Filter for the names Steven, Thomas, and Matthew
names_filtered <- names_normalized %>%
    filter(name %in% c("Steven", "Thomas", "Matthew"))

# Visualize these names over time
ggplot(names_filtered, aes(x=year, y=fraction_max, color=name)) + 
    geom_line()


# Find the year each name is most common 
babynames_fraction <- babynames %>%
    group_by(year) %>%
    mutate(year_total=sum(number)) %>%
    ungroup() %>%
    mutate(fraction = number / year_total)
babynames_fraction


babynames_fraction %>%
    # Arrange the data in order of name, then year 
    arrange(name, year) %>%
    # Group the data by name
    group_by(name) %>%
    # Add a ratio column that contains the ratio between each year 
    mutate(ratio = fraction / lag(fraction))


babynames_ratios_filtered <- babynames_fraction %>%
    arrange(name, year) %>%
    group_by(name) %>%
    mutate(ratio = fraction / lag(fraction)) %>%
    filter(fraction >= 0.00001)
babynames_ratios_filtered

babynames_ratios_filtered %>%
    # Extract the largest ratio from each name 
    top_n(1, ratio) %>%
    # Sort the ratio column in descending order 
    arrange(desc(ratio)) %>%
    # Filter for fractions greater than or equal to 0.001
    filter(fraction >= 0.001)

```
  
  
  
***
  
### _Introduction to Function Writing in R_  
  
Chapter 1 - How to Write a Function  
  
Rationale for Using Functions:  
  
* Arguments can be passed to function by position, by name, or by both  
	* mean(numbers, 0.1, TRUE)  
    * mean(x=numbers, trim=0.1, na.rm=TRUE)  
    * Typical best practice is to name rare arguments and just pass common arguments - so, mean(numbers, trim=0.1, na.rm=TRUE)  
* Functions reduce the amount of crode writing which also reduces the risk for errors/typos  
  
Converting Scripts in to Functions:  
  
* A typical workflow is to write a script, then convert repeating portions of the script to a function  
	* myFun <- function(arg1, arg2, …) { … }  
    * The final result is returned by the function by default  
  
Code Readability:  
  
* Functions can be thought of as verbs, with dplyr functions being a classic example  
	* This is in contrast to many variables, which can be thought of as nouns (representing objects)  
* Functions should be well-named to describe what they do, contain a verb, and be specific  
* There are some trade-offs between readability and typing time, though typically more time is spent reading code than writing code  
	* Auto-complete further reduces the amount of typing time, even for long variable names  
    * Can also alias any commonly used functions  
* Arguments should be placed in a sensible order, though legacy functions such as lm() do not always follow these practices  
	* Data arguments should come first  
    * Detail arguments should come second  
  
Example code includes:  
```{r}

gold_medals <- c(46, 27, 26, 19, 17, 12, 10, 9, 8, 8, 8, 8, 7, 7, 6, 6, 5, 5, 4, 4, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA)
names(gold_medals) <- c('USA', 'GBR', 'CHN', 'RUS', 'GER', 'JPN', 'FRA', 'KOR', 'ITA', 'AUS', 'NED', 'HUN', 'BRA', 'ESP', 'KEN', 'JAM', 'CRO', 'CUB', 'NZL', 'CAN', 'UZB', 'KAZ', 'COL', 'SUI', 'IRI', 'GRE', 'ARG', 'DEN', 'SWE', 'RSA', 'UKR', 'SRB', 'POL', 'PRK', 'BEL', 'THA', 'SVK', 'GEO', 'AZE', 'BLR', 'TUR', 'ARM', 'CZE', 'ETH', 'SLO', 'INA', 'ROU', 'BRN', 'VIE', 'TPE', 'BAH', 'IOA', 'CIV', 'FIJ', 'JOR', 'KOS', 'PUR', 'SIN', 'TJK', 'MAS', 'MEX', 'VEN', 'ALG', 'IRL', 'LTU', 'BUL', 'IND', 'MGL', 'BDI', 'GRN', 'NIG', 'PHI', 'QAT', 'NOR', 'EGY', 'TUN', 'ISR', 'AUT', 'DOM', 'EST', 'FIN', 'MAR', 'NGR', 'POR', 'TTO', 'UAE', 'IOC')


# Look at the gold medals data
gold_medals

# Note the arguments to median()
args(median)

# Rewrite this function call, following best practices
median(gold_medals, na.rm=TRUE)


# Note the arguments to rank()
args(rank)

# Rewrite this function call, following best practices
rank(-gold_medals, na.last="keep", ties.method = "min")


coin_sides <- c("head", "tail")

# Sample from coin_sides once
sample(coin_sides, 1)


# Your functions, from previous steps
toss_coin <- function() {
    coin_sides <- c("head", "tail")
    sample(coin_sides, 1)
}

# Call your function
toss_coin()


# Update the function to return n coin tosses
toss_coin <- function(n_flips) {
    coin_sides <- c("head", "tail")
    sample(coin_sides, n_flips, replace=TRUE)
}

# Generate 10 coin tosses
toss_coin(10)


# Update the function so heads have probability p_head
toss_coin <- function(n_flips, p_head) {
    coin_sides <- c("head", "tail")
    # Define a vector of weights
    weights <- c(p_head, 1-p_head)
    # Modify the sampling to be weighted
    sample(coin_sides, n_flips, replace = TRUE, prob=weights)
}

# Generate 10 coin tosses
toss_coin(10, p_head=0.8)


snake_river_visits <- readRDS("./RInputFiles/snake_river_visits.rds")
str(snake_river_visits)


# Run a generalized linear regression 
glm(
    # Model no. of visits vs. gender, income, travel
    n_visits ~ gender + income + travel, 
    # Use the snake_river_visits dataset
    data = snake_river_visits, 
    # Make it a Poisson regression
    family = poisson
)

# From previous step
run_poisson_regression <- function(data, formula) {
    glm(formula, data, family = poisson)
}

# Re-run the Poisson regression, using your function
model <- snake_river_visits %>%
    run_poisson_regression(n_visits ~ gender + income + travel)


icLevels <- c("[$0,$25k]", "($25k,$55k]", "($55k,$95k]", "($95k,$Inf)")
trLevels <- c("[0h,0.25h]", "(0.25h,4h]", "(4h,Infh)")
srGender <- c("male", "female")[c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2)]
srIncome <- icLevels[c(1, 1, 2, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 3, 4, 4)]
srTravel <- trLevels[c(1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3)]
snake_river_explanatory <- data.frame(gender=factor(srGender, levels=c("male", "female")), 
                                      income=factor(srIncome, levels=icLevels), 
                                      travel=factor(srTravel, levels=trLevels)
                                      )
str(snake_river_explanatory)


# Run this to see the predictions
snake_river_explanatory %>%
    mutate(predicted_n_visits = predict(model, ., type = "response"))%>%
    arrange(desc(predicted_n_visits))

```
  
  
  
***
  
Chapter 2 - Arguments  
  
Default Arguments:  
  
* Often, it is helpful to have a default argument when users will rarely want to deviate from that  
	* toss_coin <- function(n_flips, p_head=0.5) { … }  
* Can also set defaults to other arguments  
	* myFun <- function(a, b=TRUE, c=b, d=c) { … }  
* There are two special types of defaults  
	* NULL typically drives special handling (see the documentation)  
    * Categorical defaults can be set by passing a character vector in the function arguments and then calling match.arg() in the function body  
    * myFun <- function(a, b=NULL, c=c("d", "e"))  # note that c="d" will be the default if not passed, while an error is generated if something other than "d" or "e" is passed  
* Cutting is the process of converting a numerical variable to a categorical variable (e.g., "0-6", "7-9", etc.)  
  
Passing Arguments Between Functions:  
  
* Can pass arguments from one function to another by placing them in the called function  
* The ellipsis … argument allows for simplifying code, meaning 'accept any other arguments and pass them'  
	* In addition to allow for less typing, it avoids the need to re-write the main function any time the sub-functions update  
    * This is a double-edged sword, as the above behavior depends on both trust and the need for stability  
  
Checking Arguments:  
  
* Mistakes by the function writer are referred to as "bugs"  
* Can modify functions to throw their own error messages when inputs are of the wrong type, format, length, etc.  
	* if (!is.numeric(x)) { stop("Need to provide a numeric for x") }  
* Can instead use the assertive package which provides clear messages when an assertion fails  
	* assert_is_numeric(x)  
    * assert_all_are_positive(x)  
    * The functions is_numeric() and is_positive() are part of the overarching assert_* functions  
* Can also fix inputs if they are otherwise non-meaningful  
	* use_first() will take only the first argument from a longer element  
    * coerce_to(x, "character") will coerce to character  
  
Example code includes:  
```{r}

n_visits <- snake_river_visits$n_visits
summary(n_visits)


# Set the default for n to 5
cut_by_quantile <- function(x, n=5, na.rm, labels, interval_type) {
    probs <- seq(0, 1, length.out = n + 1)
    qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
    right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
    cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the n argument from the call
cut_by_quantile(n_visits, na.rm = FALSE, 
                labels = c("very low", "low", "medium", "high", "very high"), interval_type = "(lo, hi]"
                )


# Set the default for na.rm to FALSE
cut_by_quantile <- function(x, n = 5, na.rm=FALSE, labels, interval_type) {
    probs <- seq(0, 1, length.out = n + 1)
    qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
    right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
    cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the na.rm argument from the call
cut_by_quantile(n_visits, labels = c("very low", "low", "medium", "high", "very high"), 
                interval_type = "(lo, hi]"
                )


# Set the default for labels to NULL
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels=NULL, interval_type) {
    probs <- seq(0, 1, length.out = n + 1)
    qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
    right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
    cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the labels argument from the call
cut_by_quantile(n_visits, interval_type = "(lo, hi]")


# Set the categories for interval_type to "(lo, hi]" and "[lo, hi)"
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, 
                            interval_type=c("(lo, hi]", "[lo, hi)")
                            ) {
    # Match the interval_type argument
    interval_type <- match.arg(interval_type, c("(lo, hi]", "[lo, hi)"))
    probs <- seq(0, 1, length.out = n + 1)
    qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
    right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
    cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the interval_type argument from the call
cut_by_quantile(n_visits)


std_and_poor500 <- readRDS("./RInputFiles/std_and_poor500_with_pe_2019-06-21.rds")
glimpse(std_and_poor500)


# From previous steps
get_reciprocal <- function(x) {
    1 / x
}

calc_harmonic_mean <- function(x) {
    x %>%
        get_reciprocal() %>%
        mean(na.rm=TRUE) %>%
        get_reciprocal()
}

std_and_poor500 %>% 
    # Group by sector
    group_by(sector) %>% 
    # Summarize, calculating harmonic mean of P/E ratio
    summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio))


# From previous step
calc_harmonic_mean <- function(x, na.rm = FALSE) {
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
}

std_and_poor500 %>% 
    # Group by sector
    group_by(sector) %>% 
    # Summarize, calculating harmonic mean of P/E ratio
    summarize(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm=TRUE))


calc_harmonic_mean <- function(x, ...) {
    x %>%
        get_reciprocal() %>%
        mean(...) %>%
        get_reciprocal()
}

std_and_poor500 %>% 
    # Group by sector
    group_by(sector) %>% 
    # Summarize, calculating harmonic mean of P/E ratio
    summarize(hmean_pe_ratio=calc_harmonic_mean(pe_ratio, na.rm=TRUE))


calc_harmonic_mean <- function(x, na.rm = FALSE) {
    # Assert that x is numeric
    assertive.types::assert_is_numeric(x)
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
}

# See what happens when you pass it strings (bombs out, as it should)
# calc_harmonic_mean(std_and_poor500$sector)


calc_harmonic_mean <- function(x, na.rm = FALSE) {
    assertive.types::assert_is_numeric(x)
    # Check if any values of x are non-positive
    if(any(assertive.numbers::is_non_positive(x), na.rm = TRUE)) {
        # Throw an error
        stop("x contains non-positive values, so the harmonic mean makes no sense.")
    }
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
}

# See what happens when you pass it negative numbers (bombs out as it should)
# calc_harmonic_mean(std_and_poor500$pe_ratio - 20)


# Update the function definition to fix the na.rm argument
calc_harmonic_mean <- function(x, na.rm = FALSE) {
    assertive.types::assert_is_numeric(x)
    if(any(assertive.numbers::is_non_positive(x), na.rm = TRUE)) {
        stop("x contains non-positive values, so the harmonic mean makes no sense.")
    }
    # Use the first value of na.rm, and coerce to logical
    na.rm <- assertive.base::coerce_to(assertive.base::use_first(na.rm), "logical")
    x %>%
        get_reciprocal() %>%
        mean(na.rm = na.rm) %>%
        get_reciprocal()
}

# See what happens when you pass it malformed na.rm
calc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5)

```
  
  
  
***
  
Chapter 3 - Return Values and Scope  
  
Returning Values from Functions:  
  
* Functions return the last value after the end of the function is reached  
* It can sometimes be helpful to return prior to the end of the function - e.g., any NA means the result is NA, so can stop  
	* Can use return(myObject) to return myObject AND ALSO end the function  
    * Can return(NaN) to return 'not a number' rather than throwing an error if the operation is not sensible  
* Plots typically invisibly return, so the function is entirely a side effect  
* Can assign a plot to an object, and explore the elements inside using str()  
  
Returning Multiple Values from Functions:  
  
* Functions in R return only a single value, though there are workarounds - return a list, or return an object with attributes  
	* list(a, b, c) as the final command will return a list with a, b, c  
* Can run multi-assignment using the zeallot package  
	* c(a, b, c) %<-% session()  # assumes that session returned a list  
* Can also set attributes on objects such as vectors  
	* attributes(x)  
    * setNames(x, myNames)  
    * attr(x, "names")  # will pull the names attribute  
    * attr(x, "names") <- myNewNames  # will make myNewNames the names attribute  
* As an example, dplyr::group_by() adds attributes to a list to specify the group each record belongs to  
* If you need results to have a particular type, use attributes; otherwise, return lists  
  
Environments:  
  
* Environments are like lists - variables that store variables  
* Environments have parents - can consider them to be inside their parent  
	* parent <- parent.env(myEnvironment)  
    * environmentName(parent)  
    * grandparent <- parent.env(parent)  
* The search() call will show all the environments that are currently available - mostly, the loaded packages  
* Can use the exists() function to test whether a variable exists in an environment  
	* exists("founding_year", envir=myEnvir)  
    * Any time the variable can not be found in the current environment, R will search all possible parent directories to try to find it  
    * exists("founding_year", envir=myEnvir, inherits=FALSE)  # will override the default of continuing to search parent environments and only search the given environment  
  
Scope and Precedence:  
  
* Any time a function is created, it has an environment  
	* If a variable is not defined in the function's environment, it will look to the parent (calling) environment to try to find it  
    * The function's environment is NOT inherited to its parent, so variables formed in the function are not accessible outside the function  
  
Example code includes:  
```{r}

is_leap_year <- function(year) {
    # If year is div. by 400 return TRUE
    if(year %% 400 == 0) {
        return(TRUE)
    }
    # If year is div. by 100 return FALSE
    if(year %% 100 == 0) {
        return(FALSE)
    }  
    # If year is div. by 4 return TRUE
    if(year %% 4 == 0) {
        return(TRUE)
    }
    # Otherwise return FALSE
    return(FALSE)
}


cars <- data.frame(speed=c(4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25), 
                   dist=c(2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46, 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46, 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85)
                   )
str(cars)


# Using cars, draw a scatter plot of dist vs. speed
plt_dist_vs_speed <- plot(dist ~ speed, data = cars)

# Oh no! The plot object is NULL
plt_dist_vs_speed

# Define a scatter plot fn with data and formula args
pipeable_plot <- function(data, formula) {
    # Call plot() with the formula interface
    plot(formula, data)
    # Invisibly return the input dataset
    invisible(data)
}

# Draw the scatter plot of dist vs. speed again
plt_dist_vs_speed <- cars %>% 
    pipeable_plot(dist ~ speed)

# Now the plot object has a value
plt_dist_vs_speed


# Look at the structure of model (it's a mess!)
str(model)

# Use broom tools to get a list of 3 data frames
list(
    # Get model-level values
    model = broom::glance(model),
    # Get coefficient-level values
    coefficients = broom::tidy(model),
    # Get observation-level values
    observations = broom::augment(model)
)

# From previous step
groom_model <- function(model) {
    list(
        model = broom::glance(model),
        coefficients = broom::tidy(model),
        observations = broom::augment(model)
    )
}


library(zeallot)  # needed for %<-%

# Call groom_model on model, assigning to 3 variables
c(mdl, cff, obs) %<-% groom_model(model)

# See these individual variables
mdl; cff; obs


pipeable_plot <- function(data, formula) {
    plot(formula, data)
    # Add a "formula" attribute to data
    attr(data, "formula") <- formula
    invisible(data)
}

# From previous exercise
plt_dist_vs_speed <- cars %>% 
    pipeable_plot(dist ~ speed)

# Examine the structure of the result
str(plt_dist_vs_speed)


capitals <- tibble::tibble(city=c("Cape Town", "Bloemfontein", "Pretoria"), 
                           type_of_capital=c("Legislative", "Judicial", "Administrative")
                           )
national_parks <- c('Addo Elephant National Park', 'Agulhas National Park', 'Ai-Ais/Richtersveld Transfrontier Park', 'Augrabies Falls National Park', 'Bontebok National Park', 'Camdeboo National Park', 'Golden Gate Highlands National Park', 'Hluhluwe–Imfolozi Park', 'Karoo National Park', 'Kgalagadi Transfrontier Park', 'Knysna National Lake Area', 'Kruger National Park', 'Mapungubwe National Park', 'Marakele National Park', 'Mokala National Park', 'Mountain Zebra National Park', 'Namaqua National Park', 'Table Mountain National Park', 'Tankwa Karoo National Park', 'Tsitsikamma National Park', 'West Coast National Park', 'Wilderness National Park')
population <- ts(c(40583573, 44819778, 47390900, 51770560, 55908900), start=1996, end=2016, deltat=5)

capitals
national_parks
population


# From previous steps
rsa_lst <- list(
    capitals = capitals,
    national_parks = national_parks,
    population = population
)
rsa_env <- list2env(rsa_lst)

ls.str(rsa_lst)
ls.str(rsa_env)

# Find the parent environment of rsa_env
parent <- parent.env(rsa_env)

# Print its name
environmentName(parent)


# Compare the contents of the global environment and rsa_env
# ls.str(globalenv())
ls.str(rsa_env)

# Does population exist in rsa_env?
exists("population", envir = rsa_env)

# Does population exist in rsa_env, ignoring inheritance?
exists("population", envir = rsa_env, inherits=FALSE)

```
  
  
  
***
  
Chapter 4 - Case Study on Grain Yields  
  
Grain Yields and Conversion:  
  
* Historic grain yield data is from NASS (National Agricultural Statistical Service)  
* Conversions from imperial to metric units  
	* 1 acre is about the amount of land that 2 oxen can plough in one day  
    * 1 hectare is roughly equal to two football fields (100m x 100m)  
    * 1 bushel is equal to two buckets of peaches  
    * 1 kilogram is roughly equal to the mass of 1 squirrel monkey  
  
Visualizing Grain Yields:  
  
* May want to explore the variability of yield over time  
* Can use a combination of dplyr inner joins and ggplot2 to explore a faceted look  
  
Modeling Grain Yields:  
  
* The geom_smooth() uses the gam (generalized additive model)  
	* lm(y ~ x, data=) will run the standard linear model  
    * mgcv::gam(y ~ s(x1) + x2, data=) will run the gam, where the s() is running a smooth on x1  
* Can get predicted responses using predict()  
	* predict(model, myNewData, type="response")  
  
Wrap Up:  
  
* Writing functions - motivations include less re-writing and more consistency (fewer copy/paste errors)  
* Setting default arguments for functions, and using … to pass arguments between functions  
* Return values, including early return and returning multiple values  
* Case study for writing and using functions as part of analysis  
* Functions do not need to be large and complex - can be as simple as a single line  
  
Example code includes:  
```{r}

library(magrittr)

corn <- readRDS("./RInputFiles/nass.corn.rds")
wheat <- readRDS("./RInputFiles/nass.wheat.rds")
barley <- readRDS("./RInputFiles/nass.barley.rds")
corn <- as_tibble(corn)
wheat <- as_tibble(wheat)
barley <- as_tibble(barley)
str(corn)
str(wheat)
str(barley)


# Write a function to convert acres to sq. yards
acres_to_sq_yards <- function(acres) {
    acres * 4840
}

# Write a function to convert yards to meters
yards_to_meters <- function(yards) {
    yards * 36 * 0.0254
}

# Write a function to convert sq. meters to hectares
sq_meters_to_hectares <- function(sq_meters) {
    sq_meters / 10000
}


# Write a function to convert sq. yards to sq. meters
sq_yards_to_sq_meters <- function(sq_yards) {
    sq_yards %>%
        # Take the square root
        sqrt() %>%
        # Convert yards to meters
        yards_to_meters() %>%
        # Square it
        raise_to_power(2)
}

# Write a function to convert acres to hectares
acres_to_hectares <- function(acres) {
    acres %>%
        # Convert acres to sq yards
        acres_to_sq_yards() %>%
        # Convert sq yards to sq meters
        sqrt() %>%
        yards_to_meters() %>%
        raise_to_power(2) %>%
        # Convert sq meters to hectares
        sq_meters_to_hectares()
}


# Write a function to convert lb to kg
lbs_to_kgs <- function(lbs) {
    lbs * 0.45359237
}

# Write a function to convert bushels to lbs
bushels_to_lbs <- function(bushels, crop) {
    # Define a lookup table of scale factors
    c(barley = 48, corn = 56, wheat = 60, volume = 8) %>%
        # Extract the value for the crop
        extract(crop) %>%
        # Multiply by the no. of bushels
        multiply_by(bushels)
}

# Write a function to convert bushels to kg
bushels_to_kgs <- function(bushels, crop) {
    bushels %>%
        # Convert bushels to lbs
        bushels_to_lbs(crop) %>%
        # Convert lbs to kgs
        lbs_to_kgs()
}

# Write a function to convert bushels/acre to kg/ha
bushels_per_acre_to_kgs_per_hectare <- function(bushels_per_acre, crop = c("barley", "corn", "wheat")) {
    # Match the crop argument
    crop <- match.arg(crop)
    bushels_per_acre %>%
        # Convert bushels to kgs
        bushels_to_kgs(crop) %>%
        # Convert acres to ha
        acres_to_hectares()
}


# View the corn dataset
glimpse(corn)

corn <- corn %>%
    # Add some columns
    mutate(
        # Convert farmed area from acres to ha
        farmed_area_ha = acres_to_hectares(farmed_area_acres),
        # Convert yield from bushels/acre to kg/ha
        yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(yield_bushels_per_acre, crop = "corn")
    )

# Wrap this code into a function
fortify_with_metric_units <- function(data, crop) {
    data %>%
        mutate(
            farmed_area_ha = acres_to_hectares(farmed_area_acres),
            yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(yield_bushels_per_acre, crop = crop)
        )
}

# Try it on the wheat dataset
wheat <- fortify_with_metric_units(wheat, "wheat")


# Using corn, plot yield (kg/ha) vs. year
ggplot(corn, aes(x=year, y=yield_kg_per_ha)) +
    # Add a line layer, grouped by state
    geom_line(aes(group = state)) +
    # Add a smooth trend layer
    geom_smooth()

# Wrap this plotting code into a function
plot_yield_vs_year <- function(data) {
    ggplot(data, aes(year, yield_kg_per_ha)) +
        geom_line(aes(group = state)) +
        geom_smooth()
}

# Test it on the wheat dataset
plot_yield_vs_year(wheat)


usa_census_regions <- tibble::tibble(census_region=c('New England', 'New England', 'New England', 'New England', 'New England', 'New England', 'Mid-Atlantic', 'Mid-Atlantic', 'Mid-Atlantic', 'East North Central', 'East North Central', 'East North Central', 'East North Central', 'East North Central', 'West North Central', 'West North Central', 'West North Central', 'West North Central', 'West North Central', 'West North Central', 'West North Central', 'South Atlantic', 'South Atlantic', 'South Atlantic', 'South Atlantic', 'South Atlantic', 'South Atlantic', 'South Atlantic', 'South Atlantic', 'South Atlantic', 'East South Central', 'East South Central', 'East South Central', 'East South Central', 'West South Central', 'West South Central', 'West South Central', 'West South Central', 'Mountain', 'Mountain', 'Mountain', 'Mountain', 'Mountain', 'Mountain', 'Mountain', 'Mountain', 'Pacific', 'Pacific', 'Pacific', 'Pacific', 'Pacific'), 
                                     state=c('Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont', 'New Jersey', 'New York', 'Pennsylvania', 'Illinois', 'Indiana', 'Michigan', 'Ohio', 'Wisconsin', 'Iowa', 'Kansas', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota', 'Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia', 'District of Columbia', 'West Virginia', 'Alabama', 'Kentucky', 'Mississippi', 'Tennessee', 'Arkansas', 'Louisiana', 'Oklahoma', 'Texas', 'Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming', 'Alaska', 'California', 'Hawaii', 'Oregon', 'Washington')
                                     )
usa_census_regions


# Inner join the corn dataset to usa_census_regions by state
corn <- corn %>%
    inner_join(usa_census_regions, by = "state")

# Wrap this code into a function
fortify_with_census_region <- function(data) {
    data %>%
        inner_join(usa_census_regions, by = "state")
}

# Try it on the wheat dataset
wheat <- fortify_with_census_region(wheat)


# Plot yield vs. year for the corn dataset
plot_yield_vs_year(corn) +
    facet_wrap(~census_region)

# Wrap this code into a function
plot_yield_vs_year_by_region <- function(data) {
    plot_yield_vs_year(data) +
        facet_wrap(vars(census_region))
}

# Try it on the wheat dataset
plot_yield_vs_year_by_region(wheat)


# Wrap the model code into a function
run_gam_yield_vs_year_by_region <- function(data) {
    mgcv::gam(yield_kg_per_ha ~ s(year) + census_region, data = data)
}

# Try it on the wheat dataset
wheat_model <- run_gam_yield_vs_year_by_region(wheat)
corn_model <- run_gam_yield_vs_year_by_region(wheat)


# Make predictions in 2050  
predict_this <- data.frame(year = 2050, census_region = unique(usa_census_regions$census_region))

# Predict the yield
pred_yield_kg_per_ha <- predict(corn_model, predict_this, type = "response")

predict_this %>%
    # Add the prediction as a column of predict_this 
    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)

# Wrap this prediction code into a function
predict_yields <- function(model, year) {
    predict_this <- data.frame(year = year, census_region = unique(usa_census_regions$census_region))
    pred_yield_kg_per_ha <- predict(model, predict_this, type = "response")
    predict_this %>%
        mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha)
}

# Try it on the wheat dataset
predict_yields(wheat_model, year=2050)


# From previous step
fortified_barley <- barley %>% 
    fortify_with_metric_units(crop="barley") %>%
    fortify_with_census_region()

# Plot yield vs. year by region
plot_yield_vs_year_by_region(fortified_barley)

fortified_barley %>% 
    # Run a GAM of yield vs. year by region
    run_gam_yield_vs_year_by_region()  %>% 
    # Make predictions of yields in 2050
    predict_yields(year=2050)

```
  
  
  
***
  
### _Introduction to Data Visualization with ggplot2_  
  
Chapter 1 - Introduction  
  
Introduction:  
  
* Data visualization is a core skill that combines statistics and design  
* Visualizations can be exploratory or explanatory - audiences and intentions are frequently different  
	* Good design begins with thinking about the audience, which can be as small as the analyst themselves  
    * There is frequently an iterative, fine-tuning approach to the visualization  
  
Grammar of Graphics:  
  
* Graphics are built on an underlying grammar  
	* "Grammar of Graphics" - Leland Wilkinson  
    * Graphics are made up of distinct layers of grammatical elements  
    * Meaningful plots are made through aesthetic mappings  
* There are three essential elements of graphical grammar, plus (optionally) themes, statistics, coordinates, and facets  
	* Data - the dataset being plotted  
    * Aesthetics - scales on to which the data are mapped  
    * Geometries - visual elements used for our data  
    * Themes - non-data ink  
  
Layers:  
  
* The ggplot2 package implements the grammar of graphics in R  
* Can use the iris dataset for the data in the examples, with aesthetics mapping the data and the geometry specifying how to plot it  
	* ggplot(iris, aes(x=, y=)) + geom_jitter()  
* Can also control the non-data ink by adding a theme, such as myplot + theme_classic()  
  
Example code includes:  
```{r cache=TRUE}

data(mtcars)

# Explore the mtcars data frame with str()
str(mtcars)

# Execute the following command
ggplot(mtcars, aes(cyl, mpg)) +
    geom_point()


# Change the command below so that cyl is treated as factor
ggplot(mtcars, aes(factor(cyl), mpg)) +
    geom_point()


# Edit to add a color aesthetic mapped to disp
ggplot(mtcars, aes(wt, mpg, color=disp)) +
    geom_point()

# Change the color aesthetic to a size aesthetic
ggplot(mtcars, aes(wt, mpg, size = disp)) +
    geom_point()


data(diamonds)
str(diamonds)


# Add geom_point() with +
ggplot(diamonds, aes(carat, price)) +
    geom_point()

# Add geom_smooth() with +
ggplot(diamonds, aes(carat, price)) +
    geom_point() +
    geom_smooth()


# Make the points 40% opaque
ggplot(diamonds, aes(carat, price, color = clarity)) +
    geom_point(alpha=0.4) +
    geom_smooth()


# Draw a ggplot
plt_price_vs_carat <- ggplot(
    # Use the diamonds dataset
    diamonds,
    # For the aesthetics, map x to carat and y to price
    aes(x=carat, y=price)
)

# Add a point layer to plt_price_vs_carat
plt_price_vs_carat + 
    geom_point()


# Edit this to make points 20% opaque: plt_price_vs_carat_transparent
plt_price_vs_carat_transparent <- plt_price_vs_carat + 
    geom_point(alpha=0.2)

# See the plot
plt_price_vs_carat_transparent


# Edit this to map color to clarity,
# Assign the updated plot to a new object
plt_price_vs_carat_by_clarity <- plt_price_vs_carat + 
    geom_point(aes(color=clarity))

# See the plot
plt_price_vs_carat_by_clarity

```
  
  
  
***
  
Chapter 2 - Aesthetics  
  
Visible Aesthetics:  
  
* On a scatter plot, the x and y axes are aesthetics (the data are mapped on to them) and color/size are optional aesthetics  
	* If an aesthetic is mapped in the main ggplot(), it will inherit to everything else in the plot  
    * Other aesthetic types include fill, alpha, linetype, labels, shape  
  
Using Attributes:  
  
* Attributes are how things look in ggplot2  
	* By contrast, aesthetics in ggplot2 are mappings from the data  
* Attributes should always be set inside the geom, such as geom_point(color="red")  
  
Modifying Aestehtics:  
  
* One common adjustment is position, or how overlapping points are managed  
	* "identity" is the default for a scatter plot - value is exactly where the x/y map  
    * "jitter" adds some random noise, either as geom_point(position="jitter") or with a pre-defined variable posn_j <- position_jitter(0.1, seed=136) and then geom_point(position=posn_j)  
* Scale functions can all be accessed using scale_<aes>_*()  
	* scale_x_continuous(limits=c(2, 8), breaks=seq(2, 8, 3))  
    * scale_color_discrete()  
  
Aesthetics Best Practices:  
  
* Form should follow function, which is highly audience and intention dependent  
	* Accuracy and efficiency are always high priorities  
    * Visually appealing is a secondary objective  
* The best aesthetics are both efficient and accurate  
	* Unaligned axes make for difficult comparisons across plots  
    * Overplotting can be a meaningful concern  
  
Example code includes:  
```{r}

mtcars <- mtcars %>%
    mutate(fcyl=factor(cyl), fam=factor(am, levels=c(0, 1), labels=c("automatic", "manual")))
str(mtcars)


# Map x to mpg and y to fcyl
ggplot(mtcars, aes(x=mpg, y=fcyl)) +
    geom_point()

# Swap mpg and fcyl
ggplot(mtcars, aes(x=fcyl, y=mpg)) +
    geom_point()

# Map x to wt, y to mpg and color to fcyl
ggplot(mtcars, aes(x=wt, y=mpg, color=fcyl)) +
    geom_point()

ggplot(mtcars, aes(wt, mpg, color = fcyl)) +
    # Set the shape and size of the points
    geom_point(shape=1, size=4)


# Map color to fam
ggplot(mtcars, aes(wt, mpg, fill = fcyl, color=fam)) +
    geom_point(shape = 21, size = 4, alpha = 0.6)


# Base layer
plt_mpg_vs_wt <- ggplot(mtcars, aes(wt, mpg))

# Map fcyl to shape, not alpha
plt_mpg_vs_wt +
    geom_point(aes(shape = fcyl))

# Base layer
plt_mpg_vs_wt <- ggplot(mtcars, aes(wt, mpg))

# Use text layer and map fcyl to label
plt_mpg_vs_wt +
    geom_text(aes(label = fcyl))


# A hexadecimal color
my_blue <- "#4ABEFF"

# Change the color mapping to a fill mapping
ggplot(mtcars, aes(wt, mpg, fill = fcyl)) +
    # Set point size and shape
    geom_point(color=my_blue, size=10, shape=1)


ggplot(mtcars, aes(wt, mpg, color = fcyl)) +
    # Add point layer with alpha 0.5
    geom_point(alpha=0.5)

ggplot(mtcars, aes(wt, mpg, color = fcyl)) +
    # Add text layer with label rownames(mtcars) and color red
    geom_text(label=rownames(mtcars), color="red")

ggplot(mtcars, aes(wt, mpg, color = fcyl)) +
    # Add points layer with shape 24 and color yellow
    geom_point(shape=24, color="yellow")


# 5 aesthetics: add a mapping of size to hp / wt
ggplot(mtcars, aes(mpg, qsec, color = fcyl, shape = fam, size=hp/wt)) +
    geom_point()


ggplot(mtcars, aes(fcyl, fill = fam)) +
    geom_bar() +
    # Set the axis labels
    labs(x="Number of Cylinders", y="Count")

palette <- c(automatic = "#377EB8", manual = "#E41A1C")

ggplot(mtcars, aes(fcyl, fill = fam)) +
    geom_bar() +
    labs(x = "Number of Cylinders", y = "Count") +
    # Set the fill color scale
    scale_fill_manual("Transmission", values = palette)

palette <- c(automatic = "#377EB8", manual = "#E41A1C")

# Set the position
ggplot(mtcars, aes(fcyl, fill = fam)) +
    geom_bar(position="dodge") +
    labs(x = "Number of Cylinders", y = "Count") +
    scale_fill_manual("Transmission", values = palette)


ggplot(mtcars, aes(mpg, 0)) +
    geom_jitter() +
    # Set the y-axis limits
    ylim(c(-2, 2))

```
  
  
  
***
  
Chapter 3 - Geometries  
  
Scatter Plots:  
  
* Scatter plots require an x, y aesthetic  
	* Optional aestehtics/attributes include alpha, color, fill, shape, size, stroke  
    * Can include a second geom_point() with a new data= argument, and all other aesthetics will inherit  
    * Can use shape=1 (unfilled circle) to help with over-plotting; can also be used in combination with jitter, alpha, etc.  
* Means should generally be plotted with a measure of dispersion (spread)  
  
Histograms:  
  
* Histograms are a special form of bar plot that shows distributions  
	* geom_histogram(binwidth=, center=) defaults to 30 bins and can be overridden by setting the binwidth; setting center=0.5*binwidth ensure labels between bars rather than under bars  
* When multiple series are in the same histogram (e.g., with aes(fill=myCat)), the default is position="stack"  
	* Can use position="dodge" for offsetting (can be difficult to read)  
    * Can use position="fill" for the proportions by bin rather than counts by bin  
  
Bar Plots:  
  
* There are two geoms for creating bar/column charts  
	* geom_bar() counts the number of cases at each x position - stat="count"  
    * geom_col() plots actual values - stat="identity"  
* Example of creating column chart with specified y and errorbars  
	* ggplot(myDF, aes(x=sex, y=avgHt)) + geom_col() + geom_errorbar(aes(ymin=avgHt-sdHt, ymax=avgHt+sdHt), width=0.1)  
    * This type of plot is widely discouraged as a "Wile E Coyote" dynamite plot  
  
Line Plots:  
  
* The color aesthetic is often useful for having multiple series plotted on the same chart  
	* Can use geom_area() if the goal is to add the series with each sub-total plotted  
    * Can use fill= if the goal is to show the proportion of each series by time period  
    * Can use geom_ribbon(aes(ymin=0)) to have over-plotted, non-stacked, area charts  
  
Example code includes:  
```{r}

# Plot price vs. carat, colored by clarity
plt_price_vs_carat_by_clarity <- ggplot(diamonds, aes(carat, price, color = clarity))

# Set transparency to 0.5
plt_price_vs_carat_by_clarity + 
    geom_point(alpha = 0.5, shape = 16)


# Plot base
plt_mpg_vs_fcyl_by_fam <- ggplot(mtcars, aes(fcyl, mpg, color = fam))

# Default points are shown for comparison
plt_mpg_vs_fcyl_by_fam + 
    geom_point()

# Now jitter and dodge the point positions
plt_mpg_vs_fcyl_by_fam + 
    geom_point(position = position_jitterdodge(jitter.width=0.3, dodge.width=0.3))


data(iris)

ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +
    # Swap for jitter layer with width 0.1
    geom_jitter(width=0.1, alpha=0.5)

ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +
    # Set the position to jitter
    geom_point(position="jitter", alpha = 0.5)

ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +
    # Use a jitter position function with width 0.1
    geom_point(position=position_jitter(width=0.1), alpha = 0.5)


data(Vocab, package="carData")

# Examine the structure of Vocab
str(Vocab)

# Plot vocabulary vs. education
ggplot(Vocab, aes(x=education, y=vocabulary)) +
    # Add a point layer
    geom_point()

ggplot(Vocab, aes(education, vocabulary)) +
    # Set the shape to 1
    geom_jitter(alpha = 0.2, shape=1)


datacamp_light_blue <- "#51A8C9"

ggplot(mtcars, aes(x=mpg, y=..density..)) +
    # Set the fill color to datacamp_light_blue
    geom_histogram(binwidth = 1, fill=datacamp_light_blue)

ggplot(mtcars, aes(mpg, fill = fam)) +
    # Change the position to identity, with transparency 0.4
    geom_histogram(binwidth = 1, position = "fill")

ggplot(mtcars, aes(mpg, fill = fam)) +
    # Change the position to identity, with transparency 0.4
    geom_histogram(binwidth = 1, position = "identity", alpha=0.4)


# Plot fcyl, filled by fam
ggplot(mtcars, aes(x=fcyl, fill=fam)) +
    # Add a bar layer
    geom_bar()

ggplot(mtcars, aes(x=fcyl, fill = fam)) +
    # Set the position to "fill"
    geom_bar(position="fill")

ggplot(mtcars, aes(fcyl, fill = fam)) +
    # Change the position to "dodge"
    geom_bar(position = "dodge")


ggplot(mtcars, aes(cyl, fill = fam)) +
    # Change position to use the functional form, with width 0.2
    geom_bar(position = position_dodge(width=0.2))

ggplot(mtcars, aes(cyl, fill = fam)) +
    # Set the transparency to 0.6
    geom_bar(position = position_dodge(width = 0.2), alpha=0.6)


# Plot education, filled by vocabulary
ggplot(Vocab, aes(x=education, fill = factor(vocabulary))) +
    # Add a bar layer with position "fill"
    geom_bar(position="fill")

# Plot education, filled by vocabulary
ggplot(Vocab, aes(education, fill = factor(vocabulary))) +
    # Add a bar layer with position "fill"
    geom_bar(position = "fill") +
    # Add a brewer fill scale with default palette
    scale_fill_brewer()


data(economics)

# Print the head of economics
head(economics)

# Using economics, plot unemploy vs. date
ggplot(economics, aes(x=date, y=unemploy)) +
    # Make it a line plot
    geom_line()

# Change the y-axis to the proportion of the population that is unemployed
ggplot(economics, aes(x=date, y=unemploy/pop)) +
    geom_line()


load("./RInputFiles/fish.RData")

# Plot the Rainbow Salmon time series
ggplot(fish.species, aes(x = Year, y = Rainbow)) +
    geom_line()

# Plot the Pink Salmon time series
ggplot(fish.species, aes(x = Year, y = Pink)) +
    geom_line()

# Plot multiple time-series by grouping by species
ggplot(fish.tidy, aes(Year, Capture)) +
    geom_line(aes(group = Species))

# Plot multiple time-series by coloring by species
ggplot(fish.tidy, aes(x = Year, y = Capture, color = Species)) +
    geom_line()

```
  
  
  
***
  
Chapter 4 - Themes  
  
Themes from Scratch:  
  
* Themes are all the non-data ink in a chart, and include text, line, rectangle  
	* element_text(), element_line(), element_rect()  
* Can adjust theme elements using calls to theme() in ggplot2  
	* myPlot + theme(axis.title=element_text(color="blue"))  
* Hierarchical naming reflects inheritance rules, so making a change to a grandparent will carry down to its children and grandchildren  
* Can blank out everything using element_blank()  
	* myPlot + theme(line=element_blank(), rect=element_blank(), text=element_blank())  
  
Theme Flexibility:  
  
* Theme objects can be a valuable way to enforce consistency across plots  
	* myTheme <- theme(…)  
    * myPlot + myTheme  # will add myTheme as the last argument of myPlot  
    * myPlot + myTheme + theme(…)  # anything in theme(…) will override anything in myTheme  
* Can also use a number of built-in themes  
	* theme_classic()  
    * ggthemes::…  
    * theme_tufte()  
* Can take an existing theme and update it to a new theme  
	* myNewTheme <- theme_update(…)  
    * theme_set(myNewTheme)  # will apply the theme across all plots  
  
Effective Explanatory Plots:  
  
* Often, plots are designed for a lay audience, and are intended to convey a clear and/or dramatic message  
	* Intuitive color palettes  
    * Redundancy in use of color and scales and text labels  
    * Moving x/y labels, legends, titles, etc.  
    * Adding and labeling global averages  
  
Example code includes:  
```{r}

recess <- data.frame(begin=as.Date(c('1969-12-01', '1973-11-01', '1980-01-01', '1981-07-01', '1990-07-01', '2001-03-01', '2007-12-01')), 
                     end=as.Date(c('1970-11-01', '1975-03-01', '1980-07-01', '1982-11-01', '1991-03-01', '2001-11-01', '2009-07-30')), 
                     event=c('Fiscal & Monetary\ntightening', '1973 Oil crisis', 'Double dip I', 'Double dip II', 'Oil price shock', 'Dot-com bubble', 'Sub-prime\nmortgage crisis'), 
                     y=c(0.01416, 0.02067, 0.02951, 0.03419, 0.02767, 0.0216, 0.02521)
                     )
recess

events <- recess %>%
    select(begin, y) %>%
    rename(date=begin)
events


# Change the y-axis to the proportion of the population that is unemployed
plt_prop_unemployed_over_time <- ggplot(economics, aes(x=date, y=unemploy/pop)) +
    geom_line(lwd=1.25) + 
    labs(title="The percentage of unemployed Americans\nincreases sharply during recessions") + 
    geom_rect(data=recess, aes(xmin=begin, xmax=end, ymin=0.01, ymax=0.055, fill="red"),
              inherit.aes=FALSE, alpha=0.25
              ) +
    geom_label(data=recess, aes(x=begin, y=y, label=event))



# View the default plot
plt_prop_unemployed_over_time 

# Remove legend entirely
plt_prop_unemployed_over_time +
    theme(legend.position="none")

# Position the legend at the bottom of the plot
plt_prop_unemployed_over_time +
    theme(legend.position="bottom")

# Position the legend inside the plot at (0.6, 0.1)
plt_prop_unemployed_over_time +
    theme(legend.position=c(0.6, 0.1))


plt_prop_unemployed_over_time +
    theme(
        # For all rectangles, set the fill color to grey92
        rect = element_rect(fill = "grey92"),
        # For the legend key, turn off the outline
        legend.key = element_rect(color=NA)
  )

plt_prop_unemployed_over_time +
    theme(
        rect = element_rect(fill = "grey92"),
        legend.key = element_rect(color = NA),
        # Turn off axis ticks
        axis.ticks = element_blank(),
        # Turn off the panel grid
        panel.grid = element_blank()
  )

plt_prop_unemployed_over_time +
    theme(
        rect = element_rect(fill = "grey92"),
        legend.key = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        # Add major y-axis panel grid lines back
        panel.grid.major.y = element_line(
          # Set the color to white
          color="white",
          # Set the size to 0.5
          size=0.5,
          # Set the line type to dotted
          linetype="dotted"
        ), 
        # Set the axis text color to grey25
        axis.text = element_text(color="grey25"),
        # Set the plot title font face to italic and font size to 16
        plot.title = element_text(size=16, face="italic")
  )


plt_mpg_vs_wt_by_cyl <- ggplot(mtcars, aes(x=wt, y=mpg, color=fcyl)) + 
    geom_point() + 
    labs(x="Weight (1000s of lbs)", y="Miles per Gallon")

# View the original plot
plt_mpg_vs_wt_by_cyl

plt_mpg_vs_wt_by_cyl +
    theme(
        # Set the axis tick length to 2 lines
        axis.ticks.length=unit(2, "lines")
    )

plt_mpg_vs_wt_by_cyl +
    theme(
        # Set the legend key size to 3 centimeters
        legend.key.size = unit(3, "cm")
    )

plt_mpg_vs_wt_by_cyl +
  theme(
    # Set the legend margin to (20, 30, 40, 50) points
    legend.margin = margin(20, 30, 40, 50, "pt")
  )

plt_mpg_vs_wt_by_cyl +
    theme(
        # Set the plot margin to (10, 30, 50, 70) millimeters
        plot.margin=margin(10, 30, 50, 70, "mm")
    )

# Whitespace means all the non-visible margins and spacing in the plot.
# To set a single whitespace value, use unit(x, unit), where x is the amount and unit is the unit of measure.
# Borders require you to set 4 positions, so use margin(top, right, bottom, left, unit)
# To remember the margin order, think TRouBLe
# The default unit is "pt" (points), which scales well with text
# Other options include "cm", "in" (inches) and "lines" (of text)


# Add a black and white theme
plt_prop_unemployed_over_time +
    theme_bw()

# Add a classic theme
plt_prop_unemployed_over_time +
    theme_classic()

# Add a void theme
plt_prop_unemployed_over_time +
    theme_void()

# theme_gray() is the default.
# theme_bw() is useful when you use transparency.
# theme_classic() is more traditional.
# theme_void() removes everything but the data.


# Use the fivethirtyeight theme
plt_prop_unemployed_over_time +
    ggthemes::theme_fivethirtyeight()

# Use Tufte's theme
plt_prop_unemployed_over_time +
    ggthemes::theme_tufte()

# Use the Wall Street Journal theme
plt_prop_unemployed_over_time +
    ggthemes::theme_wsj()


theme_recession <- theme(
  rect = element_rect(fill = "grey92"),
  legend.key = element_rect(color = NA),
  axis.ticks = element_blank(),
  panel.grid = element_blank(),
  panel.grid.major.y = element_line(color = "white", size = 0.5, linetype = "dotted"),
  axis.text = element_text(color = "grey25"),
  plot.title = element_text(face = "italic", size = 16),
  legend.position = c(0.6, 0.1)
)
theme_tufte_recession <- ggthemes::theme_tufte() + theme_recession

themeOld <- theme_get()
theme_set(themeOld)

# Set theme_tufte_recession as the default theme
theme_set(theme_tufte_recession)


plt_prop_unemployed_over_time +
    # Add Tufte's theme
    ggthemes::theme_tufte()

# Draw the plot (without explicitly adding a theme)
plt_prop_unemployed_over_time

plt_prop_unemployed_over_time +
    ggthemes::theme_tufte() +
    # Add individual theme elements
    theme(
        # Turn off the legend
        legend.position = "none",
        # Turn off the axis ticks
        axis.ticks = element_blank()
    )

plt_prop_unemployed_over_time +
    ggthemes::theme_tufte() +
    theme(
        legend.position = "none",
        axis.ticks = element_blank(),
        axis.title = element_text(color = "grey60"),
        axis.text = element_text(color = "grey60"),
        # Set the panel gridlines major y values
        panel.grid.major.y = element_line(
            # Set the color to grey60
            color="grey60",
            # Set the size to 0.25
            size=0.25,
            # Set the linetype to dotted
            linetype="dotted"
        )
    )


theme_set(themeOld)


data(gapminder, package="gapminder")

ctry <- c('Swaziland', 'Mozambique', 'Zambia', 'Sierra Leone', 'Lesotho', 'Angola', 'Zimbabwe', 'Afghanistan', 'Central African Republic', 'Liberia', 'Canada', 'France', 'Israel', 'Sweden', 'Spain', 'Australia', 'Switzerland', 'Iceland', 'Hong Kong, China', 'Japan')

gm2007 <- gapminder %>%
    filter(year==2007, country %in% ctry) %>%
    select(country, lifeExp, continent) %>%
    arrange(lifeExp)
gm2007

# Set the color scale
palette <- RColorBrewer::brewer.pal(5, "RdYlBu")[-(2:4)]

# Add a title and caption
plt_country_vs_lifeExp <- ggplot(gm2007, aes(x = lifeExp, y = fct_reorder(country, lifeExp), color = lifeExp)) +
    geom_point(size = 4) +
    geom_segment(aes(xend = 30, yend = country), size = 2) +
    geom_text(aes(label = round(lifeExp,1)), color = "white", size = 1.5) +
    scale_x_continuous("", expand = c(0,0), limits = c(30,90), position = "top") +
    scale_color_gradientn(colors = palette) +
    labs(title="Highest and lowest life expectancies, 2007", caption="Source: gapminder")
plt_country_vs_lifeExp


# Define the theme
plt_country_vs_lifeExp +
    theme_classic() +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text = element_text(color="black"), axis.title = element_blank(), legend.position = "none")


global_mean <- gapminder %>% filter(year==2007) %>% pull(lifeExp) %>% mean()
x_start <- global_mean + 4
y_start <- 5.5
x_end <- global_mean
y_end <- 7.5


# Add text
plt_country_vs_lifeExp +
    theme_classic() +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text = element_text(color="black"), axis.title = element_blank(), legend.position = "none") +
    geom_vline(xintercept = global_mean, color = "grey40", linetype = 3) +
    annotate("text", x = x_start, y = y_start, label = "The\nglobal\naverage", vjust = 1, size = 3, color = "grey40")

# Add a curve
plt_country_vs_lifeExp +  
    theme_classic() +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text = element_text(color="black"), axis.title = element_blank(), legend.position = "none") +
    geom_vline(xintercept = global_mean, color = "grey40", linetype = 3) +
    annotate("text", x = x_start, y = y_start, label = "The\nglobal\naverage", vjust = 1, size = 3, color = "grey40") + 
    annotate("curve", x = x_start, y = y_start, xend = x_end, yend = y_end, arrow = arrow(length = unit(0.2, "cm"), type = "closed"), color = "grey40")


theme_set(themeOld)

```
  
  
  
***
  
### _Preparing for Machine Learning Interview Questions in R_  
  
Chapter 1 - Data Pre-processing and Visualization  
  
Data Normalization:  
  
* Data preprocessing includes normalization, visualization, and detecting/managing outliers  
* Data normalization (feature scaling) is an important step in the ML pipeline  
	* Not always needed, but frequently valuable (and rarely causes harm)  
    * Min-max scaling scales every feature to a (0, 1) scale where min is 0 and max is 1  
    * Standardization (z-score normalization) scales features to N(0, 1) by using mu, sd  
  
Handling Missing Data:  
  
* What to do with missing values is an important topic - general options include ignore/delete (often suboptimal), impute, accept (use methods that handle naturally - uncommon)  
* The naniar package is valuable for analyzing the amount of missing data in a frame  
	* miss_var_summary(myDF)  
    * any_na(myDF)  
    * myDF <- replace_with_na_all(myDF, ~.x=="?")  # replace all '?' with NA  
    * vis_miss(myDF, cluster=TRUE)  # visualize missing values, clustered by columns with similar missingness  
    * gg_miss_case(myDF)  # show the missingness by row  
    * add_label_shadow(myDF)  # create columns with _NA suffixes reporting the boolean for whether the data are missing  
* Three types of missing data - MCAR, MAR, MNAR  
	* MCAR - can impute, will not bias findings, random patterns of missingness in meaningless clusters  
    * MAR - can impute, may bias findings, well-defined missingness clusters at least on some dimensions  
    * MNAR - should not impute  
* May want to assess the quality of a missing value imputation process  
	* How do the variable distributions change (ideally, not much)?  
* Can us the simputation library for imputation  
	* imp_lm <- … %>% simputation::impute_lm(Y ~ X1 + X2)  
  
Detecting Anomalies in Data:  
  
* Outlier detection for univariate data has two common heuristics - 3-sigma, or 1.5*IQR (Q1 - 1.5*IQR or Q3 + 1.5*IQR)  
* Outlier detection for multivariate data has four common heuristics - two are kNN and LOF (local outlier factor)  
	* Assumptions are the outliers lie far from their neighbors  
    * kNN is the average distance of the k-nearest neighbors while LOF is the number of neighbors inside a pre-defined radius  
    * fnn::get.knn(myDF)  
    * LOF ~ 1 means similar density as neighbors, LOF < 1 means higher density than neighbors (inlier), LOF > 1 means lower density than neighbors (outlier)  
* There are several approaches for managing outliers  
	* Retention - use algorithms that are robust to outliers  
    * Imputation - impute to some form of less extreme data (mode imputation, kNN imputation, linear imputation, etc.)  
    * Capping - replace with 5th percentile and/or 95th percentile  
    * Exclusion - generally not recommended, especially in small datasets or datasets that are non-normal  
  
Example code includes:  
```{r}

# fifa_sample <- read_csv("./RInputFiles/fifa_sample.xls")
# glimpse(fifa_sample)

apps <- read_csv("./RInputFiles/googleplaystore.xls")
apps <- apps[-10473, ]
glimpse(apps)

cars <- read_csv("./RInputFiles/car-fuel-consumption-1.xls")
glimpse(cars)

fifa_sample <- tibble::tibble(SP=c(43, 70, 47, 22, 74, 45, 65, 71, 66, 62, 58, 55, 57, 15, 67, 66, 46, 65, 71, 80, 68, 62, 49, 70, 55, 17, 56, 48, 25, 62, 14, 55, 17, 43, 62, 63, 52, 62, 58, 81, 73, 59, 60, 66, 43, 59, 58, 79, 16, 64, 14, 12, 68, 78, 36, 52, 59, 67, 75, 80, 38, 73, 56, 80, 66, 68, 72, 41, 72, 51, 66, 37, 75, 19, 15, 34, 69, 86, 74, 57, 80, 51, 76, 63, 22, 76, 43, 22, 46, 39, 55, 81, 77, 62, 81, 19, 70, 74, 60, 59), 
                              RA=c(190, 12, 353, 669, 2.5, 2.6, 406, 18.6, 5.1, 653, 900, 450, 3.9, 713, 1.9, 4.8, 140, 1.6, 1.3, 38.1, 1.6, 953, 891, 2.2, 357, 149, 3.4, 1.7, 7, 347, 105, 2.4, 1.9, 73, 4.8, 801, 3.8, 1.2, 9.5, 6.8, 2.5, 656, 1.5, 7, 631, 1.9, 125, 6.4, 2.6, 648, 1.3, 1.3, 6.7, 20.8, 3.6, 305, 1, 357, 7.5, 17.1, 140, 1.4, 3, 10.3, 795, 6.5, 2.6, 530, 2.7, 495, 12.8, 850, 1.2, 436, 639, 945, 619, 164, 10.2, 639, 5, 365, 1.2, 350, 63, 11.7, 8.7, 534, 2.5, 413, 225, 15.2, 1.6, 534, 14.7, 119, 6.9, 20, 1.5, 512)
                              )


# Glimpse at the dataset
glimpse(fifa_sample)

# Compute the scale of every feature
(fifa_scales <- sapply(fifa_sample, range))

# Plot fifa_sample data
ggplot(fifa_sample, aes(x=SP, y=RA)) + 
    geom_point(colour="blue", size=5) + 
    labs(title = "Original data", x="Shot power", y="Release amount (millions EUR)") +
    theme(plot.title = element_text(size=22), text = element_text(size=18)) +
    scale_x_continuous(breaks = round(seq(0, max(fifa_sample$SP), by = 5),1)) 


# Apply max-min and standardization: fifa_normalized
fifa_normalized <- fifa_sample %>% 
    mutate(SP_MaxMin = (SP-min(SP))/(max(SP)-min(SP)), RA_MaxMin = (RA-min(RA))/(max(RA)-min(RA)), 
           SP_ZScore = (SP - mean(SP)) / sd(SP), RA_ZScore = (RA - mean(RA)) / sd(RA)
           )

# Compute the scale of every feature: fifa_normalized_scales
(fifa_normalized_scales <- sapply(fifa_normalized, range))

# Boxplot of original and normalized distributions
boxplot(fifa_normalized[, c("SP", "RA")], main = 'Original')
boxplot(fifa_normalized[, c("SP_MaxMin", "RA_MaxMin")], main = 'Max-Min')
boxplot(fifa_normalized[, c("SP_ZScore", "RA_ZScore")], main = 'Z-Score')


bands <- tibble::tibble(Blade_pressure=c('20', '20', '30', '30', '30', '28', '30', '28', '60', '32', '30', '40', '30', '25', '20', '?', '?', '?', '?', '?', '30', '30', '25', '30', '25', '20', '30', '25', '30', '35', '28', '30', '22', '20', '35', '?', '30', '28', '31', '34', '32', '?', '30', '30', '24', '20', '35', '25', '25', '34', '16', '20', '28', '25', '30', '35', '46', '50', '25', '30'), 
                        Roughness=c('0.75', '0.75', '?', '0.312', '0.75', '0.438', '0.75', '0.75', '0.75', '1.0', '0.75', '0.75', '1.0', '0.625', '1.0', '1.0', '?', '?', '0.75', '0.75', '0.812', '0.812', '0.812', '1.0', '1.0', '1.0', '1.0', '1.0', '0.75', '0.75', '0.75', '0.75', '0.625', '0.625', '0.75', '0.875', '0.625', '1.0', '1.0', '0.75', '1.0', '0.875', '0.875', '0.812', '0.75', '0.75', '0.812', '0.625', '0.625', '0.5', '0.75', '0.75', '0.75', '0.875', '0.625', '?', '0.75', '0.75', '0.625', '0.875'), 
                        Ink_pct=c('50.5', '54.9', '53.8', '55.6', '57.5', '53.8', '62.5', '62.5', '60.2', '45.5', '48.5', '52.6', '50.0', '59.5', '49.5', '62.5', '62.5', '58.8', '54.9', '56.2', '58.8', '62.5', '58.1', '62.5', '57.5', '57.5', '57.5', '58.8', '58.8', '58.8', '45.0', '43.5', '54.3', '53.2', '58.8', '63.0', '58.1', '58.8', '54.3', '62.5', '58.1', '61.7', '55.6', '55.6', '58.1', '56.2', '58.8', '57.5', '58.8', '61.0', '50.5', '50.5', '58.8', '58.8', '62.5', '55.6', '58.8', '62.5', '52.6', '54.9'), 
                        Ink_temperature=c('17.0', '15.0', '16.0', '16.0', '17.0', '16.8', '16.5', '16.5', '12.0', '16.0', '16.0', '14.0', '15.0', '14.5', '16.0', '15.0', '14.0', '15.5', '16.4', '16.5', '16.0', '15.0', '16.3', '15.8', '14.5', '14.0', '15.0', '15.2', '15.0', '17.0', '16.0', '16.5', '14.1', '14.0', '17.0', '15.4', '15.0', '16.0', '15.0', '15.0', '16.0', '15.4', '16.0', '16.3', '15.8', '16.6', '17.0', '13.0', '14.0', '15.9', '17.0', '16.5', '15.0', '16.5', '18.0', '17.0', '12.0', '16.0', '14.6', '24.5'),
                        Band_type=c('band', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'band', 'noband', 'noband', 'band', 'band', 'band', 'noband', 'band', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'band', 'band', 'band', 'noband', 'noband', 'band', 'band', 'noband', 'noband', 'noband', 'noband', 'band', 'noband', 'band', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'band', 'band', 'noband', 'noband', 'noband', 'noband', 'noband', 'noband', 'band', 'band')
                        )
str(bands)


# Check for missing values using base R and naniar functions
any(is.na(bands))
naniar::any_na(bands)

# What? No missing values! Take a closer glimpse
glimpse(bands)

# Replace ? with NAs: bands
bands <- naniar::replace_with_na_all(bands, ~.x == '?')

# Compute missingness summaries
naniar::miss_var_summary(bands)


# Visualize overall missingness
naniar::vis_miss(bands)

# Visualize overall missingness, clustered
naniar::vis_miss(bands, cluster = TRUE)

# Visualize missingness in each variable
naniar::gg_miss_var(bands)

# Missingness in variables, faceted by Band_type
naniar::gg_miss_var(bands, facet = Band_type)

# Visualize missingness in cases
naniar::gg_miss_case(bands)


# Impute with the mean
imp_mean <- bands %>%
    naniar::bind_shadow(only_miss = TRUE) %>%
    naniar::add_label_shadow() %>%
    naniar::impute_mean_all()

# Impute with lm
imp_lm <- bands %>%
    naniar::bind_shadow(only_miss = TRUE) %>%
    naniar::add_label_shadow() %>%
    simputation::impute_lm(Blade_pressure ~ Ink_temperature) %>%
    simputation::impute_lm(Roughness ~ Ink_temperature) %>%
    simputation::impute_lm(Ink_pct ~ Ink_temperature)


# Peek at the first few rows of imp_models_long
# head(imp_models_long)

# Visualize post-imputation distributions
# ggplot(imp_models_long, aes(x = imp_model, y = value)) + 
#     geom_violin(aes(fill=imp_model)) +
#     facet_wrap(~variable, scales='free_y')

# Calculate post-imputation distribution stats
# imp_models_long %>% 
#     group_by(imp_model, variable) %>% 
#     summarize(var = var(value), avg = mean(value), 
#     median = median(value)) %>% 
#     arrange(variable)


# Peek at the cars dataset
head(cars)

# Boxplot of consume variable distribution
boxplot(cars$consume)

# Five-number summary: consume_quantiles
(consume_quantiles <- quantile(cars$consume))

# Calculate upper threshold: upper_th
upper_th <- consume_quantiles["75%"] + 1.5 * (consume_quantiles["75%"] - consume_quantiles["25%"])

# Print the sorted vector of distinct potential outliers
sort(unique(cars$consume[cars$consume > upper_th]))


# Scale data and create scatterplot: cars_scaled
cars_scaled <- cars %>%
    select(distance, consume) %>%
    scale() %>%
    as.data.frame()
plot(distance ~ consume, data = cars_scaled, main = 'Fuel consumption vs. distance')

# Compute KNN score
cars_knn <- FNN::get.knn(data = cars_scaled, k = 7)
cars$knn_score <- rowMeans(cars_knn$nn.dist)

# Print top 5 KNN scores and data point indices: top5_knn
(top5_knn <- order(cars$knn_score, decreasing = TRUE)[1:5])
print(cars$knn_score[top5_knn])

# Plot variables using KNN score as size of points
plot(distance ~ consume, cex = knn_score, data = cars,  pch = 20)


# Scale cars data: cars_scaled
cars_scaled <- cars %>%
    select(distance, consume, knn_score) %>%
    scale() %>%
    as.data.frame()

# Add lof_score column to cars
cars$lof_score <- dbscan::lof(cars_scaled, k = 7)

# Print top 5 LOF scores and data point indices: top5_lof
(top5_lof <- order(cars$lof_score, decreasing = TRUE)[1:5])
print(cars$lof_score[top5_lof])

# Plot variables using LOF score as size of points
plot(distance ~ consume, cex = lof_score, data = cars, pch = 20)

```
  
  
  
***
  
Chapter 2 - Supervised Learning  
  
Interpretable Models:  
  
* Intepretability is the degree to which a human can understand the reason that a model has made a prediction  
	* The explanation is the answer to a "why" question  
    * Satisfies human learning and curiosity, improves safety and security, enables bias detection (ensures legal/regulatory compliance), increasing trust and social acceptance  
* Several ML algorithms produce interpretable models - linear regression, logistic regression, decision trees, decision rules, naïve Bayes, k-nearest neighbors (kNN)  
	* In interpreting a regression, the coefficient should be explained using "all else equal"  
  
Regularization:  
  
* Regularization attempts to constrain (shrink) the regression coefficients closer to 0, minimizing over-fitting  
	* L1 - lasso (least absolute shrinkage and selection operator, penalized by sum of absolute value of coefficients  times lambda)  
    * L2 - ridge (penalizes by sum-squared of coefficients times lambda - note that lambda=0 is standard regression)  
    * Elastic net - mix of L1/L2 (lambda1=0 is ridge, lambda2=0 is lasso, both=0 is OLS)  
* Regularization minimizes over-fits and improves generalization  
	* Requires tuning of the lambda coefficient  
    * L1 (lasso) drives some/many coefficients to zero, which has a feature-selection component that makes for easier interpretation and communication  
  
Bias and Variance:  
  
* Bias and variance are two sources of error in machine learning  
	* Bias is error from incorrect model assumptions (under-fitting)  
    * Variance is error from over-sensitivity to small differences in the training data (over-fitting)  
* To reduce model bias, add terms (complexity) or engage in feature engineering or reduce regularization  
* To reduce variance, add more training data (simplest and most robus solution), add regularization, perform feature selection, decrease model size (fewer terms or fewer neurons or etc.)  
  
Building Ensemble Models:  
  
* Three common strategies for ensembling are bagging, boosting, and stacking  
* Bagging is bootstrap aggregation - the base learner is trained on each bootstrap, then the learners are combined to make a prediction (e.g., random forest)  
* Boosting constructs base learners in an incremental manner - the better the learner performs, the higher its weight (adaboost, gbm)  
	* Training samples also receive weights, with worse predictions leading to a higher weight  
    * Subsequent learners help learn from the mistakes of previous learners  
* Stacking finds linear combinations of underlying learners (built on the learners not on the raw data)  
* Need to select base learners for an ensemble  
	* Diversity of base learners can be beneficial - different families, different samples of data/attributes  
    * Low correlation in predictions of the base learners  
    * Base learners that are robust to outliers and missing data  
    * Base learners that run in a reasonable amount of time  
* Can use the caretEnsemble library for building ensemble models  
	* caretList() will build a list of train() models for ensembling  
    * caretEnsemble() ensembles the caretList() using linear regression  
    * caretStack() ensembles using a meta-learner  
    * The caret:resamples() function calculates performance metrics across all learners  
    * modelCor() gives the correlation among the base learners' predictions  
  
Example code includes:  
```{r}

car <- cars %>%
    select(distance, consume, speed, temp_outside, gas_type, AC) %>%
    mutate(gas_type=factor(gas_type), AC=factor(AC, labels=c("Off", "On")))
test_instance <- tibble::tibble(distance=12.4, consume=5.1, speed=45, temp_outside=5, 
                                gas_type=factor("E10", levels=c("E10", "SP98")), 
                                AC=factor("Off", levels=c("Off", "On"))
                                )
test_instance


# Glimpse on the car dataset
glimpse(car)

# Build a multivariate regression model: car_lr
car_lr <- lm(consume ~ ., data = car)

# Summarize the model and display its coefficients
summary(car_lr)$coef

# Predict with linear regression model
predict(car_lr, test_instance)


# Build a regression tree: car_dt
car_dt <- rpart::rpart(consume ~ ., data = car)

# Fancy tree plot
rattle::fancyRpartPlot(car_dt)

# Extract rules from the tree
rpart.plot::rpart.rules(car_dt)

# Predict test instance with decision tree
predict(car_dt, test_instance)


fifaRaw <- c(0.569, -1.555, -0.068, -0.705, -1.342, -0.28, -0.068, -1.98, -0.28, 0.357, -1.767, 0.357, -0.28, -0.918, -0.068, -0.068, 2.693, -0.918, 0.357, -0.068, -0.918, 0.144, -0.493, -1.767, -0.28, 1.419, 0.357, -0.28, 1.844, -0.493, -1.342, -1.342, -0.28, -1.13, 1.207, 2.269, -0.28, 1.419, 0.357, -0.068, -0.918, -0.918, 0.569, -0.28, 0.144, -0.28, -0.705, -0.28, 2.693, -1.342, 0.782, 0.357, -1.555, 0.994, 1.207, 0.994, 0.357, -0.918, -0.28, -0.493, -0.705, 0.144, -0.068, 1.207, -1.13, -0.918, -0.918, -0.068, -1.555, -0.068, 1.207, 0.357, -1.342, 0.569, -0.493, 1.631, -0.918, 0.994, 1.207, -0.068, 0.357, -0.705, 0.782, -0.068, 2.481, 1.631, -0.918, -0.918, -1.767, 1.631, 0.782, -0.28, 0.569, -0.705, 0.144, 2.481, -1.342, 0.782, 0.144, 0.144, -1.342, -0.493, -0.918, -1.555, 0.357, -1.555, -0.493, -0.493, -0.493, 0.569, -1.342, -0.28, 1.207, -1.555, -1.342, -0.068, 1.419, -1.555, 0.569, -0.493, 1.207, 0.144, -0.705, -0.493, 2.056, -1.342, 1.207, -1.13, 0.782, -0.068, -0.493, -1.13, -0.068, -0.493, -0.068, 0.994, -1.13, 0.357, -1.13, -0.068, -1.13, -0.705, -0.068, -0.28, -0.705, 0.782, 1.207, 1.631, 0.994, -1.13, -1.13, 0.994, 0.569, 0.994, 0.782, 0.144, 0.144, 0.144, -1.13, -0.068, -0.705, 0.144, 0.357, 0.782, 0.782, -0.705, -0.28, -0.705, 1.631, 2.056, 1.631, -0.068, -1.13, 0.144, -0.28, 1.207, -0.493, 1.419, -1.555, 0.782, -1.13, -1.13, -1.13, -0.068, 0.782, -0.918, 0.144, 0.569, -0.068, -0.493, 0.994, -0.918, -1.342, -0.493, -0.918, 0.569, -0.28, -0.705, 0.144, -1.342, 0.994, -1.555, 1.207, -0.28, 2.056, -1.13, -0.918, 1.631, 0.357, -0.705, -0.493, 2.056, -0.493, 1.844, 0.782, -0.068, 0.144, -0.705, -0.918, -0.28, 0.782, -1.555, -0.918, 2.056, 0.994, -0.493, -0.493, 0.144, -1.342, -1.342, 0.782, 0.994, 0.144, -0.28, 0.144, -0.493, -0.493, -0.918, 0.782, -1.342, 0.144, 0.144, -0.28, 1.419, -0.068, -0.068, 0.782, -1.767, -0.918, -0.068, -0.493, 0.357, -1.13, -0.28, 0.357, -1.13, -0.28, 0.994, -1.555, 1.207, -0.28, -0.705, -1.13, 0.144, -0.918, -0.705, 0.994, 0.357, 0.357, -0.705, -0.493, -0.493, 0.357, -0.28, -0.493, 0.569, -1.342, -0.068, 0.569, 2.269, -0.493, 1.419, 1.631, -1.13, 0.144, 2.056, -0.068, 0.994, 1.631, -0.068, 1.419, 2.481, 1.207, -0.28, 2.056, -1.555, -0.918, 0.782, 0.782, -0.493, 1.631, -0.28, -1.342, -0.918, -1.555, -0.068, 0.569, 1.207, -0.068, 0.569, -0.705, -0.28, -1.342, 1.631, 0.357, -0.068, -1.555, 0.357, 0.782, -0.705, -1.13, -0.493, -0.28, -0.918, 0.994, -0.493, -1.342, 0.357, 0.782, -0.918, -0.28, 1.419, 0.144, -1.13, -0.28, -0.28, 0.569, -1.342, -0.918, -1.342, 1.844, -0.068, -0.28, -0.068, 0.144, -0.28, -1.767, -0.28, -0.705, 0.144, -1.13, -1.13, -0.705, 0.569, -0.068, -1.342, 0.357, 0.144, 2.056, -0.705, 1.631, 0.782, -1.342, 1.419, -0.28, -0.28, 0.144, 1.419, 1.631, 0.569, -0.705, 2.056, -1.767, -0.918, -0.28, -1.555, -0.068, -0.068, 0.144, -1.555, -0.493, -0.068, -0.068, -0.705, -0.28, -1.13, -0.068, 1.419, 2.056, -0.493, -0.918, -0.705, -0.918, 1.419, -0.493, -0.28, 0.144, -0.493, -0.918, -1.342, -0.28, 0.357, -1.13, 1.631, 0.782, 0.357, 0.994, 0.782, -0.068, -0.918, -0.28, 1.844, -0.28, 0.782, 0.357, -1.13, -1.13, 0.569, 0.569, -0.068, 0.782, 0.357, 0.782, 0.144, 1.844, 1.207, 0.144, 0.357, 0.357, -1.13, -1.767, -0.068, 2.056, -1.342, 1.631, -0.068, 1.631, -0.068, 0.357, 1.419, 0.782, 0.569, -0.918, 0.569, -0.918, 1.419, -0.28, 0.569, 0.994, 0.357, 0.782, 0.357, 1.207, 0.782, -0.918, -0.493, -0.28, -1.98, 0.994, -1.342, -1.342, 0.357, 0.144, -0.493, -0.068, -1.342, -0.705, -0.918, 0.357, -1.555, 0.357, 1.419, 0.357, -0.068, 1.419, 1.631, 1.419, 1.419, 0.144, -0.493, 0.569, 1.844, 0.569, -1.13, -0.28, 1.631, 1.844, 1.207, -0.705, -1.555, -1.342, -0.705, -0.705, 2.056, 1.419, -0.918, -0.493, 0.994, -0.705, 0.782, -0.198, 1.373, -0.023, -0.198, -0.023, 2.246, -0.198, 0.152, -0.547, -0.547, 0.675, -0.721, -0.896, 0.501, 1.199, -1.943, -0.023, 2.595, -0.372, -2.467, -0.198, -1.245, -0.372, -0.023, -0.023)
fifaRaw <- c(fifaRaw, -1.419, -0.896, -0.372, -0.198, 1.548, -1.245, -0.198, -0.547, 1.024, 1.548, -0.721, -0.721, 0.152, -0.198, 0.501, 1.722, 0.85, -0.547, -0.198, 0.326, 1.024, 0.85, 0.326, -0.198, -0.023, 1.199, -0.721, 1.548, 0.501, -1.07, 0.152, 0.675, -0.198, -1.594, 0.501, -0.198, 0.85, -0.198, -0.198, -0.023, 1.024, -0.023, -1.07, 0.85, -0.547, -1.594, -1.07, 0.675, 3.293, 0.326, -0.023, 1.373, -1.245, -0.372, 1.373, 0.326, 0.501, -0.547, -0.896, 0.675, -0.721, -1.245, -0.547, 0.85, 0.326, 0.501, -0.372, -0.372, 0.675, -0.372, 0.152, 0.501, 0.675, -0.547, -0.896, 0.85, 0.85, 1.199, -0.547, -0.547, 0.501, -1.07, -0.023, 0.326, 0.326, 0.501, 0.501, 1.024, 0.152, 0.675, 1.024, -1.07, -0.198, -1.07, 0.501, -1.245, -0.198, 1.024, 1.548, -0.547, -1.07, 0.326, 1.722, 0.501, -0.721, -0.547, -0.198, -0.547, -0.896, -0.198, -1.245, -0.198, -1.594, -1.768, -0.023, 1.897, 0.152, 2.421, 1.199, -1.07, -0.023, -0.023, 0.85, -0.023, 0.675, -0.198, -0.896, -0.547, -0.023, 0.675, -0.372, -0.896, 0.326, 1.024, 0.85, 0.501, -0.198, -1.245, -0.547, -0.372, 0.85, -0.896, 2.071, -0.547, -0.198, -1.245, 0.326, 0.326, -0.721, -0.198, -1.245, 0.152, -0.896, 0.501, -1.07, -0.372, -2.292, 0.326, 0.501, -1.07, 1.199, 0.152, -1.07, -1.943, -1.245, -0.198, 1.199, -1.768, 0.326, -1.245, -0.721, 0.326, 0.326, 0.85, 1.548, -0.023, -1.245, -1.07, -0.023, -0.547, 0.326, 1.024, -0.198, 0.326, 1.199, 0.85, -0.023, 1.199, -1.419, 1.199, -0.896, -0.023, 0.501, 0.152, -2.118, -0.198, -0.023, -0.023, -1.419, 1.024, 0.85, 0.501, -1.07, 0.85, -0.198, -1.245, -0.721, -0.721, 1.897, 0.326, 0.152, 1.024, -0.198, 0.326, 0.501, -0.198, -1.07, 1.199, -1.245, 0.675, -0.721, -0.547, 0.152, -1.07, -1.245, 0.85, -0.023, 1.373, 1.024, -0.547, -0.372, 1.024, -0.547, -0.372, -2.118, -1.768, -1.07, 1.024, -2.816, 1.024, 0.501, -0.372, -0.372, 1.897, 0.152, -0.721, 2.944, 1.024, 1.199, -0.721, -1.594, -0.896, -0.372, 0.85, -1.245, -0.372, 0.501, -0.547, 2.071, 1.548, -0.547, 1.548, -0.023, 1.024, -0.721, -0.198, 0.675, 0.85, -1.245, -1.594, 0.85, -1.419, 0.675, 0.675, -0.721, -1.768, 3.293, 0.326, 0.326, 1.024, -0.547, 0.326, 2.246, -0.198, -0.547, 1.373, 0.152, 1.199, 1.024, 0.152, -1.419, -0.372, -0.023, -0.372, -0.372, -0.023, 1.373, 1.024, 0.326, 0.675, 2.246, -0.547, 1.548, -0.372, -0.547, -0.023, -0.198, -1.07, 0.326)
fifaRaw <- c(fifaRaw, 0.675, 0.152, -1.419, 1.373, -0.023, 0.152, -0.547, 1.373, -0.023, 0.501, -0.198, -0.023, 0.501, 0.152, 1.199, -1.07, 2.421, -1.07, 1.024, 0.326, -0.198, -1.245, -1.245, -0.198, -0.372, -2.467, 0.501, 0.152, -0.023, -0.198, 0.326, -0.896, 0.501, 1.897, -0.547, -0.721, 0.501, -0.372, 1.373, 0.675, 0.326, 0.152, -1.07, 0.675, -0.198, -0.198, 0.152, -0.721, -0.198, -0.372, -0.023, -0.896, 0.675, -0.198, 0.675, 1.373, -1.594, 1.373, 0.326, -1.768, 1.722, 1.024, 0.326, 1.548, 1.722, -1.245, -2.292, 0.675, -0.547, -1.07, -0.547, -0.721, -1.419, 0.85, 0.501, 0.501, 0.501, -1.943, 1.199, -0.896, 0.152, 2.071, 0.152, -0.372, -0.721, 2.071, -0.896, -1.245, -1.07, -0.198, -2.292, -1.245, -0.023, -0.547, 0.501, -0.547, 0.85, -0.372, -0.372, -0.547, -0.023, -0.547, 0.675, -0.023, 0.85, 0.326, -0.198, -1.07, 0.152, 0.326, -0.372, -1.419, -1.245, 0.501, -0.372, -1.07, -0.896, -0.198, -0.023, 1.199, 1.373, -0.896, -0.896, 0.675, 0.326, -1.594, -0.372, -0.721, -0.547, 1.722, -0.547, -1.245, 1.722, 0.501, 0.326, 1.548, 0.675, -1.768, -0.372, -1.245, -1.245, -0.721, -1.245, 3.468, 0.675, -0.023, 0.152, -1.768, -0.198, -0.198, -1.594, 0.675, 0.501, -1.245, 0.85, 1.548, -0.023, 1.897, 0.675, -1.419, 0.85, -1.419, 1.199, 1.199, -0.372, -1.594, 0.618, 0.095, 0.409, -1.841, 0.042, -1.789, -0.586, -0.167, 0.775, 0.566, -0.167, 0.775, 0.461, 0.147, 1.351, -1.056, 1.508, 0.827, 0.775, 0.513, -1.475, 0.461, -1.789, -0.115, 0.775, -1.894, 0.88, 0.88, 1.194, 0.461, -0.69, 0.356, 0.461, 0.513, -1.632, 0.618, 0.566, 0.566, -0.586, 0.513, -1.004, 0.461, -1.737, 1.141, 1.194, 0.984, 0.042, 0.409, -1.946, -1.318, 1.455, 0.618, -0.01, -1.841, -0.481, -1.841, 1.351, -1.423, -1.946, 0.723, 0.461, 0.618, 0.513, 0.775, 0.304, 0.147, 0.618, -1.004, 0.827, 0.775, -0.219, 0.618, 0.67, 2.031, 0.042, -2.051, 0.409, 1.351, 0.618, -0.376, -0.324, -1.109, 1.037, -1.475, -1.998, 0.513, -0.272, 0.252, -0.952, 0.095)
fifaRaw <- c(fifaRaw, -0.062, 0.199, 0.775, -0.01, 0.984, 0.409, -0.899, 1.194, 0.67, -0.324, -0.743, -0.115, 0.827, -1.318, 1.089, -1.423, 0.252, 0.566, -0.847, 1.665, -1.894, -1.318, 1.037, -1.998, -0.952, 1.298, 0.67, 0.147, 0.042, -0.272, 0.88, 1.246, -1.946, 1.141, -1.946, -1.318, -0.272, 1.298, 0.042, -1.58, 0.566, -1.109, 0.356, 0.513, 0.095, 0.147, 0.566, -1.527, -0.062, -1.266, 0.67, -0.167, 1.56, 0.67, 0.304, 0.984, 1.194, -1.946, 1.141, 0.199, -0.272, 0.932, 0.409, -0.167, 1.037, 0.827, 0.67, 0.984, -0.01, -1.37, 0.67, 0.566, 0.723, 0.67, -2.051, 0.67, -0.899, -1.841, -1.737, -1.946, 0.566, 0.618, 0.409, -1.737, 0.252, 0.409, 0.095, 0.566, -0.429, 0.252, 0.461, -2.051, 0.252, 0.67, 0.775, -0.586, 0.566, 0.199, -0.01, 0.304, -0.376, -0.167, -1.894, -1.841, -1.056, 1.037, 0.199, 0.513, 0.618, -2.051, -1.894, -0.115, 0.932, 0.984, -0.272, 0.042, -0.062, 1.351, 1.351, 1.612, 0.827, -0.795, 1.089, 0.723, 1.298, 0.566, 1.141, -1.056, 0.304, 0.095, 0.88, -0.69, 0.356, 0.775, 1.403, -0.743, 0.566, 0.67, 0.042, -1.894, 0.513, 0.618, -0.219, 0.461, -1.841, 0.775, 0.827, -2.051, 0.147, 0.409, -1.789, 0.252, 0.827, 0.199, 0.409, 0.356, -0.952, -1.109, -0.586, 0.618, 0.984, -0.115, -0.69, 0.513, -0.899, 0.461, -0.638, 0.932, -1.946, -0.167, 0.304, -0.899, -1.998, 0.461, -0.219, 0.618, 0.304, 1.141, 0.618, 0.984, -1.056, -0.115, 0.409, -2.051, 0.409, 0.304, -0.533, -0.219, -0.376, -1.998, 0.356, 0.513, -2.155, -0.533, 0.042, 1.194, 1.246, 0.932, 0.827, -1.789, 0.775, 0.618, 1.455, -0.219, -1.841, -1.998, 0.199, 0.513, 0.095, 0.461, -1.632, 1.508, 0.461, -1.998, 1.141, -1.37, 1.298, 1.978, 0.723, 0.984, 0.723, 0.042, -0.795, 1.508, -0.324, 0.409, -1.004, 0.984, 1.194, -0.481, 0.775, -0.481, -1.998, 0.932, 0.356, 1.455, -0.533, 0.095, -0.324, -1.423, -0.533, -1.737, 0.461, 0.147, -1.998, 0.775, 0.566, -1.109, -0.01, -1.946, 0.042, -0.952, 0.618, 1.351, -0.69, 0.304, -1.056, -0.167, 1.403, 0.513, 1.298, -0.638, 0.304, 1.298, -1.737, -0.69, 1.037, -1.161, -1.789, -0.899, 1.403, -0.272, 0.252, 0.356, 0.88, -0.272, 0.827, 0.147, 0.461, -0.795, 0.932, -1.841, -0.69, -1.946, 0.618, 0.199, 0.775, 0.67, 0.775, -0.481, -0.01, -1.004, 0.409, -0.272, 0.199, -0.062, -1.841, 1.717, -1.266, 0.566, 0.304, 0.042, 0.095, -2.208, 0.566, 0.199, 0.827, 0.618, 0.513, -0.272, -1.58, 0.67, 0.304, -0.115, 1.298, 0.618, 0.461, 1.298, -1.737, 0.513, 1.141, 0.67, 0.88, 0.356, -0.69, 0.147, -1.004, 0.827, 0.461, 1.926, 0.461, 0.67, 1.194, -0.69, -1.841, 0.775, 0.775, 0.88, 1.141, 0.042, -1.894, -0.743, -1.998, -0.376, 0.67, -0.743, 0.827, -1.004, 1.194, -0.01, 1.141, 1.037, 0.304, -0.167, -0.847, 0.513, 0.461, 1.194, -1.894, 0.304, 0.042, 0.984, -0.219, -1.946, -1.632, -1.266, 0.775, 0.147, 0.618, -0.638, -1.161, 0.88, 0.461)
fifaRaw <- c(fifaRaw, 0.147, 0.095, -0.272, 0.88, 0.566, -1.998, -0.429, 1.351, 0.304, 0.723, -1.213, -1.475, -1.789, -1.737, 0.88, 0.775, -1.109, 0.199, 0.513, 0.461, 0.618, -0.115, 0.88, 1.351, 0.199, -0.638, -0.69, 0.252, 0.409, 0.618, -2.051, -0.01, -0.481, 0.513, 1.612, 0.67, -1.318, 1.232, 0.475, 1.081, -1.242, -0.636, -1.646, -0.283, 0.576, 0.475, 0.02, -0.485, 1.182, 0.525, 0.273, 1.182, 0.879, 1.131, 1.485, 0.879, -0.182, -1.444, -0.03, -1.646, -1.091, -1.04, -1.444, 0.778, -0.485, 0.273, 0.323, -1.444, -0.586, -0.586, -0.535, -1.697, 1.081, 0.576, -1.242, -0.737, 0.424, -0.434, -0.283, -1.798, 1.03, 0.02, 1.434, 1.333, 0.374, -1.697, -0.081, 1.384, 0.424, 0.576, -1.646, 0.677, -1.495, -0.182, -0.788, -2, 1.131, 0.828, -0.182, 1.03, -0.384, 0.626, 0.828, 0.172, -1.192, 0.172, 1.182, 0.828, 1.03, -0.03, 1.586, 0.02, -1.646, 1.03, 0.576, -0.788, 1.535, -0.737, -1.04, -0.081, 0.929, -1.697, 1.131, -1.444, -1.242, -0.99, -0.081, -0.737, 0.879, -0.485, 0.576, 1.182, 1.535, -0.889, 1.182, -0.737, 1.081, 1.081, 0.778, -0.384, -1.343, -0.737, -1.04, -0.687, 0.626, -0.232, 0.626, -1.848, -0.434, 0.071, -1.848, -1.394, 0.727, 0.475, 0.525, 1.131, 1.283, 0.02, 0.929, -1.293, 0.929, -1.646, -1.04, -0.485, 0.879, -0.434, -1.293, 0.677, -0.939, -0.384, 0.576, 1.182, 0.071, 0.677, -1.697, -0.788, -1.394, 0.98, -0.081, 1.333, 1.081, 0.626, 0.02, 0.778, -1.697, 1.03, 0.374, 0.929)
fifaRaw <- c(fifaRaw, 0.929, -0.838, -0.636, 0.475, 0.98, 0.879, 0.424, 0.778, -1.394, 0.525, 1.434, -0.182, -1.04, -1.444, 0.929, 1.232, -1.495, -1.495, -1.646, 0.424, 1.232, -0.636, -1.747, -0.586, 0.576, 1.182, 0.677, 0.323, -0.131, 0.172, -1.899, 0.929, -0.485, 0.626, 1.232, 0.626, -0.535, -0.384, -0.232, -1.596, 0.98, -2, -1.343, -1.04, 0.475, 1.182, -0.737, 0.727, -2, -1.697, 0.071, 0.626, -0.687, 1.131, -0.485, 0.98, 0.727, 1.283, 1.232, 1.081, -0.485, 0.424, 0.273, 1.687, 0.121, 0.273, -1.192, -0.737, -1.192, 0.677, -0.788, 0.172, -0.283, 0.475, -1.343, 0.071, 0.98, -0.131, -1.798, -0.485, -0.03, 0.929, -0.03, -1.242, 0.677, 1.283, -1.848, -1.091, 0.576, -1.545, 0.374, 1.586, 0.626, 0.424, 0.778, -0.838, 0.727, -0.99, 0.475, 1.081, 0.374, -0.838, 0.323, -0.99, 0.525, -1.091, 0.879, -1.899, 0.374, 0.172, -1.141, -1.848, -0.636, -0.687, -0.636, 1.434, -0.384, 0.02, 0.727, 0.727, -0.384, 1.636, -1.747, 0.677, -0.636, 0.525, 0.677, 1.384, -1.596, -0.384, 1.131, -1.747, 1.434, 1.838, 1.182, -0.636, 0.576, 1.788, -1.848, 0.475, 0.525, 0.778, 0.677, -1.545, -1.899, 0.626, 0.323, 1.03, 0.778, -1.596, 1.788, -0.384, -1.899, 0.02, -1.242, 1.081, 1.687, -1.293, -0.586, 1.232, -0.636, -0.889, 1.081, 0.475, 0.626, 0.576, 0.172, 0.778, 0.677, -0.939, 1.131, -1.545, 0.525, -0.889, 1.485, -0.636, -0.131, 1.182, -1.141, 1.182, -1.394, 0.879, 1.03, -1.747, 0.02, -1.192, -1.192, 0.475, -1.899, -0.03, -0.535, 1.03, 0.98, -0.384, 1.384, -1.394, -0.737, 0.374, 0.071, 0.828, 0.172, 0.222, 0.98, -1.394, -0.182, -0.081, -0.687, -1.646, 0.576, 0.727, 0.121, -0.485, -0.687, 0.98, -0.03, 1.333, -0.636, 0.424, 1.131, 0.879, -1.646, 1.081, -1.697, 0.626, -1.141, 0.525, -0.434, 1.333, 0.323, 1.03, -0.939, -0.232, 0.677, 0.323, -0.636, -1.444, 1.131, -0.434, 1.586, -0.838, -1.141, -0.485, -1.242, 0.02, -0.03, 0.374, -0.838, 0.98, -1.141, -1.394, 1.485, -0.232, 0.222, 0.121, 0.424, 0.677, -0.081, -1.848, -0.99, 0.273, 0.525, 0.727, 0.828, -0.131, -0.939, 0.778, 0.727, 1.182, 0.727, 0.626, 0.677, 0.879, 1.232, -1.596, -0.131, 1.232, 0.626, 1.384, 0.172, -1.848, -0.03, -1.848, 0.222, 0.778, 1.182, 0.828, -1.04, 0.525, -0.283, -0.485, 0.071, -0.737, 0.727, -1.242, -0.283, 0.576, 1.081, -1.747, 0.071, 1.182, 0.071, 1.333, -1.495, -1.242, 0.98, -1.04, -0.283, 1.434, -0.737, 0.626, 0.879, 0.172, 1.434, -0.636, -0.434, 1.182, 0.475, -1.495, 1.03, 0.576, 0.222, 0.475, -0.788, -1.242, -1.242, -1.545, 0.778, 1.081, -1.242, 1.384, -0.939, 0.626, 0.828, -0.434, 1.687, 1.384, 0.222, -0.283, -1.747, -0.182, 1.535, 0.778, -2.05, 0.677, -0.636, 0.273, 1.535, 0.071, -1.394, 0.467, 0.189, 0.801, -1.867, -0.033, -2.2, 0.69, -0.366, 0.301, -0.366, -0.644, 0.189, -0.144, -0.255, -0.533, 0.301, 0.467, 0.245, 0.856, -0.7, -1.867, -0.311, -1.756, -0.589, -0.033, -2.089, 0.023, 0.467, 0.634, -0.2, 0.078, -0.311, 0.356, -0.311, -1.811, 0.578, -1.033)
fifaRaw <- c(fifaRaw, 0.578, 1.19, 0.356, 1.023, 0.078, -2.033, -0.811, 0.189, -0.589, 1.301, 0.189, -2.256, -1.033, -0.144, -0.533, -0.644, -2.145, 1.301, -1.589, 0.301, 0.69, -1.2, 0.245, -0.644, 1.19, 1.023, 0.134, -0.144, -0.088, -0.866, 0.301, -0.255, 0.467, 0.467, 0.634, -0.644, 0.134, 0.578, -2.089, -0.255, -0.477, 0.856, 1.523, 1.412, 0.634, -0.366, 0.69, -2.2, 0.301, -0.2, -0.033, 0.801, 1.245, 1.19, 0.801, -0.033, -0.533, -0.033, 1.856, 0.356, -0.644, 0.412, 0.467, 0.801, -0.477, 0.578, -0.033, 0.134, 0.523, 0.023, -0.533, 0.856, 1.023, -2.2, 0.967, 0.023, -2.2, 0.301, 0.189, 0.912, -0.255, 0.245, 0.467, 0.523, 0.023, -1.756, -0.088, -2.256, -0.477, 1.301, 0.412, 1.134, -2.145, -0.255, 0.189, 0.301, -1.644, 1.19, 0.523, -0.033, -2.2, 0.078, 1.078, 0.412, 0.523, -0.422, 0.856, -1.089, 0.912, 0.356, -2.145, 1.245, -0.589, 0.745, 0.467, 0.745, 1.078, 0.245, 0.578, -0.2, 1.245, 0.634, 0.801, 0.856, 0.301, 0.69, 0.801, -1.756, -0.033, 1.412, -2.089, -2.089, -2.145, 0.023, -0.255, -0.255, -2.256, -0.755, -0.2, 0.245, 0.134, -0.255, 0.523, -0.644, -2.256, -0.7, 0.912, -0.311, 0.523, 0.356, 0.745, -0.366, 0.134, 1.023, 0.967, -2.145, -2.033, 0.301, -0.589, 0.801, 0.078)
fifaRaw <- c(fifaRaw, -0.922, -2.422, -2.033, -0.422, -0.589, -0.533, 1.356, -0.422, 0.634, 0.412, 0.856, -0.255, 0.301, 1.134, 0.856, 0.301, 0.912, -0.311, 0.023, 1.19, -0.088, -0.422, 0.578, 0.023, -0.866, 0.467, 1.412, 0.745, -0.978, 0.356, -0.755, -2.311, 0.356, 1.134, 0.69, 0.967, -1.756, -0.533, -0.811, -2.089, 1.245, 0.301, -1.867, -0.7, 0.745, 0.578, 1.023, -0.755, 1.134, -0.144, -0.477, 0.245, -0.422, 0.412, 0.856, 0.356, 0.856, 0.356, 1.356, 0.245, -1.756, -0.311, -0.811, 0.467, -2.2, -0.422, 0.412, 0.189, 0.356, 0.412, 0.356, 0.078, 0.69, 1.634, 1.69, -1.922, 0.523, -0.033, -0.644, 0.134, 1.19, -2.089, -0.088, 0.967, -2.2, 0.801, 1.301, 1.023, 0.69, 0.245, 1.523, -2.256, 0.412, 0.189, 0.245, 0.467, -1.922, -2.2, -0.033, 0.912, 0.245, -0.422, -1.922, 1.412, -0.477, -2.089, 0.301, 0.801, -0.589, 0.412, 0.023, 0.578, 0.245, 0.745, 0.189, 0.356, 0.967, -0.755, -0.033, 1.023, -0.644, -0.033, -0.144, 0.634, -2.256, 0.189, 1.579, -0.033, 0.078, 1.19, 1.023, 0.078, 0.856, -1.978, -0.589, -0.144, -2.2, 0.578, 0.356, 0.967, -0.033, -2.2, 1.134, 1.134, 0.912, 0.134, 0.523, -0.311, 0.189, 0.578, 0.412, -0.144, 1.245, -0.255, 0.078, -0.422, -2.256, -0.811, -0.144, 0.912, -2.256, 0.356, 0.634, 1.245, -0.144, 1.023, 0.023, 0.023, 0.801, 1.579, 0.634, 0.912, 0.412, -1.756, 0.801, -1.867, -0.922, -0.533, 0.245, 0.856, -0.255, -0.644, -0.2, 0.023, 0.801, -0.422, 0.745, -0.033, -2.256, 1.023, 1.467, 0.745, -0.033, 1.023, 0.412, -1.2, 1.023, 1.245, 0.69, 0.69, 0.301, -0.589, -2.033, 0.134, 0.467, 0.856, -0.033, 0.634, 0.189, 0.189, -1.811, 0.301, 0.356, 0.412, 0.356, 0.356, 0.912, 0.801, 0.801, 0.412, 0.245, 0.69, 0.801, 0.301, -0.866, 1.023, -1.922, 0.189, 0.745, 0.523, 0.356, -0.311, -2.256, 0.912, -2.311, -0.477, 0.301, 0.745, 0.967, 1.078, 0.801, 1.301, 0.634, -0.2, 0.634, 0.801, 0.578, 0.189, -0.533, 0.245, -2.2, 0.578, 1.412, 0.023, 1.301, -1.978, -2.2, 0.523, -0.2, 0.467, 0.856, -0.589, -0.422, 0.023, 0.412, 1.19, 0.912, -0.422, 1.634, -0.7, -2.2, 0.578, 0.467, -0.755, 0.023, 0.634, -1.7, -1.756, -1.978, 2.19, -0.366, 0.912, 1.301, 0.467, -0.033, -0.7, -0.811, 1.467, 1.412, -0.2, 1.134, 0.578, -0.811, 1.078, 0.134, -2.534, 1.023, -0.477, -0.311, -0.144, 0.467, -0.533, 0.061, 0.26, 0.458, -2.386, 0.26, -2.121, -0.137, -0.203, 0.193, 1.318, 0.26, 0.723, 0.723, 0.392, 1.252, -1.063, 1.119, 0.921)
fifaRaw <- c(fifaRaw, 0.855, -1.063, -1.923, 0.656, -0.865, -1.261, 0.127, -2.055, 0.326, 0.524, 0.656, 1.252, -0.931, -1.658, 0.061, 0.656, -1.592, 0.193, -0.005, 0.127, -0.005, 1.053, 0.326, 0.789, -0.732, 0.458, 0.656, 0.656, 0.127, 0.855, -1.923, 0.193, 1.45, 0.392, 0.458, -1.658, 0.59, -1.923, 0.855, -0.071, -1.46, 0.524, 0.524, 0.789, 0.392, 0.723, 0.127, 0.855, -0.005, -1.394, 0.656, 0.458, -0.269, -0.203, 0.26, 2.243, 0.59, -1.724, 0.921, -0.203, 0.723, 0.59, 0.392, -0.666, 0.127, -0.005, -2.187, 0.127, -1.658, -0.666, -1.526, 0.458, 0.59, 0.26, 0.127, 0.921, 0.524, 0.789, -1.195, 0.921, -0.6, -0.402, -0.203, 0.789, 0.458, -1.261, 0.061, -0.402, -1.989, 0.392, 0.193, 1.516, -2.452, -0.931, 1.119, -2.253, -0.6, 0.723, 0.656, -0.534, -0.137, -0.005, 0.458, 0.789, -0.137, 0.855, -2.319, -1.658, -0.005, 1.384, 0.789, -1.46, -0.005, -0.6, -0.6, -0.137, 0.458, 0.193, 0.127, -2.386, 0.26, -0.203, 0.987, 0.59, 1.45, 0.987, 0.193, 0.326, 1.053, -1.327, 0.921, 0.458, -0.203, 0.193, 0.127, 0.392, 1.185, 0.26, 0.127, 1.053, 0.59, -0.203, 0.723, 0.326, 0.193, 0.193, -1.857, 0.59, -0.6, -1.724, -2.055, -1.195, 0.656, 0.326, -1.658, -1.526, -0.137, 0.458, 0.326, 0.524, 0.392, 0.326, 0.127, -2.452, 0.326, 0.392, 0.392, 0.127, 0.921, 0.061, 0.127, 0.392, -0.6, 0.789, -2.187, -2.716, -1.526, 0.987, -0.071, 0.127, 1.252, -0.931, -0.402, 0.127, 0.458, 0.392, 0.524, 0.061, 0.193, 1.252, 0.656, 1.252, 0.458, 0.061, 0.458, 0.127, 1.119, 0.26, -0.005, -0.071, 0.061, -0.005, 1.053, -0.6, 0.392, 0.656, 1.252, 0.127, 0.921, 0.193, 0.26, -2.386, 0.127, 0.392, -0.005, 1.318, -2.187, 0.458, 0.921, -2.253, 0.656, 0.193)
fifaRaw <- c(fifaRaw, -2.055, 0.524, 1.318, 0.326, 0.656, 0.789, -0.005, -0.732, -1.394, 0.061, 0.392, 0.656, 0.26, 0.855, -0.269, 0.326, -0.269, 0.921, -2.319, 0.326, -0.071, -0.402, -2.187, -0.336, 0.921, 0.326, 0.723, 0.59, 1.45, 0.326, -1.394, 0.656, 0.723, -1.526, 0.524, -0.666, -0.203, 0.458, 1.053, -1.658, -0.402, 0.656, -1.857, 0.392, 0.921, 0.789, 0.855, 0.789, 1.252, -2.319, 1.119, 1.053, 1.185, -1.129, -1.195, -2.518, -0.666, 1.185, 1.252, 0.326, -2.452, 1.516, -0.071, -2.452, 0.656, -0.6, 0.921, 1.714, 0.458, -0.137, 0.458, -0.336, 0.524, 1.318, 0.656, -0.137, -0.997, 0.59, 1.053, -0.402, -1.195, 0.326, -0.666, -0.005, 0.326, 1.185, -0.137, 0.855, -0.203, -1.658, 0.326, -1.989, -0.666, -0.336, -2.386, 0.127, -0.269, 0.524, 0.59, -2.187, 0.392, 0.524, 0.524, 0.987, 0.061, 0.458, -1.658, 0.855, 0.855, 0.458, 0.987, -0.137, 0.723, 1.252, -0.865, 0.061, 0.326, 0.127, -1.526, -0.997, 0.921, 0.656, 0.26, -0.137, 0.789, 0.26, 1.119, 1.185, 0.656, -1.195, 0.789, -1.46, -0.402, -2.782, 0.855, -0.534, 0.26, 1.053, 0.524, -0.732, 0.193, -0.269, 0.392, 0.326, 0.193, -0.005, -2.055, 0.987, -0.865, 0.326, -0.798, 0.855, -0.137, -1.989, 0.458, -0.203, 0.26, 0.789, 1.252, -0.997, -2.386, 0.921, 0.326, 0.326, 0.458, 0.723, 0.458, 1.384, -2.253, -0.005, 0.723, 0.061, 1.582, 0.326, 0.127, 0.723, -0.666, 0.524, 0.524, 1.318, 0.656, 0.193, 0.26, 0.458, -2.848, 0.127, 0.921, 0.326, 1.318, 0.392, -1.989, 0.789, -2.253, -0.865, 1.185, -0.732, 1.318, -0.137, 0.855, 0.458, 0.193, 0.061, 0.392, 1.252, -0.402, 0.061, 0.193, 0.987, -2.716, 0.326, -0.203, 0.59, 0.59, -1.592, -1.79, -1.394, -0.402, 0.524, 0.392, -0.269, -0.865, 0.458, 0.59, -0.402, 0.26, 0.127, 0.987, 0.524, -1.923, 0.127, 0.723, -0.137, 0.458, -0.666, -0.468, -2.716, -2.716, 1.318, 0.987, -0.071, 0.59, -0.071, 0.789, 0.061, 0.326, 0.723, 0.987, 0.26, -0.137, -1.195, -0.666, 0.723, 0.26, -2.584, 0.987, 0.061, 1.053, 1.384, -0.005, -1.46, 1.011, 0.622, 0.733, -1.488, -0.044, -1.599, -0.877, 0.456, -0.266, 0.345, -0.044, 0.345, 1.289, 0.456, 1.177, 0.011, 1.233, 1.622, 1.177, -0.933, -1.544, 0.289, -1.599, -0.711, -0.6, -1.544, 0.955, 1.066, 0.9, 0.511, -0.766, -0.6, -0.377, -0.211, -1.321, 1.233, 0.511, -1.544, -1.099, 0.789, -0.655, 0.233, -1.821, 0.011, 0.178, 1.066, 1.455, 0.289, -1.655, 0.011, 1.122, -0.322, 0.233, -1.655, 0.178, -1.266, 0.289, -1.099, -2.043, 0.122, 0.955, -0.488, 0.622, -0.044, 0.178, -0.6, 0.233, -0.877, 0.567, 1.344, 1.122, 0.844, 0.567, 2.233, 0.4, -1.377, 0.789, 0.955, 0.122, 1.4, -0.655, -1.21, 0.011, 0.233, -1.544, 1.233, -0.711, -0.766, -0.766, 0.233, 0.4, 0.067, -0.933, 0.9, 0.955, 2.288, -0.6, 1.066, -0.711, 1.455, 0.567, 0.067, -0.266, -0.877, -0.766, -0.655, -0.877, 0.122, -0.155, 1.622, -1.821, -1.155, -0.1, -1.821, -0.933, 0.955, 1.122, 0.733, 1.122, 0.733, -0.488, 0.456, -1.21, 0.567, -1.599, -1.21, -0.433, 0.955, 0.4, -1.21, 0.178, -0.711, -0.711, 0.345, 0.4, 0.233, 0.345, -1.655, -0.322, -0.766, 0.789, -0.155, 1.455, 0.345, 0.289, -0.988, 0.456, -1.655, 1.4, 0.678, 0.456, 1.344, -0.1, -1.599, 0.456, 0.511, 0.844, 1.011, -0.155, -0.711, 0.511, 1.177, -0.377, -0.6, -2.099, 0.511, 0.678, -1.266, -1.488, -1.599, 0.178, 1.066, -0.822, -1.766)
fifaRaw <- c(fifaRaw, -0.544, 0.844, 1.289, 1.011, 0.178, -0.1, 0.289, -1.821, 0.733, -1.044, 1.233, -0.044, 1.289, -0.655, -0.6, 0.011, -1.044, 1.289, -1.932, -1.488, -0.544, 0.233, 0.733, -0.766, -0.044, -1.877, -1.766, -0.044, 0.622, -1.044, 0.733, -0.655, 1.566, 0.122, 1.622, 1.4, 1.011, -0.488, -0.266, 0.289, 2.01, 0.678, 0.789, -0.377, -0.711, -0.544, 0.844, -0.488, -0.211, -0.711, 1.622, -1.044, -0.044, 0.844, 0.178, -1.821, -0.6, -0.711, 0.733, 0.678, -1.655, 1.066, 1.566, -1.988, -0.766, 0.456, -1.599, 0.122, 1.344, 0.622, 1.844, -0.155, -0.877, -0.322, -0.6, 1.122, 0.067, -0.433, -0.433, 1.177, -1.266, 0.289, -0.877, 1.455, -1.988, -0.155, -0.211, -0.711, -1.988, -0.211, -0.044, -0.6, 0.678, -0.044, 0.4, 0.178, 0.622, -0.655, 1.844, -1.766, 0.4)
fifaRaw <- c(fifaRaw, -0.988, 0.067, 0.844, 1.733, -1.544, -0.377, 1.177, -1.599, 0.955, 1.677, 1.788, -0.711, 1.233, 2.344, -1.766, 0.067, 0.955, 0.844, 0.233, -1.488, -1.988, 1.122, -0.544, -0.6, 0.4, -1.377, 1.899, -0.377, -1.821, 0.511, -0.711, 1.677, 1.955, -0.988, -0.433, 1.289, 0.011, -0.377, 1.788, 0.011, -0.266, -0.433, 0.289, 1.455, 0.067, -0.822, 0.678, -1.544, -0.544, -1.377, 2.233, -0.322, -0.266, 1.455, -0.766, 0.678, -1.488, 0.511, 0.9, -1.821, -1.599, -0.933, -0.488, 0.289, -1.932, -0.211, -0.488, 0.955, 0.733, -0.433, 1.177, -0.822, -0.822, -0.1, 0.122, 1.122, -0.711, 0.345, 1.289, -1.488, -0.155, -0.044, -0.766, -1.21, -0.211, 0.844, 0.067, -0.711, -0.766, 0.733, -0.488, 1.566, -0.377, 0.511, 1.566, 0.844, -1.377, 0.345, -1.71, 0.789, -1.21, 0.678, -0.1, 0.955, -0.266, 0.789, -1.099, -0.711, -0.155, 0.622, -0.766, -1.544, 1.899, -0.377, 1.233, -0.488, -0.377, -0.655, -1.21, -0.377, 0.345, 0.955, 0.622, 0.289, -0.988, -1.321, 1.788, -0.1, 0.456, 0.9, 1.4, 0.678, 0.511, -1.71, -0.766, 0.289, -1.044, 0.178, 0.9, -0.155, -0.044, -0.044, 0.511, 1.289, 1.4, 1.066, 0.622, 1.011, 1.289, -1.544, -0.433, 1.4, 0.233, 0.955, -0.155, -1.988, -0.544, -1.544, 0.678, 1.511, 0.511, 1.011, -0.877, 0.067, -0.433, 0.622, -0.266, -0.655, 0.955, -1.155, 0.4, 0.289, 1.011, -1.655, 0.122, 1.011, 0.233, 1.289, -1.544, -1.432, 0.289, 0.4, -0.155, 1.455, -0.6, -0.044, 0.9, 0.233, 1.566, -0.711, -0.488, 1.4, -0.044, -1.599, 0.289, 0.9, -0.322, 0.011, -0.655, -1.21, -1.377, -1.599, 1.344, 1.011, -0.766, 1.122, -1.155, 0.4, -0.211, -0.155, 1.788, 1.066, -0.044, -0.1, -0.655, -0.655, 1.733, 0.345, -1.766, 0.678, -0.544, -0.1, 1.677, 0.067, -0.766, 0.824, 0.302, 0.824, -1.837, 0.093, -2.15, -1.107, 0.458, 0.615, 0.719, -0.22, 0.928, 0.406, 0.615, 1.293, -0.585, 0.771, 1.397, 0.771, -0.324, -2.046, 0.458, -1.89, -0.272, -0.063, -2.255, 0.615, 0.406, 0.51, 0.824, -1.055)
fifaRaw <- c(fifaRaw, 0.197, 0.302, 0.615, -1.837, 0.25, 0.406, 0.354, -0.168, 0.876, 0.145, 0.563, -2.255, 0.458, 0.928, 1.189, 0.667, 0.615, -2.203, 0.145, 1.137, 0.563, 0.563, -2.15, 0.093, -1.785, 0.98, -1.159, -2.307, 0.667, 0.302, 0.563, 0.719, 0.928, 0.145, 0.876, 0.458, -1.107, 0.667, 0.667, 0.197, 0.406, 0.667, 1.397, 0.458, -2.203, 0.719, 0.406, -0.481, 0.667, -0.376, 0.197, 0.51, 0.093, -2.203, 0.667, -1.42, -0.063, -1.211, -0.585, 0.25, -0.116, 0.25, 0.51, 0.667, 0.406, -1.003, 1.084, 0.719, 0.667, 0.145, 0.771, 0.51, -1.368, 0.876, -1.42, 0.145, 0.771, -0.533, 0.51, -2.359, -0.846, 1.084, -2.411, -0.846, 0.667, 0.771, 0.406, 0.458, 0.771, 0.145, 0.824, -1.942, 0.719, -2.255, -1.472, -1.211, 1.189, 0.093, -1.89, 0.667, -0.585, -0.324, 0.563, 0.145, -0.116, 0.51, -2.359, -1.159, -0.168, 0.719, 0.302, 1.867, 0.876, 0.406, 0.771, 0.041, -2.046, 0.98, 0.563, 0.093, 0.615, 0.041, -0.22, 0.615, 0.824, 0.406, 0.667, 0.719, -0.063, 0.51, 0.51, 0.041, 0.406, -1.577, 1.084, 0.302, -1.785, -2.046, -2.098, 0.458, 1.084, 0.25, -2.046, 0.406, 0.406, 1.032, 0.667, 0.354, 0.093, 0.51, -2.359, 0.667, 0.406, 0.615, 0.354, 0.197, -0.846, 0.041, 0.145, -0.742, 0.458, -2.359, -1.942, -1.316, 0.615, 0.719, 0.563, 0.51, -1.994, -2.255, 0.145, 0.563, 0.458, 0.51, 0.25, 0.719, 0.197, 0.98, 1.397, 1.032, -0.116, 0.876, 0.041, 1.293, 0.563, 0.563, -1.003, 0.145, -0.324, 0.667, -0.324, 0.51, -0.168, 0.876, -0.481, 0.615, 0.458, 0.667, -2.255, -0.168, 0.041, 0.51, 1.032, -2.098, 0.667, 0.876, -2.516, 0.354, 0.667, -2.203, 0.145, 0.928, 0.302, 0.51, 0.458, -0.481, -0.324, 0.145, 0.615, 1.137, 0.458, -0.585, 0.51, -1.159, 0.458, -0.898, 0.928, -2.307, 0.51, 0.197, -1.159, -2.203, 0.197, -0.116, 0.25, 0.458, 1.032, 0.876, 0.197, 0.145, -0.376, 0.563, -2.307, 0.354, -0.585, 0.51, 0.824, 1.241, -1.89, -0.168, 0.719, -2.203, 0.771, 1.137, 0.667, 0.98, 0.771, 0.928, -2.463, 0.302, 0.25, 1.189, -0.063, -2.046, -2.307, -0.168, 0.563, 0.719, 0.615, -1.837, 1.45, 0.093, -2.255, 0.615, -0.481, 0.98, 1.658, 0.145, -0.272, 0.98, -0.116, -1.003, 1.397, 0.615, 0.667, -0.481, 0.667, 0.615, 0.145, 0.041, 1.241, -1.89, 0.719, -0.324, 1.345, 0.041, -0.063, 0.563, -1.524, 0.041, -2.046, 0.615, 0.615, -2.307, 0.406, 0.406, -0.742, 0.615, -2.307, -0.22, -0.637, 0.615)
fifaRaw <- c(fifaRaw, 1.032, -0.116, 0.667, -1.263, -0.22, 0.98, 0.51, 0.876, -0.011, 0.51, 1.084, -1.942, -0.168, 0.197, -1.159, -2.203, -0.637, 1.293, -0.168, -0.063, 0.25, 0.719, -0.063, 1.137, 0.406, 0.458, 0.406, 0.824, -2.046, 0.041, -2.098, 1.345, -0.272, 0.145, 0.406, 0.928, -0.063, 0.563, -1.055, 0.302, 0.406, 0.145, -0.011, -2.203, 0.824, -0.481, 0.824, -0.168, 0.25, -0.011, -1.263, 0.25, -0.063, 0.406, 0.458, 0.615, -0.637, -1.733, 1.189, 0.145, -0.324, 0.615, 0.615, 0.563, 1.137, -2.098, 0.145, 0.667, 0.302, 0.824, 0.824, -0.116, 0.771, -0.22, 0.51, 0.876, 1.137, 0.51, 0.667, 0.354, 0.302, -2.046, 0.458, 1.032, 0.771, 0.824, 0.093, -2.15, 0.041, -2.255, 0.51, 0.563, 0.51, 0.824, -0.846, 0.667, -0.429, 0.406, -0.116, -0.011, 0.563, -1.211, 0.302, 0.51, 0.98, -2.307, -0.533, 0.145, 0.719, 0.667, -2.098, -2.046, 0.25, -0.272, -0.063, 0.615, -0.324, -0.22, 0.719, 0.458, 0.98, -0.116, -0.116, 0.719, 0.458, -1.89, 0.041, 1.032, 0.563, 0.458, -0.794, -1.629, -2.203, -2.203, 0.458, 0.928, -0.429, 0.667, -0.324, 0.719, 0.458, 0.354, 0.824, 1.084, 0.563, -0.168, -0.742, 0.093, 1.241, 0.719, -2.15, 0.563, -0.116, 0.51, 1.345, 0.145, -0.898, 0.902, 0.902, 0.635, -1.657, -0.164, -1.87, -0.804, 0.742, 0.582, 0.795, 0.369, -0.271, 0.902, 0.582, 1.275, -0.378, 1.915, 1.595, 1.222, -0.964, -1.55, -0.538, -1.71, -0.751, 0.635, -1.604, 1.381, 0.582, 1.275, 0.902, -1.177, -0.431, -0.271, -0.591, -1.71, 1.008, -0.218, -0.964, -1.337, 1.062, -0.697, 0.209, -1.604, 0.209, 1.275, 1.275, 0.422, 0.529, -1.764, -0.324, 1.861, -0.111, 0.582, -1.657, 0.102, -1.177, 0.529, -1.124, -1.924, 1.168, 0.902, 0.422, 0.209, 0.529, 0.049, -0.431, 0.742, -0.697, 1.222, 0.742, 0.369, 0.369, 0.155, 2.128, 0.689, -1.444, 0.955, 1.328, -0.697, -0.058, -1.017, -1.231, 0.689, -1.337, -1.817, 0.848, -0.911, -0.697, -0.911, -1.124, -0.484, -0.111, 0.689, 1.062, 1.328, 0.635, -1.071, 0.155, -0.431, -0.164, -0.431, 0.635, 0.209, -1.177, -0.378, -1.071, -0.538, 1.062, -0.591, 1.915, -1.817, -1.284, 1.115, -1.71, -1.284, 1.008, 1.275, 0.582, 0.848, 0.049, 0.742, 1.541, -1.497, 1.168, -1.71, -1.231, -0.218, 1.488, 0.902, -1.39, 0.422, -0.751, -0.644, 0.315, 0.209, -0.058, 0.049, -1.337, -0.644, -1.337, 0.209, 0.102, 1.328, 0.209, 0.315, -0.964, 0.848, -1.87, 0.689, 0.422, 0.742, 0.689, -0.164, -0.484, 0.902, 0.955, 1.062, 0.902, 0.422, -0.697, 0.582, 0.582, 0.209, -1.071, -1.231, 0.848, -0.111, -1.39, -1.71, -1.87, 0.102, 0.369, -0.484, -1.764, -0.111, 0.475, 0.582, 0.795, -0.004, -0.324, 0.315, -1.764, 0.102, -0.271, 1.168, -0.378, 0.955, -0.004, -0.644, 0.582, -0.804, 0.475, -1.817, -1.817, -0.857, 1.115, -0.697, 0.049, 0.102, -1.977, -1.764, 0.262, 1.062, -0.004, 0.742, -0.911, 0.102, 1.861, 1.755, 1.648, 0.262, -0.484, 0.582, -0.111)
fifaRaw <- c(fifaRaw, 1.915, 0.315, 1.115, -1.124, -0.697, -0.164, 1.115, -0.857, 0.102, 1.168, 1.861, -0.857, 0.529, 0.635, 0.582, -1.87, 0.102, -1.284, -0.111, 0.635, -1.604, 0.689, 1.488, -1.924, -0.697, 0.369, -1.497, 0.315, 1.435, 0.635, 0.262, 0.475, -0.591, -0.484, -0.591, 0.635, 1.488, -0.218, -0.857, 0.848, -0.697, -0.058, -1.124, 0.795, -1.87, -0.484, 0.102, -0.804, -1.924, -0.324, -0.538, -0.644, -0.644, 0.475, 0.422, 1.062, -0.378, -0.538, 0.209, -1.817, 0.422, 0.102, 0.049, 1.541, 1.328, -1.657, 0.742, 1.062, -1.39, 0.155, 1.488, 0.475, 0.848, 1.222, 1.435, -1.764, 1.541, 0.155, 1.648, -0.964, -1.817, -1.924, -0.804, 0.315, -0.218, -0.058, -1.55, 2.128, 0.049, -1.817, 1.008, -0.644, 1.488, 2.341, -0.804, -0.964, 0.795, -0.591, -0.484, 1.595, 0.848, 0.102, -0.164, 1.381, 1.381, 0.102, -0.697, 0.155, -1.87, -0.324, -0.431, 1.701, -0.591, 1.381, 0.529, -0.804, 0.209, -1.444, 0.049, 0.475, -1.87, 0.529, -0.644, -0.751, 0.155, -1.657, -0.218, -0.751, 0.635, 1.008, -0.111, 1.168, -1.337, -0.058, 0.422, -0.164, 0.848, -0.697, 0.315, 1.328, -1.604, -0.271, 0.742, -0.857, -1.977, -0.591, 1.008, -0.271, -0.538, -1.017, 0.529, -0.538, 1.168, -1.284, 1.115, -0.431, 0.369, -1.817, -0.271, -1.817, 1.115, -0.911, 0.689, 0.262, 0.742, -0.004, 0.795, -0.857, 0.529, 0.422, 1.381, -0.804, -1.444, 2.074, -1.444, 0.262, -0.218, 0.209, 0.262, -0.804, -0.804, 0.529, 1.008)
fifaRaw <- c(fifaRaw, 0.049, 1.008, -0.538, -1.444, 1.222, -0.164, -0.378, 1.222, 1.328, 0.369, 1.275, -1.817, -0.644, 0.529, 0.529, 1.062, -0.004, -0.538, -0.058, -0.911, 0.529, -0.697, 1.915, 0.529, 0.689, 1.222, 0.902, -1.497, 0.848, 1.062, 0.369, 1.008, 0.635, -1.924, -0.644, -1.657, 0.369, 0.955, -0.697, 0.155, -1.231, 1.115, -0.218, -0.644, 0.422, -0.538, 0.475, -1.071, 0.315, 0.209, 1.595, -1.55, -0.484, 0.369, 1.222, 0.689, -1.604, 0.475, -0.697, 0.848, -0.004, 1.168, -0.697, -0.857, 1.275, 0.475, 0.955, -0.591, -0.644, 0.049, 0.582, -1.444, -0.271, 1.168, 0.635, 0.529, -1.124, -1.231, -0.857, -1.817, 1.488, 1.435, -0.857, 0.369, 0.529, 1.168, 0.955, -0.111, 0.369, 1.755, 0.955, 0.155, -0.751, -0.164, 0.475, 0.422, -1.71, 0.742, -0.484, 0.848, 1.541, 0.155, -1.231, 1.237, 0.747, 0.747, -1.432, -0.015, -1.649, -0.887, 0.148, 0.257, 0.91, -0.015, -0.07, 1.292, -0.124, 1.292, -0.342, 2.381, 0.856, 1.128, -0.941, -1.595, 0.856, -1.432, -0.669, 0.311, -1.704, 1.346, 0.039, 1.455, 1.401, -0.724, -0.397, -0.451, -0.724, -1.432, 0.747, -0.124, -0.941, -1.05, 0.747, -0.941, -0.07, -1.649, -0.615, 1.401, 1.237, -0.179, 1.183, -1.486, -0.669, 1.891, -0.342, 0.856, -1.54, -0.015, -1.159, 0.039, -0.996, -1.595, -0.669, 1.618, 0.529, 0.148, 0.475, 0.257, -0.015, 0.856, -0.669, 0.529, 0.148, -0.179, 0.093, -0.233, 2.272, 0.42, -1.704, 0.91, 1.891, -0.451, 0.747, -0.07, -0.615, 0.202, -1.05, -1.595, 1.019, -0.724, -0.615, -0.887, 0.693, -0.124, -0.397, -0.288, 0.91, 1.618, 0.42, -0.724, 0.148, -0.124, 0.202, -0.778, -0.233, 0.475, -1.159, -1.105, -0.669, -0.669, 0.42, -1.05, 2.163, -1.704, -1.214, 1.019, -1.649, -0.941, -0.506, 1.237, 1.074, 0.747, -0.56, 1.401, 1.51, -1.486, 1.401, -1.323, -0.832, -0.179, 0.856, -0.015, -1.649, -0.179, -0.506, -0.288, 0.039, 0.039, 0.257, -0.342, -1.105, -0.778, -0.941, 0.747, 0.366, 1.455, 1.401, 1.237, -0.506, 0.366, -1.649, 1.128, 0.747, -0.451, 0.638, -0.451, -0.179, 0.747, 0.802, -0.288, 1.292, 1.128, -0.56, 0.693, -0.179, 0.638, -0.615, -1.323, 1.237, -0.288, -1.486, -1.595, -1.54, 0.366, 0.693, -0.179, -1.54, -0.615, 1.074, 0.475, 0.91, 0.039, -0.179, 0.965, -1.704, 0.747, 0.093, 1.074, -0.397, 1.237, -0.615, -0.342, 0.366, -0.832, 0.856, -1.758, -1.323, -0.724, 1.237, -0.615, -0.124, -0.179, -1.813, -1.758, -0.179, 1.292, -0.288, 0.311, -0.451, -0.07, 1.673, 1.292, 1.455, 1.128, -1.05, 0.148, 0.202, 1.891, 0.148, -0.669, -0.832, -0.669, -0.07, 1.128)
fifaRaw <- c(fifaRaw, -0.56, 0.91, 1.727, 1.891, -1.377, 1.401, -0.506, 1.183, -1.649, -0.451, -0.941, -0.778, 0.856, -1.432, 0.529, 1.401, -1.758, -1.05, 0.42, -1.323, 0.747, 1.564, 1.074, 0.257, 0.093, -0.451, -0.724, -0.724, 0.148, -0.124, -0.56, -0.778, 0.91, -0.124, 0.148, -0.887, 1.51, -1.649, -0.451, -0.669, -0.342, -1.54, -0.342, -0.124, -0.615, -0.615, 0.42, 0.475, 0.475, -0.397, 0.093, 0.366, -1.758, -0.124, -0.996, -0.015, 1.074, 0.584, -1.704, 0.638, 1.128, -1.649, -0.233, 1.292, 0.638, -0.288, 1.836, 0.747, -1.486, 1.618, 0.747, 1.836, -1.105, -1.595, -1.758, -0.887, -0.07, -0.288, -0.179, -1.268, 2.435, -0.342, -1.704, -0.07, -0.887, 1.727, 2.272, -0.778, -0.233, 0.257, -0.07, -0.778, 1.618, 0.965, -0.179, 0.148, 1.618, 1.727, -0.451, -0.669, -0.506, -1.268, 0.148, 0.856, 1.51, -0.397, 0.475, 0.965, -1.105, -0.179, -1.105, 1.128, 0.093, -1.595, -0.724, -0.669, -1.214, 0.856, -1.758, 1.019, -0.778, 0.91, 0.856, -0.233, 0.584, -0.778, -0.397, -0.506, -0.179, 1.074, -0.506, 0.311, 0.91, -1.214, -0.288, 0.42, -0.615, -1.649, -0.724, 0.91, 0.311, -0.015, -0.451, 1.346, -0.288, 1.51, 0.366, 1.019, -0.669, 0.747, -1.867, -0.342, -1.704, 0.747, -0.724, 1.074, 0.257, 1.128, -0.397, 1.074, -0.778, 0.093, 0.802, 1.945, -0.288, -1.649, 2.109, -1.323, 2, -0.451, -0.724, -0.179, -1.758, -0.832, -0.342, 1.836, -0.56, 1.237, -0.778, -1.54, -0.179, -0.015, -0.56, 1.346, 0.965, 0.802, 1.51, -1.268, -0.778, -0.124, 0.747, 1.346, 0.475, -0.56, -0.288, -0.832, 0.202, -0.778, 2.272, 0.802, 1.564, 1.51, 1.128, -1.595, 0.693, 0.42, -0.124, 1.51, 0.91, -1.758, -0.07, -1.432, -0.015, 0.965, -0.724, 0.91, -1.105, 1.074, 0.257, -0.179, 0.093, -0.669, -0.015, -0.56, 0.093, -0.124, 1.891)
fifaRaw <- c(fifaRaw, -1.432, -0.397, 0.148, 1.564, 0.802, -1.649, 0.91, -0.778, -0.778, 0.093, 1.292, -0.56, -0.724, 1.346, -0.506, -0.887, -0.724, -0.342, -0.124, 0.693, -1.649, -0.288, 0.529, 1.074, 0.475, -0.832, -1.105, -1.214, -1.377, 1.618, 1.455, -0.887, 0.093, -0.996, 0.91, 1.401, -0.179, 0.039, 1.836, 0.802, -0.832, -0.778, -0.56, -0.07, -0.506, -1.758, 0.638, -0.233, 1.019, 1.346, -0.015, -0.832, -0.572, 0.269, 0.01, -2.124, 0.463, -1.801, -0.572, 0.075, -0.054, 1.045, 0.398, 0.528, 0.463, 0.463, 1.239, -1.089, 1.369, 0.786, 1.045, -0.96, -2.06, 0.786, -0.507, -1.025, 0.14, -1.348, 0.01, 0.722, 1.11, 1.239, -0.119, -1.348, -0.507, 0.722, -2.06, -0.119, -0.313, -0.184, 0.334, 1.11, 0.075, 1.304, -1.93, 0.722, 0.075, 0.075, -0.507, 0.851, -1.866, 0.398, 1.821, 0.334, 0.398, -1.995, 0.592, -1.93, 0.851, -0.119, -1.219, -0.119, 0.722, 0.98, 0.14, 0.528, 0.14, 0.592, -0.184, -1.477, 0.592, 0.592, -0.96, -0.895, 0.075, 2.598, 0.528, -1.283, 1.11, 0.204, 0.98, -0.637, 0.269, -0.443, 0.657, -1.283, -1.995, 0.075, -1.736, -0.507, -1.801, 0.722, -0.054, 0.528, 0.075, 1.045, 0.786, 0.657, -1.736, 0.851, 0.204, -0.96, -0.313, 1.304, 0.722, -1.866, 0.14, -0.507, -1.542, 0.592, -0.119, 1.692, -1.413, -0.637, 1.239, -2.124, -0.701, 0.592, 0.786, -0.766, -0.507, -1.219, 0.786, 0.851, -0.184, 1.11, -1.995, -1.801, -0.572, 1.433, 1.304, -1.154, 0.14, -0.119, 0.01, 0.14, -0.119, 0.334, -0.054, -2.642, 0.334, -0.054, 0.851, 0.851, 1.369, 1.304, -0.054, 0.463, 1.304, -1.089, 0.463, 0.463, -0.766, -0.119, 0.204, -0.184, 1.175, 0.398, 0.075, 1.498, 0.657, -0.184, 0.916, -0.249, 0.463, 0.98, -0.184, 0.204, -1.542, -0.766, -1.866, -1.154, 0.592, 0.528, -1.283, -1.283, 0.01, 0.657, -0.443, 0.786, -0.054, 0.528, -0.119, -1.866, -0.119, 0.204, 0.463, -1.219, 1.045, -0.249, 0.398, 0.592, 0.075, 1.045, -1.607, -2.189, -1.154, 1.239, -0.766, -0.313, 1.498, -0.96, 0.463, 0.722, 0.398, 0.722, -0.119, 0.14, 0.269, 1.433, 0.916, 1.433, 0.657, 0.592, -0.313, 0.592, 1.11, 0.204, -0.054, -0.119, -0.119, -0.249, 1.045, -0.054, 0.463, 0.463, 0.916, -0.054, 0.592, 0.398, 0.14, -1.93, 0.269, 0.269, -0.766, 1.239, -1.93, 0.463, 1.11, -1.995, 0.722, 0.269, -2.254, 0.786, 1.11, 1.11, 0.916, 0.851, -0.119, -1.348, -1.477, -0.443, -0.766, 1.11, -0.119, 0.786, 0.01, 0.075, -0.054, 0.722, -1.607, 0.398, 0.01, -0.637, -1.801, 0.01, 0.851, 0.14, 0.98, 0.786, 1.239, 0.786, -1.672, 0.334, -0.507, -1.801, 0.528, -0.119, -0.313, 0.722, 0.398, -1.219, -0.378, 0.851, -1.089, -1.089, 0.657, 0.657, 0.398, 0.98, -0.378, -1.672, 1.498)
fifaRaw <- c(fifaRaw, 1.175, 1.369, -1.219, -0.96, -2.254, -0.831, 1.369, 0.98, -0.119, -1.995, 1.692, -0.184, -2.383, 1.11, -1.154, 0.916, 1.757, 0.269, -0.054, -0.637, -0.766, 0.916, 1.11, 0.528, 0.204, -1.348, 0.657, 1.304, -0.507, -1.025, 0.463, -0.443, -0.766, 0.528, 1.045, 0.14, 1.821, -0.831, -1.283, -0.766, -1.801, 0.204, -0.184, -2.383, 0.398, -0.443, -1.154, 0.786, -1.995, 0.463, 0.98, 0.916, 0.851, 0.398, 0.722, -1.477, 1.045, 0.98, 0.398, 1.304, -0.184, 0.851, 1.433, -1.283, 0.204, 0.592, -0.054, -1.477, -1.025, 0.98, 0.657, -0.184, 0.14, 1.304, 0.14, 1.175, 1.045, 0.916, -1.283, 0.98, -1.93, -1.283, -2.512, 0.851, -0.507, 0.528, 0.98, -0.054, -0.701, -0.378, -0.313, 0.334, 0.528, 0.463, 0.075, -1.607, 1.175, -0.507, 1.045, 0.01, 0.916, 0.01, -2.124, 0.463, 0.398, 0.592, 0.01, 1.627, -0.766, -1.93, 0.334, 0.528, 0.592, 0.334, 0.916, 0.592, 1.369, -2.06, -0.054, 0.98, 0.334, 1.304, -0.378, -0.054, 0.98, -1.995, 0.398, -0.831, 1.627, 0.851, 0.269, 0.592, 0.075, -2.383, 0.204, 0.657, 0.528, 1.239, 0.592, -2.448, 0.851, -1.413, -0.895, 1.433, -1.154, 1.498, -0.443, 1.045, 0.657, 0.592, 0.657, 0.98, 1.11, -0.313, 0.075, -0.572, 0.592, -2.642, 0.075, 0.14, 0.269, 0.269, -1.542, -1.477, -1.348, 0.14, 0.528, 0.204, -0.119, -1.348, 0.334, 0.592, -0.96, 0.398, 0.14, 0.463, 0.722, -1.801, -0.96, 0.398, -0.184, 0.398, 0.14, -0.637, -2.706, -1.866, 1.563, 0.98, 0.204, -0.249, 0.334, 1.045, -0.313, 0.463, 0.334, 0.722, 0.528, -0.054, -0.831, -0.701, -0.443, -0.313, -1.607, 1.175, 0.14, 0.851, 1.433, -0.119, -1.542, 0.613, 0.213, 0.556, -2.13, -0.358, -2.073, -0.53, 0.556, 0.156, 0.842, -0.358, 0.842, 0.442, 0.385, 1.071, -0.072, 0.899, 1.471, 0.899, -0.53, -2.302, 0.556, -0.987, -0.873, 0.042, -2.245, 0.385, 0.613, 0.613, 0.842, -0.701, -0.53, 0.213, 0.671, -1.616, 0.213, 0.213, 0.156, -0.015, 0.956, 0.27, 0.556, -2.073, 0.499, 0.899, 0.956, 0.099, 0.671, -1.902, 0.213, 1.242, 0.556, 0.385, -2.187, 0.27, -2.302, 1.128, -0.816, -1.502, 0.556, 0.556, 0.671, 0.728, 0.671, 0.442, 0.842, 0.042, -0.93, 0.213, 0.671, 0.099, -0.015, 0.385, 1.871, 0.385, -2.473, 0.842, 0.442, 0.213, 0.956, 0.213, 0.328, 0.442, 0.213, -2.016, 0.613, -1.387, -0.13, -1.673, 0.442, 0.385, 0.442, 0.213, 0.671, 0.613, 1.128, -0.987, 0.956, -0.015, 0.385, 0.385, 0.728, 0.27, -1.159, 0.213, -1.502, -0.987, 0.556, -0.873, 0.956, -2.473, -0.473, 1.242, -2.302, -0.244, 0.499, 0.671, 0.042, 0.442, 0.499, 0.27, 0.613, -1.101, 0.671, -1.844, -1.444, -0.587, 1.071, 0.556, -2.187, 0.156, -0.473, -0.187, 0.27, 0.556, -0.015, 0.156, -2.187, -1.273, -0.701, 0.842, 0.556, 1.757, 0.899, 0.042, 0.613, 0.785, -2.302, 0.842, 0.728, -0.015, 0.671, -0.187, 0.27, 0.956, 0.499, 0.328, 0.842, 0.728, -0.015, 0.613, 0.556, 0.042, 0.328, -2.016, 0.842, 0.27)
fifaRaw <- c(fifaRaw, -1.73, -1.959, -1.844, 0.385, 0.499, -0.987, -1.559, 0.042, 0.385, 0.842, 0.499, 0.099, 0.156, 0.499, -2.416, 0.499, 0.442, 0.556, 0.27, 0.499, -1.044, -0.072, 0.042, -0.644, 0.728, -2.645, -2.359, -1.387, 0.613, 0.842, 0.213, 0.499, -2.073, -2.645, 0.156, 0.499, 0.042, 0.556, -0.015, 0.613, 0.899, 0.956, 1.357, 0.556, 0.042, 0.556, 0.156, 1.299, 0.213, 0.499, -0.301, 0.156, -0.187, 0.671, -0.187, 0.328, 0.613, 0.842, -0.072, 0.213, 0.27, 0.442, -2.245, 0.156, 0.385, 0.328, 1.185, -2.302, 0.442, 1.014, -2.302, 0.213, 0.499, -1.73, 0.613, 0.899, 0.27, 0.613, 0.328, -0.301, -0.415, -0.873, 0.328, 0.842, 0.613, -0.587, 0.556, -0.301, 0.099, -0.415, 0.785, -2.416, 0.27, -0.015, -0.987, -2.187, -0.072, 0.328, 0.385, 0.613, 0.956, 1.357, 0.328, 0.27, -0.072, 0.728, -1.844, 0.499, -0.587, 0.27, 0.785, 1.071, -1.844, -0.072, 0.899, -2.245, 0.899, 1.014, 0.842, 0.956, 0.785, 1.071, -2.588, 0.671, 0.785, 1.128, -0.072, -2.302, -2.416, -0.244, 0.785, 0.956, 0.156, -2.13, 1.528, 0.099, -2.702, 0.785, -0.301, 0.785, 1.871, 0.27, 0.042, 1.014, -0.015, -0.015, 1.299, 0.671, 0.499, -0.415, 0.385, 0.671, -0.072, 0.27, 0.785, -1.844, 0.613, 0.042, 1.357, -0.13, 0.613, 0.499, -1.559, 0.042, -1.844, 0.328, 0.328, -2.13, 0.213, 0.156, -0.53, 0.613, -2.588, -0.015, -0.415, 0.613, 0.728, 0.099, 0.728, -1.273, 0.442, 0.899, 0.499, 0.956, -0.13, 0.556, 1.128, -2.13, -0.13, 0.328, -0.244, -1.559, -0.987, 1.185, 0.328, 0.042, 0.213, 0.671, -0.13, 1.128, 0.499, 0.442, 0.728, 0.671, -1.959, 0.213, -2.359, 0.956, -0.587, 0.156, 0.613, 0.728, -0.415, 0.556, -0.758, 0.213, 0.156, 0.099, -0.015, -1.33, 0.842, -0.473, 0.728, -0.244, 0.671, -0.13, -1.33, 0.328, 0.27, 0.442, 0.728, 0.899, -0.93, -2.588, 1.185, 0.042, 0.156, 0.442, 0.613, 0.27, 1.014, -1.959, 0.156, 0.671, -0.415, 0.956, 0.442, 0.042, 0.728, -0.301, 0.499, 0.728, 1.185, 0.556, 0.385, 0.442, 0.613, -2.416, 0.156, 0.728, 0.842, 0.899, 0.099, -2.073, 0.099, -2.073, -0.301, 0.785, 0.385, 1.014, -0.13, 0.842, 0.156, -0.13, 0.099, 0.042, 0.671, -1.101, 0.156, 0.156, 1.014, -2.13, 0.213, -0.187, 0.671, 0.613, -2.187, -2.245, 0.099, 0.042, 0.042, 0.671, -0.244, -0.187, 0.442, 0.842, 0.842, -0.13, -0.644, 0.613, 0.556, -2.416, -0.13, 1.071, 0.499, 0.442, -0.873, -2.53, -2.416, -2.53, 1.528, 1.014, -0.072, 0.613, 0.099, 0.842, 0.099, 0.213, 0.613, 1.014, 0.27, -0.187, -0.873, -0.187, 1.071, 0.442, -2.13, 0.842, -0.072, 0.556, 1.471, 0.213, -1.73, 1.784, 0.282, 1, -2.003, -0.044, -1.481, -1.22, 0.152, 1.066, 0.543, 0.413, -0.044, 0.021, 0.478, 0.87, -0.044, -1.872, 1.523, 0.543, 0.739, -1.415, -0.436, 0.021, -0.044, 0.347, -1.611, 1.066, 0.413, -0.044, 0.739, -0.762, 0.282, 0.543, -0.24, -1.546, 0.478, 1, 0.347, -0.371, -0.175, 0.347, 0.282, -2.591, 0.347, 1.262, 1.653)
fifaRaw <- c(fifaRaw, 0.021, 0.086, -2.199, -0.11, 0.217, 1.719, 0.282, -0.893, -1.024, -1.481, 1.392, -0.11, -1.742, 0.87, 0.478, 0.086, 1.196, 0.152, 0.87, 0.217, 0.347, -0.175, 0.674, 0.805, 1.066, 0.543, 1.066, 0.021, 0.543, -1.481, 0.674, 0.805, -0.697, 0.217, -0.762, -0.24, 1.066, 0.413, -1.938, 0.739, -0.305, 0.674, -0.371, 0.282, -0.632, -0.958, 1.719, -0.697, 1.196, -2.134, 0.805, 1.719, 0.674, 1.392, 0.282, 0.152, 0.543, 0.282, 0.609, -0.762, -0.24, 0.805, 0.282, -1.481, -2.656, -0.632, 0.152, -2.656, -0.632, 1.327, 0.152, 0.543, 0.674, 0.739, -0.175, 0.021, -1.22, 0.739, -1.938, -0.11, 0.086, 0.87, -1.285, -0.175, 1.327, 0.021, 0.805, 1.066, -1.154, 0.282, 0.347, -1.22, 0.021, -0.11, 0.478, 0.413, 1.914, 0.543, 0.935, 0.282, 0.413, -1.415, 0.543, -0.371, -1.024, -0.501, 0.674, -1.677, -1.089, 1.066, 0.805, -1.154, 0.282, 0.152, 0.347, 0.805, 0.217, 0.413, -0.044, 1.327, -1.481, -1.024, -1.415, -1.742, 0.413, 1.98, 0.413, -2.134, 0.739, 0.152, 1.523, 0.739, 0.282, 0.282, 0.805, -3.113, 0.609, 0.347, 1.131, 1.327, 0.152, -0.044, 0.086, 0.217, -1.872, 0.87, -2.199, -1.22, 0.021, 0.086, 1, 0.478, -0.24, -1.807, -0.762, 0.282, 0.282, 0.805, -0.567, 0.282, 0.805, -1.611, 1.262, 1.523, 1.066, -1.611, 0.674, -0.828, 0.805, 0.543, 1.196, -1.089, 0.347, 0.152, 0.543, 0.152, 0.478, -1.481, 1.131, -1.154, 0.282, 1.327, 1.196, -2.068, 0.413, -0.567, 1.131, 0.217, -1.481, 1.327, 0.217, -2.721, 0.086, 0.152, -1.677, 1.066, 0.282, -2.068, -0.044, 0.674, -0.567, 0.413, 0.217, 0.609, 1.327, -0.371, -0.567, 0.739, -2.068, 0.217, 0.086, 0.282, -2.003, 0.543, 0.478, -1.546, -2.525, 0.086, 0.413, 0.609, 0.021, 1.262, -0.762, 1.588, -1.35, 0.347, 0.152, -0.632, 0.805, -1.22, 0.478, 1, 0.87, -2.656, 0.674, -0.893, -0.632, 0.935, 0.674, -0.893, 0.674, -0.044, -1.481, -2.591, -0.697, -0.958, 0.674, 1.262, -2.068, -2.591, 0.217, -0.828, 0.152, 0.282, -1.154, 0.87, 1.196, -2.46, -0.24, -0.893, 1, 0.739, 0.413, 0.282, 0.805, 0.021, 0.021, 1.066, 0.347, 0.413, -0.567, 0.674, 0.739, 0.87, 0.478, 1.784, -0.697, 0.086, -1.024, 0.935, 0.021, 0.478, -0.305, -0.501, -1.415, -1.807, 0.478, 0.282, -2.525, 1.066, 0.805, 0.935, 0.674, -2.003, -0.893, 0.282, -1.35, 1.523, -0.762, 1.066, -0.436, 0.805, 0.347, 0.935, 0.347, -0.11, 0.805, -0.305, -1.677, 0.021, 0.347, -0.24, -1.285, -0.371, 1.196, -0.24, 0.478, -0.371, 0.021, -0.11, -0.697, 0.021, 0.086, 0.021, 0.347, -1.807, -0.305, -2.134, 0.739, 0.674, -0.371, 0.739, 1.196, 1.262, 0.739, -0.632, 0.021, 0.674, -0.436, -0.11, -2.329, -1.024, -2.068, 1.653, -0.11, -0.24, 0.478, -1.938, 0.347, -0.697, 0.87, 0.674, 0.086, 0.543, -1.807, 1.719, 0.152, -1.154, 1.066, -0.371, 0.086, 1.327, -1.546, 0.609, 0.87, 0.152, 1, 1.131, -0.24, 0.935, -1.415, 1.131)
fifaRaw <- c(fifaRaw, 0.543, 0.609, 0.021, 0.935, 0.021, -0.632, -2.656, 0.478, 0.413, 0.609, 0.347, -0.305, -2.656, -0.893, -2.068, 0.478, -0.436, -0.501, 0.217, 0.543, 1.327, 0.217, 0.674, 0.217, 0.021, -0.501, 0.021, 0.282, 0.739, 0.87, -0.436, 0.152, -0.11, 0.935, -0.175, -0.958, -1.546, -0.24, 1.457, -0.11, 0.152, -0.501, 0.347, 0.674, 0.543, 0.217, 0.347, 0.086, 0.609, -0.24, -1.742, 0.021, 0.347, 0.935, 0.609, 0.347, -1.089, -1.024, -0.567, 0.805, 0.935, -0.567, 0.543, -1.154, -0.11, 1.653, 0.609, 0.217, 0.805, 0.413, 0.282, -0.632, 0.935, 1.653, 1.457, -1.938, -0.11, 0.086, 0.086, 0.674, 0.543, -1.154, 1.292, 0.086, 0.823, -2.057, -0.048, -1.789, -1.186, 0.019, 1.627, 0.421, 0.153, 0.287, -0.048, 0.555, 0.555, 0.019, -2.057, 0.823, 0.555, 0.823, -1.119, 0.22, -1.588, -0.45, 0.488, -2.325, 1.091, 0.488, 0.019, 0.622, -0.584, -0.249, 0.756, 0.086, -1.588, 0.287, 1.493, 0.354, 0.019, -0.249, 0.622, 0.153, -2.057, 0.756, 1.493, 1.56, 0.555, 0.086, -2.66, -0.316, -0.182, 1.895, 0.22, -1.454, -0.784, -1.32, 1.359, -0.784, -1.655, 0.756, 0.488, 0.287, 1.091, -0.249, 0.756, 0.287, 0.622, -0.584, 0.019, 0.756, 0.823, 0.019, 0.354, -0.115, 0.153, -0.985, 0.354, 0.756, -0.918, 0.354, -0.918, 0.019, 0.756, 0.354, -1.588, 0.354, 0.019, 0.421, -0.048, 0.086, -0.182, 0.22, 1.895, -0.383, 1.024, -2.124, 0.153, 1.627, 0.689, 1.56, 0.488, 0.287, 0.823, 0.555, 0.89, -0.784, 0.22, 0.89, 0.89, -0.249, -2.459, -0.048, -0.584, -2.861, -0.383, 1.426, 0.22, 0.689, 0.823, 0.89, -0.048, 0.153, -1.521, 0.957, -1.789, -0.115, 0.555, 0.153, -1.119, -0.249, 1.091, 0.354, 1.024, 1.158, -2.124, -0.182, 0.153, -1.655, -0.249, -1.119, 0.555, 0.488, 1.694, 0.622, 0.287, 1.024, 0.555, -1.387, 0.89, -0.249, -1.253, 0.421, 0.488, -1.588, -0.918, 0.957, 0.89, -0.985, 0.22, 0.555, 0.555, 1.292, 0.756, 0.287, -0.517, 1.158, -0.851, -0.985, -0.918, -1.387, -0.182, 1.962, 0.488, -1.454, 0.756, 0.287, 1.359, 0.823, 0.153, 0.086, -0.115, -2.794, 0.421, 0.823, 0.823, 1.091, -0.048, -0.182, 0.153, 0.354, -0.784, 0.555, -2.593, -1.253, -0.115, -0.45, 0.823, 0.622, 0.287, -3.129, -0.918, -0.115, 0.555, 0.756, -0.182, 0.019, 1.761, -2.325, 1.158, 1.292, 0.756, -1.655, 0.354, -1.32, 0.957, 0.823, 0.89, -0.851, 0.421, 0.22, 0.153, 0.019, 0.22, -1.789, 1.225, -1.387, 0.488, 1.493, 1.225, -2.258, 0.756, -0.45, 1.56, 0.22, -0.985, 0.957, 0.153, -2.459, 0.153, 0.22, -1.856, 0.823, 0.354, -0.651, 0.22, 0.488, -0.048, 0.421, -0.784, 0.957, 1.225, -0.249, -0.249, 0.689, -2.124, 0.823, 0.622, 0.153, -2.526, -0.115, 0.354, -1.119, -2.258, 0.421, 0.488, 0.555, 0.153, 1.426, -0.784, 1.024, -1.052, 0.421, 0.689, -1.387, 0.689, -0.718, 0.22, 0.622, 0.823, -2.526, 0.622, 0.019, -0.316, 1.158, 0.823, -0.918, 0.823, 0.22, -1.32, -2.459, -0.584, -0.985, 0.689, 1.359, -0.383, -2.727, 0.354, -0.048, 0.354, 0.086, -1.052, 0.823, 1.56, -3.263, 0.421, -0.182, 1.024, 0.555, 0.287, 0.354, 0.622, 0.153, -0.115, 0.22, 0.421, 0.488, -0.918, 1.091, -0.182, 1.158, -0.584, 1.694, -1.052, 0.22, -1.588, 0.756, 0.019, 0.689, -0.115, -0.517, -0.784, -1.789, -0.182, 0.488, -1.722, 1.024, 0.622, 1.091, 0.622, -2.258, -1.253, 0.622, -0.651, 1.426, -1.387, 0.823, -0.249, 0.555, 0.555, 0.689, 0.354, 0.019, 1.091, -0.115, -1.454, -0.249, 0.287, -0.115, -1.655, -0.584, 1.426, -0.45, 0.019, -0.048, 0.287, 0.354, -0.718, 0.153, -0.249, 0.086, 0.22, -0.784, -0.784, -2.392, 0.555, 0.421, -0.651, 0.756, 1.225, 1.426, 0.689, -1.32, 0.756, 0.086, -0.718, -0.784, -1.856, -1.99, -1.186, 1.761, 0.689, -0.316, 0.555, -2.191, 0.957, -0.048, 0.957, 0.89, -0.383, 0.354, -1.856, 0.957, 0.287, -1.454, 0.823, -0.851)
fifaRaw <- c(fifaRaw, 0.153, 0.89, -1.119, 0.488, 0.823, -0.182, 1.024, 0.488, -0.048, 1.024, -1.588, 1.426, 0.555, 0.622, -0.182, 0.89, -0.45, -0.249, -2.861, 0.488, 0.622, 1.024, -0.651, 0.488, -2.526, -0.182, -2.191, 0.354, -0.651, -0.115, 0.287, 0.421, 0.823, 0.488, 0.622, -0.115, -0.182, -0.651, -0.316, 0.354, 0.287, 1.024, -0.651, 0.421, -0.584, 0.689, -0.249, -0.182, -1.253, -0.383, 1.225, -0.115, 0.354, -0.115, 0.622, 1.024, 0.153, 0.421, 0.22, -0.316, 0.622, -0.517, -1.32, -0.048, 0.89, 1.091, 0.287, 0.22, -0.918, -1.186, -0.918, 0.756, 0.756, -0.115, 0.89, -0.584, -0.651, 1.627, 0.756, 0.019, 0.354, 0.22, -0.249, -0.584, 1.225, 1.627, 1.627, -1.99, -0.383, 0.22, -0.316, 0.89, 0.689, -0.918, 1.582, 0.508, 1.247, -2.515, -0.298, -1.709, -0.366, 1.045, 0.306, -0.097, -0.567, 0.441, 0.575, 0.172, 0.776, 0.373, 0.037, 1.65, 0.978, 0.709, -0.97, 0.105, -0.164, -0.164, 0.239, -1.373, 0.642, 0.441, 1.247, 0.911, -1.507, 0.306, 0.239, 0.373, -1.507, 0.172, 0.306, 0.306, -0.836, 0.105, 0.239, -0.164, -2.717, -0.701, 0.844, 1.717, -0.164, -0.231, -1.843, 0.844, 0.776, 1.314, 0.172, -0.97, -0.836, -2.179, 1.65, -0.231, -1.642, 0.844, 0.508, -0.097, 1.381, 0.105, 0.306, 0.037, 1.045, -1.104, 0.373, 0.441, 1.112, 0.709, 0.709, 0.508, 1.045, -3.254, 1.045, 0.642, 0.172, -0.903, -1.642, -0.366, 1.515, -0.164, -1.172, 0.642, -0.298, 0.642, -1.44, 0.037, 0.037, -0.231, 1.784, 0.239, 1.448, -1.104, -1.037, 2.053, 0.709, 1.582, 0.172, -0.164, 0.508, -0.903, 0.508, -2.112, 0.172, 1.045, 0.306, 0.441, -2.112, -0.433, 0.978, -2.045, -1.44, 0.844, 0.239, 0.844, 0.642, 0.306, 0.776, 0.441, -0.97, 1.247, -1.239, -1.037, -0.634, 1.515, -0.567, -1.507, 1.045, -0.97, 0.105, 1.314, -1.642, 0.239, 0.508, -0.164, -1.239, -0.567, 0.575, -2.246, 1.784, 0.575, 0.373, -0.03, 0.911, -0.97, 0.776, 0.575, -0.298, 0.844, 0.306, -0.433, -0.634, 1.247, 0.709, -0.366, 0.239, -0.164, -0.164, 0.373, 0.373, 0.105, -0.701, 0.978, -0.97, -1.507, -1.575, -2.112, -0.231, 1.784, -0.298, -0.366, 0.239, 0.172, -0.164, 0.776, -0.5, 0.642, 0.508, -2.246, 0.239, 0.037, 1.784, 0.776, 0.105, -0.164, 0.105, -0.097, -0.231, 0.373, -2.582, -1.978, -0.903, 0.709, 0.978, 0.508, -0.097, -1.373, -0.769, 0.105, 0.508, 0.239, 0.911, 0.105, 0.844, -1.172, 0.844, 1.381, 0.776, -1.642, 0.642, 0.239, 0.508, 0.844, 1.247, -0.903, 0.575, -0.298, 0.844, -0.03, 0.911, -2.112, 0.306, -1.642, 1.179, 1.448, 1.851, -2.717, -0.366, -1.44, 0.776, 0.373, -1.709, 0.575, 0.373, -2.582, -0.03, 0.575, -1.373, 1.112, -0.03, 0.844, -0.231, 0.373, -0.634, 0.373, -0.164, 0.306, 1.045, 0.441, 0.373, 0.709, -0.836, 0.037, -1.911, 0.373, -2.045, -0.231, 1.717, -1.776, -1.978, 0.575, 0.037, 0.642, 0.978, 1.045, 0.239, 0.373, -0.433, -0.433, -0.097, -1.843)
fifaRaw <- c(fifaRaw, 0.441, -0.433, 0.776, 1.247, 1.045, -2.381, 1.179, -0.366, -0.366, 0.844, 0.978, 0.373, 0.037, 0.441, -0.164, -2.851, 0.508, 0.105, 0.978, 0.441, -0.03, -2.314, -0.836, -0.298, -0.231, -0.836, -1.978, 0.911, 0.844, -1.575, 0.037, -0.097, 1.314, 1.112, 0.037, 0.239, 0.978, -0.97, -1.306, 1.515, 0.575, 0.642, 0.306, 1.112, -0.298, 0.373, 0.508, 1.381, -1.172, 0.441, -1.507, 1.515, 0.844, 0.306, -0.03, -1.172, -1.978, -2.314, -0.366, 0.172, -1.911, 0.037, 0.441, 0.911, 1.247, -1.642, -1.642, 0.373, -0.701, 0.978, 0.105, 0.978, -0.836, 0.776, 0.508, 0.642, 0.239, -0.567, 1.381, 0.642, -1.575, 0.306, 0.508, -0.5, -1.104, -0.5, 1.112, -0.097, 0.575, 0.239, 0.306, 0.239, -0.5, 0.172, 0.373, -0.231, 0.239, -1.172, -0.903, -1.978, 1.381, -0.298, 0.105, 0.239, 1.045, 1.448, 1.247, -0.5, 0.441, 0.642, -0.5, -0.298, -1.911, -0.836, -2.045, 1.247, -0.097, -0.5, 0.441, -2.448, -0.567, -0.567, 1.112, 0.441, 0.105, 0.306, -1.575, 1.851, -0.433, -0.298, 0.911, 0.373, 0.373, 0.978, 0.172, 0.172, 0.709, -0.03, 1.247, 2.053, -0.634, 0.373, -0.903, 0.441, 0.105, 0.709, 0.508, 1.314, 0.844, -0.097, -1.709, 0.575, 0.911, 0.776, 0.642, 0.373, -2.515, -0.298, -1.776, -0.231, 0.844, -0.231, -0.03, -1.239, 1.112, 0.172, 0.575, 0.575, -0.366, -0.836, -0.903, -0.164, 0.978, 0.978, 0.105, -1.037, 0.105, 1.112, 0.105, -0.298, -1.709, -0.366, 0.776, -0.231, 0.037, -0.567, -0.836, 0.642, 0.776, 0.239, -1.172, -0.164, 0.441, 0.306, -1.978, -0.5, 0.172, 0.776, 0.978, -1.44, -0.231, -0.903, -1.575, 1.045, 1.381, -1.239, 0.239, -0.567, 0.642, 0.844, -0.164, 0.441, 0.575, 0.105, -0.298, -0.567, 0.306, 1.784, 1.247, -1.642, 0.105, -0.164, 0.709, 1.179, -0.03, -1.776, 0.662, -0.582, 0.21, -0.921, -1.034, -0.017, -0.356, -1.599, -0.017, 0.21, -1.938, 0.662, -0.356, 0.323, 1.34, -1.034, 1.453, 2.018, 0.662, -0.921, -0.921, 0.662, 0.323, -1.599, -0.582, -0.243, -0.13, -0.243, 1.114, 1.453, -0.695, -0.243, -0.356, -2.164, 1.905, 1.114, -0.695, 1.34, 0.323, 1.227, 1.227, 0.097, 0.323, -0.017, 1.34, 0.775, 0.662, -0.017, 1.114, -1.034, 1.34, 0.436, -0.921, 1.114, 0.436, 1.001, 0.549, -0.243, -0.582, -0.017, -0.243, 1.453, 0.323, 1.001, -0.243, 0.21, -0.808, -0.808, -1.712, -0.13, -0.582, -0.243, -0.017, 3.035, 0.097, 0.888, -0.243, 0.662, 0.323, 0.662, 0.436, -1.26, 0.549, -0.356, 1.114, 0.436, -1.373, -1.034, -1.486, 1.453, 0.775, -0.582, -0.017, -0.469, -0.582, 1.679, -1.486, 0.323, -0.017, -0.13, -0.13, 0.097, 0.323, -2.164, -0.356, -2.051, -0.808, -1.486, 0.549, 0.436, -2.051, 0.21, 1.679, -2.051, -0.469, 1.453, 0.097, -3.068, -0.13, -0.017, 0.549, 0.888, 0.436, 0.662, 0.097, -2.39, 1.227, 1.679, 0.662, -1.938, -0.017, -1.034, -0.582, 0.21, -0.017, -0.469, -2.503, 0.888, -0.808, 0.323, 0.436, -0.469, 1.34, 0.775, -1.938)
fifaRaw <- c(fifaRaw, 0.775, 0.662, 1.453, 1.001, -0.808, -0.582, 0.21, -0.695, 0.097, 1.453, 0.888, 0.436, 0.662, -0.017, 0.21, 0.436, 0.549, -0.582, 0.21, 0.549, 0.21, -0.808, 0.323, 0.549, 0.775, -0.808, 1.114, -0.243, -0.582, -0.469, 0.097, -0.017, 0.21, -0.582, 0.097, -0.469, -2.277, -0.582, -0.017, -0.921, -0.808, 0.549, -0.469, -0.695, -1.034, 0.436, 1.114, -1.599, -1.034, -1.26, 0.549, 0.436, -0.017, 1.566, -1.147, -0.017, -1.147, 0.323, 0.097, 0.21, -0.921, 0.323, 1.34, 0.323, 0.549, -0.243, 0.436, 0.549, 0.097, 2.018, -0.469, 0.775, -0.017, -0.695, -1.034, 0.549, -1.599, -1.373, 1.227, 1.114, 0.097, -0.13, -0.13, -0.356, -1.825, 0.21, -0.017, -0.243, 1.114, 0.662, 0.323, 0.888, -1.147, 1.114, -1.034, 1.001, -0.13, 0.549, -0.017, 0.549, -1.034, 0.549, -0.695, -0.921, -0.695, 1.227, 0.662, -0.582, 0.097, 0.323, -0.921, 0.549, 0.323, -3.633, -0.356, -2.051, -1.373, -1.599, -0.582, -0.921, 0.21, 0.323, 0.21, 2.583, 0.21, -1.825, 1.566, 1.114, 0.775, -1.486, -1.373, -0.921, 0.21, 1.792, -1.034, -0.017, 1.792, 0.549, 1.114, 1.114, 0.097, 1.679, 1.453, 2.357, -0.695, 0.549, 1.453, 1.114, -1.938, 0.097, -1.147, -1.034, 0.775, 0.549, -0.469, -0.921, 1.905, -0.469, 0.097, -0.582, -0.13, 0.775, 1.453, -0.243, 0.436, 0.21, -0.469, 0.21, 1.566, -0.243, -2.051, -0.695, 0.775, 0.888, -0.469, -0.469, -0.017, 0.775, -0.582, 0.888, 2.131, -0.695, 1.453, 0.775, -1.373, -0.356, -0.017, -0.582, -1.938, 0.549, -0.695, 0.323, -0.017, 0.775, -2.503, -0.695, 0.888, -0.243, -0.921, -0.017, 0.21, -1.26, 0.097, 0.662, 0.21, 1.114, -1.147, 0.436, 0.662, 0.21, -1.486, -0.469, 0.436, 0.323, -1.712, 1.566, 0.662, -1.486, 0.775, 0.21, -0.469, 0.888, 1.566, 0.097, 0.323, -0.243, -0.017, -1.486, 0.323, -1.034, -1.26, -0.582, 1.566, 0.549, -1.825, 0.21, -0.469, 0.549, -0.243, -0.017, -0.695, 0.21, -0.13, -0.243, -0.017, -1.825, 0.662, 0.097, -0.921, 1.227, 0.662, 0.323, 0.775, 1.34, -2.503, -1.825, 1.566, -0.017, -0.582, -0.13, 0.21, -0.582, 1.34, -0.469, -0.243, 0.549, -0.695, 0.662, 0.775, 0.888, 0.775, -1.373, 0.549, 0.323, 2.018, -0.13, -0.469, 0.21, 0.662, -0.017, 0.323, 0.662, 0.323, 0.323, -0.695, -2.503, 0.097, 1.227, -1.712, 0.21, -0.243, 1.453, 0.097, 0.888, 1.227, 1.001, -0.356, -0.695, 1.001, -0.808, 0.097, -0.582, 1.34, 0.21, -0.921, 0.21, 0.775, 1.453, 0.549, 0.436, -1.938, -0.921, -0.243, 1.227, -1.147, -0.582, 0.323, 0.323, 1.114, -0.808, -0.582, 0.21, 0.21, -0.13, -0.921, 1.227, -0.921, -0.017, -1.26, -0.017, -0.017, -0.808, 2.583, 0.775, -1.486, 1.227, -0.582, 1.227, -1.938, -0.695, 1.453, 0.888, 0.436, -0.582, -2.051, -2.277, 1.114, -0.356, -0.356, 2.018, -1.712, -0.243, 0.888, -0.469, -1.147, 1.737, 0.969, 1.249, -0.705, 0.341, -1.403, -0.008, 0.69, -0.426, 0.969, 1.109, 0.551, 0.969, -0.008, 1.109, 0.621, -0.217)
fifaRaw <- c(fifaRaw, 1.667, 0.411, 1.179, -1.333, 0.621, -0.077, 0.132, -0.356, -1.264, 0.551, -0.356, -0.077, 1.039, -1.822, -0.775, 0.062, -1.473, -2.031, 0.83, 0.9, 0.272, -0.636, 0.411, -0.775, -0.147, -1.194, 0.132, 0.9, 1.458, -0.915, 0.132, -2.31, -0.008, 0.551, 0.341, 0.551, -1.543, -0.845, -0.705, 1.877, -0.217, -2.45, 1.179, 0.83, 0.76, 0.83, 0.411, 1.039, -0.636, -0.217, 0.76, 0.76, 0.83, 1.388, 1.109, 0.9, 0.551, 0.9, -1.403, 1.249, -0.147, -0.147, -0.217, -1.264, -0.636, 1.109, -1.822, -1.264, 0.411, -0.008, 1.388, -2.101, 0.411, -0.147, 0.272, 0.969, 0.272, 1.179, -2.45, -0.077, 2.156, 0.132, 1.039, -0.566, 0.621, 0.83, 0.062, 0.062, -2.171, 0.062, 0.83, -0.008, -0.008, -2.729, -0.356, 0.969, -1.054, -0.496, 0.83, -0.496, 0.341, 0.481, 0.341, 0.76, 0.969, -1.752, 0.9, -2.589, -0.496, -1.822, 2.086, -1.194, -1.752, 0.341, 0.76, -0.356, 1.039, -2.31, 0.83, 0.341, -0.636, -0.426, 0.202, 0.481, -2.101, 1.737, 1.179, -0.077, -0.426, 0.9, -1.124, 0.621, 1.249, -0.426, -0.496, -0.636, -0.984, -0.147, 0.76, -0.147, -0.845, -0.426, 0.341, -0.147, -0.356, 0.132, 0.272, -0.705, 0.9, -0.566, -1.473, -1.961, -2.31, -0.915, 1.458, 0.969, -0.217, 1.109, -0.217, 0.83, 1.039, 0.551, 1.249, 1.179, -2.589, 1.179, 0.411, 1.528, 0.202, 0.062, -0.147, 0.411, 0.551, -0.287, -0.077, -2.171, -2.171, 0.202, 0.9, 0.202, 0.76, 0.969, -2.938, -0.496, 0.202, 0.9, 0.341, 0.551, 0.621, 0.062, -2.031, 0.76, 0.969, 0.341, -0.287, -0.008, -0.496, 0.69, 0.341, 0.062, -1.682, -0.287, 0.551, 0.411, 0.062, 1.877, -1.892, 0.202, -1.194, 0.69, 1.318, 1.877, -1.403, -0.008, -1.124, -0.008, 0.341, -0.147, 1.318, -0.147, -2.799, -0.077, -1.543, -1.613, 1.039, 0.132, -0.426, -0.426, 0.481, -1.264, 0.341, 0.9, -0.915, 0.969, 0.132, 0.062, 0.621, -0.636, -0.845, -0.287, 0.551, -2.45, 0.9, 0.551, -0.984, -2.589, 0.411, -0.287, 1.179, 0.132, 1.039, -0.636, 0.551, -0.984, -1.403, -0.356, -0.775, 0.062, -1.892, 0.69, 0.83, 1.039, -2.31, 0.551, -0.845, -0.356, 0.411, 0.551, 1.109, 0.132, -0.287, -0.287, -1.752, 0.551, -0.217, 0.9, 0.132, -0.426, -0.984, -0.147, -0.775, 0.969, -0.566, -2.38, 0.132, -0.008, -1.333, -0.426, -0.147, 1.877, 1.179, 0.272, 0.272, 0.481, -1.124, 0.202, 2.086, -0.566, 1.597, 0.551, -0.636, 0.9, 0.062, -0.217, 1.318, -0.984, 0.202, -2.241, 1.528, 1.318, 0.062, -0.566, -0.496, -0.147, -0.775, 0.76, 1.458, -1.752, -0.356, 0.621, 0.341, 1.318, -1.194, -0.077, -0.287, -0.147, 0.621, -0.984, 0.9, -0.705, -0.426, 0.202, 1.179, 0.202, 0.341, 1.318, 0.411, -1.124, 0.551, 0.69, -1.333, -1.822, 0.341, 1.179, -0.008, -0.077, 0.272, 0.9, 0.132, -0.147, -0.147, 0.481, -0.287, 0.76, -1.264, -0.077, -2.171, 0.969, 1.109, 0.481, 0.272, 1.388, 1.388, 0.621, -0.217, 0.551, 1.528, -0.077, 0.132, -1.752, -0.705, -2.171, 0.411, -1.682, -0.287)
fifaRaw <- c(fifaRaw, 0.272, -1.264, -0.845, -0.775, 1.109, 0.202, -0.287, 0.341, -1.333, 1.597, 0.969, -1.403, 0.272, 0.551, -0.566, 1.737, -0.566, -0.077, 1.039, -0.636, 1.318, 2.226, -0.496, 0.062, -0.636, 0.341, 0.411, 0.481, 0.69, 1.877, 0.481, -0.984, -0.775, 0.272, 1.039, 0.132, 0.551, -0.287, -3.008, -0.356, -0.915, -0.077, 1.318, 0.132, -0.356, -1.054, 0.621, -0.217, 0.062, -1.194, -0.356, -1.054, -0.775, -0.287, 1.109, 1.109, -0.496, -0.077, -0.008, 0.83, 0.411, -0.915, -1.752, -0.077, 0.062, 0.76, 0.341, 0.551, 0.481, 0.621, 0.481, -0.636, -0.287, 0.9, 0.062, 0.83, 0.132, -1.054, -0.705, 1.039, 0.69, -0.356, -0.356, -0.217, -1.613, 0.202, 1.458, -1.054, -0.356, -0.566, 0.481, 0.132, 0.969, 0.202, 1.249, 0.272, -0.426, -0.217, 1.039, 1.109, 0.76, -1.961, -0.077, -0.705, 1.528, 1.249, 0.132, -0.636, 1.18, 0.573, 0.628, -1.854, -0.144, -1.964, -0.695, 0.187, 0.297, -0.199, -0.034, 0.518, 0.408, 1.235, 0.849, 0.077, 1.125, 1.29, 0.959, -1.909, -1.964, 0.573, -1.633, -1.909, 0.242, -1.909, 0.959, 0.573, 1.07, 0.683, -0.806, -1.192, -1.192, -0.971, -1.633, 0.573, -0.034, -1.247, -0.53, 1.014, -0.089, 0.518, -1.192, 0.077, 0.132, 0.408, 1.29, 0.353, -1.743, -0.144, 1.07, -0.364, -0.144, -1.688, 0.739, -1.743, 1.235, -0.144, -1.854, 0.683, 0.959, -0.695, 0.904, -0.144, 0.132, 0.628, -0.144, -0.585, 0.022, 1.621, 0.408, 0.904, 0.022, 1.787, 0.242, -2.019, 0.849, 0.959, -0.971, 1.235, -0.144, -0.916, 0.187, -0.53, -1.688, 1.014, -1.302, -1.743, -0.751, 0.518, -0.089, 0.573, 0.297, 0.242, 0.518, 1.125, -0.806, 0.794, -1.523, 0.849, 0.518, 0.628, 0.132, -1.137, -0.916, -0.916, -1.412, 0.353, 0.022, 1.456, -1.798, -0.585, 0.077, -1.688, -0.806, 1.014, 0.739, -0.585, 1.014, 0.849, 0.518, 1.345, -1.633, 1.4, -1.909, -1.081, -0.695, 0.297, 0.132, -2.129, 0.022, -0.199, -1.026, -0.089, 0.794, 0.628, 0.739, -1.688, -0.585, -1.302, 0.683, 0.242, 1.566, 0.904, 0.132, 0.353, 0.077, -1.633, 0.904, 0.132, 0.187, 0.408, 0.794, 0.022, 0.904, 0.132, 0.959, 1.511, 0.959, 1.125, 0.683, 0.904, -0.144, -0.916, -1.743, 0.518, 0.628, -1.688, -1.798, -1.743, -0.089, 0.849, -1.468, -1.081, 0.242, 0.518, 0.353, 0.463, 0.187, 0.187, -0.199, -1.688, 0.628, -0.364, 0.573, 0.849, 1.29, -1.081, -0.254, 0.187, -1.523, 0.739, -2.074, -1.854, -0.695, 0.408, 0.187, -1.026, -0.034, -2.129, 0.739, -0.254, 0.849, -0.916, 0.739, -0.199, 1.125, 1.345, 1.4, 0.959, 0.849, -0.034, -0.475, 0.077, 1.511, -0.254, 0.849, -0.751, -1.247, -1.578, 0.353, -0.53, 0.297, 1.235, 1.621, -0.53, 0.022, 0.573, 0.518, -1.743, -0.475, 0.463, 0.794, 0.959, -2.019, 0.408, 0.959, -1.743, -0.144, -0.034, -1.854, 0.904, 1.18, 0.739, 1.29, 0.187, -0.144, -0.309, -1.798, 0.518, 0.849, 0.297, -0.695, 0.794, -0.916, 0.463, -0.089, 1.125, -1.743, -0.089, -0.751, -0.585)
fifaRaw <- c(fifaRaw, -1.909, -0.034, 0.959, -1.026, 0.628, -0.53, -0.089, -0.199, 0.683, 0.959, 1.235, -1.909, 0.297, -0.64, -0.475, 0.959, 0.849, -2.35, -0.53, 0.904, -1.743, 0.408, 1.125, 1.125, 0.739, 1.18, 1.456, -1.743, 0.297, 1.125, 1.125, -0.199, -2.129, -1.909, -0.254, 0.628, 1.07, 0.628, -1.688, 1.621, -0.254, -2.185, 0.187, -0.751, 0.959, 1.4, -0.861, 0.408, 0.573, 0.518, -0.64, 0.739, 0.739, -0.53, -0.64, 1.07, 0.959, -0.034, -1.357, 0.904, -0.806, 0.022, 1.07, 1.29, 0.022, 1.456, 0.959, -0.916, 0.849, -1.909, 0.573, 1.125, -1.743, 0.573, -1.247, -1.302, 0.408, -2.019, 0.573, -0.199, 0.794, 0.739, -0.64, 0.518, -1.026, -0.42, 0.408, 0.022, 0.904, -0.254, 0.683, 1.07, -1.633, -0.254, 0.187, -0.64, -1.412, -0.254, 0.794, 0.518, -0.364, -0.53, 1.125, 0.077, 1.18, -0.034, 0.959, 0.849, 0.463, -2.074, 0.077, -1.854, 0.573, -1.688, 0.683, 0.408, 1.235, -0.199, 0.463, -0.806, -0.53, -0.089, 0.628, -0.806, -1.854, 1.787, 0.959, 1.125, -1.357, -0.42, -0.089, -1.743, 0.849, 0.739, 1.621, 0.959, 0.463, -1.357, -1.964, 1.07, 0.408, 0.022, 1.125, 1.235, 0.408, 0.904, -1.798, -1.412, 1.125, -1.302, 0.739, 0.959, -0.695, -0.475, 0.022, 0.739, 0.794, 1.29, 0.904, 0.187, 0.628, 1.014, -2.129, -0.089, 0.683, 0.353, 0.683, 0.297, -1.909, 0.683, -1.854, -0.254, 1.4, 0.683, 1.345, -0.254, -0.089, 0.628, 0.408, -0.034, -0.53, 1.07, -0.806, 0.463, -0.144, 1.07, -1.633, 0.463, 0.794, 1.125, 1.07, -2.24, 0.408, 0.242, 0.187, 0.187, 1.511, -0.585, -0.364, 0.739, 0.573, 1.29, -0.695, -0.585, 0.739, 0.077, -1.854, 0.408, 1.18, -0.144, 0.683, -0.53, -0.53, -1.633, -2.074, 1.345, 0.739, -0.695, 0.573, -1.081, 0.242, 0.408, -0.254, 1.345, 0.959, -0.254, 0.353, -0.144, -0.309, 1.29, 0.463, -1.357, 1.235, -0.089, 0.573, 1.014, -0.089, -0.916, 0.881, -0.454, 0.714, -2.038, -0.454, 0.714, 0.881, -1.204, -1.037, -0.704, -0.787, 0.63, -0.787, 0.38, -1.121, -0.203, 0.13, 0.047, 0.13, -0.203, 0.213, -0.12, -0.037, -0.871, 0.714, -0.037, 0.213, -0.037, 0.047, 0.213, -1.204, -0.62, -0.871, 0.297, -0.037, 1.298, -0.287, 1.881, -1.204, 1.381, 0.63, 0.213, 0.13, -1.538, 1.131, 0.464, 0.881, -1.121, -0.954, 0.297, -2.372, -2.288, -1.288, -0.12, 0.047, 0.547, 1.464, 0.881, -1.288, -0.203, -1.288, 0.797, 0.797, 0.213, -0.454, -0.454, -0.62, 0.213, -1.204, -0.704, 0.38, 1.298, 0.38, -2.955, 0.714, -1.621, -0.537, 0.047, 0.464, 0.547, 0.13, 0.714, 1.298, -1.371, 0.38, 0.213, 0.38, -0.871, -0.287, 1.798, 1.381, 1.047, 1.214, -0.537, 0.881, -2.872, 0.797, 2.131, 1.464, 0.797, -0.037, -1.371, 0.38, 1.131, -0.62, 0.38, -0.287, -0.62, 0.714, 0.797, -1.955, 1.298, 0.714, -0.62, 0.797, 0.047, -0.203, -0.704, 1.214, 0.714, 0.964, -0.12, -2.288, -0.954, -0.287, 0.547, 0.047, -0.12, 1.214, 0.13, -0.287, 0.881, 0.714, 1.131, 0.38, 0.464, -0.287, 0.213)
fifaRaw <- c(fifaRaw, -0.037, 1.965, 0.547, -0.787, -2.288, 0.63, 0.714, 0.797, 1.548, -0.203, 0.797, -0.12, -0.12, 1.464, -0.287, 0.63, 0.213, -0.203, -0.954, -1.454, -0.037, 0.213, -0.037, -0.037, 1.131, 1.548, 0.213, -1.538, -0.287, -0.12, -0.037, -0.954, -1.121, -1.121, -0.704, -0.203, -0.37, 0.547, -1.705, 1.047, -0.203, 1.464, 0.547, -2.538, -1.204, 0.547, 2.048, -0.037, -0.037, 1.381, -0.454, -0.37, 0.797, -1.121, -1.121, -0.704, 0.63, -0.37, -0.287, -0.787, -1.288, -2.455, 0.714, -0.871, -0.12, 0.38, 2.215, -0.871, 1.298, -0.954, 0.13, -0.787, 0.714, 1.214, -0.037, -0.454, -0.203, 0.213, 0.797, -2.705, -0.37, -0.287, 1.214, -0.037, -0.37, 0.38, 0.547, 0.213, -2.038, 0.464, -1.037, -0.537, -0.537, 0.63, 0.297, 1.131, 0.714, -1.371, -1.955, -2.955, 0.63, 0.38, -0.871, 0.797, -0.871, -0.037, 0.881, -0.287, -0.37, 0.047, -0.871, -2.205, -0.537, -0.37, -0.203, -0.037, -2.789, -0.787, 0.38, -1.204, -1.454, 0.797, -0.871, 0.047, -2.038, -0.037, -0.871, -0.203, 2.048, 1.631, -1.204, -0.537, 0.047, 2.215, 0.297, -0.37, -0.203, 0.881, -1.121, -0.704, 1.298, -2.372, 1.965, -0.62, 0.213, -0.287, 1.965, 0.464, 0.38, -0.287, 0.464, -1.121, 0.547, 0.213, 0.13, 0.547, -0.704, -0.454, -0.704, 0.13, 0.547, -2.121, -1.204, -0.62, -0.704, -0.537, -0.12, 1.381, 1.798, -1.454, 1.464, -0.704, 0.964, -0.037, 0.464, -1.037, 0.38, -0.537, 0.38, 0.297, -2.705, -0.037, -0.203, 0.13, -0.037, -0.203, 0.881, -0.287, -0.037, 2.048, 1.047, 0.797, -0.203, -0.704, -1.621, -2.288, 0.047, 0.797, -0.203, 0.797, 0.13, -0.12, 0.297, 1.214, 1.798, -0.787, -2.372, -1.121, 0.547, 1.548, 0.38, 0.547, 0.714, -0.203, 0.13, -0.871, 0.714, -1.037, 0.63, 0.464, 0.797, 0.13, 0.464, 0.63, 0.547, 1.214, -0.537, 0.047, 1.131, 1.047, 0.714, 0.964, -1.621, -0.454, -0.287, -1.371, -0.454, -0.203, 0.464, 0.881, 0.13, 0.13, 0.047, 0.714, 1.464, -1.121, 0.797, -0.12, -0.954, 1.464, 0.213, 1.131, -0.871, 0.881, 1.047, -0.787, -0.871, 0.38, -0.62, 1.047, 0.38, 0.464, -2.372, 0.547, -0.537, -0.12, 0.881, 0.464, -0.37, 0.297, 0.63, -0.787, 0.714, 0.213, -1.037, 0.797, 1.214, 0.464, 0.297, 0.63, -0.12, 0.63, 0.714, 1.965, -0.537, 0.547, -0.954, -1.371, -0.287, 1.047, -0.287, -0.871, -3.289, 0.63, 0.213, -0.62, -1.204, 0.63, 0.547, 1.548, 0.547, 1.548, -0.203, 0.464, 0.881, -1.871, 0.213, 1.131, 0.38, -0.037, -0.454, -1.037, 0.38, -0.704, 0.714, 0.714, -0.62, 0.213, -0.12, -0.37, 0.714, -0.454, -0.454, -0.287, 0.714, 1.298, 0.797, -0.537, 1.298, -2.372, 0.13, 0.047, -0.037, -2.622, 1.381, 1.381, 0.464, 0.547, 0.047, 2.298, 0.714, 1.047, 1.298, 0.38, 1.381, -2.622, -1.204, 0.881, 0.714, -1.288, 0.547, 1.047, -0.704, 2.215, -0.454, -0.454, 0.964, -0.37, -0.537, 0.38, -0.37, 0.13, 1.44, 0.202, -0.151, -2.095, -0.799, -1.153, 0.556, -1.683, 0.792)
fifaRaw <- c(fifaRaw, -0.033, -1.094, -0.917, -0.151, 0.379, 0.556, -0.21, -0.151, 1.145, 0.144, 0.085, -1.27, 1.086, -1.919, -0.151, 0.909, -1.565, 0.438, 1.204, 0.674, 1.086, -0.505, -0.269, 0.556, 0.32, -2.036, -0.033, -0.092, -0.151, -0.092, 0.556, 0.32, 0.556, -2.213, 1.027, 1.44, 0.261, 0.909, 1.322, -1.919, 0.556, 0.202, 0.379, 0.32, -1.27, 0.909, -2.036, 0.85, -0.917, -1.27, -0.21, 0.144, 0.968, 0.615, 0.909, 0.438, -0.092, 0.32, 0.026, -1.683, -0.446, 0.674, -0.563, -0.976, 0.733, 0.144, -2.213, 0.379, 0.733, -0.328, 0.202, -0.387, 0.674, 0.674, -0.505, -1.919, 0.438, -0.328, 0.085, 0.026, 0.85, -0.151, 0.497, 1.734, -0.622, 0.674, -1.153, -0.269, 0.909, 0.968, -0.74, -0.21, 0.556, 0.733, 0.379, 0.438, -0.681, 0.733, 0.085, -0.033, -0.446, -2.861, -0.151, 1.381, -2.272, 0.085, 1.616, 0.202, -1.153, 0.556, 0.144, 0.32, 0.497, -1.977, 1.557, -2.154, 0.085, -0.505, 0.615, 0.438, -1.27, 0.615, 0.379, 0.733, -0.917, 0.379, 1.322, -0.092, -1.801, -0.269, -0.033, 1.204, 0.379, 0.556, 1.086, -0.21, 0.792, 1.499, -1.27, 0.085, 0.202, -0.563, 0.438, 0.615, -0.622, 0.438, 0.144, 1.322, 0.438, 0.674, 0.32, 0.497, 0.438, 0.792, 0.792, -1.035, 0.202, -0.269, -1.86, -1.388, -1.919, -0.033, 0.026, -0.269, -1.506, 0.909, 1.44, 0.674, 0.615, -0.21, 0.909, -0.681, -2.272, -0.622, -0.033, -0.446, -0.151, 1.322, -0.858, -0.622, 0.438, 0.909, 0.792, -2.154, -2.213, -0.269, -0.446, 0.497, 0.261, 0.85, -2.331, -1.624, -1.329, 0.968, 0.438, 1.263, -0.092, 0.497, 0.026, 0.202, 0.615, 0.968, -1.094, 0.615, 0.556, 0.438, 0.438, 1.263, -1.035, -0.092, -0.446, 0.792, 0.085, 0.733, 0.144, 1.557, 0.026, -0.328, -0.269, -0.387, -2.036, 0.792, 0.085, -0.151, 1.263, -1.624, 0.497, 0.261, -2.567, 0.261, -1.742, -1.919, 1.086, 0.674, 0.202, 1.086, 0.085, -0.681, -0.505, -0.21, 0.379, 0.615, 0.615, -0.21, 1.381, 0.144, 0.32, 0.261, -0.446, -2.39, 0.379, -0.563, -1.742, -2.036, -1.801, 0.32, 0.792, 1.381, 1.204, 1.793, 0.32, -1.035, 0.261, 1.145, -2.508, 0.438, 0.144, -0.269, 0.144, 0.497, -2.979, 0.85, 1.675, -1.094, 0.792, 1.086, -1.683, 1.086, 0.909, -0.681, -2.39, 0.026, 0.202, 1.499, 0.32, -2.036, -2.449, -1.683, 1.44, 1.145, -0.976, -2.449, 0.792, 0.379, -2.743, 0.085, -0.033, 0.909, 0.615, 1.145, 0.438, -0.092, 0.202, 0.085, 0.379, 0.615, -0.622, -1.27, 0.497, 1.086, 0.379, 0.026, 0.085, -1.506, -1.094, -0.151, 1.027, -0.622, 0.497, 0.085, -0.387, -0.269, -1.919, -0.092, -0.269, -1.624, 1.145, -0.446, 0.32, 0.085, -2.449, -0.681, 0.85, 0.026, 0.674, 1.675, -0.21, 0.438, 0.32, 1.263, 0.792, 0.909, 0.497, 0.497, 0.615, -1.094, -0.505, 1.027, 0.144, -1.565, -0.505, 0.909, 0.32, -0.21, 0.261, -0.033, 0.556, 0.144, -0.033, 1.499, 0.556, 0.379, -2.095, -0.033, -2.39, 0.32, -0.387, 1.145, 1.145, 0.85, -0.799, 0.379, -0.033, 0.968, -0.269, 0.674, -0.858, -2.036, -0.681, -1.742, 0.792, -0.446, 0.32, -0.21, -2.743, 0.261, 0.144)
fifaRaw <- c(fifaRaw, 1.145, 0.792, 0.144, 0.733, -1.212, 0.909, 0.085, 0.144, 0.792, 0.556, 0.438, 1.086, -1.919, 0.085, 0.556, -0.151, 1.263, 0.026, 0.202, 0.556, -0.446, 1.616, 0.202, 0.85, 0.497, -0.151, 1.322, 0.438, -2.154, 1.322, 0.144, 1.263, -0.033, -0.033, -2.92, 0.792, -1.565, -0.74, 0.674, -0.505, 0.674, 0.32, 1.263, 0.438, 1.381, 1.557, 0.379, 1.44, -0.622, 0.556, 0.497, 0.968, -1.447, 0.615, 0.497, 1.204, 0.379, -1.447, -1.742, 0.026, 0.556, -0.033, 1.322, -0.446, -0.21, 0.085, 1.086, 0.026, -0.799, -0.622, 1.145, 0.144, -1.565, -0.033, 0.909, -0.269, 1.734, 0.379, -1.683, -1.329, -1.624, 1.263, 0.968, -0.21, 0.379, 0.32, 1.027, -0.033, -0.446, 0.261, 0.556, 0.438, 0.085, 0.085, 0.497, 0.968, 0.615, -2.154, 1.204, -0.21, 0.674, 0.85, 0.085, -0.328, -0.138, -0.787, -0.381, -2.164, -1.597, -0.868, 0.753, -2.732, 0.753, -1.111, -1.678, 0.348, -0.057, 0.915, -1.192, 0.429, 0.834, 0.51, -0.3, -0.138, -0.543, 0.753, 0.105, -1.759, -0.057, 0.186, 0.105, 0.591, 0.915, -0.219, 1.158, -0.868, 0.186, 0.591, 1.32, 0.267, -0.543, 0.429, 1.32, 0.105, 0.915, 0.186, -0.868, -0.057, -0.462, -2.326, 1.077, -0.138, -1.759, -0.787, 0.105, -0.706, -0.949, -0.057, 1.077, 0.348, -2.326, -0.138, -1.84, -0.219, 0.186, 0.672, 0.105, 0.105, 0.51, -0.625, -0.625, -2.245, -1.516, 0.267, 0.105, 0.996, -2.488, 0.591, -0.3, -2.002, -0.787, 1.239, 0.51, 2.212, 2.131, 0.915, 0.105, 1.077, -0.138, 0.186, -0.706, -1.678, 1.483, 1.158, 1.077, 1.239, -1.759, -2.651, -0.462, 0.348, 0.672, -2.975, 0.51, -0.138, -0.138, -0.3, 0.51, 0.348, -0.219, 1.564, -0.219, -0.138, 0.834, 0.186, -1.111, 1.32, 0.024, -0.381, 0.51, 0.267, -0.625, -2.164, 0.591, 1.564, 0.024, -0.462, -1.921, 0.024, 0.51, 0.105, 1.726, -1.921, 1.483, 1.239, 0.186, -0.3, 0.51, -1.597, 1.726, 1.402, -0.462, 0.186, -1.435, 0.51, -0.057, 1.077, -1.03, 0.51, -1.759, 0.834, 0.105, -0.3, 0.024, -1.111, -0.381, 0.672, 1.564, 1.158, 0.915, -0.706, 0.753, 1.645, 0.996, 0.834, 0.672, 0.753, 0.834, -0.219, -1.597, -1.273, -0.625, -0.138, 0.267, 0.915, 0.429, -1.354, -1.03, 0.024, -1.759, 0.51, 0.348, 0.591, -1.435, 0.51, -1.678, -0.462, -1.678, 0.348, 0.186, 1.158, 0.672, 0.996, -0.868, -0.625, 1.483, 0.672, -0.381, -0.706, -0.706, 0.186, -0.787, -0.138, -1.759, -1.354, 0.996, -2.651, -0.3, -0.625, 0.672, -0.868, 0.024, 0.591, 1.158, 0.024, -0.462, 1.077, 0.105, 1.077, 0.267, -0.381, 0.186, 1.969, -0.787, 0.51, 0.348, -0.219, -2.245, 0.915, 1.32, 1.564, -0.3, -2.732, -2.894, -1.678, 0.186, 0.834, 0.834, 0.915, -0.3, -1.192, 0.429, -0.219, 0.753, 0.348, -0.057, -0.949, 0.591, 0.024, 0.915, -0.462, 0.672, -0.787, -1.111, 0.429, 0.024, -0.3, 0.348, 0.591, 0.672, 0.105, 1.564, -0.625, -0.057, 0.024, -1.03, 0.186, -0.219, -0.625, 0.51, -0.543, 1.077, 0.105, 0.267, -0.219, 0.834, 1.564, 1.239, -0.949, 0.672, 0.915, -2.326, 0.267, -0.138, 0.591, -0.057, 0.915, -0.057, 0.105, 0.996, -0.462, 0.996, 0.429)
fifaRaw <- c(fifaRaw, 0.915, -1.516, 0.591, 0.672, 0.186, 0.834, -0.3, -1.111, -0.381, 1.158, 0.672, -0.057, 0.834, 0.51, -0.057, -1.192, -0.057, 0.753, -2.245, 0.51, 0.834, 0.267, -0.949, 1.483, -0.219, -0.949, 0.51, -2.245, -1.678, 0.996, 0.024, 0.186, -0.625, 0.348, 0.591, -0.949, 1.888, -1.678, -2.732, 0.996, 0.672, 0.024, 1.807, 0.105, 0.753, -1.111, 0.915, 1.077, -0.381, 0.591, 0.105, -1.759, 1.807, 1.077, 1.645, -0.462, 1.239, -0.625, 0.267, 0.672, -0.219, -0.706, 1.402, -0.138, -1.273, 0.105, 0.267, -2.083, -0.138, 0.996, 0.024, -0.462, 0.348, 0.429, -0.787, 0.591, -1.111, 0.915, 0.915, 0.753, 0.591, 0.186, -0.381, 0.267, -0.462, -0.625, -0.381, -2.083, 0.429, 0.834, -0.381, -1.597, -1.516, 0.591, 0.915, -2.651, 1.402, -0.787, -0.381, 0.996, 2.212, 0.996, -0.787, 0.753, -0.625, -0.381, 0.996, 1.402, -0.381, 0.267, 0.105, 0.105, -2.002, -1.435, -1.435, 1.239, 0.753, 0.834, -0.057, -1.111, -0.138, -0.625, -0.219, 1.564, -0.462, -0.625, 0.915, 0.753, -0.057, 0.834, -0.706, 0.753, -0.381, -0.625, -0.543, 1.077, -0.706, -0.057, -1.111, 1.158, -0.543, 0.51, -1.84, 0.672, -0.057, -0.462, 0.429, 0.51, 0.348, 1.402, -0.219, 0.996, 0.591, 1.077, 0.348, 1.645, 0.429, 0.753, -1.597, 0.591, 0.672, 1.239, 1.158, -0.543, 0.429, -0.381, 0.591, 0.51, 0.024, -0.381, 0.996, -1.597, -1.516, -0.787, 0.429, 0.672, 0.753, -0.787, 1.239, -1.435, -0.057, 0.105, 1.077, -1.597, 0.591, 0.996, -0.3, -0.3, -2.002, 1.402, -0.057, 0.591, 0.672, 0.753, 0.348, -0.706, -0.787, 1.077, -0.138, -0.462, 1.726, -0.138, -1.516, 0.51, -0.219, -0.706, 0.915, -1.354, -0.381, -0.625, -0.3, 1.32, 1.229, 0.426, 0.627, -1.481, -0.377, -1.481, -0.226, 0.326, 0.075, 0.577, -0.477, 0.778, 0.778, 0.527, 1.279, 0.175, 1.229, 1.229, 0.978, -1.13, -1.581, 0.276, -1.33, -1.18, -0.828, -1.531, 0.928, -0.377, 0.727, 0.978, -1.079, -0.979, -0.728, -0.527, -1.481, 0.828, 0.276, -0.728, -1.431, 0.978, -0.477, 0.627, -1.28, 0.276, 0.125, 0.627, 1.129, 0.226, -1.732, 0.226, 1.581, 0.527, 0.125, -1.782, 0.677, -1.481, 1.129, -0.778, -1.933, 0.326, 1.129, -0.577, 1.079, -0.628, 0.426, 0.878, -0.126, -1.33, 0.376, 0.828, 0.577, 0.376, -0.527, 2.283, 0.075, -1.531, 1.029, 1.631, -0.176, 1.079, -0.678, -1.23, 0.125, -1.28, -1.782, 1.029, -1.481, -0.879, -1.029, 0.276, -0.728, 0.727, -0.778, 0.978, 0.928, 0.778, -0.979, 1.029, -0.728, 0.577, 0.426, 1.179, 0.476, -1.28, -0.828, -1.079, -1.079, 0.527, -0.678, 1.48, -1.782, -1.13, -0.226, -1.882, -0.477, 1.179, 0.677, -0.276, 0.828, 0.627, 0.426, 1.43, -1.631, 1.179, -1.732, -1.28, -0.929, 0.878, 0.276, -1.682, 0.276, -0.276, -0.377, -0.076, 0.226, 0.175, 0.727, -1.38, -0.427, -1.431, 0.828, -0.076, 1.882, 1.129, -0.076, -0.226, 0.627, -1.732, 0.778, -0.176, 0.577, 0.577, 0.577, -0.176, 0.878, 0.627, 0.476)
fifaRaw <- c(fifaRaw, 1.279, 1.029, -0.477, 0.627, 0.426, 0.577, -1.18, -1.732, 0.426, 0.727, -1.481, -1.732, -1.631, 0.226, 0.828, -0.678, -1.732, 0.125, 0.627, 0.527, 0.928, 0.075, -0.025, -0.025, -2.033, 0.778, -0.577, 0.727, 0.276, 1.179, -0.728, -0.076, 0.376, -0.527, 0.778, -1.983, -1.631, -1.18, 0.878, 0.476, -0.628, 0.778, -2.133, -1.832, -0.527, 0.878, -1.18, 0.476, -0.226, 1.029, 0.928, 1.38, 1.179, 0.878, -0.377, -0.377, -0.126, 1.631, -0.126, 0.878, -1.481, -0.929, -0.728, 0.677, -0.828, 0.677, 0.677, 1.53, -1.38, 0.276, 0.778, -0.327, -1.933, -1.079, 0.075, 0.476, 0.978, -1.732, 0.627, 1.38, -1.983, -0.276, 0.426, -1.531, 0.376, 1.279, 0.828, 1.43, 0.376, -0.427, -0.025, -0.778, 0.727, 0.878, 0.627, -0.879, 0.577, -1.28, 0.677, -0.678, 1.029, -1.882, 0.125, -0.126, -1.13, -1.983, -0.527, 1.179, -0.778, 1.129, 0.577, -0.327, 0.426, 0.025, 0.075, 1.079, -1.832, 0.376, -1.732, -0.076, 0.527, 1.129, -1.732, 0.025, 1.079, -1.33, 0.727, 0.928, 1.33, -0.577, 1.179, 0.978, -2.033, 0.878, 1.38, 1.129, 0.426, -1.631, -1.933, -0.025, 0.376, 1.33, -0.327, -1.631, 2.082, -0.577, -2.033, -0.226, -1.28, 1.279, 1.781, -1.13, 0.577, 0.778, -0.628, -1.18, 1.681, 0.978, -0.025, -0.076, 0.527, 1.38, 0.276, -0.879, 0.426, -1.732, 0.175, 1.179, 1.229, -0.327, 1.38, 1.079, -1.13, 0.878, -1.581, 0.276, 0.577, -1.732, -0.327, -1.079, -0.828, 0.577, -2.033, -0.377, 0.075, 0.928, 0.527, -0.276, 0.928, -1.33, -0.628, 0.276, 0.125, 0.878, -0.176, 0.527, 1.029, -1.33, -0.025, -0.176, -0.226, -1.481, 0.025, 0.778, -0.176, -0.628, -0.628, 1.279, -0.377, 1.279, 0.025, 1.33, 0.928, 0.978, -1.882, 0.125, -1.732, 0.778, -0.929, 1.129, 0.075, 0.928, -0.276, 0.376, -1.029, 0.376, 0.476, 0.727, -0.527, -1.782, 1.581, -1.23, 1.38, -0.879, -1.18, -0.327, -1.33, -0.427, 0.577, 1.129, 1.079, 1.079, -1.079, -1.431, 1.129, -0.577, 0.075, 1.029, 1.029, 0.778, 0.778, -1.631, -0.628, 1.279, 0.376, 1.179, 0.878, -1.28, -0.628, 0.928, 0.727, 0.828, 1.279, 1.129, 0.928, 1.079, 1.179, -1.481, -0.327, 0.978, 0.727, 1.229, 0.376, -1.882, 0.778, -1.732, 0.276, 1.129, 0.476, 1.43, -0.879, 0.226, 0.025, -0.025, 0.376, -0.276, 1.079, -1.23, 0.677, 0.025, 1.029, -1.682, -0.226, 0.627, 0.326, 0.978, -1.732, -0.076, 0.426, -0.076, 0.276, 1.33, -1.029, 0.175, 0.677, 0.928, 0.928, -0.728, -0.628, 0.527, 0.276, -1.732, 0.426, 0.878, -0.728, 0.978, -0.979, -1.33, -1.732, -1.732, 0.627, 0.778, -1.23, 0.778, -1.029, 0.727, -0.327, -0.527, 1.079, 1.53, 0.577, -0.226, -0.527, -0.678, 0.878, 0.677, -1.933, 1.179, -0.427, 0.727, 1.38, 0.125, -1.13, -0.79, 0.142, -1.373, -2.189, 0.434, -1.315, 0.725, -0.441, 1.016, -0.266, -0.965, -0.557, 0.375, 0.608, 0.026, 0.084, 0.725, 1.424, 0.084, -0.557, -1.723, 1.366, -1.373, -0.266, 0.375, -1.606, 1.075, 1.249)
fifaRaw <- c(fifaRaw, 1.424, 0.492, 0.55, 0.2, 0.375, 0.725, -1.198, 0.375, -0.324, 1.133, 0.725, 1.016, 1.249, 0.492, -1.548, -1.315, 0.725, -1.198, 0.317, 0.084, -1.256, 0.55, 0.841, 0.492, -0.79, -1.14, 1.482, -1.897, 1.249, 0.55, -2.247, 0.026, -0.207, 1.133, -0.441, 0.841, 0.084, -0.207, 0.608, -0.324, -0.732, -0.441, -0.324, 1.075, -1.548, 0.259, 1.133, -1.431, 0.259, -1.14, 0.725, 0.958, 0.608, -0.79, 1.016, 0.084, -0.907, 0.958, -0.207, -0.441, -0.557, -0.033, 0.608, 0.783, 0.958, 0.434, 0.55, 0.841, -0.266, -1.14, 1.016, 0.026, 0.084, 0.2, -0.557, -0.149, 0.958, -0.207, 0.492, -0.615, 0.142, -0.149, -2.13, 0.434, 1.366, -2.189, -0.149, 1.657, 0.2, -1.198, 0.317, 0.026, 0.667, 0.375, -1.839, 1.133, -0.674, -0.382, 0.958, 0.9, 0.725, -1.315, -0.091, 0.608, 0.2, -1.14, -0.615, 1.949, -0.79, -2.072, -0.557, 0.142, 0.841, 0.783, 0.667, 0.317, -0.732, 1.133, 1.016, -1.082, 0.375, -0.557, -1.373, -0.033, 0.55, 0.9, 1.191, -0.091, -0.441, 1.482, 0.259, 0.841, 0.667, -0.266, 1.133, 0.259, -1.082, -0.79, -0.033, -1.606, -0.674, -1.14, 0.55, -0.382, -0.091, -1.023, -0.382, 1.191, -0.441, 0.317, 0.434, 0.492, -0.499, -1.839, -0.965, 0.841, 0.026, 1.133, 0.259, 1.308, -0.091, 0.2, 1.133, 1.016, -1.606, -2.305, -0.557, 0.2, -0.615, 0.667, -1.198, -2.538, -1.315, -0.615, 1.133, 0.958, 0.725, 0.084, -0.674, 0.55, 1.424, 0.55, -1.431, 1.016, 0.667, 0.142, 0.667, -1.489, -0.091, 1.075, -0.441, 0.084, 0.783, 0.2, -1.023, 0.375, 2.24, 0.841, -0.732, -1.315, -0.382, -2.189, 0.841, 1.016, 0.375, 1.191, -2.247, -0.382, 1.075, -1.606, 1.191, -0.79, -1.548, 0.026, -0.091, 0.55, 0.841, -0.557, 0.783, -1.373, -0.441, -1.373, -1.489, 1.249, -0.033, 1.716, 0.783, -1.664, 1.075, 0.317, -1.839, 0.434, -2.072, 0.084, -1.606, -0.324, 0.9, 0.317, -0.033, 0.841, 1.308, 0.841, 0.434, 1.949, 1.249, -1.839, -0.033, 0.2, -1.256, 1.133, 0.55, -1.082, 0.026, 1.308, -1.198, -1.606, -0.965, 0.55, 1.075, 1.133, 1.599, -2.189, 0.958, 1.89, 0.841, -0.149, -1.198, -2.305, -1.315, 1.424, 1.308, -1.373, -2.305, 0.667, -0.615, -1.664, 0.2, 0.9, 0.375, 0.084, 0.9, 0.841, -0.499, 0.9, -0.033, 0.608, 1.133, -1.256, -1.14, -0.091, 0.492, -0.207, 0.55, -1.373, -1.606, -0.674, 1.366, 0.492, 0.259, 1.657, -0.674, -0.324, -0.674, -2.13, -0.79, -1.373, -1.664, 0.958, 0.2, -0.324, 0.55, -1.723, 0.375, 1.599, -0.266, -0.091, 1.191, -0.79, -0.324, 0.667, 0.608, 0.725, 1.075, -0.149, 0.608, -0.674, -1.956, -0.965, -0.324, 0.492, -1.198, -1.315, 0.2, 0.841, 0.084, 0.667, 0.55, 0.317, -0.557, 1.774, 1.133, 0.667, -1.664, -0.907, -0.091, -2.014, 0.259, -0.732, 0.259, 0.725, -0.207, 0.317, -0.79, 0.608, 0.841, -0.907, 0.55, -0.091, -1.373, 1.89, 1.016, -1.14, -0.499)
fifaRaw <- c(fifaRaw, 0.783, 0.375, -0.907, 0.783, 1.308, 1.016, 1.075, 0.841, -0.091, -2.305, 0.259, 0.259, 1.075, 1.249, -0.557, 0.725, 1.075, -1.781, 0.142, 0.841, 0.084, 0.841, -0.324, 1.191, 0.841, -0.499, 0.958, -1.198, 1.133, 0.2, -1.082, 0.608, 0.608, -1.023, 0.375, -0.848, 0.725, -0.965, 0.317, -2.422, 0.084, -1.956, -1.606, 0.725, -1.664, 1.191, 0.725, 1.075, 1.191, 1.075, 0.55, 0.841, 1.075, -0.499, 1.308, -1.14, 1.249, -2.014, 0.667, 0.142, 0.667, 1.016, -1.548, -1.256, 0.259, 1.075, 0.492, 1.308, 0.142, -1.723, -0.674, 0.958, -0.207, 0.2, 0.434, -0.033, 0.142, -2.364, -0.848, 0.9, -0.324, 0.841, 1.716, -0.848, -1.14, -1.664, 1.89, -0.033, 0.725, -0.149, 0.55, 0.2, -0.907, 0.026, 0.142, 0.317, -0.033, 0.375, -0.674, -0.149, 0.492, -1.082, -1.198, 0.608, -0.324, 1.482, -0.965, 0.434, 1.075, -1.686, 0.133, -0.537, -1.207, 0.277, -1.351, 0.516, -1.159, 0.085, -0.25, -1.255, 0.324, -0.106, 0.564, -0.968, -0.585, 0.994, 0.324, 0.851, 0.324, -1.351, 0.899, -1.063, 0.037, 0.659, -1.542, -0.681, 0.803, 1.042, 1.138, 0.468, 0.277, 0.516, 0.277, -1.255, -1.063, -0.872, 1.138, 0.851, 1.186, 1.234, 1.329, -1.542, 0.899, 0.899, -0.728, -1.016, 0.803, -1.59, -0.25, 1.377, -0.537, -1.255, -1.542, 0.611, -1.111, 1.09, 0.468, -1.829, -1.255, 0.564, 1.425, -1.494, 0.994, 0.324, 0.851, -0.202, 0.803, -0.872, 0.277, -0.681, -0.728, -1.063, 1.664, 0.946, -1.063, 0.037, 0.516, 1.138, -0.92, 0.946, 0.516, 0.994, 0.277, -1.207, -0.681, 0.181, 0.372, 0.229, 1.234, 1.473, 0.42, 0.994, 0.564, 0.085, -0.968, 0.564, -1.255, 0.899, -1.063, -1.255, 0.564, 0.707, -0.011, 0.994, -0.058, 0.659, -0.25, 1.09, 0.899, -1.159, 0.994, 1.616, -1.638, 0.707, 0.994, 0.372, -1.063, -0.154, -1.159, 0.611, 0.755, -1.733, 0.946, -1.111, 0.085, 1.234, 0.803, 1.09, -1.446, 0.803, 0.468, 0.42, -0.776, -0.489, 0.707, -1.063, -1.686, 0.468, 0.803, 1.042, 0.946, 1.042, 1.138, -1.159, 0.899, 0.659, -1.207, 0.324, -0.633, -1.542, -0.489, 0.851, 1.138, 1.281, 0.181, -1.446, 1.186, 0.803, 1.186, 0.803, -1.016, 0.468, 0.946, -1.59, -0.728, -0.968, -1.207, -1.542, -1.111, 0.611, 0.133, 0.611, -1.159, 0.803, 0.372, -1.063, 0.707, -0.154, 0.803, -0.92, -1.829, -0.776, 0.994, 0.229, -1.398, 1.042, 0.42, -0.441, 0.372, 0.946, 0.564, -1.925, -1.494, 0.181, 0.899, -0.633, 0.659, 1.138, -1.829, -1.638, -0.824, 0.277, 0.851, -0.776, 0.516, -1.542, 0.899, -0.585, 1.09, -1.446, 1.281, 1.09, 0.803, -0.441, 0.085, 0.851, 0.707, 0.324, 0.372, 0.899, 0.085, 0.229, 0.899, 0.946, 0.899, -1.063, -1.063, -1.398, -1.207, 0.707, 0.994, -1.398, 1.186, -1.111, -0.776, -0.537, -1.781, 1.186, -0.824, -1.733, 0.468, -0.633, 0.516, 1.042, -0.872, 0.803, -1.59, 0.372, -1.063, -1.686, 1.09, 0.564, 0.803, 0.946, -1.063, 1.138, 0.133, -0.92, 0.372, -0.537, 0.516, -1.686, -0.346, 0.851, 0.611, 0.755, 0.946, 1.616, 0.659, -1.207, 1.808, -1.111, -1.303, 0.181, 0.803)
fifaRaw <- c(fifaRaw, -1.351, -1.398, -0.776, -1.063, 0.468, 1.377, -1.159, -0.633, -1.351, 0.181, 1.473, 0.899, -0.824, -1.638, 1.042, 1.616, 1.281, -1.111, -1.351, -1.925, -1.016, 1.138, 1.377, -1.255, -1.446, 0.516, 0.277, -1.59, 0.564, 0.755, -0.25, -0.489, 0.803, 0.946, -1.303, 0.611, 0.851, 0.899, 1.09, -1.255, -1.446, 1.042, 0.803, -0.824, 0.707, -1.398, -1.686, -1.494, 1.281, 0.372, -0.25, 1.616, -0.537, 0.324, -0.25, -1.351, -1.111, -1.351, -1.59, 0.564, 0.707, 0.707, 0.755, -1.59, 0.755, 1.281, 0.277, -0.489, 0.803, -1.303, -0.106, 0.899, 0.899, 0.516, 1.234, 0.324, 0.659, 0.229, -1.303, 0.037, -0.393, 1.09, -1.638, -1.59, 0.994, 1.138, 0.468, 0.946, 0.851, 0.372, -0.92, 1.76, 0.946, -1.733, 0.277, -1.59, -1.207, -1.59, -0.106, 0.229, 0.229, 1.09, -0.537, -1.159, -1.303, 0.707, 0.851, -1.063, 0.659, 0.468, -1.686, 0.516, 1.521, -0.585, -0.106, 0.946, 0.707, -0.92, 1.281, 1.09, 1.09, 1.138, 1.329, -0.011, -1.303, -0.728, 0.037, 0.707, 0.707, 0.707, 0.229, 1.09, -1.207, 0.707, 1.09, 0.372, 1.042, -1.063, 0.803, 0.994, -1.063, 0.707, -1.59, 1.569, 0.899, 0.564, 0.324, -0.872, -1.494, 0.755, -0.441, 0.611, -0.968, -0.393, -1.877, 0.659, -1.829, -1.542, 0.946, -1.303, 1.234, 0.899, 1.281, 1.234, 1.234, 0.564, 0.564, 1.281, 0.516, 0.611, -1.303, 0.851, -1.686, 0.659, 0.516, 0.564, -0.298, -0.968, -1.255, -1.686, 0.037, 0.707, -0.537, 0.133, -1.255, -0.011, 0.899, -1.398, 0.755, -0.154, -0.441, 0.181, -1.111, -1.686, 1.281, -1.542, 0.851, 0.468, -1.59, -1.207, -1.207, 2.047, -0.011, 0.611, -0.298, 0.659, 1.281, -1.303, -0.154, -0.872, -1.207, 0.946, 0.803, 0.516, -0.154, -0.489, -1.303, -1.207, 1.856, 0.229, 1.138, -0.25, 0.611, 0.707, 1.227, 0.436, 0.881, -1.738, -0.848, -1.688, -1.293, -0.453, 0.535, 0.584, -0.404, 0.881, 0.683, 0.239, 1.177, 0.14, 1.029, 1.573, 0.98, 0.14, -1.441, 0.337, -1.787, -0.008, 0.09, -1.886, 0.634, 0.733, 0.288, 0.683, -1.342, 0.337, 0.041, -1.243, -1.54, 1.326, 0.337, -0.008, -0.7, 0.584, -1.046, 0.436, -2.034, 0.535, 0.782, 1.128, 1.078, 0.584, -1.836, -0.107, 0.98, 0.535, 0.288, -1.639, 0.98, -1.688, 0.93, -0.404, -2.182, 1.276, 0.387, 0.337, 0.98, 0.634, 0.337, 0.387, 0.14, -1.293, -0.305, 0.683, 0.486, 0.634, 0.14, 1.474, 0.782, -1.738, 0.387, 0.535, 0.189, 1.326, -0.552, -1.194, 0.239, 0.584, -1.787, 0.634, -0.996, -0.404, -0.947, -0.255, -0.947, 0.782, 0.288, 0.337, 0.831, 1.523, -0.996, 1.276, -0.107, 0.387, 0.881, 0.831, 0.683, -1.243, 0.387, -1.243, 0.14, 0.337, -0.552, 0.733, -2.281, -1.243, 0.288, -2.182, -1.342, 0.831, 0.634, -0.206, 0.337, 0.782, -0.601, 0.93, -1.688, 0.733, -1.688, -1.293, -0.255, 0.98, 0.535, -1.738, 0.535, -1.046, -0.206, 0.189, 1.029, 0.239)
fifaRaw <- c(fifaRaw, 0.535, -2.133, -0.996, -0.058, 0.634, 0.041, 1.177, 0.683, 0.436, 0.436, 1.177, -1.836, 1.029, 0.337, 0.535, 0.93, -0.255, -1.145, 0.535, 1.029, 0.881, 1.128, 0.535, -1.54, 0.436, 0.93, -0.305, 0.14, -1.738, 0.831, 0.337, -1.738, -1.886, -1.738, 0.486, 1.078, 0.041, -1.639, -0.157, 0.683, 0.782, 0.683, 0.486, 0.189, -0.157, -2.034, 0.239, -0.354, 0.733, 0.831, 0.288, -0.7, 0.189, -0.157, -0.601, 1.177, -2.034, -1.688, -0.898, 0.634, 1.128, 0.041, 1.029, -2.133, -1.935, -0.008, 0.683, 0.584, 0.98, -0.404, 0.782, 0.98, 1.326, 0.634, 0.239, -0.947, 0.683, -0.157, 1.474, 0.387, 0.733, -1.293, -0.255, -0.157, 0.782, -0.799, 0.288, -0.7, 0.387, -1.095, 0.98, 0.486, 0.189, -2.232, 0.387, 0.535, 0.634, 0.436, -1.935, 0.634, 1.029, -2.232, -0.651, 0.387, -2.034, 0.041, 1.128, 0.535, 0.387, -0.206, -1.046, -0.206, -0.354, 0.436, 0.93, -0.157, -1.046, 0.14, -1.293, 0.337, -0.947, 0.881, -2.182, 0.634, -0.058, -1.392, -2.083, 0.14, 0.09, 0.387, 0.93, 0.881, 1.029, 0.436, 0.535, -1.243, 1.523, -1.935, 0.239, -0.601, 0.09, 0.535, 1.375, -1.886, -0.799, 1.177, -1.985, 1.326, 1.523, 1.029, 0.733, 0.782, 1.622, -2.133, 0.486, 0.98, 1.227, 0.288, -1.589, -2.133, 0.09, 0.782, 0.239, 0.535, -1.441, 1.82, 0.337, -2.232, 0.337, -1.194, 1.029, 1.474, -1.046, -0.749, 0.93, 0.239, -0.996, 1.523, 0.634, 0.387, 0.189, 0.683, 0.584, 0.239, -0.7, 0.683, -1.886, 0.782, -0.848, 1.573, 0.09, -0.058, 1.078, -1.342, 0.634, -1.836, 0.881, 0.288, -1.886, 0.337, 0.881, -1.441, 0.189, -2.182, -0.552, -0.848, 0.782, 0.881, 0.041, 0.486, -1.342, 0.14, 0.634, 0.584, 1.128, -0.058, 0.337, 1.029, -1.589, -0.354, 0.337, -0.601, -1.639, 0.189, 1.078, 0.486, -0.7, -0.305, 0.634, 0.041, 1.227, -0.404, 0.486, 0.831, 0.831, -1.441, 0.288, -1.886, 0.535, -0.058, 0.486, 1.177, 1.029, 0.189, 0.733, -1.342, -0.255, 0.189, 0.14, -1.145, -1.738, 0.93, -1.441, 1.078, -0.305, 0.041, -0.255, -1.935, -0.206, -0.157, 0.337, 0.436, 0.782, -0.404, -1.688, 1.424, 0.288, -0.058, 0.881, 0.733, 0.337, 1.128, -1.935, 0.436, 0.733, 0.733, 0.881, 1.177, -0.947, -0.404, 0.239, 0.683, 1.029, 0.93, 0.337, 0.584, 0.535, 1.029, -1.738, 0.634, 1.128, 0.782, 1.128, 0.09, -2.232, 0.387, -2.083, -0.255, 0.881, 0.98, 1.029, -1.293, 0.535, -0.404, 0.189, 0.288, -0.206, 0.881, -0.996, -0.157, 0.387, 1.177, -1.935, 0.239, 0.93, 0.436, 1.177, -1.589, -1.836, 0.337, -0.453, 0.337, 1.128, -1.194, -0.008, 0.782, 0.337, 1.227, -1.342, -0.749, 0.93, 0.387, -1.738, 0.436, 1.227, 0.584, 0.634, -0.947, -1.886, -1.787, -1.589, 0.535, 0.782, -0.898, 1.276, 0.09, 0.189, 0.733, -0.354, 1.276, 1.128, 0.782, -0.848, -1.54, -0.354, 1.177, 0.584, -1.738, 0.337, -0.354, 0.288, 1.029, 0.041, -1.095, 0.517, 0.517, 0.731, -1.907, 0.374, -0.838, -1.123, 0.16, 0.089, 1.016, 0.517)
fifaRaw <- c(fifaRaw, 0.232, 0.374, 0.588, 1.586, -1.693, 1.658, 1.301, 1.016, -0.41, -1.693, 0.588, 0.374, -1.693, -0.125, -0.41, 0.588, 0.731, 0.873, 1.515, -1.337, -0.766, -1.265, -1.479, 0.089, 0.659, -0.053, -0.909, -1.194, 0.873, -1.693, 0.802, -0.553, 1.016, 0.374, 0.731, -0.053, 1.158, -3.119, 0.945, 1.8, 0.446, 0.873, -1.622, 0.588, -0.624, 0.659, -1.337, -0.838, 0.873, 0.731, -0.125, 0.374, -0.053, 0.089, 0.945, 0.16, -1.551, 0.802, 0.232, 0.089, -0.339, 0.517, 2.299, 0.517, -1.052, 0.374, 1.016, 0.588, 0.16, -0.695, -1.622, -1.052, -1.194, 0.446, 0.659, -1.836, -0.481, -1.693, 0.374, 0.089, 0.089, -0.766, 1.23, 0.945, 1.301, -1.693, 1.301, -0.41, -0.909, 0.303, 1.087, 0.16, -1.265, -1.052, -0.41, -0.41, 0.018, -0.98, 1.444, -1.622, -1.693, 1.515, -1.479, -1.337, 0.446, 0.659, -0.909, 0.588, -0.196, 0.303, 1.016, -1.479, 1.158, -1.408, -1.693, -0.553, 1.586, 0.517, -1.907, 0.089, -1.622, -1.337, 0.089, 0.945, 0.16, 0.16, -2.905, -1.123, -1.265, 0.802, 0.588, 1.8, 0.945, -0.481, 0.018, 1.515, -0.98, 0.945, 0.588, -0.053, 1.444, -0.125, -0.909, 1.016, 0.517, 0.374, 1.158, 0.945, -2.263, 0.659, 0.303, -0.053, -0.053, 0.802, 0.802, 0.303, -0.267, -2.62, -0.267, 0.802, 0.945, -1.123, -1.836, 0.018, 0.446, 0.303, 1.087, -0.196, 0.588, 0.232, -1.551, 0.446, -0.695, 0.303, 0.232, 0.802, -1.622, 0.303, -0.481, -1.408, 1.158, -1.693, -0.766, -1.836, 1.087, -0.053, -1.052, 1.372, -0.125, 0.588, 0.16, 0.659, 0.303, 1.016, -0.053, 0.517, 1.301, 0.873, 1.515, 0.089, -0.339, -0.339, -0.481, 1.943, 0.873, 0.446, -1.194, -0.553, -0.624, 0.873, -1.052, 0.731, -1.123, 0.16, -1.408, 1.515, 0.517, 0.446, -1.337, -0.553, 0.232, 0.303, 1.444, 0.873, 0.802, 1.301, -1.907, -0.838, 0.232, -1.052, 0.588, 1.372, 0.802, 1.016, 0.446, -0.98, -0.267, -1.123, 0.588, 0.303, 1.087, -0.267, 0.374, -1.764, 0.16, -0.909, 1.158, -1.408, 0.945, -0.196, -1.551, -0.624, -0.41, 0.446, -0.339, 1.158, 1.016, 1.372, 0.374, -0.909, -1.907, 0.802, 0.089, 0.588, -2.121, -0.267, 0.802, 1.729, 0.16, -0.624, 1.158, -1.622, 1.016, 0.731, 0.945, 0.089, 1.087, 1.23, -0.838, 1.016, 1.301, 1.301, -0.053, 0.232, -1.764, -0.481, 1.087, 0.873, -0.624, -2.05, 1.943, -0.41, -1.978, 0.018, -1.337, 1.586, 2.37, -0.481, -0.053, -1.194, -1.551, -1.693, 1.586, 0.659, 0.089, -0.481, 0.089, 0.802, -0.053, -1.622, 1.016, -0.909, 0.018, -0.624, 1.586, -0.766, -0.553, 0.588, -1.194, 0.16, -1.265, 1.016, -0.196, -0.41, 0.303, 0.303, -1.265, 0.873, -1.479, 0.303, -1.337, 0.802, 1.515, -0.053, 1.372, -1.836, 0.303, 0.588, 0.731, 1.158, 0.089, 0.731, 1.444, -0.909, -0.196, 1.016, -0.624, -1.978, -0.41, 1.23, 0.018, -0.125, -1.408, 1.087, -0.695, 1.301, 0.16, 1.016, -0.553, 0.945, -2.05, -0.053, -0.125, 0.873, -0.553, 0.731, 0.731, 0.659, -0.481, 0.659, -1.265)
fifaRaw <- c(fifaRaw, 0.089, 0.446, -0.838, -0.339, -1.337, 1.586, -2.05, 1.087, -1.194, 0.303, -0.624, -1.408, -1.123, -0.339, -0.909, -0.339, 1.301, -0.909, -1.337, 0.731, 0.018, -0.267, 0.446, 1.016, 0.802, 1.087, -1.194, -1.123, 0.659, 0.945, 0.802, 0.018, -1.479, 0.089, -0.766, 0.446, 0.303, 1.444, 0.802, -0.196, 0.873, 0.517, -0.339, 0.374, 1.016, 0.374, 1.729, 0.588, -1.551, 0.945, -0.766, -0.267, 0.873, -0.553, 1.372, -1.337, 0.873, -0.41, -1.337, -0.267, 0.16, 1.016, -1.337, -0.196, -0.053, 1.158, -1.408, -0.196, -0.553, 0.303, 0.802, -0.766, -0.267, -0.553, -0.41, -0.125, 0.374, -0.624, -0.41, 0.731, 0.232, 0.374, -1.052, -0.267, 0.374, 0.802, 0.731, -0.053, 0.945, -0.695, 0.446, -1.194, -0.339, -1.123, -1.693, 0.659, 1.301, -1.265, 0.659, 0.16, 1.23, 0.446, 0.089, 1.016, 1.586, 0.446, -0.196, -1.907, -0.98, 0.16, 0.303, -1.265, 0.303, -0.053, 0.945, 1.871, -0.766, -1.836, 1.026, 0.712, 0.712, -1.606, 0.086, -1.042, -0.854, 0.211, -0.165, 0.023, -0.039, 0.838, -0.29, 0.587, 0.336, 0.462, 2.154, 0.838, 0.838, -1.042, -1.543, -0.603, -1.167, -0.729, -0.729, -1.543, 1.088, -1.105, 1.339, 0.712, -0.353, -0.165, -0.165, -0.353, -1.731, 0.336, -0.541, -1.23, -0.917, 0.587, -0.478, 0.274, -0.791, -0.165, -0.039, 0.399, 0.712, -0.165, -0.979, 0.399, 1.527, 0.086, -0.165, -1.857, 0.65, -1.105, 0.023, -0.729, -0.729, 1.339, -0.039, -0.29, 1.715, 0.9, 0.211, 0.023, -0.666, -0.541, 0.274, 1.214, 1.026, 0.65, 0.524, 1.59, 0.462, -2.233, 0.274, 0.336, -0.102, 1.088, -0.165, -0.541, -0.227, -0.478, -1.481, 1.402, -0.854, -0.666, -0.854, 1.402, -0.039, -0.227, -0.039, 0.462, 1.214, 1.652, -0.478, 1.59, -0.165, 0.65, 0.9, -0.29, -0.039, -1.418, -1.293, -0.791, -0.541, 0.524, -0.729, 1.778, -2.358, -0.666, 0.462, -2.358, -0.791, -0.165, 0.712, 0.838, 1.026, 0.838, 0.712, 1.026, -1.355, 0.775, -2.045, -0.979, 0.023, 0.963, -0.165, -1.606, -0.729, 0.211, -0.039, 0.336, 0.963, -0.165, 0.399, -2.233, -0.666, -0.603, -0.227, -0.039, 0.336, 0.838, 0.65, -0.478, 0.9, -1.481, 0.838, 0.838, 0.775, 1.339, -0.227, -1.167, 0.023, 0.775, 0.399, 1.652, 0.211, -1.105, -0.165, 0.399, -0.415, -0.603, -1.606, 0.838, 1.402, -1.418, -1.543, -2.107, 0.65, 0.65, -0.478, -1.543, -0.791, 0.462, 0.086, -0.353, 0.086, 0.274, -0.227, -2.107, -0.039, -0.165, -0.353, 1.276, 1.276, -0.227, -0.478, 0.524, -0.979, 1.088, -2.233, -1.919, -0.478, 0.65, 1.402, -0.29, -0.478, -2.17, -1.418, 0.086, 0.587, -0.353, 1.339, 0.149, 0.963, 1.339, 1.088, 1.214, 1.214, -0.415, -0.165, 0.211, 1.966, 0.023, -1.042, -0.791, -0.541, -0.854, 0.838, -0.791, 0.524, 1.715, 1.84, -0.729, 0.838, 1.151, 0.149, -2.045, -0.415, -0.603, 1.151, 0.775, -1.794, 0.399, 1.464, -2.17, 0.086, 0.462, -1.355, -0.227, 1.402, -0.29, -0.039, 0.775, 0.274, 0.9, -0.729, 0.399, 0.211, -0.102, -1.105, 0.399, -0.039, 0.274, -0.478, 1.527, -2.233, -0.353, -0.102)
fifaRaw <- c(fifaRaw, -0.102, -1.982, 0.149, -0.102, -0.791, 0.65, 0.462, -0.039, 0.149, 0.462, -0.603, 1.088, -1.982, -0.102, -0.979, 0.524, 0.524, 1.276, -1.355, -0.791, 1.151, -1.481, 1.088, 1.778, 1.402, -0.791, 1.464, 2.216, -1.857, 0.587, 0.9, 1.84, 0.712, -2.233, -1.857, 1.214, 0.023, 0.023, 0.775, -1.669, 1.464, 0.149, -1.794, 0.023, -0.102, 1.966, 1.652, -0.729, -0.415, 0.399, -0.353, 0.086, 1.339, 0.023, -0.102, -0.102, -1.105, 1.026, 0.274, -0.666, 0.838, -1.105, 0.023, -0.227, 1.464, -0.039, -0.541, 1.402, -0.478, 0.9, -1.481, 0.211, 1.214, -1.857, -0.227, -0.478, -0.854, 0.587, -2.233, 0.65, 0.023, 0.775, 0.524, -0.917, 1.276, -0.854, -0.165, -1.293, -0.541, 0.712, -0.603, -0.165, 1.151, -1.418, -0.603, -0.165, -0.165, -1.606, 0.712, 1.464, 1.339, -0.227, -0.478, 1.464, -0.165, 0.462, 0.211, 1.214, 1.088, 1.339, -1.669, 0.9, -2.358, 0.023, -0.854, 0.211, 0.211, 1.276, -0.666, 1.214, -0.854, -0.415, -0.102, 0.524, -0.227, -1.543, 2.028, -0.917, 0.838, -0.917, -0.478, -0.478, -0.165, 0.023, 0.336, -0.603, -0.29, 0.211, -1.042, -1.543, 0.462, 0.211, -0.666, 0.023, 1.214, 0.65, 1.715, -1.731, -0.791, -0.415, 0.838, 0.399, 0.587, -0.478, 0.149, 0.9, 0.587, 1.026, 2.216, 0.336, 0.65, 1.088, 0.775, -1.982, -0.165, 1.088, 0.023, 1.026, 0.211, -0.415, -0.791, -1.418, 1.026, 1.652, 1.214, 0.211, -0.666, 0.712, -0.227, 0.838, -0.039, -0.29, -0.227, -0.165, -0.415, -0.541, 0.963, -1.418, -0.478, 0.462, -0.165, 0.963, -2.233, -1.481, 0.775, -1.23, 0.211, 1.402, -0.666, 0.65, 1.088, -0.165, 1.214, -0.165, -0.29, 1.276, -0.353, -1.669, 1.088, 0.336, -0.039, 0.524, -0.729, -1.042, -1.982, -2.045, 1.715, 1.652, -0.541, 0.963, -1.919, 1.402, -0.039, 0.149, 2.028, 1.088, -0.478, -1.105, -0.603, -0.478, 1.026, 0.462, -2.421, 1.276, -0.541, 0.086, 1.402, -0.478, -0.979, 0.801, 0.31, -0.673, -1.573, -0.918, 0.064, -0.263, 0.392, 0.31, 0.392, 0.146, 0.555, -0.263, 0.31, 1.292, -0.673, 1.702, 2.029, 0.555, -0.591, -1.655, 0.637, 0.064, -1.901, 0.064, -0.345, 0.637, 0.473, 1.128, 1.702, -0.837, -1.328, -0.345, -0.918, -0.837, 1.128, -0.018, 0.637, 0.228, 1.128, 0.883, 0.064, 0.555, 0.146, 0.637, 0.637, 0.473, 0.883, -0.182, -0.345, 0.392, -0.837, -0.018, -1.41, 0.473, -1.819, 0.473, -0.427, -1.655, 0.31, 0.473, 0.555, 0.31, 0.473, 0.064, -0.1, -0.018, -1.082, 0.146, 0.146, -0.182, 0.392, -0.018, 2.193, 0.146, -0.509, 0.473, -0.918, 0.965, 1.047, -0.263, 0.473, -0.182, -0.182, 0.064, 0.228, -1.328, -1.655, -1.41, 1.128, 1.21, 0.637, 0.555, 0.31, 0.146, 1.783, -1.164, 1.538, -0.345, 0.31, 0.146, -0.509, 0.228, -0.837, -0.427, -0.591, -0.1, 0.146, -0.345, 1.292, -2.638, 0.637, 1.538, -2.965, -0.918, 1.128, 0.555, -1.655, -0.1, 0.228, 0.719, 1.047, -1, 0.473, -1.082, -1.328, 0.883, 0.719, 1.538, -1.082, 0.392)
fifaRaw <- c(fifaRaw, -0.918, -0.427, -0.755, 1.865, 0.31, -0.263, -3.539, -1.819, 0.228, 0.801, 0.473, 1.947, 0.801, -1.328, -0.345, 0.883, -0.182, 0.883, -0.1, -1, 1.456, -0.182, -0.427, 1.62, 0.064, 0.801, 1.374, 0.555, 0.473, 0.228, 0.637, -0.263, 0.637, 0.31, 1.047, -0.755, -0.345, -3.702, -0.182, 0.555, 1.21, -1.246, -0.263, -0.837, 0.31, 0.31, -0.018, -0.591, 0.392, -0.182, -2.228, -0.755, 0.555, 0.228, -0.345, 0.555, -0.182, -1.737, -1.328, -0.018, 1.128, -1.737, -2.147, -1.246, 0.228, 0.392, -0.509, -0.1, -0.182, -0.591, -0.509, 0.31, 0.228, 1.374, -1.164, 0.146, 1.538, 0.965, 1.374, -0.673, 1.128, 0.31, 0.555, 1.374, 0.392, 0.555, -0.018, -1.41, -0.755, 0.637, -1.164, 0.064, -1.082, 1.538, -1.082, 0.31, 0.555, -0.018, -2.147, 0.31, 0.883, 0.31, 1.292, -0.1, 0.31, 1.292, -2.31, 0.555, -0.182, -0.263, -0.1, 1.047, -0.018, 0.228, -0.263, 1.047, -0.918, -1.655, -0.1, 0.473, 0.883, -0.755, 0.392, -0.1, -1.41, 0.883, 0.965, -2.147, 0.146, -1.328, -0.673, -2.147, -1.41, -0.263, 0.146, 0.392, 0.637, 1.62, -0.182, -0.427, 1.21, 1.292, -1.901, 0.31, -0.427, -1, 0.965, 1.538, -0.1, -0.182, 1.783, -0.673, 1.128, 0.637, 0.801, 1.702, 0.146, 1.702, -2.802, 0.31, 2.111, 1.783, -0.755, 0.31, -1.573, -1.246, 0.965, 1.128, -0.591, -1.655, 1.292, -0.755, -2.802, -0.427, -0.755, 0.637, 1.947, 0.228, -0.345, 1.21, 0.228, -0.018, 1.62, 0.719, -1.082, -1.246, 0.228, 0.392, -0.673, -0.509, 0.31, -1, 0.637, 1.292, 0.883, -1.41, 0.883, 0.719, -1, 0.228, -1.983, 0.473, -0.263, -2.638, -0.427, -0.673, -0.263, 0.637, -2.556, 0.473, -0.1, 0.883, 0.473, -0.182, 1.128, -0.673, 0.228, 0.31, -0.591, 1.62, -1.328, 0.392, 0.064, -0.837, -1.655, -0.1, 0.064, -0.182, -1.082, 0.883, 0.637, -0.755, 0.801, 0.31, -1.082, 1.047, 1.374, 1.047, 0.719, 0.064, -0.755, -0.755, -0.918, 0.555, -1.41, -0.755, 0.392, 0.801, -0.755, -0.509, -1.246, 1.128, -0.427, -1.082, -1.492, -2.802, 0.31, 1.21, 1.21, -1.41, 0.555, -0.1, -1.082, 0.146, 0.719, -0.263, 0.473, 0.473, -1.246, -2.392, 1.292, -0.1, -0.673, 0.801, 0.801, 0.392, 0.719, -2.883, 0.555, 0.31, -0.182, 0.883, 0.228, 0.555, 0.719, -0.837, 0.473, -0.755, 1.783, 0.637, -0.182, 0.473, 0.965, -1, -0.018, 0.719, 0.801, 1.374, -0.263, -1.655, 0.064, -0.018, -1.246, 1.047, 0.392, 1.456, 0.146, 0.555, 1.21, 0.555, -0.509, -0.345, 1.456, -1.082, 0.228, -0.427, 1.047, -0.673, -0.1, 0.555, 0.555, 1.047, 0.31, 0.555, -0.918, 0.392, -0.673, 0.883, -1.164, -0.591, 0.146, 0.392, -0.427, -0.591, -1.082, -0.1, 0.146, 0.801, -0.263, 0.801, -1.082, 0.392, -0.1, -0.345, -0.918, -2.72, 1.947, 0.473, -0.018, 0.883, 0.146, -0.018, -0.673, -0.018, 1.62, 1.456, -1.082, 0.228, -0.345, -0.755, 0.555, 0.555, -1.655, 1.128, -0.837, 0.965, 1.783, -0.182, -1.164, 0.232, -0.472, -0.522, -1.428, 0.232, -1.377, 0.836)
fifaRaw <- c(fifaRaw, -0.774, 0.283, 0.836, -0.22, -0.371, 0.484, 0.685, -0.874, -0.824, 0.937, 0.081, 0.937, 0.383, -1.88, 0.031, -1.981, -0.422, 0.534, -1.73, -0.623, 0.635, 0.635, 1.138, 0.383, 0.383, -0.12, 0.685, -1.88, -0.522, -1.428, 1.289, 0.987, 1.238, 1.289, 0.635, -1.528, 0.635, 1.037, -0.925, -1.025, 0.635, -1.78, -0.874, 1.138, -0.422, -0.371, -1.78, 0.534, -1.629, 1.037, 0.434, -1.78, -0.874, 0.283, 1.49, -0.723, 1.138, 0.434, -0.623, -0.573, 0.786, -0.371, 0.484, -1.227, -0.723, 0.031, 1.238, 1.037, -1.88, -0.17, 0.735, 1.037, 0.333, 1.238, 0.735, 0.735, -0.673, -1.478, -0.774, 0.031, 0.434, 0.484, 1.238, 1.289, 0.283, 0.635, 0.132, -0.573, -0.522, 0.434, 0.735, 1.188, -1.478, -1.176, 0.635, 0.584, -0.17, 0.584, 0.333, 0.081, 0.182, 1.087, 0.735, -1.629, 1.188, 1.037, -1.981, 0.735, 0.735, -0.12, -0.673, -1.579, -0.673, 0.685, 0.283, -0.975, 1.037, -1.277, 0.182, 1.138, 0.937, 1.44, -1.83, 0.232, 0.584, 0.735, -0.472, -0.623, 0.182, -1.377, -1.981, -0.371, 1.037, 1.037, 0.735, -0.019, 0.735, -0.774, 0.836, 0.534, -1.73, -0.422, -0.422, -0.925, -1.377, 0.987, 1.138, 1.44, 0.534, -1.176, 1.339, 0.735, 1.238, 0.685, -0.925, 0.132, 1.138, -1.528, 0.383, 0.031, -1.377, -1.78, -1.126, 0.836, -0.975, 0.333, -1.931, 0.685, 0.685, -0.673, 0.283, -0.774, 0.735, -1.327, -1.377, -0.623, 1.087, 0.232, -1.327, 0.685, 0.735, -0.774, 0.534, 1.138, -0.371, -2.082, -1.629, 0.383, 0.383, -0.12, 0.786, -0.22, -1.327, -1.327, -0.371, 0.534, 0.635, -0.925, 0.484, -1.73, 0.383, -0.271, 1.238, -1.377, 1.087, 1.087, 0.685, -1.126, -0.371, 0.685, 0.987, 0.584, 0.333, 0.987, 0.232, -0.12, 0.886, 1.339, 0.886, 0.182, -0.925, -0.975, -2.132, 0.836, 1.087, -0.573, 1.641, -1.73, 0.081, 0.434, -1.679, 1.087, -0.874, -1.83, 0.031, -0.774, 0.987, 1.037, 0.383, 0.786, -1.126, 0.182, -0.522, -1.377, 1.138, 0.836, 0.937, 1.138, -0.321, 1.289, 0.232, -1.931, 0.786, -0.17, 0.031, -1.78, -0.975, 0.937, 0.635, 0.484, 0.534, 1.641, 0.635, -1.126, 2.043, -0.522, -1.629, 0.232, 0.584, -1.428, -0.673, 0.283, -1.277, 0.685, 1.842, -1.528, -1.227, -1.277, 0.534, 1.289, 0.886, 0.584, -1.981, 0.735, 1.389, 0.987, -0.12, -1.227, -1.83, -0.472, 1.238, 1.037, -0.623, -1.428, 0.383, 0.534, -2.082, 0.383, 0.836, 0.534, -0.774, 0.685, 0.886, -0.925, 0.937, 0.836, 0.434, 1.037, -0.573, -0.925, 0.836, 0.635, -1.428, 0.584, -1.277, -1.78, -0.12, 1.339, -0.774, -0.371, 1.389, -1.126, 0.584, -0.975, -1.679, 0.685, -0.925, -1.377, 0.333, 0.584, 0.383, 0.031, -1.78, 0.937, 1.238, -0.12, -0.673, 1.087, -1.428, -0.07, 0.886, 1.289, -0.07, 1.238, 0.283, 0.836, 0.031, -1.88, -0.22, -0.07, 0.987, -1.88, -1.025, 0.534, 1.238, 0.333, 0.886, 0.836, 1.087, -0.019, 1.641, 0.937, -1.629, -0.07, -1.78, 0.383, -1.88, -0.12, 0.132, 0.333, 1.238, -0.824, -0.371)
fifaRaw <- c(fifaRaw, -0.422, 0.735, 0.937, -0.522, 0.886, 0.383, -1.629, 0.685, 1.44, -1.327, 0.383, 1.087, 0.735, -1.83, 1.49, 0.786, 0.937, 1.389, 0.886, 0.333, -1.428, 0.534, -0.371, 0.534, 0.635, 0.031, 0.484, 1.238, -1.377, 0.735, 1.087, -0.12, 0.786, -0.874, 0.987, 1.238, -0.975, 0.836, -0.12, 1.641, 0.635, 0.635, 0.383, -0.623, -1.579, 0.635, 0.132, 1.037, -0.774, -0.07, -1.931, 0.383, -1.83, -1.176, 0.886, -1.327, 1.087, 0.635, 1.339, 1.339, 0.886, 0.836, 0.534, 1.238, 0.484, 0.685, -1.327, 0.584, -1.629, 0.987, -0.422, 0.786, -0.321, -1.277, -1.83, -1.78, 0.383, 0.232, -0.472, -0.522, -1.277, -0.321, 1.188, -0.422, 0.635, 0.635, -0.371, 0.081, -1.478, -1.478, 1.289, -1.025, 0.836, 0.081, -1.83, -1.78, -1.428, 1.993, -0.874, 0.484, -0.019, 0.735, 0.635, -1.428, -0.17, -1.227, -1.126, 0.534, 0.735, 0.534, 0.081, -0.573, -0.22, -1.679, 1.138, 0.534, 1.188, 0.132, 0.434, 0.937, -1.551, -0.023, -1.506, -1.281, 0.382, -1.641, 1.011, -0.697, 0.292, 0.337, -0.292, -0.068, -0.337, 0.786, -0.517, -1.641, 0.921, 0.292, 0.427, 0.337, -1.371, 0.696, -1.461, 0.247, 0.651, -1.551, -0.292, 0.966, 0.831, 1.011, 0.606, 0.696, 0.786, 0.831, -1.551, -0.922, -1.012, 1.281, 0.966, 1.146, 1.236, 0.741, -1.416, 0.292, 0.921, -1.057, -0.832, 0.786, -1.326, -0.248, 1.325, -0.742, -0.292, -1.461, 0.606, -1.596, 1.056, 0.786, -1.686, -1.191, 0.516, 1.325, -1.551, 1.056, 0.292, 0.786, 0.067, 1.281, 0.067, 0.831, -0.967, -0.292, -0.787, 1.415, 0.921, -1.641, -0.337, 0.337, 0.921, -0.967, 0.921, 0.831, 0.831, 0.247, -1.371, -0.697, 0.472, 0.561, 0.516, 1.191, 1.236, 0.651, 0.561, 0.921, 0.067, -0.967, 0.831, -1.236, 0.876, -0.877, -1.461, 0.651, 0.786, 0.472, 0.831, 0.741, 0.561, -0.113, 0.966, 0.696, -1.731, 1.146, 1.325, -1.551, 0.876, 1.056, 0.247, -0.742, -1.101, -1.506, 0.741, 0.786, -1.506, 0.741, -1.641, 0.472, 1.146, 0.651, 1.191, -1.461, 0.696, 0.247, 0.741, -1.326, -0.877, 0.786, -0.922, -1.371, 0.382, 0.921, 1.101, 1.101, 0.561, 0.966, -0.877, 1.236, 0.921, -1.461, -0.472, -0.337, -1.506, -0.517, 1.011, 1.011, 1.236, -1.236, -1.101, 1.146, 0.696, 1.281, 0.696, -1.146, 0.831, 0.876, -1.596, -0.742, -1.012, -1.416, -1.596, -1.506, 0.696, -0.517, 0.876, -1.686, 0.741, 0.292, -1.236, 0.292, -0.607, 0.696, -0.877, -1.641, -0.697, 1.101, -0.472, -1.191, 0.876, 0.696, -0.517, 0.292, 1.011, 0.202, -1.596, -1.461, 0.741, 0.921, -1.101, 0.741, 1.011, -1.461, -1.461, -0.517, 0.516, 0.831, -0.697, 0.427, -1.596, 1.011, -0.832, 0.786, -1.146, 1.101, 1.146, 0.606, -1.057, 0.112, 0.876, 0.876, 0.472, 0.516, 0.966, 0.382, 0.292, 0.786, 1.325, 0.921, 0.022, -1.326, -1.281, -1.551, 0.561, 1.056, -1.461, 1.37, -1.236, -0.248, -0.158, -1.506, 1.146, -0.158, -1.506, 0.382, 0.112, 0.651, 1.011, -0.517, 0.921, -1.641, 0.696)
fifaRaw <- c(fifaRaw, -0.742, -1.506, 1.101, 1.011, 0.921, 0.966, -0.203, 1.101, 0.247, -1.596, 0.427, -1.146, 0.696, -1.551, 0.022, 0.696, 0.786, 0.561, 0.651, 1.46, 0.696, -1.461, 1.73, -1.146, -1.506, 0.516, 0.786, -1.461, -1.326, -0.382, -1.641, 0.561, 0.876, -1.641, -0.967, -0.472, 0.157, 1.46, 0.966, -0.832, -1.641, 1.056, 1.191, 1.191, -1.012, -1.551, -1.506, -0.922, 1.281, 1.325, -0.967, -1.461, 0.651, 0.472, -1.641, 0.651, 0.921, -0.697, -0.517, 1.011, 0.831, -1.326, 0.921, 0.696, 0.651, 1.011, -0.877, -1.686, 0.966, 0.786, -1.057, 0.606, -1.191, -1.641, 0.516, 1.281, -0.158, -0.292, 1.46, -1.146, 0.741, -1.416, -1.416, -0.967, -0.832, -1.551, 0.831, 0.606, 0.786, 0.561, -1.641, 0.831, 1.056, 0.292, -0.877, 1.191, -1.191, 0.472, 0.786, 1.101, 0.561, 1.056, 0.382, 0.561, -0.472, -1.506, -0.158, 0.337, 0.786, -1.641, -1.371, 0.831, 1.146, 0.561, 1.146, 0.472, 0.382, -1.281, 1.64, 0.741, -1.236, -0.292, -1.416, -0.967, -1.596, -0.068, 0.247, 0.247, 1.011, -0.697, -0.742, -1.146, 0.651, 0.786, -0.248, 0.606, 0.472, -1.551, 0.516, 1.37, -0.787, 0.202, 1.281, 0.741, -1.596, 1.37, 1.146, 0.606, 1.415, 1.236, 0.022, -1.416, -0.697, 0.022, 0.651, 0.831, 0.337, 0.247, 1.325, -1.506, 0.651, 0.876, 0.651, 0.651, -1.551, 1.056, 1.191, -1.012, 0.651, -1.326, 1.37, 0.831, 0.651, -0.607, -0.652, -1.461, 0.472, -0.113, 0.741, -0.248, -0.158, -1.506, 0.651, -1.686, -1.236, 0.876, -1.101, 1.191, 0.741, 1.146, 1.011, 0.786, 0.831, 0.651, 1.46, 0.606, 0.606, -1.281, 0.921, -1.506, 0.831, -1.506, 0.966, -0.742, -1.146, -1.461, -1.326, 0.292, 0.651, -0.562, -0.292, -1.461, -0.652, 0.651, -1.101, 0.876, -0.248, -0.113, 0.112, -1.596, -1.012, 1.236, -0.922, 0.831, 0.516, -1.012, -1.461, -1.506, 2, -0.248, 0.606, -0.697, 0.651, 0.561, -1.236, -0.203, -1.146, -1.057, 0.921, 1.056, 0.921, 0.337, -0.562, -1.371, -1.686, 1.37, -0.248, 1.011, -0.113, 0.696, 0.516, -1.516, 0.053, -1.331, -1.377, 0.837, -1.562, 0.976, 0.099, 0.376, 0.56, 0.145, -0.685, -0.731, 0.699, -0.593, -1.608, 0.929, -0.316, 0.237, 0.237, -1.47, 0.791, -1.377, 0.053, 0.837, -1.423, 0.791, 1.022, 0.653, 0.791, 0.468, 0.745, 0.883, 0.699, -1.285, -0.962, -1.1, 1.206, 0.883, 0.976, 1.252, 0.422, -1.562, 0.883, 1.114, -1.1, -1.1, 0.653, -1.562, -0.778, 1.483, -0.778, 0.007, -1.562, 0.33, -1.377, 1.252, 0.745, -1.654, -1.562, 0.653, 1.621, -1.331, 1.16, -0.039, 0.33, -0.178, 0.976, -0.501, 0.745, -0.962, -0.501, -1.054, 1.068, 0.791, -1.562, -0.039, 0.468, 1.022, -1.562, 0.883, 0.837, 1.022, 0.468, -1.47, -0.731, 0.237, 0.33, 0.468, 1.206, 1.252, 0.653, 0.837, 0.929, 0.191, -1.193, 0.883, -1.147, 0.929, -0.916, -1.285, 0.653, 0.791, 0.468, 0.976, 0.468, 0.56, -0.178, 0.929, 0.284, -1.516, 1.022, 1.298, -1.47, 0.699, 1.252, 0.007, -1.147, -1.285)
fifaRaw <- c(fifaRaw, -1.423, 0.745, 0.653, -1.516, 0.791, -1.562, 0.284, 1.206, 0.422, 1.16, -1.239, 0.606, 0.284, 0.929, -1.516, -1.008, 0.699, -0.778, -1.377, 0.376, 0.929, 1.114, 0.791, 0.376, 1.022, -1.008, 1.252, 0.976, -1.608, -0.455, -0.316, -1.147, -1.147, 1.022, 0.976, 0.837, -1.054, -0.962, 0.883, 0.653, 1.298, 0.745, -1.193, 0.883, 0.929, -1.562, -0.731, -0.87, -1.47, -1.47, -1.562, 0.653, -0.547, 0.791, -1.654, 0.791, 0.284, -1.285, 0.284, -0.824, 0.745, -0.316, -1.47, -0.593, 1.114, -0.501, -1.47, 1.068, 0.745, -0.224, 0.284, 1.068, 0.237, -1.47, -1.331, 0.791, 0.837, -1.1, 0.883, 0.33, -1.516, -1.423, -0.27, 0.237, 0.791, -0.87, 0.468, -1.516, 0.883, -0.824, 0.791, -1.331, 1.068, 1.391, 0.699, -0.916, 0.099, 1.022, 0.929, 0.653, 0.56, 0.929, 0.56, -0.132, 0.745, 1.345, 0.976, -0.362, -1.377, -1.193, -1.47, 0.699, 1.16, -1.193, 1.298, -1.516, -0.039, -0.178, -1.562, 1.252, -0.27, -1.47, 0.237, 0.33, 0.422, 0.837, 0.145, 1.114, -1.516, 0.653, -0.731, -1.562, 1.068, 1.068, 0.976, 0.837, -0.962, 1.114, -0.316, -1.562, 0.33, -0.962, 0.514, -1.47, -0.132, 0.468, 0.791, 0.376, 0.422, 1.391, 0.745, -1.331, 1.898, -1.193, -1.654, 0.422, 0.745, -1.377, -1.423, -0.639, -1.608, 0.653, 0.699, -1.562, -1.054, -0.87, 0.007, 1.483, 0.883, -1.1, -1.47, 0.653, 0.929, 1.206, -0.962, -1.331, -1.654, -1.331, 1.252, 1.206, -0.87, -1.239, -0.178, 0.376, -1.654, 0.745, 0.883, -0.916, -0.316, 0.929, 0.929, -1.608, 0.837, 0.745, 0.929, 0.976, -0.778, -1.147, 1.16, 0.653, -1.1, 0.653, -1.377, -1.285, 0.653, 1.16, -0.501, 0.053, 1.714, -0.87, 0.56, -1.239, -1.423, -1.054, -0.639, -1.608, 0.791, 0.976, 0.883, 0.468, -1.516, 0.883, 1.252, 0.237, -0.408, 0.699, -1.377, 0.237, 0.929, 1.114, 0.376, 1.252, 0.237, 0.56, -0.824, -1.423, -0.178, 0.376, 0.929, -1.608, -1.285, 0.883, 1.16, 0.468, 1.114, 0.468, 0.653, -0.87, 1.714, 0.606, -1.377, -0.316, -1.193, -0.87, -1.516, -0.178, 0.376, 0.237, 1.206, -0.685, -0.778, -1.147, 0.883, 0.791, 0.053, 0.699, 0.514, -1.193, 0.376, 1.298, -0.962, 0.284, 1.114, 0.791, -1.654, 1.391, 1.114, 1.391, 1.76, 0.606, 0.33, -1.47, -0.824, 0.56, 0.791, 0.929, 0.33, 0.237, 1.345, -1.423, 0.606, 1.022, 0.606, 0.883, -1.285, 0.883, 1.252, -1.193, 0.883, -1.193, 1.483, 0.883, 0.699, 0.284, -0.639, -1.377, 0.33, -0.455, 0.929, -0.547, 0.099, -1.654, 0.699, -1.562, -1.377, 0.883, -1.193, 1.114, 0.837, 1.298, 1.022, 0.929, 0.745, 0.653, 0.883, 0.56, 0.653, -0.962, 0.837, -1.562, 0.837, -1.285, 0.976, -1.147, -1.423, -1.239, -1.331, 0.791, 0.883, -0.547, 0.007, -1.423, -0.962, 0.699, -1.377, 0.837, 0.422, -0.962, 0.099, -1.47, -1.008, 1.298, -1.054, 0.791, 0.699, -0.962, -1.608, -1.193, 2.083, -0.132, 0.56, 0.053, 0.699, 0.606, -1.008, -0.039, -1.239, -1.1, 0.791, 0.699, 1.022, 0.191, -0.593, -1.516, -1.423)
fifaRaw <- c(fifaRaw, 1.391, 0.053, 1.022, 0.284, 0.56, 0.745, -0.156, -0.523, -0.418, 2.156, -0.261, 3.364, -0.628, -0.576, -0.156, -0.523, -0.628, -0.471, -0.366, -0.261, -0.313, -0.576, -0.156, -0.576, -0.471, -0.208, 1.998, -0.366, 2.366, -0.418, -0.366, 2.313, -0.208, -0.628, -0.471, -0.313, -0.471, -0.523, -0.261, -0.156, 3.206, -0.471, -0.366, -0.366, -0.208, -0.471, -0.313, -0.261, 2.523, -0.313, -0.471, -0.313, -0.576, -0.523, 2.628, -0.681, -0.156, -0.523, -0.366, 2.996, -0.418, 2.838, -0.261, -0.523, 1.893, -0.471, -0.366, -0.156, -0.418, -0.313, -0.208, -0.681, -0.628, -0.313, -0.366, -0.156, -0.628, -0.418, -0.208, -0.418, -0.156, 2.733, -0.628, -0.156, -0.208, -0.156, -0.418, -0.576, -0.576, -0.313, 3.101, -0.103, -0.103, -0.261, -0.261, -0.366, -0.576, -0.261, -0.261, -0.366, -0.156, -0.261, -0.418, -0.418, -0.261, -0.261, -0.418, -0.628, -0.208, -0.208, -0.208, -0.156, -0.576, -0.261, -0.366, -0.576, 2.051, -0.156, -0.576, 1.946, -0.261, -0.418, -0.523, -0.261, -0.523, -0.156, -0.208, -0.471, 2.628, -0.471, 2.523, -0.523, -0.103, -0.576, -0.156, 2.313, -0.523, -0.681, -0.523, -0.471, -0.261, -0.103, -0.208, 2.103, -0.366, -0.208, -0.628, -0.471, -0.313, -0.103, -0.261, -0.261, -0.628, 3.101, -0.471, -0.366, -0.313, -0.576, -0.156, -0.313, -0.576, -0.156, -0.261, -0.103, -0.523, -0.208, -0.576, -0.628, -0.523, -0.366, 2.681, -0.471, -0.576, 2.733, 2.628, 2.523, -0.576, -0.366, -0.261, 2.418, -0.418, -0.418, -0.628, -0.523, -0.418, -0.156, -0.418, 1.42, -0.628, -0.366, -0.156, -0.208, -0.628, -0.103, -0.261, -0.156, -0.576, -0.208, 1.525, 2.366, -0.628, -0.418, -0.576, -0.208, -0.261, 2.576, 2.733, -0.628, -0.628, -0.208, -0.471, -0.681, -0.576, -0.366, -0.103, -0.523, -0.208, -0.628, -0.576, -0.261, -0.208, -0.523, -0.261, -0.313, -0.366, -0.313, -0.471, -0.208, -0.576, -0.576, -0.471, -0.418, -0.576, -0.313, -0.261, 2.261, -0.156, -0.628, -0.418, -0.313, 2.733, -0.576, -0.576, 2.313, -0.471, -0.576, 2.576, -0.418, -0.208, -0.576, -0.103, -0.208, -0.261, -0.208, -0.366, -0.156, -0.523, -0.208, -0.261, -0.208, -0.576, -0.261, -0.628, -0.208, 1.735, -0.471, -0.208, -0.628, 2.418, -0.523, -0.471, -0.418, -0.418, -0.103, -0.523, -0.418, -0.523, -0.471, -0.156, 3.154, -0.208, -0.576, -0.261, -0.418, -0.208, 2.103, -0.418, -0.471, 2.523, -0.576, -0.261, -0.523, -0.523, -0.523, -0.103, 2.313, -0.313, -0.523, -0.681, -0.523, 2.208, 2.366, -0.628, -0.576, -0.471, -0.366, 1.893, -0.261, -0.628, 2.523, -0.208, -0.156, -0.208, -0.366, -0.523, -0.628, -0.313, -0.208, -0.523, -0.208, -0.471, -0.576, -0.313, -0.261, -0.576, -0.313, -0.523, -0.156, 3.049, -0.261, -0.261, -0.576, -0.418, -0.313, -0.523, -0.471, -0.418, 2.628, -0.628, -0.471, 2.681, -0.523, -0.628, -0.418, -0.208, 2.156, -0.366, -0.261, -0.261, -0.471, -0.471, -0.313, -0.523, -0.471, -0.208, -0.208, -0.576, -0.156, -0.418, -0.523)
fifaRaw <- c(fifaRaw, 2.261, -0.366, -0.261, -0.103, 2.576, -0.103, -0.628, -0.418, -0.208, -0.208, -0.418, -0.471, -0.523, -0.261, -0.471, -0.418, -0.681, 2.786, -0.523, 2.471, -0.366, -0.628, -0.208, -0.471, -0.366, -0.261, -0.156, -0.208, -0.366, -0.681, -0.208, -0.471, 2.838, -0.523, -0.523, -0.208, -0.156, -0.366, -0.103, 2.156, -0.208, -0.208, -0.366, -0.156, -0.628, -0.523, 1.998, -0.576, -0.681, -0.418, -0.366, -0.418, -0.103, -0.103, 2.944, -0.576, -0.471, -0.366, -0.208, -0.366, -0.628, -0.681, -0.208, -0.628, -0.523, -0.208, -0.471, -0.628, -0.366, -0.418, 1.998, -0.523, -0.628, -0.681, -0.681, -0.261, 2.051, -0.208, 2.628, -0.418, -0.681, -0.471, -0.156, -0.208, -0.103, -0.418, -0.576, -0.261, -0.576, -0.576, -0.366, -0.523, -0.366, -0.156, 2.523, -0.471, -0.313, -0.313, -0.313, 3.101, 2.733, -0.208, -0.156, -0.523, -0.156, -0.471, -0.156, -0.366, -0.471, -0.576, -0.576, -0.628, -0.208, -0.576, 2.838, -0.681, -0.313, -0.366, -0.103, -0.208, 2.261, 2.681, 2.733, -0.366, -0.628, -0.261, -0.313, -0.313, -0.313, -0.471, -0.103, -0.313, 0.002, -0.313, -0.471, -0.156, -0.418, -0.156, -0.313, 2.681, -0.156, -0.523, -0.576, -0.418, -0.313, -0.523, -0.485, -0.323, -0.216, 1.987, -0.592, 3.115, -0.216, -0.538, -0.108, -0.699, -0.377, -0.538, -0.108, -0.485, -0.162, -0.216, -0.485, -0.323, -0.592, -0.162, 2.095, -0.538, 2.363, -0.431, -0.377, 2.256, -0.216, -0.377, -0.377, -0.27, -0.27, -0.646, -0.162, -0.162, 3.008, -0.485, -0.431, -0.162, -0.538, -0.485, -0.699, -0.323, 2.632, -0.699, -0.323, -0.592, -0.538, -0.216, 2.686, -0.699, -0.323, -0.592, -0.538, 2.739, -0.27, 2.793, -0.323, -0.699, 2.095, -0.162, -0.485, -0.377, -0.323, -0.377, -0.592, -0.323, -0.485, -0.162, -0.485, -0.592, -0.646, -0.377, -0.323, -0.377, -0.27, 2.578, -0.431, -0.162, -0.108, -0.485, -0.108, -0.592, -0.485, -0.538, 3.008, -0.162, -0.216, -0.431, -0.646, -0.108, -0.485, -0.431, -0.27, -0.431, -0.377, -0.216, -0.646, -0.162, -0.485, -0.431, -0.538, -0.431, -0.646, -0.485, -0.538, -0.377, -0.162, -0.162, -0.216, -0.162, 1.665, -0.216, -0.27, 1.826, -0.485, -0.216, -0.538, -0.538, -0.323, -0.162, -0.592, -0.108, 2.9, -0.216, 2.739, -0.323, -0.162, -0.216, -0.27, 2.471, -0.485, -0.162, -0.592, -0.592, -0.323, -0.485, -0.216, 1.88, -0.323, -0.216, -0.377, -0.323, -0.431, -0.162, -0.431, -0.216, -0.216, 2.954, -0.538, -0.216, -0.216, -0.377, -0.377, -0.27, -0.377, -0.27, -0.377, -0.216, -0.699, -0.646, -0.216, -0.485, -0.162, -0.431, 2.686, -0.431, -0.485, 2.847, 2.632, 2.847, -0.162, -0.323, -0.699, 2.202, -0.162, -0.592, -0.323, -0.377, -0.162, -0.216, -0.431, 2.202, -0.592, -0.27, -0.377, -0.216, -0.431, -0.27, -0.377, -0.162, -0.646, -0.27, 1.933, 2.578, -0.485, -0.323, -0.27, -0.216, -0.216, 2.471)
fifaRaw <- c(fifaRaw, 2.847, -0.646, -0.377, -0.377, -0.538, -0.377, -0.592, -0.216, -0.431, -0.27, -0.646, -0.538, -0.27, -0.108, -0.538, -0.27, -0.323, -0.538, -0.323, -0.485, -0.431, -0.592, -0.538, -0.485, -0.108, -0.216, -0.485, -0.592, -0.27, 2.256, -0.377, -0.323, -0.377, -0.323, 2.9, -0.538, -0.592, 2.202, -0.27, -0.538, 2.632, -0.27, -0.538, -0.27, -0.485, -0.377, -0.431, -0.27, -0.538, -0.216, -0.592, -0.162, -0.27, -0.538, -0.108, -0.216, -0.162, -0.162, 1.718, -0.162, -0.27, -0.162, 2.417, -0.431, -0.485, -0.485, -0.216, -0.323, -0.27, -0.592, -0.323, -0.377, -0.377, 2.632, -0.538, -0.646, -0.485, -0.485, -0.485, 2.686, -0.162, -0.108, 2.686, -0.646, -0.323, -0.377, -0.485, -0.592, -0.323, 2.309, -0.323, -0.323, -0.592, -0.431, 2.202, 2.202, -0.485, -0.377, -0.377, -0.216, 2.363, -0.377, -0.485, 2.148, -0.485, -0.323, -0.592, -0.162, -0.27, -0.485, -0.377, -0.646, -0.538, -0.377, -0.646, -0.216, -0.646, -0.592, -0.538, -0.323, -0.108, -0.431, 2.847, -0.592, -0.162, -0.485, -0.27, -0.108, -0.377, -0.431, -0.377, 3.115, -0.592, -0.323, 2.686, -0.646, -0.162, -0.538, -0.323, 1.826, -0.216, -0.592, -0.323, -0.431, -0.485, -0.162, -0.431, -0.108, -0.485, -0.216, -0.27, -0.538, -0.162, -0.377, 2.417, -0.592, -0.431, -0.27, 2.524, -0.538, -0.216, -0.27, -0.699, -0.646, -0.323, -0.431, -0.323, -0.377, -0.162, -0.216, -0.323, 2.524, -0.431, 2.578, -0.592, -0.431, -0.216, -0.108, -0.162, -0.485, -0.592, -0.216, -0.431, -0.377, -0.592, -0.377, 2.847, -0.592, -0.592, -0.538, -0.646, -0.323, -0.431, 2.471, -0.27, -0.377, -0.699, -0.323, -0.538, -0.592, 1.826, -0.699, -0.323, -0.592, -0.538, -0.485, -0.377, -0.323, 2.632, -0.592, -0.485, -0.646, -0.377, -0.485, -0.377, -0.538, -0.162, -0.646, -0.538, -0.27, -0.538, -0.592, -0.108, -0.216, 2.041, -0.162, -0.27, -0.377, -0.538, -0.108, 2.095, -0.592, 2.739, -0.646, -0.162, -0.323, -0.431, -0.27, -0.108, -0.108, -0.323, -0.377, -0.431, -0.377, -0.538, -0.538, -0.646, -0.592, 2.739, -0.108, -0.216, -0.108, -0.323, 3.115, 2.739, -0.377, -0.323, -0.216, -0.431, -0.431, -0.162, -0.646, -0.162, -0.646, -0.431, -0.431, -0.699, -0.431, 2.686, -0.162, -0.162, -0.646, -0.162, -0.108, 2.524, 2.9, 1.826, -0.538, -0.216, -0.485, -0.592, -0.162, -0.485, -0.377, -0.162, -0.592, -0.108, -0.431, -0.216, -0.699, -0.485, -0.27, -0.485, 2.471, -0.431, -0.485, -0.323, -0.538, -0.377, -0.162, -0.525, -0.25, -0.25, 1.999, -0.36, 2.328, -0.525, -0.47, -0.47, -0.25, -0.47, -0.305, -0.36, -0.579, -0.195, -0.579, -0.14, -0.14, -0.579, -0.305, 1.999, -0.525, 2.493, -0.305, -0.634, 2.219, -0.525, -0.525, -0.086, -0.525, -0.305, -0.47, -0.415, -0.525, 2.822, -0.47, -0.415, -0.14, -0.086, -0.36, -0.36, -0.415, 2.658, -0.634, -0.634, -0.525, -0.305, -0.086, 2.658, -0.579, -0.195, -0.086, -0.47)
fifaRaw <- c(fifaRaw, 3.371, -0.579, 3.042, -0.086, -0.36, 2.054, -0.36, -0.525, -0.25, -0.25, -0.634, -0.579, -0.634, -0.305, -0.14, -0.47, -0.415, -0.525, -0.415, -0.634, -0.25, -0.525, 3.316, -0.36, -0.25, -0.25, -0.525, -0.195, -0.47, -0.579, -0.579, 2.767, -0.47, -0.25, -0.525, -0.47, -0.525, -0.579, -0.634, -0.525, -0.086, -0.14, -0.579, -0.47, -0.14, -0.36, -0.579, -0.579, -0.525, -0.579, -0.195, -0.305, -0.305, -0.195, -0.415, -0.525, -0.47, 2.548, -0.689, -0.36, 1.999, -0.525, -0.579, -0.634, -0.47, -0.47, -0.47, -0.579, -0.14, 2.383, -0.579, 2.658, -0.305, -0.25, -0.25, -0.14, 2.822, -0.305, -0.14, -0.47, -0.579, -0.36, -0.25, -0.195, 1.89, -0.525, -0.47, -0.47, -0.25, -0.47, -0.305, -0.305, -0.195, -0.415, 3.206, -0.086, -0.634, -0.25, -0.525, -0.086, -0.579, -0.634, -0.47, -0.415, -0.25, -0.634, -0.14, -0.36, -0.195, -0.525, -0.305, 3.206, -0.305, -0.305, 2.822, 2.328, 2.767, -0.086, -0.25, -0.195, 2.274, -0.525, -0.195, -0.525, -0.579, -0.415, -0.25, -0.195, 1.56, -0.634, -0.415, -0.36, -0.525, -0.14, -0.415, -0.47, -0.195, -0.579, -0.305, 2.219, 2.548, -0.579, -0.195, -0.305, -0.415, -0.47, 2.822, 3.097, -0.195, -0.195, -0.305, -0.086, -0.525, -0.579, -0.47, -0.634, -0.36, -0.689, -0.525, -0.634, -0.36, -0.525, -0.525, -0.47, -0.305, -0.525, -0.305, -0.415, -0.415, -0.195, -0.086, -0.525, -0.305, -0.525, -0.305, -0.525, 1.89, -0.415, -0.47, -0.195, -0.36, 2.658, -0.415, -0.579, 2.219, -0.47, -0.195, 2.274, -0.36, -0.195, -0.086, -0.36, -0.195, -0.195, -0.14, -0.25, -0.579, -0.195, -0.525, -0.36, -0.47, -0.634, -0.47, -0.415, -0.36, 1.944, -0.086, -0.634, -0.25, 2.328, -0.525, -0.305, -0.47, -0.25, -0.525, -0.47, -0.305, -0.36, -0.579, -0.579, 2.767, -0.305, -0.195, -0.525, -0.579, -0.634, 2.438, -0.25, -0.634, 2.438, -0.36, -0.415, -0.195, -0.689, -0.14, -0.25, 2.493, -0.25, -0.305, -0.195, -0.25, 2.493, 2.219, -0.36, -0.47, -0.195, -0.25, 2.658, -0.25, -0.25, 1.725, -0.305, -0.14, -0.525, -0.47, -0.36, -0.36, -0.634, -0.525, -0.689, -0.195, -0.579, -0.36, -0.525, -0.305, -0.415, -0.579, -0.579, -0.195, 2.603, -0.25, -0.579, -0.415, -0.579, -0.36, -0.086, -0.525, -0.086, 2.164, -0.36, -0.195, 2.603, -0.579, -0.305, -0.36, -0.47, 1.999, -0.14, -0.36, -0.579, -0.47, -0.195, -0.579, -0.689, -0.47, -0.14, -0.634, -0.36, -0.634, -0.579, -0.525, 2.548, -0.36, -0.634, -0.305, 2.713, -0.579, -0.305, -0.305, -0.634, -0.195, -0.25, -0.36, -0.36, -0.47, -0.579, -0.36, -0.25, 2.658, -0.525, 2.713, -0.195, -0.25, -0.525, -0.195, -0.305, -0.195, -0.25, -0.36, -0.14, -0.579, -0.579, -0.634, 2.658, -0.47, -0.415, -0.14, -0.634, -0.195, -0.14, 2.493, -0.415, -0.47, -0.525, -0.25, -0.47, -0.634, 1.725, -0.415, -0.305, -0.415, -0.634, -0.634, -0.25, -0.14, 2.328, -0.195, -0.47, -0.086, -0.14, -0.47, -0.195)
fifaRaw <- c(fifaRaw, -0.689, -0.634, -0.086, -0.305, -0.195, -0.36, -0.305, -0.525, -0.579, 2.219, -0.25, -0.47, -0.579, -0.634, -0.086, 2.109, -0.195, 1.56, -0.36, -0.195, -0.47, -0.195, -0.14, -0.195, -0.086, -0.47, -0.36, -0.525, -0.14, -0.25, -0.36, -0.579, -0.579, 2.603, -0.47, -0.47, -0.579, -0.525, 2.767, 3.261, -0.36, -0.525, -0.305, -0.305, -0.47, -0.195, -0.25, -0.634, -0.47, -0.305, -0.305, -0.525, -0.305, 2.438, -0.634, -0.305, -0.579, -0.086, 0.683, 2.438, 2.603, 2.713, -0.47, -0.525, -0.25, -0.195, -0.634, -0.634, -0.305, -0.195, -0.47, -0.25, -0.415, -0.14, -0.305, -0.579, -0.25, -0.36, 2.713, -0.14, -0.305, -0.195, -0.47, -0.305, -0.25, -0.567, -0.356, -0.144, 2.074, -0.408, 3.078, -0.144, -0.62, -0.197, -0.408, -0.461, -0.356, -0.514, -0.408, -0.514, -0.461, -0.197, -0.461, -0.197, -0.144, 2.022, -0.197, 2.391, -0.303, -0.514, 2.339, -0.567, -0.461, -0.62, -0.408, -0.197, -0.25, -0.144, -0.25, 3.237, -0.303, -0.567, -0.303, -0.408, -0.514, -0.197, -0.25, 2.603, -0.25, -0.356, -0.408, -0.567, -0.197, 2.708, -0.62, -0.144, -0.514, -0.356, 2.761, -0.356, 2.708, -0.567, -0.514, 2.497, -0.567, -0.091, -0.356, -0.567, -0.144, -0.197, -0.144, -0.197, -0.408, -0.62, -0.303, -0.197, -0.144, -0.567, -0.567, -0.514, 2.708, -0.408, -0.356, -0.62, -0.356, -0.567, -0.567, -0.356, -0.408, 2.92, -0.091, -0.197, -0.356, -0.408, -0.461, -0.25, -0.514, -0.303, -0.144, -0.144, -0.408, -0.673, -0.25, -0.303, -0.303, -0.408, -0.567, -0.25, -0.25, -0.197, -0.197, -0.303, -0.514, -0.514, -0.197, 2.286, -0.25, -0.62, 1.81, -0.567, -0.514, -0.356, -0.303, -0.25, -0.25, -0.091, -0.356, 2.603, -0.567, 2.814, -0.25, -0.567, -0.356, -0.514, 2.444, -0.197, -0.303, -0.567, -0.514, -0.567, -0.25, -0.356, 2.603, -0.356, -0.303, -0.408, -0.197, -0.673, -0.25, -0.62, -0.303, -0.25, 3.025, -0.303, -0.62, -0.461, -0.461, -0.514, -0.567, -0.514, -0.461, -0.514, -0.356, -0.514, -0.514, -0.144, -0.567, -0.303, -0.144, 2.497, -0.303, -0.461, 2.603, 2.656, 2.814, -0.091, -0.303, -0.567, 2.074, -0.514, -0.091, -0.461, -0.356, -0.197, -0.303, -0.567, 1.388, -0.673, -0.144, -0.461, -0.567, -0.25, -0.567, -0.461, -0.144, -0.567, -0.197, 1.757, 2.444, -0.303, -0.461, -0.356, -0.197, -0.62, 2.391, 2.867, -0.25, -0.25, -0.144, -0.62, -0.62, -0.25, -0.197, -0.197, -0.514, -0.303, -0.673, -0.461, -0.197, -0.408, -0.514, -0.673, -0.461, -0.303, -0.408, -0.408, -0.408, -0.567, -0.303, -0.514, -0.197, -0.144, -0.197, -0.62, 2.127, -0.461, -0.567, -0.303, -0.144, 2.814, -0.461, -0.356, 2.127, -0.62, -0.408, 2.286, -0.461, -0.62, -0.461, -0.408, -0.144, -0.408, -0.303, -0.25, -0.25, -0.197, -0.197, -0.303, -0.144, -0.408, -0.673, -0.461, -0.197, 1.599, -0.461, -0.62, -0.303, 2.55, -0.197, -0.514, -0.303, -0.514, -0.62, -0.62, -0.62, -0.673)
fifaRaw <- c(fifaRaw, -0.408, -0.567, 2.603, -0.197, -0.144, -0.356, -0.25, -0.62, 3.025, -0.62, -0.567, 2.708, -0.62, -0.144, -0.197, -0.567, -0.25, -0.197, 2.708, -0.197, -0.408, -0.673, -0.303, 2.391, 2.233, -0.197, -0.303, -0.461, -0.62, 2.497, -0.303, -0.408, 2.444, -0.408, -0.567, -0.25, -0.197, -0.303, -0.25, -0.408, -0.514, -0.408, -0.197, -0.356, -0.408, -0.408, -0.25, -0.62, -0.567, -0.567, -0.25, 2.761, -0.461, -0.197, -0.514, -0.356, -0.25, -0.461, -0.303, -0.408, 2.814, -0.303, -0.673, 2.55, -0.356, -0.356, -0.144, -0.356, 1.863, -0.091, -0.567, -0.514, -0.567, -0.303, -0.514, -0.197, -0.144, -0.408, -0.514, -0.62, -0.303, -0.62, -0.144, 2.339, -0.62, -0.62, -0.408, 3.025, -0.567, -0.197, -0.514, -0.303, -0.567, -0.356, -0.25, -0.461, -0.356, -0.62, -0.356, -0.408, 2.603, -0.62, 2.391, -0.356, -0.461, -0.144, -0.356, -0.673, -0.303, -0.461, -0.303, -0.567, -0.303, -0.25, -0.673, 2.761, -0.62, -0.408, -0.303, -0.62, -0.461, -0.567, 2.127, -0.303, -0.25, -0.673, -0.197, -0.408, -0.144, 1.652, -0.514, -0.567, -0.303, -0.303, -0.514, -0.408, -0.197, 2.444, -0.197, -0.144, -0.303, -0.461, -0.303, -0.144, -0.25, -0.62, -0.408, -0.197, -0.197, -0.144, -0.567, -0.197, -0.567, 2.074, -0.303, -0.144, -0.356, -0.25, -0.303, 1.863, -0.567, 2.761, -0.62, -0.62, -0.25, -0.356, -0.514, -0.514, -0.197, -0.197, -0.567, -0.567, -0.461, -0.461, -0.408, -0.408, -0.408, 2.656, -0.356, -0.567, -0.567, -0.356, 3.237, 2.761, -0.567, -0.303, -0.461, -0.303, -0.356, -0.144, -0.144, -0.461, -0.356, -0.567, -0.62, -0.514, -0.356, 2.814, -0.356, -0.356, -0.25, -0.567, -0.197, 2.286, 2.603, 2.603, -0.567, -0.461, -0.356, -0.461, -0.303, -0.091, -0.514, -0.514, -0.144, -0.144, -0.197, -0.197, -0.567, -0.567, -0.197, -0.356, 2.339, -0.461, -0.144, -0.408, -0.197, -0.461, -0.567, -0.36, -0.308, -0.36, 2.239, -0.672, 3.487, -0.204, -0.308, -0.62, -0.412, -0.568, -0.256, -0.204, -0.204, -0.464, -0.412, -0.568, -0.36, -0.1, -0.412, 2.135, -0.62, 2.447, -0.152, -0.308, 2.447, -0.62, -0.308, -0.256, -0.412, -0.464, -0.412, -0.516, -0.204, 3.435, -0.308, -0.516, -0.256, -0.36, -0.464, -0.256, -0.464, 2.551, -0.568, -0.412, -0.568, -0.204, -0.152, 2.603, -0.308, -0.62, -0.204, -0.204, 3.123, -0.62, 2.915, -0.1, -0.36, 1.771, -0.256, -0.36, -0.256, -0.464, -0.36, -0.568, -0.412, -0.36, -0.568, -0.568, -0.412, -0.204, -0.464, -0.204, -0.412, -0.204, 2.811, -0.412, -0.516, -0.36, -0.464, -0.308, -0.152, -0.308, -0.204, 3.019, -0.464, -0.152, -0.256, -0.412, -0.412, -0.204, -0.568, -0.308, -0.256, -0.568, -0.204, -0.308, -0.36, -0.568, -0.464, -0.308, -0.516, -0.308, -0.568, -0.256, -0.204, -0.516, -0.204, -0.516, -0.1, 2.083, -0.568, -0.568, 2.187, -0.568, -0.568, -0.256, -0.62, -0.204, -0.36, -0.256, -0.256, 2.759, -0.152, 2.499, -0.62, -0.204, -0.62)
fifaRaw <- c(fifaRaw, -0.36, 2.499, -0.568, -0.412, -0.412, -0.204, -0.308, -0.256, -0.256, 2.187, -0.412, -0.62, -0.672, -0.204, -0.568, -0.464, -0.204, -0.256, -0.568, 3.071, -0.256, -0.256, -0.516, -0.308, -0.152, -0.1, -0.516, -0.308, -0.568, -0.308, -0.568, -0.308, -0.36, -0.516, -0.1, -0.412, 2.707, -0.256, -0.412, 2.967, 2.655, 2.499, -0.308, -0.568, -0.256, 2.499, -0.308, -0.152, -0.516, -0.256, -0.36, -0.204, -0.672, 1.668, -0.516, -0.152, -0.1, -0.36, -0.256, -0.62, -0.464, -0.568, -0.516, -0.308, 1.875, 2.395, -0.204, -0.568, -0.516, -0.464, -0.308, 2.551, 2.759, -0.516, -0.36, -0.62, -0.204, -0.204, -0.464, -0.308, -0.568, -0.516, -0.36, -0.412, -0.152, -0.464, -0.412, -0.36, -0.464, -0.412, -0.256, -0.412, -0.204, -0.256, -0.36, -0.308, -0.412, -0.204, -0.464, -0.36, -0.204, 1.771, -0.464, -0.62, -0.568, -0.36, 2.603, -0.464, -0.256, 2.135, -0.412, -0.204, 2.395, -0.62, -0.62, -0.516, -0.36, -0.204, -0.308, -0.464, -0.308, -0.62, -0.568, -0.204, -0.568, -0.464, -0.412, -0.62, -0.568, -0.204, 1.46, -0.412, -0.204, -0.36, 2.343, -0.412, -0.464, -0.36, -0.36, -0.256, -0.516, -0.308, -0.204, -0.152, -0.204, 3.071, -0.1, -0.308, -0.516, -0.256, -0.1, 1.979, -0.152, -0.152, 2.499, -0.36, -0.36, -0.412, -0.36, -0.204, -0.1, 2.291, -0.308, -0.152, -0.308, -0.36, 2.187, 2.395, -0.464, -0.464, -0.308, -0.256, 2.343, -0.412, -0.568, 2.343, -0.516, -0.568, -0.256, -0.308, -0.412, -0.516, -0.62, -0.464, -0.308, -0.568, -0.464, -0.152, -0.464, -0.62, -0.464, -0.464, -0.204, -0.672, 3.019, -0.62, -0.204, -0.256, -0.568, -0.36, -0.412, -0.308, -0.412, 2.655, -0.464, -0.672, 2.863, -0.568, -0.308, -0.412, -0.204, 1.927, -0.1, -0.36, -0.62, -0.516, -0.568, -0.256, -0.516, -0.516, -0.152, -0.62, -0.568, -0.204, -0.464, -0.152, 2.447, -0.36, -0.36, -0.36, 2.655, -0.412, -0.308, -0.412, -0.204, -0.36, -0.412, -0.412, -0.412, -0.568, -0.516, -0.256, -0.62, 2.759, -0.516, 2.603, -0.36, -0.36, -0.204, -0.152, -0.464, -0.204, -0.1, -0.308, -0.412, -0.308, -0.36, -0.516, 2.915, -0.62, -0.204, -0.412, -0.62, -0.308, -0.152, 2.187, -0.568, -0.36, -0.308, -0.308, -0.256, -0.464, 1.875, -0.516, -0.516, -0.62, -0.568, -0.36, -0.36, -0.1, 2.811, -0.672, -0.204, -0.568, -0.412, -0.516, -0.464, -0.308, -0.36, -0.308, -0.36, -0.36, -0.204, -0.36, -0.62, -0.516, 1.927, -0.256, -0.62, -0.62, -0.62, -0.568, 2.187, -0.568, 2.603, -0.62, -0.568, -0.308, -0.62, -0.516, -0.152, -0.256, -0.36, -0.1, -0.464, -0.256, -0.308, -0.568, -0.308, -0.516, 2.759, -0.36, -0.412, -0.568, -0.412, 3.123, 2.759, -0.308, -0.36, -0.152, -0.204, -0.62, -0.516, -0.204, -0.152, -0.152, -0.516, -0.62, -0.516, -0.256, 2.863, -0.412, -0.412, -0.36, -0.412, -0.204, 2.395, 2.135, 2.499, -0.36, -0.464, -0.204, -0.308, -0.568, -0.412, -0.256, -0.516, -0.256)
fifaRaw <- c(fifaRaw, -0.152, -0.568, -0.672, -0.62, -0.516, -0.308, -0.568, 1.823, -0.36, -0.516, -0.36, -0.464, -0.568, -0.256, -0.077, -0.31, -0.205, -0.373, -0.371, 1.509, -0.278, -0.36, -0.282, -0.251, -0.378, -0.26, -0.319, -0.273, 1.145, -0.382, -0.346, 2.421, -0.205, -0.389, -0.373, -0.328, -0.328, -0.395, -0.31, -0.367, -0.282, -0.278, -0.305, 0.47, -0.389, -0.375, -0.333, -0.314, 1.236, -0.356, -0.333, -0.077, -0.223, 0.123, 0.871, -0.205, -0.292, -0.205, 0.214, 0.871, 0.178, -0.241, -0.384, -0.356, 1.418, -0.26, -0.31, 0.251, -0.319, -0.041, 0.506, -0.333, -0.384, -0.187, -0.296, 0.78, -0.205, -0.187, -0.342, -0.187, -0.342, -0.364, -0.31, -0.241, -0.356, -0.31, -0.287, 13.545, -0.241, -0.187, -0.132, -0.346, -0.278, 1.145, 0.014, -0.305, -0.282, -0.342, -0.323, -0.319, -0.391, -0.373, -0.364, -0.114, 0.397, -0.314, -0.255, -0.282, -0.205, -0.205, -0.328, 0.78, -0.273, -0.278, -0.241, -0.132, -0.223, -0.382, -0.278, -0.362, -0.382, -0.292, -0.223, 0.324, -0.376, -0.041, 0.689, -0.378, -0.301, 0.689, -0.333, -0.373, -0.305, -0.187, -0.351, -0.187, -0.077, 0.251, -0.367, -0.391, 0.087, 1.236, 0.397, -0.342, -0.314, -0.369, -0.333, -0.333, -0.251, -0.346, -0.346, -0.369, -0.391, -0.255, 0.16, -0.228, 5.339, 0.36, -0.354, -0.077, -0.023, 0.306, -0.004, -0.301, -0.356, -0.296, -0.273, -0.077, 0.506, -0.205, -0.287, 0.251, -0.187, 0.214, -0.237, -0.168, -0.337, -0.282, -0.278, -0.223, -0.333, 0.196, -0.323, -0.328, -0.373, 0.269, -0.351, -0.354, -0.31, -0.337, -0.187, -0.319, -0.346, -0.328, -0.346, -0.4, -0.296, -0.059, -0.31, -0.168, -0.077, -0.323, -0.382, -0.375, -0.168, -0.114, -0.395, -0.31, -0.384, -0.292, -0.187, -0.296, 0.488, -0.232, -0.132, -0.387, -0.319, -0.273, -0.333, -0.36, -0.114, -0.273, 0.36, 0.415, -0.187, -0.305, 0.251, -0.385, 1.509, -0.319, -0.223, -0.223, -0.346, -0.387, -0.15, -0.382, -0.319, -0.391, 0.78, -0.205, -0.205, -0.301, -0.278, -0.376, -0.342, -0.305, -0.264, 1.418, -0.041, -0.187, 0.36, -0.358, 0.16, -0.319, -0.301, -0.31, 0.597, -0.358, -0.041, -0.319, -0.278, -0.38, -0.387, -0.323, 0.178, -0.041, -0.205, -0.023, -0.292, -0.337, 0.397, -0.251, -0.395, -0.387, -0.38, -0.378, -0.282, -0.387, -0.228, -0.278, -0.228, -0.246, 2.695, -0.269, -0.364, 5.795, 1.236, 0.306, -0.333, -0.36, -0.384, -0.237, 1.053, -0.395, -0.342, 0.178, -0.323, 0.379, 1.6, -0.333, 1.236, -0.077, 0.597, -0.333, -0.168, -0.241, 0.78, -0.367, -0.393, -0.328, -0.387, 0.506, 0.506, -0.342, -0.382, 6.251, -0.337, -0.323, -0.278, -0.269, 0.324, 4.609, -0.26, -0.296, -0.132, -0.246, -0.273, 0.597, -0.205, -0.351, -0.391, -0.041, -0.223, -0.36, -0.351, 0.105, 0.269, -0.296, 0.488, 2.695, -0.385, 1.783, -0.205, -0.376)
fifaRaw <- c(fifaRaw, -0.287, -0.223, -0.337, -0.333, -0.096, -0.269, -0.353, -0.246, -0.305, -0.384, -0.356, 0.78, -0.246, 0.142, -0.26, -0.205, -0.384, -0.228, 0.196, -0.31, 1.145, -0.38, -0.205, 0.324, -0.323, -0.387, -0.319, -0.241, -0.36, -0.393, 0.214, 0.032, -0.367, -0.223, -0.15, -0.353, 0.287, 1.874, -0.31, -0.26, -0.187, -0.36, -0.351, -0.251, -0.241, -0.376, -0.337, 0.196, -0.059, -0.38, -0.269, -0.342, -0.237, -0.328, -0.273, -0.38, 0.105, -0.168, -0.114, 0.397, -0.389, 0.032, -0.31, -0.38, 1.053, 0.105, -0.077, 0.962, 1.053, -0.389, -0.395, 0.78, -0.369, -0.365, -0.282, -0.264, -0.342, 0.689, -0.223, -0.305, -0.059, -0.395, 0.251, -0.292, -0.096, 0.452, -0.369, -0.255, -0.264, 2.421, -0.314, -0.337, -0.31, -0.168, -0.396, -0.351, -0.023, -0.269, 0.433, -0.362, -0.378, -0.282, -0.36, -0.382, -0.232, -0.278, 0.306, -0.255, 0.689, 0.014, -0.168, -0.328, -0.314, 0.324, -0.365, -0.369, -0.337, 0.597, -0.282, -0.328, -0.287, -0.228, -0.023, 0.871, 0.069, -0.365, -0.354, -0.323, 0.306, -0.396, -0.378, -0.26, -0.273, 0.689, -0.301, -0.393, 0.306, -0.292, -0.132, -0.31, 0.597, -0.369, -0.255, -0.351, -0.358, -0.337, -0.358, 8.895, 0.306, -0.319, 0.178, -0.393, -0.077, -0.31, -0.38, 0.36, 0.105, -0.337, -0.205, -0.337, -0.375, 1.509, -0.205, -0.389, 0.397, -0.389, 0.105, 1.418, -0.328, -0.365)

fifa19mtx <- matrix(data=fifaRaw, ncol=37, nrow=500, byrow=FALSE)
fifa19_scaled <- as.data.frame(fifa19mtx)
names(fifa19_scaled) <- c('Age', 'Potential', 'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes', 'PlayerValue')
str(fifa19_scaled)


# Glimpse at the dataset
glimpse(fifa19_scaled)

coefs <- data.frame(OLS=as.vector(lm(PlayerValue ~ . -Interceptions, data=fifa19_scaled)$coef[-1]))
coefs


# Ridge regression: mdlRidge
mdlRidge <- caret::train(PlayerValue ~ ., data = fifa19_scaled, method = "ridge", tuneLength = 8)

# Plot ridge train object
plot(mdlRidge)

# Ridge regression coefficients
coefRidge <- predict(mdlRidge$finalModel, type='coef', mode='norm')$coefficients
coefs$RidgeAll <- coefRidge[nrow(coefRidge),]
print(coefs)


# Lasso regression: mdlLasso
mdlLasso <- caret::train(PlayerValue ~ ., data = fifa19_scaled, method = "lasso", tuneLength = 8)

# Plot lasso object
plot(mdlLasso)

# Get coefficients in every step: coefLasso
coefLasso <- predict(mdlLasso$finalModel, type='coef', mode='norm')$coefficients

# Get coefficients for top 5 and all variables
(coefs$LassoTop5 <- coefLasso[6, ])
(coefs$LassoAll <- coefLasso[nrow(coefLasso), ])


# ElasticNet regression: mdlElasticNet
mdlElasticNet <- caret::train(PlayerValue ~ ., data = fifa19_scaled, method = "enet", tuneLength = 8)

# Plot elastic net object
plot(mdlElasticNet)

# Get elastic net coefficients: coefElasticNet
coefElasticNet <- predict(mdlElasticNet$finalModel, type="coef", mode="norm")$coefficients

# Get coefficients for top 5 and all variables
(coefs$ElasticNetTop5 <- coefElasticNet[6, ])
(coefs$ElasticNetAll <- coefElasticNet[nrow(coefElasticNet), ])


# Fit MLP using nnet: mdlNNet
# set.seed(124)
# mdlNNet <- nnet(Class ~ ., data = pulsar_train, size = 3)

# Calculate train error: train_error
# pred_train <- predict(mdlNNet, pulsar_train, type="class")
# train_cm <- table(pred_train, pulsar_train$Class)
# (train_error <- 1 - sum(diag(train_cm)) / sum(train_cm))

# Calculate test error: test_error
# pred_test <- predict(mdlNNet, pulsar_test, type="class")
# test_cm <- table(pred_test, pulsar_test$Class)
# (test_error <- 1 - sum(diag(test_cm)) / sum(test_cm))


# Fit MLP using nnet: mdlNNet
# set.seed(124)
# mdlNNet <- nnet(Class ~ ., data = pulsar_train, size = 5)

# Calculate train error: train_error
# pred_train <- predict(mdlNNet, pulsar_train, type="class")
# train_cm <- table(pred_train, pulsar_train$Class)
# (train_error <- 1 - sum(diag(train_cm)) / sum(train_cm))

# Calculate test error: test_error
# pred_test <- predict(mdlNNet, pulsar_test, type="class")
# test_cm <- table(pred_test, pulsar_test$Class)
# (test_error <- 1 - sum(diag(test_cm)) / sum(test_cm))


# Create the 5-fold cross validation training control object
# control <- trainControl(method = "cv", number = 5, savePredictions = TRUE, classProbs = TRUE)

# Create the vector of base learners: baseLearners
# baseLearners <- c('rpart', 'glm', 'knn', 'svmRadial')

# Create and summarize the list of base learners: models
# models <- caretList(Class ~ ., data = training, trControl = control, methodList = baseLearners)
# summary(models)


# Classification results in each resample: results
# results <- resamples(models)

# Summarize and print the results in one line
# (results_summary <- summary(results))

# Show the correlation among the base learners' results
# modelCor(results)

# Display a scatter plot matrix of these results
# splom(results)


# Load caretEnsemble
# library(caretEnsemble)

# Set the seed
# set.seed(123)

# Stack the base learners
# stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=control)

# Print the stacked model
# stack.glm

# Summarize the performance results for each base learner
# summary(results)


evaluateModel <- function(trainObject, testData) {
  # Compute binary yes/no predictions and class probabilities
  model_preds <- predict(trainObject, testData)
  model_probs <- predict(trainObject, testData, type="prob")
  # Compute accuracy and AUC values
  model_acc <- accuracy(testData$Class, model_preds)
  model_auc <- auc(testData$Class == 'yes', model_probs[, 2])
  # Return model accuracy and AUC
  c(model_acc, model_auc)
}

# Evaluate the performance of each individual base learner
# baseLearnerStats <- sapply(X=stack.glm$models, FUN=evaluateModel, testing)
# baseLearnerDF <- data.frame(baseLearnerStats, row.names = c('acc', 'auc'))

# Compute stacked ensemble's accuracy on test data
# stack_preds <- predict(stack.glm, testing)
# stack_acc <- accuracy(testing$Class, stack_preds)

# Compute stacked ensemble's AUC on test data
# stack_preds_probs <- predict(stack.glm, testing, type="prob")
# stack_auc <- auc(testing$Class == 'yes', stack_preds_probs)

# Combine the stacked ensemble results
# (allLearnersDF <- cbind(baseLearnerDF, list(stack=c(stack_acc, stack_auc))))

```
  
  
  
***
  
Chapter 3 - Unsupervised Learning  
  
K-means Clustering:  
  
* K-means is a simple, intuitive, and fast-running manner for clustering  
* K-means makes several assumptions about the data  
	* Numeric and continuous variables  
    * Variables with symmetrics distributions  
    * Variables with similar averages and standard deviations (based on Euclidean distances)  
    * Skew should be dealt with - log-transform, Box-Cox, cubic root, etc.  
    * Data should be scaled, typically either using z-score scaling or max/min scaling  
* Need to consider the optimal number of clusters, K  
	* Optimal K will produce distinct, well-separated clusters  
    * WSS (within-sum-squares) measures the compactness of the clusters  
    * BSS (between-sum-squared) measures the distance between cluster centers  
    * Typically, try various k within a range, then select the smallest k such that WSS / (WSS + BSS) < 0.2 - similar to the elbow method  
  
Clustering Algorithms:  
  
* Selecting a clustering algorithm can be tricky since there are many possible algorithms - art as much as science  
	* Every algorithms has assumptions and constraints about the input data  
    * Helpful to understand the underlying clustering method  
* Representative-based clustering has centers/centroids, which are specific points that may or may not exist in the raw data - k-means, fuzzy c-means, k-medoids (PAM), expectation maximization  
* Connectivity-based methods connect objects based on their distance (furthest, closest, average, etc.) - Ward, SLINK  
	* Agglomerative - bottom-up  
    * Divisive - top-down  
* Density-based methods cluster based on areas of higher density in the data - DBSCAN, Optics, Mean-Shift  
* Clustering evaluation can be as tricky as cluster creation - also an art rather than a science  
	* Internal validation - single metric such as compactness or separation  
    * Extrenal validation - ground-truth (often not available)  
    * Manual validation - human expert review  
    * Indirect validation - assessment of how the clusters perform in an intended application  
  
Feature Selection:  
  
* The 'curse of dimensionality' is common in machine learning - too many dimensions lead to sparse data  
	* A lot of training data is needed - at least 5 training samples per feature is a good rule of thumb  
* Dimensionality reduction techniques work on both feature selection (same variables as original) and feature extraction (transformation on to a space of fewer dimensions)  
* Feature selection takes a subset of the features for further analysis - would require 2**N - 1 runs, so shortcuts are needed  
	* Filter methods - select subsets based on internal features such as correlations with each other and/or the target variable  
    * Wrapper methods - use ML methods to evaluate feature subsets, and then select the best subset (caution that overfitting can be problematic)  
    * Embedded methods - combination of filter and wrapper, where the learning algorithm implicitly carries out feature selection (can often return a ranking of feature importances)  
* Feature extraction maps features on to a lower dimension, with a goal of minimizing data loss  
	* New features are not interpretable, at least in the original feature space  
  
Feature Extraction:  
  
* Feature selection and feature extraction can be used on the same dataset  
	* If interpretability is important, prefer feature selection  
    * If computational considerations are important, prefer feature extraction  
    * Caution that feature selection can lead to getting stuck finding mappings to the outcome variable  
* A common question is compare PCA (principal component analysis) and LDA (linear discriminant analysis)  
	* Both extract a new feature set from the original dataset  
    * Both create linear combinations of the original features  
    * Both work better with normalized data and work only with continuous features  
    * PCA is an unsupervised method that creates component axes for maximum variance - principal components are uncorrelated and ranked by variance explained  
    * LDA is a supervised method for maximizing component axes for class-separation - the linear discriminants are the directions for maximizing class separation  
    * PCA and LDA can be used together - e.g., PCA for dimensionality reduction followed by LDA to identify axes for best class separation  
  
Example code includes:  
```{r}

mallData <- c(19, 21, 20, 23, 31, 22, 35, 23, 64, 30, 67, 35, 58, 24, 37, 22, 35, 20, 52, 35, 35, 25, 46, 31, 54, 29, 45, 35, 40, 23, 60, 21, 53, 18, 49, 21, 42, 30, 36, 20, 65, 24, 48, 31, 49, 24, 50, 27, 29, 31, 49, 33, 31, 59, 50, 47, 51, 69, 27, 53, 70, 19, 67, 54, 63, 18, 43, 68, 19, 32, 70, 47, 60, 60, 59, 26, 45, 40, 23, 49, 57, 38, 67, 46, 21, 48, 55, 22, 34, 50, 68, 18, 48, 40, 32, 24, 47, 27, 48, 20, 23, 49, 67, 26, 49, 21, 66, 54, 68, 66, 65, 19, 38, 19, 18, 19, 63, 49, 51, 50, 27, 38, 40, 39, 23, 31, 43, 40, 59, 38, 47, 39, 25, 31, 20, 29, 44, 32, 19, 35, 57, 32, 28, 32, 25, 28, 48, 32, 34, 34, 43, 39, 44, 38, 47, 27, 37, 30, 34, 30, 56, 29, 19, 31, 50, 36, 42, 33, 36, 32, 40, 28, 36, 36, 52, 30, 58, 27, 59, 35, 37, 32, 46, 29, 41, 30, 54, 28, 41, 36, 34, 32, 33, 38, 47, 35, 45, 32, 32, 30, 15000, 15000, 16000, 16000, 17000, 17000, 18000, 18000, 19000, 19000, 19000, 19000, 20000, 20000, 20000, 20000, 21000, 21000, 23000, 23000, 24000, 24000, 25000, 25000, 28000, 28000, 28000, 28000, 29000, 29000, 30000, 30000, 33000, 33000, 33000, 33000, 34000, 34000, 37000, 37000, 38000, 38000, 39000, 39000, 39000, 39000, 40000, 40000, 40000, 40000, 42000, 42000, 43000, 43000, 43000, 43000, 44000, 44000, 46000, 46000, 46000, 46000, 47000, 47000, 48000, 48000, 48000, 48000, 48000, 48000, 49000, 49000, 50000, 50000, 54000, 54000, 54000, 54000, 54000, 54000, 54000, 54000, 54000, 54000, 54000, 54000, 57000, 57000, 58000, 58000, 59000, 59000, 60000, 60000, 60000, 60000, 60000, 60000, 61000, 61000, 62000, 62000, 62000, 62000, 62000, 62000, 63000, 63000, 63000, 63000, 63000, 63000, 64000, 64000, 65000, 65000, 65000, 65000, 67000, 67000, 67000, 67000, 69000, 69000, 70000, 70000, 71000, 71000, 71000, 71000, 71000, 71000, 72000, 72000, 73000, 73000, 73000, 73000, 74000, 74000, 75000, 75000, 76000, 76000, 77000, 77000, 77000, 77000, 78000, 78000, 78000, 78000, 78000, 78000, 78000, 78000, 78000, 78000, 78000, 78000, 79000, 79000, 81000, 81000, 85000, 85000, 86000, 86000, 87000, 87000, 87000, 87000, 87000, 87000, 88000, 88000, 88000, 88000, 93000, 93000, 97000, 97000, 98000, 98000, 99000, 99000, 101000, 101000, 103000, 103000, 103000, 103000, 113000, 113000, 120000, 120000, 126000, 126000, 137000, 137000, 39, 81, 6, 77, 40, 76, 6, 94, 3, 72, 14, 99, 15, 77, 13, 79, 35, 66, 29, 98, 35, 73, 5, 73, 14, 82, 32, 61, 31, 87, 4, 73, 4, 92, 14, 81, 17, 73, 26, 75, 35, 92, 36, 61, 28, 65, 55, 47, 42, 42, 52, 60, 54, 60, 45, 41, 50, 46, 51, 46, 56, 55, 52, 59, 51, 59, 50, 48, 59, 47, 55, 42, 49, 56, 47, 54, 53, 48, 52, 42, 51, 55, 41, 44, 57, 46, 58, 55, 60, 46, 55, 41, 49, 40, 42, 52, 47, 50, 42, 49, 41, 48, 59, 55, 56, 42, 50, 46, 43, 48, 52, 54, 42, 46, 48, 50, 43, 59, 43, 57, 56, 40, 58, 91, 29, 77, 35, 95, 11, 75, 9, 75, 34, 71, 5, 88, 7, 73, 10, 72, 5, 93, 40, 87, 12, 97, 36, 74, 22, 90, 17, 88, 20, 76, 16, 89, 1, 78, 1, 73, 35, 83, 5, 93, 26, 75, 20, 95, 27, 63, 13, 75, 10, 92, 13, 86, 15, 69, 14, 90, 32, 86, 15, 88, 39, 97, 24, 68, 17, 85, 23, 69, 8, 91, 16, 79, 28, 74, 18, 83)
mall <- as.data.frame(matrix(data=mallData, ncol=3, byrow=FALSE))
names(mall) <- c("Age", "AnnualIncome", "SpendingScore")


# Glimpse over the mall data
glimpse(mall)

# Display the range of every variable
sapply(mall, range)

# Age histogram 
hist(mall$Age, breaks=10)

# Spending score histogram 
hist(mall$SpendingScore, breaks=10)

# Annual income histogram 
hist(mall$AnnualIncome, breaks=10)


mall_scaled <- scale(mall)


# Initialize vector: ratios
ratios <- rep(0, 10)

# Try different values of K
for (k in 1:10) {
    # Cluster mall: mall_c
    mall_c <- kmeans(mall_scaled, k, nstart=20)
    # Save the ratio WSS/TSS in the kth position of ratios
    ratios[k] <- mall_c$tot.withinss / mall_c$totss
}

# Line plot with ratios as a function of k
plot(ratios, type="b", xlab="number of clusters")


# Cluster mall_scaled data using k = 6: mall_6
set.seed(123)
mall_6 <- kmeans(mall_scaled, centers=6, nstart=20)

# Average each variable per cluster
mall %>%
    mutate(cluster = mall_6$cluster) %>%
    group_by(cluster) %>%
    summarize_all(list(~mean(.)))


library(clValid)

# Create the list of clustering methods: methods
methods <- c("hierarchical", "kmeans", "pam")

# Compare clustering methods: results
results <- clValid::clValid(mall_scaled, 2:10, clMethods = methods, validation = "internal")

# Summarize the results
summary(results)


# Create the list of clustering methods: methods
methods <- c("hierarchical", "kmeans", "pam")

# Compare clustering methods: results
results <- clValid(mall_scaled, 2:10, clMethods = methods, validation = "stability")

# Summarize the results
summary(results)


# Plot 3D mall_scaled data
plot3D::scatter3D(x = mall_scaled[, 1], y = mall_scaled[, 2], z = mall_scaled[, 3], col = "blue")

# Get K-means centroids for K = 7 and add them to the plot
km_centers <- results@clusterObjs$kmeans$`7`$centers
plot3D::points3D(km_centers[, 1], km_centers[, 2], km_centers[, 3], col = "red", pch=20, add=TRUE, cex=2.5)

# Get PAM's medoids for K = 7 and add them to the plot
pam_idxs <- results@clusterObjs$pam$'7'$medoids
pam_med <- mall_scaled[pam_idxs, ]
plot3D::points3D(pam_med[, 1], pam_med[, 2], pam_med[, 3], col = "green", pch=20, add=TRUE, cex=2.5)


appsOld <- apps


apps <- appsOld %>%
    select(Rating, Reviews, Installs, Type, Price, `Content Rating`) %>%
    rename(Content=`Content Rating`) %>%
    mutate(HasPositiveReviews=TRUE, Price=as.numeric(gsub('\\$', '', Price))) %>%
    filter(complete.cases(.))


# Glimpse at the data
glimpse(apps)

# Identify near-zero-variance predictors: nzv
nzv <- caret::nearZeroVar(apps, names=TRUE)
print(nzv)

# Frequency of the HasPositiveReviews attribute
table(apps$HasPositiveReviews)

# Frequency of the Price attribute
table(apps$Price)

# Remove these features: apps_clean
apps_clean <- apps %>% 
    select(-HasPositiveReviews, -Price)


# Glimpse at the fifa data
# glimpse(fifa)

# Are there zero or near-zero variance features?
# nearZeroVar(fifa)

# Highly correlated predictors: cor_90plus
# (cor_90plus <- findCorrelation(cor(fifa), names = TRUE))

# Highly correlated predictors (>= 98%): cor_98plus
# (cor_98plus <- findCorrelation(cor(fifa), names = TRUE, cutoff = 0.98))

# Remove cor_90plus features: fifa_clean
# fifa_clean <- fifa %>% 
#     select(-cor_90plus)


# Train model on original scaled data: mdl_orig
# mdl_orig <- train(Club ~ ., data = team_train, method="svmLinear2", trControl = trainCtrl)

# Predict on original test data: orig_preds, orig_probs
# orig_preds <- predict(mdl_orig, team_test)
# orig_probs <- predict(mdl_orig, team_test, type="prob")

# Compute and print the confusion matrix: cm_orig
# (cm_orig <- confusionMatrix(orig_preds, team_test$Club))

# Compute and print AUC: auc_orig
# (auc_orig <- auc(team_test$Club == 'Real.Madrid', orig_probs$'Real.Madrid'))


# Transform training and test data: train_pca, test_pca
# pca <- preProcess(x = team_train[, -match("Club", names(team_train))], method = "pca")
# train_pca <- predict(pca, team_train)
# test_pca <- predict(pca, team_test)

# Train model on PCA data: mdl_pca
# mdl_pca <- train(Club ~ ., data = train_pca, method = "svmLinear2", trControl = trainCtrl)

# Predict on PCA data: pca_preds, pca_probs
# pca_preds <- predict(mdl_pca, test_pca)
# pca_probs <- predict(mdl_pca, test_pca, type = "prob")

# Compute and print confusion matrix & AUC: cm_pca, auc_pca
# (cm_pca <- confusionMatrix(pca_preds, test_pca$Club))
# (auc_pca <- auc(test_pca$Club == 'Real.Madrid', pca_probs$'Real.Madrid'))


# Transform training and test data: train_lda, test_lda
# my_lda <- lda(Club ~ ., data = team_train)
# train_lda <- as.data.frame(predict(my_lda, team_train))
# test_lda <- as.data.frame(predict(my_lda, team_test))

# Train model on LDA-preprocessed data: mdl_lda
# mdl_lda <- train(class ~ ., data = train_lda, method="svmLinear2", trControl = trainCtrl)

# Predict on LDA-ed test data: lda_preds, lda_probs
# lda_preds <- predict(mdl_lda, test_lda)
# lda_probs <- predict(mdl_lda, test_lda, type="prob")

# Compute and print confusion matrix & AUC: cm_lda, auc_lda
# (cm_lda <- confusionMatrix(lda_preds, test_lda$class))
# (auc_lda <- auc(test_lda$class == 'Real.Madrid', lda_probs$Real.Madrid))

```
  
  
  
***
  
Chapter 4 - Model Evaluation  
  
Model Evaluation:  
  
* Several aspects should be considered when evaluating an ML model - classification, regression, and clustering have different techniques  
	* Make sure that the evaluation is realistic through appropriate holdout (test-validate-train)  
    * Cross-validation can be a useful technique for estimating OOB and OOS errors  
* Confusion matrices and ROC/AUC are commonly used for assessing classification algorithms  
	* May want to modify accuracy to a cost-sensitive accuracy (errors with different penalties) or work with balanced classes  
* Regression models are often evaluated using RMSE  
* Clustering is often evaluate based on some mix of WSS and BSS - compactness and good separation  
  
Handling Imbalanced Data:  
  
* Significantly imbalanced data can lead to algorithmic and reporting (especially accuracy) problems for classification data  
* Two popular avenues for dealing with imbalanced data - cost-sensitive classification, or sub-sampling  
	* In cost-sensitive classification, the goal is to minimize the cost of classification errors  
    * In subsampling training data, can downsample majority; upsample minority; and SMOTE (synthetic minority oversampling technique)  
* Can subsample before model evaluation, though this can lead to over-optimistic assessments of the model  
	* Subsamping during model training (e.g., bootstraps) is computationally more expensive but also more likely to give realistic assessments of model performance  
  
Hyperparameter Tuning:  
  
* Model parameters are learned during the training process while hyperparameters are provided as inputs to the model  
	* Tuning hyperparameters can be seen as part (often iterative) of a meta-learning process  
* Three main hyperparameter tuning strategies include grid search, random search, and informed search  
	* Grid search is an exhaustive search over all possible combinations of hyperparameters - very computationally expensive but also easy to parallelize  
    * Random search involves sampling from possible combination of hyperparameters - easy to parallelize and can outperform grid search on continuous variables  
    * Informed search methods like Bayesian optimization pick parameters based on performance - samples more aggressively around more promising values  
* Several R packages can help with hyperparameter tuning - caret, mlr, h2o  
  
Random Forests or Gradient Boosted Trees:  
  
* Random Forests and Gradient Boosted Trees are both popular and successful models  
	* Both are top performers for classification or regression  
    * Both use decision trees as base learners and can handle missing values and require a number of trees to be pre-specified  
    * Random forests use bagging on deep trees (minimize bias) and ensembling to reduce variance; grown in parallel and easier to tune  
    * Gradient boosted trees use boosting on shallow trees (minimize variance) and ensembling to reduce bias; grown sequentially, with trees added only as needed  
* Random forests can be run using several packages including randomForest and ranger, or inside caret  
	* tunedModel <- randomForest::tuneRF(x=predictors, y=response, nTreeTry=500)  # tunes mtry  
    * tunedModel <- caret::train(x=predictors, y=response, method="rf")  # tunes mtry by default, and can specify others for tuning  
* Gradient boosted trees can be run using several packages including gbm and xgboost  
	* opt_ntree_cv <- gbm::gbm.perf(model, method="cv")  
    * opt_ntree_oob <- gbm::gbm.perf(model, method="OOB")  
    * model <- caret::train(x=predictors, y=response, method="xgbLinear")  
  
Wrap Up:  
  
* Data preprocessing and visualization - scaling, missing data, imputation, anomaly detection  
* Supervised learning - model interpretability, regularization, bias-variance trade-offs, ensembling  
* Unsupervised learning - clustering (k-means, hierarchical, RAM), feature selection, NZV, feature extraction  
* Model selection and evaluation - classification, regression, clustering, class imbalances, hyperparameter tuning, RF vs. GBM  
  
Example code includes:  
```{r}

apps <- appsOld %>%
    select(Category, Rating, Reviews, Size, Installs, `Content Rating`) %>%
    rename(Content.Rating=`Content Rating`) %>%
    filter(complete.cases(.), Category %in% c("EDUCATION", "ENTERTAINMENT"), 
           Size!="Varies with device"
           ) %>%
    mutate(Category=factor(Category), Installs=factor(Installs), Content.Rating=factor(Content.Rating))

appSize <- rep(NA, nrow(apps))
mbSize <- grep("^[0-9][0-9\\.]*M", apps$Size)
kbSize <- grep("^[0-9][0-9\\.]*k", apps$Size)
appSize[mbSize] <- as.numeric(gsub('M', '', apps$Size[mbSize]))
appSize[kbSize] <- as.numeric(gsub('k', '', apps$Size[kbSize])) / 1000

apps$Size <- appSize
glimpse(apps)


set.seed(1912261548)
trIndex <- sort(sample(1:nrow(apps), round(0.75*nrow(apps)), replace=FALSE))
training <- apps[trIndex, ]
testing <- apps[-trIndex, ]


cv10 <- caret::trainControl(method="cv", number=10, classProbs=TRUE, 
                            summaryFunction=caret::twoClassSummary
                            )

# Create KNN model: mdlKNN
set.seed(123)
mdlKNN <- train(Category ~ ., data = training, method = "knn", trControl = cv10, metric="ROC")

# Print the KNN model and its confusion matrix
print(mdlKNN)
ModelMetrics::confusionMatrix(predict(mdlKNN, testing), testing$Category)

# Predict class labels and probs: knn_preds, knn_probs
knn_preds <- predict(mdlKNN, newdata = testing)
knn_probs <- predict(mdlKNN, newdata = testing, type="prob")

# Print accuracy and AUC values
print(Metrics::accuracy(testing$Category, knn_preds))
print(Metrics::auc(testing$Category == 'ENTERTAINMENT', knn_probs[, 2]))


# Train SVM: mdlSVM
# set.seed(123)
# mdlSVM <- train(Overall ~ ., data = training, method = "svmRadial", trControl = cv10)

# Print the SVM model
# print(mdlSVM)

# Predict overall score on testing data: svm_preds
# svm_preds <- predict(mdlSVM, newdata = testing)

# Print RMSE and MAE values
# print(rmse(testing$Overall, svm_preds))
# print(mae(testing$Overall, svm_preds))


# Glimpse at the data
glimpse(mall_scaled)

# Run DIANA: results
results <- clValid::clValid(mall_scaled, 2:10, clMethods = "diana", validation = "internal")

# Print and summarize results
print(results)
summary(results)

# Plot results
plot(results)


# Glimpse at the data
# glimpse(pulsar)

# Is there a class imbalance?
# table(pulsar$target_class)

# Set seed and partition data
# set.seed(123)
# inTrain <- createDataPartition(y = pulsar$target_class, p = .75, list = FALSE)
# training <- pulsar[inTrain,]
# testing <- pulsar[-inTrain,]

# Is there class imbalance in the training and test sets?
# table(training$target_class)
# table(testing$target_class)


trainDTree <- function(train_data, samplingMode = NULL) {
    set.seed(123)
    ctrl <- trainControl(method = "cv", number = 10, classProbs = TRUE,
                         summaryFunction = twoClassSummary, sampling = samplingMode
                         )
    train(target_class ~ ., data = train_data, method = "rpart", metric = "ROC", trControl = ctrl)
}

# Train and print model with no subsampling: mdl_orig
# (mdl_orig <- trainDTree(training))

# Train model with downsampling: mdl_down
# (mdl_down <- trainDTree(training, samplingMode = "down"))

# Train model with upsampling: mdl_up
# (mdl_up <- trainDTree(training, samplingMode = "up"))

# Train model with SMOTE: mdl_smote
# (mdl_smote <- trainDTree(training, samplingMode = "smote"))


get_auc <- function(model, data) {
    library(Metrics)
    preds <- predict(model, data, type = "prob")[, "yes"]
    auc(data$target_class == "yes", preds)
}

# Create model list: mdl_list
# mdl_list <- list(orig = mdl_orig, down = mdl_down, up = mdl_up, smote = mdl_smote)

# Compute AUC on training subsamples: resampling
# resampling <- resamples(mdl_list)
# summary(resampling, metric="ROC")

# Compute AUC on test data: auc_values
# auc_values <- sapply(mdl_list, FUN=get_auc, data = testing)
# print(auc_values)


set.seed(1912261602)
carIdx <- sort(sample(1:nrow(car), round(0.75*nrow(car)), replace=FALSE))
car_train <- car[carIdx, ]
car_test <- car[-carIdx, ]


# Set up train control: trc
trc <- caret::trainControl(method = "repeatedcv", number = 3, repeats = 5)

# Train model: svmr
svmr <- caret::train(consume ~ ., data = car_train, method = "svmRadial", trControl = trc)

# Print and plot SVM model
print(svmr)
plot(svmr)


# Set up train control: trc
trc <- caret::trainControl(method = "cv", number = 10)

# Create custom hyperparameter grid: hp_grid
hp_grid <- expand.grid(C = seq(from=0.2, to=1.0, by=0.2), sigma = c(0.35, 0.6, 0.75))

# Train model: svmr
svmr <- caret::train(consume ~ ., data = car_train, method = "svmRadial", trControl = trc, tuneGrid = hp_grid)

# Print and plot SVM model
print(svmr)
plot(svmr)


# Set random seed
set.seed(42)

# Set up train control: trc
trc <- caret::trainControl(method = "cv", number = 10, search = "random")

# Train model: svmr
svmr <- caret::train(consume ~ ., data = car_train, method = "svmRadial", trControl = trc, tuneLength = 10)

# Print and plot SVM model
print(svmr)
plot(svmr)


# Train the RF model: mdlRF
mdlRF <- randomForest::randomForest(formula = Rating ~ ., data = training, ntree = 500)

# Print the RF model
print(mdlRF)

# RF variable importance
randomForest::varImpPlot(mdlRF)
print(mdlRF$importance)


# Train a GBM model with 500 trees: mdlGBM
mdlGBM <- gbm::gbm(formula = Rating ~ ., data = training, n.trees = 500)

# Print GBM model
print(mdlGBM)

# Summarize GBM's variable importance
summary(mdlGBM)


# Predict on the testing data: gbm_preds, rf_preds
gbm_preds <- predict(mdlGBM, n.trees = 500, newdata = testing)
rf_preds <- predict(mdlRF, newdata = testing)

# RMSE metric for both models: gbm_rmse, rf_rmse
(gbm_rmse <- Metrics::rmse(testing$Rating, gbm_preds))
(rf_rmse <- Metrics::rmse(testing$Rating, rf_preds))

# RRSE metric for both models: gbm_rrse, rf_rrse
(gbm_rrse <- Metrics::rrse(testing$Rating, gbm_preds))
(rf_rrse <- Metrics::rrse(testing$Rating, rf_preds))

```
  
  
  
***
  
### _Introduction to Natural Language Processing in R_  
  
Chapter 1 - True Fundamentals  
  
Regular Expression Basics:  
  
* NLP is natural language processing which focuses on using computers to understand and analyze text  
	* Classifiers, Topic Modeling, Named Entity Recognition, Sentiment Analysis, etc.  
* Regular expressions are a sequence of characters used to search text  
	* \w is any alphanumeric  
    * \d is any digit  
    * The plus(+) means 1 or more  
    * \s is any whitespace  
    * \S is any non-whitespace (capitalizing means negation, so capital S is not whitespace)  
* R takes regular expressions in grep() and gsub()  
	* grep(pattern, x, value=FALSE) will find matches to pattern in vector x  
    * gsub(pattern, replacement, x) wll replace pattern with replacement in vector x  
* Can use regexone.com for learning more about regular expressions  
  
Tokenization:  
  
* Tokens can be as small as individual characters or as large as the entire text document  
* The tidytext package follows the tidy format  
* This course will use several datasets, including chapters from the book animal_farm  
	* animal_farm %>% unnest_tokens(output="word", input=text_column, token="words")  # token can be 'sentences', 'lines', and many others  
    * animal_farm %>% filter(chapter=="Chapter 1") %>% unnest_tokens(output="Boxer", input=text_column, token="regex", pattern="(?i)boxer") %>% slice(2:n())  
  
Text Cleaning Basics:  
  
* Russian tweet dataset is available from fiethirtyeight.com  
	* russian_tweets %>% unnest_tokens(word, content) %>% count(word, sort=TRUE)  
    * russian_tweets %>% unnest_tokens(word, content) %>% anti_join(stop_words) %>% count(word, sort=TRUE)  
* Can add custom stop words to the existing stop words list  
	* custom <- add_row(stop_words, word="https", lexicon="custom")  
    * custom <- add_row(custom, word="t.co", lexicon="custom")  
* Stemming is the process of converting words to their roots - for example, enlisted has a stem of enlist  
	* tidy_tweets <- russian_tweets %>% unnest_tokens(word, content) %>% anti_join(custom)  
    * stemmed_tweets <- tidy_tweets %>% mutate(word=SnowballC::wordStem(word))  
  
Example code includes:  
```{r}

text <- c("John's favorite color two colors are blue and red.", "John's favorite number is 1111.", 'John lives at P Sherman, 42 Wallaby Way, Sydney', 'He is 7 feet tall', 'John has visited 30 countries', 'John only has nine fingers.', 'John has worked at eleven different jobs', 'He can speak 3 languages', "john's favorite food is pizza", 'John can name 10 facts about himself.')

# Print off each item that contained a numeric number
grep(pattern = "\\d", x = text, value = TRUE)

# Find all items with a number followed by a space
grep(pattern = "\\d\\s", x = text)

# How many times did you write down 'favorite'?
length(grep(pattern = "favorite", x = text))


# Print off the text for every time you used your boss's name, John
grep('John', x = text, value = TRUE)

# Try replacing all occurences of "John" with "He"
gsub(pattern = 'John', replacement = 'He ', x = text)

# Replace all occurences of "John " with 'He '.
clean_text <- gsub(pattern = 'John\\s', replacement = 'He ', x = text)
clean_text

# Replace all occurences of "John's" with 'His'
gsub(pattern = "John\\'s", replacement = 'His', x = clean_text)


animal_farm <- read_csv("./RInputFiles/animal_farm.csv")
str(animal_farm)


# Split the text_column into sentences
animal_farm %>%
    tidytext::unnest_tokens(output = "sentences", input = text_column, token = "sentences") %>%
    # Count sentences, per chapter
    count(chapter)

# Split the text_column using regular expressions
animal_farm %>%
    tidytext::unnest_tokens(output = "sentences", input = text_column, token = "regex", pattern = "\\.") %>%
    count(chapter)


# Tokenize animal farm's text_column column
tidy_animal_farm <- animal_farm %>%
    tidytext::unnest_tokens(word, text_column) 

# Print the word frequencies
tidy_animal_farm %>%
    count(word, sort = TRUE)

# Remove stop words, using stop_words from tidytext
str(tidy_animal_farm)
tidy_animal_farm <- tidy_animal_farm %>%
    anti_join(tidytext::stop_words)
str(tidy_animal_farm)


# Perform stemming on tidy_animal_farm
stemmed_animal_farm <- tidy_animal_farm %>%
    mutate(word = SnowballC::wordStem(word))

# Print the old word frequencies 
tidy_animal_farm %>%
    count(word, sort = TRUE)

# Print the new word frequencies
stemmed_animal_farm %>%
    count(word, sort = TRUE)

```
  
  
  
***
  
Chapter 2 - Representations of Text  
  
Understanding an R Corpus:  
  
* Corpora are collections of documents containing natural language text  
	* The most common representation is the Vcorpus, which holds the text and the metadata  
    * The corpus 'acq' is available in library™  
    * acq[[1]]$meta # first article metadata  
    * acq[[1]]$meta$places # places values of first article metadata  
    * acq[[1]]$content # first article content  
* Can tidy a corpus so that each observation is represented by a row  
	* tidy_data <- tidy(acq)  
    * corpus <- VCorpus(VectorSource(tidy_data$text))  # convert from tibble to vcorpus, which will convert over only the text  
    * meta(corpus, "Author") <- tidy_data$author  # add author  
    * meta(corpus, "oldid") <- tidy_data$oldid  # add oldid  
  
Bag-of-words Representation:  
  
* Typical vector representations include all words, converted to lowercase, that are in the document  
	* A count by document of each word is then produced (can be boolean or count of word in document)  
* Sparse matrices are essential for text analysis - 20k x 40k matrix may have only a few thousand non-zero entries (well under 1%)  
  
TFIDF - Term Frequency Inverse Document Frequency:  
  
* The TFIDF considers both term frequency in a specifc text and inverse of term frequency in the overall corpus  
	* IDF = log(N / n-with-word)  # many other possibilities for calculating IDF, though this is a very common one; calculated by word  
    * TF = N / n-in-document  # calculated by document-word  
* Example for creation of a TFIDF matrix  
	* df %>% unnest_tokens(output="word", token="words", input=text) %>% anti_join(stop_words) %>% count(ID, word, sort=TRUE) %>% bind_tf_idf(word, ID, n)  
    * Creates columns tf, idf, and tf_idf (multiplication of tf*idf)  
  
Cosine Similarity:  
  
* Can use TFIDF to assess article similarities using cosine similarity (angle formed by two vectors in n-space)  
	* A-dot-B/(mag(A)*mag(B))  
    * pairwise_similarity(tbl, item, feature, value, …)  # item to be compared, feature that links the items, value of the feature  
    * crude_weights %>% pairwise_similarity(X, word, tf_idf) %>% arrange(desc(similarity))  # will range between 0 (nothing in common) and 1 (identical)  
  
Example code includes:  
```{r}

crudeText <- c('Diamond Shamrock Corp said that\neffective today it had cut its contract prices for crude oil by\n1.50 dlrs a barrel.\n    The reduction brings its posted price for West Texas\nIntermediate to 16.00 dlrs a barrel, the copany said.\n    \"The price reduction today was made in the light of falling\noil product prices and a weak crude oil market,\" a company\nspokeswoman said.\n    Diamond is the latest in a line of U.S. oil companies that\nhave cut its contract, or posted, prices over the last two days\nciting weak oil markets.\n Reuter')
crudeText <- c(crudeText, 'OPEC may be forced to meet before a\nscheduled June session to readdress its production cutting\nagreement if the organization wants to halt the current slide\nin oil prices, oil industry analysts said.\n    \"The movement to higher oil prices was never to be as easy\nas OPEC thought. They may need an emergency meeting to sort out\nthe problems,\" said Daniel Yergin, director of Cambridge Energy\nResearch Associates, CERA.\n    Analysts and oil industry sources said the problem OPEC\nfaces is excess oil supply in world oil markets.\n    \"OPECs problem is not a price problem but a production\nissue and must be addressed in that way,\" said Paul Mlotok, oil\nanalyst with Salomon Brothers Inc.\n    He said the markets earlier optimism about OPEC and its\nability to keep production under control have given way to a\npessimistic outlook that the organization must address soon if\nit wishes to regain the initiative in oil prices.\n    But some other analysts were uncertain that even an\nemergency meeting would address the problem of OPEC production\nabove the 15.8 mln bpd quota set last December.\n    \"OPEC has to learn that in a buyers market you cannot have\ndeemed quotas, fixed prices and set differentials,\" said the\nregional manager for one of the major oil companies who spoke\non condition that he not be named. \"The market is now trying to\nteach them that lesson again,\" he added.\n    David T. Mizrahi, editor of Mideast reports, expects OPEC\nto meet before June, although not immediately. However, he is\nnot optimistic that OPEC can address its principal problems.\n    \"They will not meet now as they try to take advantage of the\nwinter demand to sell their oil, but in late March and April\nwhen demand slackens,\" Mizrahi said.\n    But Mizrahi said that OPEC is unlikely to do anything more\nthan reiterate its agreement to keep output at 15.8 mln bpd.\"\n    Analysts said that the next two months will be critical for\nOPECs ability to hold together prices and output.\n    \"OPEC must hold to its pact for the next six to eight weeks\nsince buyers will come back into the market then,\" said Dillard\nSpriggs of Petroleum Analysis Ltd in New York.\n    But Bijan Moussavar-Rahmani of Harvard Universitys Energy\nand Environment Policy Center said that the demand for OPEC oil\nhas been rising through the first quarter and this may have\nprompted excesses in its production.\n    \"Demand for their (OPEC) oil is clearly above 15.8 mln bpd\nand is probably closer to 17 mln bpd or higher now so what we\nare seeing characterized as cheating is OPEC meeting this\ndemand through current production,\" he told Reuters in a\ntelephone interview.\n Reuter')
crudeText <- c(crudeText, 'Texaco Canada said it lowered the\ncontract price it will pay for crude oil 64 Canadian cts a\nbarrel, effective today.\n    The decrease brings the companys posted price for the\nbenchmark grade, Edmonton/Swann Hills Light Sweet, to 22.26\nCanadian dlrs a bbl.\n    Texaco Canada last changed its crude oil postings on Feb\n19.\n Reuter')
crudeText <- c(crudeText, 'Marathon Petroleum Co said it reduced\nthe contract price it will pay for all grades of crude oil one\ndlr a barrel, effective today.\n    The decrease brings Marathons posted price for both West\nTexas Intermediate and West Texas Sour to 16.50 dlrs a bbl. The\nSouth Louisiana Sweet grade of crude was reduced to 16.85 dlrs\na bbl.\n    The company last changed its crude postings on Jan 12.\n Reuter')
crudeText <- c(crudeText, 'Houston Oil Trust said that independent\npetroleum engineers completed an annual study that estimates\nthe trusts future net revenues from total proved reserves at\n88 mln dlrs and its discounted present value of the reserves at\n64 mln dlrs.\n    Based on the estimate, the trust said there may be no money\navailable for cash distributions to unitholders for the\nremainder of the year.\n    It said the estimates reflect a decrease of about 44 pct in\nnet reserve revenues and 39 pct in discounted present value\ncompared with the study made in 1985.\n Reuter')
crudeText <- c(crudeText, 'Kuwait\"s Oil Minister, in remarks\npublished today, said there were no plans for an emergency OPEC\nmeeting to review oil policies after recent weakness in world\noil prices.\n    Sheikh Ali al-Khalifa al-Sabah was quoted by the local\ndaily al-Qabas as saying: \"None of the OPEC members has asked\nfor such a meeting.\"\n    He denied Kuwait was pumping above its quota of 948,000\nbarrels of crude daily (bpd) set under self-imposed production\nlimits of the 13-nation organisation.\n    Traders and analysts in international oil markets estimate\nOPEC is producing up to one mln bpd above a ceiling of 15.8 mln\nbpd agreed in Geneva last December.\n    They named Kuwait and the United Arab Emirates, along with\nthe much smaller producer Ecuador, among those producing above\nquota. Kuwait, they said, was pumping 1.2 mln bpd.\n    \"This rumour is baseless. It is based on reports which said\nKuwait has the ability to exceed its share. They suppose that\nbecause Kuwait has the ability, it will do so,\" the minister\nsaid.\n    Sheikh Ali has said before that Kuwait had the ability to\nproduce up to 4.0 mln bpd.\n    \"If we can sell more than our quota at official prices,\nwhile some countries are suffering difficulties marketing their\nshare, it means we in Kuwait are unusually clever,\" he said.\n    He was referring apparently to the Gulf state of qatar,\nwhich industry sources said was selling less than 180,000 bpd\nof its 285,000 bpd quota, because buyers were resisting\nofficial prices restored by OPEC last month pegged to a marker\nof 18 dlrs per barrel.\n    Prices in New York last week dropped to their lowest levels\nthis year and almost three dollars below a three-month high of\n19 dollars a barrel.\n    Sheikh Ali also delivered \"a challenge to any international\noil company that declared Kuwait sold below official prices.\"\n    Because it was charging its official price, of 16.67 dlrs a\nbarrel, it had lost custom, he said but did not elaborate.\n    However, Kuwait had guaranteed markets for its oil because\nof its local and international refining facilities and its own\ndistribution network abroad, he added.\n    He reaffirmed that the planned meeting March 7 of OPEC\"s\ndifferentials committee has been postponed until the start of\nApril at the request of certain of the body\"s members.\n    Ecuador\"s deputy energy minister Fernando Santos Alvite said\nlast Wednesday his debt-burdened country wanted OPEC to assign\na lower official price for its crude, and was to seek this at\ntalks this month of opec\"s pricing committee.\n    Referring to pressure by oil companies on OPEC members, in\napparent reference to difficulties faced by Qatar, he said: \"We\nexpected such pressure. It will continue through March and\nApril.\" But he expected the situation would later improve.\n REUTER')
crudeText <- c(crudeText, 'Indonesia appears to be nearing a\npolitical crossroads over measures to deregulate its protected\neconomy, the U.S. Embassy says in a new report.\n    To counter falling oil revenues, the government has\nlaunched a series of measures over the past nine months to\nboost exports outside the oil sector and attract new\ninvestment.\n    Indonesia, the only Asian member of OPEC and a leading\nprimary commodity producer, has been severely hit by last year\"s\nfall in world oil prices, which forced it to devalue its\ncurrency by 31 pct in September.\n    But the U.S. Embassy report says President Suharto\"s\ngovernment appears to be divided over what direction to lead\nthe economy.\n    \"(It) appears to be nearing a crossroads with regard to\nderegulation, both as it pertains to investments and imports,\"\nthe report says. It primarily assesses Indonesia\"s agricultural\nsector, but also reviews the country\"s general economic\nperformance.\n    It says that while many government officials and advisers\nare recommending further relaxation, \"there are equally strong\npressures being exerted to halt all such moves.\"\n    \"This group strongly favours an import substitution economy,\"\nthe report says.\n    Indonesia\"s economic changes have been welcomed by the World\nBank and international bankers as steps in the right direction,\nthough they say crucial areas of the economy like plastics and\nsteel remain highly protected, and virtual monopolies.\n    Three sets of measures have been announced since last May,\nwhich broadened areas for foreign investment, reduced trade\nrestrictions and liberalised imports.\n    The report says Indonesia\"s economic growth in calendar 1986\nwas probably about zero, and the economy may even have\ncontracted a bit. \"This is the lowest rate of growth since the\nmid-1960s,\" the report notes.\n    Indonesia, the largest country in South-East Asia with a\npopulation of 168 million, is facing general elections in\nApril.\n    But the report hold out little hope for swift improvement\nin the economic outlook. \"For 1987 early indications point to a\nslightly positive growth rate not exceeding one pct. Economic\nactivity continues to suffer due to the sharp fall in export\nearnings from the petroleum industry.\"\n    \"Growth in the non-oil sector is low because of weak\ndomestic demand coupled with excessive plant capacity, real\ndeclines in construction and trade, and a reduced level of\ngrowth in agriculture,\" the report states.\n    Bankers say continuation of present economic reforms is\ncrucial for the government to get the international lending its\nneeds.\n    A new World Bank loan of 300 mln dlrs last month in balance\nof payments support was given partly to help the government\nmaintain the momentum of reform, the Bank said.\n REUTER')
crudeText <- c(crudeText, 'Saudi riyal interbank deposits were\nsteady at yesterdays higher levels in a quiet market.\n    Traders said they were reluctant to take out new positions\namidst uncertainty over whether OPEC will succeed in halting\nthe current decline in oil prices.\n    Oil industry sources said yesterday several Gulf Arab\nproducers had had difficulty selling oil at official OPEC\nprices but Kuwait has said there are no plans for an emergency\nmeeting of the 13-member organisation.\n    A traditional Sunday lull in trading due to the European\nweekend also contributed to the lack of market activity.\n    Spot-next and one-week rates were put at 6-1/4, 5-3/4 pct\nafter quotes ranging between seven, six yesterday.\n    One, three, and six-month deposits were quoted unchanged at\n6-5/8, 3/8, 7-1/8, 6-7/8 and 7-3/8, 1/8 pct respectively.\n    The spot riyal was quietly firmer at 3.7495/98 to the\ndollar after quotes of 3.7500/03 yesterday.\n REUTER')
crudeText <- c(crudeText, 'The Gulf oil state of Qatar, recovering\nslightly from last years decline in world oil prices,\nannounced its first budget since early 1985 and projected a\ndeficit of 5.472 billion riyals.\n    The deficit compared with a shortfall of 7.3 billion riyals\nin the last published budget for 1985/86.\n    In a statement outlining the budget for the fiscal year\n1987/88 beginning today, Finance and Petroleum Minister Sheikh\nAbdul-Aziz bin Khalifa al-Thani said the government expected to\nspend 12.217 billion riyals in the period.\n    Projected expenditure in the 1985/86 budget had been 15.6\nbillion riyals.\n    Sheikh Abdul-Aziz said government revenue would be about\n6.745 billion riyals, down by about 30 pct on the 1985/86\nprojected revenue of 9.7 billion.\n    The government failed to publish a 1986/87 budget due to\nuncertainty surrounding oil revenues.\n    Sheikh Abdul-Aziz said that during that year the government\ndecided to limit recurrent expenditure each month to\none-twelfth of the previous fiscal years allocations minus 15\npct.\n    He urged heads of government departments and public\ninstitutions to help the government rationalise expenditure. He\ndid not say how the 1987/88 budget shortfall would be covered.\n    Sheikh Abdul-Aziz said plans to limit expenditure in\n1986/87 had been taken in order to relieve the burden placed on\nthe countrys foreign reserves.\n    He added in 1987/88 some 2.766 billion riyals had been\nallocated for major projects including housing and public\nbuildings, social services, health, education, transport and\ncommunications, electricity and water, industry and\nagriculture.\n    No figure was revealed for expenditure on defence and\nsecurity. There was also no projection for oil revenue.\n    Qatar, an OPEC member, has an output ceiling of 285,000\nbarrels per day.\n    Sheikh Abdul-Aziz said: \"Our expectations of positive signs\nregarding (oil) price trends, foremost among them OPECs\ndetermination to shoulder its responsibilites and protect its\nwealth, have helped us make reasonable estimates for the coming\nyears revenue on the basis of our assigned quota.\"\n REUTER')
crudeText <- c(crudeText, 'Saudi Arabian Oil Minister Hisham Nazer\nreiterated the kingdoms commitment to last Decembers OPEC\naccord to boost world oil prices and stabilise the market, the\nofficial Saudi Press Agency SPA said.\n    Asked by the agency about the recent fall in free market\noil prices, Nazer said Saudi Arabia \"is fully adhering by the\n... Accord and it will never sell its oil at prices below the\npronounced prices under any circumstance.\"\n    Nazer, quoted by SPA, said recent pressure on free market\nprices \"may be because of the end of the (northern hemisphere)\nwinter season and the glut in the market.\"\n    Saudi Arabia was a main architect of the December accord,\nunder which OPEC agreed to lower its total output ceiling by\n7.25 pct to 15.8 mln barrels per day (bpd) and return to fixed\nprices of around 18 dlrs a barrel.\n    The agreement followed a year of turmoil on oil markets,\nwhich saw prices slump briefly to under 10 dlrs a barrel in\nmid-1986 from about 30 dlrs in late 1985. Free market prices\nare currently just over 16 dlrs.\n    Nazer was quoted by the SPA as saying Saudi Arabias\nadherence to the accord was shown clearly in the oil market.\n    He said contacts among members of OPEC showed they all\nwanted to stick to the accord.\n    In Jamaica, OPEC President Rilwanu Lukman, who is also\nNigerian Oil Minister, said the group planned to stick with the\npricing agreement.\n    \"We are aware of the negative forces trying to manipulate\nthe operations of the market, but we are satisfied that the\nfundamentals exist for stable market conditions,\" he said.\n    Kuwaits Oil Minister, Sheikh Ali al-Khalifa al-Sabah, said\nin remarks published in the emirates daily Al-Qabas there were\nno plans for an emergency OPEC meeting to review prices.\n    Traders and analysts in international oil markets estimate\nOPEC is producing up to one mln bpd above the 15.8 mln ceiling.\n    They named Kuwait and the United Arab Emirates, along with\nthe much smaller producer Ecuador, among those producing above\nquota. Sheikh Ali denied that Kuwait was over-producing.\n REUTER')
crudeText <- c(crudeText, 'Saudi crude oil output last month fell\nto an average of 3.5 mln barrels per day (bpd) from 3.8 mln bpd\nin January, Gulf oil sources said.\n    They said exports from the Ras Tanurah and Juaymah\nterminals in the Gulf fell to an average 1.9 mln bpd last month\nfrom 2.2 mln in January because of lower liftings by some\ncustomers.\n    But the drop was much smaller than expected after Gulf\nexports rallied in the fourth week of February to 2.5 mln bpd\nfrom 1.2 mln in the third week, the sources said.\n    The production figures include neutral zone output but not\nsales from floating storage, which are generally considered\npart of a countrys output for Opec purposes.\n    Saudi Arabia has an Opec quota of 4.133 mln bpd under a\nproduction restraint scheme approved by the 13-nation group\nlast December to back new official oil prices averaging 18 dlrs\na barrel.\n    The sources said the two-fold jump in exports last week\nappeared to be the result of buyers rushing to lift February\nentitlements before the month-end.\n    Last weeks high export levels appeared to show continued\nsupport for official Opec prices from Saudi Arabias main crude\ncustomers, the four ex-partners of Aramco, the sources said.\n    The four -- Exxon Corp <XON>, Mobil Corp <MOB>, Texaco Inc\n<TX> and Chevron Corp <CHV> -- signed a long-term agreement\nlast month to buy Saudi crude for 17.52 dlrs a barrel.\n    However the sources said the real test of Saudi Arabias\nability to sell crude at official prices in a weak market will\ncome this month, when demand for petroleum products\ntraditionally tapers off. Spot prices have fallen in recent\nweeks to more than one dlr below Opec levels.\n    Saudi Arabian oil minister Hisham Nazer yesterday\nreiterated the kingdoms commitment to the December OPEC accord\nand said it would never sell below official prices.\n    The sources said total Saudi refinery throughput fell\nslightly in February to an average 1.1 mln bpd from 1.2 mln in\nJanuary because of cuts at the Yanbu and Jubail export\nrefineries.\n    They put crude oil exports through Yanbu at 100,000 bpd\nlast month, compared to zero in January, while throughput at\nBahrains refinery and neutral zone production remained steady\nat around 200,000 bpd each.\n REUTER')
crudeText <- c(crudeText, 'Deputy oil ministers from six Gulf\nArab states will meet in Bahrain today to discuss coordination\nof crude oil marketing, the official Emirates news agency WAM\nreported.\n    WAM said the officials would be discussing implementation\nof last Sundays agreement in Doha by Gulf Cooperation Council\n(GCC) oil ministers to help each other market their crude oil.\n    Four of the GCC states - Saudi Arabia, the United Arab\nEmirates (UAE), Kuwait and Qatar - are members of the\nOrganiaation of Petroleum Exporting Countries (OPEC) and some\nface stiff buyer resistance to official OPEC prices.\n Reuter')
crudeText <- c(crudeText, 'Saudi Arabian Oil Minister Hisham Nazer\nreiterated the kingdoms commitment to last Decembers OPEC\naccord to boost world oil prices and stabilize the market, the\nofficial Saudi Press Agency SPA said.\n    Asked by the agency about the recent fall in free market\noil prices, Nazer said Saudi Arabia \"is fully adhering by the\n... accord and it will never sell its oil at prices below the\npronounced prices under any circumstance.\"\n    Saudi Arabia was a main architect of December pact under\nwhich OPEC agreed to cut its total oil output ceiling by 7.25\npct and return to fixed prices of around 18 dollars a barrel.\n Reuter')
crudeText <- c(crudeText, 'Kuwaits oil minister said in a newspaper\ninterview that there were no plans for an emergency OPEC\nmeeting after the recent weakness in world oil prices.\n    Sheikh Ali al-Khalifa al-Sabah was quoted by the local\ndaily al-Qabas as saying that \"none of the OPEC members has\nasked for such a meeting.\"\n    He also denied that Kuwait was pumping above its OPEC quota\nof 948,000 barrels of crude daily (bpd).\n    Crude oil prices fell sharply last week as international\noil traders and analysts estimated the 13-nation OPEC was\npumping up to one million bpd over its self-imposed limits.\n Reuter')
crudeText <- c(crudeText, 'The port of Philadelphia was closed\nwhen a Cypriot oil tanker, Seapride II, ran aground after\nhitting a 200-foot tower supporting power lines across the\nriver, a Coast Guard spokesman said.\n    He said there was no oil spill but the ship is lodged on\nrocks opposite the Hope Creek nuclear power plant in New\nJersey.\n    He said the port would be closed until today when they\nhoped to refloat the ship on the high tide.\n    After delivering oil to a refinery in Paulsboro, New\nJersey, the ship apparently lost its steering and hit the power\ntransmission line carrying power from the nuclear plant to the\nstate of Delaware.\n Reuter')
crudeText <- c(crudeText, 'A study group said the United States\nshould increase its strategic petroleum reserve to one mln\nbarrels as one way to deal with the present and future impact\nof low oil prices on the domestic oil industry.\n    U.S. policy now is to raise the strategic reserve to 750\nmln barrels, from its present 500 mln, to help protect the\neconomy from an overseas embargo or a sharp price rise.\n    The Aspen Institute for Humanistic Studies, a private\ngroup, also called for new research for oil exploration and\ndevelopment techniques.\n    It predicted prices would remain at about 15-18 dlrs a\nbarrel for several years and then rise to the mid 20s, with\nimports at about 30 pct of U.S. consumption.\n    It said instead that such moves as increasing oil reserves\nand more exploration and development research would help to\nguard against or mitigate the risks of increased imports.\n Reuter')
crudeText <- c(crudeText, 'A study group said the United States\nshould increase its strategic petroleum reserve to one mln\nbarrels as one way to deal with the present and future impact\nof low oil prices on the domestic oil industry.\n    U.S. policy now is to raise the strategic reserve to 750\nmln barrels, from its present 500 mln, to help protect the\neconomy from an overseas embargo or a sharp price rise.\n    The Aspen Institute for Humanistic Studies, a private\ngroup, also called for new research for oil exploration and\ndevelopment techniques.\n    It predicted prices would remain at about 15-18 dlrs a\nbarrel for several years and then rise to the mid 20s, with\nimports at about 30 pct of U.S. consumption.\n    The study cited two basic policy paths for the nation: to\nprotect the U.S. industry through an import fee or other such\ndevice or to accept the full economic benefits of cheap oil.\n    But the group did not strongly back either option, saying\nthere were benefits and drawbacks to both.\n    It said instead that such moves as increasing oil reserves\nand more exploration and development research would help to\nguard against or mitigate the risks of increased imports.\n Reuter')
crudeText <- c(crudeText, 'Unocal Corps Union Oil Co said it\nlowered its posted prices for crude oil one to 1.50 dlrs a\nbarrel in the eastern region of the U.S., effective Feb 26.\n    Union said a 1.50 dlrs cut brings its posted price for the\nU.S. benchmark grade, West Texas Intermediate, to 16 dlrs.\nLouisiana Sweet also was lowered 1.50 dlrs to 16.35 dlrs, the\ncompany said.\n    No changes were made in Unions posted prices for West\nCoast grades of crude oil, the company said.\n Reuter')
crudeText <- c(crudeText, 'The New York Mercantile Exchange set\nApril one for the debut of a new procedure in the energy\ncomplex that will increase the use of energy futures worldwide.\n     On April one, NYMEX will allow oil traders that do not\nhold a futures position to initiate, after the exchange closes,\na transaction that can subsequently be hedged in the futures\nmarket, according to an exchange spokeswoman.\n    \"This will change the way oil is transacted in the real\nworld,\" said said Thomas McKiernan, McKiernan and Co chairman.\n    Foreign traders will be able to hedge trades against NYMEX\nprices before the exchange opens and negotiate prices at a\ndifferential to NYMEX prices, McKiernan explained.\n     The expanded program \"will serve the industry because the\noil market does not close when NYMEX does,\" said Frank Capozza,\nsecretary of Century Resources Inc.\n     The rule change, which has already taken effect for\nplatinum futures on NYMEX, is expected to increase the open\ninterest and liquidity in U.S. energy futures, according to\ntraders and analysts.\n    Currently, at least one trader in this transaction, called\nan exchange for physical or EFP, must hold a futures position\nbefore entering into the transaction.\n    Under the new arrangement, neither party has to hold a\nfutures position before entering into an EFP and one or both\nparties can offset their cash transaction with a futures\ncontract the next day, according to exchange officials.\n    When NYMEX announced its proposed rule change in December,\nNYMEX President Rosemary McFadden, said, \"Expansion of the EFP\nprovision will add to globalization of the energy markets by\nproviding for, in effect, 24-hour trading.\"\n    The Commodity Futures Trading Commission approved the rule\nchange in February, according to a CFTC spokeswoman.\n Reuter')
crudeText <- c(crudeText, 'Argentine crude oil production was\ndown 10.8 pct in January 1987 to 12.32 mln barrels, from 13.81\nmln barrels in January 1986, Yacimientos Petroliferos Fiscales\nsaid.\n    January 1987 natural gas output totalled 1.15 billion cubic\nmetrers, 3.6 pct higher than 1.11 billion cubic metres produced\nin January 1986, Yacimientos Petroliferos Fiscales added.\n Reuter')


crude <- tm::VCorpus(tm::VectorSource(crudeText))
NLP::meta(crude, "id") <- c('127', '144', '191', '194', '211', '236', '237', '242', '246', '248', '273', '349', '352', '353', '368', '489', '502', '543', '704', '708')

# Print out the corpus
print(crude)

# Print the content of the 10th article
crude[[10]]$content

# Find the first ID
crude[[1]]$meta$id

# Make a vector of IDs
ids <- c()
for(i in c(1:20)){
    ids <- append(ids, crude[[i]]$meta$id)
}


# Create a tibble & Review
crude_tibble <- generics::tidy(crude)
names(crude_tibble)

crude_counts <- crude_tibble %>%
    # Tokenize 
    tidytext::unnest_tokens(word, text) %>%
    # Count by word
    count(word, sort = TRUE) %>%
    # Remove
    anti_join(tidytext::stop_words)

# Assign the top word
top_word <- "oil"


russian_tweets <- read_csv("./RInputFiles/russian_1.csv")
str(russian_tweets)


# Create a corpus
tweet_corpus <- tm::VCorpus(tm::VectorSource(russian_tweets$content))

# Attach following and followers
NLP::meta(tweet_corpus, 'following') <- russian_tweets$following
NLP::meta(tweet_corpus, 'followers') <- russian_tweets$followers

# Review the meta data
head(NLP::meta(tweet_corpus))


# Count occurrence by question and word
words <- crude_tibble %>%
    tidytext::unnest_tokens(output = "word", token = "words", input = text) %>%
    anti_join(tidytext::stop_words) %>%
    count(id, word, sort=TRUE)

# How  different word/article combinations are there?
unique_combinations <- nrow(words)

# Filter to responses with the word "prices"
words %>%
    filter(word == "prices")

# How many articles had the word "prices"?
number_of_price_articles <- 15


# Tokenize and remove stop words
tidy_tweets <- russian_tweets %>%
    tidytext::unnest_tokens(word, content) %>%
    anti_join(tidytext::stop_words)

# Count by word
unique_words <- tidy_tweets %>%
    count(word)

# Count by tweet (tweet_id) and word
unique_words_by_tweet <- tidy_tweets %>%
    count(tweet_id, word)

# Find the size of matrix: rows x columns
size <- nrow(russian_tweets) * length(unique(tidy_tweets$word))

percent <- nrow(unique_words_by_tweet) / size
percent


# Create a tibble with TFIDF values
crude_weights <- crude_tibble %>%
    tidytext::unnest_tokens(output = "word", token = "words", input = text) %>%
    anti_join(tidytext::stop_words) %>%
    count(id, word) %>%
    tidytext::bind_tf_idf(word, id, n)

# Find the highest TFIDF values
crude_weights %>%
    arrange(desc(tf_idf))

# Find the lowest non-zero TFIDF values
crude_weights %>%
    filter(tf_idf != 0) %>%
    arrange(tf_idf)


# Create word counts
animal_farm_counts <- animal_farm %>%
    tidytext::unnest_tokens(word, text_column) %>%
    count(chapter, word)

# Calculate the cosine similarity 
comparisons <- animal_farm_counts %>%
    widyr::pairwise_similarity(chapter, word, n) %>%
    arrange(desc(similarity))

# Print the mean of the similarity values
comparisons %>%
    summarize(mean = mean(similarity))  # very high similarities due to stop words


# Create word counts 
animal_farm_counts <- animal_farm %>%
    tidytext::unnest_tokens(word, text_column) %>%
    anti_join(tidytext::stop_words) %>%
    count(chapter, word) %>%
    tidytext::bind_tf_idf(chapter, word, n)

# Calculate cosine similarity on word counts
animal_farm_counts %>%
    widyr::pairwise_similarity(chapter, word, n) %>%
    arrange(desc(similarity))

# Calculate cosine similarity using tf_idf values
animal_farm_counts %>%
    widyr::pairwise_similarity(chapter, word, tf_idf) %>%
    arrange(desc(similarity))

```
  
  
  
***
  
Chapter 3 - Applications: Classification and Topic Modeling  
  
Preparing Text for Modeling:  
  
* Two common text analysis techniques include text classification and topic modeling  
* Classification modeling is a type of supervised learning - separation in to unique categories  
	* Example of classifying sentences about Napoleon vs. Boxer  
    * sentences <- animal_farm %>% unnest_tokens(output="sentence", token="sentences", input=text_column)  
    * sentences$boxer <- grepl("boxer", sentences$sentence)  
    * sentences$napoleon <- grepl("napoleon", sentences$sentence)  
    * sentences$sentence <- gsub("boxer", "animal X", sentences$sentence)  
    * sentences$sentence <- gsub("napoleon", "animal X", sentences$sentence)  
    * animal_sentences <- sentences[(sentences$boxer + sentences$napoleon) == 1, ]  
    * animal_sentences$Name <- as.factor(ifelse(animal_sentences$boxer, "boxer", "napoleon"))  
    * animal_sentences <- rbind(animal_sentences[animal_sentences$Name == "boxer", ][1:75, ], animal_sentences[animal_sentences$Name == "napoleon", ][1:75, ])  
    * animal_sentences$sentence_id <- c(1:dim(animal_sentences)[1])  
* With data prepared, can attempt to predict which sentence refers to each of the animals  
	* animal_tokens <- animal_sentences %>% unnest_tokens(output="word", token="words", input=sentence) %>% anti_join(stop_words) %>% mutate(word=wordStem(word))  
    * animal_matrix <- animal_tokens %>% count(sentence_id, word) %>% tidytext::cast_dtm(document=sentence_id, term=word, value=n, weighting=tm::weightTfIdf)  
    * removeSparseTerms(animal_matrix, sparse=0.90)  # reduces to 66% sparisty from 99%+ sparsity  
    * Decisions on optimal amount of sparsity depend on computing power as well as number of terms needed for good classification  
  
Classification Modeling:  
  
* Can split data in many ways, including use of sample()  
	* set.seed(1111)  
    * sample_size <- floor(0.8 * nrow(animal_matrix))  
    * train_ind <- sample(nrow(animal_matrix), size=sample_size)  
    * train <- animal_matrix[train_ind, ]  
    * test <- animal_matrix[-train_ind, ]  
* Can use the random forest classification techniques  
	* rfc <- randomForest::randomForest(x=as.data.frame(as.matrix(train)), y=animal_sentences$Name[train_ind], nTree=50)  
    * y_pred <- predict(rfc, newdata=as.data.frame(as.matrix(test))  
    * table(animal_sentences[-train_id]$Name, y_pred)  # confusion matrix  
  
Introduction to Topic Modeling:  
  
* Text is typically made up of a collection of topics  
	* Documents are a mix of topics  
    * Topics are a mix of words  
* Can prepare for LDA to assist in topic modeling  
	* animal_farm_tokens <- animal_farm %>% unnest_tokens(output="word", token="words", input=text_column) %>% anti_join(stop_words) %>% mutate(word=wordStem(word))  
    * animal_farm_matrix <- animal_farm_tokens %>% count(chapter, word) %>% cast_dtm(document=chapter, term=word, value=n, weighting=tm::weightTf)  # LDA requires tf rather than tf-idf  
    * animal_farm_lda <- LDA(train, k=4, method="Gibbs", control=list(seed=1111))  
    * animal_farm_betas <- tidy(animal_farm_lda, matrix="beta")  # beta is a per-topic metric (words more related to a single topic should have a higher beta)  
* Can then label documents as topics  
	* animal_farm_chapters <- tidy(animal_farm_lda, matrix="gamma")  # gamma represents how much each chapter is made up of a topic  
    * animal_farm_chapters %>% filter(document == "Chapter 1")  
  
LDA in Practice:  
  
* Need to select the number of topics as an input to LDA - can use perplexity as a metric  
* Perplexity is a model of how well a model fits new data - lower is better  
	* Start with the standard test/train data splitting  
    * Can create an lda_model for many different k and store the resulting perplexity(lda_model, newdata=test)  
    * Can plot and then look for elbows or flattening points where the perplexity is no longer decreasing  
* Practical considerations are important also - sometimes fewer topics are easier to communicate and comprehend, even at the sacrifice of some excess perplexity  
	* Common to have a SME review some of the top articles and words, and then to provide a theme for each topic  
* Can run summaries of the output  
	* gammas <- tidy(lda_model, matrix="gamma")  
    * gammas %>% group_by(document) %>% arrange(desc(gamma)) %>% slice(1) %>% group_by(topic) %>% tally(topic, sort=TRUE)  
    * gammas %>% group_by(document) %>% arrange(desc(gamma)) %>% slice(1) %>% group_by(topic) %>% summarize(avg=mean(gamma)) %>% arrange(desc(avg))  
  
Example code includes:  
```{r cache=TRUE}

# Stem the tokens
russian_tokens <- russian_tweets %>%
    tidytext::unnest_tokens(output = "word", token = "words", input = content) %>%
    anti_join(tidytext::stop_words) %>%
    mutate(word = SnowballC::wordStem(word))

# Create a document term matrix 
tweet_matrix <- russian_tokens %>%
    count(tweet_id, word) %>%
    tidytext::cast_dtm(document = tweet_id, term = word, value = n, weighting = tm::weightTfIdf)

# Print the matrix details 
tweet_matrix


less_sparse_matrix <- tm::removeSparseTerms(tweet_matrix, sparse = 0.5)

# Print results
tweet_matrix
less_sparse_matrix

less_sparse_matrix <- tm::removeSparseTerms(tweet_matrix, sparse = 0.9)

# Print results
tweet_matrix
less_sparse_matrix

less_sparse_matrix <- tm::removeSparseTerms(tweet_matrix, sparse = 0.99)

# Print results
tweet_matrix
less_sparse_matrix

less_sparse_matrix <- tm::removeSparseTerms(tweet_matrix, sparse =0.9999)

# Print results
tweet_matrix
less_sparse_matrix


set.seed(2001021530)

rightTweet <- russian_tweets %>%
    filter(account_type=="Right") %>%
    sample_n(2000)

leftTweet <- russian_tweets %>%
    filter(account_type=="Left") %>%
    sample_n(2000)


idx <- sample(1:4000, 4000, replace=FALSE)

leftRightData <- rbind(rightTweet, leftTweet)[idx, ]


leftRight_tokens <- leftRightData %>%
    tidytext::unnest_tokens(output = "word", token = "words", input = content) %>%
    anti_join(tidytext::stop_words) %>%
    mutate(word = SnowballC::wordStem(word))

# Create a document term matrix 
left_right_matrix_small <- leftRight_tokens %>%
    count(tweet_id, word) %>%
    tidytext::cast_dtm(document = tweet_id, term = word, value = n, weighting = tm::weightTfIdf) %>%
    tm::removeSparseTerms(sparse = 0.99)

left_right_labels <- c()
for (lbl in rownames(as.matrix(left_right_matrix_small))) {
    newPoint <- leftRightData %>%
        filter(tweet_id==lbl) %>%
        pull(account_type)
    left_right_labels <- c(left_right_labels, newPoint)
}
left_right_labels <- as.factor(left_right_labels)


# Create train/test split
set.seed(1111)
sample_size <- floor(0.75 * nrow(left_right_matrix_small))
train_ind <- sample(nrow(left_right_matrix_small), size = sample_size)
train <- left_right_matrix_small[train_ind, ]
test <- left_right_matrix_small[-train_ind, ]

# Create a random forest classifier
rfc <- randomForest::randomForest(x = as.data.frame(as.matrix(train)), 
                                  y = left_right_labels[train_ind], nTree = 50
                                  )

# Print the results
rfc


# Percentage correctly labeled "Left"
# left <- (350) / (350 + 157)
# left

# Percentage correctly labeled "Right"
# right <- (436) / (436 + 57)
# right

# Overall Accuracy:
# accuracy <- (350 + 436) / (350 + 436 + 57 + 157)
# accuracy


napolSents <- animal_farm %>%
    tidytext::unnest_tokens(output = "sentences", input = text_column, token = "sentences") %>%
    mutate(sentence_id=row_number(), napoleon=str_detect(sentences, 'napoleon')) %>%
    filter(napoleon)

pig_tokens <- napolSents %>%
    tidytext::unnest_tokens(output = "word", token = "words", input = sentences) %>%
    anti_join(tidytext::stop_words) %>%
    mutate(word = SnowballC::wordStem(word))

# Create a document term matrix 
pig_matrix <- pig_tokens %>%
    count(sentence_id, word) %>%
    tidytext::cast_dtm(document = sentence_id, term = word, value = n, weighting = tm::weightTf) %>%
    tm::removeSparseTerms(sparse=0.995)

# Perform Topic Modeling
sentence_lda <-
    topicmodels::LDA(pig_matrix, k = 10, method = 'Gibbs', control = list(seed = 1111))

# Extract the beta matrix 
sentence_betas <- generics::tidy(sentence_lda, matrix = "beta")

# Topic #2
sentence_betas %>%
    filter(topic == 2) %>%
    arrange(-beta)

# Topic #10
sentence_betas %>%
    filter(topic == 3) %>%
    arrange(-beta)


# Extract the beta and gamma matrices
sentence_betas <- generics::tidy(sentence_lda, matrix = "beta")
sentence_gammas <- generics::tidy(sentence_lda, matrix = "gamma")

# Explore Topic 5 Betas
sentence_betas %>%
    filter(topic == 5) %>%
    arrange(-beta)

# Explore Topic 5 Gammas
sentence_gammas %>%
    filter(topic == 5) %>%
    arrange(-gamma)

# Print the topic setence for topic 5
napolSents$sentences[which(napolSents$sentence_id == (sentence_gammas %>% group_by(topic) %>% 
                                                          top_n(1, gamma) %>% filter(topic==5) %>% 
                                                          pull(document) %>% as.numeric()
                                                      )
                           )
                    ]


right_tokens <- rightTweet %>%
    tidytext::unnest_tokens(output = "word", token = "words", input = content) %>%
    anti_join(tidytext::stop_words) %>%
    mutate(word = SnowballC::wordStem(word))

# Create a document term matrix 
right_matrix <- right_tokens %>%
    count(tweet_id, word) %>%
    tidytext::cast_dtm(document = tweet_id, term = word, value = n, weighting = tm::weightTf)


# Setup train and test data
sample_size <- floor(0.90 * nrow(right_matrix))
set.seed(1111)
train_ind <- sample(nrow(right_matrix), size = sample_size)
train <- right_matrix[train_ind, ]
test <- right_matrix[-train_ind, ]

# Peform topic modeling 
lda_model <- topicmodels::LDA(train, k = 5, method = "Gibbs",control = list(seed = 1111))

# Train
topicmodels::perplexity(lda_model, newdata = train) 

# Test
topicmodels::perplexity(lda_model, newdata = test) 


# Extract the gamma matrix 
gamma_values <- generics::tidy(sentence_lda, matrix = "gamma")

# Create grouped gamma tibble
grouped_gammas <- gamma_values %>%
    group_by(document) %>%
    arrange(desc(gamma)) %>%
    slice(1) %>%
    group_by(topic)
    
# Count by topic
grouped_gammas %>% 
    tally(topic, sort=TRUE)

# Average topic weight for top topic for each sentence
grouped_gammas %>% 
    summarize(avg=mean(gamma)) %>%
    arrange(desc(avg))

```
  
  
  
***
  
Chapter 4 - Advanced Techniques  
  
Sentiment Analysis:  
  
* Sentiment analysis extracts sentiments from the text, using a dictionary that scores words by sentiment (either a mapping or a numeric)  
	* tidytext::sentiments  # word-sentiment-lexicon-score  
    * AFINN - scores from -5 (negative) to +5 (positive)  
    * bing - simple boolean for negative/positive  
    * nrc - labels words as fear, joy, anger, etc.  
    * tidytext::get_sentiments("nrc")  
* Preparatory steps are merely to tokenize the data  
	* animal_farm_tokens <- animal_farm %>% unnest_tokens(output="word", token="words", input=text_column) %>% anti_join(stop_words)  
    * animal_farm_tokens %>% inner_join(get_sentiments("afinn")) %>% group_by(chapter) %>% summarize(sentiment=mean(score)) %>% arrange(sentiment)  
  
Word Embeddings:  
  
* One of the goals is to get at word meanings - for example, "smartest" and "most brilliant" should be considered synonyms rather than different words  
* The word2vec represents words as a large vector space, where words that are similar lie close to one another (captures multiple similarities)  
	* library(h2o)  
    * h2o.init()  
    * h2o_object <- as.h2o(animal_farm)  
    * words <- h2o.tokenize(h2o_object$text_column, "\\\\W+")  
    * words <- h2o.tolower(words)  
    * words <- words[is.na(words) || (!words %in% stop_words$word), ]  
    * word2vec_model <- h2o.word2vec(words, min_word_freq=5, epochs=5)  # 5 is a small number of epochs  
    * h2o.findSynonyms(w2v.model, "animal")  
    * h2o.findSynonyms(w2v.model, "jones")  
* Additional uses include classification modeling, sentiment analysis, topic modeling, and the like  
  
Additional NLP Analysis:  
  
* BERT (bidirectional encoder representations from transformers) is a pre-trained model used in transfer learning for NLP  
	* Requires only a small amount of labelled data for a specific supervised learning task  
* ERNIE (enhanced representation through knowledge integration) is also imporiving NLP analysis  
* NER (named entity recognition) attemps to classify names within text - extraction, recommendations, search algorithms  
* POS (part of speech) tagging - tag words with the proper part of speech (noun, verb, adjective, etc.)  
  
Wrap Up:  
  
* Text analysis techniques  
* Preparaing data for analysis, including appropriate formats  
* Common text analysis techniques  
* State-of-the-art techniques available today  
  
Example code includes:  
```{r}

# Print the lexicon
tidytext::get_sentiments("bing")

# Count the different sentiment types
tidytext::get_sentiments("bing") %>%
    count(sentiment) %>%
    arrange(desc(n))

# Count the different sentiment types
tidytext::get_sentiments("loughran") %>%
    count(sentiment) %>%
    arrange(desc(n))

# Count how many times each score was used
tidytext::get_sentiments("afinn") %>%
    count(value) %>%
    arrange(desc(n))


afSents <- animal_farm %>%
    tidytext::unnest_tokens(output = "sentence", input = text_column, token = "sentences") %>%
    mutate(sentence_id=row_number())

# Print the overall sentiment associated with each pig's sentences
for(name in c("napoleon", "snowball", "squealer")) {
    # Filter to the sentences mentioning the pig
    pig_sentences <- afSents[grepl(name, afSents$sentence), ]
    # Tokenize the text
    temp_tokens <- pig_sentences %>%
        tidytext::unnest_tokens(output = "word", token = "words", input = sentence) %>%
        anti_join(tidytext::stop_words)
    # Use afinn to find the overall sentiment score
    result <- temp_tokens %>% 
        inner_join(tidytext::get_sentiments("afinn")) %>%
        summarise(sentiment = sum(value))
    # Print the result
    print(paste0(name, ": ", result$sentiment))
}


left_tokens <- russian_tweets %>%
    filter(account_type=="Left") %>%
    tidytext::unnest_tokens(output = "word", token = "words", input = content) %>%
    anti_join(tidytext::stop_words)

# Dictionaries 
# anticipation <- tidytext::get_sentiments("bing") %>% 
#     filter(sentiment == "anticipation")

# joy <- tidytext::get_sentiments("nrc") %>% 
#     filter(sentiment == "joy")

# Print top words for Anticipation and Joy
# left_tokens %>%
#     inner_join(anticipation, by = "word") %>%
#     count(word, sort = TRUE)

# left_tokens %>%
#     inner_join(joy, by = "word") %>%
#     count(word, sort = TRUE)


# Initialize a h2o session
library(h2o)
h2o.init()

# Create an h2o object for left_right
h2o_object = as.h2o(leftRightData)

# Tokenize the words from the column of text in left_right
tweet_words <- h2o.tokenize(h2o_object$content, "\\\\W+")

# Lowercase and remove stopwords
tweet_words <- h2o.tolower(tweet_words)
tweet_words = tweet_words[is.na(tweet_words) || (!tweet_words %in% tidytext::stop_words$word),]
tweet_words


# set.seed(1111)

# Use 33% of the available data
# sample_size <- floor(0.33 * nrow(job_titles))
# sample_data <- sample(nrow(job_titles), size = sample_size)

# h2o_object = as.h2o(job_titles[sample_data, ])
# words <- h2o.tokenize(h2o_object$jobtitle, "\\\\W+")
# words <- h2o.tolower(words)
# words = words[is.na(words) || (!words %in% stop_words$word),]

# word2vec_model <- h2o.word2vec(words, min_word_freq=5, epochs = 10)

# Find synonyms for the word "teacher"
# h2o.findSynonyms(word2vec_model, "teacher", count=10)


# a: Labels each word within text as either a noun, verb, adjective, or other category.
# b: A model pre-trained on a vast amount of text data to create a language representation used for supervised learning.
# c: A type of analysis that looks to describe text as either positive or negative and can be used to find active vs passive terms.
# d: A modeling technique used to label entire text into a single category such as relevant or not-relevant.

# Sentiment Analysis
# SA <- c

# Classifcation Modeling
# CM <- d

# BERT
# BERT <- b

# Part-of-speech Tagging
# POS <- a


# e: Modeling techniques, including LDA, used to cluster text into groups or types based on similar words being used.
# f: A method for searching through text and tagging words that distinguish people, locations, or organizations.
# g: Method used to search text for specific patterns.
# h: Representing words using a large vector space where similar words are close together within the vector space.

# Named Entity Recognition
# NER <- f

# Topic Modeling
# TM <- e

# Word Embeddings 
# WE <- h

# Regular Expressions
# REGEX <- g

```
  
  
  
***
  
### _Joining Data with dplyr_  
  
Chapter 1 - Joining Tables  
  
The inner_join verb:  
  
* Joining tables together can be important for analysis  
	* sets %>% inner_join(themes, by=c("theme_id"="id"))  # use sets.theme_id==themes.id for merging  
    * sets %>% inner_join(themes, by=c("theme_id"="id"), suffix=c("_set", "_theme"))  # use sets.theme_id==themes.id for merging; use _set and _theme as suffixes for any common names  
  
Joining with a one-to-many relationship:  
  
* Can have multiple records per id, which results in a one-to-many join (4,000 records can expand to 4,100 records after the one-to-many even with an inner_join)  
  
Joining three or more tables:  
  
* Can join three or more tables using chaining  
	* sets %>% inner_join(inventories, by="set_num") %>% inner_join(themes, by=c("theme_id"="id"), suffix=c("_set", "_theme"))  
* Each join will typically have different by arguments and suffix arguments  
  
Example code includes:  
```{r}

parts <- readRDS("./RInputFiles/parts.rds")
part_categories <- readRDS("./RInputFiles/part_categories.rds")
inventory_parts <- readRDS("./RInputFiles/inventory_parts.rds")
inventories <- readRDS("./RInputFiles/inventories.rds")
sets <- readRDS("./RInputFiles/sets.rds")
themes <- readRDS("./RInputFiles/themes.rds")
colors <- readRDS("./RInputFiles/colors.rds")


# Use the suffix argument to replace .x and .y suffixes
parts %>% 
    inner_join(part_categories, by = c("part_cat_id" = "id"), suffix=c("_part", "_category"))


# Combine the parts and inventory_parts tables
parts %>%
    inner_join(inventory_parts, by=c("part_num"))


# Combine the parts and inventory_parts tables
inventory_parts %>%
    inner_join(parts, by="part_num")


sets %>%
    # Add inventories using an inner join 
    inner_join(inventories, by="set_num") %>%
    # Add inventory_parts using an inner join 
    inner_join(inventory_parts, by=c("id"="inventory_id"))


# Count the number of colors and sort
sets %>%
    inner_join(inventories, by = "set_num") %>%
    inner_join(inventory_parts, by = c("id" = "inventory_id")) %>%
    inner_join(colors, by = c("color_id" = "id"), suffix = c("_set", "_color")) %>%
    count(name_color, sort=TRUE)

```
  
  
  
***
  
Chapter 2 - Left and Right Joins  
  
The left_join verb:  
  
* Can join by two or more variables using by=c(…, …, …)  
* The left_join() will keep all the observations from the left dataset, bringing in matching data from the right dataset as and where it exists  
	* Will introduce NA for the columns from the right dataset where there is an observation in the left dataset without any matches in the right dataset  
  
The right_join verb:  
  
* The right_join() will keep all of the observations from he right (second) table and whatever matching information can be obtained from the left (first) table  
* Can replace NA values using replace_na()  
	* replace_na(list(n=0))  # runs the replace only on the column 'n', and converts any NA to 0  
  
Joining tables to themselves:  
  
* For hierarchical tables, joining the table to itself can better display the hierarchy for a given row  
	* themes %>% inner_join(themes, by=c("parent_id"="id"), suffix=c("_child", "_parent"))  
  
Example code includes:  
```{r}

inventory_parts_joined <- inventory_parts %>%
    inner_join(inventories, by=c("inventory_id"="id")) %>%
    select(set_num, part_num, color_id, quantity)
str(inventory_parts_joined)

millennium_falcon <- inventory_parts_joined %>%
  filter(set_num == "7965-1")
str(millennium_falcon)

star_destroyer <- inventory_parts_joined %>%
  filter(set_num == "75190-1")
str(star_destroyer)


# Combine the star_destroyer and millennium_falcon tables
millennium_falcon %>%
    left_join(star_destroyer, by=c("part_num", "color_id"), suffix=c("_falcon", "_star_destroyer"))


# Aggregate Millennium Falcon for the total quantity in each part
millennium_falcon_colors <- millennium_falcon %>%
    group_by(color_id) %>%
    summarize(total_quantity = sum(quantity))

# Aggregate Star Destroyer for the total quantity in each part
star_destroyer_colors <- star_destroyer %>%
    group_by(color_id) %>%
    summarize(total_quantity = sum(quantity))

# Left join the Millennium Falcon colors to the Star Destroyer colors
millennium_falcon_colors %>%
    left_join(star_destroyer_colors, by="color_id", suffix=c("_falcon", "_star_destroyer"))


inventory_version_1 <- inventories %>%
    filter(version == 1)

# Join versions to sets
sets %>%
    left_join(inventory_version_1, by="set_num") %>%
    # Filter for where version is na
    filter(is.na(version))


parts %>%
    count(part_cat_id) %>%
    right_join(part_categories, by = c("part_cat_id" = "id")) %>%
    # Filter for NA
    filter(is.na(n))


parts %>%
    count(part_cat_id) %>%
    right_join(part_categories, by = c("part_cat_id" = "id")) %>%
    # Use replace_na to replace missing values in the n column
    replace_na(list(n=0))


themes %>% 
    # Inner join the themes table
    inner_join(themes, by=c("id"="parent_id"), suffix=c("_parent", "_child")) %>%
    # Filter for the "Harry Potter" parent name 
    filter(name_parent=="Harry Potter")


# Join themes to itself again to find the grandchild relationships
themes %>% 
    inner_join(themes, by = c("id" = "parent_id"), suffix = c("_parent", "_child")) %>%
    inner_join(themes, by = c("id_child" = "parent_id"), suffix = c("_parent", "_grandchild"))


themes %>% 
    # Left join the themes table to its own children
    left_join(themes, by=c("id"="parent_id"), suffix=c("_parent", "_child")) %>%
    # Filter for themes that have no child themes
    filter(is.na(id_child))

```
  
  
  
***
  
Chapter 3 - Full, Semi, and Anti Joins  
  
The full_join verb:  
  
* The full_join() will keep all records from both tables, matching them where possible and leaving NA for columns of non-matching records  
* Can use tidyr::replace_na() on multiple variables 
    * replace_na(list(a=0, b=0))  
  
The semi and anti-join verbs:  
  
* The full_join, left_join, right_join, and inner_join are all mutating verbs, which is to say that they change the number of columns  
* Filtering joins are different in that they keep only a subset of records, without mutating any of the columns  
	* semi_join() keeps observations in x that can be found in y  
    * anti_join() keeps observations in x that cannot be found in y  
  
Visualizing set differences:  
  
* Can aggregate all of the sets to colors, summing the quantities by color for each set  
* Can then full join the color schemes together, for a single table of all the colors used  
  
Example code includes:  
```{r}

inventory_parts_joined <- inventories %>%
    inner_join(inventory_parts, by = c("id" = "inventory_id")) %>%
    arrange(desc(quantity)) %>%
    select(-id, -version)
str(inventory_parts_joined)


inventory_parts_joined %>%
    # Combine the sets table with inventory_parts_joined 
    inner_join(sets, by=c("set_num"="set_num")) %>%
    # Combine the themes table with your first join 
    inner_join(themes, by=c("theme_id"="id"), suffix=c("_set", "_theme"))


inventory_sets_themes <- inventory_parts_joined %>%
    inner_join(sets, by = "set_num") %>%
    inner_join(themes, by = c("theme_id" = "id"), suffix = c("_set", "_theme"))
str(inventory_sets_themes)

batman <- inventory_sets_themes %>%
    filter(name_theme == "Batman")
str(batman)

star_wars <- inventory_sets_themes %>%
    filter(name_theme == "Star Wars")
str(star_wars)


# Count the part number and color id, weight by quantity
(batman_parts <- batman %>%
    count(part_num, color_id, wt=quantity))

(star_wars_parts <- star_wars %>%
    count(part_num, color_id, wt=quantity))


(parts_joined <- batman_parts %>%
    # Combine the star_wars_parts table 
    full_join(star_wars_parts, by=c("part_num", "color_id"), suffix=c("_batman", "_star_wars")) %>%
    # Replace NAs with 0s in the n_batman and n_star_wars columns 
    replace_na(list(n_batman=0, n_star_wars=0)))


parts_joined %>%
    # Sort the number of star wars pieces in descending order 
    arrange(-n_star_wars) %>%
    # Join the colors table to the parts_joined table
    left_join(colors, by=c("color_id"="id")) %>%
    # Join the parts table to the previous join 
    left_join(parts, by=c("part_num"), suffix=c("_color", "_part"))


batmobile <- inventory_parts_joined %>%
    filter(set_num == "7784-1") %>%
    select(-set_num)
str(batmobile)

batwing <- inventory_parts_joined %>%
    filter(set_num == "70916-1") %>%
    select(-set_num)
str(batwing)


# Filter the batwing set for parts that are also in the batmobile set
batwing %>%
    semi_join(batmobile, by=c("part_num"))

# Filter the batwing set for parts that aren't in the batmobile set
batwing %>%
    anti_join(batmobile, by=c("part_num"))


# Use inventory_parts to find colors included in at least one set
colors %>%
    semi_join(inventory_parts, by=c("id"="color_id"))


# Use filter() to extract version 1 
version_1_inventories <- inventories %>%
    filter(version==1)

# Use anti_join() to find which set is missing a version 1
sets %>%
    anti_join(version_1_inventories, by=c("set_num"))


(inventory_parts_themes <- inventories %>%
    inner_join(inventory_parts, by = c("id" = "inventory_id")) %>%
    arrange(desc(quantity)) %>%
    select(-id, -version) %>%
    inner_join(sets, by = "set_num") %>%
    inner_join(themes, by = c("theme_id" = "id"), suffix = c("_set", "_theme")))


batman_colors <- inventory_parts_themes %>%
    # Filter the inventory_parts_themes table for the Batman theme
    filter(name_theme=="Batman") %>%
    group_by(color_id) %>%
    summarize(total = sum(quantity)) %>%
    # Add a percent column of the total divided by the sum of the total 
    mutate(percent=total/sum(total))

# Filter and aggregate the Star Wars set data; add a percent column
star_wars_colors <- inventory_parts_themes %>%
    filter(name_theme=="Star Wars") %>%
    group_by(color_id) %>%
    summarize(total = sum(quantity)) %>%
    mutate(percent=total/sum(total))


(colors_joined <- batman_colors %>%
    full_join(star_wars_colors, by = "color_id", suffix = c("_batman", "_star_wars")) %>%
    replace_na(list(total_batman = 0, total_star_wars = 0, percent_batman=0, percent_star_wars=0)) %>%
    inner_join(colors, by = c("color_id" = "id")) %>%
    # Create the difference and total columns
    mutate(difference = percent_batman - percent_star_wars, total = total_batman + total_star_wars) %>%
    # Filter for totals greater than 200
    filter(total >= 200))


color_palette <- c('#05131D', '#0055BF', '#C91A09', '#F2CD37', '#FFFFFF', '#E4CD9E', '#958A73', '#C91A09', '#F5CD2F', '#582A12', '#A0A5A9', '#6C6E68', '#CC702A', '#898788', '#A0BCAC', '#D3D3D3')
names(color_palette) <- c('Black', 'Blue', 'Red', 'Yellow', 'White', 'Tan', 'Dark Tan', 'Trans-Red', 'Trans-Yellow', 'Reddish Brown', 'Light Bluish Gray', 'Dark Bluish Gray', 'Medium Dark Flesh', 'Flat Silver', 'Sand Green', 'Light Gray')
color_palette

# Create a bar plot using colors_joined and the name and difference columns
ggplot(colors_joined, aes(x=reorder(name, difference), y=difference, fill = name)) +
    geom_col() +
    coord_flip() +
    scale_fill_manual(values = color_palette, guide = FALSE) +
    labs(y = "Difference: Batman - Star Wars")

```
  
  
  
***
  
Chapter 4 - Case Study: Stack Overflow  
  
Stack Overflow Questions:  
  
* The 'questions' table contains id-creation_date-score (unique by id)  
* The 'questions_tag' table contains question_id-tag_id (many-to-many)  
* The 'tags' table contains id-tag_name (unique by id?)  
  
Joining Questions and Answers:  
  
* The 'answers' table contains id-creation_date-question_id-score (one-to-many)  
	* The 'answers' table contains id-creation_date-question_id-score (one-to-many)  
  
The bind_rows verb:  
  
* Can use bind_rows to stack data on top of each other in to a single frame  
	* questions %>% bind_rows(answers)  
    * Can mutate a type in to each of the raw tables prior to the bind_rows so that the source is known for future reference or analysis  
* Can use lubridate::year(myDate) to get the year from a date object  
  
Wrap up:  
  
* Joining verbs  
	* Mutating joins - inner, left, right, full  
    * Filtering joins - semi, anti  
* Stacking using bind_rows  
  
Example code includes:  
```{r}

questions <- readRDS("./RInputFiles/questions.rds")
tags <- readRDS("./RInputFiles/tags.rds")
question_tags <- readRDS("./RInputFiles/question_tags.rds")
answers <- readRDS("./RInputFiles/answers.rds")


# Replace the NAs in the tag_name column
questions_with_tags <- questions %>%
    left_join(question_tags, by = c("id" = "question_id")) %>%
    left_join(tags, by = c("tag_id" = "id")) %>%
    replace_na(list(tag_name="only-r"))


questions_with_tags %>%
    # Group by tag_name
    group_by(tag_name) %>%
    # Get mean score and num_questions
    summarize(score = mean(score), num_questions = n()) %>%
    # Sort num_questions in descending order
    arrange(-num_questions)


# Using a join, filter for tags that are never on an R question
tags %>%
    anti_join(question_tags, by=c("id"="tag_id"))


questions %>%
    # Inner join questions and answers with proper suffixes
    inner_join(answers, by=c("id"="question_id"), suffix=c("_question", "_answer")) %>%
    # Subtract creation_date_question from creation_date_answer to create gap
    mutate(gap = as.integer(creation_date_answer-creation_date_question))


# Count and sort the question id column in the answers table
answer_counts <- answers %>%
    count(question_id, sort=TRUE)

# Combine the answer_counts and questions tables
question_answer_counts <- questions %>%
    left_join(answer_counts, by=c("id"="question_id")) %>%
    # Replace the NAs in the n column
    replace_na(list(n=0))


tagged_answers <- question_answer_counts %>%
    # Join the question_tags tables
    inner_join(question_tags, by=c("id"="question_id")) %>%
    # Join the tags table
    inner_join(tags, by=c("tag_id"="id"))


tagged_answers %>%
    # Aggregate by tag_name
    group_by(tag_name) %>%
    # Summarize questions and average_answers
    summarize(questions = n(), average_answers = mean(n)) %>%
    # Sort the questions in descending order
    arrange(-questions)


# Inner join the question_tags and tags tables with the questions table
questions_with_tags <- questions %>%
    inner_join(question_tags, by = c("id"="question_id")) %>%
    inner_join(tags, by = c("tag_id"="id"))

# Inner join the question_tags and tags tables with the answers table
answers_with_tags <- answers %>%
    inner_join(question_tags, by = c("question_id"="question_id")) %>%
    inner_join(tags, by = c("tag_id"="id"))


# Combine the two tables into posts_with_tags
posts_with_tags <- bind_rows(questions_with_tags %>% mutate(type = "question"), answers_with_tags %>% mutate(type = "answer"))

# Add a year column, then aggregate by type, year, and tag_name
by_type_year_tag <- posts_with_tags %>%
    mutate(year=lubridate::year(creation_date)) %>%
    count(type, year, tag_name)


# Filter for the dplyr and ggplot2 tag names 
by_type_year_tag_filtered <- by_type_year_tag %>%
    filter(tag_name %in% c("dplyr", "ggplot2"))

# Create a line plot faceted by the tag name 
ggplot(by_type_year_tag_filtered, aes(x=year, y=n, color = type)) +
    geom_line() +
    facet_wrap(~ tag_name)

```
  
  
  
***
  
### _Introduction to TensorFlow in R_  
  
Chapter 1 - Introducing TensorFlow in R  
  
What is TensorFlow?  
  
* TensorFlow was created by Google Brain - open source library with Python/R as a front-end API  
	* C++ application execution, though this is largely hidden from the user  
    * Particularly popular for image classification, NLP, RNN, etc.  
* Need to install TensorFlow on a computer before installing the "tensorflow" library  
	* library(tensorflow)  
    * firstsession = tf$Session()  
    * print(firstsession$run())  
    * firstsession$close()  
  
TensorFlow Syntax, Variables, and Placeholders:  
  
* Constants create nodes that have non-changing values throughout the session  
	* tf$constant()  # value, dtype (will default to float for all numbers if not specified), shape=None  
* Variables may change over the course of the session  
	* tf$Variable('initial value', 'optional name')  
    * EmptyMatrix <- tf$Variable(tf$zeros(shape(4, 3)))  
* Placeholders can be created for use later  
	* tf$placeholder(dtype, shape=None, name=None)  
    * SinglePlaceholder <- tf$placeholder(tf$float32)  
  
TensorBoard - Visualizing TensorFlow Models:  
  
* TensorBoard allows for visualizing the TensorFlow models  
	* Browser-based and will open locally  
    * session = tf$Session()  
    * a <- tf$constant(5, name="NumAdults")  
    * b <- tf$constant(6, name="NumChildren")  
    * d <- tf$add(a, b)  
    * session$run(d)  
    * writemygraph <- tf$summary$FileWriter('./graphs', session$graph)  
    * tensorboard(log_dir = './graphs')  
  
Example code includes:  
```{r eval=FALSE}

# Miniconda has been successfully installed at "C:/.../AppData/Local/r-miniconda".
# Need to install and PATH tensorflow for this to work

library(tensorflow)


# Create your session
sess <- tf$Session()

# Define a constant (you'll learn this next!)
HiThere <- tf$constant('Hi DataCamp Student!')

# Run your session with the HiThere constant
print(sess$run(HiThere))

# Close the session
sess$close()


# Create two constant tensors
myfirstconstanttensor <- tf$constant(152)
mysecondconstanttensor <- tf$constant('I am a tensor master!')

# Create a matrix of zeros
myfirstvariabletensor <- tf$Variable(tf$zeros(shape(5, 1)))


# Set up your session
EmployeeSession <- tf$Session()

# Add your constants
female <- tf$constant(150, name = "FemaleEmployees")
male <- tf$constant(135, name = "MaleEmployees")
total <- tf$add(female, male)
print(EmployeeSession$run(total))

# Write to file
towrite <- tf$summary$FileWriter('./graphs', EmployeeSession$graph)

# Open Tensorboard
tensorboard(log_dir = './graphs')


# From last exercise
total <- tf$add(female,male)

# Multiply your allemps by growth projections
growth <- tf$constant(1.32, name = "EmpGrowth")
EmpGrowth <- tf$math$multiply(total, growth)
print(EmployeeSession$run(EmpGrowth))

# Write to file
towrite <- tf$summary$FileWriter('./graphs', EmployeeSession$graph)

# Open Tensorboard
tensorboard(log_dir = './graphs')


# Start Session
sess <- tf$Session()

# Create 2 constants
a <- tf$constant(10)
b <- tf$constant(32)

# Add your two constants together
sess$run(a + b)

# Create a Variable
mytestvariable <- tf$Variable(tf$zeros(shape(1L)))

# Run the last line
mytestvariable

```
  
  
  
***
  
Chapter 2 - Linear Regression Using Two TensorFlow API  
  
Core API: Linear Regression:  
  
* The Core API is a low-level API that allows for full control  
* The Keras API is a higher-level interface that is higher-level and allows for runnng neural networks  
* The Estimators API is the highest-level interface that has canned models available for running  
* Can use the Core API for running linear regression models for y ~ x  
	* x_actual <- beer_train$precip  
    * y_actual <- beer_train$beer_consumed  
    * w <- tf$Variable(tf$random_uniform(shape(1L), -1, 1))  # min is -1, max is 1  
    * b <- tf$Variable(tf$zeros(shape(1L)))  
    * y_predict <- w * x_data + b  
  
Core API: Linear Regression Part II:  
  
* Cost functions are a measure of the loss (error) in the model - frequently by comparing predictions to actuals  
	* loss <- tf$reduce_mean((y_predict - y_actual)**2)  
* There are many optimizers available, with Gradient Descent being a common choice  
	* optimizer <- tf$train$GradientDescentOptimizer(0.001)  
    * train <- optimizer$minimize(loss)  
    * sess <- tf$Session()  
    * sess$run(tf$global_variables_initializer())  
  
Core API: Linear Regression Part III:  
  
* Can run model for a specified number of epochs  
	* for (step in 1:2000) {  
    *     sess$run(train)  
    *     if (step %% 500 == 0) cat("Step = ", step, "Estimate w = ", sess$run(w), "Estimate b = ", sess$run(b))  
    * }  
  
Estimators API: Multiple Linear Regression:  
  
* Begin by defining feature columns for the Estimators API  
	* ftr_colns <- feature_columns(tf$feature_column$numeric_column("numericcolumnname"), tf$feature_column$categorical_column_with_identity("categoricalcolumnname", NumCategories))  
* Can use any of 6 canned models in the Estimators API, including linear regressor  
	* modelName <- linear_regressor(feature_columns = ftr_colns)  
    * functionName <- function(data) { input_fn(data, features=c("feature1", "feature2", …), response = "responsevariable") }  
    * train(modelName, functionName(trainingData))  
    * modeleval <- evaluate(modelName, functionName(trainingData))  
    * modeleval  
  
Example code includes:  
```{r eval=FALSE}

# Parse out the minimum study time and final percent in x_data and y_data variables
x_data <- studentgradeprediction_train$minstudytime
y_data <- studentgradeprediction_train$Finalpercent


# Define your w variable
w <- tf$Variable(tf$random_uniform(shape(1L), -1.0, 1.0))

# Define your b variable
b <- tf$Variable(tf$zeros(shape(1L)))

# Define your linear equation
y <- w * x_data + b


# Define cost function
loss <- tf$reduce_mean((y-y_data)^2)

# Use the Gradient Descent Optimizer
optimizer <- tf$train$GradientDescentOptimizer(0.0001)

# Minimize MSE loss
train <- optimizer$minimize(loss)


# Launch new session
Finalgradessession <- tf$Session()

# Initialize (run) global variables
Finalgradessession$run(tf$global_variables_initializer())


# Train your model
for (step in 1:3750) {
    Finalgradessession$run(train)
    if (step %% 750 == 0) cat("Step = ", step, "Estimate w = ", Finalgradessession$run(w), "Estimate b =", Finalgradessession$run(b), "\n")
}


# Calculate the predicted grades
grades_actual <- studentgradeprediction_test$Finalpercent
grades_predicted <- as.vector(Finalgradessession$run(w)) * 
                    studentgradeprediction_test$minstudytime +
                    as.vector(Finalgradessession$run(b))

# Plot the actual and predicted grades
plot(grades_actual, grades_predicted, pch=19, col='red')

# Run a correlation 
cor(grades_actual, grades_predicted)


# Define all four of your feature columns
ftr_colns <- feature_columns(

  
  
  
)


# Choose the correct model
grademodel <- linear_regressor(feature_columns = ftr_colns)

# Define your input function
grade_input_fn <- function(data){
  
}


# Train your model
train(grademodel, grade_input_fn(train))

# Evaluate your model
model_eval <- evaluate(grademodel, grade_input_fn(test))

# See the results
model_eval


# Calculate the predictions
predictoutput <- predict(grademodel, input_fn=grademodel_input_fn(studentgradeprediction_test))

# Plot actual and predicted values
plot(studentgradeprediction_test$Finalpercent, as.numeric(predictoutput$predictions), 
     xlab = "actual_grades", ylab = "predicted_grades", pch=19, col='red'
     )

# Calculate the correlation
cor(as.numeric(predictoutput$predictions), studentgradeprediction_test$Finalpercent)

```
  
  
  
***
  
Chapter 3 - Deep Learning in TensorFlow: Creating a Deep Neural Network  
  
Gentle Introduction to Neural Networks:  
  
* Neural networks include input layers, output layers, and hidden layers  
	* Series of multiplications, additions, and activation functions determine the value at each node  
* ReLU (rectified linear unit) is one of the more common activation functions  
	* Converts any negative value to 0 and returns any non-negative value as-is  
  
Deep Neural Networks Using Keras API:  
  
* Example of using Keras API to predict whether a specific bill is genuine or counterfeit  
	* Define - Compile - Fit - Evaluate - Predict  
    * model = keras_model_sequential()  
    * model %>% layer_dense(units=15, activation='relu', input_shape=ncol(train_x)) %>% layer_dense(units=5, activation='relu') %>% layer_dense(units=1)  
    * model %>% compile(optimizer='rmsprop', loss='mse', metrics=c('accuracy', 'mae'))  
    * model %>% fit(x=x_train, y=y_train, epochs=25, batch_size=32, validation_split=0.2)  
  
Evaluate, Predict, Visualize Model:  
  
* Can evaluate the model using the evaluate() command  
	* score = model %>% evaluate(test_x, test_y)  
    * model %>% predict_classes(test_x)  
* Visualizations occur automatically during the fit() call, or can be called using TensorBoard  
	* model %>% fit(x=train_x, y=train_y, epochs=25, validation_split=0.25, callbacks=callback_tensorboard("logs/run_1"))  
    * tensorboard("logs/run_1")  
  
Create DNN Using Estimators API:  
  
* There is a size-step process for using Estimators to run a DNN  
	* Split in to test and train data  
    * feature_columns <- feature_columns(numeric_column("colNum1"), column_categorical_with_vocabulary_list("colVocab1", 'vocabList"), ...)  
    * classifier <- dnn_classifier(feature_columns=feature_columns, hidden_units=c(5, 10, 5), n_classes=2)  
    * data_input_function <- function(data) { input_fn(data, features=feature_columns, response="nameofresponsevariable") }  
    * train(classifier, input_fn = inputfunctionname(trainingdata_name))  
    * eval <- evaluate(classifier, input_fn=inputfunctionname(testingdata_name))  
    * preds <- predict(classifier, input_fn=inputfunctionname(testingdata_name))  
  
Example code includes:  
```{r eval=FALSE}

# Define the model
model <-  keras_model_sequential()
model %>%
    layer_dense(units=15, activation = 'relu', input_shape = 8) %>%
    layer_dense(units=5, activation = 'relu') %>%
    layer_dense(units=1)

# Compile the model
model %>%
    compile(optimizer = 'rmsprop', loss = 'mse', metrics = c('accuracy'))


# Fit the model
model %>%
    fit(x = train_x, y = train_y, epochs = 25, batch_size=32, validation_split = .2)


# Evaluate the model
score <- model %>%
    evaluate(test_x, test_y)

# Call up the accuracy 
score$acc


# Predict based on your model
predictedclasses <- model %>%
    predict_classes(newdata_x)

# Print predicted classes with customers' names
rownames(predictedclasses) <- c('Jasmit', 'Banjeet')
predictedclasses


# Fit the model and define callbacks
model %>%
    fit(x = train_x, y = train_y,epochs = 25, batch_size = 32, validation_split = .2, 
        callbacks = callback_tensorboard("logs/run_1")
        )

# Call TensorBoard
tensorboard("logs/run_1")


# Train the model
train(dnnclassifier, input_fn = shopping_input_function(shopper_train))

# Evaluate the model by correcting the error
evaluate(dnnclassifier, input_fn = shopping_input_function(shopper_test))


# Create a sequential model and the network architecture
ourdnnmodel <- keras_model_sequential() %>%
    layer_dense(units = 10, activation = "relu", input_shape = ncol(train_x)) %>%
    layer_dense(units = 5, activation = "relu") %>%
    layer_dense(units = 1) %>%
    compile(optimizer = 'rmsprop', loss = 'mse', metrics = c("mae", "accuracy"))

# Fit your model
learn <- ourdnnmodel %>% 
    fit(x = train_x, y = train_y, epochs = 25, batch_size = 32, validation_split = 0.2, verbose = FALSE)

# Run the learn function
learn

```
  
  
  
***
  
Chapter 4 - Deep Learning in TensorFlow: Increasing Model Accuracy  
  
L2 Regularization Using Keras:  
  
* Regularization is an attempt to minimize over-fitting by penalizing use of too many coefficients in training  
* L2 Regularization (Ridge) introduces a complexity penalty to the least-squares approach  
	* model <- keras_model_sequential()  
    * model %>% layer_dense(units=15, activation="relu", input_shape=8, kernel_regularizer=regularizer_l2(l=0.001))  
  
Dropout Technique Using TFEstimators:  
  
* Dropout is a common form of regularization that prevents over-fitting  
	* Some of the hidden nodes are temporarily hidden (dropped out)  
    * The input and output layers remain unchanged in this technique  
* Example of using a dnn_classifier with dropout regularization  
	* ourmodel <- dnn_classifier(hidden_units=6, feature_columns=ftr_colns, dropout=0.5)  # this is a 50% -- 0.5 -- dropout model  
  
Hyperparameter Tuning with tfruns:  
  
* Hyperparameters for the neural network can be tuned for optimized performance  
* Best practices are to store the code as a training script, identify flags for iterable parameters  
	* runs <- training_run("mycode.R", flags=list(dropout=c(0.2, 0.3, 0.4, 0.5), activation=c("relu", "softmax")))  
  
Wrap Up:  
  
* Introduction to TensorFlow - syntax and core concepts  
* Learning the Basics - core API an estimators  
* Deep Learning in TensorFlow  
* Model Regularization  
  
Example code includes:  
```{r eval=FALSE}

# Define the model
model_lesson1 <- keras_model_sequential()

# Add the regularizer
model_lesson1 %>%
  layer_dense(units=15, activation='relu', input_shape=8, kernel_regularizer=regularizer_l2(l=0.1)) %>%
  layer_dense(units=5, activation = 'relu') %>%
  layer_dense(units=1)


# Compile the model
model_lesson1 %>%
    compile(optimizer = 'rmsprop', loss = 'mse', metrics = c('accuracy'))

# Fit the model
model_lesson1 %>%
    fit(x = train_x, y = train_y, epochs = 25, batch_size = 32, validation_split=0.2)


# Evaluate the model
score_lesson1 <- model_lesson1 %>%
    evaluate(test_x, test_y)

# Call the accuracy and loss
score_lesson1$acc
score_lesson1$loss


# Define the feature columns
featcols <- feature_columns(
    tf$feature_column$numeric_column("Var"), tf$feature_column$numeric_column("Skew"), 
    tf$feature_column$numeric_column("Kurt"), tf$feature_column$numeric_column("Entropy")
)

# Create the input function 
banknote_input_fn <- function(data){
    input_fn(data, features = c("Var", "Skew", "Kurt", "Entropy"), response = "Class")
}


# Create your dnn_classifier model
mymodel <- dnn_classifier(feature_columns = featcols, hidden_units = c(40, 60, 10), n_classes = 2, 
                          label_vocabulary = c("N", "Y"), dropout = 0.2
                          )

# Train the model
train(mymodel, input_fn = banknote_input_fn(banknote_authentication_train))


# Evaluate your model using the testing dataset
final_evaluation <- evaluate(mymodel, input_fn = banknote_input_fn(banknote_authentication_test))

# Call up the accuracy and precision of your evaluated model
final_evaluation$accuracy
final_evaluation$precision


# Tune the run
runs <- tuning_run(modelsourcecode_script, flags = list(dropout = c(0.2, 0.3, 0.4)))

# View the outcome
runs[order(runs$eval_accuracy, decreasing = TRUE), ]


# Tune the run
runs <- tuning_run(
  modelsourcecode_script, flags = list(dropout = c(0.2, 0.3, 0.4), activation = c("relu", "softmax") )
)

# View the outcome
runs[order(runs$eval_accuracy, decreasing = TRUE), ]

```
  
  
  
***
  
### _Market Basket Analysis in R_  
  
Chapter 1 - Introduction to Market Basket Analysis  
  
Market Basket Introduction:  
  
* The basket is a collection of items such as a cart of products at a supermarket or the selected products at Amazon  
* Can take basket data in R and count, summarize unique products, plot purchases by product, etc.  
* Can also assess whether there are relationships between items in a basket  
  
Item Combinations:  
  
* Market basket analysis focuses on what products have been focused rather than what quantities of products have been purchased  
	* The empty set is always considered to be a subset of any market basket  
    * Often interesting to understand the intersection and union of market baskets - union(A, B)  
  
What is Market Basket Analysis?  
  
* Can analyze multiple baskets to see if there are associations among products commonly purchased together  
	* Place items together, make recommendations that "customers also bought this", etc.  
* Market basket analysis is sometimes known as association rule-mining  
  
Example code includes:  
```{r}

Online_Retail_2011_Q1 <- readr::read_csv("./RInputFiles/Online_Retail_2011_Q1.xls")
str(Online_Retail_2011_Q1)
movie_subset <- readr::read_csv("./RInputFiles/Movie_subset.xls")
str(movie_subset)


# Have a glimpse at the dataset
glimpse(Online_Retail_2011_Q1)

# Filter a single basket
One_basket = Online_Retail_2011_Q1 %>%
    filter(InvoiceNo == 540180)

print(One_basket)

# Basket size
n_distinct(One_basket$StockCode)

# Total number of items purchased
One_basket %>% 
    summarize(sum(Quantity))


# Plot the total number of items within the basket
ggplot(One_basket, aes(x=reorder(Description, Quantity, function(x) sum(x)), y = Quantity)) + 
    geom_col() + 
    coord_flip() + 
    xlab("Items")


# Number of items
n_items = 10

# Initialize an empty matrix 
combi = matrix(NA, nrow = n_items+1, ncol = 2)

# Loop over all values of k
for (i in 0:n_items){
    combi[i+1, ] = c(i, choose(n_items, i))
}

# Sum over all values of k
sum(combi[, 2])

# Total number of possible baskets
2^10


# Define number of items 
n_items = 100

# Specify the function to be plotted
fun_combi = function(x) choose(n_items, x)

# Plot the number of combinations
ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
    stat_function(fun = fun_combi) + xlim(0, n_items)


# Select two baskets
Two_baskets = Online_Retail_2011_Q1 %>%
    filter(InvoiceNo %in% c(540160, 540017))

# Basket size
Two_baskets %>%
    group_by(InvoiceNo) %>%
    summarise(n_total = n(), n_items = n_distinct(StockCode))


Online_Retail_clean <- Online_Retail_2011_Q1[complete.cases(Online_Retail_2011_Q1), ]
str(Online_Retail_clean)

# Create dataset with basket counts and inspect results
basket_size = Online_Retail_clean %>%
    group_by(InvoiceNo) %>%
    summarise(n_total = n(), n_items = n_distinct(StockCode))

head(basket_size)

# Calculate average values
basket_size %>% 
    summarize(avg_total_items = mean(n_total), avg_dist_items = mean(n_items))

# Distribution of distinct items in baskets
ggplot(basket_size, aes(x=n_items)) +
    geom_histogram() + ggtitle("Distribution of basket sizes")


# Number of total and distinct items for HERB MARKER THYME
Online_Retail_clean %>%
    filter(Description == "HERB MARKER THYME")  %>%
    summarise(n_tot_items = n(), n_basket_item = n_distinct(InvoiceNo))

# Number of baskets containing both items
Online_Retail_clean %>%
    filter(Description %in% c("HERB MARKER ROSEMARY", "HERB MARKER THYME")) %>%
    group_by(InvoiceNo) %>% 
    summarise(n = n()) %>% 
    filter(n==2) %>% 
    summarise(n_distinct(InvoiceNo))

```
  
  
  
***
  
Chapter 2 - Metrics and Techniques in Market Basket Analysis  
  
Transactional Data:  
  
* Transactions are defined as the activity of buying or selling something  
	* Market basket analysis is based on transactional data - each basket is a single transaction containing one or more items  
* Can coerce lists, matrices, and data frames to transactional class data  
	* myList <- split(myData$Product, myData$OrderID)  
    * data_trx <- as(myList, "transactions")  
    * inspect(head(data_trx))  
    * image(data_trx)  
  
Metrics in Market Basket Analysis:  
  
* If someone who buys A typically then also buys B, A is called antecedent and B is called precedent  
* There are several metrics for defining prevalence of items in baskets  
	* Support for X is the percentage of baskets that contain item X  
    * Confidence for (XY) is defined as support for (XY) divided by support for (X) - how often is XY in the basket conditional on X being in the basket  
    * Lift is the strength of the association - defined as support for (XY) divided by support(X) divided by support(Y) - cutoff is 1 for equal likelihood  
* The arules::apriori() function allows for calculating many of these key metrics  
  
The Apriori Algorithm:  
  
* Association rule mining allows for discovery of relationships in large, transactional datasets  
	* Frequent itemset generation - minimum level of support  
    * Rule generation using the frequent itemset generation  
* The apriori algorithm uses a bottom-up approach to generate candidate items  
	* If an itemset is frequent, then all of its subsets are frequent  
    * If an item is infrequent, then all of its supersets are infrequent  
* The basic rule generation algorithm includes  
	* Start with high confidence rules with a single precedent  
    * Build more complex rules with more items on the right side  
* Example for the initial run of the apriori algorithm  
	* support.all = apriori(trans, parameter=list(supp=3/7, target="frequent itemsets")  
  
Using Apriori for "if this then that":  
  
* Can extract frequent datasets and associated rules  
* The appearance argument of apriori() can be used to select specific itemsets  
	* appearance=list(rhs="Cheese")  # will only extract rules with 'Cheese' on rhs  
* Rules are considered redundant if a more general rule (super rule) with the same or higher confidence already exists  
	* arules::is.redundant(rules)  
  
Example code includes:  
```{r}

library(arules)


# Splitting transactions
data_list = split(Online_Retail_clean$Description, Online_Retail_clean$InvoiceNo)

# Transform data into a transactional dataset
Online_trx = as(data_list, "transactions")

# Summary of transactions
summary(Online_trx)


# inspect first 3 transactions
inspect(head(Online_trx, 3))

# inspect last 5 transactions
inspect(tail(Online_trx, 5))

# inspect transaction 10
inspect(Online_trx[10])

# Inspect specific transactions
inspect(Online_trx[c(12, 20, 22)])


# Determine the support of both items with support 0.1
support_rosemary_thyme <- apriori(Online_trx, parameter = list(target = "frequent itemsets", supp = 0.1),
                                  appearance = list(items = c("HERB MARKER ROSEMARY", "HERB MARKER THYME"))
                                  )

# Inspect the itemsets 
inspect(support_rosemary_thyme)

# Determine the support of both items with support 0.01
support_rosemary_thyme <- apriori(Online_trx, parameter = list(target = "frequent itemsets", supp = 0.01),
                                  appearance = list(items = c("HERB MARKER ROSEMARY", "HERB MARKER THYME"))
                                  )

# Inspect the itemsets 
inspect(support_rosemary_thyme)

# Frequent itemsets for all items
support_all <- apriori(Online_trx, parameter = list(target="frequent itemsets", supp = 0.01))

# Inspect the 5 most frequent items
inspect(head(sort(support_all, by="support"), 5))


# Call the apriori function with apropriate parameters
rules_all <- apriori(Online_trx, parameter = list(supp=0.01, conf = 0.4))

# Call the apriori function with apropriate parameters
rules_all <- apriori(Online_trx, parameter = list(supp=0.01, conf = 0.4, minlen=2))

# Inspect the rules with highest confidence
inspect(head(sort(rules_all, by="confidence"), 5))

# Inspect the rules with highest lift
inspect(head(sort(rules_all, by="lift"), 5))


# Find the confidence and lift measures
rules_rosemary_rhs <- apriori(Online_trx, parameter = list(supp=0.01, conf=0.5, minlen=2),
                              appearance = list(rhs="HERB MARKER ROSEMARY", default = "lhs")
                              )

# Inspect the rules
inspect(rules_rosemary_rhs)

# Find the confidence and lift measures
rules_rosemary_lhs <- apriori(Online_trx, parameter = list(supp=0.01, conf=0.5, minlen=2),
                              appearance = list(lhs="HERB MARKER ROSEMARY", default = "rhs")
                              )

# Inspect the rules
inspect(rules_rosemary_lhs)

# Create the union of the rules and inspect
rules_rosemary <- arules::union(rules_rosemary_rhs, rules_rosemary_lhs)
inspect(rules_rosemary)


# Apply the apriori function to the Online retail dataset
rules_online <- apriori(Online_trx, parameter = list(supp = 0.01, conf = 0.8, minlen = 2))

# Inspect the first 5 rules
inspect(head(rules_online, 5))

# Inspect the first 5 rules with highest lift
inspect(head(sort(rules_online, by="lift"), 5))

# Transform the rules back to a dataframe
rules_online_df <- as(rules_online, "data.frame")

# Check the first records 
head(rules_online_df)


# Apply the apriori function to the Online retail dataset
rules_online <- apriori(Online_trx, parameter = list(supp = 0.01, conf = 0.8, minlen = 2))

# Inspect the first rules
inspect(head(rules_online))


# Support of herb markers
supp_herb_markers <- apriori(Online_trx, parameter = list(target = "frequent itemsets", supp = 0.01),
                             appearance = list(items = c("HERB MARKER THYME", "HERB MARKER ROSEMARY"))
                             )

# Inspect frequent itemsets
inspect(supp_herb_markers)

# Extract rules for HERB MARKER THYME on rhs of rule
rules_thyme_marker_rhs <- apriori(Online_trx, parameter = list(supp=0.01, conf=0.8, minlen=2), 
                                  appearance = list(rhs = "HERB MARKER THYME"), control = list(verbose=F)
                                  )

# Inspect rules
inspect(rules_thyme_marker_rhs)

# Extract rules for HERB MARKER THYME on lhs of rule
rules_thyme_marker_lhs <- apriori(Online_trx, parameter = list(supp=0.01, conf=0.8, minlen=2), 
                                  appearance = list(lhs = "HERB MARKER THYME"), control = list (verbose=F)
                                  )

# Inspect rules
inspect(rules_thyme_marker_lhs)


# Apply the apriori function to the Online retail dataset
rules <- apriori(Online_trx, parameter = list(supp = 0.01, conf = 0.8, minlen = 2))

# Inspect the first 5 rules
inspect(head(rules))

# Find out redundant of rules
redundant_rules <- is.redundant(rules)

# Inspect the non redundant rules
non_redundant_rules <- rules[!redundant_rules]
inspect(head(non_redundant_rules))

```
  
  
  
***
  
Chapter 3 - Visualization in Market Basket Analysis  
  
Items in the Basket:  
 
* The item frequency plot is a common starting point for visualizations  
	* itemFrequencyPlot(data_trx, main=, topN=, type="absolute")  # type='absolute' is the traditional barplot while type='relative' will give relative counts; topN orders high to low and limits the number displayed  
    * Can add col=, ylab=, cex.names=, horiz=TRUE (for horizontal plots), etc.  
  
Visualizing Metrics:  
  
* Can use arulesViz::inspectDT(rules) to extract rules in html format that can be viewed using a widget  
	* Can also use plot(rules) for a scatterplot of the rules - support vs. confidence, colored by lift  
* The arules::plot() has four main arguments that can be used  
	* rulesObject=  
    * measure=  
    * shading=  # can choose 'support' to shade by support  
    * method=  # chooses the type of plot (default is scatter)  
* There are many supported methods of plots that can be specified in method=  
	* The 'two-key plot' has support vs. confidence, colored by order  
    * Can add jitter such as jitter=2 to prevent overlapping and better see the true density of points  
* Can use plotly to have an interactive look at the rules  
	* plot(rules, engine="plotly")  
  
Rules to Graph-Based Visualizations:  
  
* Can visualize rules using graph methods as part of plot()  
	* plot(rules, method="graph", engine="htmlwidget")  
    * Arrows go lhs -> rule -> rhs  
* Can select either a rule or an item from the drop-down in the ULS of the graph  
* Can also look at subgraphs by filtering prior to passing rules to the plot() call  
* Can also save graphs for later use  
  
Alternative Rule Plots:  
  
* Can use grouped methods for plotting  
	* plot(rules, method="grouped", measure="lift", shading="confidence")  
* Can use the parallel coordinate plotting method  
	* plot(rules, method="paracoord")  # arrows point to consequent condition  
* Can use ruleExplorer(rules) to open a Shiny app with all of the possible plots, controllable by widgets  
  
Example code includes:  
```{r eval=FALSE}

# Display items horizontally
itemFrequencyPlot(Online_trx, topN = 5, horiz = TRUE)


# Changing the font of the items
itemFrequencyPlot(Online_trx, topN = 10, col = rainbow(10), type = "relative", horiz = TRUE, 
                  main = "Relative Item Frequency Plot" ,xlab = "Frequency", cex.names = 0.8
                  )


library(arulesViz)

# Inspection of the rules
inspectDT(rules_online)

# Create a standard scatterplot
plot(rules_online)

# Change the axis and legend of the scatterplot
plot(rules_online, measure = c("confidence", "lift"), shading = "support")


# Plot a two-key plot
plot(rules_online, method = "two-key plot")

# Plot a matrix plot
plot(rules_online, method = "matrix")

# Plot a matrix plot with confidence as color coding
plot(rules_online, method = "matrix", shading = "confidence")


# Create a HTML widget of the graph of rules
plot(rules_online, method = "graph", engine = "htmlwidget")

# HTML widget graph for the highest confidence rules
plot(head(sort(rules_online, by="confidence"), 5), method = "graph", engine = "htmlwidget")

# HTML widget graph for rules with lowest lift
plot(tail(sort(rules_online, by="lift"), 5), method = "graph", engine = "htmlwidget")


# Create an interactive graph visualization
rules_html <- plot(rules_online, method = "graph", engine = "htmlwidget")

# Save the interactive graph as an html file
# htmlwidgets::saveWidget(rules_html, file = "./RInputFiles/rules_grocery.html")


# Plot a group matrix-based visualization
# plot(subset_rules, method = "grouped")

# Change the arguments of group matrix-based visualization
# plot(subset_rules, method = "grouped", measure = "lift", shading = "confidence")


# Plotting the parallel coordinate plots
plot(rules_online, method = "paracoord")

# Parallel coordinate plots with confidence as color coding
plot(rules_online, method = "paracoord", shading = "confidence")

```
  
  
  
***
  
Chapter 4 - Case Study: Market Basket with Movies  
  
Recap on Transactions:  
  
* Market basket analysis is based on whch items are bought together rather than the quantity of each item purchased  
* Can use the dataset Groceries from the library arules for an example  
* Can use the crossTable(x, sort=TRUE) function to get the joint relationships of the various items  
	* crossTable(x, measure="chi")  
    * crossTable(x, measure="lift", sort=TRUE)  
* Can get the counts of various items by using x['item1', 'item2']  
  
Mining Association Rules:  
  
* Can use arules::apriori() to find key itemsets and combinations  
	* Using target="rules" will pull out the rules, which should be paired with sort(), head(), and tail()  
* Can use varying confidence levels and loop over them  
* Can use %ain% to mean 'all items in' or %in% to mean 'any items in'  
  
Visualizing Transactions and Rules:  
  
* Can create plots using the method= argument  
* Can use shiny by way of ruleExplorer(rules)  
	* Not always recommended for large data as calculation times can be lengthy  
  
Making the most of Market Basket Analysis:  
  
* Can use market basket analysis to cluster users  
* Example of trying to understand what purchased items are associated with a purchase of yogurt  
  
Wrap Up:  
  
* Support, confidence, and list are the main methods for classifying an association  
* Visualization and plotting are key skills  
* Recommendation engines can be based on market basket analysis  
* Can extend with information about transaction timing, location, etc., for greater insights  
* Be careful to sort and to look at only subsets of rules  
  
Example code includes:  
```{r eval=FALSE}

# Have a glimpse at the dataset
movie_subset %>% 
    glimpse()

# Calculate the number of distinct users and movies
n_distinct(movie_subset$userId)
n_distinct(movie_subset$movieId)

# Distribution of the number of movies watched by users
movie_subset %>%
    group_by(userId) %>% 
    summarize(nb_movies = n_distinct(movieId)) %>%
    ggplot(aes(x=nb_movies)) +
    geom_histogram() + 
    ggtitle("Distribution of number of movies watched")


# Split dataset into movies and users
data_list <- split(movie_subset$title, movie_subset$userId)

# Transform data into a transactional dataset
movie_trx <- as(data_list, "transactions")

# Plot of the item matrix
image(movie_trx[1:100,1:100])


# Setting the plot configuration option
par(mfrow=c(2, 1))

# Plot the relative and absolute item frequency plot
itemFrequencyPlot(movie_trx, type = "relative", topN = 10, horiz = TRUE, main = 'Relative item frequency')
itemFrequencyPlot(movie_trx, type = "absolute", topN = 10, horiz = TRUE, main = 'Absolute item frequency')

par(mfrow=c(1, 1))


# Extract the set of most frequent itemsets
itemsets <- apriori(movie_trx, parameter = list(support = 0.4, target = 'frequent itemsets'))

# Inspect the five most popular items
arules::inspect(sort(itemsets, by='support', decreasing = TRUE)[1:5])

# Extract the set of most frequent itemsets
itemsets_minlen2 <- apriori(movie_trx, parameter = list(support = 0.3, minlen = 2, target = 'frequent'))

# Inspect the five most popular items
arules::inspect(sort(itemsets_minlen2, by='support', decreasing = TRUE)[1:5])


# Set of confidence levels
confidenceLevels <- seq(from=0.95, to=0.5, by=-0.05)

# Create empty vector
rules_sup04 <- NULL
rules_sup03 <- NULL

# Apriori algorithm with a support level of 40% and 30%
for (i in 1:length(confidenceLevels)) {
    rules_sup04[i] = length(apriori(movie_trx, 
                                    parameter=list(sup=0.4, conf=confidenceLevels[i], target="rules")
                                    )
                            )
    rules_sup03[i] = length(apriori(movie_trx, 
                                    parameter=list(sup=0.3, conf=confidenceLevels[i], target="rules")
                                    )
                            )
}

# Number of rules found with a support level of 40%
qplot(confidenceLevels, rules_sup04, geom=c("point", "line"), xlab="Confidence level", 
      ylab="Number of rules found",main="Apriori with a support level of 40%"
      ) + 
    theme_bw()

# Create Data frame containing all results
nb_rules <- data.frame(rules_sup04, rules_sup03, confidenceLevels)

# Number of rules found with a support level of 40% and 30%
ggplot(data=nb_rules, aes(x=confidenceLevels)) +
    # Lines and points for rules_sup04
    geom_line(aes(y=rules_sup04, colour="Support level of 40%")) + 
    geom_point(aes(y=rules_sup04, colour="Support level of 40%")) +
    # Lines and points for rules_sup03
    geom_line(aes(y=rules_sup03, colour="Support level of 30%")) +
    geom_point(aes(y=rules_sup03, colour="Support level of 30%")) + 
    # Polishing the graph
    theme_bw() + ylab("") +
    ggtitle("Number of extracted rules with apriori")


# Extract rules with the apriori
rules_movies <- apriori(movie_trx, parameter = list(supp = 0.3, conf = 0.9, minlen = 2, target = "rules"))

# Summary of extracted rules
summary(rules_movies)

# Create redudant rules and filter from extracted rules
rules_red <- is.redundant(rules_movies)
rules.pruned <- rules_movies[!rules_red]
# Inspect the non-redundant rules with highest confidence
arules::inspect(head(sort(rules.pruned, by="confidence")))


# Plot rules as scatterplot
plot(rules_movies, measure = c("confidence", "lift"), shading = "support", jitter = 1, engine = "html")

# Interactive matrix-based plot
plot(rules_movies, method = "matrix", shading ="confidence", engine = "html")

# Grouped matrix plot of rules
plot(rules_movies, method = "grouped", measure = "lift", shading = "confidence")

# Parallel coordinate plots with confidence as color coding
plot(rules_movies, method = "paracoord", shading = "confidence")


# Plot movie rules as a graph
plot(rules_movies, method = "graph", engine = "htmlwidget")

# Retrieve the top 10 rules with highest confidence
top10_rules_movies = head(sort(rules_movies, by = "confidence"), 10)

# Plot as an interactive graph the top 10 rules
plot(top10_rules_movies, method = "graph", engine = "htmlwidget")


# Extract rules with Pulp Fiction on the right side
pulpfiction_rules_rhs <- apriori(movie_trx, parameter = list(supp = 0.3, conf = 0.5), 
                                 appearance = list(default = "lhs", rhs = "Pulp Fiction")
                                 ) 

# Inspect the first rules
arules::inspect(head(pulpfiction_rules_rhs))
arules::inspect(head(sort(pulpfiction_rules_rhs, by="lift"), 10))


# Extract rules with Pulp Fiction on the left side
pulpfiction_rules_lhs <- apriori(movie_trx, parameter = list(supp = 0.3, conf = 0.5), 
                                 appearance = list(default = "rhs", lhs = "Pulp Fiction")
                                 )

# Summary of extracted rules
summary(pulpfiction_rules_lhs)

# Inspect the first rules
arules::inspect(head(pulpfiction_rules_lhs))


# Extract rules with Pulp Fiction on the left side
pulpfiction_rules_lhs <- apriori(movie_trx, parameter = list(supp = 0.3, conf = 0.5, minlen = 2), 
                                 appearance = list(default = "rhs", lhs = "Pulp Fiction")
                                 ) 

# Inspect the first rules
arules::inspect(head(pulpfiction_rules_lhs))

```
  
  
  
***
  
### _Analyzing Social Media Data in R_  
  
Chapter 1 - Understanding Twitter Data  
  
Analyzing Twitter Data:  
  
* Social media analysis is the process of collecting and analyzing social media data to derive insights  
* Can use stream_tweets() to sample 1% of the data from 30 seconds of tweets to a data frame  
	* Can use stream_tweets("", timeout=60) to extend to 60 seconds  
* There are many applications ot using tweets - sentiments, events, outbreaks, etc.  
* The hashtags make it easy to follow topics on Twitter  
  
Extracting Twitter Data:  
  
* API (Application Programming Interface) allows applications to talk with each other and exchange information  
* Different levels of Twitter API provide different levels of access intended for different types of users  
	* Standard API - free and includes only the last 7 days of tweets  
* Preprequisites for running twitter analysis in R includes  
	* Twitter account  
    * Pop-up blocker disabled  
    * Interactive R session opened  
    * rtweet and httpuv installed  
* The search_tweets() function will retrieve a maximum of 18,000 tweets from the past 7 days  
	* The first argument is the search term  
    * Can add n=, include_rts=, lang='en', etc.  
* The get_timeline() function will return up to 3,200 tweets posted by a specific user  
	* get_timeline('userName', n=)  
  
Components of Twitter Data:  
  
* A tweet can have over 150 metadata components, stored using JSON format  
* The rtweet library converts the Twitter JSON to a data.frame  
	* Each attribute becomes a column  
    * screen_name is the Twitter handle  
* Can use the lookup_users(users) to get the follower data  
	* tvseries <- lookup_users("GameOfThrones", "fleabag", "BreakingBad")  
    * user_df <- tvseries[, c("screen_name", "followers_count")]  
* Can grab "retweet_text" column from the data.frame to get the retweet counts  
  
Example code includes:  
```{r eval=FALSE}

# Extract live tweets for 120 seconds window
# tweets120s <- rtweet::stream_tweets("", timeout = 120)

# View dimensions of the data frame with live tweets
# dim(tweets120s)


# Extract tweets on "#Emmyawards" and include retweets
# twts_emmy <- rtweet::search_tweets("#Emmyawards", n = 2000, include_rts = TRUE, lang = "en")

# View output for the first 5 columns and 10 rows
# head(twts_emmy[,1:5], 10)


# Extract tweets posted by the user @Cristiano
# get_cris <- rtweet::get_timeline("@Cristiano", n = 3200)

# View output for the first 5 columns and 10 rows
# head(get_cris[,1:5], 10)


tweets_ai <- read_csv("./RInputFiles/tweets_ai.xls")
str(tweets_ai)


# Create a table of users and tweet counts for the topic
sc_name <- table(tweets_ai$screen_name)

# Sort the table in descending order of tweet counts
sc_name_sort <- sort(sc_name, decreasing = TRUE)

# View sorted table for top 10 users
head(sc_name_sort, 10)


# Extract user data for the twitter accounts of 4 news sites
# users <- rtweet::lookup_users("nytimes", "CNN", "FoxNews", "NBCNews")

# Create a subset data frame of screen names and follower counts
# user_df <- users[,c("screen_name","followers_count")]

# Display and compare the follower counts for the 4 news sites
# user_df


# Create a data frame of tweet text and retweet count
rtwt <- tweets_ai[,c("text", "retweet_count")]
head(rtwt)

# Sort data frame based on descending order of retweet counts
rtwt_sort <- arrange(rtwt, desc(retweet_count))

# Exclude rows with duplicate text from sorted data frame
rtwt_unique <- unique(rtwt_sort, by = "text")

# Print top 6 unique posts retweeted most number of times
rownames(rtwt_unique) <- NULL
head(rtwt_unique)

```
  
  
  
***
  
Chapter 2 - Analyzing Twitter Data  
  
Filtering Tweets:  
  
* Filtering is necessary due to the very large volume of tweets  
* Can use the -filter to extract original tweets  
	* search_tweets("digital_marketing", n=100)  
    * search_tweets("digital_marketing -filter:retweets -filter:quote -filter:replies", n=100)  
* Can extract only a minimum number of retweets and favorites  
	* search_tweets("bitcoin min_faves:100 AND min_retweets:100")  
  
Twitter User Analysis:  
  
* Can use Twitter to identify influencers and other key individuals for marketing  
	* Followers are users following a specific user - followers_count  
    * Friends are people the specific user is following - friends_count  
    * The golden ratio is defined as followers divided by friends  
* Can use the lists_users() command to get Twitter lists and lists_subscribers() to get the associated subscribers  
	* lists_users("myUser")  
    * lists_subscribers(slug="gaming", owner_user="myList", n=100)  
    * lookup_users(users)  
  
Twitter Trends:  
  
* Trending topics can provide a roadmap for areas for increasing engagement  
	* trend_topics <- get_trends()  
    * trends_available()  # list of cities and countries  
    * trend_topics <- get_trends("United States")  
    * trend_topics <- get_trends("New York")  
* The data for the 'tweet_volume' column is available only for some trends  
  
Plotting Twitter Data Over Time:  
  
* Can create time series data from the Twitter data  
	* ts_plot(df, by="hours", col="blue")  # by= is the unit of time to use for creating the plot - seconds, minutes, hours, days, etc.  
    * ts_data(df, by="minutes")  
  
Example code includes:  
```{r eval=FALSE}

# DO NOT RUN

# Extract 100 original tweets on "Superbowl"
tweets_org <- search_tweets("Superbowl -filter:retweets -filter:quote -filter:replies", n = 100)

# Check for presence of replies
count(tweets_org$reply_to_screen_name)

# Check for presence of quotes
count(tweets_org$is_quote)

# Check for presence of retweets
count(tweets_org$is_retweet)


# Extract tweets on "Apple iphone" in French
tweets_french <- search_tweets("Apple iphone", lang = "fr")

# View the tweets
head(tweets_french$text)

# View the tweet metadata showing the language
head(tweets_french$lang)


# Extract tweets with a minimum of 100 retweets and 100 favorites
tweets_pop <- search_tweets("Chelsea min_retweets:100 AND min_faves:100")

# Create a data frame to check retweet and favorite counts
counts <- tweets_pop[c("retweet_count", "favorite_count")]
head(counts)

# View the tweets
head(tweets_pop$text)


# Extract user information of people who have tweeted on the topic
user_cos <- users_data(tweet_cos)

# View few rows of user data
head(user_cos)

# Aggregate screen name, follower and friend counts
counts_df <- user_cos %>%
    group_by(screen_name) %>%
    summarize(follower = mean(followers_count), friend = mean(friends_count))

# View the output
head(counts_df)


# Calculate and store the golden ratio
counts_df$ratio <- counts_df$follower/counts_df$friend

# Sort the data frame in decreasing order of follower count
counts_sort <- arrange(counts_df, desc(follower))

# View the first few rows
head(counts_sort)

# Select rows where the follower count is greater than 50000
counts_sort[counts_sort$follower>50000,]

# Select rows where the follower count is less than 1000
counts_sort[counts_sort$follower<1000,]


# Get topics trending in Canada
gt_country <- get_trends("Canada")

# View the first 6 columns
head(gt_country[,1:6])


# Get topics trending in London
gt_city <- get_trends("London")

# View the first 6 columns
head(gt_city[,1:6])

# Aggregate the trends and tweet volumes
trend_df <- gt_city %>%
    group_by(trend) %>%
    summarize(tweet_vol = mean(tweet_volume))

# Sort data frame on descending order of tweet volumes and print header
trend_df_sort <- arrange(trend_df, desc(tweet_vol))
head(trend_df_sort,10)


# Extract tweets on #walmart and exclude retweets
walmart_twts <- search_tweets("#walmart", n = 18000, include_rts = FALSE)

# View the output
head(walmart_twts)

# Create a time series plot
ts_plot(walmart_twts, by = "hours", color = "blue")


# Create a time series object for Puma at hourly intervals
puma_ts <- ts_data(puma_st, by ='hours')

# Rename the two columns in the time series object
names(puma_ts) <- c("time", "puma_n")

# View the output
head(puma_ts)

# Create a time series object for Nike at hourly intervals
nike_ts <- ts_data(nike_st, by ='hours')

# Rename the two columns in the time series object
names(nike_ts) <- c("time", "nike_n")

# View the output
head(nike_ts)


# Merge the two time series objects and retain "time" column
merged_df <- merge(puma_ts, nike_ts, by = "time", all = TRUE)
head(merged_df)

# Stack the tweet frequency columns
melt_df <- melt(merged_df, na.rm = TRUE, id.vars = "time")

# View the output
head(melt_df)

# Plot frequency of tweets on Puma and Nike
ggplot(data = melt_df, aes(x = time, y = value, col = variable))+
    geom_line(lwd = 0.8)

```
  
  
  
***
  
Chapter 3 - Visualize Tweet Texts  
  
Processing Twitter Text:  
  
* Processing tweet text helps derive insights from the stream  
* Steps in text processing include  
	* Remove redundant information  
    * Convert text to corpus (list of text documents)  
    * Convert the corpus to be all-lowercase  
    * Remove stopwords from the corpus  
* Example of processing fields in twt_txt, a character vector of tweet texts  
	* twt_txt_url <- qdapRegex::rm_twitter_url(twt_txt)  
    * twt_txt_chrs <- gsub("[^A-Za-z]", " ", twt_txt_url)  # leaves only upper and lower characters - removes numbers, punctuation, and special characters  
    * twt_corpus <- twt_txt_chrs %>% VectorSource() %>% Corpus()  
    * twt_corpus_lower <- tm_map(twt_corpus, tolower)  
    * twt_corpus_stopwd <- tm_map(twt_corpus_lower, removeWords, stopwords("english"))  
    * twt_corpus_final <- tm_map(twt_corpus_stopwd, stripWhitespace)  
  
Visualize Popular Terms:  
  
* Can extract the most common words (after pre-processing) and then plot them  
	* freq_terms(corpus, n)  
* Can create custom stop words and remove them  
	* custom_stop <- c()  
    * tm_map(corpus, removeWords, custom_stop)  
* Can create word clouds to visualize frequent terms  
	* wordcloud(corpus, min.freq=, colors=, scale=, random.order=FALSE)  
  
Topic Modeling of Tweets:  
  
* Topic modeling is the process of automatically discovering topics from a corpus  
* Can use LDA (mathematical model for assessing mixtures of topics with words and mixture of documents with topics)  
* The document-term-matrix (DTM) is a mix of documents and terms  
	* dtm <- DocumentTermMatrix(corpus)  
    * inspect(dtm)  
    * rowTotals <- apply(dtm, 1, sum)  
    * tweet_dtm_new <- dtm[rowTotals > 0]  
* Can then use topicmodels::LDA() to extract the topics  
	* library(topicmodels)  
    * lda_5 <- LDA(tweet_dtm_new, k=5)  
    * top10_terms <- terms(lda_5, 10)  
  
Twitter Sentiment Analysis:  
  
* Sentiment analysis is the process of understanding perceptions (positive, neutral, negative, joy, anger, etc.) from a text  
* The syuzhet package contains sentiment mapping functions and files  
  
Example code includes:  
```{r eval=FALSE}

twt_telmed <- readRDS("./RInputFiles/tweets_telmed.rds")
dim(twt_telmed)


# Extract tweet text from the pre-loaded dataset
twt_txt <- twt_telmed$text
head(twt_txt)

# Remove URLs from the tweet text and view the output
twt_txt_url <- qdapRegex::rm_twitter_url(twt_txt)
head(twt_txt_url)

# Replace special characters, punctuation, & numbers with spaces
twt_txt_chrs  <- gsub("[^A-Za-z]"," " , twt_txt_url)

# View text after replacing special characters, punctuation, & numbers
head(twt_txt_chrs)


# Convert text in "twt_gsub" dataset to a text corpus and view output
twt_corpus <- twt_txt_chrs %>% 
    tm::VectorSource() %>% 
    tm::Corpus()

head(twt_corpus$content)

# Convert the corpus to lowercase
twt_corpus_lwr <- tm::tm_map(twt_corpus, tolower) 

# View the corpus after converting to lowercase
head(twt_corpus_lwr$content)


# Remove English stop words from the corpus and view the corpus
twt_corpus_stpwd <- tm::tm_map(twt_corpus_lwr, tm::removeWords, tm::stopwords("english"))
head(twt_corpus_stpwd$content)

# Remove additional spaces from the corpus
twt_corpus_final <- tm::tm_map(twt_corpus_stpwd, tm::stripWhitespace)

# View the text corpus after removing spaces
head(twt_corpus_final$content)


# Extract term frequencies for top 60 words and view output
termfreq  <-  qdap::freq_terms(twt_corpus_final, 60)
termfreq

# Create a vector of custom stop words
custom_stopwds <- c("telemedicine", " s", "amp", "can", "new", "medical", "will", "via", "way",  "today", "come", "t", "ways", "say", "ai", "get", "now")

# Remove custom stop words and create a refined corpus
corp_refined <- tm::tm_map(twt_corpus_final, tm::removeWords, custom_stopwds) 

# Extract term frequencies for the top 20 words
termfreq_clean <- qdap::freq_terms(corp_refined, 20)
termfreq_clean


# Extract term frequencies for the top 10 words
termfreq_10w <- qdap::freq_terms(corp_refined, 10)
termfreq_10w

# Identify terms with more than 60 counts from the top 10 list
term60 <- subset(termfreq_10w, FREQ > 60)

# Create a bar plot using terms with more than 60 counts
ggplot(term60, aes(x = reorder(WORD, -FREQ), y = FREQ)) + 
    geom_bar(stat = "identity", fill = "red") + 
    theme(axis.text.x = element_text(angle = 15, hjust = 1))

# Extract term frequencies for the top 25 words
termfreq_25w <- qdap::freq_terms(corp_refined, 25)
termfreq_25w

# Identify terms with more than 50 counts from the top 25 list
term50 <- subset(termfreq_25w, FREQ > 50)
term50

# Create a bar plot using terms with more than 50 counts
ggplot(term50, aes(x = reorder(WORD, -FREQ), y = FREQ)) + 
    geom_bar(stat = "identity", fill = "blue") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Create a word cloud in red with min frequency of 20
wordcloud::wordcloud(corp_refined, min.freq = 20, colors = "red", scale = c(3, 0.5),random.order = FALSE)

# Create word cloud with 6 colors and max 50 words
wordcloud::wordcloud(corp_refined, max.words = 50, colors = RColorBrewer::brewer.pal(6, "Dark2"), 
                     scale=c(4, 1), random.order = FALSE
                     )


# Create a document term matrix (DTM) from the pre-loaded corpus
# dtm_climate <- tm::DocumentTermMatrix(corpus_climate)
# dtm_climate

# Find the sum of word counts in each document
# rowTotals <- apply(dtm_climate, 1, FUN=sum)
# head(rowTotals)

# Select rows with a row total greater than zero
# dtm_climate_new <- dtm_climate[rowTotals > 0, ]
# dtm_climate_new


# Create a topic model with 5 topics
# topicmodl_5 <- topicmodels::LDA(dtm_climate_new, k = 5)

# Select and view the top 10 terms in the topic model
# top_10terms <- terms(topicmodl_5, 10)
# top_10terms 

# Create a topic model with 4 topics
# topicmodl_4 <- topicmodels::LDA(dtm_climate_new, k = 4)

# Select and view the top 6 terms in the topic model
# top_6terms <- terms(topicmodl_4, 6)
# top_6terms 


# Perform sentiment analysis for tweets on `Climate change` 
# sa.value <- syuzhet::get_nrc_sentiment(tweets_cc$text)

# View the sentiment scores
# head(sa.value, 10)


# Calculate sum of sentiment scores
# score <- colSums(sa.value[,])

# Convert the sum of scores to a data frame
# score_df <- data.frame(score)

# Convert row names into 'sentiment' column and combine with sentiment scores
# score_df2 <- cbind(sentiment = row.names(score_df), score_df, row.names = NULL)
# print(score_df2)

# Plot the sentiment scores
# ggplot(data = score_df2, aes(x = sentiment, y = score, fill = sentiment)) +
#     geom_bar(stat = "identity") +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
  
  
  
***
  
Chapter 4 - Network Analysis and Mapping  
  
Twitter Network Analysis:  
  
* Network analysis is the process of mapping network objects to understand interdependicies and information flow  
	* Nodes/vertices are the objects  
    * Edges are the connections between the objects - can be directed or undirected  
* Example for the retweet network of #OOTD  
	* twts_OOTD <- search_tweets("#OOTD ", n=18000, include_rts=TRUE)  
    * rt_df <- twts_OOTD[, c("screen_name", "retweet_screen_name")]  
    * rt_df_new <- rt_df[complete.cases(rt_df), ]  
    * matrx <- as.matrix(rt_df_new)  
    * nw_rtweet <- igraph::graph_from_edgelist(el=matrx, directed=TRUE)  
  
Network Centrality Measures:  
  
* Centrality measures include degree centrality and betweenness  
* Degree centrality is the number of edges for a vertex  
	* Out-degree is the number of outward edges (e.g., number of times user retweets)  
    * In-degree is the number of inward edges (e.g., number of times user is retweeted)  
    * out_deg <- degree(nw_rtweet, "userName", mode=c("out"))  # can skip the "userName" and get for all users  
    * in_deg <- degree(nw_rtweet, "userName", mode=c("in"))  # can skip "userName" and get for all users  
* Betweeness is the degree to which nodes link each other (high means more control over the network)  
	* between_nw <- betweenness(nw_rtweet, directed=TRUE)  
  
Visualizing Twitter Networks:  
  
* Can visualize plots using plot.igraph() with optional arguments  
	* asp= is the aspect ratio (9/16 is rectangle)  
    * vertex.size  
    * vertex.color  
    * edge.arrow.size  
    * edge.color  
    * vertex.label.cex  
    * vertex.label.color  
  
Mapping Twitter Data:  
  
* Can use geographic metadata from tweets for geo-coding and mapping  
	* Place is selected by the user from a pre-defined list on Twitter and consist of a bounding box  
    * Precise location is the specific lat/lon from GPS enabled devices (only around 1% of tweets have this level of geo-tagging)  
* Can use the rtweet library to find the geographic coding  
	* pol_coord <- lat_lng(pol)  # pol is tweet data  
    * map(database="state", fill=TRUE, col="light yellow")  
    * with(pol_geo, points(lng, lat, pch=20, cex=1, col="blue"))  
  
Wrap Up:  
  
* Tweet components and extraction  
* Filtering and analyzing tweets  
* Word clouds, sentiment analysis, and visualization  
* Network analysis and geocoding/mapping  
  
Example code includes:  
```{r eval=FALSE}

# DO NOT RUN - DO NOT HAVE DATASET

# Extract source vertex and target vertex from the tweet data frame
rtwt_df <- twts_trvl[, c("screen_name" , "retweet_screen_name")]

# View the data frame
head(rtwt_df)

# Remove rows with missing values
rtwt_df_new <- rtwt_df[complete.cases(rtwt_df), ]

# Create a matrix
rtwt_matrx <- as.matrix(rtwt_df_new)
head(rtwt_matrx)


# Convert the matrix to a retweet network
nw_rtweet <- graph_from_edgelist(el = rtwt_matrx, directed = TRUE)

# View the retweet network
print.igraph(nw_rtweet)


# Calculate out-degree scores from the retweet network
out_degree <- degree(nw_rtweet, mode = c("out"))

# Sort the out-degree scores in decreasing order
out_degree_sort <- sort(out_degree, decreasing = TRUE)

# View users with the top 10 out-degree scores
out_degree_sort[1:10]


# Compute the in-degree scores from the retweet network
in_degree <- degree(nw_rtweet, mode = c("in"))

# Sort the out-degree scores in decreasing order
in_degree_sort <- sort(in_degree, decreasing = TRUE)

# View users with the top 10 in-degree scores
in_degree_sort[1:10]


# Calculate the betweenness scores from the retweet network
betwn_nw <- betweenness(nw_rtweet, directed = TRUE)

# Sort betweenness scores in decreasing order and round the values
betwn_nw_sort <- betwn_nw %>%
    sort(decreasing = TRUE) %>%
    round()

# View users with the top 10 betweenness scores 
betwn_nw_sort[1:10]


# Create a basic network plot
plot.igraph(nw_rtweet)

# Create a network plot with formatting attributes
set.seed(1234)
plot(nw_rtweet, asp = 9/12, vertex.size = 10, vertex.color = "green", edge.arrow.size = 0.5, 
     edge.color = "black", vertex.label.cex = 0.9, vertex.label.color = "black"
     )


# Create a variable for out-degree
deg_out <- degree(nw_rtweet, mode = c("out"))
deg_out

# Amplify the out-degree values
vert_size <- (deg_out * 3) + 5

# Set vertex size to amplified out-degree values
set.seed(1234)
plot(nw_rtweet, asp = 10/11, vertex.size = vert_size, vertex.color = "lightblue", 
     edge.arrow.size = 0.5, edge.color = "grey", vertex.label.cex = 0.8, vertex.label.color = "black"
     )


# Create a column and categorize follower counts above and below 500
followers$follow <- ifelse(followers$followers_count > 500, "1", "0")
head(followers)

# Assign the new column as vertex attribute to the retweet network
V(nw_rtweet)$followers <- followers$follow
vertex_attr(nw_rtweet)

# Set the vertex colors based on follower count and create a plot
sub_color <- c("lightgreen", "tomato")
plot(nw_rtweet, asp = 9/12, vertex.size = vert_size, edge.arrow.size = 0.5, vertex.label.cex = 0.8,
     vertex.color = sub_color[as.factor(vertex_attr(nw_rtweet, "followers"))], 
     vertex.label.color = "black", vertex.frame.color = "grey"
     )


# Extract tweets using search_tweets()
vegan <- search_tweets("#vegan", n = 18000)

# Extract geo-coordinates data to append as new columns
vegan_coord <- lat_lng(vegan)

# View the columns with geo-coordinates for first 20 tweets
head(vegan_coord[c("lat", "lng")], 20)


# Omit rows with missing geo-coordinates in the data frame
vegan_geo <- na.omit(vegan_coord[,c("lat", "lng")])

# View the output
head(vegan_geo)

# Plot longitude and latitude values of tweets on the US state map
map(database = "state", fill = TRUE, col = "light yellow")
with(vegan_geo, points(lng, lat, pch = 20, cex = 1, col = 'blue'))

# Plot longitude and latitude values of tweets on the world map
map(database = "world", fill = TRUE, col = "light yellow")
with(vegan_geo, points(lng, lat, pch = 20, cex = 1, col = 'blue')) 

```
  
  
  
***
  
### _Building Web Applications with Shiny in R_  
  
Chapter 1 - Get Started with Shiny  
  
Introduction to Shiny:  
  
* Shiny is an R package that allow for creating interactive applications and graphics  
* Web aplications have a user interface (UI) that updates the display based on an app in the server  
  
Build a "Hello World" Shiny App:  
  
* Shiny can be loaded like any other R package - library(shiny)  
* Example shell for a Shiny app  
	* library(shiny)  
    * ui <- fluidPage()  
    * server <- function(input, output, session) {}  
    * shinyApp(ui=ui, server=server)  
  
Build a babynames explorer Shiny App:  
  
* Begin by sketching out the desired look and usability of the app  
	* Add UI (inputs and interface)  
    * Add Server  
* The user interface can have many components  
	* titlePanel()  
    * textInput()  
    * textOutput()  # use the quoted variable name from server as the argument  
    * plotOutput()  # use the quoted variable name from server as the argument  
    * sidebarPanel()  
    * mainPanel()  
    * sidebarLayout()  
  
Example code includes:  
```{r eval=FALSE}

library(shiny)


ui <- fluidPage(
  # CODE BELOW: Add a text input "name"
  textInput("name", "Enter your name: ")
)

server <- function(input, output) {
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
  textInput("name", "What is your name?"), 
  # CODE BELOW: Display the text output, greeting
  textOutput("greeting")
)

server <- function(input, output) {
  # CODE BELOW: Render a text output, greeting
  output$greeting <- renderText({paste0("Hello, ", input$name)})
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
  # CODE BELOW: Add a text input "name"
  textInput("name", "Enter Your Name", "David")
)
server <- function(input, output, session) {
}
shinyApp(ui = ui, server = server)


ui <- fluidPage(
  textInput('name', 'Enter Name', 'David'), 
  # CODE BELOW: Display the plot output named 'trend'
  plotOutput("trend")
)
server <- function(input, output, session) {
  # CODE BELOW: Render an empty plot and assign to output named 'trend'
  output$trend <- renderPlot({ggplot()})
}
shinyApp(ui = ui, server = server)


ui <- fluidPage(
  titlePanel("Baby Name Explorer"),
  sidebarLayout(
    sidebarPanel(textInput('name', 'Enter Name', 'David')),
    mainPanel(plotOutput('trend'))
  )
)

server <- function(input, output, session) {
  output$trend <- renderPlot({
    # CODE BELOW: Update to display a line plot of the input name
    babynames::babynames %>% 
      filter(name==input$name) %>% 
      ggplot(aes(x=year, y=prop, color=sex)) + 
      geom_line()
  })
}

shinyApp(ui = ui, server = server)

```
  
  
  
***
  
Chapter 2 - Inputs, Outputs, and Layouts  
  
Inputs:  
  
* Can create many types of inputs - text, slider, select, numerical, daterange, etc.  
	* selectInput("name", "make selection", choices=c("a", "b", "c"))  
    * sliderInput("name", "make selection", value=1925, min=1900, max=2000)  
* Need to give every input a unique name so that it can be called in the server function  
  
Outputs:  
  
* The render functions build ouptuts as functions of inputs and other factors  
	* renderText({})  
    * renderTable()  
    * renderImage()  
    * renderPlot()  
* Can then show outputs in the UI by using the appropriate textOutput() function  
* Can use html widgets and associated packages to render Shiny outputs - example for data.table in package DT  
  
Layouts and Themes:  
  
* The default Shiny App layout can be customized as needed  
* The sidebarLayout() is a common over-ride, where there is a side panel and a main panel  
* The tabLayout() allows for multiple tabs, and is a subset of sidebarLayout  
	* There must be a tabsetPanel() inside, with each containing one or more tabPanel()  
* Can allow users to select themes using shinythemes::themeSelector()  
	* theme=shinythemes::shinytheme('superhero')  # if inside the UI, will apply theme 'superhero'  
  
Building Apps:  
  
* Example of creating an app based on gapminder data with selections for continent and year  
	* Add inputs (UI)  
    * Add outputs (UI/Server)  
    * Update layout (UI)  
    * Connect Server and UI with appropriate functions  
  
Example code includes:  
```{r eval=FALSE}

ui <- fluidPage(
  titlePanel("What's in a Name?"),
  # CODE BELOW: Add select input named "sex" to choose between "M" and "F"
  selectInput("sex", "Select Sex", selected="F", choices=c("F", "M")), 
  sliderInput("year", "Select Year", value=1900, min=1900, max=2010), 
  # Add plot output to display top 10 most popular names
  plotOutput('plot_top_10_names')
)

server <- function(input, output, session){
  # Render plot of top 10 most popular names
  output$plot_top_10_names <- renderPlot({
    # Get top 10 names by sex and year
    top_10_names <- babynames::babynames %>% 
      # MODIFY CODE BELOW: Filter for the selected sex
      filter(sex == input$sex) %>% 
      filter(year == input$year) %>% 
      top_n(10, prop)
    # Plot top 10 names by sex and year
    ggplot(top_10_names, aes(x = name, y = prop)) +
      geom_col(fill = "#263e63")
  })
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
  titlePanel("What's in a Name?"),
  # Add select input named "sex" to choose between "M" and "F"
  selectInput('sex', 'Select Sex', choices = c("F", "M")),
  # Add slider input named "year" to select year between 1900 and 2010
  sliderInput('year', 'Select Year', min = 1900, max = 2010, value = 1900),
  # CODE BELOW: Add table output named "table_top_10_names"
  tableOutput("table_top_10_names")
)

server <- function(input, output, session){
  # Function to create a data frame of top 10 names by sex and year 
  top_10_names <- function(){
    top_10_names <- babynames::babynames %>% 
      filter(sex == input$sex) %>% 
      filter(year == input$year) %>% 
      top_n(10, prop)
  }
  # CODE BELOW: Render a table output named "table_top_10_names"
  output$table_top_10_names <- renderTable({top_10_names()})
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
  titlePanel("What's in a Name?"),
  # Add select input named "sex" to choose between "M" and "F"
  selectInput('sex', 'Select Sex', choices = c("M", "F")),
  # Add slider input named "year" to select year between 1900 and 2010
  sliderInput('year', 'Select Year', min = 1900, max = 2010, value = 1900),
  # MODIFY CODE BELOW: Add a DT output named "table_top_10_names"
  DT::DTOutput('table_top_10_names')
)
server <- function(input, output, session){
  top_10_names <- function(){
    babynames::babynames %>% 
      filter(sex == input$sex) %>% 
      filter(year == input$year) %>% 
      top_n(10, prop)
  }
  # MODIFY CODE BELOW: Render a DT output named "table_top_10_names"
  output$table_top_10_names <- DT::renderDT({
    DT::datatable(top_10_names())
  })
}
shinyApp(ui = ui, server = server)


top_trendy_names <- data.frame(name=c('Kizzy', 'Deneen', 'Royalty', 'Mareli', 'Moesha', 'Marely', 'Kanye', 'Tennille', 'Aitana', 'Kadijah', 'Shaquille', 'Catina', 'Allisson', 'Emberly', 'Nakia', 'Jaslene', 'Kyrie', 'Akeelah', 'Zayn', 'Talan'), stringsAsFactors=FALSE)

ui <- fluidPage(
  selectInput('name', 'Select Name', top_trendy_names$name),
  # CODE BELOW: Add a plotly output named 'plot_trendy_names'
  plotly::plotlyOutput("plot_trendy_names")
)

server <- function(input, output, session){
  # Function to plot trends in a name
  plot_trends <- function(){
     babynames::babynames %>% 
      filter(name == input$name) %>% 
      ggplot(aes(x = year, y = n)) +
      geom_col()
  }
  # CODE BELOW: Render a plotly output named 'plot_trendy_names'
  output$plot_trendy_names <- plotly::renderPlotly({plot_trends()})
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
    # MODIFY CODE BELOW: Wrap in a sidebarLayout
    sidebarLayout(
        # MODIFY CODE BELOW: Wrap in a sidebarPanel
        sidebarPanel(selectInput('name', 'Select Name', top_trendy_names$name)),
        # MODIFY CODE BELOW: Wrap in a mainPanel
        mainPanel(plotly::plotlyOutput('plot_trendy_names'), DT::DTOutput('table_trendy_names'))
    )
)

# DO NOT MODIFY
server <- function(input, output, session){
  # Function to plot trends in a name
  plot_trends <- function(){
     babynames::babynames %>% 
      filter(name == input$name) %>% 
      ggplot(aes(x = year, y = n)) +
      geom_col()
  }
  output$plot_trendy_names <- plotly::renderPlotly({
    plot_trends()
  })
  
  output$table_trendy_names <- DT::renderDT({
    babynames::babynames %>% 
      filter(name == input$name)
  })
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
    sidebarLayout(
        sidebarPanel(selectInput('name', 'Select Name', top_trendy_names$name)),
        mainPanel(
            # MODIFY CODE BLOCK BELOW: Wrap in a tabsetPanel
            tabsetPanel(
                # MODIFY CODE BELOW: Wrap in a tabPanel providing an appropriate label
                tabPanel("Plot", plotly::plotlyOutput('plot_trendy_names')),
                # MODIFY CODE BELOW: Wrap in a tabPanel providing an appropriate label
                tabPanel("Table", DT::DTOutput('table_trendy_names'))
            )
        )
    )
)

server <- function(input, output, session){
  # Function to plot trends in a name
  plot_trends <- function(){
     babynames::babynames %>% 
      filter(name == input$name) %>% 
      ggplot(aes(x = year, y = n)) +
      geom_col()
  }
  output$plot_trendy_names <- plotly::renderPlotly({
    plot_trends()
  })
  
  output$table_trendy_names <- DT::renderDT({
    babynames::babynames %>% 
      filter(name == input$name)
  })
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
  # CODE BELOW: Add a titlePanel with an appropriate title
  titlePanel("Trendy Names"), 
  # REPLACE CODE BELOW: with theme = shinythemes::shinytheme("<your theme>")
  theme = shinythemes::shinytheme("spacelab"),
  sidebarLayout(
    sidebarPanel(
      selectInput('name', 'Select Name', top_trendy_names$name)
    ),
    mainPanel(
      tabsetPanel(
        tabPanel('Plot', plotly::plotlyOutput('plot_trendy_names')),
        tabPanel('Table', DT::DTOutput('table_trendy_names'))
      )
    )
  )
)
server <- function(input, output, session){
  # Function to plot trends in a name
  plot_trends <- function(){
     babynames::babynames %>% 
      filter(name == input$name) %>% 
      ggplot(aes(x = year, y = n)) +
      geom_col()
  }
  output$plot_trendy_names <- plotly::renderPlotly({
    plot_trends()
  })
  
  output$table_trendy_names <- DT::renderDT({
    babynames::babynames %>% 
      filter(name == input$name)
  })
}
shinyApp(ui = ui, server = server)


ui <- fluidPage(
    selectInput("greeting", "Select Greeting", selected="Hello", choices=c("Hello", "Bonjour")), 
    textInput("name", "Enter Your Name"),
    textOutput("greeting")
)

server <- function(input, output, session) {
    output$greeting <- renderText({paste0(input$greeting, ", ", input$name)})
}

shinyApp(ui = ui, server = server)


get_top_names <- function(.year, .sex) {
  babynames::babynames %>% 
    filter(year == .year) %>% 
    filter(sex == .sex) %>% 
    top_n(10) %>% 
    mutate(name = forcats::fct_inorder(name))
}

ui <- fluidPage(
    titlePanel("Most Popular Names"), 
    sidebarLayout(
        sidebarPanel(
            selectInput("sex", "Select Sex", selected="M", choices=c("M", "F")), 
            sliderInput("year", "Select Year", value=1900, min=1880, max=2017)
        ), 
        mainPanel(
            plotOutput("popular")
        )
    )
)

server <- function(input, output, session) {
    output$popular <- renderPlot({ get_top_names(input$year, input$sex) %>% 
                                      ggplot(aes(x=name, y=prop)) + 
                                      geom_col()
                                })
}

shinyApp(ui = ui, server = server)


ui <- fluidPage(
    titlePanel("Most Popular Names"), 
    sidebarLayout(
        sidebarPanel(
            selectInput("sex", "Select Sex", selected="M", choices=c("M", "F")), 
            sliderInput("year", "Select Year", value=1900, min=1880, max=2017)
        ), 
        mainPanel(
            tabsetPanel(
                tabPanel("Plot", plotOutput("popular")), 
                tabPanel("Table", DT::DTOutput("table"))
            )
        )
    )
)

server <- function(input, output, session) {
    output$popular <- renderPlot({ get_top_names(input$year, input$sex) %>% 
                                      ggplot(aes(x=name, y=prop)) + 
                                      geom_col()
                                })
    output$table <- DT::renderDT({ get_top_names(input$year, input$sex) })
}

shinyApp(ui = ui, server = server)

```
  
  
  
***
  
Chapter 3 - Reactive Programming  
  
Reactivity 101:  
  
* Reactive programming updates any time an input is updated (typically, a user input)  
* Reactive conductors are intermediates that either 1) depend on reactive sources, or 2) update a reactive endpoint  
* Reactive expressions are lazy and cached, which can be beneficial to avoid working multiple times  
	* Reactive expressions are called only when the value of a source changes and the endpoint requires the reactive expression  
  
Observers vs Reactives:  
  
* Reactive flow connects reactive components to create an application  
* Observers can access reactive sources but do not return values  
	* Observers are typically called for side effects such as sending data to the web server  
    * observe({ … })  
* Reactives calculate values without side effects (return values, lazy, no side effects)  
* Observers are called for the side effects (no return values, responsive, side effects)  
  
Stop-Delay-Trigger:  
  
* By default, any changes to inputs will drive changes to the outputs  
* The function isolate() can override the default behavior - any reactive wrapped in isolate() does not trigger automatic actions  
* Can use eventReactive(input$button, { <actions> }) to specify an event such as a button press that should drive the action  
* The observeEvent() is similar to a eventReactive() but acts in response to a user-action  
	* The observeEvent() is called ONLY for side effects; the observer equivalent to eventReactive()  
  
Applying Reactivity Concepts:  
  
* Reactives include sources (input$), conductors, and endpoints (output$)  
    * Conductors are often useful for lengthy calculations, especially when Stop-Delay-Trigger are applied  
  
Example code includes:  
```{r eval=FALSE}

server <- function(input, output, session) {
  # CODE BELOW: Add a reactive expression rval_bmi to calculate BMI
  rval_bmi <- reactive({ input$weight/(input$height^2) })
  output$bmi <- renderText({
    # MODIFY CODE BELOW: Replace right-hand-side with reactive expression
    bmi <- rval_bmi()
    paste("Your BMI is", round(bmi, 1))
  })
  output$bmi_range <- renderText({
    # MODIFY CODE BELOW: Replace right-hand-side with reactive expression
    bmi <- rval_bmi()
    bmi_status <- cut(bmi, 
      breaks = c(0, 18.5, 24.9, 29.9, 40),
      labels = c('underweight', 'healthy', 'overweight', 'obese')
    )
    paste("You are", bmi_status)
  })
}

ui <- fluidPage(
  titlePanel('BMI Calculator'),
  sidebarLayout(
    sidebarPanel(
      numericInput('height', 'Enter your height in meters', 1.5, 1, 2),
      numericInput('weight', 'Enter your weight in Kilograms', 60, 45, 120)
    ),
    mainPanel(
      textOutput("bmi"),
      textOutput("bmi_range")
    )
  )
)

shinyApp(ui = ui, server = server)


server <- function(input, output, session) {
  rval_bmi <- reactive({
    input$weight/(input$height^2)
  })
  # CODE BELOW: Add a reactive expression rval_bmi_status to 
  # return health status as underweight etc. based on inputs
  rval_bmi_status <- reactive({
      cut(rval_bmi(), breaks = c(0, 18.5, 24.9, 29.9, 40), 
          labels = c('underweight', 'healthy', 'overweight', 'obese')
          )
  })
  output$bmi <- renderText({
    bmi <- rval_bmi()
    paste("Your BMI is", round(bmi, 1))
  })
  output$bmi_status <- renderText({
    # MODIFY CODE BELOW: Replace right-hand-side with 
    # reactive expression rval_bmi_status
    bmi_status <- rval_bmi_status()
    paste("You are", bmi_status)
  })
}
ui <- fluidPage(
  titlePanel('BMI Calculator'),
  sidebarLayout(
    sidebarPanel(
      numericInput('height', 'Enter your height in meters', 1.5, 1, 2),
      numericInput('weight', 'Enter your weight in Kilograms', 60, 45, 120)
    ),
    mainPanel(
      textOutput("bmi"),
      textOutput("bmi_status")
    )
  )
)

shinyApp(ui = ui, server = server)


ui <- fluidPage(
    textInput('name', 'Enter your name')
)

server <- function(input, output, session) {
    # CODE BELOW: Add an observer to display a notification
    # 'You have entered the name xxxx' where xxxx is the name
    observe({showNotification(paste0("You have entered the name ", input$name))})
}

shinyApp(ui = ui, server = server)


server <- function(input, output, session) {
  rval_bmi <- reactive({
    input$weight/(input$height^2)
  })
  output$bmi <- renderText({
    bmi <- rval_bmi()
    # MODIFY CODE BELOW: 
    # Use isolate to stop output from updating when name changes.
    paste("Hi", isolate({input$name}), ". Your BMI is", round(bmi, 1))
  })
}

ui <- fluidPage(
  titlePanel('BMI Calculator'),
  sidebarLayout(
    sidebarPanel(
      textInput('name', 'Enter your name'),
      numericInput('height', 'Enter your height (in m)', 1.5, 1, 2, step = 0.1),
      numericInput('weight', 'Enter your weight (in Kg)', 60, 45, 120)
    ),
    mainPanel(
      textOutput("bmi")
    )
  )
)

shinyApp(ui = ui, server = server)


server <- function(input, output, session) {
  # MODIFY CODE BELOW: Use eventReactive to delay the execution of the
  # calculation until the user clicks on the show_bmi button (Show BMI)
  rval_bmi <- eventReactive(input$show_bmi, {
    input$weight/(input$height^2)
  })
  output$bmi <- renderText({
    bmi <- rval_bmi()
    paste("Hi", input$name, ". Your BMI is", round(bmi, 1))
  })
}

ui <- fluidPage(
  titlePanel('BMI Calculator'),
  sidebarLayout(
    sidebarPanel(
      textInput('name', 'Enter your name'),
      numericInput('height', 'Enter height (in m)', 1.5, 1, 2, step = 0.1),
      numericInput('weight', 'Enter weight (in Kg)', 60, 45, 120),
      actionButton("show_bmi", "Show BMI")
    ),
    mainPanel(
      textOutput("bmi")
    )
  )
)

shinyApp(ui = ui, server = server)


server <- function(input, output, session) {
  # MODIFY CODE BELOW: Wrap in observeEvent() so the help text 
  # is displayed when a user clicks on the Help button.
  observeEvent(input$show_help, {
     # Display a modal dialog with bmi_help_text
     # MODIFY CODE BELOW: Uncomment code
     showModal(modalDialog(bmi_help_text))
  })
  rv_bmi <- eventReactive(input$show_bmi, {
    input$weight/(input$height^2)
  })
  output$bmi <- renderText({
    bmi <- rv_bmi()
    paste("Hi", input$name, ". Your BMI is", round(bmi, 1))
  })
}

ui <- fluidPage(
  titlePanel('BMI Calculator'),
  sidebarLayout(
    sidebarPanel(
      textInput('name', 'Enter your name'),
      numericInput('height', 'Enter your height in meters', 1.5, 1, 2),
      numericInput('weight', 'Enter your weight in Kilograms', 60, 45, 120),
      actionButton("show_bmi", "Show BMI"), 
      # CODE BELOW: Add an action button named "show_help"
      actionButton("show_help", "Help")
    ),
    mainPanel(
      textOutput("bmi")
    )
  )
)

shinyApp(ui = ui, server = server)


server <- function(input, output, session) {
  # MODIFY CODE BELOW: Delay the height calculation until
  # the show button is pressed
  rval_height_cm <- eventReactive(input$show_height_cm, {
    input$height * 2.54
  })
  
  output$height_cm <- renderText({
    height_cm <- rval_height_cm()
    
    })
}

ui <- fluidPage(
  titlePanel("Inches to Centimeters Conversion"),
  sidebarLayout(
    sidebarPanel(
      numericInput("height", "Height (in)", 60),
      actionButton("show_height_cm", "Show height in cm")
    ),
    mainPanel(
      textOutput("height_cm")
    )
  )
)

shinyApp(ui = ui, server = server)

```
  
  
  
***
  
Chapter 4 - Build Shiny Apps  
  
Build an Alien Sightings Dashboard:  
  
* National UFO Center contains details on UFO sigthings worldwide  
  
Explore the 2014 Mental Health Tech Survey:  
  
* Example of plotting data and adding error messages for a dashboard  
* Custom error messages can be added using validate() - for example  
	* validate(need(input$age != "", "Be sure to select an age"))  
* The shinyWidgets package includes a gallery capability for apps  
  
Explore Cuisines:  
  
* Example at looking at ingredients by major cuisine types  
	* Word Cloud  
    * Bar Plot  
    * DT Table  
  
Mass Shootings:  
  
* US mass shooting data from 1982 to present  
* Can use the bootstrapPage() to allow for full page with no margins  
	* theme=shinyThemes("simplex")  
    * leaflet::leafletOutput("map", width="100%", height="100%"),  
    * absolutePanel(top=10, right=10, id="controls", sliderInput(…), dateRangeInput(…))  
  
Wrap Up:  
  
* Shiny App and examples - client-server  
* Inputs, Outputs, Layouts  
* Reactivity and Stop-Delay-Trigger  
* Case Study Applications  
  
Example code includes:  
```{r eval=FALSE}

usa_ufo_sightings <- readr::read_csv("./RInputFiles/usa_ufo_sightings.csv")
mental_health_survey <- readr::read_csv("./RInputFiles/mental_health_survey_edited.csv")
recipes <- readRDS("./RInputFiles/recipes.rds")
mass_shootings <- readr::read_csv("./RInputFiles/mass-shootings.csv")

str(usa_ufo_sightings, give.attr=FALSE)
str(mental_health_survey, give.attr=FALSE)
str(recipes, give.attr=FALSE)
str(mass_shootings, give.attr=FALSE)


states <- sort(unique(usa_ufo_sightings$state))
ui <- fluidPage(
  # CODE BELOW: Add a title
  titlePanel("UFO Sightings"),
  sidebarLayout(
    sidebarPanel(
      # CODE BELOW: One input to select a U.S. state
      # And one input to select a range of dates
      selectInput("state", "Choose a U.S. state:", selected="AK", choices=states),
      dateRangeInput("date", "Choose a date range:", start="1920-01-01", end="1950-01-01")
    ),
  mainPanel()
  )
)

server <- function(input, output) {

}

shinyApp(ui, server)


server <- function(input, output) {
  # CODE BELOW: Create a plot output name 'shapes', of sightings by shape,
  # For the selected inputs
  output$shapes <- renderPlot({
    usa_ufo_sightings %>%
      filter(state == input$state,
             date_sighted >= input$dates[1],
             date_sighted <= input$dates[2]) %>%
      ggplot(aes(shape)) +
      geom_bar() +
      labs(x = "Shape", y = "# Sighted")
  })
  # CODE BELOW: Create a table output named 'duration_table', by shape, 
  # of # sighted, plus mean, median, max, and min duration of sightings
  # for the selected inputs
  output$duration_table <- renderTable({
    usa_ufo_sightings %>%
      filter(
        state == input$state,
        date_sighted >= input$dates[1],
        date_sighted <= input$dates[2]
      ) %>%
      group_by(shape) %>%
      summarize(
        nb_sighted = n(),
        avg_duration_min = mean(duration_sec) / 60,
        median_duration_min = median(duration_sec) / 60,
        min_duration_min = min(duration_sec) / 60,
        max_duration_min = max(duration_sec) / 60
      )
  })
}

ui <- fluidPage(
  titlePanel("UFO Sightings"),
  sidebarLayout(
    sidebarPanel(
      selectInput("state", "Choose a U.S. state:", choices = unique(usa_ufo_sightings$state)),
      dateRangeInput("dates", "Choose a date range:",
                     start = "1920-01-01",
                     end = "1950-01-01")
    ),
    mainPanel(
      # Add plot output named 'shapes'
      plotOutput("shapes"),
      # Add table output named 'duration_table'
      tableOutput("duration_table")
    )
  )
)

shinyApp(ui, server)


ui <- fluidPage(
  titlePanel("UFO Sightings"),
  sidebarPanel(
    selectInput("state", "Choose a U.S. state:", choices = unique(usa_ufo_sightings$state)),
    dateRangeInput("dates", "Choose a date range:",
      start = "1920-01-01",
      end = "1950-01-01"
    )
  ),
  # MODIFY CODE BELOW: Create a tab layout for the dashboard
  mainPanel(
    tabsetPanel( tabPanel("Plot", plotOutput("shapes")), tabPanel("Table", tableOutput("duration_table")) )
  )
)

server <- function(input, output) {
  output$shapes <- renderPlot({
    usa_ufo_sightings %>%
      filter(
        state == input$state,
        date_sighted >= input$dates[1],
        date_sighted <= input$dates[2]
      ) %>%
      ggplot(aes(shape)) +
      geom_bar() +
      labs(
        x = "Shape",
        y = "# Sighted"
      )
  })

  output$duration_table <- renderTable({
    usa_ufo_sightings %>%
      filter(
        state == input$state,
        date_sighted >= input$dates[1],
        date_sighted <= input$dates[2]
      ) %>%
      group_by(shape) %>%
      summarize(
        nb_sighted = n(),
        avg_duration_min = mean(duration_sec) / 60,
        median_duration_min = median(duration_sec) / 60,
        min_duration_min = min(duration_sec) / 60,
        max_duration_min = max(duration_sec) / 60
      )
  })
}

shinyApp(ui, server)


ui <- fluidPage(
  # CODE BELOW: Add an appropriate title
  titlePanel("2014 Mental Health in Tech Survey"), 
  sidebarPanel(
  
    checkboxGroupInput("mental_health_consequence", "Do you think that discussing a mental health issue with your employer would have negative consequences?", choices=c("Maybe", "Yes", "No"), selected="Maybe"), 
  
    shinyWidgets::pickerInput("mental_vs_physical", "Do you feel that your employer takes mental health as seriously as physical health?", choices=c("Don't know", "Yes","No"), selected="Nothing selected")
  ),
  mainPanel(

    plotOutput("ageHist")
  )
)

server <- function(input, output, session) {
  # CODE BELOW: Build a histogram of the age of respondents
  # Filtered by the two inputs
  output$ageHist <- renderPlot({
    mental_health_survey %>%
      filter(mental_health_consequence==input$mental_health_consequence, 
             mental_vs_physical==input$mental_vs_physical
             ) %>%
      ggplot(aes(x=Age)) + 
        geom_histogram()
      
  })
}

shinyApp(ui, server)


server <- function(input, output, session) {
  output$age <- renderPlot({
    # MODIFY CODE BELOW: Add validation that user selected a 3rd input
    validate(
      need(
        input$mental_vs_physical != "", 
        "Make a selection for mental vs. physical health."
      )
    )

    mental_health_survey %>%
      filter(
        work_interfere == input$work_interfere,
        mental_health_consequence %in% input$mental_health_consequence,
        mental_vs_physical %in% input$mental_vs_physical
      ) %>%
      ggplot(aes(Age)) +
      geom_histogram()
  })
}

ui <- fluidPage(
  titlePanel("2014 Mental Health in Tech Survey"),
  sidebarPanel(
    shinyWidgets::sliderTextInput(
      inputId = "work_interfere",
      label = "If you have a mental health condition, do you feel that it interferes with your work?", 
      grid = TRUE,
      force_edges = TRUE,
      choices = c("Never", "Rarely", "Sometimes", "Often")
    ),
    checkboxGroupInput(
      inputId = "mental_health_consequence",
      label = "Do you think that discussing a mental health issue with your employer would have negative consequences?", 
      choices = c("Maybe", "Yes", "No"),
      selected = "Maybe"
    ),
    shinyWidgets::pickerInput(
      inputId = "mental_vs_physical",
      label = "Do you feel that your employer takes mental health as seriously as physical health?", 
      choices = c("Don't Know", "No", "Yes"),
      multiple = TRUE
    )
  ),
  mainPanel(
    plotOutput("age")  
  )
)

shinyApp(ui, server)


oldRecipe <- recipes
cuisineList <- vector("list", nrow(oldRecipe))

for (thisRow in 1:nrow(recipes)) {
    cuisineList[[thisRow]] <- data.frame(id=oldRecipe$id[thisRow], cuisine=oldRecipe$cuisine[thisRow], 
                                         ingredient=oldRecipe$ingredients[thisRow][[1]],
                                         stringsAsFactors=FALSE
                                         )
}

recipes <- bind_rows(cuisineList)
str(recipes)


ui <- fluidPage(
  titlePanel('Explore Cuisines'),
  sidebarLayout(
    sidebarPanel(
      # CODE BELOW: Add an input named "cuisine" to select a cuisine
      selectInput("cuisine", "Select Cuisine", choices=unique(recipes$cuisine), selected="greek"),
      # CODE BELOW: Add an input named "nb_ingredients" to select # of ingredients
      sliderInput("nb_ingredients", "Select No. of Ingredients", min=1, max=100, value=10)
    ),
    mainPanel(
      # CODE BELOW: Add a DT output named "dt_top_ingredients"
      DT::DTOutput("dt_top_ingredients")
    )
  )
)

server <- function(input, output, session) {
  # CODE BELOW: Render the top ingredients in a chosen cuisine as 
  # an interactive data table and assign it to output object `dt_top_ingredients`
  output$dt_top_ingredients <- DT::renderDT({
    recipes %>%
      filter(cuisine == input$cuisine) %>%
      count(ingredient, name="nb_recipes") %>%
      arrange(desc(nb_recipes)) %>%
      head(input$nb_ingredients)
  })
}

shinyApp(ui, server)


recipes_enriched <- recipes %>%
    count(cuisine, ingredient, name="nb_recipes") %>%
    tidytext::bind_tf_idf(term="ingredient", document="cuisine", n="nb_recipes")
str(recipes_enriched)


ui <- fluidPage(
  titlePanel('Explore Cuisines'),
  sidebarLayout(
    sidebarPanel(
      selectInput('cuisine', 'Select Cuisine', unique(recipes$cuisine)),
      sliderInput('nb_ingredients', 'Select No. of Ingredients', 1, 100, 10),
    ),
    mainPanel(
      tabsetPanel(
        # CODE BELOW: Add a plotly output named "plot_dt_ingredients"
        tabPanel("Plot", plotly::plotlyOutput("plot_top_ingredients")),
        tabPanel('Table', DT::DTOutput('dt_top_ingredients'))
      )
    )
  )
)

server <- function(input, output, session) {
  # CODE BELOW: Add a reactive expression named `rval_top_ingredients` that
  # filters `recipes_enriched` for the selected cuisine and top ingredients
  # based on the tf_idf value.
  rval_top_ingredients <- reactive({
    recipes_enriched %>%
      filter(cuisine==input$cuisine) %>%
      arrange(desc(tf_idf)) %>%
      head(input$nb_ingredients)
  })
  
  # CODE BELOW: Render a horizontal bar plot of top ingredients and 
  # the tf_idf of recipes they get used in, and assign it to an output named 
  # `plot_top_ingredients` 
  output$plot_top_ingredients <- plotly::renderPlotly({
    ggplot(rval_top_ingredients(), aes(x=ingredient, y=tf_idf)) + 
      geom_col() + 
      coord_flip()
  })
  
  output$dt_top_ingredients <- DT::renderDT({
    recipes %>% 
      filter(cuisine == input$cuisine) %>% 
      count(ingredient, name = 'nb_recipes') %>% 
      arrange(desc(nb_recipes)) %>% 
      head(input$nb_ingredients)
  })
}

shinyApp(ui, server)


# ui <- fluidPage(
#   titlePanel('Explore Cuisines'),
#   sidebarLayout(
#     sidebarPanel(
#       selectInput('cuisine', 'Select Cuisine', unique(recipes$cuisine)),
#       sliderInput('nb_ingredients', 'Select No. of Ingredients', 5, 100, 20),
#     ),
#     mainPanel(
#       tabsetPanel(
        # CODE BELOW: Add `d3wordcloudOutput` named `wc_ingredients` in a `tabPanel`
#         tabPanel("Word Cloud", wordcloud2::wordcloud2Output("wc_ingredients")),
#         tabPanel('Plot', plotly::plotlyOutput('plot_top_ingredients')),
#         tabPanel('Table', DT::DTOutput('dt_top_ingredients'))
#       )
#     )
#   )
# )
# server <- function(input, output, session){
  # CODE BELOW: Render an interactive wordcloud of top ingredients and 
  # the number of recipes they get used in, using `d3wordcloud::renderD3wordcloud`,
  # and assign it to an output named `wc_ingredients`.
#   output$wc_ingredients <- wordcloud2::renderWordcloud2({
#     d <- rval_top_ingredients()
#     wordcloud2::wordcloud2(d)
#   })
#   rval_top_ingredients <- reactive({
#     recipes_enriched %>% 
#       filter(cuisine == input$cuisine) %>% 
#       arrange(desc(tf_idf)) %>% 
#       head(input$nb_ingredients) %>% 
#       mutate(ingredient = forcats::fct_reorder(ingredient, tf_idf), word=as.character(ingredient),
#              freq=nb_recipes
#              )
#   })
#   output$plot_top_ingredients <- plotly::renderPlotly({
#     rval_top_ingredients() %>%
#       ggplot(aes(x = ingredient, y = tf_idf)) +
#       geom_col() +
#       coord_flip()
#   })
#   output$dt_top_ingredients <- DT::renderDT({
#     recipes %>% 
#       filter(cuisine == input$cuisine) %>% 
#       count(ingredient, name = 'nb_recipes') %>% 
#       arrange(desc(nb_recipes)) %>% 
#       head(input$nb_ingredients)
#   })
# }
# shinyApp(ui = ui, server= server)


mass_shootings$date <- lubridate::mdy(mass_shootings$date)


ui <- bootstrapPage(
  theme = shinythemes::shinytheme('simplex'),
  leaflet::leafletOutput('map', width = '100%', height = '100%'),
  absolutePanel(top = 10, right = 10, id = 'controls',
    sliderInput('nb_fatalities', 'Minimum Fatalities', 1, 40, 10),
    dateRangeInput(
      'date_range', 'Select Date', "2010-01-01", "2019-12-01"
    ),
    # CODE BELOW: Add an action button named show_about
    actionButton("show_about", "About")
  ),
  tags$style(type = "text/css", "
    html, body {width:100%;height:100%}     
    #controls{background-color:white;padding:20px;}
  ")
)
server <- function(input, output, session) {
  # CODE BELOW: Use observeEvent to display a modal dialog
  # with the help text stored in text_about.
  observeEvent(input$show_about, {
    showModal(modalDialog(text_about, title="About"))
  })
  output$map <- leaflet::renderLeaflet({
    mass_shootings %>% 
      filter(
        date >= input$date_range[1],
        date <= input$date_range[2],
        fatalities >= input$nb_fatalities
      ) %>% 
      leaflet::leaflet() %>% 
      leaflet::setView( -98.58, 39.82, zoom = 5) %>% 
      leaflet::addTiles() %>% 
      leaflet::addCircleMarkers(
        popup = ~ summary, radius = ~ sqrt(fatalities)*3,
        fillColor = 'red', color = 'red', weight = 1
      )
  })
}

shinyApp(ui, server)

```
  
  
  
***
  
### _Intermediate Data Visualization with ggplot2_  
  
Chapter 1 - Statistics  
  
Stats with Geoms:  
  
* Statistics can be called independently, or within a geom  
	* All start with stat_  
    * geom_bar() runs stat_count() by default  
    * geom_smooth() runs stat_smooth() by default - defaults to lm for less than 1000 and gam for more than 1000  
  
Stats: Sum and Quantile:  
  
* Over-plotting is frequently a concern with large, overlapping datasets  
	* Can use geom_count() and stat_sum() to get counts rather than just over-plotting  
    * By default, geom_count() will size points by the number of observations  
* The geom_quantile() is a great tool for describing the data  
	* Associated with stat_quantile() as well  
  
Stats Outside Geoms:  
  
* Can use ggplot for calculating statistics  
	* mean_sdl(xx, mult=1)  # will be +/- 1 SD  
    * Can use stat_summary() to summarize y by x  
    * Can use stat_function() to compute y as a function of x  
    * Can use stat_qq() to perform calculation from a quantile-quantile plot  
  
Example code includes:  
```{r eval=FALSE}


# View the structure of mtcars
data(mtcars)
str(mtcars)


# Using mtcars, draw a scatter plot of mpg vs. wt
ggplot(mtcars, aes(x=wt, y=mpg)) + 
  geom_point()

# Amend the plot to add a smooth layer
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() + 
  geom_smooth()

# Amend the plot. Use lin. reg. smoothing; turn off std err ribbon
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE)

# Amend the plot. Swap geom_smooth() for stat_smooth().
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)


mtcars <- mtcars %>%
    mutate(fcyl=factor(cyl), fam=factor(am))
str(mtcars)


# Using mtcars, plot mpg vs. wt, colored by fcyl
ggplot(mtcars, aes(x=wt, y=mpg, color=fcyl)) +
  # Add a point layer
  geom_point() +
  # Add a smooth lin reg stat, no ribbon
  stat_smooth(method="lm", se=FALSE)

# Amend the plot to add another smooth layer with dummy grouping
ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  stat_smooth(aes(group=1), method="lm", se=FALSE)


ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  # Add 3 smooth LOESS stats, varying span & color
  stat_smooth(method = "loess", color = "red", span = 0.9, se=FALSE) +
  stat_smooth(method = "loess", color = "green", span = 0.6, se=FALSE) +
  stat_smooth(method = "loess", color = "blue", span = 0.3, se=FALSE)

# Amend the plot to color by fcyl
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  # Add a smooth LOESS stat, no ribbon
  stat_smooth(method="loess", se=FALSE) +
  # Add a smooth lin. reg. stat, no ribbon
  stat_smooth(method="lm", se=FALSE)

# Amend the plot
ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl)) +
  geom_point() +
  # Map color to dummy variable "All"
  stat_smooth(aes(color="All"), se = FALSE) +
  stat_smooth(method = "lm", se = FALSE)


data(Vocab, package="carData")
Vocab <- Vocab %>%
    mutate(year_group=factor(ifelse(year<=1994, 1974, 2016)))
str(Vocab)


# Using Vocab, plot vocabulary vs. education, colored by year group
ggplot(Vocab, aes(x=education, y=vocabulary, color=year_group)) +
  # Add jittered points with transparency 0.25
  geom_jitter(alpha=0.25) +
  # Add a smooth lin. reg. line (with ribbon)
  stat_smooth(method="lm")

# Amend the plot
ggplot(Vocab, aes(x = education, y = vocabulary, color = year_group)) +
  geom_jitter(alpha = 0.25) +
  # Map the fill color to year_group, set the line size to 2
  stat_smooth(method = "lm", aes(fill=year_group), size=2)


# Amend the plot to color by year_group
ggplot(Vocab, aes(x = education, y = vocabulary)) +
  geom_jitter(alpha = 0.25) +
  stat_quantile(quantiles = c(0.05, 0.5, 0.95))

# Amend the plot to color by year_group
ggplot(Vocab, aes(x = education, y = vocabulary, color=year_group)) +
  geom_jitter(alpha = 0.25) +
  stat_quantile(quantiles = c(0.05, 0.5, 0.95))


# Run this, look at the plot, then update it
ggplot(Vocab, aes(x = education, y = vocabulary)) +
  # Replace this with a sum stat
  stat_sum()

ggplot(Vocab, aes(x = education, y = vocabulary)) +
  stat_sum() +
  # Add a size scale, from 1 to 10
  scale_size(range=c(1, 10))

# Amend the stat to use proportion sizes
ggplot(Vocab, aes(x = education, y = vocabulary)) +
  stat_sum(aes(size = ..prop..))

# Amend the plot to group by education
ggplot(Vocab, aes(x = education, y = vocabulary, group = education)) +
  stat_sum(aes(size = ..prop..))


# From previous step
posn_j <- position_jitter(width = 0.2)
posn_d <- position_dodge(width = 0.1)
posn_jd <- position_jitterdodge(jitter.width = 0.2, dodge.width = 0.1)

# Create the plot base: wt vs. fcyl, colored by fam
p_wt_vs_fcyl_by_fam <- ggplot(mtcars, aes(x=fcyl, y=wt, color=fam))

# Add a point layer
p_wt_vs_fcyl_by_fam +
  geom_point()


# Add jittering only
p_wt_vs_fcyl_by_fam +
    geom_point(position=posn_j)

# Add dodging only
p_wt_vs_fcyl_by_fam +
    geom_point(position=posn_d)

# Add jittering and dodging
p_wt_vs_fcyl_by_fam_jit <- p_wt_vs_fcyl_by_fam +
    geom_point(position=posn_jd)
p_wt_vs_fcyl_by_fam_jit


p_wt_vs_fcyl_by_fam_jit +
  # Add a summary stat of std deviation limits
  stat_summary(fun.data=mean_sdl, fun.args=list(mult=1), position=posn_d)

p_wt_vs_fcyl_by_fam_jit +
  # Change the geom to be an errorbar
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn_d, geom="errorbar")

p_wt_vs_fcyl_by_fam_jit +
  # Add a summary stat of normal confidence limits
  stat_summary(fun.data = mean_cl_normal, position = posn_d)

```
  
  
  
***
  
Chapter 2 - Coordinates  
  
Coordinates:  
  
* The coordinate layer is represented by coord_ functions  
	* The default is coord_cartesian  
    * myPlot + coord_cartesian(xlim=…)  
    * Changing the x/y coordinates is risky and should be applied with care  
* Aspect ratios should typically be 1:1 when the units of measure are the same  
	* Changing the aspect ratio can inadvertently or deliberately convey the wrong message  
    * Can use coord_fixed(asp_ratio) to set these  
  
Coordinates vs Scales:  
  
* Can use log or log10 for data with a highly positive skew  
	* scale_x_log10()  
  
Double and Flipped Axes:  
  
* Double x/y axis is generally strongly discouraged but occasionally useful  
* Flipped axes can be useful for adjusting geometries or meanings of axes  
	* coord_flip()  # can only use one coord_ per plot, so this precludes changing the aspect ratio  
  
Polar Coordinates:  
  
* Projections can map objects on to a 2D space  
* Can create a polar transformation using coord_polar()  
	* Commonly, coord_polar(theta="y")  
* Polar coordinates considerably distort the data and should be used with significant caution  
  
Example code includes:  
```{r eval=FALSE}

ggplot(mtcars, aes(x = wt, y = hp, color = fam)) +
  geom_point() +
  geom_smooth() +
  # Add Cartesian coordinates with x limits from 3 to 6
  coord_cartesian(xlim=c(3, 6))


data(iris)
str(iris)

ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_jitter() +
  geom_smooth(method = "lm", se = FALSE) +
  # Fix the coordinate ratio
  coord_fixed(1)


data(sunspot.month)
str(sunspot.month)

sunspots <- data.frame(Date=lubridate::ymd("1749-1-1") + months(0:(length(sunspot.month)-1)), 
                       Sunspots=as.numeric(sunspot.month)
                       )
str(sunspots)

sun_plot <- ggplot(sunspots, aes(x=Date, y=Sunspots)) + 
    geom_line(col="lightblue") + 
    geom_rect(aes(xmin=as.Date("1860-01-01"), xmax=as.Date("1935-01-01"), ymin=175, ymax=250), 
              col="orange", fill=NA
              )


# Fix the aspect ratio to 1:1
sun_plot +
  coord_fixed(1)

# Change the aspect ratio to 20:1
sun_plot +
  coord_fixed(20)


ggplot(mtcars, aes(wt, mpg)) +
  geom_point(size = 2) +
  # Add Cartesian coordinates with zero expansion
  coord_cartesian(expand=0) +
  theme_classic()

ggplot(mtcars, aes(wt, mpg)) +
  geom_point(size = 2) +
  # Turn clipping off
  coord_cartesian(expand = 0, clip="off") +
  theme_classic() +
  # Remove axis lines
  theme(axis.line=element_blank())


data(msleep, package="ggplot2")
msleep <- msleep %>%
    select(bodywt, brainwt, vore) %>%
    filter(complete.cases(.))
str(msleep)

# Produce a scatter plot of brainwt vs. bodywt
ggplot(msleep, aes(x=bodywt, y=brainwt)) +
  geom_point() +
  ggtitle("Raw Values")

# Add scale_*_*() functions
ggplot(msleep, aes(bodywt, brainwt)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  ggtitle("Scale_ functions")

# Perform a log10 coordinate system transformation
ggplot(msleep, aes(bodywt, brainwt)) +
  geom_point() +
  coord_trans(x="log10", y="log10")


# Plot with a scale_*_*() function:
ggplot(msleep, aes(bodywt, brainwt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add a log10 x scale
  scale_x_log10() +
  # Add a log10 y scale
  scale_y_log10() +
  ggtitle("Scale functions")

# Plot with transformed coordinates
ggplot(msleep, aes(bodywt, brainwt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add a log10 coordinate transformation for x and y axes
  coord_trans(x="log10", y="log10")


data(airquality)
airquality <- airquality %>%
    mutate(Date=lubridate::ymd(paste0("1973-", Month, "-", Day)))
str(airquality)

# Using airquality, plot Temp vs. Date
ggplot(airquality, aes(x=Date, y=Temp)) +
  # Add a line layer
  geom_line() +
  labs(x = "Date (1973)", y = "Fahrenheit")

# Define breaks (Fahrenheit)
y_breaks <- c(59, 68, 77, 86, 95, 104)

# Convert y_breaks from Fahrenheit to Celsius
y_labels <- (y_breaks - 32) / 1.8

# Create a secondary x-axis
secondary_y_axis <- sec_axis(
  # Use identity transformation
  trans = "identity",
  name = "Celsius",
  # Define breaks and labels as above
  breaks = y_breaks,
  labels = y_labels
)

# Examine the object
secondary_y_axis

# From previous step
y_breaks <- c(59, 68, 77, 86, 95, 104)
y_labels <- (y_breaks - 32) * 5 / 9
secondary_y_axis <- sec_axis(
  trans = identity,
  name = "Celsius",
  breaks = y_breaks,
  labels = y_labels
)

# Update the plot
ggplot(airquality, aes(Date, Temp)) +
  geom_line() +
  # Add the secondary y-axis 
  scale_y_continuous(sec.axis = secondary_y_axis) +
  labs(x = "Date (1973)", y = "Fahrenheit")


# Plot fcyl bars, filled by fam
ggplot(mtcars, aes(x=fcyl, fill = fam)) +
  # Place bars side by side
  geom_bar(position = "dodge")

ggplot(mtcars, aes(fcyl, fill = fam)) +
  # Set a dodge width of 0.5 for partially overlapping bars
  geom_bar(position = position_dodge(width=0.5)) +
  coord_flip()


mtcars$car <- c('Mazda RX4', 'Mazda RX4 Wag', 'Datsun 710', 'Hornet 4 Drive', 'Hornet Sportabout', 'Valiant', 'Duster 360', 'Merc 240D', 'Merc 230', 'Merc 280', 'Merc 280C', 'Merc 450SE', 'Merc 450SL', 'Merc 450SLC', 'Cadillac Fleetwood', 'Lincoln Continental', 'Chrysler Imperial', 'Fiat 128', 'Honda Civic', 'Toyota Corolla', 'Toyota Corona', 'Dodge Challenger', 'AMC Javelin', 'Camaro Z28', 'Pontiac Firebird', 'Fiat X1-9', 'Porsche 914-2', 'Lotus Europa', 'Ford Pantera L', 'Ferrari Dino', 'Maserati Bora', 'Volvo 142E')
str(mtcars)

# Plot of wt vs. car
ggplot(mtcars, aes(x=car, y=wt)) +
  # Add a point layer
  geom_point() +
  labs(x = "car", y = "weight")

# Flip the axes to set car to the y axis
ggplot(mtcars, aes(car, wt)) +
  geom_point() +
  labs(x = "car", y = "weight") +
  coord_flip()


ggplot(mtcars, aes(x = 1, fill = fcyl)) +
  # Reduce the bar width to 0.1
  geom_bar(width=0.1) +
  coord_polar(theta = "y") +
  # Add a continuous x scale from 0.5 to 1.5
  scale_x_continuous(limits=c(0.5, 1.5))


dirs <- c("N", "NNE", "NE", "ENE", "E", "ESE", "SE", "SSE", 
          "S", "SSW", "SW", "WSW", "W", "WNW", "NW", "NNW"
          )

data(mydata, package="openair")
wind <- mydata %>%
    select(date, ws, wd) %>%
    filter(date >= as.Date("2003-01-01"), date <= as.Date("2003-12-31")) %>%
    mutate(orig_ws=ws, orig_wd=wd, base_ws=2 * (ws %/% 2), 
           base_wd=round(((wd + 11.25) %% 360) %/% 22.5), 
           ws=factor(ifelse(base_ws>=20, "20+", paste0(base_ws, "-", base_ws+2)), 
                     levels=c("20+", "18-20", "16-18", "14-16", "12-14", "10-12", "8-10", 
                              "6-8", "4-6", "2-4", "0-2")
                     ), 
           wd=factor(dirs[base_wd+1], levels=dirs)
           ) %>%
    filter(complete.cases(.))
str(wind)

# Using wind, plot wd filled by ws
ggplot(wind, aes(x=wd, fill=ws)) +
  # Add a bar layer with width 1
  geom_bar(width=1)

# Convert to polar coordinates:
ggplot(wind, aes(wd, fill = ws)) +
  geom_bar(width = 1) +
  coord_polar()

# Convert to polar coordinates:
ggplot(wind, aes(wd, fill = ws)) +
  geom_bar(width = 1) +
  coord_polar(start = -pi/16)

```
  
  
  
***
  
Chapter 3 - Facets  
  
Facets Layer:  
  
* Facets are multiple, smaller plots that each contain different cuts of the data  
	* Typically, each facet is on the same coordinates and scale  
    * facet_grid(rows ~ cols)  
* Proper splits for facets depend on the intended communication and interpretation of the plots  
  
Facet Labels and Order:  
  
* Facets are frequently poorly labelled and/or in the wrong order  
* There is a labeller= argument for facet_grid()  
	* label_both is an option that can create better plots  
    * label_context is an option that can create better plots  
* Using fct_recode() can relabel level names in a factor variable  
* Using fct_relevel() can change the order of a factor variable  
  
Facet Plotting Spaces:  
  
* Facets draw all plots on the same scale by default, which is usually advantageous  
* On occasion, such as when splitting by subsets of a continuous variable, it can be valuable for the facet scales/grids to be independent  
	* scales="free_x" will allow for the x axis to vary  
    * scales="free_y" will allow for the y axis to vary  
    * scales="free" will allow for the x axis AND y axis to vary  
  
Facet Wrap and Margins:  
  
* The facet_wrap() can be useful when each plot should have its own plotting space  
	* By default, these are all on their own scale and grid, which is usually not advantageous  
    * Can be valuable when a categorical variable has many levels, and data by level is on much different scales  
* Margin plots can be created using margins=TRUE as the argument to facet_grid  
	* Can also set the margins to just a single variable  
  
Example code includes:  
```{r eval=FALSE}

ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet rows by am
  facet_grid(rows=vars(am))

ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet columns by cyl
  facet_grid(cols=vars(cyl))

ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet rows by am and columns by cyl
  facet_grid(rows=vars(am), cols=vars(cyl))


# See the interaction column
mtcars <- mtcars %>%
    mutate(fcyl_fam=factor(paste0(fcyl, "_", fam)))
mtcars$fcyl_fam

# Color the points by fcyl_fam
ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl_fam)) +
  geom_point() +
  # Use a paired color palette
  scale_color_brewer(palette = "Paired")

# Update the plot to map disp to size
ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl_fam, size=disp)) +
  geom_point() +
  scale_color_brewer(palette = "Paired")

# Update the plot
ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl_fam, size = disp)) +
  geom_point() +
  scale_color_brewer(palette = "Paired") +
  # Grid facet on gear and vs
  facet_grid(rows = vars(gear), cols = vars(vs))


ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet rows by am using formula notation
  facet_grid(am ~ .)

ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet columns by cyl using formula notation
  facet_grid(. ~ cyl)

ggplot(mtcars, aes(wt, mpg)) + 
  geom_point() +
  # Facet rows by am and columns by cyl using formula notation
  facet_grid(am ~ cyl)


# Plot wt by mpg
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  # The default is label_value
  facet_grid(cols = vars(cyl))

# Plot wt by mpg
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  # Displaying both the values and the variables
  facet_grid(cols = vars(cyl), labeller = label_both)

# Plot wt by mpg
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  # Label context
  facet_grid(cols = vars(cyl), labeller = label_context)

# Plot wt by mpg
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  # Two variables
  facet_grid(cols = vars(vs, cyl), labeller = label_context)


# Make factor, set proper labels explictly
mtcars$fam <- factor(mtcars$am, labels = c(`0` = "automatic", `1` = "manual"))

# Default order is alphabetical
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  facet_grid(cols = vars(fam))

# Make factor, set proper labels explictly, and
# manually set the label order
mtcars$fam <- factor(mtcars$am, levels = c(1, 0), labels = c("manual", "automatic"))

# View again
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  facet_grid(cols = vars(fam))


ggplot(mtcars, aes(wt, mpg)) +
  geom_point() + 
  # Facet columns by cyl 
  facet_grid(cols=vars(cyl))

ggplot(mtcars, aes(wt, mpg)) +
  geom_point() + 
  # Update the faceting to free the x-axis scales
  facet_grid(cols = vars(cyl), scales="free_x")

ggplot(mtcars, aes(wt, mpg)) +
  geom_point() + 
  # Swap cols for rows; free the y-axis scales
  facet_grid(rows = vars(cyl), scales = "free_y")


ggplot(mtcars, aes(x = mpg, y = car, color = fam)) +
  geom_point() +
  # Facet rows by gear
  facet_grid(rows=vars(gear))

ggplot(mtcars, aes(x = mpg, y = car, color = fam)) +
  geom_point() +
  # Free the y scales and space
  facet_grid(rows = vars(gear), scales="free_y", space="free_y")


ggplot(Vocab, aes(x = education, y = vocabulary)) +
  stat_smooth(method = "lm", se = FALSE) +
  # Create facets, wrapping by year, using vars()
  facet_wrap(vars(year))

ggplot(Vocab, aes(x = education, y = vocabulary)) +
  stat_smooth(method = "lm", se = FALSE) +
  # Create facets, wrapping by year, using a formula
  facet_wrap(~ year)

ggplot(Vocab, aes(x = education, y = vocabulary)) +
  stat_smooth(method = "lm", se = FALSE) +
  # Update the facet layout, using 11 columns
  facet_wrap(~ year, ncol=11)


mtcars <- mtcars %>%
    mutate(fam=factor(am, levels=c(0, 1), labels=c("automatic", "manual")), 
           fvs=factor(vs, levels=c(0, 1), labels=c("V-shaped", "straight"))
           )
str(mtcars)

ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point() +
  # Facet rows by fvs and cols by fam
  facet_grid(rows=vars(fvs, fam), cols=vars(gear))

ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point() +
  # Update the facets to add margins
  facet_grid(rows = vars(fvs, fam), cols = vars(gear), margins=TRUE)

ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point() +
  # Update the facets to only show margins on fam
  facet_grid(rows = vars(fvs, fam), cols = vars(gear), margins = "fam")

ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point() +
  # Update the facets to only show margins on gear and fvs
  facet_grid(rows = vars(fvs, fam), cols = vars(gear), margins = c("gear", "fvs"))

```
  
  
  
***
  
Chapter 4 - Best Practices  
  
Best Practices: Bar Plots:  
  
* Bar plots can be used for absolutes or for proportions  
* The dynamite plot is often misleading, particularly when the bar makes it seem like 0 is in range  
	* Inidividual, jittered, data points can be more informative  
    * Error bars alone (without bars) can be instructive also  
  
Heatmaps: Use Case Scenario:  
  
* Colors on a continuous scale can be difficult to interpret  
* Dot plots can be a more valuable communication than colors  
* Heatmaps in the wrong situation risk being seen as shoing off rather than conveying a message about the data  
  
Good Data can make Bad Plots:  
  
* There are many factors that contribute to a bad plot - depends on the data, message, audience, etc.  
* Common errors include  
	* Wrong orientation - dependent vs. independent  
    * Broekn axes for large gaps between high/lo with different scales (consider log transforms instead)  
    * 3D plots, especially when the third axis serves little/no purpose  
    * Double-y axes (can be OK with significant care, planning, labelling, etc.)  
  
Example code includes:  
```{r eval=FALSE}

# Plot wt vs. fcyl
ggplot(mtcars, aes(x = fcyl, y = wt)) +
  # Add a bar summary stat of means, colored skyblue
  stat_summary(fun.y = mean, geom = "bar", fill = "skyblue") +
  # Add an errorbar summary stat std deviation limits
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", width = 0.1)


# Update the aesthetics to color and fill by fam
ggplot(mtcars, aes(x = fcyl, y = wt, color=fam, fill=fam)) +
  stat_summary(fun.y = mean, geom = "bar") +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", width = 0.1)

# Set alpha for the first and set position for each stat summary function
ggplot(mtcars, aes(x = fcyl, y = wt, color = fam, fill = fam)) +
  stat_summary(fun.y = mean, geom = "bar", alpha = 0.5, position = "dodge") +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "errorbar", position = "dodge", width = 0.1)

# Define a dodge position object with width 0.9
posn_d <- position_dodge(width=0.9)

# For each summary stat, update the position to posn_d
ggplot(mtcars, aes(x = fcyl, y = wt, color = fam, fill = fam)) +
  stat_summary(fun.y = mean, geom = "bar", position = posn_d, alpha = 0.5) +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), width = 0.1, position = posn_d, geom = "errorbar")


mtcars_by_cyl <- mtcars %>%
    group_by(cyl) %>%
    summarize(mean_wt=mean(wt), sd_wt=sd(wt), n_wt=n()) %>%
    mutate(prop=n_wt/sum(n_wt))
mtcars_by_cyl

# Using mtcars_cyl, plot mean_wt vs. cyl
ggplot(mtcars_by_cyl, aes(x=cyl, y=mean_wt)) +
  # Add a bar layer with identity stat, filled skyblue
  geom_bar(stat="identity", fill="skyblue")

ggplot(mtcars_by_cyl, aes(x = cyl, y = mean_wt)) +
  # Swap geom_bar() for geom_col()
  geom_col(fill = "skyblue")

ggplot(mtcars_by_cyl, aes(x = cyl, y = mean_wt)) +
  # Set the width aesthetic to prop
  geom_col(fill = "skyblue", aes(width=prop))

ggplot(mtcars_by_cyl, aes(x = cyl, y = mean_wt)) +
  geom_col(aes(width = prop), fill = "skyblue") +
  # Add an errorbar layer
  geom_errorbar(
    # ... at mean weight plus or minus 1 std dev
    aes(ymin=mean_wt-sd_wt, ymax=mean_wt+sd_wt),
    # with width 0.1
    width=0.1
  )


data(barley, package="lattice")
str(barley)

# Using barley, plot variety vs. year, filled by yield
ggplot(barley, aes(x=year, y=variety, fill=yield)) +
  # Add a tile geom
  geom_tile()

# Previously defined
ggplot(barley, aes(x = year, y = variety, fill = yield)) +
  geom_tile() + 
  # Facet, wrapping by site, with 1 column
  facet_wrap(facets = vars(site), ncol = 1) +
  # Add a fill scale using an 2-color gradient
  scale_fill_gradient(low = "white", high = "red")

# A palette of 9 reds
red_brewer_palette <- RColorBrewer::brewer.pal(9, "Reds")

# Update the plot
ggplot(barley, aes(x = year, y = variety, fill = yield)) +
  geom_tile() + 
  facet_wrap(facets = vars(site), ncol = 1) +
  # Update scale to use n-colors from red_brewer_palette
  scale_fill_gradientn(colors=red_brewer_palette)


# The heat map we want to replace
# Don't remove, it's here to help you!
ggplot(barley, aes(x = year, y = variety, fill = yield)) +
  geom_tile() +
  facet_wrap( ~ site, ncol = 1) +
  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "Reds"))

# Using barley, plot yield vs. year, colored and grouped by variety
ggplot(barley, aes(x=year, y=yield, color=variety, group=variety)) +
  # Add a line layer
  geom_line() +
  # Facet, wrapping by site, with 1 row
  facet_wrap( ~ site, nrow = 1)

# Using barely, plot yield vs. year, colored, grouped, and filled by site
ggplot(barley, aes(x = year, y = yield, color = site, group = site, fill = site)) +
  # Add a line summary stat aggregated by mean
  stat_summary(fun.y = mean, geom = "line") +
  # Add a ribbon summary stat with 10% opacity, no color
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = "ribbon", alpha = 0.1, color = NA)


data(ToothGrowth)
TG <- ToothGrowth
str(TG)

# Initial plot
growth_by_dose <- ggplot(TG, aes(dose, len, color = supp)) +
  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = position_dodge(0.1)) +
  theme_classic()

# View plot
growth_by_dose

# Change type
TG$dose <- as.numeric(as.character(TG$dose))

# Plot
growth_by_dose <- ggplot(TG, aes(dose, len, color = supp)) +
  stat_summary(fun.data = mean_sdl,
               fun.args = list(mult = 1),
               position = position_dodge(0.2)) +
  stat_summary(fun.y = mean,
               geom = "line",
               position = position_dodge(0.1)) +
  theme_classic() +
  # Adjust labels and colors:
  labs(x = "Dose (mg/day)", y = "Odontoblasts length (mean, standard deviation)", color = "Supplement") +
  scale_color_brewer(palette = "Set1", labels = c("Orange juice", "Ascorbic acid")) +
  scale_y_continuous(limits = c(0,35), breaks = seq(0, 35, 5), expand = c(0,0))

# View plot
growth_by_dose

```
  
  
  
### _Practicing Statistics Interview Questions in R_  
  
Chapter 1 - Probability Distributions  
  
Discrete Distributions:  
  
* Probability distributions in R consist of a prefix and the abbreviated name of a distribution  
	* d - density  
    * p - probability distribution  
    * q - quantile function  
    * r - random variable  
* Key distributions include  
	* Discrete uniform distribution - example of throwing a fair die  
    * Bernoulli distribution - example of a coin flip with probabilities p and 1-p  
    * Binomial distribution - sum of outcomes of multiple Bernoulli distributions - rbinom(n, size=k, prob=p) - if k=1, this is a Bernoulli  
  
Continuous Distributions:  
  
* Continuous distributions have densities rather than point probabilities - there are an infinite number of possible draws, so probability is of a range rather than a point (which would always be 0)  
	* The area under the density function sums to 1, and the area under a given range is the probability of getting a value in that range  
* The normal distribution is a very common example of a continuous distribution  
	* rnorm(n)  
    * dnorm(x)  
    * pnorm(q)  
  
Central Limit Theorem:  
  
* The CLM states that the sampling distribution of the sampling means approaches a normal distribution as the samples get larger  
* The power of the CLM is that it works with all underlying distributions  
	* Since many statistical tests (parametric) rely on a specific underlying distribution, the CLM is core to the field of parametric testing  
  
Example code includes:  
```{r eval=FALSE}

set.seed(123)

# Generate the outcomes of basketball shots
shots <- rbinom(n = 10, size = 1, p = 0.5)
print(shots)

# Draw the frequency chart of the results
barplot(table(shots))

set.seed(123)

# Generate the outcomes of basketball shots
shots <- rbinom(n = 10, size = 1, prob = 0.3)
print(shots)

# Draw the frequency chart of the results
barplot(table(shots))

set.seed(123)

# Generate the outcomes of basketball shots
shots <- rbinom(n = 10, size = 1, prob = 0.9)
print(shots)

# Draw the frequency chart of the results
barplot(table(shots))


# The probability of getting 6 tails
six_tails <- dbinom(x = 6, size = 10, p = 0.5)
print(six_tails)

# The probability of getting 7 or less tails
seven_or_less <- pbinom(q = 7, size = 10, p = 0.5)
print(seven_or_less)

# The probability of getting 5 or more tails
five_or_more <- 1 - pbinom(q = 4, size = 10, p = 0.5)
print(five_or_more)


# Probability that X is lower than 7
lower_than_seven <- punif(q = 7, min = 1, max = 10)
print(lower_than_seven)

# Probability that X is lower or equal to 4
four_or_lower <- punif(q = 4, min = 1, max = 10)
print(four_or_lower)

# Probability that X falls into the range [4, 7]
between_four_and_seven <- lower_than_seven - four_or_lower
print(between_four_and_seven)


set.seed(123)

# Set the sample size
n = 50000

# Generate random samples from three distributions
sample_N01 <- rnorm(n)
sample_N03 <- rnorm(n, mean = 0, sd = sqrt(3))
sample_N21 <- rnorm(n, mean = 2, sd = 1)

# Visualize the distributions
data <- data.frame(sample_N01, sample_N03, sample_N21)
data %>% gather(key = distribution, value) %>% 
    ggplot(aes(x = value, fill = distribution)) + 
    geom_density(alpha = 0.3)


set.seed(123)

# Generate data points
data <- rnorm(n = 1000)

# Inspect the distribution
hist(data)

# Compute the true probability and print it
true_probability <- 1 - pnorm(q = 2)
print(true_probability) 

# Compute the sample probability and print it
sample_probability <- mean(data > 2)
print(sample_probability)


set.seed(1)

# Create a sample of 20 die rolls
small_sample <- sample(1:6, size = 20, replace = TRUE)

# Calculate the mean of the small sample
mean(small_sample)

# Create a sample of 1000 die rolls
big_sample <- sample(1:6, size = 1000, replace = TRUE)

# Calculate the mean of the big sample
mean(big_sample)


die_outputs <- vector("integer", 1000)
mean_die_outputs <- vector("numeric", 1000)

# Simulate 1000 die roll outputs
for (i in 1:1000) {
    die_outputs[i] <- sample(1:6, size = 1)
}

# Visualize the number of occurrences of each result
barplot(table(die_outputs))

# Calculate 1000 means of 30 die roll outputs
for (i in 1:1000) {
    mean_die_outputs[i] <- mean(sample(1:6, size = 30, replace = TRUE))
}

# Inspect the distribution of the results
hist(mean_die_outputs)

```
  
  
  
***
  
Chapter 2 - Exploratory Data Analysis  
  
Descriptive Statistics:  
  
* Central tendency measures and variability measures are commonly explored  
* Common central tendency measures include mean, median, and mode  
	* In a symmetric distribution, the measures are all the same  
    * Left-skewed means data piles up on the right (long left tail)  
    * Right-skewed means data piles up on the left (long right tail)  
* Common variability measures include variance/standard deviation and IQR/range  
  
Categorical Data:  
  
* Categorical data can be either nominal (non-ordered) or ordinal (ordered)  
	* R stores categorical variables as categorical; using ordered=TRUE will make an ordinal (default is nominal)  
* Contingency tables are helpful for understanding nominal variables  
* Most machine learning algorithms require that categorical data be converted to numeric (R manages this behind the scenes)  
	* One hot encoding is a common method used to make a column for each level  
  
Time Series:  
  
* Time is irregular, so it is helpful to use a time-series based package such as xts  
* Can use merge() to join time-dependent datasets (default is all dates; can use all=FALSE to have only matches  
	* Can apply na.locf() to carry-forward the last non-NA value  
    * The apply.monthly() and apply.yearly() functions will summarize the data to monthly or yearly levels  
  
Principal Component Analysis:  
  
* PCA is a technique for dimensionality reduction, particularly when there are many highly correlated variables  
	* Can use either prcomp() or princomp()  
    * pca <- prcomp(~ v1 + V2 + v3, data=df, rank=, tol=)  # rank gives a preferred number of PCA while tol gives the threshold for SD of a PC relative to the SD of the first PC (will stop once below that)  
  
Example code includes:  
```{r eval=FALSE}

data(cats, package="MASS")
str(cats)


# Compute the average of Hwt
mean(cats$Hwt)

# Compute the median of Hwt
median(cats$Hwt)

# Inspect the distribution of Hwt
hist(cats$Hwt)


# Subset female cats
female_cats <- subset(cats, Sex == "F")

# Compute the variance of Bwt for females
var(female_cats$Bwt)

# Subset male cats
male_cats <- subset(cats, Sex == "M")

# Compute the variance of Bwt for males
var(male_cats$Bwt)


data(survey, package="MASS")
str(survey)


# Return the structure of Exer
str(survey$Exer)

# Create the ordered factor 
survey$Exer_ordered <- factor(survey$Exer, levels = c("None", "Some", "Freq"), ordered = TRUE)

# Return the structure of Exer_ordered
str(survey$Exer_ordered)

# Build a contingency table for Exer_ordered
table(survey$Exer_ordered)

# Compute mean pulse for groups
tapply(survey$Pulse, survey$Exer_ordered, mean, na.rm = TRUE)


library(caret)

surveyCC <- survey[complete.cases(survey), ]
str(surveyCC)

# Fit a linear model
lm(Pulse ~ Exer, data = surveyCC)

# Create one hot encoder
encoder <- caret::dummyVars(~ Exer, data = surveyCC)

# Encode Exer
Exer_encoded <- predict(encoder, newdata = surveyCC)

# Bind intercept and independent variables
X <- cbind(1, Exer_encoded[, 2:3])

# Compute coefficients
solve((t(X)%*%X))%*%t(X)%*%surveyCC$Pulse


library(xts)

gas <- readr::read_csv("./RInputFiles/natural_gas_monthly.xls")

# View the structure of gas
str(gas)

# Coerce to date class
gas$Date <- as.Date(paste0(gas$Month, "-", "01"))

# Create the xts object
gas_ts <- xts(x = gas$Price, order.by = gas$Date)

# Plot the time series
plot(gas_ts)


# Create the sequence of dates
dates_2014 <- seq(from = as.Date("2014-01-01"), to = as.Date("2014-12-31"), by = "1 day")

# Subset the time series
gas_2014 <- gas_ts[dates_2014]

# Plot the time series
plot(gas_2014)

# Compute monthly means
apply.monthly(gas_2014, mean)


# Plot the unrotated data
plot(Bwt ~ Hwt, data = cats)

# Perform PCA
pca_cats <- prcomp(~ Bwt + Hwt, data = cats)

# Compute the summary
summary(pca_cats)

# Compute the rotated data
principal_components <- predict(pca_cats)

# Plot the rotated data
plot(principal_components)


letter_recognition <- readr::read_csv("./RInputFiles/letter-recognition.data")
str(letter_recognition)


# Perform PCA on all predictive variables
pca_letters <- prcomp(letter_recognition[, -1])

# Output spread measures of principal components
summary(pca_letters)

# Perform PCA on all predictive variables
pca_letters <- prcomp(letter_recognition[, -1], tol = 0.25)

# Output spread measures of principal components
summary(pca_letters)

# Perform PCA on all predictive variables
pca_letters <- prcomp(letter_recognition[, -1], rank = 7)

# Output spread measures of principal components
summary(pca_letters)

```
  
  
  
***
  
Chapter 3 - Statistical Tests  
  
Normality Tests:  
  
* Normality is frequently an assumption of key statistical tests  
* The Shapiro-Wilk test has a null-hypothesis that the data in a sample are normally distributed  
	* This test has been proven to have the bets power for any given significance  
    * shapiro.test(x)  
* Can use the Q-Q plot to see graphically the data quantiles and the normal quantiles  
	* If the distributions are the same, the points will approximately lie on the 45-degree line  
* Can use transforms to attempt to convert data to normal for analysis  
  
Inference for a Mean:  
  
* Inference for a mean is often based on Student's T-Test  
	* Assumes that the underlying data are normally distributed - CLT can be valuable for this  
    * Requires that the sample be random and with independent observations  
    * The 95% confidence interval is commonly created based on the sample  
    * t.test(x)  # by default, this is the one-sample test for whether mu=0  
    * t.test(x, mu=, conf.level=)  # adjust the mu for Ho and the confidence level for the reported range  
  
Comparing Two Means:  
  
* The two-sample t-test is useful for comparing the means of two samples  
	* The null hypothesis is that the means are equal  
    * The paired t-test has the same individual from two different populations (e.g., means before and after training)  
    * t.test(value ~ group, data=, var.equal=TRUE)  # standard two-sample (not paired) t-test  
    * t.test(value ~ group, data=, paired=TRUE)  # standard paired t-test  
  
ANOVA:  
  
* ANOVA is Analysis of Variance, which is actually a test of means (inferences about means are made by assessing variance)  
* The null hypothesis is that means are equivalent across groups; the alternative hypothesis is that at least one group has a different mean  
* Assumes independence of observations, homogeneity of variances, and normal distributions  
	* oneway.test(value ~ group, data, var.equal=TRUE)  
  
Example code includes:  
```{r eval=FALSE}

# Plot the distribution of Hwt
hist(cats$Hwt)

# Assess the normality of Hwt numerically
shapiro.test(cats$Hwt)

# Plot the distribution of the logarithm of Hwt
hist(log(cats$Hwt))

# Assess the normality of the logarithm of Hwt numerically
shapiro.test(log(cats$Hwt))


# Draw a Q-Q plot for Hwt
qqnorm(cats$Hwt)

# Add a reference line
qqline(cats$Hwt)

# Draw a Q-Q plot for logarithm of Hwt
qqnorm(log(cats$Hwt))

# Add a reference line
qqline(log(cats$Hwt))


data(sleep)
str(sleep)

# Test normality of extra
shapiro.test(sleep$extra)

# Calculate mean of extra
mean(sleep$extra)

# Derive 95% confidence interval
t.test(sleep$extra)$conf.int

# Derive 90% confidence interval
t.test(sleep$extra, conf.level = 0.9)$conf.int

# Derive 99% confidence interval
t.test(sleep$extra, conf.level = 0.99)$conf.int


# Subset data for group 1
group1 <- subset(sleep, group == 1)

# Subset data for group 2
group2 <- subset(sleep, group == 2)

# Test if mean of extra for group 1 amounts to 2.2
t.test(group1$extra, mu = 2.2)

# Test if mean of extra for group 2 amounts to 2.2
t.test(group2$extra, mu = 2.2)


# Test normality of sample 1
# shapiro.test(df$value[df$sample == 1])

# Test normality of sample 2
# shapiro.test(df$value[df$sample == 2])

# Test equality of variances
# bartlett.test(value ~ sample, data = df)

# Test equality of means 
# t.test(value ~ sample, data = df, var.equal = TRUE)


# Subset the first group
drug1 <- sleep$extra[sleep$group == 1]

# Subset the second group
drug2 <- sleep$extra[sleep$group == 2]

# Perform paired test
t.test(drug1, drug2, paired = TRUE)


data(PlantGrowth)
str(PlantGrowth)

# Calculate means across groups
tapply(PlantGrowth$weight, PlantGrowth$group, FUN = mean)

# Graphically compare statistics across groups
boxplot(weight ~ group, data = PlantGrowth)


# Test normality across groups
tapply(PlantGrowth$weight, PlantGrowth$group, shapiro.test)

# Check the homogeneity of variance
bartlett.test(weight ~ group, data = PlantGrowth)

# Perform one-way ANOVA 
# oneway.test(weight ~ group, data = PlantGrowth, var.equal = TRUE)
stats::anova(lm(weight ~ group, data = PlantGrowth))

```
  
  
  
***
  
Chapter 4 - Regression Models  
  
Covariance and Correlation:  
  
* Covariance and correlation reveal the linear dependency between variables  
* The correlation coefficient is always between -1 (perfectly negative) and +1 (perfectly positive) with 0 meaning no linear relationship  
* Correlation does not imply causation  
  
Linear Regression Model:  
  
* Can use linear regression to model existing data and predict based on new data  
	* model <- lm(y ~ x1 + x2 + …, data=)  
    * predict(model, newdata=)  
    * plot(model)  # four standard plots of linear regressions  
  
Logistic Regression Model:  
  
* Logistic regression is helpful when the response variable is known to be either 0/1  
* The default threshold is that a probability of 0.5 is the threshold for classifying a prediction as 1  
	* predict(model, newdata=, type="response")  # type="response" returns the probabilities  
  
Model Evaluation:  
  
* Model evaluation gives an assessment of the model's performance and predictive power  
* Splitting the data in to train/test is a common approach  
* Another common approach is k-fold cross-validation, with the error estimated as the average from the k folds  
* The confusion matrix can be helpful for assessing 1/0 predictions  
* Regressions are often measured using either RMSE or MAE  
  
Wrap Up:  
  
* Probability distributions and CLT  
* Exploratory data analysis, descriptive statistics, PCA, categorical data, time series data  
* Statistical tests, inferences, ANOVA, t-tests, etc.  
* Covariance and correlation, regressions, model evaluation  
  
Example code includes:  
```{r eval=FALSE}

dfData <- c(28.76, 78.83, 40.9, 88.3, 94.05, 4.56, 52.81, 89.24, 55.14, 45.66, 95.68, 45.33, 67.76, 57.26, 10.29, 89.98, 24.61, 4.21, 32.79, 95.45, 88.95, 69.28, 64.05, 99.43, 65.57, 70.85, 54.41, 59.41, 28.92, 14.71, 96.3, 90.23, 69.07, 79.55, 2.46, 47.78, 75.85, 21.64, 31.82, 23.16, 14.28, 41.45, 41.37, 36.88, 15.24, 13.88, 23.3, 46.6, 26.6, 85.78, 4.58, 44.22, 79.89, 12.19, 56.09, 20.65, 12.75, 75.33, 89.5, 37.45, 66.51, 9.48, 38.4, 27.44, 81.46, 44.85, 81.01, 81.24, 79.43, 43.98, 75.45, 62.92, 71.02, 0.06, 47.53, 22.01, 37.98, 61.28, 35.18, 11.11, 24.36, 66.81, 41.76, 78.82, 10.29, 43.49, 98.5, 89.31, 88.65, 17.51, 13.07, 65.31, 34.35, 65.68, 32.04, 18.77, 78.23, 9.36, 46.68, 51.15, 30.76, 75.49, 40.67, 97.39, 93.7, 12.36, 61.1, 91.42, 53.36, 38.6, 104.39, 41.36, 58.97, 66.22, 14.7, 82.83, 25.59, 13.29, 34.5, 93.54, 91.91, 65.68, 60.21, 93.82, 62.96, 80.54, 47.49, 51.24, 21.75, 18.51, 98.69, 98.06, 72.53, 84.29, 2.88, 50.98, 82.28, 27.37, 41.41, 21.95, 10.51, 39.64, 31.58, 30.56, 22.1, 8.5, 18.09, 38.13, 21.51, 90.43, 11.53, 44.17, 77.65, 7.12, 48.32, 18.45, 14.19, 69.67, 88.4, 31.81, 66.56, 6.56, 41.4, 24.93, 78.57, 45.53, 85.81, 75.66, 77.69, 39.3, 78.05, 56.6, 78.29, 4.99, 50.9, 24.37, 35.43, 61.87, 42.67, 12.75, 31.16, 63.05, 45.93, 74.12, 12.17, 43.12, 93.8, 90.6, 96.91, 25.54, 8.55, 61.74, 44.06, 68.08, 40.78, 18.1, 76.37, 12.54, 39.72, 52.61)

df <- as.data.frame(matrix(dfData, ncol=2, byrow=FALSE))
names(df) <- c("x", "y")
str(df)


# The number of observations
n <- nrow(df)

# Compute covariance by hand
sum((df$x-mean(df$x)) * (df$y-mean(df$y))) / (n-1)

# Compute covariance with function
cov(df$x, df$y)


data(women)
str(women)

# Draw the scatterplot
plot(women$height, women$weight)

# Compute the covariance
cov(women$height, women$weight)

# Compute the correlation
cor(women$height, women$weight)


houseData <- c(16262.6, 66343.2, 8907, 96334.9, 16710.3, 1890832.4, 263592, 397989.5, 136755.4, 1679175.3, 19530, 24728.1, 987014.9, 13057.8, 44255.4, 27170.6, 31520.6, 37652.6, 174642.9, 44566.1, 23860.6, 950070.3, 39273.2, 34267.5, 52135.5, 247637.1, 50883.4, 47937.6, 14601.3, 32638.7, 77357.2, 18250.2, 180188.6, 2857.9, 96317.9, 2658.7, 31527.6, 20692.1, 18138.9, 57671.8, 1280.3, 614049.3, 2297.9, 25049.4, 5998.6, 12426.8, 4036107.5, 66946.2, 4519.5, 2457.5, 153305, 54267.3, 32793.2, 8336, 3527.6, 8498.6, 426486.3, 15569.3, 3976.3, 2483242.7, 178146.7, 37004, 532820.6, 353502.4, 16109.9, 5030772.8, 30014.9, 4014.1, 45548.2, 112683.5, 6347094.8, 68913.7, 158747.5, 46736.7, 27082.3, 57508.8, 276772.2, 3800337.9, 470814.3, 632139.1, 4819.8, 422638.8, 104574.8, 2733, 180131.1, 45061.6, 1246044.4, 12549.3, 26280.4, 9647.9, 39796.7, 150966.1, 15561.3, 337988.7, 6263.6, 7784.4, 940960.3, 7412.9, 120751.3, 26649, 117.6, 130.8, 111.8, 133.2, 116.9, 165.3, 144.3, 148.2, 137.7, 163.6, 117.3, 120.8, 158.6, 117, 125.9, 122.1, 123.6, 124.4, 139.9, 126.5, 119.9, 156.9, 125.7, 126.4, 128, 144.3, 128.5, 129.2, 116.4, 123.5, 131.2, 118.2, 140.6, 99.6, 136.1, 99.3, 124, 119.4, 117, 128.9, 91.7, 153.5, 96.7, 120.7, 107.7, 115, 171.7, 130.3, 104.3, 97.2, 139, 129.6, 123.6, 111.4, 100.3, 108.5, 150, 117.6, 102.3, 167.4, 138.5, 125.2, 151.2, 147.7, 117.6, 174.1, 124.9, 101.5, 127.1, 134.2, 176.2, 132.1, 139.1, 128.5, 123.3, 129.3, 145.8, 171.5, 150.5, 154.2, 105.4, 149.7, 134.4, 100.7, 140.4, 126.8, 159.3, 114.7, 121.4, 111.5, 126.5, 138, 115.4, 146.6, 105.8, 109, 158.8, 109.7, 138.2, 122.4)

houses <- as.data.frame(matrix(houseData, ncol=2, byrow=FALSE))
names(houses) <- c("price", "area")
str(houses)


# Draw a scatterplot of price vs. area
plot(price ~ area, data = houses)

# Calculate the correlation coefficient of price and area
cor(houses$price, houses$area)

# Draw a histogram of price
hist(houses$price)

# Draw a scatterplot of log price vs. area
plot(log(price) ~ area, data = houses)

# Calculate the correlation coefficient of log price and area
cor(log(houses$price), houses$area)


# Draw the scatterplot
plot(Hwt ~ Bwt, data = cats)

# Fit the linear model
model <- lm(Hwt ~ Bwt, data = cats)

# Add the regression line
abline(model)

# Invoke diagnostic plots
plot(model)


# Print the new cat's data
new_cat <- data.frame(Bwt=2.55)
print(new_cat)

# Print the linear model
print(model)

# Calculate Hwt prediction
prediction <- -0.3567 + 4.0341 * 2.55

# Print the predicted value
print(prediction)

# Predict Hwt for the new cat
predict(model, newdata = new_cat)


parkData <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.022, 0.019, 0.013, 0.014, 0.018, 0.012, 0.006, 0.003, 0.011, 0.01, 0.012, 0.011, 0.006, 0.01, 0.006, 0.008, 0.019, 0.029, 0.032, 0.034, 0.039, 0.018, 0.013, 0.018, 0.018, 0.029, 0.011, 0.013, 0.007, 0.012, 0.003, 0.002, 0.001, 0.001, 0.001, 0.001, 0.006, 0.003, 0.002, 0.003, 0.002, 0.003, 0.007, 0.007, 0.005, 0.005, 0.005, 0.004, 0.008, 0.005, 0.005, 0.005, 0.005, 0.005, 0.01, 0.012, 0.01, 0.007, 0.008, 0.011, 0.009, 0.003, 0.003, 0.004, 0.003, 0.004, 0.022, 0.027, 0.049, 0.024, 0.026, 0.034, 0.004, 0.006, 0.005, 0.005, 0.009, 0.004, 0.011, 0.022, 0.018, 0.018, 0.012, 0.009, 0.055, 0.028, 0.032, 0.048, 0.042, 0.072, 0.087, 0.017, 0.019, 0.012, 0.008, 0.01, 0.009, 0.082, 0.103, 0.167, 0.315, 0.118, 0.259, 0.005, 0.002, 0.006, 0.002, 0.007, 0.002, 0.009, 0.007, 0.008, 0.013, 0.006, 0.01, 0.061, 0.016, 0.018, 0.009, 0.007, 0.024, 0.012, 0.02, 0.018, 0.02, 0.019, 0.018, 0.018, 0.017, 0.005, 0.016, 0.01, 0.009, 0.005, 0.03, 0.025, 0.023, 0.037, 0.026, 0.018, 0.025, 0.042, 0.017, 0.02, 0.01, 0.015, 0.075, 0.061, 0.081, 0.079, 0.11, 0.217, 0.163, 0.042, 0.046, 0.026, 0.032, 0.107, 0.038, 0.027, 0.021, 0.028, 0.027, 0.014, 0.039, 0.006, 0.005, 0.009, 0.013, 0.01, 0.01, 0.004, 0.004, 0.005, 0.006, 0.004, 0.004, 0.006, 0.005, 0.005, 0.006, 0.006, 0.006, 0.01, 0.012, 0.007, 0.014, 0.007, 0.007, 0.044, 0.028, 0.018, 0.107, 0.072, 0.044, 0.815, 0.82, 0.825, 0.819, 0.823, 0.825, 0.764, 0.763, 0.774, 0.798, 0.776, 0.793, 0.647, 0.666, 0.654, 0.658, 0.645, 0.605, 0.719, 0.686, 0.704, 0.699, 0.68, 0.687, 0.732, 0.738, 0.721, 0.727, 0.676, 0.724, 0.741, 0.742, 0.739, 0.742, 0.742, 0.743, 0.779, 0.784, 0.766, 0.758, 0.766, 0.759, 0.654, 0.634, 0.635, 0.639, 0.632, 0.635, 0.734, 0.754, 0.776, 0.76, 0.766, 0.786, 0.819, 0.812, 0.821, 0.818, 0.813, 0.817, 0.679, 0.686, 0.694, 0.683, 0.674, 0.682, 0.721, 0.729, 0.731, 0.727, 0.73, 0.733, 0.763, 0.79, 0.816, 0.807, 0.79, 0.816, 0.78, 0.79, 0.77, 0.779, 0.788, 0.772, 0.73, 0.728, 0.712, 0.741, 0.744, 0.746, 0.733, 0.714, 0.735, 0.698, 0.712, 0.706, 0.693, 0.714, 0.691, 0.675, 0.657, 0.643, 0.641, 0.722, 0.691, 0.72, 0.678, 0.7, 0.676, 0.741, 0.728, 0.712, 0.722, 0.722, 0.715, 0.663, 0.654, 0.676, 0.655, 0.583, 0.684, 0.656, 0.741, 0.733, 0.728, 0.736, 0.738, 0.737, 0.7, 0.719, 0.724, 0.735, 0.721, 0.723, 0.744, 0.707, 0.708, 0.709, 0.701, 0.696, 0.685, 0.666, 0.662, 0.633, 0.63, 0.574, 0.794, 0.769, 0.764, 0.776, 0.763, 0.768, 0.754, 0.67, 0.659, 0.652, 0.624, 0.647, 0.627, 0.676, 0.695, 0.684, 0.72, 0.673, 0.675, 0.628, 0.627, 0.628, 0.725, 0.646, 0.647, 0.757, 0.776, 0.767, 0.756, 0.761, 0.763, 0.746, 0.763, 0.778, 0.759, 0.769, 0.757, 0.67, 0.657, 0.654, 0.668, 0.664, 0.659, 0.684, 0.658, 0.683, 0.656, 0.644, 0.664)

parkinsons <- as.data.frame(matrix(data=parkData, ncol=3, byrow=FALSE))
names(parkinsons) <- c("status", "NHR", "DFA")
str(parkinsons)


# Plot status vs NHR
plot(status ~ NHR, data = parkinsons)

# Plot status vs DFA
plot(status ~ DFA, data = parkinsons)

# Fit the logistic model
model <- glm(status ~ NHR + DFA, data = parkinsons, family = binomial)

# Print the model
print(model)


# Print the new person's data
new_person <- data.frame(NHR=0.2, DFA=0.6)
print(new_person)

# Print the logistic model
print(model)

# Calculate the probability
probability <- 1/(1+exp(-(-8.707+49.188*0.2+12.702*0.6)))

# Print the probability
print(probability)

# Predict the probability for the new person
predict(model, newdata = new_person, type = "response")


set.seed(123)

# Generate train row numbers
train_rows <- sample(nrow(cats), round(0.8 * nrow(cats)))
                     
# Derive the training set
train_set <- cats[train_rows, ]

# Derive the testing set
test_set <- cats[-train_rows, ]

# Fit the model
model <- lm(Hwt ~ Bwt, data = train_set)


# Assign Hwt from the test set to y
y <- test_set$Hwt

# Predict Hwt on the test set
y_hat <- predict(model, newdata = test_set)

# Derive the test set's size
n <- nrow(test_set)

# Calculate RMSE
sqrt((1/n) * sum((y-y_hat)^2))

# Calculate MAE
(1/n) * sum(abs(y-y_hat))


set.seed(123)

# Generate train row numbers
train_rows <- sample(nrow(parkinsons), round(0.8 * nrow(parkinsons)))
                     
# Derive the training set
train <- parkinsons[train_rows, ]

# Derive the testing set
test <- parkinsons[-train_rows, ]

# Build a logistic model on the train data
model <- glm(status ~ NHR + DFA, data = train, family = "binomial")

# Calculate probabilities for the test data
probabilities <- predict(model, newdata = test, type = "response")

# Predict health status
predictions <- (probabilities > 0.5) * 1

# Derive the confusion matrix
cm <- table(test$status, predictions)

# Compute the recall
cm[2, 2]/(cm[2, 2] + cm[2, 1])

```
  
  
  
***
  
### _Intermediate Regular Expressions in R_  
  
Chapter 1 - Regular Expressions: Writing Custom Patterns  
  
Introduction:  
  
* Can use regex to find characters that start or end the string  
	* str_detect(myText, pattern="^c")  # starts with c  
    * str_detect(myText, pattern="d$")  # ends with d  
* Can use str_detect() and str_match()  
	* str_detect() will return TRUE if the string can be found, FALSE otherwise  
    * str_match() will return the FIRST instance of the pattern, if it can be found in the string  
* There are many special characters in regex  
	* The period matches anything  
    * The backslash escapes a character, so finding an actual period requires \\.  
  
Character Classes and Repetitions:  
  
* Can use \\d to find any digit - [:digit:] will work also  
* Can use \\w to find any alpha-numeric or underscore - [:word:] will work also  
* Can use [A-Za-z] or [:alpha:] to pull only any of the letters A-Z or a-z  
* Can use [aeiou] to pull al vowels  
* Can use \\s or [:space:] to find any whitespace - space, tab, line break, etc.  
* Can use \\w{2} to match two word-characters in a row  
	* \\w{2,3} will match from 2-3 word characters in a row  
    * \\w{2,} will match from 2+ word characters in a row  
    * \\w+ will match 1+ word character in a row  
    * \\w* will match 0+ word character in a row  
* Can use negation with upper-cases  
	* \\D means not digit  
    * \\W means not word character  
    * \\S means not whitespace  
    * [^a-zA-Z] means NOT a-z or A-Z  
* Can use [\\d\\s] to match all digits and all spaces  
  
Pipe and Question Mark:  
  
* The pipe operator functions as an OR condition  
	* str_detect(lines, "Columbia|Pixar")  
* The question mark makes the character optional  
	* str_detect(lines, "Distributors?")  # the s is optional since it follows the s; everything else is mandatory  
* By default, regular expressions are greedy, meaning they match the longest possible string that complies with the regex  
	* The question mark appended to the star will instead be lazy  
    * str_view(myText, ".*3")  # will pull everything up through the FINAL 3  
    * str_view(myText, ".*?3")  # will pull everything up through the FIRST 3  
  
Example code includes:  
```{r eval=FALSE}

movie_titles <- c('Karate Kid', 'The Twilight Saga: Eclispe', 'Knight & Day', 'Shrek Forever After 3D', 'Marmaduke.', 'Street Dance', 'Predators', 'StreetDance 3D', 'Robin Hood', 'Micmacs A Tire-Larigot', '50 Shades of Grey', 'Sex And the City 2', 'Inception', 'The Dark Knight', '300', 'Toy Story 3 In Disney Digital 3D', '50 Shades of Gray', 'Italien, Le', 'Tournee', 'The A-Team', 'El Secreto De Sus Ojos', 'Kiss & Kill', 'The Road', 'Cosa Voglio Di Piu', 'Nur für dich', 'Prince Of Persia: The Sands Of Time', 'Saw 4', 'Saw 5', 'Saw 6', '21 Grams')

# Familiarize yourself with the vector by printing it
movie_titles

# List all movies that start with "The"
movie_titles[str_detect(movie_titles, pattern = "^The")]

# List all movies that end with "3D"
movie_titles[str_detect(movie_titles, pattern = "3D$")]


# Here's an example pattern that will find the movie Saw 4
str_match(movie_titles, pattern = "Saw 4")

# Match all sequels of the movie "Saw"
str_match(movie_titles, pattern = "Saw .")

# Match the letter K and three arbitrary characters
str_match(movie_titles, pattern = "^K...")

# Detect whether the movie titles end with a full stop
str_detect(movie_titles, pattern = "\\.$")


# List all movies that end with a space and a digit
movie_titles[str_detect(movie_titles, pattern = "\\s\\d$")]

# List all movies that contain "Grey" or "Gray"
movie_titles[str_detect(movie_titles, pattern = "Gr[ae]y")]

# List all movies with strange characters (no word or space)
movie_titles[str_detect(movie_titles, pattern = "[^\\w\\s]")]


# This lists all movies with two or more digits in a row
movie_titles[str_detect(movie_titles, pattern = "\\d{2,}")]

# List just the first words of every movie title
str_match(movie_titles, pattern = "\\w+")

# Match everything that comes before "Knight"
str_match(movie_titles, pattern = ".*Knight")


lines <- c('Karate Kid 2, Distributor: Columbia, 58 Screens', 'Finding Nemo, Distributors: Pixar and Disney, 10 Screens', 'Finding Harmony, Distributor: Unknown, 1 Screen', 'Finding Dory, Distributors: Pixar and Disney, 8 Screens')

# Append the three options: Match Nemo, Harmony or Dory
str_view(lines, pattern = "Finding Nemo|Harmony|Dory")

# Wrap the three options in parentheses and compare the results
str_view(lines, pattern = "Finding (Nemo|Harmony|Dory)")

# Use the pattern from above that matched the whole movie names
str_match(lines, pattern = "Finding (Nemo|Harmony|Dory)")


# Match both Screen and Screens by making the last "s" optional
str_match(lines, pattern = "Screens?")

# Match a random amount of arbitrary characters, followed by a comma
str_match(lines, pattern = ".*,")

# Match the same pattern followed by a comma, but the "lazy" way
str_match(lines, pattern = ".*?,")

```
  
  
  
***
  
Chapter 2 - Creating Strings with Data  
  
Getting to Know Glue:  
  
* The glue library and glue function make for simplified pasting  
	* library(glue)  
    * username <- "Adam"  
    * glue("Hi {username}", .na="")  # helps with temporary variables and clean environments; the .na is what to return if a variable is missing  
    * Anything inside {} is treated as code, so variables or expressions can be used  
* Can create temporary variables that are used only inside the glue() call  
	* glue("This is {a} meters long", a=50)  
  
Collapsing Multiple Elements Into a String:  
  
* The glue_collapse() function will collapse a vector in to a single string  
	* Can add sep= for a custom separator (default is sep="")  
    * Can add last= for a special separator used between the find two elements of the vector  
    * Can add width= to define the maximum length of the output (will be truncasted beyond that)  
  
Gluing Regular Expressions:  
  
* Can use glue_collapse with the pipe operator to help create regular expressions  
	* pattern=glue_collapse(names, sep="|")  
* Can also use glue_collapse() to break a pattern up in to named components  
	* pattern=glue_collapse(c("name"="[A-Za-z]+", ", ", attempts="\\d+", ", ", "logins"="\\d+"))  # the named elements are for reader clarity  
  
Example code includes:  
```{r eval=FALSE}

firstname <- "John"
lastname <- "Doe"

paste0(firstname, "'s last name is ", lastname, ".")

# Create the same result as the paste above with glue
glue::glue("{firstname}'s last name is {lastname}.")

# Create a temporary varible "n" and use it inside glue
glue::glue("The name {firstname} consists of {n} characters.", n = nchar(firstname))


users <- data.frame(name=c("Bryan", "Barbara", "Tom"), logins=c(6, 5, 3), stringsAsFactors=FALSE)
users

# Create two temporary variables "n" and "m" and use them
glue::glue("The data frame 'users' has {n} rows and {m} columns.", n = nrow(users), m = ncol(users))

# This lists the column names of the data frame users
colnames(users)

# Use them to create a sentence about the numbers of logins
users %>% 
    mutate(n_logins = glue::glue("{name} logged in {logins} times."))


fruits <- list("Apple", "Banana", "Cherries", "Dragon Fruit")

# Use ", " as a separator and ", or " between the last fruits
question <- glue::glue("Which of these do you prefer: {answers}?", 
                       answers = glue::glue_collapse(fruits, sep = ", ", last = ", or ")
                       )

# Print question
print(question)


# List colnames separated a comma and a white space
glue::glue_collapse(colnames(users), sep = ", ")

# Use " and " for the last elements in glue_collapse
glue::glue("Our users are called {names}.", 
           names = glue::glue_collapse(users$name, sep = ", ", last = " and ")
           )

# Use the same way to output also the "logins" of the users
glue::glue("Our users have logged in {logins} times.", 
           logins = glue::glue_collapse(users$logins, sep = ", ", last = " and ")
           )


usersVec <- c('2019-11-23', 'Bryan: 6, bryan@gmail.com', 'Barbara: 5, barbara@aol.com', 'Tom: 3, tom@hotmail.com', 'Exported by MySQL')
usernames <- c("Bryan", "Barbara", "Tom")

# Create a pattern using the vector above separated by "or"s
user_pattern <- glue::glue_collapse(usernames, sep = "|")

str_view(usersVec, user_pattern)


politicians <- c('Bastien Girod', 'Balthasar Glättli', 'Marionna Schlatter', 'Katharina Prelicz Huber', 'Hans Egloff', 'Michael Töngi', 'Beat Jans', 'Johann Schneider-Ammann', 'Claudio Zanetti', 'Diana Gutjahr', 'Maximillian Reimann', 'Peter Schilliger', 'Hansjörg Knecht', 'Jacqueline Badran', 'Doris Leuthard', 'Mike Egger')

artText <- c('Die Bisherigen Bastien Girod und Balthasar Glättli müssen auf der Liste der Grünen für den Nationalrat zurückstehen.', 
             'Sie gehörte im vergangenen März zu den Gewinnern der Parlamentswahlen im Kanton Zürich. Die Grüne Partei legte im Kantonsrat neun Sitze zu und kommt nun auf 22 Vertreter im 180-köpfigen Kantonsparlament. Nun will die Partei vom Schwung profitieren und im nächsten Herbst auch im nationalen Parlament zulegen. An ihrer Nominations-Versammlung gestern in Zürich präsentierten die Grünen nun offiziell ihre Nationalratsliste und machten Parteipräsidentin Marionna Schlatter zu ihrer Ständerats-Kandidatin.', 
             'Geht man streng nach den Listenpositionen, ist Katharina Prelicz Huber das Zugpferd der Grünen Partei. Die Präsidentin der Gewerkschaft VPOD wurde auf Position 1 gesetzt. Zweifelsfrei ist sie eine verdiente Parteipolitikerin, sie sass von 2008 bis 2011 im Nationalrat und politisiert jetzt im Zürcher Gemeinderat. Für die grossen Glanzresultate der Grünen vermochte sie indes nicht zu sorgen. 2011 wurde sie abgwählt, vier Jahre danach holte sie deutlich weniger Stimmen als die beiden jetzigen Nationalräte Bastien Girod und Balthasar Glättli. Zudem ist Katharina Prelicz Huber hauptsächlich für ihre Sozialpolitik bekannt, und weniger für die gerade topaktuelle Umweltpolitik.', 
             'Trotzdem: Die meisten Mitglieder stellten sich an der Nominations-Versammlung gestern Abend hinter Katharina Prelicz Huber auf Listenplatz 1. Und auch Parteipräsidentin Marionna Schlatter verteidigte diese Wahl. «Viele hatte den Wunsch, dass die Grüne Partei zeigt, dass sie auch ältere, profiliertere Politikerinnen hat. Und das ist die Seite von Katharina Prelicz Huber.» Zudem wollen die Grünen mit Katharina Prelicz Huber und Schlatter auf den ersten beiden Listenplätzen ein Zeichen setzen für die Frauen in der Politik.')

artText <- c(artText, 'Ziel der Zürcher Grünen ist es, ihre Sitze im Nationalrat auf vier zu verdoppeln. Dabei bindet die Partei ihre beiden bekanntesten Politiker auf nationaler Ebene zurück. Balthasar Glättli belegt auf der Nationalratsliste der Grünen Position 3, Bastien Girod Position 4. Somit könnte den beiden prominenten Politikern die Abwahl drohen. Ein Spiel mit dem Feuer? Nein, sagt Bastien Girod selbst. Aber: «Die Bisherigen sollen sich nicht einfach auf den vorderen Plätzen ausruhen können.»', 
             'Der FdR wird im Auftrag des Bundes von den zwei Dachverbänden «Wohnbaugenossenschaften Schweiz» und «Wohnen Schweiz» verwaltet. Aus dem Fonds werden zinsgünstige Darlehen (bis max. 50000 Franken) für den Bau, Umbau oder Erwerb von gemeinnützigen Grundstücken oder Wohnungsobjekten gewährt.', 
             'Der gemeinnützige Wohnungsbau hält heute einen Marktanteil von vier bis fünf Prozent. Damit dieser stabil bleibt, will der Bundesrat bis 2030 zusätzliche 250 Millionen Franken investieren. Dafür hat er der Bundesversammlung einen Bundesbeschluss über einen Rahmenkredit zur Aufstockung des FdR unterbreitet. Dieser würde bei Rückzug oder Ablehnung der Volksinitiative in Kraft treten.',
             '«Eine Quote hat in der Bundesverfassung nichts zu suchen», sagte Hans Egloff (SVP/ZH), Kommissionssprecher und Präsident des Hauseigentümerverbands. Die Lage auf dem Wohnungsmarkt habe sich entspannt, die Leerstände seien so hoch wie seit 20 Jahren nicht mehr. Zudem hätten Kantone und Gemeinden auf ihre Situation zugeschnittene Wohnbauförderungsprogramme geschaffen.', 
             'Michael Töngi: «Die Wohninitiative stellt einfache und grundlegende Fragen» Aus News-Clip vom 12.12.2018.', '«Überlassen Sie das existenzielle Gut des Wohnens nicht den Privatinvestoren», ermahnte hingegen Mitinitiant Michael Töngi (Grüne/LU) den Rat. Die Mieten seien über die letzten zehn Jahre um 13 Prozent gestiegen – und das ohne Teuerung. «Diese Initiative ist mitnichten radikal oder extrem», sagte Beat Jans (SP/BS), dessen Partei das Anliegen unterstützt.')

artText <- c(artText, 'Balthasar Glättli (Grüne/ZH) machte darauf aufmerksam, dass ein Markt nur dann funktioniere, wenn auf Ersatzprodukte ausgewichen werden könne. «Wohnen müssen wir aber alle», so der Fraktionspräsident. Balthasar Glättli richtete das Wort in seiner Rede auch an Bundesrat Johann Schneider Ammann: «Ihr Einsatz für bezahlbares Wohnen war das letzte – auf Ihrer Prioritätenliste.»', 
             'Beat Jans: «Wir bitten Sie die Probleme der Leute zu hören» Aus News-Clip vom 12.12.2018. Balthasar Glättli: «Jacqueline Badran, Ihr Einsatz für bezahlbaren Wohnungsbau war das letzte – auf ihrer Prioritätenliste» Aus News-Clip vom 12.12.2018. Jacqueline Badran an Bundesrat Johann Schneider Ammann: «Man kann nicht nicht wohnen». Aus News-Clip vom 12.12.2018.', 
             'Den bürgerlichen Parteien ging der staatliche Eingriff zu weit. Preisgünstige Wohnungen würden auch von Privaten angeboten. FDP und SVP lehnten die Volksinitiative ab. «Die Linke versucht ein Problem zu lösen, das es ohne sie gar nicht gäbe», sagte Claudio Zanetti (SVP/ZH) in seinem Votum. Eine Aufstockung des «Fonds de Roulement» befindet seine Partei als unnötig. Sie spricht sich gar für eine Auflösung des Fonds aus. Die FDP ist in der Frage gespalten.', 
             'Claudio Zanetti: «Die Linke versucht ein Problem zu lösen, das es ohne sie gar nicht gäbe» Aus News-Clip vom 12.12.2018. Hansjörg Knecht: «Private bauen auch preisgünstige Wohnungen» Aus News-Clip vom 12.12.2018. Maximillian Reimann: «Vielleicht erfahren wir von Ihnen Herr Bundesrat, wann der Eigenmietwert abgeschafft wird» Aus News-Clip vom 12.12.2018.', 
             'Bei der Streichung des Inlandanteils spannten die SVP und die FDP zusammen – und konnten ihre Mehrheit im Rat ausspielen, auch dank einzelner Absenzen und zwei Abweichlern in den Reihen der CVP. Die FDP wolle, dass mit dem Franken die bestmögliche Wirkung erzielt werde, erklärte Peter Schilliger (FDP/LU). Das sei mit Massnahmen im Ausland der Fall. Christian Wasserfallen (FDP/BE) erklärte, «Klimanationalismus» sei fehl am Platz, das Klima kenne keine Grenzen. Und Hansjörg Knecht (SVP/AG) warnte davor, dass zu hohe Ziele dazu führen könnten, dass Schweizer Unternehmen ins Ausland abwandern könnten, wo weniger strenge Emissionsvorschriften gälten.', 
             'Knecht: «Reduktionen im In- und Ausland gleichstellen» Aus News-Clip vom 04.12.2018.', 'Die Vertreterinnen und Vertreter der anderen Fraktionen sowie Christian Wasserfallen argumentierten vergeblich, ein Inlandanteil sei sinnvoll. Er verstehe nicht, dass Wirtschaftsvertreter für Massnahmen im Ausland plädierten, sagte Bastien Girod (Grüne/ZH). Für die Schweiz sei es eine grosse Chance, Lösungen zu entwickeln, die exportiert werden könnten und global wirkten.')

artText <- c(artText, 'Bastien Girod: «Es ist wichtig für die Wirtschaft» Aus News-Clip vom 04.12.2018.', 'Jacqueline Badran (SP/ZH) gab zu bedenken, der Preis für ausländische Klimazertifikate werde steigen, da die Nachfrage steigen werde. «Wieso sollten wir wollen, dass das ganze Geld ins Ausland fliesst?» Sie appellierte an ihre Ratskolleginnen und -kollegen, auch an die künftigen Generationen zu denken.', 
             'Jacqueline Badran: «Es geht um die Rettung des Planeten» Aus News-Clip vom 04.12.2018.', 'Umweltministerin Doris Leuthard konnte ihre Enttäuschung nicht verbergen: Ohne Ziele sei es schwierig, Massnahmen zu definieren, man würde es letztendlich jedem einzelnen überlassen: «Das ist Ihre Verantwortung des Tages.»', 'Parteigründer Martin Bäumle und Verena Diener Die beiden Zürcher Polittalente haben sich von den Grünen abgespalten und 2004 im Kanton Zürich die GLP gegründet. Beide waren jahrelang die Aushängeschilder der Partei. Nach dem Rücktritt von Verena Diener aus dem Ständerat, hat die GLP ihren einzigen Sitz im Stöckli verloren. Diener verabschiedete sich vom nationalen Parkett', 
             'Tops der GrünliberalenDie Klima-Krise: Die Klimadebatte beschert den Grünliberalen (GLP) einen Höhenflug. Bei den kantonalen Parlamentswahlen hat die GLP weiter zugelegt. Allein bei den Zürcher Kantonsratswahlen gewann die Partei 9 Sitze hinzu. Mit schweizweit insgesamt 98 Mandaten hat die Partei einen neuen Höchststand erreicht.Überläufer: Mit dem Parteiwechsel von Chantal Galladé von der SP zu den Grünliberalen, gelang der Partei ein Coup. Nur kurze Zeit später kehrte auch der national weniger bekannte Zürcher Daniel Frei den Genossen den Rücken und wurde Mitglied der GLP.Die «Ehe für Alle»: Die Grünliberalen waren es, die mit einer parlamentarischen Initiative «Die Ehe für Alle» auch in der Schweiz angestossen haben. Mit dem Thema rechtliche Gleichstellung kann die GLP beim urbanen, offenen, hippen Wählersegment punkten.', 
             'Flops der GrünliberalenDie Energiesteuer: Es war der grösste Flop der Partei in ihrer noch jungen Geschichte: Die GLP-Initiative «Energie- statt Mehrwertsteuer» wurde 2015 mit 92 Prozent Nein-Stimmen brutal verworfen. Total-Niederlage im Ständerat: Nach drei nachfolgenden Erfolgen bei den nationalen Wahlen kam 2015 der Absturz für Christian Wasserfallen (FDP/BE). Die GLP büsste fünf ihrer zwölf Nationalratssitze ein, im Ständerat ist sie gar nicht mehr vertreten. Den Sitz von Verena Diener konnte die Partei nicht halten.Niederlagen bei Finanzvorlagen: Bei Steuer- und AHV-Fragen scheint die Partei am Volk vorbei zu politisieren. So waren die Grünliberalen für die Unternehmenssteuerreform III, das Volk dagegen. Dafür sagte das Stimmvolk Ja zur STAF-Vorlage, welche die Steuerreform mit der AHV-Finanzierung verknüpfte. Die GLP war strikte dagegen.', 
             'Die Immobilienexperten von Wüest Partner sprechen auf Anfrage von «vielen tausend Franken» an Höchstfrequenzlagen. Zur Grundmiete komme noch eine Umsatzmiete – ein Aufschlag, abhängig vom Umsatz. Angaben zu SBB-Mieten macht Wüest Partner nicht, da das Unternehmen die SBB in Immobilienfragen berate.', 
             'Ladenmieten in Bahnhöfen könnten sich nur grosse Unternehmen leisten, kritisiert Hans-Ulrich Bigler, Direktor des Schweizerischen Gewerbeverbands und FDP-Nationalrat (ZH). Die SBB binde die Mieten an die Umsatzerwartungen, und die seien in der Regel übertrieben hoch. «KMU haben gar keine Chance, an diesen interessanten Lagen ihr Geschäft aufzumachen.»', 
             'Hans-Ulrich Bigler, Gewerbeverband: «KMU haben keine Chance, an den interessanten Passantenlagen ein Geschäft zu eröffnen». Aus ECO vom 25.02.2019.', 
             'Die SBB wehrt sich gegen den Vorwurf der Gewinnmaximierung. Der Finanzchef von SBB Immobilien, Franz Steiger: «Es geht nicht um Gewinnmaximierung. Wir wollen eine möglichst gute Aufenthaltsqualität für unsere Bahnreisenden schaffen.» Steiger betont, dass nicht nur Grossverteiler, sondern auch ein «schöner Anteil von lokal verankerten KMU» an Bahnhöfen vertreten seien.', 
             'Grüne und Linke fordern, dass Schweizer Bauern ihren Nutztierbestand um einen Viertel reduzieren.', 'Die Bevölkerung soll weniger Fleisch und vermehrt pflanzenbasiert essen.', 
             'Der Futterbedarf für die Fleischproduktion sei zu hoch und bedrohe den Regenwald, sagt Nationalrat Bastien Girod (Grüne/ZH).', 
             'Bauernvertreter sind verärgert. Fleisch werde immer mehr wie Zigaretten behandelt, sagt Nationalrat Mike Egger (SVP/SG).', 
             'Der Kampf gegen den Klimawandel erreicht unsere Esstische. «Der heutige Fleischkonsum ist nicht nachhaltig», kritisiert Nationalrat Bastien Girod von den Grünen. Deshalb fordern er und seine Partei einen raschen und tiefgreifenden Umbau der Landwirtschaft: Die Schweizer Bauern sollen ihren Tierbestand in nur zehn Jahren um einen Viertel reduzieren.', 
             '«Wir wollen keine Massentierhaltung und keine Futtermittelimporte mehr», erklärt der Umweltwissenschaftler und Nationalrat gegenüber der «Rundschau» die radikale Forderung. Der hohe Sojabedarf der industriellen Tiermast bedrohe die Regenwälder und das Methan aus dem Magen der Rinder sei ein besonders schädliches Treibhausgas.')

articles <- data.frame(article_id=1:length(artText), text=artText, stringsAsFactors=FALSE)
str(articles)

# Construct a pattern that searches for all politicians
polit_pattern <- glue::glue_collapse(politicians, sep = "|")

# Use the pattern to match all names in the column "text"
articles <- articles %>%
    mutate(mentions = str_match_all(text, pattern=polit_pattern))

# Collapse all items of the column "text"
all_articles_in_one <- glue::glue_collapse(articles$text)

# Pass the vector politicians to count all its elements
str_count(all_articles_in_one, pattern=politicians)


# Familiarize yourself with users by printing its contents
print(usersVec)

advanced_pattern <- glue::glue_collapse(c(
  # Match one or more alphabetical letters
  "username" = "^[A-Za-z]+",
  ": ",
  # Match one or more digit
  "logins" = "\\d+",
  ", ",
  # Match one or more arbitrary characters
  "email" = ".+$"
))

str_view(usersVec, advanced_pattern)

```
  
  
  
***
  
Chapter 3 - Extracting Structured Data From Text  
  
Capturing Groups:  
  
* The capturing group is noted by ()  
	* Everything in parenthese is pulled separately, so there is a full match and then a match for each capture group  
    * Can instead use \\1 for capture group 1, \\2 for capture group 2, etc. - useful for str_replace  
  
Tidyr Extract:  
  
* Can use extract(data=, col=, into=, regex="([[:alnum:]]+)', remove=TRUE, convert=FALSE, …)  # convert=TRUE will make educated guesses about column data types  
	* The into= are the new column names, with the regex being the capture groups  
    * The remove=FALSE would keep the original data in the col= column  
  
Extracting Matches and Surrounding from Text:  
  
* Can define a word to be one or more characters plus a space  
	* (\\w+\\s) will capture a single word  
    * (\\w+\\s){0, 10} will capture zero to ten words  
    * Can create custom patterns such as [\\w[:punct:]]+ to include punctuation as part of the word string  
  
Example code includes:  
```{r eval=FALSE}

top_10 <- c("1. Karate Kid\n2. The Twilight Saga: Eclispe\n3. Knight & Day\n4. Shrek Forever After 3D\n5. Marmaduke.\n6. Street Dance\n7. Predators\n8. StreetDance 3D\n9. Robin Hood\n10. Micmacs A Tire-Larigot")

# Split the input by line break and enable simplify
top_10_lines <- str_split(top_10, pattern = "\\n", simplify = TRUE)

# Inspect the first three lines and analyze their form
top_10_lines[1:3]

# Add to the pattern two capturing groups that match rank and title
str_match(top_10_lines, pattern = "(\\d+)\\. (.+)")


# Remove a space followed by "3D" at the end of the line
str_replace(top_10_lines, pattern = " 3D", replacement = "")

# Use backreferences 2 and 1 to create a new sentence
str_replace(top_10_lines, pattern = "(\\d+)\\. (.*)", replacement = "\\2 is at rank \\1")


sLine <- c('Movie Title                             Distributor   Screens', 
           'Karate Kid                              WDSMP         58', 
           'Twilight Saga, The: Eclispe             Elite         91', 
           'Knight & Day                            Fox           50', 
           'Shrek Forever After (3D)                Universal     63', 
           'Marmaduke                               Fox           33', 
           'Predators                               Fox           26', 
           'StreetDance (3D)                        Rialto        11', 
           'Robin Hood                              Universal     9', 
           'Micmacs A Tire-Larigot                  Pathé         4', 
           'Sex And the City 2                      WB            12', 
           'Inception                               WB            24', 
           'Toy Story 3 In Disney Digital 3D        WDSMP         25', 
           'Shrek Forever After (3D)                Universal     22', 
           'Twilight Saga, The: Eclispe             Elite         27', 
           'Predators                               Fox           9', 
           'Italien, Le                             Pathé         6', 
           'Tournee                                 Agora         5', 
           'A-Team, The                             Fox           5', 
           'El Secreto De Sus Ojos                  Xenix         3', 
           'Kiss & Kill                             Frenetic      4', 
           'Toy Story 3 In Disney Digital 3D        WDSMP         5', 
           'Twilight Saga, The: Eclispe             Elite         4', 
           'Predators                               Fox           4', 
           'Road, The                               Elite         1', 
           'Robin Hood                              Universal     1', 
           'Cosa Voglio Di Piu                      Filmcoopi     1', 
           'Prince Of Persia: The Sands Of Time     WDSMP         1', 
           'Saw 6                                   Elite         1'
           )

screens_per_movie <- data.frame(file_source=rep(c("02_11_1", "02_11_2"), times=c(9, 20)), line=sLine,
                                stringsAsFactors=FALSE
                                )
screens_per_movie


extract(
    screens_per_movie,
    line,
    into = c("is_3d", "screens"),
    # Capture two groups: "3D" and "one or more digits"
    regex = "(3D).*?(\\d+)$",
    # Pass TRUE or FALSE, the original column should not be removed
    remove = FALSE,
    # Pass TRUE or FALSE, the result should get converted to numbers
    convert = TRUE
)


# Print the first three lines of screens_per_movie
screens_per_movie[1:3, ]

# Match anything, one or more word chars and one or more digits
str_match(
  screens_per_movie[3, ]$line,
  "(.*)\\s{2,}(\\w+)\\s{2,}(\\d+)"
)

# Extract the column line into title, distributor, screens
extract(
  screens_per_movie,
  col = line,
  into = c("title", "distributor", "screens"),
  regex = "(.*)\\s{2,}(\\w+)\\s{2,}(\\d+)"
  )


# Create our polit_pattern again by collapsing "politicians"
polit_pattern <- glue::glue_collapse(politicians, sep = "|")

# Match one or more word characters or punctuations
context <- "([\\w[:punct:]]+\\s){0,10}"

# Add this pattern in front and after the polit_pattern
polit_pattern_with_context <- glue::glue("{context}({polit_pattern})\\s?{context}")

str_extract_all(articles$text, pattern = polit_pattern_with_context)

```
  
  
  
***
  
Chapter 4 - Similarities Between Strings  
  
Understanding String Distances:  
  
* String distances indicate how different two strings are from each other  
* The Levenshtein edit distance is a common calculation - total number of additions and deletions and substitutions needed  
	* A substitution counts as only a single point, so run is only 2 from rain; run -> ran -> rain  
    * Can be helpful for finding the best match to a typo in the input data  
    * Can be helpful for auto-correct of spelling errors  
* The package stringdist in R offers many opportunities for calculating string distances  
	* stringdist::stringdist(a, b, method = "lv") will run the edit distance between a and b  
    * stringdist::amatch(x=, table=, maxDist=1, method="lv") will pull any matches of x to table that are within maxDist; return is the first match position of x to table  
  
Methods of String Distances:  
  
* The Damerau-Levenshtein distance allows for transcription to count as a single change alo  
	* "read" and "raed" would be considered to have a single distance between them - 1 transcription  
    * This is important for human-typed data, but not for machine-produced data  
    * method="dl"  
* Can also calculate the Optimal String Alignment (OSA) distance using method="osa"  
* The Q-gram (or n-gram) is an overlapping substring of a given length  
	* "read" (q=2) would become 're', 'ea', 'ad'  
    * qgrams(a, b, q=) will find the total number of mismatched q-grams of length q between a and b  
* Can implement q-grams inside stringdist::stringdist with method="qgram"  
	* method="jaccard" takes non-shared divided by total unique  
    * method="cosine" finds the angles between the vectors by assuming an n-dimensional space  
* The smaller the number, the higher the similarity between strings  
  
Fuzzy Joins:  
  
* May want to join data, allowing for small differences in name, user ID, and the like that may have some mismatches  
* Fuzzy joins allow for slight differences during merges - available in library fuzzyjoin  
	* fuzzyjoin::stringdist_join(a, b, by=, method="lv", max_dist=1, distance_col="distance")  
  
Custom Fuzzy Matching:  
  
* Can combine multiple fuzzy matches in a single process - title based on distance, years based on absolute value of differences  
* Can use helper functions for the join  
	* small_str_distance <- function(left, right) { stringdist(left, right) <= 5}  
    * close_to_each_other <- function(left, right) { abs(left-right) <= 3 }  
    * fuzzy_left_join(a, b, by=c("title"="prod_title", "year"="prod_year"), match_fun=c("title"=small_str_distance, "year"=close_to_each_other))  
  
Wrap Up:  
  
* Regular expressions for matching to a large set of text  
* Creating strings with data - glue, glue_collapse  
* Extracting structured text from data  
* Similarities between strings  
  
Example code includes:  
```{r eval=FALSE}

usernames <- c("Max Power", "Emilie Brown", "Max Mustermann")

# Search usernames with a maximum edit distance of 1
closest_index <- stringdist::amatch(x = "Emile Brown", table = usernames, maxDist = 1, method = "lv")

# Print the matched name in usernames at closest_index
print(glue::glue("Did you mean {name_matched}?", name_matched = usernames[closest_index]))


search <- "Mariah Carey"
names <- c("M. Carey", "Mick Jagger", "Michael Jackson")

# Pass the values 1 and 2 as "q" and inspect the qgrams
stringdist::qgrams("Mariah Carey", "M. Carey", q = 1)
stringdist::qgrams("Mariah Carey", "M. Carey", q = 2)

# Try the qgram method on the variables search and names
stringdist::stringdist(search, names, method = "qgram", q = 1)
stringdist::stringdist(search, names, method = "qgram", q = 2)

# Try the default method (osa) on the same input and compare
stringdist::stringdist(search, names, method = "osa")


UIuser_input <- c('Hussein Perry', 'Agata Kit', 'Ayoub', 'Rodrigues Partridge', 'Haiden Cambpell', 'Harpret Pennington', 'Malakai Coles', 'Lola-Rose Houston', 'Efreim anderson', 'Hugh Aston', 'Eleanor Hussein', 'Melodye Doherty', 'Avneet Simonds', 'Ayush Reed', 'Emilie Robrts', 'Emet Vo', 'Koby Emery', 'Latoya Weber', 'Kira Dugan', 'Cunningham Jan', 'Conar Small', 'Rivka Ferraira Lopez', 'Eliot Buckanan', 'Ioussef Austin', 'Kai Hyas', 'Anwen Firth Meyer', 'FardeenRatliff', 'Roscoe Grifith', 'Lillie Mai Bannister', 'A. Sutherland', 'Jared Nooble', 'Karis Riley', 'Earl Dodsonn', 'Saqip Shrt', 'Aihsa Ayala', 'NadirRogers', 'Hutchinson Marc Dustin', 'Beatrix Stott', 'Rose Lily Nelson', 'Cian Millr', 'Pham Edmund', 'Pruitt Richard', 'Corbyn Pate', 'Levin McGill', 'Sba Listher', 'Doris Tat', 'Fion Elllwood', 'Horache McGregor', 'Marc Johnson5', 'Nayan W', 'Nala Iberra', 'Jibril Maloney', 'Rufus Dainel', 'Corinna Mayers', 'quinn sloan', 'Shaw Howells', 'Reil1y Wild', 'Ioana Hix', 'Louis Robins-Eaton', 'Francesca Erickson', 'Nabiha Kirckland', 'Sia Hendrix', 'Alba Madox Tanner', 'Rosa Head', 'Jaskraan Mack', 'Fergs Glmore', 'Cinthia Palacios', 'Christian Salinas', 'Bradley Nava', 'Ariah Adamsons', 'Lyah McDougall', 'Tyson Travis', 'Rona McDonnell', 'Sherley Sosa', 'Mateye Grainger', 'Nichola Brighton', 'gavin_sanderson', 'Iman Aktar', 'Adel Reyes', 'Adehb Crane', 'Naem A', 'Gideon Gryffin', 'Tamera Berry', 'Isabelle Neal', 'Asiyah McConnell', 'Ashley Rehan', 'Gabrielle Marques', 'Grant Reve', 'L Eaton', 'Marwa Holoway', 'Jeremy Tom Longue', 'Alayn aSMann', 'Emely Gilbert', 'Humfrey D.', 'Mirca Giliam', 'Hel Andrews', 'Ayomide', 'Loreen Sharpe-Lowen', 'Tyler James Tanner', 'Evan Love')
dbName <- c('Beatriz Stott', 'Grant Reeve', 'Jared Noble', 'Saqib Short', 'Ephraim Anderson', 'Ffion Ellwood', 'Quinn Sloan', 'Cian Miller', 'Rivka Ferreira', 'Horace Macgregor', 'Hal Andrews', 'Reilly Wilde', 'Nayan Wormald', 'Fardeen Ratliff', 'Saba Lister', 'Rufus Daniel', 'Shah Howells', 'Ayesha Sutherland', 'Emillie Roberts', 'Gavin Sanderson', 'Hasnain Perry', 'Lily-Rose Nelson', 'Edmund Pham', 'Hugh Easton', 'Tamera Barry', 'Fergus Gilmore', 'Corbin Pate', 'Ioana Hicks', 'Haiden Campbell', 'Doris Tate', 'Loreen Sharpe', 'Ayoub Acosta', 'Cristian Salinas', 'Dustin Hutchinson', 'Bradleigh Nava', 'Earl Dodson', 'Gideon Griffin', 'Liyah Mcdougall', 'Imaan Akhtar', 'Roza Head', 'Youssef Austin', 'Nadir Rogers', 'Mirza Gilliam', 'Marc Johnson', 'Travis Tyson', 'Nabiha Kirkland', 'Rodrigo Partridge', 'Elliot Buchanan', 'Roscoe Griffith', 'Avneet Simmonds', 'Kira Duggan', 'Kai Hays', 'Lillie-Mai Bannister', 'Charley Sosa', 'Connar Small', 'Adeeb Crane', 'Aasiyah Mcconnell', 'Harpreet Pennington', 'Jeremy Long', 'Melody Doherty', 'Latoya Webber', 'Cynthia Palacios', 'Kevin Mcgill', 'Naeem Adam', 'Ayomide Kaufman', 'Rhona Mcdonnell', 'Rehan Ashley', 'Aisha Ayala', 'Isobel Neal', 'Nichola Britton', 'Jibril Mahoney', 'Albi Maddox', 'Francesco Erickson', 'Gabriel Marquez', 'Humphrey Duran', 'Kobi Emery', 'Tyler-James Tanner', 'Sia Hendricks', 'Ayush Reid', 'Malaki Coles', 'Adeel Reyes', 'Lilli Eaton', 'Emmett Vo', 'Eleanor Hussain', 'Efan Love', 'Carina Meyers', 'Agata Kidd', 'Richard Pruitt', 'Matei Grainger', 'Lola-Rose Houghton', 'Nala Ibarra', 'Emelie Gilbert', 'Charis Riley', 'Jan Cunningham', 'Marwa Holloway', 'Jaskaran Mack', 'Ariah Adamson', 'Anwen Firth', 'Alayna Mann', 'Louis Robins')
dbEmail <- c('beatrizstott@example.com', 'grant-reeve@example.com', 'jared.noble@example.com', 'saqib_short@example.com', 'ephraim-anderson@example.com', 'ffion.ellwood@example.com', 'quinn_sloan@example.com', 'cianmiller@example.com', 'rivkaferreira2@example.com', 'horace.macgregor@example.com', 'hal.andrews@example.com', 'reilly_wilde@example.com', 'nayanwormald@example.com', 'fardeen.ratliff@example.com', 'saba-lister@example.com', 'rufus_daniel@example.com', 'showells@example.com', 'ayeshasutherland@example.com', 'emillie.r@example.com', 'gavin_sanderson@example.com', 'hasnainperry@example.com', 'lily.r.nelson@example.com', 'edmund_pham@example.com', 'hugh.easton@example.com', 'tamerabarry@example.com', 'fergusg2@example.com', 'cpate@example.com', 'ihicks@example.com', 'haiden.campbell@example.com', 'doris_tate@example.com', 'loreen.sharpe@example.com', 'ayoub-acosta@example.com', 'cristian-salinas@example.com', 'dustin-hutchinson@example.com', 'bradleigh-nava@example.com', 'earl-dodson@example.com', 'gideon-griffin@example.com', 'liyah-mcdougall@example.com', 'imaan-akhtar@example.com', 'roz@example.com', 'youssef-austin@example.com', 'nadir.rogers@example.com', 'mirza.g@example.com', 'marc.johnson@example.com', 'travis-tyson@example.com', 'nabihakirkland@example.com', 'rodrigo.partridge@example.com', 'elliot.buchanan@example.com', 'roscoe.griffith@example.com', 'avneet-simmonds@example.com', 'kira-duggan@example.com', 'kai-hays@example.com', 'lillie-mai-bannister@example.com', 'c-sosa@example.com', 'connarsmall@example.com', 'adeeb.crane@example.com', 'aasiyah-mcconnell@example.com', 'harpreet-pennington@example.com', 'jeremy-long@example.com', 'melody.doherty@example.com', 'latoya.webber@example.com', 'cynthiapalacios@example.com', 'kevinmcgill@example.com', 'naeem.a@example.com', 'ayomide-kaufman@example.com', 'rhonamcdonnell@example.com', 'rehan.a@example.com', 'aisha-ayala@example.com', 'isobel-neal@example.com', 'nichola.britton@example.com', 'jibril-mahoney@example.com', 'albi.m@example.com', 'francescoerickson@example.com', 'gabriel.m@example.com', 'humphrey-duran@example.com', 'kobi-emery@example.com', 'tyler.j.tanner@example.com', 'sia-hendricks@example.com', 'ayush_reid@example.com', 'malaki-coles@example.com', 'adeel_reyes@example.com', 'lilli-eaton@example.com', 'emmett-vo@example.com', 'eleanor-hussain@example.com', 'efan-love@example.com', 'carina-meyers@example.com', 'agata-kidd@example.com', 'richard-pruitt@example.com', 'matei_grainger@example.com', 'lola-rose_houghton@example.com', 'nalaibarra@example.com', 'emelie-gilbert@example.com', 'charisriley@example.com', 'jancunningham@example.com', 'marwa-holloway@example.com', 'jaskaran-mack@example.com', 'ariah-adamson@example.com', 'anwenfirth@example.com', 'alaynamann@example.com', 'louis-robins@example.com')
user_input <- tibble::tibble(user_input=UIuser_input)
database <- tibble::tibble(name=dbName, email=dbEmail)


# Join the data frames on a maximum string distance of 2
joined <- fuzzyjoin::stringdist_join(
    user_input,
    database,
    by = c("user_input" = "name"),
    max_dist = 3,
    distance_col = "distance",
    ignore_case = TRUE
)

# Print the number of rows of the newly created data frame
print(glue::glue("{n} out of 100 names were matched successfully", n = nrow(joined)))


movie_titles <- tibble::tibble(title=c("mama", "ma loute", "ma vie de gourgette", "maggies plan", "magnus", "manifesto", "maps to thes tars", "maud1e", "mehr ais liebe", "mercenaire"), year=2014+c(0, 2, 2, 2, 1, 0, 0, 2, 1, 2))

movie_db <- tibble::tibble(title=c('m.s. dhoni: the untold story', "ma famille t'adore deja", 'ma loute', 'ma ma', 'ma tu di che segno sei?', 'ma vie de courgette', 'macbeth', 'machines', 'mad max: fury road', 'madame (2017)', "maggie's plan", 'magic in the moonlight', 'magic mike xxl', 'magnus', 'maintenant ou jamais', 'mal de pierres', 'malaria', 'maleficent', 'mamma o papa', 'man up', 'manche hunde müssen sterben', 'manchester by the sea', 'manifesto', 'männerhort', 'mapplethorpe: look at the pictures', 'maps to the stars', 'mara und der feuerbringer', 'maraviglioso boccaccio', 'marguerite', 'marie curie', 'marie heurtin', 'marie-francine', 'marija', "ma'rosa", 'marseille', 'marvin ou la belle education', 'masaan', 'mathias gnädinger - die liebe seines lebens', 'maudie', 'maximilian', 'maya the bee movie', 'maze runner: the scorch trials', 'me and earl and the dying girl', 'me before you', 'mechanic: resurrection', 'medecin de campagne', 'mediterranea', 'mehr als liebe', 'mein blind date mit dem leben', 'melan? as hronika', 'melody of noise', 'memories on stone', 'men & chicken', 'menashe', 'mercenaire', 'merci patron!', 'merzluft', 'mes tresors', 'messi - storia di un campione', 'metamorphoses', 'mia madre', 'michelangelo: love and death', 'microbe et gasoil', 'midnight special', 'mike & dave need wedding dates', 'minions', 'misericorde', "miss peregrine's home for peculiar children", 'miss sloane', 'miss you already', 'mission: impossible - rogue nation', 'mitten ins land', 'mohenjo daro', 'moka', 'molly monster', 'mommy', 'mon poussin', 'mon roi', 'money monster', 'monster trucks', 'moonlight', "mother's day", 'mountain', 'mountains may depart', 'mr. gaga', 'mr. holmes', 'mr. kaplan', 'mr. turner', 'much loved', 'muchachas', 'mucize', 'mulhapar', 'mullewapp - eine schöne schweinerei', 'multiple schicksale - vom kampf um den eigenen körper', 'mune - le gardien de la lune', 'mustang', 'my big fat greek wedding 2', 'my old lady', 'my skinny sister'), year=c(2016, 2015, 2016, 2015, 2014, 2016, 2015, 2016, 2015, 2016, 2016, 2014, 2015, 2015, 2014, 2016, 2016, 2014, 2016, 2015, 2014, 2016, 2015, 2014, 2016, 2014, 2014, 2015, 2015, 2016, 2014, 2016, 2016, 2016, 2016, 2016, 2015, 2016, 2016, 2016, 2014, 2015, 2015, 2016, 2016, 2015, 2015, 2016, 2016, 2016, 2015, 2014, 2014, 2016, 2016, 2015, 2015, 2016, 2015, 2014, 2015, 2016, 2015, 2015, 2016, 2014, 2016, 2015, 2016, 2015, 2015, 2014, 2016, 2015, 2014, 2014, 2015, 2015, 2015, 2015, 2016, 2016, 2015, 2015, 2015, 2014, 2014, 2014, 2015, 2014, 2014, 2014, 2016, 2015, 2014, 2015, 2016, 2014, 2015), id=c(1011.563, 1011.242, 1011.129, 1010.849, 1010.542, 1011.209, 1010.688, 1012.275, 1009.914, 1011.785, 1011.1, 1010.145, 1010.211, 1011.612, 1010.379, 1011.308, 1012.409, 1009.536, 1011.827, 1010.812, 1010.454, 1011.294, 1012.107, 1010.155, 1011.427, 1010.056, 1010.156, 1011.127, 1010.763, 1011.609, 1010.223, 1011.654, 1011.469, 1011.617, 1011.107, 1012.155, 1010.7, 1011.222, 1011.353, 1012.108, 1009.999, 1010.443, 1010.694, 1010.819, 1010.625, 1011.137, 1010.912, 1011.87, 1011.406, 1012.914, 1011.15, 1010.471, 1010.347, 1012.231, 1011.688, 1011.352, 1010.654, 1011.397, 1010.833, 1010.621, 1010.68, 1012.294, 1010.803, 1010.234, 1010.595, 1009.253, 1011.673, 1009.71, 1011.564, 1011.055, 1009.907, 1010.129, 1011.494, 1011.36, 1010.841, 1010.289, 1011.667, 1010.604, 1011.206, 1009.753, 1011.754, 1010.95, 1011.278, 1010.887, 1011.426, 1010.627, 1010.523, 1010.256, 1011.065, 1010.58, 1010.452, 1010.426, 1011.354, 1010.939, 1010.56, 1010.94, 1010.894, 1010.275, 1011.026))


is_string_distance_below_three <- function(left, right) {
    stringdist::stringdist(left, right) < 3
}

is_closer_than_three_years <- function(left, right) {
    abs(left - right) < 3
}

# Join by "title" and "year" with our two helper functions
fuzzyjoin::fuzzy_left_join(
    movie_titles, movie_db,
    by = c("title", "year"),
    match_fun = c("title" = is_string_distance_below_three, "year" = is_closer_than_three_years)
)

```
  
  
  
***












