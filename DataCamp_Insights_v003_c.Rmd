---
title: "Data Camp Insights"
author: "davegoblue"
date: "June 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)

```

## Background and Overview  
DataCamp offer interactive courses related to R Programming.  While some is review, it is helpful to see other perspectives on material.  As well, DataCamp has some interesting materials on packages that I want to learn better (ggplot2, dplyr, ggvis, etc.).  This document summarizes a few key insights from:  

This document is currently split between _v003 and _v003_a and _v003_b and _v003_c due to the need to keep the number of DLL that it opens below the hard-coded maximum.  This introductory section needs to be re-written, and the contents consolidated, at a future date.
  
* R Programming (Introductory R, Intermediate R, Writing Functions in R, Object Oriented Programming in R, Introduction to Tidyverse)  
* Importing and Cleaning Data (Cleaning Data in R, Importing Data in to R)  
* Data Manipulation (dplyr, data.table, xts/zoo, dplyr joins, cases for EDA/Time Series/Pitch Data)  
* Data Visualization (base, ggplot2 parts I/II/III, ggvis, geospatial)  
* Statistics (8 refresher modules)  
* Machine Learning (3 modules + 1 text mining case)  
* R Studio (2 parts) and R Markdown (1 module)  
  
The original DataCamp_Insights_v001 and DataCamp_Insights_v002 documents have been split for this document:  
  
* This DataCamp_Insights_v003 document contains evolving sections on R Programming, Machine Learning, and RStudio / R Markdown  
* Importing and Cleaning Data components have been moved to DataCamp_ImportClean_v002  
* Data Manipulation components have been moved to DataCamp_DataManipulation_v002  
* Visualization components have been moved to DataCamp_Visualization_v002  
* Statistics components have been moved to DataCamp_Statistics_v002  
  
  
***
  

###_Hierarchical and Mixed Effects Models_  
  
Chapter 1 - Overview and Introduction  
  
What is a hierarchical model?  
  
* Hierarchical data is nested within itself, and can be analyzed using the lme package  
	* Example of students in a classroom - may not all be independent of each other due to teacher quality, building conditions, etc.  
    * Hierarchical models can help with pooling means across small sample sizes  
    * Repeated measurements (test scores each year) are also a common example of data that are not truly independent  
* Hierarchical models can include nested models and multi-level models  
* Regression frameworks can include pool information and random effects (vs. fixed effects) and mixed-effects and linear mixed-effects  
* Repeated sampling can have repeated measures modeling  
  
Parts of a regression:  
  
* Linear regression and linear model can be used interchangeably for this course - epsilon is the error term, assumed to be normal with zero mean and constant variance  
* The linear model in R is closely related to analysis of variance (ANOVA)  
	* lm (y ~ x, myData)  
    * anova( lm (y ~ x, myData) )  
* The most basic regression has an intercept, a slope, a single predictor, and an error term  
	* The concept can be extended to multiple regression with additional predictors  
* There are some limitations to the multiple regression approach  
	* Parameter estimates can be very sensitive to other variables - Simpson's paradox and the like  
    * Need to note that the regression coefficient is "after controlling for . . . " (all the other variables)  
    * Interaction terms can be important as well  
* Regressions in R for an intercept for every group are called as lm(y ~ x - 1)  
* The interaction term x1*x2 is the same as x1 + x2 + x1:x2  
  
Random effects in regression:  
  
* Nested relationships tend to be hierarchical in nature - students are part of classes are part of schools and the like  
	* Mathematically, this is referred to as a mapping among the distributions  
* The algebraic representation is that y ~ B*x + eps, with B ~ N(mu, sigma**2)  
	* library(lme4) is the best packages for this in R  
    * lme4::lmer(y ~ x + (1|randomGroup), data=myData)  
    * lme4::lmer(y ~ x + (randomSlope|randomGroup), data=myData)  
  
School data:  
  
* Appliciation of multi-level models to school data - influence of sex, teacher training, plotting parameter estmates  
  
Example code includes:  
```{r}

rawStudent <- read.csv("./RInputFiles/classroom.csv")

studentData <- rawStudent %>%
    mutate(sex=factor(sex, labels=c("male", "female")), minority=factor(minority, labels=c("no", "yes")))


# Plot the data
ggplot(data = studentData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Fit a linear model
summary( lm(mathgain ~ housepov , data = studentData))


# I have aggregated the data for you into two new datasets at the classroom- and school-levels (As a side note, if you want to learn how to aggregate data, the dplyr or data.table courses teach these skills)
# We will also compare the model outputs across all three outputs
# Note: how we aggregate the data is important
# I aggregated the data by taking the mean across the student data (in pseudo-code: mean(mathgain) by school or mean(mathgain) by classroom), 
# but another reasonable method for aggregating the data would be to aggregate by classroom first and school second

classData <- studentData %>%
    group_by(schoolid, classid) %>%
    summarize_at(vars(mathgain, mathprep, housepov, yearstea), mean, na.rm=TRUE)
str(classData)

schoolData <- studentData %>%
    group_by(schoolid) %>%
    summarize_at(vars(mathgain, mathprep, housepov, yearstea), mean, na.rm=TRUE)
str(schoolData)


# First, plot the hosepov and mathgain at the classroom-level from the classData data.frame
ggplot(data = classData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Second, plot the hosepov and mathgain at the school-level from the schoolData data.frame
ggplot(data = schoolData, aes(x = housepov, y = mathgain)) +
    geom_point() +
    geom_smooth(method = 'lm')

# Third, compare your liner regression results from the previous expercise to the two new models
summary( lm(mathgain ~ housepov, data = studentData)) ## student-level data
summary( lm(mathgain ~ housepov, data = classData)) ## class-level data
summary( lm(mathgain ~ housepov, data = schoolData)) ## school-level data


# Plot the means of your data, predictor is your x-variable, response is your y-variable, and intDemo is your data.frame
intDemo <- data.frame(predictor=factor(c(rep("a", 5), rep("b", 5), rep("c", 5))), 
                      response=c(-1.207, 0.277, 1.084, -2.346, 0.429, 5.759, 4.138, 4.18, 4.153, 3.665, 9.046, 8.003, 8.447, 10.129, 11.919)
                      )
str(intDemo)


ggIntDemo <- ggplot(intDemo, aes(x = predictor, y = response) ) +
    geom_point() +
    theme_minimal() + stat_summary(fun.y = "mean", color = "red",
                                   size = 3, geom = "point") +
    xlab("Intercept groups")
print(ggIntDemo)

# Fit a linear model to your data where response is "predicted by"(~) predictor
intModel <- lm( response ~ predictor - 1 , data = intDemo)
summary(intModel)


extractAndPlotResults <- function(intModel){
    intCoefPlot <- broom::tidy(intModel)
    intCoefPlot$term <- factor(gsub("predictor", "", intCoefPlot$term))

    plotOut <- ggIntDemo + geom_point(data = intCoefPlot,
                           aes(x = term, y = estimate),
                           position = position_dodge(width = 0.4),
                           color = 'blue', size = 8, alpha = 0.25)
    print(plotOut)
}


# Run the next code that extracts out the model's coeffiecents and plots them 
extractAndPlotResults(intModel)


multIntDemo <- data.frame(group=factor(c(rep("a", 5), rep("b", 5), rep("c", 5))), 
                          x=rep(0:4, times=3), 
                          intercept=c(4.11, -1.69, 1.09, 1.9, 1.21, 4.63, 10.29, 4.67, 12.06, 4.78, 15.22, 19.15, 4.44, 8.88, 9.47), 
                          response=c(4.11, 2.31, 9.09, 13.9, 17.21, 4.63, 14.29, 12.67, 24.06, 20.78, 15.22, 23.15, 12.44, 20.88, 25.47)
                          )
str(multIntDemo)

plot_output1 <- function(out1){
    ggmultIntgDemo <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', fill = NA, color = 'orange', size = 3)
    print(ggmultIntgDemo)
}

plot_output2 <- function(out2){
    out2Tidy <- broom::tidy(out2)
    out2Tidy$term <- gsub("group", "", out2Tidy$term)
    out2Plot <- data.frame(group = pull(out2Tidy[ -1, 1]),
                           slope = pull(out2Tidy[ 1, 2]),
                           intercept = pull(out2Tidy[ -1, 2])
                           )
    ggmultIntgDemo2 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group))+
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        geom_abline(data = out2Plot,
                    aes(intercept = intercept, slope = slope, color = group))
    print(ggmultIntgDemo2)
}

plot_output3 <- function(out3){
    ggmultIntgDemo3 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', aes(color = group), fill = NA)
    print(ggmultIntgDemo3)
}

# First, run a model without considering different intercept for each group
out1 <- lm( response ~ x, data=multIntDemo )
summary(out1)
plot_output1(out1)

# Considering same slope but different intercepts
out2 <- lm( response ~ x + group - 1, data=multIntDemo )
summary(out2)
plot_output2(out2)

# Consdering different slope and intercept for each group (i.e., an interaction)
out3 <- lm( response ~ x + group - 1 + x:group, multIntDemo)
summary(out3)
plot_output3(out3)


multIntDemo$intercept <- c(-0.87, 3.35, 1.25, 0.88, -1.05, 4.55, 1.22, 3.34, 1.26, 3.75, 7.71, 9.59, 2.28, 1.9, 13.35)
multIntDemo$response <- c(-0.87, 6.35, 7.25, 9.88, 10.95, 4.55, 4.22, 9.34, 10.26, 15.75, 7.71, 12.59, 8.28, 10.9, 25.35)

# Run model
outLmer <- lme4::lmer( response ~ x + ( 1 | group), multIntDemo)

# Look at model outputs 
summary( outLmer )
broom::tidy( outLmer )


extractAndPlotOutput <- function(outLmer, slope=3){
    multIntDemo$lmerPredict <- predict(outLmer)
    ggmultIntgDemo2 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group))+
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        geom_abline(data = multIntDemo,
                    aes(intercept = intercept, slope = slope, color = group))
    outPlot <-  ggmultIntgDemo2 +
                geom_line( data =  multIntDemo,
                      aes(x = x, y = lmerPredict, color = group),
                      linetype = 2)
    print(outPlot)
}


# Extract predictor variables and plot
extractAndPlotOutput(outLmer)


# Random effect slopes
multIntDemo$response <- c(-0.72, 1.5, 4.81, 6.61, 13.62, 10.21, 9.64, 11.91, 16.39, 16.97, 8.76, 14.79, 15.83, 15.27, 17.36)
multIntDemo$intercept <- c(-0.72, -1.5, -1.19, -2.39, 1.62, 10.21, 6.64, 5.91, 7.39, 4.97, 8.76, 11.79, 9.83, 6.27, 5.36)

outLmer2 <- lme4::lmer( response ~ ( x|group ), multIntDemo)
summary(outLmer2)
broom::tidy(outLmer2)


plotOutput <- function(outLmer2){
    multIntDemo$lmerPredict2 <- predict(outLmer2)
    ggmultIntgDemo3 <- ggplot( multIntDemo, aes(x = x, y = response) ) +
        geom_point(aes(color = group)) +
        theme_minimal() +
        scale_color_manual(values = c("blue", "black", "red")) +
        stat_smooth(method = 'lm', aes(color = group), fill = NA)
    plotOut <- ggmultIntgDemo3 +
            geom_line( data =  multIntDemo,
                      aes(x = x, y = lmerPredict2, color = group),
                      linetype = 2)
    print(plotOut)
}


# Extract and plot
plotOutput(outLmer2)


# Mixed effect model
lmerModel <- lme4::lmer(mathgain ~ sex + 
                  mathprep + mathknow + (1|classid) +
                  (1|schoolid), data = studentData, na.action = "na.omit",
                  REML = TRUE)
summary(lmerModel)


extractAndPlot <- function(lmerModel){
    modelOutPlot <- broom::tidy(lmerModel, conf.int = TRUE)
    modelOutPlot <- modelOutPlot[ modelOutPlot$group =="fixed" &
                               modelOutPlot$term != "(Intercept)", ]
    plotOut <- ggplot(modelOutPlot, aes(x = term, y = estimate,
                             ymin = conf.low,
                             ymax = conf.high)) +
            theme_minimal() +
            geom_hline(yintercept = 0.0, color = 'red', size = 2.0) +
            geom_point() +
            geom_linerange() + coord_flip()
    print(plotOut)
}


# Extract and plot 
extractAndPlot(lmerModel)

```
  
  
  
***
  
Chapter 2 - Linear Mixed-Effect Models  
  
Linear mixed effect model - Birth rates data:  
  
* Small populations are highly sensitive to stochastic effects - if the mean is 1, a group of 5 might have 0 or 10  
* Questions about how counties may impact birth rates, over and above other demographic factors  
	* Example of plotting birth rate vs. county - will see both the highest and lowest birth rates in the smallest counties  
* Random effect syntax for the lmer model includes  
	* (1 | group) - random intercept with fixed mean  
    * (1 | g1/g2) - intercepts vary among g1 and g2 within g2  
    * (1 | g1) + (1 | g2) - random intercepts for two variables  
    * x + (x | g) - correlated random slope and intercept  
    * x + (x || g) - uncorrelated random slope and intercept  
    * See lme4 documentation for additional details  
  
Understanding and reporting the outputs of lmer:  
  
* The output from lmer is similar to the output from lm, but with some key differences - if using print(), will see  
	* The method used is REML - restricted maximum likelihood - which tends to solve better than maximum likelihood for these problems  
    * There is an REML convergence criteria, which can be a helpful diagnostic  
    * Can see the standard deviations for both the state and the residual, along with the number of observations  
    * Get the fixed effects coefficients in a similar form as lm()  
* The summary() call on lmer produces several additional outputs  
	* Residuals summary  
    * Fixed effects estimates include SE and t-values (but not p-values)  
    * Correlations of fixed effects  
* Can grab only the fixed effects using fixef(myLMERObject)  
	* Can grab only the random effects using ranef(myLMERObject), though these will not have confidence intervals  
    * The random effects confidence intervals could be estimated using bootstrapping or Bayesian methods per the author of lme4 - but actual random effects are just unobserved random variables rather than parameters  
* Can grab only the confidence intervals using confint(myLMERObject)  
* Need to be careful in reporting the results - figures vs. tables vs. in-line descriptions  
  
Statistical inference with Maryland crime data:  
  
* The Maryland crime data is available on data.gov - interesting for many public and private purposes  
* The null hypothsis test can be used with LMER - frequentist approach  
	* By default, lmer does not provide p-values, as there is ongoing debate as to the degrees of freedom and impact on reported results  
    * Can use lmerTest package to calculate and report on the p-values  
* Can use ANOVA to look at the variability explained by one model versus another model, and the associated degrees of freedom needed  
  
Example code includes:  
```{r}

# Read in births data
rawBirths <- read.csv("./RInputFiles/countyBirthsDataUse.csv")
countyBirthsData <- rawBirths
str(countyBirthsData)


# First, build a lmer with state as a random effect. Then look at the model's summary and the plot of residuals. 
birthRateStateModel <- lme4::lmer(BirthRate ~ (1|State), data=countyBirthsData)
summary(birthRateStateModel)
plot(birthRateStateModel)

# Next, plot the predicted values from the model ontop of the plot shown during the video.
countyBirthsData$birthPredictState <- predict(birthRateStateModel, countyBirthsData)
ggplot() + theme_minimal() +
    geom_point(data =countyBirthsData, aes(x = TotalPopulation, y = BirthRate)) + 
    geom_point(data = countyBirthsData, aes(x = TotalPopulation, y = birthPredictState),
               color = 'blue', alpha = 0.5
               )

# Include the AverageAgeofMother as a fixed effect within the lmer and state as a random effect
ageMotherModel <- lme4::lmer( BirthRate ~ AverageAgeofMother + (1|State), data=countyBirthsData)
summary(ageMotherModel)

# Compare the random-effect model to the linear effect model 
summary(lm(BirthRate ~ AverageAgeofMother, data = countyBirthsData))


# Include the AverageAgeofMother as a correlated random-effect slope parameter
ageMotherModelRandomCorrelated <- lme4::lmer(BirthRate ~ AverageAgeofMother + (AverageAgeofMother|State),
                       countyBirthsData)
summary(ageMotherModelRandomCorrelated)


# Include the AverageAgeofMother as a correlated random-effect slope parameter
ageMotherModelRandomUncorrelated <- lme4::lmer(BirthRate ~ AverageAgeofMother + 
                                                    (AverageAgeofMother || State), data=countyBirthsData
                                               )
summary(ageMotherModelRandomUncorrelated)


out <- ageMotherModelRandomUncorrelated

# Extract the fixed-effect coefficients
lme4::fixef(out)

# Extract the random-effect coefficients
lme4::ranef(out)

# Estimate the confidence intervals 
(ciOut <- confint(out))


# Technical note: Extracting out the regression coefficients from lmer is tricky (see discussion between the lmer and broom authors development)
# Extract out the parameter estimates and confidence intervals and manipulate the data
dataPlot <- data.frame(cbind( lme4::fixef(out), ciOut[ 4:5, ]))
rownames(dataPlot)[1] <- "Intercept"
colnames(dataPlot) <- c("mean", "l95", "u95")
dataPlot$parameter <- rownames(dataPlot)

# Print the new dataframe
print(dataPlot)

# Plot the results using ggplot2
ggplot(dataPlot, aes(x = parameter, y = mean,
                     ymin = l95, ymax = u95)) +
    geom_hline( yintercept = 0, color = 'red' ) +
    geom_linerange() + geom_point() + coord_flip() + theme_minimal()



# Read in crime data
rawCrime <- read.csv("./RInputFiles/MDCrime.csv")
MDCrime <- rawCrime
str(MDCrime)


plot1 <- ggplot(data = MDCrime, aes(x = Year, y = Crime, group = County)) +
    geom_line() + theme_minimal() +
    ylab("Major crimes reported per county")
print(plot1)

plot1 + geom_smooth(method = 'lm')


# Null hypothesis testing uses p-values to see if a variable is "significant"
# Recently, the abuse and overuse of null hypothesis testing and p-values has caused the American Statistical Association to issue a statement about the use of p-values
# Because of these criticisms and other numerical challenges, Doug Bates (the creator of the lme4 package) does not include p-values as part of his package
# However, you may still want to estimate p-values, because p-values are sill commonly used. Several packages exist, including the lmerTest package
# https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf

# Load lmerTest
# library(lmerTest)

# Fit the model with Year as both a fixed and random-effect
lme4::lmer(Crime ~ Year + (1 + Year | County) , data = MDCrime)

# Fit the model with Year2 rather than Year
out <- lme4::lmer(Crime ~ Year2 + (1 + Year2 | County) , data = MDCrime)

# Examine the model's output
summary(out)


## Build the Null model with only County as a random-effect
null_model <- lme4::lmer(Crime ~ (1 | County) , data = MDCrime)

## Build the Year2 model with Year2 as a fixed and random slope and County as the random-effect
year_model <- lme4::lmer(Crime ~ Year2 + (1 + Year2 | County) , data = MDCrime)

## Compare the two models using an anova
anova(null_model, year_model)

```
  
  
  
***
  
Chapter 3 - Generalized Linear Mixed-Effect Models  
  
Crash course on GLMs - relaxing the assumptions around normality of the residuals:  
  
* Non-normal data can be transformed using arcsin or the like  
* However, with advances in methodology, it is possible to more directly model the data using binomial and poisson distributions  
* The basic glm call is glm(y ~ x, family="")  # default is family="gaussian", which same as the lm()  
* The Poisson distribution is frequently best for count data, such as website visitors per hour - mean equals variance (generally best for small counts less than 30; can use normals for large counts)  
* For logistic regression, data can be entered in any of three formats  
	* Binary (y=0 or 1) - glm(y ~ x, family="binomial")  
    * Wilkinson-Rogers - glm(cbind(success, failure) ~ x, family="binomial")  
    * Weighted - glm(y ~ x, weights=weights, family="binomial")  
    * These methods differ primarily in the degrees of freedom (and thus deviance)  
  
Binomial data - modeling data with only two outcomes:  
  
* Traditional method for analysis includes looking at proportion of successes  
* The GLM allows for direct looks at the data - logistic regression (logit)  
* Binomial data can be fit using glmer(y ~ x + (1/group), family="error term")  
* The regression coefficients can be difficult to explain, sometimes leading to the use of odds ratios instead  
	* The odds ratio of 2.0 would mean 2:1 odds for that specific group  
  
Count data:  
  
* Examples like number of events per hour (website hits) or counts per area (birds)  
* The count data differs from the binomial in that there is no pre-specified upper boundary  
* While Chi-squared is often used for goodness of fit or test of association for count data, the Poisson GLM can be a nice alternative  
	* glm(y ~ x, family="poisson")  
    * glmer(y ~ x + (1|group), family="poisson")  
  
Example code includes:  
```{r cache=TRUE}

# In this case study, we will be working with simulated dose-response data
# The response is mortality (1) or survival (0) at the end of a study. During this exercise, we will fit a logistic regression using all three methods described in the video
# You have been given two datasets. dfLong has the data in a "long" format with each row corresponding to an observation (i.e., a 0 or 1)
# dfShort has the data in an aggregated format with each row corresponding to a treatment (e.g., 6 successes, 4 failures, number of replicates = 10, proportion = 0.6)

dfLong <- data.frame(dose=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10), 
                     mortality=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1)
                     )
str(dfLong)

dfShort <- dfLong %>% 
    group_by(dose) %>%
    summarize(mortality=sum(mortality), nReps=n()) %>%
    mutate(survival=nReps-mortality, mortalityP=mortality/nReps)
dfShort


# Fit a glm using data in a long format
fitLong <- glm(mortality ~ dose, data = dfLong, family = "binomial")
summary(fitLong)

# Fit a glm using data in a short format with two columns
fitShort <- glm( cbind(mortality , survival ) ~ dose , data = dfShort, family = "binomial")
summary(fitShort)

# Fit a glm using data in a short format with weights
fitShortP <- glm( mortalityP ~ dose , data = dfShort, weights = nReps , family = "binomial")
summary(fitShortP)


y <- c(0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 1, 2, 0, 1, 1, 0, 1, 5, 1, 1)
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)


# Fit the linear model
summary(lm(y ~ x))

# Fit the generalized linear model
summary(glm(y ~ x, family = "poisson"))


# Often, we want to "look" at our data and trends in our data
# ggplot2 allows us to add trend lines to our data
# The defult lines are created using a technique called local regression
# However, we can specify different models, including GLMs
# During this exercise, we'll see how to plot a GLM

# Plot the data using jittered points and the default stat_smooth
ggplot(data = dfLong, aes(x = dose, y = mortality)) + 
    geom_jitter(height = 0.05, width = 0.1) +
    stat_smooth(fill = 'pink', color = 'red') 

# Plot the data using jittered points and the the glm stat_smooth
ggplot(data = dfLong, aes(x = dose, y = mortality)) + 
    geom_jitter(height = 0.05, width = 0.1) +
    stat_smooth(method = 'glm',  method.args = list(family = "binomial"))


# library(lmerTest)

df <- data.frame(dose=rep(rep(c(0, 2, 4, 6, 8, 10), each=20), times=3), 
                 mortality=c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1), 
                 replicate=factor(rep(letters[1:3], each=120))
                 )
str(df)


glmerOut <- lme4::glmer(mortality ~ dose + (1|replicate), family = 'binomial', data = df)
summary(glmerOut)


# library(lmerTest)
# Fit the model and look at its summary 
# modelOut <- lme4::glmer( cbind(Purchases, Pass) ~ friend + ranking + (1|city), data = allData, family = 'binomial')
# summary( modelOut) 

# Compare outputs to a lmer model
# summary(lme4::lmer( Purchases/( Purchases + Pass) ~ friend + ranking + (1|city), data = allData))


# Run the code to see how to calculate odds ratios
# summary(modelOut) 
# exp(fixef(modelOut)[2])
# exp(confint(modelOut)[3, ])


# Load lmerTest
# library(lmerTest)


userGroups <- data.frame(group=factor(rep(rep(LETTERS[1:4], each=10), times=2)), 
                         webpage=factor(rep(c("old", "new"), each=40)), 
                         clicks=c(0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 3, 2, 3, 1, 2, 4, 2, 1, 0, 2, 0, 1, 2, 0, 2, 1, 1, 2, 4, 2, 8, 1, 1, 1, 2, 1, 1, 0, 0, 3, 0, 1, 4, 1, 2, 0, 1, 1, 0, 0, 3, 2, 0, 3, 1, 2, 2, 0, 2, 3, 1, 3, 2, 4, 4, 2, 1, 5, 2)
                         )
str(userGroups)


# Fit a Poisson glmer
summary( lme4::glmer(clicks ~ webpage + (1|group), family = 'poisson', data = userGroups))


# library(lmerTest)


rawIL <- read.csv("./RInputFiles/ILData.csv")
ILdata <- rawIL
str(ILdata)

# Age goes before year
modelOut <- lme4::glmer(count ~ age + year + (year|county), family = 'poisson', data = ILdata)
summary(modelOut)


# Extract out fixed effects
lme4::fixef(modelOut)

# Extract out random effects 
lme4::ranef(modelOut)


# Run code to see one method for plotting the data
ggplot(data = ILdata, aes(x = year, y = count, group = county)) +
    geom_line() +
    facet_grid(age ~ . ) +
    stat_smooth( method = 'glm',
                method.args = list( family = "poisson"), se = FALSE,
                alpha = 0.5) +
    theme_minimal()

```
  
  
  
***
  
Chapter 4 - Repeated Measures  
  
An introduction to repeated measures:  
  
* Sampling the same thing over time is a repeated measure, a specific example of a mixed effects model  
	* Follow the same individual through time - cohorts allow for controlling for individuality  
    * The paired t-test is often used for assessing a repeated measures dataset - t.test(x1, x2, paired=TRUE)  # x1 and x2 need to be the same length and each element needs to be the same individual  
* Repeated measures ANOVA is a conceptual extension of the paired t-test - are the means constant over time  
	* anova(lmer(y ~ time + (1|individual)))  
    * Can be used with glmer() also  
    * Note that degrees of freedom is still an open question - different packages calculate this differently  
  
Sleep study:  
  
* Applying LMER to the sleep study dataset - impact of drugs on sleep patterns for 10 patients followed over time  
	* This is the classic "Student" dataset due to Guinness at the time not allowing its employees to publish  
    * Ho will be that the amount of sleep does not vary with the treatments  
    * Modeling will be done using a linear mixed model  
* Modeling approach - iteratively;  
	* EDA  
    * Simple regression  
    * Model of interest  
    * Extract information from model  
    * Visualize final data  
  
Hate in NY state?  
  
* Change in rate of hate crimes over time by county - available from data.gov for 2010-2016  
* Level of technical details in reporting should vary significantly by audience - blend data in to story for wider audiences, while being reporducible/technical for a scientifc audience  
  
Wrap up:  
  
* Hiearchical data, mixed effects models, case studies  
* Start with the LME4 documentation for additional explorations and details  
  
Example code includes:  
```{r}

y <- c(0.23, 2.735, -0.038, 6.327, -0.643, 1.69, -1.378, -1.228, -0.252, 2.014, -0.073, 6.101, 0.213, 3.127, -0.29, 8.395, -0.33, 2.735, 0.223, 1.301)
treat <- rep(c("before", "after"), times=10)
x <- rep(letters[1:10], each=2)

# Run a standard, non-paired t-test
t.test(y[treat == "before"], y[treat == "after"], paired = FALSE)

# Run a standard, paired t-test
t.test(y[treat == "before"], y[treat == "after"], paired = TRUE)


library(lmerTest)
library(lme4)

# Run the paired-test like before
t.test(y[treat == "before"], y[treat == "after"], paired = TRUE)

# Run a repeated-measures ANOVA
anova(lmer( y ~ treat + (1|x)))


data(sleepstudy, package="lme4")
str(sleepstudy)

# Plot the data
ggplot(data = sleepstudy) +
    geom_line(aes(x = Days, y = Reaction, group = Subject)) +
    stat_smooth(aes(x = Days, y = Reaction), method = 'lm', size = 3, se = FALSE)

# Build a lm 
lm( Reaction ~ Days, data = sleepstudy)

# Build a lmer
(lmerOut <- lmer( Reaction ~ Days + (1|Subject), data = sleepstudy))


# The lmer model you built during the previous exercise has been saved as lmerOut
# During this exercise, you will examine the effects of drug type using both an ANOVA framework and a regression framework

# Run an anova
anova(lmerOut)

# Look at the regression coefficients
summary(lmerOut)


# Read in NY hate data
rawHate <- read.csv("./RInputFiles/hateNY.csv")
hate <- rawHate
str(hate)


ggplot( data = hate, aes(x = Year, y = TotalIncidents, group = County)) +
    geom_line() + 
    geom_smooth(method = 'lm', se = FALSE)


# During this exercise, you will build a glmer
# Because most of the incidents are small count values, use a Poisson (R function family poisson) error term
# First, build a model using the actually year (variable Year, such as 2006, 2007, etc) - this model will fail
# Second, build a model using the rescaled year (variable Year2, such as 0, 1, 2, etc)
# This demonstrates the importance of considering where the intercept is located when building regression models
# Recall that a variable x can be both a fixed and random effect in a lmer() or glmer(): for example lmer(y ~ x + (x| group) demonstrates this syntax

# glmer with raw Year
glmer(TotalIncidents ~ Year + (Year|County), data = hate, family = "poisson")

# glmer with scaled Year, Year2
glmerOut <- glmer(TotalIncidents ~ Year2 + (Year2|County), data = hate, family = "poisson")
summary(glmerOut)


# Extract and manipulate data
countyTrend <- ranef(glmerOut)$County
countyTrend$county <- factor(row.names(countyTrend), levels =row.names(countyTrend)[order(countyTrend$Year2)])

# Plot results 
ggplot(data = countyTrend, aes(x = county, y = Year2)) + geom_point() +
    coord_flip() + 
    ylab("Change in hate crimes per year")  +
    xlab("County")

```
  
  
  
***
  
###_Forecasting Product Demand in R_  
  
Chapter 1 - Forecasting Demand with Time Series  
  
Loading data in to an xts object:  
  
* The xts object will be the buidling block for the course - extensible time series (xts) is an extension of the zoo package - basically, a time index attached to the data matrix  
* Can create dates using dates=seq(as.Date("MM-DD-YYYY"), length=, by="weeks")  # to create weekly data  
	* xts(myData, order.by=dates)  # will create an XTS using dates as the index  
  
ARIMA Time Series 101:  
  
* AR - AutoRegressive (lags help to determine today's values - "long memory models")  
* MA - Moving Average (shocks/errors help to determine today's shocks/errors - "short memory models" due to dissipation)  
* I - Integrated (does the data have a dependency across time, and how long does it last) - make the time series stationary  
	* Stationarity is the idea that effects disipate over time - today has more impact on tomorrow than on time periods in the future  
    * Differencing (monthly, seasonal, etc.) the data can be a useful approach for data with stationarity  
* Begin by creating training dataset and valiadation training dataset  
* The auto.arima() function tries to estimate the best ARIMA for a given data series  
	* ARIMA(p, d, q) is ARIMA(AR, Differencing, MA)  
  
Forecasting and Evaluating:  
  
* Can use the ARIMA data to forecast the data forward - extrapolating the signal (forecasting) and estimating the amount of noise (error or CI)  
* The forecast() function in R simplifies the process - forecast(myModel, h=) which will forecast forward h time periods  
* Two common error measurements include MAE (mean average error) and MAPE (mean average percentage error)  
	* MAPE is better at putting things on a common scale  
  
Example code includes:  
```{r}


# Read in beverages data
rawBev <- read.csv("./RInputFiles/Bev.csv")
bev <- rawBev
str(bev)


# Load xts package 
library(xts)
library(forecast)


# Create the dates object as an index for your xts object
dates <- seq(as.Date("2014-01-19"), length = 176, by = "weeks")

# Create an xts object called bev_xts
bev_xts <- xts(bev, order.by = dates)


# Create the individual region sales as their own objects
MET_hi <- bev_xts[,"MET.hi"]
MET_lo <- bev_xts[,"MET.lo"]
MET_sp <- bev_xts[,"MET.sp"]

# Sum the region sales together
MET_t <- MET_hi + MET_lo + MET_sp

# Plot the metropolitan region total sales
plot(MET_t)


# Split the data into training and validation
MET_t_train <- MET_t[index(MET_t) < "2017-01-01"]
MET_t_valid <- MET_t[index(MET_t) >= "2017-01-01"]

# Use auto.arima() function for metropolitan sales
MET_t_model <- auto.arima(MET_t_train)


# Forecast the first 22 weeks of 2017
forecast_MET_t <- forecast(MET_t_model, h = 22)

# Plot this forecast #
plot(forecast_MET_t)


# Convert to numeric for ease
for_MET_t <- as.numeric(forecast_MET_t$mean)
v_MET_t <- as.numeric(MET_t_valid)

# Calculate the MAE
MAE <- mean(abs(for_MET_t - v_MET_t))

# Calculate the MAPE
MAPE <- 100*mean(abs(for_MET_t - v_MET_t)/v_MET_t)

# Print to see how good your forecast is!
print(MAE)
print(MAPE)


# Convert your forecast to an xts object
for_dates <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_t_xts <- xts(forecast_MET_t$mean, order.by = for_dates)

# Plot the validation data set
plot(for_MET_t_xts, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(MET_t_valid, col = "blue")


# Plot the validation data set
plot(MET_t_valid, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(for_MET_t_xts, col = "blue")

# Convert the limits to xts objects
lower <- xts(forecast_MET_t$lower[, 2], order.by = for_dates)
upper <- xts(forecast_MET_t$upper[, 2], order.by = for_dates)

# Adding confidence intervals of forecast to plot
lines(lower, col = "blue", lty = "dashed")
lines(upper, col = "blue", lty = "dashed")

```
  
  
  
***
  
Chapter 2 - Components of Demand  
  
Price elasticity:  
  
* Price is one of the obvious factors that impacts demand, with the relationship called price elasticity (% dDemand / % dPrice)  
	* Elastic goods have elasticity > 1, meaning demand changes more quickly (percentage wise) than price  
    * Inelastic goods have elasticity < 1, for example gasoline  
    * Unit elastic goods have elasticity = 1, meaning that X% increase in price drives X% decrease in demand  
    * Linear regression can be employed to estimate the elasticity for a given product - the log-log transform helps get the % vs % coefficients  
  
Seasonal/holiday/promotional effects:  
  
* Seasonal products are common - can be bought any time of the year, though certain seasons have higher demand (holidays are a common example)  
* Promotions are attempts by companies to influence demand  
* Linear regression can help determine relationships between demand and many other factors  
	* If an xts vector has been created for key dates, can merge(train, holiday, fill=0) and the holiday column will be 0 wherever there is no match to holiday  
  
Forecasting with regression:  
  
* Forecasting with time series is straightforward due to the lag nature of the models - tomorrow forecasts today and today forecasts tomorrow and etc.  
* Forecasting with regression can be more tricky, particularly since we need the future inputs (such as price) in order to predict the future demand  
	* Even when there are contractually fixed prices, promotions can effectively create a de facto price change anyways  
* Need to have the same column names in the test/validation dataset as were used in the modeling  
	* Then, can use predict(myModel, myData)  
    * May need to exponentiate in case the data are currently on the log scale rather than the absolute scale  
  
Example code includes:  
```{r}

bev_xts_train <- bev_xts[index(bev_xts) < "2017-01-01"]
bev_xts_valid <- bev_xts[index(bev_xts) >= "2017-01-01"]

# Save the prices of each product
l_MET_hi_p <- log(as.vector(bev_xts_train[, "MET.hi.p"]))

# Save as a data frame
MET_hi_train <- data.frame(as.vector(log(MET_hi[index(MET_hi) < "2017-01-01"])), l_MET_hi_p)
colnames(MET_hi_train) <- c("log_sales", "log_price")

# Calculate the regression
model_MET_hi <- lm(log_sales ~ log_price, data = MET_hi_train)


# Plot the product's sales
plot(MET_hi)

# Plot the product's price
MET_hi_p <- bev_xts_train[, "MET.hi.p"]
plot(MET_hi_p)


# Create date indices for New Year's week
n.dates <- as.Date(c("2014-12-28", "2015-12-27", "2016-12-25"))

# Create xts objects for New Year's
newyear <- as.xts(rep(1, 3), order.by = n.dates)

# Create sequence of dates for merging
dates_train <- seq(as.Date("2014-01-19"), length = 154, by = "weeks")

# Merge training dates into New Year's object
newyear <- merge(newyear, dates_train, fill = 0)


# Add newyear variable to your data frame
MET_hi_train <- data.frame(MET_hi_train, newyear=as.vector(newyear))

# Build regressions for the product
model_MET_hi_full <- lm(log_sales ~ log_price + newyear, data = MET_hi_train)


# Subset the validation prices #
l_MET_hi_p_valid <- log(as.vector(bev_xts_valid[, "MET.hi.p"]))

# Create a validation data frame #
MET_hi_valid <- data.frame(l_MET_hi_p_valid)
colnames(MET_hi_valid) <- "log_price"


# Predict the log of sales for your high end product
pred_MET_hi <- predict(model_MET_hi, MET_hi_valid)

# Convert predictions out of log scale
pred_MET_hi <- exp(pred_MET_hi)


# Convert to an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
pred_MET_hi_xts <- xts(pred_MET_hi, order.by = dates_valid)

# Plot the forecast
plot(pred_MET_hi_xts)

# Calculate and print the MAPE
MET_hi_v <- bev_xts_valid[,"MET.hi"]

MAPE <- 100*mean(abs((pred_MET_hi_xts - MET_hi_v)/MET_hi_v))
print(MAPE)

```
  
  
  
***
  
Chapter 3 - Blending Regression with Time Series  
  
Residuals from regression model:  
  
* The residuals from the regression models can be used for further modeling - see if the residuals are related over time, and model them with time series if so  
* Need to start by gathering the residuals and then converting them to an XTS object - explore for patterns in this XTS object  
  
Forecasting residuals:  
  
* When the residuals are related across time, we can use time series to model the residuals - basically, patterns to the errors provide an opportunity for further modeling  
* Can use auto.arima() on the residuals data, to see what the best ARIMA model for the residuals is  
	* Can then forecast the residuals in to the future using forecast(myModel, h=) # h being the time periods to predict forward  
  
Transfer functions and ensembling:  
  
* Techniques for combining forecasts - single model (transfer function) or averaging of models (ensembling)  
* Demand can be based on both regression (modeling external factors) and time series (residuals)  
* Ensembling is a combination (blend) of the forecasts, with simple averaging being the simplest approach  
	* Basically, build a stand-alone time series model and a stand-alone regression model  
    * The ensemble forecast can be better or worse than any of the stand-alone models  
  
Example code includes:  
```{r}

# Calculate the residuals from the model
MET_hi_full_res <- resid(model_MET_hi_full)

# Convert the residuals to an xts object
MET_hi_full_res <- xts(MET_hi_full_res, order.by = dates_train)


# Plot the histogram of the residuals
hist(MET_hi_full_res)

# Plot the residuals over time
plot(MET_hi_full_res)


# Build an ARIMA model on the residuals: MET_hi_arima
MET_hi_arima <- auto.arima(MET_hi_full_res)

# Look at a summary of the model
summary(MET_hi_arima)


# Forecast 22 weeks with your model: for_MET_hi_arima
for_MET_hi_arima <- forecast(MET_hi_arima, h=22)

# Print first 10 observations
head(for_MET_hi_arima$mean, n = 10)


# Convert your forecasts into an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_arima <- xts(for_MET_hi_arima$mean, order.by = dates_valid)

# Plot the forecast
plot(for_MET_hi_arima)


# Convert your residual forecast to the exponential version
for_MET_hi_arima <- exp(for_MET_hi_arima)

# Multiply your forecasts together!
for_MET_hi_final <- for_MET_hi_arima * pred_MET_hi_xts


# Plot the final forecast - don't touch the options!
plot(for_MET_hi_final, ylim = c(1000, 4300))

# Overlay the validation data set
lines(MET_hi_v, col = "blue")


# Calculate the MAE
MAE <- mean(abs(for_MET_hi_final - MET_hi_v))
print(MAE)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_hi_final - MET_hi_v)/MET_hi_v)
print(MAPE)


# Build an ARIMA model using the auto.arima function
MET_hi_model_arima <- auto.arima(MET_hi)

# Forecast the ARIMA model
for_MET_hi <- forecast(MET_hi_model_arima, h = length(MET_hi_v))

# Save the forecast as an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_xts <- xts(for_MET_hi$mean, order.by = dates_valid)

# Calculate the MAPE of the forecast
MAPE <- 100 * mean(abs(for_MET_hi_xts - MET_hi_v) / MET_hi_v)
print(MAPE)


# Ensemble the two forecasts together
for_MET_hi_en <- 0.5 * (for_MET_hi_xts + pred_MET_hi_xts)

# Calculate the MAE and MAPE
MAE <- mean(abs(for_MET_hi_en - MET_hi_v))
print(MAE)

MAPE <- 100 * mean(abs(for_MET_hi_en - MET_hi_v) / MET_hi_v)
print(MAPE)

```
  
  
  
***
  
Chapter 4 - Hierarchical Forecasting  
  
Bottom-Up Hierarchical Forecasting:  
  
* The hierarchical data structuring can be an advantage in forecasting, provided that the data has a natural hierarchy  
* The sum of all the lower-level forecasts should equal the higher-level forecasts  
	* Bottom-up: Forecast at the lowest level and aggregate (easiest but requires the most number of forecasts)  
    * Top-down: Forecast at the top level and the apply downwards  
    * Middle-out: Forecast at the middle levels and then apply both upwards and downwards  
  
Top-Down Hierarchical Forecasting:  
  
* The top-down forecasting process is typically quicker but less accurate than the bottom-up forecasting process  
* Two techniques available for top-down reconciliation  
	* Average of historical proportions - mean percentage that each component contributes to the total (calculated by sub-component such as week)  
    * Proportion of historical averages - mean percentage that each component contributes to the total (calculated by aggregate)  
* Reconciled forecasts at lower levels are typically less accurate than the direct forecast of the lower levels  
  
Middle-Out Hierarchical Forecasting:  
  
* Bottom-up forecasting is higher quality but more time-consuming than top-down forecasting  
* The middle-out forecasting method is a sometimes successful blend of the methods, getting decent accuracy at a lesser time commitment  
  
Wrap up:  
  
* Using time series to forecast demand forward  
* Incorporating external factors using linear regression  
* Blending time series and regression approaches  
* Top-down, bottom-up, middle-out approaches to aggregation and forecasting at various levels (hierarchical)  
* Can extend by looking at cross-elasticities (impact of competitor pricing)  
* Can better forecast proportions using time series analysis  
* Additional demand forecasting models include neural networks, exponential smoothing, etc.  
  
Example code includes:  
```{r}

# Build a time series model 
MET_sp_model_arima <- auto.arima(MET_sp)

# Forecast the time series model for 22 periods
for_MET_sp <- forecast(MET_sp_model_arima, h=22)

# Create an xts object
for_MET_sp_xts <- xts(for_MET_sp$mean, order.by=dates_valid)

MET_sp_v <- MET_sp["2017"]

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_sp_xts - MET_sp_v) / MET_sp_v)
print(MAPE)


MET_sp_train <- bev_xts_train %>%
    transform(log_sales = log(MET.sp), log_price=log(MET.sp.p))
MET_sp_train <- MET_sp_train[, c("log_sales", "log_price")]
MET_sp_train$newyear <- 0
MET_sp_train$valentine <- 0
MET_sp_train$christmas <- 0
MET_sp_train$mother <- 0

MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-12-28", "2015-12-27", "2016-12-25")), "newyear"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-02-09", "2015-02-08", "2016-02-07")), "valentine"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-12-21", "2015-12-20", "2016-12-18")), "christmas"] <- 1
MET_sp_train[index(MET_sp_train) %in% as.Date(c("2014-05-04", "2015-05-03", "2016-05-01")), "mother"] <- 1


# THE BELOW IS TOTAL NONSENSE
# Build a regression model
model_MET_sp <- lm(log_sales ~ log_price + newyear + valentine + christmas + mother, data = MET_sp_train)


MET_sp_valid <- as.data.frame(bev_xts_valid) %>%
    mutate(log_sales = log(MET.sp), log_price=log(MET.sp.p)) %>%
    select("log_sales", "log_price")
MET_sp_valid$newyear <- 0
MET_sp_valid$valentine <- 0
MET_sp_valid$christmas <- 0
MET_sp_valid$mother <- 0  

MET_sp_valid[7, "valentine"] <- 1
MET_sp_valid[19, "mother"] <- 1
MET_sp_valid$log_sales <- NULL


# Forecast the regression model using the predict function 
pred_MET_sp <- predict(model_MET_sp, MET_sp_valid)

# Exponentiate your predictions and create an xts object
pred_MET_sp <- exp(pred_MET_sp)
pred_MET_sp_xts <- xts(pred_MET_sp, order.by = dates_valid)

# Calculate MAPE
MAPE <- 100*mean(abs((pred_MET_sp_xts - MET_sp_v)/MET_sp_v))
print(MAPE)


# Ensemble the two forecasts
for_MET_sp_en <- 0.5 * (for_MET_sp_xts + pred_MET_sp_xts)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_sp_en - MET_sp_v) / MET_sp_v)
print(MAPE)


# Copy over pred_MET_lo_xts
pred_MET_lo_xts <- xts(c(2960.6, 2974.1, 2943.2, 2948.6, 2915.6, 2736.4, 2953.9, 3199.4, 2934, 2898.7, 3027.7, 3165.9, 3073.1, 2842.7, 2928.7, 3070.2, 2982.2, 3018, 3031.9, 2879.4, 2993.2, 2974.1), order.by=dates_valid)


# Calculate the metropolitan regional sales forecast
for_MET_total <- pred_MET_hi_xts + for_MET_sp_en + pred_MET_lo_xts

# Calculate a validation data set 
MET_t_v <- bev_xts_valid[,"MET.hi"] + bev_xts_valid[,"MET.lo"] + bev_xts_valid[,"MET.sp"]

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_total - MET_t_v) / MET_t_v)
print(MAPE)


# Create the MET_total data
MET_total <- xts(data.frame(MET.hi=c(5942, 5600, 5541, 6892, 5586, 5943, 6329, 6693, 6938, 6138, 6361, 6378, 5423, 5097, 4937, 5496, 6870, 6626, 6356, 5657, 6577, 7202, 7381, 7404, 7204, 6667, 6153, 6035, 5633, 5283, 5178, 4758, 5058, 5254, 5954, 6166, 6247, 6304, 7202, 6662, 6814, 6174, 5412, 5380, 5674, 6472, 6912, 7404, 8614, 8849, 7174, 6489, 7174, 6555, 6402, 7671, 5012, 4790, 5075, 5238, 5615, 6113, 7706, 7811, 7898, 7232, 6585, 5870, 7084, 5125, 5330, 5553, 6349, 6195, 6271, 5851, 5333, 5854, 5609, 5649, 6051, 6409, 5786, 5190, 5085, 4949, 5151, 5147, 5426, 5509, 6956, 7870, 8224, 6685, 6153, 5802, 5244, 5162, 5036, 5025, 8378, 8944, 7109, 7605, 7846, 7598, 8012, 9551, 6102, 5366, 4932, 4962, 5392, 6194, 7239, 7621, 7460, 7097, 6596, 5848, 8306, 5344, 5848, 6341, 7364, 7269, 7053, 6682, 6971, 7521, 7063, 6298, 6003, 5227, 5047, 4877, 4851, 4628, 4516, 4442, 4935, 5181, 5431, 5866, 5919, 5704, 5957, 6019, 5962, 6021, 5880, 5674, 7439, 7415)),
                 order.by=dates_train
                 )

# Build a regional time series model
MET_t_model_arima <- auto.arima(MET_total)

# Calculate a 2017 forecast for 22 periods
for_MET_t <- forecast(MET_t_model_arima, h=22)

# Make an xts object from your forecast
for_MET_t_xts <- xts(for_MET_t$mean, order.by=dates_valid)

# Calculate the MAPE
MAPE <- 100 * mean(abs(for_MET_t_xts - MET_t_v) / MET_t_v)
print(MAPE)


# Calculate the average historical proportions
prop_hi <- mean(MET_hi/MET_total)
prop_lo <- mean(MET_lo/MET_total)
prop_sp <- mean(MET_sp/MET_total)

# Distribute out your forecast to each product
for_prop_hi <- prop_hi*for_MET_t_xts
for_prop_lo <- prop_lo*for_MET_t_xts
for_prop_sp <- prop_sp*for_MET_t_xts

# Calculate the MAPE's for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi - MET_hi_v) / MET_hi_v)
print(MAPE_hi)

MET_lo_v <- bev_xts_valid[,"MET.lo"]
MAPE_lo <- 100 * mean(abs(for_prop_lo - MET_lo_v) / MET_lo_v)
print(MAPE_lo)

MAPE_sp <- 100 * mean(abs(for_prop_sp - MET_sp_v) / MET_sp_v)
print(MAPE_sp)


# Calculate the average historical proportions
prop_hi_2 <- mean(MET_hi) / mean(MET_total)
prop_lo_2 <- mean(MET_lo) / mean(MET_total)
prop_sp_2 <- mean(MET_sp) / mean(MET_total)

# Distribute out your forecast to each product
for_prop_hi_2 <- prop_hi_2 * for_MET_t_xts
for_prop_lo_2 <- prop_lo_2 * for_MET_t_xts
for_prop_sp_2 <- prop_sp_2 * for_MET_t_xts

# Calculate the MAPE's for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi_2 - MET_hi_v) / MET_hi_v)
print(MAPE_hi)
MAPE_lo <- 100 * mean(abs(for_prop_lo_2 - MET_lo_v) / MET_lo_v)
print(MAPE_lo)
MAPE_sp <- 100 * mean(abs(for_prop_sp_2 - MET_sp_v) / MET_sp_v)
print(MAPE_sp)


SEC_total <- xts(data.frame(SEC.hi=c(700, 775, 789, 863, 765, 759, 757, 747, 746, 709, 749, 786, 796, 726, 727, 723, 778, 755, 739, 740, 723, 695, 727, 707, 725, 684, 667, 698, 727, 722, 748, 695, 742, 739, 715, 724, 686, 671, 688, 682, 710, 700, 672, 680, 695, 780, 751, 693, 809, 881, 703, 712, 768, 796, 808, 904, 641, 662, 693, 725, 719, 736, 715, 722, 732, 745, 689, 705, 811, 739, 744, 700, 745, 735, 732, 722, 721, 732, 750, 714, 752, 677, 731, 674, 720, 675, 741, 722, 715, 719, 649, 697, 743, 733, 772, 698, 690, 734, 713, 644, 788, 833, 749, 731, 670, 675, 675, 993, 773, 751, 697, 677, 750, 723, 780, 763, 721, 701, 704, 684, 985, 791, 731, 714, 704, 694, 685, 652, 708, 754, 747, 705, 711, 699, 712, 745, 706, 665, 666, 692, 676, 696, 689, 697, 689, 717, 697, 708, 660, 707, 715, 680, 922, 888)), order.by=dates_train
                 )

# Build a time series model for the region
SEC_t_model_arima <- auto.arima(SEC_total)

# Forecast the time series model
for_SEC_t <- forecast(SEC_t_model_arima, h=22)

# Make into an xts object
for_SEC_t_xts <- xts(for_SEC_t$mean, order.by=dates_valid)

SEC_t_v <- bev_xts_valid$SEC.hi + bev_xts_valid$SEC.lo
# Calculate the MAPE
MAPE <- 100 * mean(abs(for_SEC_t_xts - SEC_t_v) / SEC_t_v)
print(MAPE)


SEC_hi <- bev_xts_train[, "SEC.hi"]
SEC_lo <- bev_xts_train[, "SEC.lo"]
SEC_hi_v <- bev_xts_valid[, "SEC.hi"]
SEC_lo_v <- bev_xts_valid[, "SEC.lo"]

# Calculate the average of historical proportions
prop_hi <- mean(SEC_hi / SEC_total)
prop_lo <- mean(SEC_lo / SEC_total)

# Distribute the forecast
for_prop_hi <- prop_hi * for_SEC_t_xts
for_prop_lo <- prop_lo * for_SEC_t_xts

# Calculate a MAPE for each product
MAPE_hi <- 100 * mean(abs(for_prop_hi - SEC_hi_v) / SEC_hi_v)
print(MAPE_hi)

MAPE_lo <- 100 * mean(abs(for_prop_lo - SEC_lo_v) / SEC_lo_v)
print(MAPE_lo)


# Copy over for_M_t_xts data
for_M_t_xts <- xts(c(2207, 2021, 2010, 2052, 2075, 2074, 2065, 2058, 2056, 2055, 2053, 2052, 2050, 2049, 2048, 2047, 2046, 2045, 2044, 2043, 2043, 2042), order.by=dates_valid)

# Calculate the state sales forecast: for_state
for_state = for_SEC_t_xts + for_MET_t_xts + for_M_t_xts

# See the forecasts
for_state

```
  
  
  
***
  
###_HR Analytics in R: Exploring Employee Data_  
  
Chapter 1 - Identifying the Best Recruiting Source  
  
Introduction - Ben Teusch, HR Analytics Consultant:  
  
* HR analytics has many other names - people analytics, workforce analytics, etc.  
* Identify groups for comparison - high vs. low performers, groups with high vs. low turnover, etc.  
	* Exploratory analysis and statistics for each group, including plots of key differences  
* Course is outlines as a series of case studies, with one case per chapter  
  
Recruiting and quality of hire:  
  
* Where are the best hires coming from, and how can you get more of them  
	* Defining quality of hire is challenging - some mix of productivity, satisfaction, retention, performance reviews, etc.  
    * Attrition can be defined as the mean of a 1, 0 vector of "did the person leave in the time period T"  
  
Visualizing recruiting data:  
  
* Helpful for communicating findings to decision makers  
* The geom_col() in ggplot will make a bar chart, with the y aestehtic being the bar height  
  
Example code includes:  
```{r}

# Import the recruitment data
recruitment <- readr::read_csv("./RInputFiles/recruitment_data.csv")

# Look at the first few rows of the dataset
head(recruitment)

# Get an overview of the recruitment data
summary(recruitment)

# See which recruiting sources the company has been using
recruitment %>% 
  count(recruiting_source)


# Find the average sales quota attainment for each recruiting source
avg_sales <- recruitment %>% 
  group_by(recruiting_source) %>% 
  summarize(avg_sales_quota_pct=mean(sales_quota_pct))

# Display the result
avg_sales


# Find the average attrition for the sales team, by recruiting source, sorted from lowest attrition rate to highest
avg_attrition <- recruitment %>%
  group_by(recruiting_source) %>% 
  summarize(attrition_rate=mean(attrition)) %>%
  arrange(attrition_rate)

# Display the result
avg_attrition

# Plot the bar chart
avg_sales %>% ggplot(aes(x=recruiting_source, y=avg_sales_quota_pct)) + geom_col()

# Plot the bar chart
avg_attrition %>% ggplot(aes(x=recruiting_source, y=attrition_rate)) + geom_col()

```
  
  
  
***
  
Chapter 2 - What is driving low employee engagement  
  
Analyzing employee engagement:  
  
* Gallup defines engaged employees as those who are involved in, enthusiastic about, and committed to their workplace  
* Survey data are available in the example case study  
	* Will use both mutate() and ifelse()  
    * The ifelse() is needed for vectors of length > 1 since it can work in a vectorized manner (and is thus OK inside the mutate call)  
  
Visualizing the engagement data:  
  
* Multiple attributes in a single place can make for a more compelling report  
* The tidyr package is part of the tidyverse, and hslps arrange the data properly for plotting  
	* tidyr::gather(columns, key="key", value="value") will be the package used in this example - pull the data from the columns down to the rows  
    * ggplot(survey_gathered, aes(x = key, y = value, fill = department)) + geom_col(position = "dodge")  
    * ggplot(survey_gathered, aes(x = key, y = value, fill = department)) + geom_col(position = "dodge") + facet_wrap(~ key, scales = "free")  
  
Are differences meaningful?  
  
* Can use significance testing to assess likelhood (p-value) that the second sample could have come from the same population as the first sample  
	* This course will use t-test (continuous variables) and chi-squared test (categorical variables)  
    * t.test(tenure ~ is_manager, data = survey)  
    * chisq.test(survey$left_company, survey$is_manager)  # no data= argument is available in the function  
  
Example code includes:  
```{r}

# Import the data
survey <- readr::read_csv("./RInputFiles/survey_data.csv")

# Get an overview of the data
summary(survey)

# Examine the counts of the department variable
survey %>% count(department)


# Output the average engagement score for each department, sorted
survey %>%
  group_by(department) %>%
  summarize(avg_engagement=mean(engagement)) %>%
  arrange(avg_engagement)


# Create the disengaged variable and assign the result to survey
survey_disengaged <- survey %>% 
  mutate(disengaged = ifelse(engagement <= 2, 1, 0)) 

survey_disengaged

# Summarize the three variables by department
survey_summary <- survey_disengaged %>%
  group_by(department) %>%
  summarize(pct_disengaged=mean(disengaged), 
            avg_salary=mean(salary), 
            avg_vacation_taken=mean(vacation_days_taken)
            )

survey_summary


# Gather data for plotting
survey_gathered <- survey_summary %>% 
  gather(key = "measure", value = "value",
         pct_disengaged, avg_salary, avg_vacation_taken)

# Create three bar charts
ggplot(survey_gathered, aes(x=measure, y=value, fill=department)) +
  geom_col(position="dodge") + 
  facet_wrap(~ measure, scales="free")


# Add the in_sales variable
survey_sales <- survey %>%
  mutate(in_sales = ifelse(department == "Sales", "Sales", "Other"), 
         disengaged = ifelse(engagement < 3, 1L, 0L)
         )

# Test the hypothesis using survey_sales
chisq.test(survey_sales$disengaged, survey_sales$in_sales)
t.test(disengaged ~ in_sales, data=survey_sales)


# Test the hypothesis using the survey_sales data
t.test(vacation_days_taken ~ in_sales, data = survey_sales)

```
  
  
  
***
  
Chapter 3 - Are new hires getting paid too much?  
  
Paying new hires fairly:  
  
* Sometimes, current employees get paid less than new employees, which can drive low engagement and turnover  
* Case study will have a simulated pay dataset available for analysis  
* Can use broom::tidy() to return the outputs in a nicely formatted data frame  
	* chisq.test(survey$in_sales, survey$disengaged) %>% tidy()  
  
Omitted variable bias:  
  
* Key assumption of the tests is that the groups are the same, with the exception of the variables being tested  
* Omitted variable bias occurs when both 1) the omitted variable is correlated with the dependent variable, and 2) the omitted variable is correlated with an explanatory variable  
	* Omitted variables are often known as confounders  
    * Plotting can help to identify the issue, particularly with a stacked (to 100%) bar chart  
    * pay %>% ggplot(aes(x = new_hire, fill = department)) + geom_bar(position = "fill")  
    * The geom_bar() object has height that is fully dependent on x, in contrast to geom_col() which has a y-aestehtic  
  
Linear regression helps to test the multivariate impacts of variables:  
  
* lm(salary ~ new_hire, data = pay) %>% tidy()  # single dependent variable  
* lm(salary ~ new_hire + department, data = pay) %>% tidy()  # multiple dependent variables  
* lm(salary ~ new_hire + department, data = pay) %>% summary()  # more detailed summary of the linear regression  
  
Example code includes:  
```{r}

# Import the data
pay <- readr::read_csv("./RInputFiles/fair_pay_data.csv")

# Get an overview of the data
summary(pay)

# Check average salary of new hires and non-new hires
pay %>% 
  group_by(new_hire) %>%
  summarize(avg_salary=mean(salary))


# Perform the correct statistical test
t.test(salary ~ new_hire, data = pay)
t.test(salary ~ new_hire, data = pay) %>%
  broom::tidy()


# Create a stacked bar chart
pay %>%
  ggplot(aes(x=new_hire, fill=job_level)) + 
  geom_bar(position="fill")

# Calculate the average salary for each group of interest
pay_grouped <- pay %>% 
  group_by(new_hire, job_level) %>% 
  summarize(avg_salary = mean(salary))
  
# Graph the results using facet_wrap()  
pay_grouped %>%
  ggplot(aes(x=new_hire, y=avg_salary)) + 
  geom_col() + 
  facet_wrap(~ job_level)


# Filter the data to include only hourly employees
pay_filter <- pay %>%
  filter(job_level == "Hourly")

# Test the difference in pay
t.test(salary ~ new_hire, data=pay_filter) %>%
  broom::tidy()


# Run the simple regression
model_simple <- lm(salary ~ new_hire, data = pay)

# Display the summary of model_simple
model_simple %>% 
  summary()

# Display a tidy summary
model_simple %>% 
  broom::tidy()


# Run the multiple regression
model_multiple <- lm(salary ~ new_hire + job_level, data = pay)

# Display the summary of model_multiple
model_multiple %>% 
  summary()

# Display a tidy summary
model_multiple %>% 
  broom::tidy()

```
  
  
  
***
  
Chapter 4 - Are performance ratings being given consistently?  
  
Joining HR data:  
  
* Employee data tend to be stored in different locations, requiring joins (merges) prior to running analyses  
	* dplyr::left_join(hr_data, bonus_pay_data, by = "employee_id")  
    * All employees in hr_data will be kept, even if there is no matching record in bonus_pay_data  
    * Employee ID (or similar) is by far the best way to join data - names tend to be non-unique and can differ in different systems  
  
Performance ratings and fairness:  
  
* Performance ratings are inherently subjective and thus prone to bias  
* Unconscious bias is based on the brain's heuristics, and may include preferences for members of various groups (biases, as reflected in hiring, promotion, etc.)  
  
Logistic regression is especially helpful for modeling binary response variables:  
  
* glm(high_performer ~ salary, data = hr, family = "binomial") %>% tidy()  
* glm(high_performer ~ salary + department, data = hr, family = "binomial") %>% tidy()  
  
Example code includes:  
```{r}

# Import the data
hr_data <- readr::read_csv("./RInputFiles/hr_data.csv")
performance_data <- readr::read_csv("./RInputFiles/performance_data.csv")

# Examine the datasets
summary(hr_data)
summary(performance_data)


# Join the two tables
joined_data <- left_join(hr_data, performance_data, by = "employee_id")

# Examine the result
summary(joined_data)

# Check whether the average performance rating differs by gender 
joined_data %>%
  group_by(gender) %>%
  summarize(avg_rating = mean(rating))


# Add the high_performer column
performance <- joined_data %>%  
  mutate(high_performer = ifelse(rating >= 4, 1, 0))

# Test whether one gender is more likely to be a high performer
chisq.test(performance$gender, performance$high_performer)   
 
# Do the same test, and tidy the output
chisq.test(performance$gender, performance$high_performer) %>% broom::tidy()


# Visualize the distribution of high_performer by gender
performance %>%
  ggplot(aes(x=gender, fill=factor(high_performer))) + 
  geom_bar(position="fill")

# Visualize the distribution of all ratings by gender
performance %>%
  ggplot(aes(x=gender, fill=factor(rating))) + 
  geom_bar(position="fill")

# Visualize the distribution of job_level by gender
performance %>%
  ggplot(aes(x = gender, fill = job_level)) +
  geom_bar(position = "fill")
 
# Test whether men and women have different job level distributions
chisq.test(performance$gender, performance$job_level) 


# Visualize the distribution of high_performer by gender, faceted by job level
performance %>%
  ggplot(aes(x = gender, fill = factor(high_performer))) +
  geom_bar(position = "fill") + 
  facet_wrap(~ job_level)


# Run a simple logistic regression
logistic_simple <- glm(high_performer ~ gender, family = "binomial", data = performance) 

# View the result with summary()
logistic_simple %>%
  summary()

# View a tidy version of the result
logistic_simple %>%
  broom::tidy()


# Run a multiple logistic regression
logistic_multiple <- glm(high_performer ~ gender + job_level, family = "binomial", data = performance)

# View the result with summary() or tidy()
logistic_multiple %>% broom::tidy()

```
  
  
  
***
  
Chapter 5 - Improving employee safety with data  
  
Employee safety - looking at accident rates and drivers:  
  
* Requires joining data on multiple variables  
	* joined_data <- left_join(hr_data, safety_data, by = c("year", "employee_id"))  
    * joined_data %>% filter(is.na(accident_time)) # use is.na() instead  
  
Focusing on the location of interest:  
  
* May want to run comparisons of the same location over time  
* May want to assess differences by locations to see if they may be explanatory variables  
  
Explaining the increase in accidents:  
  
* Can use multiple regression to help test for explanatory variables that impact the accident rate  
  
Wrap up:  
  
* Key tools from the Tidyverse (ggplot2, broom, dplyr, etc.) to assess HR data  
* Analytics usage within HR, including differences in HR and other data  
* Can apply additional data science techniques on HR data  
  
Example code includes:  
```{r}

# Import the data 
hr_data <- readr::read_csv("./RInputFiles/hr_data_2.csv")
accident_data <- readr::read_csv("./RInputFiles/accident_data.csv")

# Create hr_joined with left_join() and mutate()
hr_joined <- left_join(hr_data, accident_data, by=c("year", "employee_id")) %>% 
  mutate(had_accident=ifelse(is.na(accident_type), 0, 1))
  
hr_joined


# Find accident rate for each year
hr_joined %>% 
  group_by(year) %>% 
  summarize(accident_rate = mean(had_accident))

# Test difference in accident rate between years
chisq.test(hr_joined$year, hr_joined$had_accident)

# Which location had the highest acccident rate?
hr_joined %>%
  group_by(location) %>%
  summarize(accident_rate=mean(had_accident)) %>%
  arrange(-accident_rate)


# Compare annual accident rates by location
accident_rates <- hr_joined %>% 
  group_by(location, year) %>% 
  summarize(accident_rate = mean(had_accident))
  
accident_rates

# Graph it
accident_rates %>% 
  ggplot(aes(factor(year), accident_rate)) +
  geom_col() +
  facet_wrap(~location)


# Filter out the other locations
southfield <- hr_joined %>% 
  filter(location == "Southfield")

# Find the average overtime hours worked by year
southfield %>%
  group_by(year) %>% 
  summarize(average_overtime_hours = mean(overtime_hours))

# Test difference in Southfield's overtime hours between years
t.test(overtime_hours ~ year, data=southfield) 


# Import the survey data
survey_data <- readr::read_csv("./RInputFiles/survey_data_2.csv")

# Create the safety dataset
safety <- left_join(hr_joined, survey_data, by=c("employee_id", "year")) %>%
  mutate(disengaged=ifelse(engagement <= 2, 1, 0), year=factor(year))


# Visualize the difference in % disengaged by year in Southfield
safety %>% 
    filter(location=="Southfield") %>%
    ggplot(aes(x = year, fill = factor(disengaged))) +
    geom_bar(position = "fill")
 
# Test whether one year had significantly more disengaged employees
southSafety <- safety %>% 
    filter(location=="Southfield")
chisq.test(southSafety$disengaged, southSafety$year)


# Filter out Southfield
other_locs <- safety %>% 
  filter(location != "Southfield")

# Test whether one year had significantly more overtime hours worked
t.test(overtime_hours ~ year, data = other_locs) 

# Test whether one year had significantly more disengaged employees
chisq.test(other_locs$year, other_locs$disengaged)


# Use multiple regression to test the impact of year and disengaged on accident rate in Southfield
regression <- glm(had_accident ~ year + disengaged, family = "binomial", data = southSafety)

# Examine the output
regression %>% broom::tidy()

```
  
  
  
***
  
###_Supervised Learning in R: Case Studies_  
  
Chapter 1 - Cars Data  
  
Making predictions using machine learning:  
  
* Course focuses on applied skills from predictive learning, using regression and classification as well as EDA  
	* Regression tends to be for predicting continuous, numeric variables  
    * Classification tends to be for predicting categorical variables  
* Case studies include 1) fuel efficiency, 2) Stack Overflow developer survey, 3) voter turnout, and 4) ages of nuns  
* The fuel efficiency data is stored in cars2018 and is based on data from the US Department of Energy  
	* Variables names with spaces can be handled by surrounding them with backticks  
    * Tidyverse includes tibble, readr, ggplot2, dplyr, tidyr, purrr, etc. - can be loaded as a package using library(tidyverse)  
  
Getting started with caret:  
  
* The caret package is useful for predictive modeling - full process including the test/train split for the raw dataset  
	* in_train <- createDataPartition(cars_vars$Aspiration, p = 0.8, list = FALSE)  # will stratify on 'Aspiration' variable  
    * training <- cars_vars[in_train,]  
    * testing <- cars_vars[-in_train,]  
* Can then train the model using only the training dataset  
	* fit_lm <- train(log(MPG) ~ ., method = "lm", data=training, trControl=trainControl(method = "none"))  
    * Can then use the yardstick package to assess the quality of the model  
  
Sampling data:  
  
* Bootstrap resampling means sampling with replacement, and then fitting on the resampled dataset (run multiple times)  
	* cars_rf_bt <- train(log(MPG) ~ ., method = "rf", data = training, trControl = trainControl(method = "boot"))  # default 25 resamples  
    * Can both visualize the models and assess the model statistically  
  
Example code includes:  
```{r cache=TRUE}

cars2018 <- readr::read_csv("./RInputFiles/cars2018.csv")
str(cars2018, give.attr = FALSE)
summary(cars2018)

# Print the cars2018 object
cars2018

# Plot the histogram
ggplot(cars2018, aes(x = MPG)) +
    geom_histogram(bins = 25) +
    labs(y = "Number of cars",
         x = "Fuel efficiency (mpg)")


# Deselect the 2 columns to create cars_vars
cars_vars <- cars2018 %>%
    select(-Model, -`Model Index`)

# Fit a linear model
fit_all <- lm(MPG ~ ., data = cars_vars)

# Print the summary of the model
summary(fit_all)


# Load caret
library(caret)

# Split the data into training and test sets
set.seed(1234)
in_train <- createDataPartition(cars_vars$Transmission, p = 0.8, list = FALSE)
training <- cars_vars[in_train, ]
testing <- cars_vars[-in_train, ]

# Train a linear regression model
fit_lm <- train(log(MPG) ~ ., method = "lm", data = training,
                trControl = trainControl(method = "none"))

# Print the model object
fit_lm


# Train a random forest model
fit_rf <- train(log(MPG) ~ ., method = "rf", data = training,
                trControl = trainControl(method = "none"))

# Print the model object
fit_rf


# Create the new columns
results <- training %>%
    mutate(`Linear regression` = predict(fit_lm, training),
           `Random forest` = predict(fit_rf, training))

# Evaluate the performance
yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)


# Create the new columns
results <- testing %>%
    mutate(`Linear regression` = predict(fit_lm, testing),
           `Random forest` = predict(fit_rf, testing))

# Evaluate the performance
yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)


# Fit the models with bootstrap resampling
cars_lm_bt <- train(log(MPG) ~ ., method = "lm", data = training,
                   trControl = trainControl(method = "boot"))
cars_rf_bt <- train(log(MPG) ~ ., method = "rf", data = training,
                   trControl = trainControl(method = "boot"))
                   
# Quick look at the models
cars_lm_bt
cars_rf_bt


results <- testing %>%
    mutate(`Linear regression` = predict(cars_lm_bt, testing),
           `Random forest` = predict(cars_rf_bt, testing))

yardstick::metrics(results, truth = MPG, estimate = `Linear regression`)
yardstick::metrics(results, truth = MPG, estimate = `Random forest`)

results %>%
    gather(Method, Result, `Linear regression`:`Random forest`) %>%
    ggplot(aes(log(MPG), Result, color = Method)) +
    geom_point(size = 1.5, alpha = 0.5) +
    facet_wrap(~Method) +
    geom_abline(lty = 2, color = "gray50") +
    geom_smooth(method = "lm")

```
  
  
  
***
  
Chapter 2 - Stack Overflow Developer Data  
  
Essential copying and pasting from Stack Overflow (largest and most trusted developer community):  
  
* Annual survey of developer perspectives on Stack Overflow - can be used for predictive modeling  
* Data is made available publicly at insights.stackoverflow.com/survey  
* Key question is "what makes a developer more likely to work remotely" (size of company, geography of employee, etc.)  
	* Data are calss imbalanced, with many more Non-Remote employees than Remote employees  
    * Best first step is the simplest model - logit, without any tricks  
    * simple_glm <- stackoverflow %>% select(-Respondent) %>% glm(Remote ~ ., family = "binomial", + data = .)  # Remote ~ . Means "all variables" while data=. Means from the piped dataset  
  
Dealing with imbalanced data:  
  
* Class imbalance is a common problem that can negatively impact model performance  
	* This dataset has 10x the number of non-remote, which can influence models to just start predicting non-remote in all cases  
* One approach to class imbalance is upsampling, basically running resampling with replacement on the small class until it is the same size as the large class  
	* Simple to implement, but with the risk of over-fitting  
    * up_train <- upSample(x = select(training, -Remote), y = training$Remote, yname = "Remote") %>% as_tibble()  
    * stack_glm <- train(Remote ~ ., method = "glm", family = "binomial", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
  
Predicting remote status:  
  
* Classification models can include logistic regression and random forests  
	* stack_glm <- train(Remote ~ ., method = "glm", family = "binomial", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
    * stack_rf <- train(Remote ~ ., method = "rf", data = training, trControl = trainControl(method = "boot", sampling = "up"))  
* Classification models can be evaluated using the confusion matrix  
	* confusionMatrix(predict(stack_glm, testing), testing$Remote)  
    * yardstick::accuracy(testing_results, truth = Remote, estimate = `Logistic regression`)  
    * yardstick::ppv(testing_results, truth = Remote, estimate = `Logistic regression`)  
    * yardstick::npv(testing_results, truth = Remote, estimate = `Logistic regression`)  
  
Example code includes:  
```{r cache=TRUE}

stackoverflow <- readr::read_csv("./RInputFiles/stackoverflow.csv")
stackoverflow$Remote <- factor(stackoverflow$Remote, levels=c("Not remote", "Remote"))
str(stackoverflow, give.attr = FALSE)


# Print stackoverflow
stackoverflow

# First count for Remote
stackoverflow %>% 
    count(Remote, sort = TRUE)

# then count for Country
stackoverflow %>% 
    count(Country, sort = TRUE)


ggplot(stackoverflow, aes(x=Remote, y=YearsCodedJob)) +
    geom_boxplot() +
    labs(x = NULL,
         y = "Years of professional coding experience") 


# Build a simple logistic regression model
simple_glm <- stackoverflow %>%
        select(-Respondent) %>%
        glm(Remote ~ .,
            family = "binomial",
            data = .)

# Print the summary of the model
summary(simple_glm)


stack_select <- stackoverflow %>%
    select(-Respondent)

# Split the data into training and testing sets
set.seed(1234)
in_train <- caret::createDataPartition(stack_select$Remote, p=0.8, list = FALSE)
training <- stack_select[in_train,]
testing <- stack_select[-in_train,]


up_train <- caret::upSample(x = select(training, -Remote), y = training$Remote, yname = "Remote") %>%
    as_tibble()

up_train %>%
    count(Remote)


# Sub-sample to 5% of original
inUse <- sample(1:nrow(training), round(0.05*nrow(training)), replace=FALSE)
useTrain <- training[sort(inUse), ]

# Build a logistic regression model
stack_glm <- caret::train(Remote ~ ., method="glm", family="binomial", data = training, 
                          trControl = trainControl(method = "boot", sampling = "up")
                          )

# Print the model object 
stack_glm


# Build a random forest model
stack_rf <- caret::train(Remote ~ ., method="rf", data = useTrain, 
                         trControl = trainControl(method = "boot", sampling="up")
                         )

# Print the model object
stack_rf


# Confusion matrix for logistic regression model
caret::confusionMatrix(predict(stack_glm, testing), testing$Remote)

# Confusion matrix for random forest model
caret::confusionMatrix(predict(stack_rf, testing), testing$Remote)


# Predict values
testing_results <- testing %>%
    mutate(`Logistic regression` = predict(stack_glm, testing), `Random forest` = predict(stack_rf, testing))

## Calculate accuracy
yardstick::accuracy(testing_results, truth = Remote, estimate = `Logistic regression`)
yardstick::accuracy(testing_results, truth = Remote, estimate = `Random forest`)

## Calculate positive predict value
yardstick::ppv(testing_results, truth = Remote, estimate = `Logistic regression`)
yardstick::ppv(testing_results, truth = Remote, estimate = `Random forest`)

```
  
  
  
***
  
Chapter 3 - Voting  
  
Predicting voter turnout from survey data:  
  
* Survey data available from https://www.voterstudygroup.org/publications/2016-elections/data  
	* Opinions about political and economic topics  
    * Includes whether the voter turned out (voted), based on self-reporting, in the 2016 election  
    * Data are coded as integers, requiring a data dictionary to map the questions and responses to what they mean  
  
Vote 2016:  
  
* Exploratory data analysis will help with learning about the underlying dataset  
	* There are differences on many of the individual dimensions between voters and non-voters  
    * A good first step can be to start with the very simplest model, Dependent ~ .  
  
Cross-validation is the process of sub-dividing the data into folds, with each fold used once as the validation set:  
  
* Allows for more accurate estimates of model performance on out-of-sample error  
* Each process of CV will work through the data k times (assuming there are k folds)  
	* Repeated CV is the process of running CV multiple times (this is particularly well suited to parallel processing)  
  
Comparing model performance:  
  
* Random forest models tend to be more powerful and capable of classifying the training data (and thus subject to risk of overfits and associated poor quality of test set predictions)  
  
Example code includes:  
```{r cache=TRUE}

voters <- readr::read_csv("./RInputFiles/voters.csv")
voters$turnout16_2016 <- factor(voters$turnout16_2016, levels=c("Did not vote", "Voted"))
str(voters, give.attr = FALSE)

# Print voters
voters

# How many people voted?
voters %>%
    count(turnout16_2016)


# How do the reponses on the survey vary with voting behavior?
voters %>%
    group_by(turnout16_2016) %>%
    summarize(`Elections don't matter` = mean(RIGGED_SYSTEM_1_2016 <= 2),
              `Economy is getting better` = mean(econtrend_2016 == 1),
              `Crime is very important` = mean(imiss_a_2016 == 2))


## Visualize difference by voter turnout
voters %>%
    ggplot(aes(econtrend_2016, ..density.., fill = turnout16_2016)) +
    geom_histogram(alpha = 0.5, position = "identity", binwidth = 1) +
    labs(title = "Overall, is the economy getting better or worse?")


# Remove the case_indetifier column
voters_select <- voters %>%
        select(-case_identifier)

# Build a simple logistic regression model
simple_glm <- glm(turnout16_2016 ~ .,  family = "binomial", 
                  data = voters_select)

# Print the summary                  
summary(simple_glm)


# Split data into training and testing sets
set.seed(1234)
in_train <- caret::createDataPartition(voters_select$turnout16_2016, p = 0.8, list = FALSE)
training <- voters_select[in_train, ]
testing <- voters_select[-in_train, ]


# Perform logistic regression with upsampling and no resampling
vote_glm_1 <- caret::train(turnout16_2016 ~ ., method = "glm", family = "binomial", data = training,
                           trControl = trainControl(method = "none", sampling = "up")
                           )

# Print vote_glm
vote_glm_1


useSmall <- sort(sample(1:nrow(training), round(0.1*nrow(training)), replace=FALSE))
trainSmall <- training[useSmall, ]

# Logistic regression
vote_glm <- caret::train(turnout16_2016 ~ ., method = "glm", family = "binomial", data = trainSmall,
                         trControl = trainControl(method = "repeatedcv", repeats = 2, sampling = "up")
                         )

# Print vote_glm
vote_glm


# Random forest
vote_rf <- caret::train(turnout16_2016 ~ ., method = "rf", data = trainSmall,
                        trControl = trainControl(method="repeatedcv", repeats=2, sampling = "up")
                        )

# Print vote_rf
vote_rf


# Confusion matrix for logistic regression model on training data
caret::confusionMatrix(predict(vote_glm, trainSmall), trainSmall$turnout16_2016)

# Confusion matrix for random forest model on training data
caret::confusionMatrix(predict(vote_rf, trainSmall), trainSmall$turnout16_2016)

# Confusion matrix for logistic regression model on testing data
caret::confusionMatrix(predict(vote_glm, testing), testing$turnout16_2016)

# Confusion matrix for random forest model on testing data
caret::confusionMatrix(predict(vote_rf, testing), testing$turnout16_2016)

```
  
  
  
***
  
Chapter 4 - Nuns  
  
Catholic sisters survey from 1967 - https://curate.nd.edu/show/0r967368551 with codebook at https://curate.nd.edu/downloads/0v838051f6x	 
	
* Responses from 130,000 sisters in ~400 congergations  
* There was significant change occuring during this time period, both in society at large and within the community of nuns  
* Age has been binned in groups of 10 years (has been recoded as a numeric at the top of the range, so 20 will mean 11-20 and 30 will mean 21-30 and the like)  
* Historical dataset, centered in the context of nuns in 1967  
* Good first step is to tidy the data, so that it is easier for exploratory data analysis  
	* sisters67 %>% select(-sister) %>% gather(key, value, -age)  
  
Exploratory data analysis with tidy data:  
  
* Easy to see levels of agreement (overall) using dplyr::count()  
* Agreement with specific questions by age  
	* tidy_sisters %>% filter(key %in% paste0("v", 153:170)) %>% group_by(key, value) %>% summarise(age = mean(age)) %>% ggplot(aes(value, age, color = key)) + geom_line(alpha = 0.5, size = 1.5) + geom_point(size = 2) + facet_wrap(~key)  
	* Can use the mix of responses to make estimates about the ages of the nuns  
* Data will be split in to training, validation, and test sets  
	* The validation set will be used for model selection  
  
Predicting age with supervised learning:  
  
* "rpart" - building a tree-based (CART) model  
* "xgbLinear" - extreme gradient boosting  
* "gbm" - gradient boosted ensembles  
* Validation datasets are useful for assessing hyper-parameters and model choices, leaving the test dataset pure for a final out-of-sample error estimate  
  
Wrap up:  
  
* Train-Validation-Test to select the best models, tune the parameters, and estimate the out-of-sample error rates  
* Dealing with class imbalances; improving performance with resamples (bootstraps, cross-validation, etc.)  
* Hyper-parameter tuning can be valuable, but the time investment in other areas can often generate a greater return  
* Gradient boosting and random forests tend to perform very well, but there is always value in trying out multiple models  
	* Start with EDA and begin with a very simple model  
  
Example code includes:  
```{r cache=TRUE}

sisters67 <- readr::read_csv("./RInputFiles/sisters.csv")
str(sisters67, give.attr = FALSE)


# View sisters67
glimpse(sisters67)

# Plot the histogram
ggplot(sisters67, aes(x = age)) +
    geom_histogram(binwidth = 10)


# Tidy the data set
tidy_sisters <- sisters67 %>%
    select(-sister) %>%
    gather(key, value, -age)

# Print the structure of tidy_sisters
glimpse(tidy_sisters)


# Overall agreement with all questions varied by age
tidy_sisters %>%
    group_by(age) %>%
    summarize(value = mean(value, na.rm = TRUE))

# Number of respondents agreed or disagreed overall
tidy_sisters %>%
    count(value)


# Visualize agreement with age
tidy_sisters %>%
    filter(key %in% paste0("v", 153:170)) %>%
    group_by(key, value) %>%
    summarize(age = mean(age, na.rm = TRUE)) %>%
    ggplot(aes(value, age, color = key)) +
    geom_line(show.legend = FALSE) +
    facet_wrap(~key, nrow = 3)


# Remove the sister column
sisters_select <- sisters67 %>% 
    select(-sister)

# Build a simple linear regression model
simple_lm <- lm(age ~ ., 
                data = sisters_select)

# Print the summary of the model
summary(simple_lm)


# Split the data into training and validation/test sets
set.seed(1234)
in_train <- caret::createDataPartition(sisters_select$age, p = 0.6, list = FALSE)
training <- sisters_select[in_train, ]
validation_test <- sisters_select[-in_train, ]

# Split the validation and test sets
set.seed(1234)
in_test <- caret::createDataPartition(validation_test$age, p = 0.5, list = FALSE)
testing <- validation_test[in_test, ]
validation <- validation_test[-in_test, ]


# Fit a CART model
sisters_cart <- caret::train(age ~ ., method = "rpart", data = training)

# Print the CART model
sisters_cart


inSmall <- sample(1:nrow(training), 500, replace=FALSE)
smallSisters <- training[sort(inSmall), ]

sisters_xgb <- caret::train(age ~ ., method = "xgbTree", data = smallSisters)
sisters_gbm <- caret::train(age ~ ., method = "gbm", data = smallSisters, verbose=FALSE)

# Make predictions on the three models
modeling_results <- validation %>%
    mutate(CART = predict(sisters_cart, validation),
           XGB = predict(sisters_xgb, validation),
           GBM = predict(sisters_gbm, validation))

# View the predictions
modeling_results %>% 
    select(CART, XGB, GBM)


# Compare performace
yardstick::metrics(modeling_results, truth = age, estimate = CART)
yardstick::metrics(modeling_results, truth = age, estimate = XGB)
yardstick::metrics(modeling_results, truth = age, estimate = GBM)


# Calculate RMSE
testing %>%
    mutate(prediction = predict(sisters_gbm, testing)) %>%
    yardstick::rmse(truth = age, estimate = prediction)

```
  
  
  
***
  
###_Business Process Analytics in R_  
  
Chapter 1 - Introduction to Process Analysis  
  
Introduction and overview:  
  
* Efficient processes are core to many businesses, and improved data makes further analysis possible  
* The "internet of things" has created significant amounts of event data - why, what, and who  
	* Why is the purpose  
    * What is the steps in the process  
    * Who is the person responsible for the activity (can be machines or IS or the like; referred to as "resources")  
* Process workflow is iterative across Extraction-Processing-Analysis  
  
Activities as cornerstones of processes:  
  
* Data from an online learning platform; activities are captured and can be used for further analysis  
* Activities describe the flow of the process, and are one of the most important components of the process  
	* bupaR::activities_labels() is like names() for activities data  
    * bupaR::activities() is like summary() for activities data  
* Each case is described by the sequence of activities, known as its "trace"  
	* bupaR::traces() will create a frequency table of the traces  
    * bupaR::trace_explorer() will visualize the cases  
  
Components of process data:  
  
* Cases are the objects flowing through the process, while activities are the actions performed on them  
	* An activity instance is the occurrence of an activity (which can be a series of events) - specific action, case, time, etc.  
    * The "lifecycle status" is an area like Scheduled, Started, Completed, and the like  
    * The "event log" is the journal of the events  
    * The "resources" are the actors in the process  
* Can create an event log using the eventlog() function  
	* event_data %>% eventlog(case_id = "patient", activity_id = "handling", activity_instance_id = "handling_id", timestamp = "time", lifecycle_id = "registration_type", resource = "employee")  
  
Example code includes:  
```{r}

# Load the processmapR package using library
library(processmapR)
library(bupaR)


handling <- c('Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'X-Ray', 'X-Ray', 'X-Ray', 'X-Ray', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Registration', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Triage and Assessment', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'Blood test', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'MRI SCAN', 'X-Ray', 'X-Ray', 'X-Ray', 'X-Ray', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Discuss Results', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out', 'Check-out')
patient <- c('43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '156', '170', '172', '184', '278', '348', '420', '43', '156', '170', '172', '184', '278', '348', '420', '155', '221', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '156', '170', '172', '184', '278', '348', '420', '43', '156', '170', '172', '184', '278', '348', '420', '155', '221', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493')
employee <- c('r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r5', 'r5', 'r5', 'r5', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r1', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r2', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r3', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r4', 'r5', 'r5', 'r5', 'r5', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r6', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7', 'r7')
handling_id <- c('43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '543', '655', '656', '670', '672', '684', '721', '778', '848', '920', '955', '993', '1020', '1072', '1081', '1082', '1088', '1127', '1163', '1199', '1257', '1309', '1318', '1319', '1325', '1364', '1400', '1436', '1557', '1587', '1710', '1730', '1777', '1889', '1890', '1904', '1906', '1918', '1955', '2012', '2082', '2154', '2189', '2227', '2272', '2384', '2385', '2399', '2401', '2413', '2450', '2507', '2577', '2649', '2684', '2720', '43', '155', '156', '170', '172', '184', '221', '278', '348', '420', '455', '493', '543', '655', '656', '670', '672', '684', '721', '778', '848', '920', '955', '993', '1020', '1072', '1081', '1082', '1088', '1127', '1163', '1199', '1257', '1309', '1318', '1319', '1325', '1364', '1400', '1436', '1557', '1587', '1710', '1730', '1777', '1889', '1890', '1904', '1906', '1918', '1955', '2012', '2082', '2154', '2189', '2227', '2272', '2384', '2385', '2399', '2401', '2413', '2450', '2507', '2577', '2649', '2684', '2720')
registration_type <- c('start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete')
rTime <- c('2017-02-19 04:38:51', '2017-06-03 10:05:28', '2017-06-03 10:05:28', '2017-06-17 15:10:30', '2017-06-17 23:00:33', '2017-06-27 07:48:22', '2017-08-03 17:05:27', '2017-09-26 20:22:49', '2017-11-24 08:28:44', '2018-02-08 03:39:21', '2018-03-14 21:04:28', '2018-04-29 04:55:10', '2017-02-19 07:28:53', '2017-06-04 06:27:00', '2017-06-03 13:23:14', '2017-06-17 16:31:58', '2017-06-18 18:29:13', '2017-06-28 00:14:50', '2017-08-04 07:22:06', '2017-09-27 22:57:03', '2017-11-24 10:33:00', '2018-02-08 17:33:12', '2018-03-15 15:12:41', '2018-04-30 19:40:22', '2017-02-20 19:59:18', '2017-06-04 15:18:50', '2017-06-18 22:51:07', '2017-06-21 02:43:27', '2017-07-01 23:55:10', '2017-09-28 22:58:23', '2017-11-25 12:06:18', '2018-02-12 09:01:38', '2017-02-21 06:49:49', '2017-06-04 23:23:28', '2017-06-19 06:44:30', '2017-06-21 11:16:30', '2017-07-02 11:16:08', '2017-09-29 07:28:10', '2017-11-25 21:54:56', '2018-02-12 19:43:42', '2017-06-05 00:12:24', '2017-08-05 08:25:17', '2018-03-17 10:30:24', '2018-05-02 07:32:45', '2017-02-21 14:50:43', '2017-06-05 14:03:19', '2017-06-05 10:26:16', '2017-06-19 22:46:10', '2017-06-22 04:39:35', '2017-07-03 01:28:49', '2017-08-05 22:06:23', '2017-09-29 19:13:51', '2017-11-26 06:52:23', '2018-02-17 02:44:58', '2018-03-18 00:20:51', '2018-05-02 18:14:11', '2017-02-24 14:58:43', '2017-06-05 15:58:53', '2017-06-05 15:58:53', '2017-06-20 03:48:37', '2017-06-22 08:40:55', '2017-07-03 03:39:51', '2017-08-08 23:17:45', '2017-09-29 21:16:01', '2017-11-27 04:56:53', '2018-02-20 09:49:29', '2018-03-18 08:12:07', '2018-05-03 00:11:10', '2017-02-19 07:28:53', '2017-06-03 14:19:00', '2017-06-03 13:23:14', '2017-06-17 16:31:58', '2017-06-18 01:07:42', '2017-06-27 12:22:51', '2017-08-03 19:25:12', '2017-09-26 22:17:18', '2017-11-24 10:33:00', '2018-02-08 06:01:38', '2018-03-15 00:34:01', '2018-04-29 07:39:14', '2017-02-19 21:58:08', '2017-06-04 14:23:26', '2017-06-04 06:27:00', '2017-06-18 04:14:55', '2017-06-19 00:40:19', '2017-06-28 12:48:20', '2017-08-04 21:09:17', '2017-09-28 12:00:12', '2017-11-25 00:44:30', '2018-02-09 07:05:52', '2018-03-16 04:08:03', '2018-05-01 10:37:51', '2017-02-21 03:12:26', '2017-06-04 19:35:51', '2017-06-19 03:01:11', '2017-06-21 08:02:20', '2017-07-02 07:43:48', '2017-09-29 04:58:49', '2017-11-25 18:30:43', '2018-02-12 13:57:13', '2017-02-21 09:57:05', '2017-06-05 02:46:59', '2017-06-19 11:40:53', '2017-06-21 16:09:26', '2017-07-02 16:03:16', '2017-09-29 12:44:39', '2017-11-26 02:40:30', '2018-02-12 23:53:46', '2017-06-05 04:39:38', '2017-08-05 13:56:39', '2018-03-17 14:09:40', '2018-05-02 12:24:41', '2017-02-21 17:57:58', '2017-06-05 15:58:53', '2017-06-05 14:03:19', '2017-06-20 01:44:29', '2017-06-22 08:40:55', '2017-07-03 03:39:51', '2017-08-05 23:53:27', '2017-09-29 21:16:01', '2017-11-26 09:44:37', '2018-02-17 06:17:57', '2018-03-18 03:22:17', '2018-05-02 21:17:12', '2017-02-24 16:03:49', '2017-06-05 17:22:16', '2017-06-05 17:15:30', '2017-06-20 05:36:40', '2017-06-22 10:59:58', '2017-07-03 05:00:48', '2017-08-09 00:13:39', '2017-09-29 23:42:48', '2017-11-27 06:53:23', '2018-02-20 12:04:00', '2018-03-18 10:48:34', '2018-05-03 02:11:42')
rOrder <- c(43, 155, 156, 170, 172, 184, 221, 278, 348, 420, 455, 493, 543, 655, 656, 670, 672, 684, 721, 778, 848, 920, 955, 993, 1020, 1072, 1081, 1082, 1088, 1127, 1163, 1199, 1257, 1309, 1318, 1319, 1325, 1364, 1400, 1436, 1557, 1587, 1710, 1730, 1777, 1889, 1890, 1904, 1906, 1918, 1955, 2012, 2082, 2154, 2189, 2227, 2272, 2384, 2385, 2399, 2401, 2413, 2450, 2507, 2577, 2649, 2684, 2720, 2764, 2876, 2877, 2891, 2893, 2905, 2942, 2999, 3069, 3141, 3176, 3214, 3264, 3376, 3377, 3391, 3393, 3405, 3442, 3499, 3569, 3641, 3676, 3714, 3741, 3793, 3802, 3803, 3809, 3848, 3884, 3920, 3978, 4030, 4039, 4040, 4046, 4085, 4121, 4157, 4278, 4308, 4431, 4451, 4498, 4610, 4611, 4625, 4627, 4639, 4676, 4733, 4803, 4875, 4910, 4948, 4993, 5105, 5106, 5120, 5122, 5134, 5171, 5228, 5298, 5370, 5405, 5441)

pFrame <- tibble(handling=factor(handling, levels=c('Blood test', 'Check-out', 'Discuss Results', 'MRI SCAN', 'Registration', 'Triage and Assessment', 'X-Ray')), 
                 patient=patient, 
                 employee=factor(employee, levels=c('r1', 'r2', 'r3', 'r4', 'r5', 'r6', 'r7')), 
                 handling_id=handling_id, 
                 registration_type=factor(registration_type, levels=c("complete", "start")), 
                 time=as.POSIXct(rTime), 
                 .order=rOrder
                 )

patients <- eventlog(pFrame,
    case_id = "patient",
    activity_id = "handling",
    activity_instance_id = "handling_id",
    lifecycle_id = "registration_type",
    timestamp = "time",
    resource_id = "employee")


# The function slice can be used to take a slice of cases out of the eventdata. slice(1:10) will select the first ten cases in the event log, where first is defined by the current ordering of the data.

# How many patients are there?
n_cases(patients)

# Print the summary of the data
summary(patients)

# Show the journey of the first patient
slice(patients, 1)


# How many distinct activities are there?
n_activities(patients)

# What are the names of the activities?
activity_labels(patients)

# Create a list of activities
activities(patients)


# Have a look at the different traces
traces(patients)

# How many are there?
n_traces(patients)

# Visualize the traces using trace_explorer
trace_explorer(patients, coverage=1)

# Draw process map
process_map(patients)


claims <- tibble(id=c("claim1", "claim1", "claim2", "claim2", "claim2"), 
                 action=c(10002L, 10011L, 10015L, 10024L, 10024L), 
                 action_type=c("Check Contract", "Pay Back Decision", "Check Contract", "Pay Back Decision", "Pay Back Decision"), 
                 date=as.Date(c("2008-01-12", "2008-03-22", "2008-01-13", "2008-03-23", "2008-04-14")), 
                 originator=c("Assistant 1", "Manager 2", "Assistant 6", "Manager 2", "Manager 2"), 
                 status=as.factor(c("start", "start", "start", "start", "complete"))
                 )
claims


#create eventlog claims_log 
claims_log <- eventlog(claims,
    case_id = "id",
    activity_id = "action_type",
    activity_instance_id = "action",
    lifecycle_id = "status",
    timestamp = "date",
    resource_id = "originator")

# Print summary
summary(claims_log)

# Check activity labels
activity_labels(claims_log)

# Once you have an eventlog, you can access its complete metadata using the function mapping or the functions case_id, activity_id etc., to inspect individual identifiers.

```
  
  
  
***
  
Chapter 2 - Analysis Techniques  
  
Organizational analysis:  
  
* Processes are always dependent on resources, even if automated (machines and algorithms can be resources)  
	* Who executes the task, how specialized is the knowledge, etc.  
    * resource_labels(log_hospital)  # will pull out the resources  
    * resources(log_hospital)  # will pull out frequencies by resource  
* Can create a resource-activity matrix  
	* A person who performs only a few activities is considered to be specialized in that activity  
    * If only one person ever performs a specific activity, then there is a high risk of "brain drain"  
    * The plot() function, applied to an event_log, will create the resource-activity matrix  
    * resource_map(log_hospital)  # shows arrows between the work flows  
  
Structuredness:  
  
* Control-flow refers to the succession of activities  
	* Each unique flow is referred to as a trace  
    * Metrics include entry/exit points, length of cases, presence of activities, rework, etc.  
    * log_healthcare %>% start_activities("activity") %>% plot()  
    * log_healthcare %>% end_activities("activity") %>% plot()  
* Rework is when the same activity is done multiple times for the same case  
	* Repetitions are when the activity is repeated after some intervening steps  
    * Sel-loops are when the activity is repeated immediately after itself  
* The precedence matrix shows the relationships between the activities in a more structured manner  
	* eventlog %>% precedence_matrix(type = "absolute") %>% plot  # can be type="relative" also  
  
Performance analysis:  
  
* Visuals can include performance process maps and dotted charts; metrics can include throughput time, processing time, idle time  
	* eventlog %>% process_map(type = frequency())  # normal process map  
    * eventlog %>% process_map(type = performance())  # performance process map  
* The dotted chart shows the freqency of activities over time; basically, a form of scatter plot  
	* throughput_time is total time, processing_time is the sum of activity time, idle_time is the sume of when nothing is happening  
  
Linking perspectives:  
  
* Granularity can help give the statistics at the desired levels  
	* <process_metric>(level = "log", "trace", "case", "activity", "resource", "resource-activity")  
* Categorical data can be leveraged using the group_by() functionality - each group will then be calculated separately  
	* eventlog %>% group_by(priority) %>% number_of_repetitions(level = "resource") %>% plot()  
  
Example code includes:  
```{r}


data(sepsis, package="eventdataR")
str(sepsis)


# Print list of resources
resource_frequency(sepsis, level="resource")

# Number of resources per activity
resource_frequency(sepsis, level = "activity")

# Plot Number of executions per resource-activity
resource_frequency(sepsis, level = "resource-activity") %>% plot()


# Calculate resource involvement
resource_involvement(sepsis, level="resource")

# Show graphically 
sepsis %>% resource_involvement(level = "resource") %>% plot

# Compare with resource frequency
resource_frequency(sepsis, level="resource")


# Min, max and average number of repetitions
sepsis %>% number_of_repetitions(level = "log")

# Plot repetitions per activity
sepsis %>% number_of_repetitions(level = "activity") %>% plot

# Number of repetitions per resources
sepsis %>% number_of_repetitions(level = "resource")


eci <- c('21', '21', '21', '21', '21', '21', '21', '21', '21', '31', '31', '31', '31', '31', '31', '31', '31', '31', '31', '41', '41', '41', '41', '41', '41', '41', '51', '51', '51', '51', '51', '51', '51', '61', '61', '61', '61', '61', '61', '91', '91', '91', '91', '91', '91', '101', '101', '101', '101', '101', '101', '111', '111', '111', '111', '121', '121', '121', '121', '121', '121', '121', '121', '121', '131', '131', '131', '131', '131', '131', '131', '131', '161', '161', '171', '171', '171', '171', '181', '181', '181', '181', '181', '181', '201', '201', '201', '201', '201', '201', '201', '12', '12', '12', '12', '12', '22', '22', '22', '22', '22', '22', '32', '32', '32', '32', '32', '32', '42', '42', '42', '42', '52', '52', '52', '52', '52', '82', '82', '82', '82', '82', '92', '92', '92', '92', '92', '102', '102', '102', '102', '102', '112', '112', '122', '122', '21', '21', '21', '21', '21', '21', '21', '21', '21', '31', '31', '31', '31', '31', '31', '31', '31', '31', '31', '41', '41', '41', '41', '41', '41', '41', '51', '51', '51', '51', '51', '51', '51', '61', '61', '61', '61', '61', '61', '91', '91', '91', '91', '91', '91', '101', '101', '101', '101', '101', '101', '111', '111', '111', '111', '121', '121', '121', '121', '121', '121', '121', '121', '121', '131', '131', '131', '131', '131', '131', '131', '131', '161', '161', '171', '171', '171', '171', '181', '181', '181', '181', '181', '181', '201', '201', '201', '201', '201', '201', '201', '12', '12', '12', '12', '12', '22', '22', '22', '22', '22', '22', '32', '32', '32', '32', '32', '32', '42', '42', '42', '42', '52', '52', '52', '52', '52', '82', '82', '82', '82', '82', '92', '92', '92', '92', '92', '102', '102', '102', '102', '102', '112', '112', '122', '122')
ea1 <- c('prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'prepareBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'snack', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'eatingLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast')
ea2 <- c('eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareDinner', 'eatingDinner', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'prepareBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'snack', 'eatingBreakfast', 'prepareBreakfast', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'snack', 'snack', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareDinner', 'eatingDinner', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'snack', 'prepareLunch', 'eatingLunch', 'snack', 'eatingLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareLunch', 'eatingLunch', 'snack', 'prepareBreakfast', 'eatingBreakfast', 'prepareBreakfast', 'eatingBreakfast')
eaii <- c('9', '10', '19', '23', '24', '26', '36', '40', '41', '51', '52', '58', '60', '62', '63', '67', '69', '72', '73', '86', '87', '89', '90', '104', '105', '107', '119', '120', '128', '132', '133', '138', '139', '149', '150', '156', '159', '160', '164', '174', '175', '192', '194', '195', '198', '205', '206', '208', '211', '213', '214', '229', '236', '237', '239', '245', '251', '252', '253', '255', '259', '260', '262', '264', '271', '276', '281', '287', '292', '293', '297', '299', '310', '312', '331', '332', '336', '347', '363', '364', '374', '376', '387', '389', '434', '435', '447', '448', '450', '453', '454', '462', '463', '471', '472', '475', '483', '484', '487', '491', '492', '496', '508', '509', '512', '517', '518', '522', '536', '540', '541', '543', '562', '563', '565', '566', '572', '584', '585', '589', '590', '598', '615', '616', '618', '619', '627', '639', '640', '642', '643', '653', '665', '666', '682', '683', '9', '10', '19', '23', '24', '26', '36', '40', '41', '51', '52', '58', '60', '62', '63', '67', '69', '72', '73', '86', '87', '89', '90', '104', '105', '107', '119', '120', '128', '132', '133', '138', '139', '149', '150', '156', '159', '160', '164', '174', '175', '192', '194', '195', '198', '205', '206', '208', '211', '213', '214', '229', '236', '237', '239', '245', '251', '252', '253', '255', '259', '260', '262', '264', '271', '276', '281', '287', '292', '293', '297', '299', '310', '312', '331', '332', '336', '347', '363', '364', '374', '376', '387', '389', '434', '435', '447', '448', '450', '453', '454', '462', '463', '471', '472', '475', '483', '484', '487', '491', '492', '496', '508', '509', '512', '517', '518', '522', '536', '540', '541', '543', '562', '563', '565', '566', '572', '584', '585', '589', '590', '598', '615', '616', '618', '619', '627', '639', '640', '642', '643', '653', '665', '666', '682', '683')
elci <- c('start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'start', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete', 'complete')
ets1 <- c('2012-11-12 09:42:02', '2012-11-12 09:52:33', '2012-11-12 11:05:44', '2012-11-12 13:45:49', '2012-11-12 13:48:49', '2012-11-12 15:23:00', '2012-11-12 18:47:29', '2012-11-12 22:35:21', '2012-11-12 22:35:21', '2012-11-13 08:56:37', '2012-11-13 09:04:54', '2012-11-13 10:14:04', '2012-11-13 13:47:45', '2012-11-13 14:08:24', '2012-11-13 14:19:01', '2012-11-13 17:34:23', '2012-11-13 18:51:51', '2012-11-13 23:05:07', '2012-11-13 23:17:07', '2012-11-14 09:06:08', '2012-11-14 09:17:48', '2012-11-14 10:38:16', '2012-11-14 10:44:16', '2012-11-14 21:30:09', '2012-11-14 21:37:09', '2012-11-14 22:14:23', '2012-11-15 09:37:15', '2012-11-15 09:47:12', '2012-11-15 10:11:08', '2012-11-15 14:35:27', '2012-11-15 14:41:27', '2012-11-15 22:07:26', '2012-11-15 22:26:02', '2012-11-16 10:39:14', '2012-11-16 10:52:56', '2012-11-16 12:09:10', '2012-11-16 14:13:00', '2012-11-16 14:19:00', '2012-11-16 18:11:36', '2012-11-19 10:13:23', '2012-11-19 10:25:00', '2012-11-19 15:55:22', '2012-11-19 21:47:27', '2012-11-19 21:59:27', '2012-11-19 22:31:06', '2012-11-20 10:20:00', '2012-11-20 10:21:02', '2012-11-20 11:00:16', '2012-11-20 13:03:28', '2012-11-20 14:25:11', '2012-11-20 14:41:22', '2012-11-21 10:01:00', '2012-11-21 15:02:08', '2012-11-21 15:15:08', '2012-11-21 17:50:29', '2012-11-22 01:40:42', '2012-11-22 10:19:15', '2012-11-22 10:26:15', '2012-11-22 11:02:27', '2012-11-22 11:56:06', '2012-11-22 15:05:51', '2012-11-22 15:12:55', '2012-11-22 16:43:08', '2012-11-22 18:15:32', '2012-11-23 00:36:00', '2012-11-23 01:03:00', '2012-11-23 09:49:00', '2012-11-23 12:53:06', '2012-11-23 14:01:08', '2012-11-23 14:23:08', '2012-11-23 16:57:24', '2012-11-23 17:58:00', '2012-11-26 09:06:12', '2012-11-26 09:57:12', '2012-11-27 10:20:26', '2012-11-27 10:30:50')
ets2 <- c('2012-11-27 11:54:15', '2012-11-27 19:46:15', '2012-11-28 09:27:15', '2012-11-28 09:34:15', '2012-11-28 12:28:02', '2012-11-28 13:16:33', '2012-11-28 19:30:08', '2012-11-28 22:15:02', '2012-11-30 10:43:19', '2012-11-30 10:46:19', '2012-11-30 14:51:36', '2012-11-30 15:08:36', '2012-11-30 17:30:40', '2012-11-30 22:12:05', '2012-11-30 22:16:07', '2011-11-28 10:38:00', '2011-11-28 10:43:00', '2011-11-28 14:31:06', '2011-11-28 14:42:00', '2011-11-28 20:20:55', '2011-11-29 12:09:09', '2011-11-29 12:11:01', '2011-11-29 13:25:29', '2011-11-29 15:15:14', '2011-11-29 15:23:00', '2011-11-29 16:32:20', '2011-11-30 10:23:46', '2011-11-30 10:28:46', '2011-11-30 13:05:27', '2011-11-30 14:39:42', '2011-11-30 14:56:00', '2011-11-30 16:41:05', '2011-11-30 14:37:00', '2011-12-01 11:17:05', '2011-12-01 11:20:05', '2011-12-01 14:29:37', '2011-12-02 12:29:08', '2011-12-02 12:32:08', '2011-12-02 14:47:18', '2011-12-02 14:51:00', '2011-12-02 19:40:44', '2011-12-05 12:15:45', '2011-12-05 12:18:05', '2011-12-05 15:00:55', '2011-12-05 15:14:00', '2011-12-05 19:24:11', '2011-12-06 11:30:19', '2011-12-06 11:33:02', '2011-12-06 14:41:16', '2011-12-06 14:56:00', '2011-12-06 19:22:50', '2011-12-07 11:12:17', '2011-12-07 11:17:22', '2011-12-07 14:04:32', '2011-12-07 14:14:00', '2011-12-07 19:23:55', '2011-12-08 11:25:12', '2011-12-08 11:29:01', '2011-12-09 11:00:13', '2011-12-09 11:03:33', '2012-11-12 09:50:02', '2012-11-12 09:55:29', '2012-11-12 12:39:42', '2012-11-12 14:48:14', '2012-11-12 14:53:14', '2012-11-12 15:31:53', '2012-11-12 19:00:56', '2012-11-12 22:37:55', '2012-11-12 22:40:55', '2012-11-13 09:00:26', '2012-11-13 09:10:12', '2012-11-13 10:51:55', '2012-11-13 14:03:31', '2012-11-13 14:18:36', '2012-11-13 14:42:36', '2012-11-13 17:36:34', '2012-11-13 19:45:03', '2012-11-13 23:15:33', '2012-11-13 23:37:33', '2012-11-14 09:09:41', '2012-11-14 09:21:43', '2012-11-14 11:43:23', '2012-11-14 11:06:23', '2012-11-14 21:35:17', '2012-11-14 21:47:18', '2012-11-14 22:17:47', '2012-11-15 09:44:06', '2012-11-15 09:48:08', '2012-11-15 10:23:49', '2012-11-15 15:40:32', '2012-11-15 15:46:32', '2012-11-15 22:22:44', '2012-11-15 22:31:00', '2012-11-16 10:42:13') 
ets3 <- c('2012-11-16 10:52:58', '2012-11-16 12:09:57', '2012-11-16 14:58:55', '2012-11-16 14:55:55', '2012-11-16 18:14:49', '2012-11-19 10:17:12', '2012-11-19 10:33:59', '2012-11-19 16:07:49', '2012-11-19 21:59:01', '2012-11-19 22:24:58', '2012-11-19 22:31:59', '2012-11-20 10:21:02', '2012-11-20 10:37:51', '2012-11-20 11:14:44', '2012-11-20 13:28:35', '2012-11-20 14:40:16', '2012-11-20 15:10:16', '2012-11-21 10:06:50', '2012-11-21 15:14:47', '2012-11-21 15:30:55', '2012-11-21 17:55:48', '2012-11-22 01:45:42', '2012-11-22 10:25:45', '2012-11-22 10:59:45', '2012-11-22 11:10:30', '2012-11-22 12:09:07', '2012-11-22 15:12:19', '2012-11-22 15:26:18', '2012-11-22 16:51:54', '2012-11-22 18:17:25', '2012-11-23 00:41:13', '2012-11-23 10:28:57', '2012-11-23 10:01:57', '2012-11-23 12:57:33', '2012-11-23 14:20:47', '2012-11-23 14:38:47', '2012-11-23 16:57:43', '2012-11-23 18:06:38', '2012-11-26 10:37:28', '2012-11-26 10:05:28', '2012-11-27 10:30:43', '2012-11-27 10:44:43', '2012-11-27 11:54:59', '2012-11-27 19:46:56', '2012-11-28 09:33:52', '2012-11-28 09:44:52', '2012-11-28 12:57:42', '2012-11-28 13:38:45', '2012-11-28 19:45:20', '2012-11-28 22:18:43', '2012-11-30 11:45:40', '2012-11-30 11:51:40', '2012-11-30 15:05:54', '2012-11-30 15:20:00', '2012-11-30 17:42:59', '2012-11-30 22:15:48', '2012-11-30 22:39:48', '2011-11-28 10:42:55', '2011-11-28 10:49:00', '2011-11-28 14:41:54', '2011-11-28 15:04:00', '2011-11-28 20:20:59', '2011-11-29 12:10:37', '2011-11-29 12:19:00', '2011-11-29 13:25:32', '2011-11-29 15:22:57', '2011-11-29 15:49:00', '2011-11-29 16:32:23', '2011-11-30 10:27:58', '2011-11-30 10:38:58', '2011-11-30 13:05:31', '2011-11-30 14:55:24', '2011-11-30 15:11:00', '2011-11-30 16:41:09', '2011-11-30 15:08:00', '2011-12-01 11:19:43', '2011-12-01 11:29:43', '2011-12-01 14:36:38', '2011-12-02 12:31:10', '2011-12-02 12:37:10', '2011-12-02 14:50:19', '2011-12-02 15:24:00', '2011-12-02 19:40:50', '2011-12-05 12:17:58', '2011-12-05 12:26:02', '2011-12-05 15:13:55', '2011-12-05 15:42:00', '2011-12-05 19:24:16', '2011-12-06 11:32:49', '2011-12-06 11:38:51', '2011-12-06 14:55:18', '2011-12-06 15:18:18', '2011-12-06 19:22:55', '2011-12-07 11:17:14', '2011-12-07 11:22:35', '2011-12-07 14:13:34', '2011-12-07 14:41:00', '2011-12-07 20:38:18', '2011-12-08 11:28:24', '2011-12-08 11:35:55', '2011-12-09 11:03:09', '2011-12-09 11:09:08')
etsF <- c(ets1, ets2, ets3)

eatData <- tibble(case_id=eci, 
                  activity=factor(c(ea1, ea2)), 
                  activity_instance_id=eaii, 
                  lifecycle_id=factor(elci), 
                  resource=factor("UNDEFINED"), 
                  timestamp=as.POSIXct(etsF)
                  )

eat_patterns <- eventlog(eatData,
    case_id = "case_id",
    activity_id = "activity",
    activity_instance_id = "activity_instance_id",
    lifecycle_id = "lifecycle_id",
    timestamp = "timestamp",
    resource_id = "resource")


# Create performance map
eat_patterns %>% process_map(type = performance(FUN = median, units = "hours"))

# Inspect variation in activity durations graphically
eat_patterns %>% processing_time(level = "activity") %>% plot()

# Draw dotted chart
eat_patterns %>% dotted_chart(x = "relative_day", sort = "start_day", units = "secs")


# Time per activity
# daily_activities %>% processing_time(level = "activity") %>% plot

# Average duration of recordings
# daily_activities %>% throughput_time(level="log", units = "hours")

# Missing activities
# daily_activities %>% idle_time(level="log", units = "hours")


# Distribution throughput time
# vacancies %>% throughput_time(units="days")

# Distribution throughput time per department
# vacancies %>% group_by(vacancy_department) %>% throughput_time(units="days") %>% plot()

# Repetitions of activities
# vacancies %>% number_of_repetitions(level = "activity") %>% arrange(-relative)

```
  
  
  
***
  
Chapter 3 - Event Data Processing  
  
Filtering cases:  
  
* Sometimes there are too many cases, too many activities, missing data, and the like  
	* Can filter by either cases or events (time periods or specific activity types)  
    * Three levels of cases - performance, control-flow, and time frame  
* Look at long cases for what went wrong, and short cases for what to mimic  
	* filter_throughput_time(log, interval = c(5,10))  # absolute case length is 5-10 days  
    * filter_throughput_time(log, percentage = 0.5)  # shortest 50% of the cases  
    * filter_throughput_time(log, interval = c(5,10), units = "days", reverse =TRUE)  # cases that are NOT 5-10 days  
    * filter_throughput_time(log, interval = c(5,NA), units = "days") # cases longer than 5 days  
* Control-flow filters can be based on activity presence/absence, timing, and the like  
  
Filtering events - trim, frequency, label, general attribute:  
  
* Can trim to a time period based on start or end  
	* filter_time_period(log, interval = ymd(c("20180110","20180122")), filter_method = "trim")  # discards everything else  
* Can trim based on a specific start and end activities  
	* filter_trim(start_activities = "blues")  # traces that have no blues will be discarded  
    * filter_trim(start_activities = "blues", end_activities = "greens")  # traces that do not have blues followed by greens will be discarded  
    * Can set reverse=TRUE to get the opposites of these  
* Can filter by frequencies by either activity or resource  
	* filter_activity_frequency(log, interval = c(50,100))  
    * filter_activity_frequency(log, percentage = 0.8)  
    * filter_resource_frequency(log, interval = c(60,900))  
    * filter_resource_frequency(log, percentage = 0.6)  
* Can filter by labels  
	* filter_activity(log, activities = c("reds","oranges","purples")))  
    * dplyr::filter(log, cost > 1000, priority == "High", ...)  
  
Aggregating events - Is-A and Part-of:  
  
* The Is-A is when there are many subtypes of activity that are really all part of a main activity  
	* act_unite(log, "New name" = c("Old Variant 1","Old Variant 2","Old Variant 3"), ...)  # same number of activity instances, just fewer names  
* The Part-of is when there are clearly distinct activities that can also be considered components of a higher-level activity  
	* act_collapse(log, "Sub process" = c("Part 1","Part 2","Part 3"), ...)  # fewer number of activity instances, as they are collapsed to a single activity  
  
Enriching events - mutation (adding calculated variables):  
  
* The dplyr::mutate() can be used to directly add variables such as the cost  
	* log %>% group_by_case() %>% mutate(total_cost = sum(cost, na.rm = TRUE)  # group_by_case() is a function applied to event logs  
    * log %>% group_by_case() %>% mutate(total_cost = sum(cost, na.rm = TRUE) %>% mutate(impact = case_when(cost <= 1000 ~ "Low", cost <= 5000 ~ "Medium", TRUE ~ "High"))  
    * log %>% group_by_case() %>% mutate(refund_made = any(str_detect(activity, "Pay Claim")))  
* Metric functions can be used directly, with apped=TRUE, to both calculate the metric and add to the event log  
	* log %>% througput_time(level = "case", units = "days", append = TRUE) %>% mutate(on_time = processing_time_case <= 7)  
  
Example code includes:  
```{r eval=FALSE}

# Select top 20% of cases according to trace frequency
happy_path <- filter_trace_frequency(vacancies, percentage = 0.2)

# Visualize using process map
happy_path %>% process_map(type=requency(value = "absolute_case"))

# Compute throughput time
happy_path %>% throughput_time(units="days")


# Find no_declines
no_declines <- filter_activity_presence(vacancies, activities = "Decline Candidate", reverse=TRUE)

# What is the average number of  
first_hit <- filter_activity_presence(vacancies, activities = c("Send Offer", "Offer Accepted"), method="all")

# Create a performance map
first_hit %>% process_map(type=performance())

# Compute throughput time
first_hit %>% throughput_time()


# Create not_refused
not_refused <- vacancies %>% filter_precedence(antecedents = "Receive Response", consequents = "Review Non Acceptance", precedence_type = "directly_follows", filter_method = "none") 

# Select longest_cases
worst_cases <- not_refused %>% filter_throughput_time(interval=c(300, NA))

# Show the different traces
worst_cases %>% trace_explorer(coverage=1)


# Select activities
disapprovals <- vacancies %>% filter_activity(activities=c("Construct Offer", "Disapprove Offer", "Revise Offer","Disapprove Revision", "Restart Procedure"))

# Explore traces
disapprovals %>% trace_explorer(coverage=0.8)

# Performance map
disapprovals %>% process_map(type = performance(FUN = sum, units = "weeks"))


# Select cases
high_paid <- vacancies %>% filter(vacancy_department=="R&D", vacancy_salary_range==">100000")

# Most active resources
high_paid %>% resource_frequency(level="resource")

# Create a dotted chart
high_paid %>% dotted_chart(x="absolute", sort="start")

# Filtered dotted chart
library(lubridate)
high_paid %>% filter_time_period(interval = ymd(c("20180321","20180620")), filter_method = "trim") %>% dotted_chart(x="absolute", sort="start")


# Count activities and instances
n_activities(vacancies)
n_activity_instances(vacancies)

# Combine activities
united_vacancies <- vacancies %>% 
    act_unite("Disapprove Contract Offer" = c("Disapprove Offer","Disapprove Revision"),
              "Approve Contract Offer" = c("Approve Offer","Approve Revision"), 
              "Construct Contract Offer" = c("Construct Offer","Revise Offer")
              )
              
# Count activities and instances
n_activities(united_vacancies)
n_activity_instances(united_vacancies)


# Aggregate sub processes
aggregated_vacancies <- act_collapse(united_vacancies, 
                            "Interviews" = c("First Interview","Second Interview","Third Interview"),
                            "Prepare Recruitment" = c("Publish Position","File Applications","Check References"),
                            "Create Offer" = c("Construct Contract Offer", "Disapprove Contract Offer", "Approve Contract Offer")
                            )

# Calculated number of activities and activity instances
n_activities(aggregated_vacancies)
n_activity_instances(aggregated_vacancies)

# Create performance map
aggregated_vacancies %>% process_map(type=performance())


# Add total_cost
vacancies_cost <- vacancies %>% 
    group_by_case() %>% 
    mutate(total_cost = sum(activity_cost, na.rm = TRUE))

# Add cost_impact
vacancies_impact <- vacancies_cost %>%




# Compute throughput time per impact
vacancies_impact %>% group_by(cost_impact) %>% throughput_time(units = "weeks") %>% plot()


# Create cost_profile
vacancies_profile <- vacancies_impact %>%
    mutate(cost_profile = case_when(cost_impact == "High" & urgency < 7 ~ "Disproportionate",
                                    cost_impact == "Medium" & urgency < 5 ~ "Excessive",
                                    cost_impact == "Low" & urgency > 6 ~ "Lacking",
                                    TRUE ~ "Appropriate")) 

# Compare number of cases 
vacancies_profile %>% 
    group_by(cost_profile) %>%
    n_cases()
    
# Explore lacking traces
vacancies_profile %>%
  filter(cost_profile == "Lacking") %>%
  process_map()

```
  
  
  
***
  
Chapter 4 - Case Study  
  
Preparing the event data - example includes data from Sales, Purchasing, Manufacturing, Packaging & Delivery, Accounting:  
  
* While all departments need to work together, it is common for each department to have different data, business rules, relational data, etc.  
* Need to create event data first prior to running anything in the bupar package  
* Various field names (ends in _at or _by) may indicate the timing and resource levels  
* The tidyverse tools are helpful for creating the initial data  
  
Getting to know the process:  
  
* Identify data sources, transform so that each row is an event, harmonize them, create an eventlog  
* Start with high-level understanding of the process - summary(otc)  
	* activity_presence(otc) %>% plot()  
    * trace_length(otc) %>% plot()  
    * start_activities(otc, "activity") %>% plot()  
    * end_activities(otc, "activity") %>% plot()  
  
Roles and rules:  
  
* Parallel activities can be run in any order, which can cause an explosion in the number of traces - collapsing can help with abstraction  
* Research questions may be related to performance, compliance, etc.  
* The "4-eye" pricniple says that certain activities should not be performed by the same person  
  
Fast production, fast delivery:  
  
* Dotted charts can show the progression of the cases - request for quotation may be declined, or the offer may only be sent (no response)  
* May want to look at the performance by stages (sub-groups of activities), for more fair comparisons  
  
Course recap:  
  
* Process maps  
* Process analytics  
* Data preprocessing  
* Analysis and use cases  
  
Example code includes:  
```{r cache=TRUE}

quotations <- readRDS("./RInputFiles/otc_quotations.RDS")

# Inspect quotations
str(quotations)

# Create offer_history
offer_history <- quotations %>%
    gather(key, value, -quotation_id) %>%
    separate(key, into = c("activity", "info"))

# Recode the key variable
offer_history <- offer_history %>%
    mutate(info = fct_recode(info,  "timestamp" = 'at',  "resource" = 'by'))

# Spread the info variable
offer_history <- offer_history %>%
    spread(info, value)


validations <- readRDS("./RInputFiles/otc_validations.RDS")

# Inspect validations
str(validations)

# Create validate_history
validate_history <- validations %>%
    mutate(
        activity = "Validate",
        action = paste(quotation_id, "validate",  sep = "-"))

# Gather the timestamp columns
validate_history <- validate_history  %>%
    gather(lifecycle, timestamp, started, completed)


# Recode the lifecycle column of validate_history
validate_history <- validate_history %>%
    mutate(lifecycle = fct_recode(lifecycle,
                "start" = "started",
                "complete" = "completed"))


# Add lifecycle and action column to offer_history
offer_history <- offer_history %>%
    mutate(
        lifecycle = "complete",
        action = paste(quotation_id, 1:n(), sep = "-"))

# Create sales_history
sales_history <- bind_rows(validate_history, offer_history)


sales_history <- readRDS("./RInputFiles/otc_sales_history.RDS")
order_history <- readRDS("./RInputFiles/otc_order_history.RDS")
# sales_quotations <- readRDS("./RInputFiles/otc_sales_quotation.RDS")

str(sales_history)
str(order_history)
# str(sales_quotations)

order_history <- order_history %>% 
    rename(timestamp=time, lifecycle=status) %>%
    select(-activity_cost) %>%
    mutate(activity=as.character(activity), 
           resource=as.character(activity), 
           lifecycle=as.character(lifecycle)
           )
sales_history <- sales_history %>%
    mutate(timestamp=lubridate::as_datetime(timestamp))

# sales_history <- sales_history %>% left_join(sales_quotations)
otc <- bind_rows(sales_history, order_history)


# Create the eventlog object 
otc <- otc %>%
    mutate(case_id = paste(quotation_id, sales_order_id, sep = "-")) %>%
    eventlog(
        case_id = "case_id",
        activity_id = "activity",
        activity_instance_id = "action",
        timestamp = "timestamp",
        resource_id = "resource",
        lifecycle_id = "lifecycle"
        )

# Create trace coverage graph
trace_coverage(otc, level="trace") %>% plot()

# Explore traces
otc %>%
    trace_explorer(coverage = 0.25)


# Collapse activities
otc_high_level <- act_collapse(otc, "Delivery" = c(
  "Handover To Deliverer",
  "Order Delivered",
  "Present For Collection",
  "Order Fetched")
  )

# Draw a process map
process_map(otc_high_level)

# Redraw the trace coverage graph
otc_high_level %>% trace_coverage(level="trace") %>% plot()

# Compute activity wise processing time
otc_high_level %>% processing_time(level="activity", units="days")

# Plot a resource activity matrix of otc
otc %>% resource_frequency(level = "resource-activity") %>% plot()


# Create otc_selection
otc_selection <- otc %>% filter_activity(activities = c("Send Quotation","Send Invoice"))

# Explore traces
otc %>% trace_explorer(coverage=1)

# Draw a resource map
otc_selection %>% resource_map()


# Create otc_returned
otc_returned <- otc %>% filter_activity_presence("Return Goods")

# Compute percentage of returned orders
n_cases(otc_returned)/n_cases(otc)

# Trim cases and visualize
otc_returned %>% filter_trim(start_activities="Return Goods") %>% process_map()


# Time from order to delivery
# otc %>% filter_trim(start_activities="Receive Sales Order", end_activities="Order Delivered") %>% 
#     processing_time(units="days")


# Plot processing time by type
# otc %>%
#     group_by(type) %>%
#     throughput_time() %>%
#     plot()

```
  
  
  
***
  
###_Network Science in R - A Tidy Approach_  
  
Chapter 1 - Hubs of the Network  
  
Network science - include social networks, neural networks, etc.:  
  
* Nodes and edges (connections between nodes, aka "ties") make up a network  
	* In a directed network, ties have a direction (for example, followers and follwing)  
    * In an undirected network, ties do not have a direction (for example, mutual friendship)  
    * In a weighted network, the ties have an associated weight (such as bandwidth, duration of friendship, etc.)  
* Chapter will focus on the terrorism network associated with the Madrid train bombing of 2004  
	* Ties include friendhsip, training camps, previous attacks, and other terrorists  
* The network is reflected in tidy fashion, using one data frame for nodes and another for ties  
    * g <- igraph::graph_from_data_frame(d = ties, directed = FALSE, vertices = nodes)  
    * V(g); vcount(g)  
    * E(g); ecount(g)  
* And, then working with attributes of the network  
    * g$name <- "Madrid network"; g$name  
    * V(g)$id <- 1:vcount(g)  
    * E(g)$weight  
  
Visualizing networks:  
  
* The ggraph package can help with visualizing networks  
	* ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = weight)) + geom_node_point()  
    * Much like the language of ggplot2  
  
Centrality measures:  
  
* Objective is to find the most important nodes - connections among members of the networks  
* Network science is a spinoff of data science, with the goal of measuring networks  
* The agree of "degree" measures the number of ties (edges) that a node has  
	* degree(g) # gives the number of edges per node  
    * strength(g) # sumes the weights of the edges per node  
  
Example code includes:  
```{r eval=FALSE}

# read the nodes file into the variable nodes
nodes <- readr::read_csv("./RInputFiles/nodes.csv")
nodes

# read the ties file into the variable ties
ties <- readr::read_csv("./RInputFiles/ties.csv")
ties


library(igraph)
library(ggraph)


# make the network from the data frame ties and print it
g <- graph_from_data_frame(ties, directed = FALSE, vertices = nodes)
g

# explore the set of nodes
V(g)

# print the number of nodes
vcount(g)

# explore the set of ties
E(g)

# print the number of ties
ecount(g)


# give the name "Madrid network" to the network and print the network `name` attribute
g$name <- "Madrid network"
g$name

# add node attribute id and print the node `id` attribute
V(g)$id <- 1:vcount(g)
V(g)$id

# print the tie `weight` attribute
E(g)$weight

# print the network and spot the attributes
g


# visualize the network with layout Kamada-Kawai
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# add an id label to nodes
ggraph(g, layout = "with_kk") +
  geom_edge_link(aes(alpha = weight)) +
  geom_node_point()  + 
  geom_node_text(aes(label = id), repel=TRUE)


# visualize the network with circular layout. Set tie transparency proportional to its weight
ggraph(g, layout = "in_circle") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# visualize the network with grid layout. Set tie transparency proportional to its weight
ggraph(g, layout = "grid") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point()


# compute the degrees of the nodes
dgr <- degree(g)

# add the degrees to the data frame object
nodes <- mutate(nodes, degree = dgr)

# add the degrees to the network object
V(g)$degree <- dgr

# arrange the terrorists in decreasing order of degree
arrange(nodes, -degree)


# compute node strengths
stg <- strength(g)

# add strength to the data frame object using mutate
nodes <- mutate(nodes, strength = stg)

# add the variable stg to the network object as strength
V(g)$strength <- stg

# arrange terrorists in decreasing order of strength and then in decreasing order of degree
arrange(nodes, -degree)
arrange(nodes, -strength)

```
  
  
  
***
  
Chapter 2 - Weakness and strength  
  
Tie betweenness:  
  
* Betweeness is the number of shortest paths that go through a specific tie (edge) - these removals would be the most disruptive  
* In a weighted network, the shortest path is defined as the lowest sum of weights, rather than the fewest edges  
	* Often need to inverse the weights prior to running, since a "high" weight usually means a close connection and thus an easy path  
    * dist_weight = 1 / E(g)$weight  
    * edge_betweenness(g, weights = dist_weight)  
	
Visualizing centrality measures:  
  
* Visualizing betweenness can be done within the igraph package  
    * ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = betweenness)) + geom_node_point()  
    * ggraph(g, layout = "with_kk") + geom_edge_link(aes(alpha = weight)) + geom_node_point(aes(size = degree))  
  
The strength of weak ties:  
  
* "The strength of weak ties" is a research paper written about network strengths  
	* Argument is that the "weak ties" in a network are often the most important - relationships between diverse communities, leading to diverse ideas  
    * The "strong ties" are the relationships between people who are frequently together - can lead to group-think and stasis  
    * Noted that the Madrid group (and similar) tended to be highly dispersed and thus having many weak ties  
    * ties %>% group_by(weight) %>% summarise(n = n(), p = n / nrow(ties)) %>% arrange(-n)  
  
Example code includes:  
```{r eval=FALSE}

# save the inverse of tie weights as dist_weight
dist_weight <- 1 / E(g)$weight

# compute weighted tie betweenness
btw <- edge_betweenness(g, weights = dist_weight)

# mutate the data frame ties adding a variable betweenness using btw
ties <- mutate(ties, betweenness=btw)

# add the tie attribute betweenness to the network
E(g)$betweenness <- btw


# join ties with nodes
ties_joined <- ties %>% 
  left_join(nodes, c("from" = "id")) %>% 
  left_join(nodes, c("to" = "id")) 

# select only relevant variables and save to ties
ties_selected <- ties_joined %>% 
  select(from, to, name_from = name.x, name_to = name.y, betweenness)

# arrange named ties in decreasing order of betweenness
arrange(ties_selected, -betweenness)


# set (alpha) proportional to weight and node size proportional to degree
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha=weight)) + 
  geom_node_point(aes(size=degree))

# produce the same visualization but set node size proportional to strength
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight)) + 
  geom_node_point(aes(size = strength))


# visualize the network with tie transparency proportional to betweenness
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness)) + 
  geom_node_point()

# add node size proportional to degree
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness)) + 
  geom_node_point(aes(size = degree))


# find median betweenness
q = median(E(g)$betweenness)

# filter ties with betweenness larger than the median
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = betweenness, filter = (betweenness > q))) + 
  geom_node_point() + 
  theme(legend.position="none")


# find number and percentage of weak ties
ties %>%
  group_by(weight) %>%
  summarise(number = n(), percentage=n()/nrow(.)) %>%
  arrange(-number)


# build vector weakness containing TRUE for weak ties
weakness <- ifelse(ties$weight == 1, TRUE, FALSE)

# check that weakness contains the correct number of weak ties
sum(weakness)


# visualize the network by coloring the weak and strong ties
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(color = weakness)) + 
  geom_node_point()


# visualize the network with only weak ties using the filter aesthetic
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(filter=weakness), alpha = 0.5) + 
  geom_node_point()

```
  
  
  
***
  
Chapter 3 - Connection patterns  
  
Connection patterns:  
  
* The adjacency matrix can be calculated using as_adjacency_matrix(g)  
	* For each match of row/column, there will be a 1 for adjacency and a 0 for non-adjacency  
    * Alternately, can have the weight of the tie as the entry for each row/column (with 0 as before meaning non-adjacency)  
    * A = as_adjacency_matrix(g, attr = "weight")  
    * diag(A)  
* Can use the adjacency matrix to assess similarity of nodes in the matrix  
	* The Pearson similarity measures the correlation between the columns in the matrix  
  
Pearson correlation coefficient:  
  
* Can visualize the correlations using scatterplots  
* Can compute the correlations analytically as well  
	* cor(nodes$degree, nodes$strength)  
  
Most similar and most dissimilar terrorists:  
  
* Can use named graphs with weighted ties for a graphical representation of nodes and paths  
* Can use the adjacency matrix to reprsent the ties in a manner simplified for algebra  
* Can use the data frame format (one for nodes, and one for ties) for use with dplur and ggplot2  
	* as_data_frame(g, what = "both")  
* Can easily switch back and forth between the representations of the network  
	* as_adjacency_matrix(g)  
    * graph_from_adjacency_matrix(A)  
    * as_data_frame(g, what = "both")  
    * graph_from_data_frame(df$ties, vertices = df$nodes)  
    * as_data_frame(graph_from_adjacency_matrix(A), what = "both")  
    * as_adjacency_matrix(graph_from_data_frame(df$ties, vertices = df$nodes))  
  
Example code includes:  
```{r eval=FALSE}

# mutate ties data frame by swapping variables from and to 
ties_mutated <- mutate(ties, temp = to, to = from, from = temp) %>% select(-temp)

# append ties_mutated data frame to ties data frame
ties <- rbind(ties, ties_mutated)

# use a scatter plot to visualize node connection patterns in ties setting color aesthetic to weight
ggplot(ties, aes(x = from, y = to, color = factor(weight))) +
  geom_point() +
  labs(color = "weight")


# get the weighted adjacency matrix
A <- as_adjacency_matrix(g, attr = "weight", sparse = FALSE, names = FALSE)

# print the first row and first column of A
A[1, ]
A[, 1]

# print submatrix of the first 6 rows and columns
A[1:6, 1:6]


# obtain a vector of node strengths
rowSums(A)

# build a Boolean (0/1) matrix from the weighted matrix A
B <- ifelse(A > 0, 1, 0)

# obtain a vector of node degrees using the Boolean matrix
rowSums(B)


# compute the Pearson correlation on columns of A
S <- cor(A)

# set the diagonal of S to 0
diag(S) = 0

# print a summary of the similarities in matrix S
summary(c(S))

# plot a histogram of similarities in matrix S
hist(c(S), xlab = "Similarity", main = "Histogram of similarity")


# Scatter plot of degree and strength with regression line
ggplot(nodes, aes(x = degree, y = strength)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# Pearson correlation coefficient 
cor(nodes$degree, nodes$strength)


# build weighted similarity network and save to h
h <- graph_from_adjacency_matrix(S, mode = "undirected", weighted = TRUE)

# convert the similarity network h into a similarity data frame sim_df
sim_df <- as_data_frame(h, what = "edges")

# map the similarity data frame to a tibble and save it as sim_tib
sim_tib <- as_tibble(sim_df)

# print sim_tib
sim_tib


# left join similarity and nodes data frames and then select and rename relevant variables
sim2 <- sim_tib %>% 
  left_join(nodes, c("from" = "id")) %>% 
  left_join(nodes, c("to" = "id")) %>%
  select(from, to, name_from = name.x, name_to = name.y, similarity = weight, 
         degree_from = degree.x, degree_to = degree.y, strength_from = strength.x, strength_to = strength.y)
  
# print sim2
sim2


# arrange sim2 in decreasing order of similarity. 
sim2 %>% arrange(-similarity)

# filter sim2, allowing only pairs with a degree of least 10, arrange the result in decreasing order of similarity
sim2 %>%
  filter(degree_from >= 10, degree_to >= 10) %>%
  arrange(-similarity)

# Repeat the previous steps, but in increasing order of similarity
sim2 %>%
  filter(degree_from >= 10, degree_to >= 10) %>%
  arrange(similarity)


# filter the similarity data frame to similarities larger than or equal to 0.60
sim3 <- filter(sim2, similarity >= 0.6)

# build a similarity network called h2 from the filtered similarity data frame
h2 <- graph_from_data_frame(sim3, directed = FALSE)

# visualize the similarity network h2
ggraph(h2, layout = "with_kk") + 
  geom_edge_link(aes(alpha = similarity)) + 
  geom_node_point()

```
  
  
  
***
  
Chapter 4 - Similarity Clusters  
  
Hierarchical clustering - find clusters of similar people:  
  
* Basic idea is to define a measure of similarity, then match the most similar entities to groups, proceeding until there is a single cluster containing everyone  
* The dendrogram (tree diagram) is helpful for viewing this data  
* The similarity measure between individual nodes (person similarity) exists, and needs to be extended to groups  
	* Single-linkage - similarity is the maximum of the similarities of anyone in the groups  
    * Complete-linkage - similarity is the minimum of the similarities of anyone in the groups  
    * Average-linkage - similarity is the average of the simlarities of everyone in the groups  
* The clustering algorithm works as follows  
	* Evaluate simlarity for all node pairs  
    * Assign each node to its own group  
    * Find the pair of groups with the highest simlarity, and join them  
    * Calculate simlarity of this newly formed group to all previously existing entities (groups or individuals)  
    * Repeat until there is just a single cluster remaining  
* The R implementation is hclust()  
    * D <- 1-S  
    * d <- as.dist(D)  
    * cc <- hclust(d, method = "average")  
    * cls <- cutree(cc, k = 4)  
  
Interactive visualizations with visNetwork:  
  
* visNetwork is an interactive package for viewing networks  
	* Many different layouts are available, and you can interact with the nodes and the ties  
    * Can select nodes and see their neighborhoods (nodes within a certain distance)  
    * Can select nodes by name  
    * Can partition nodes in to groups and color, highlight, etc.  
  
Wrap up:  
  
* Analysis of networks with measures of centrality and similarity  
* Visualization of networks, including interactivity  
  
Example code includes:  
```{r eval=FALSE}

# compute a distance matrix
D <- 1 - S

# obtain a distance object 
d <- as.dist(D)

# run average-linkage clustering method and plot the dendrogram 
cc <- hclust(d, method = "average")
plot(cc)

# find the similarity of the first pair of nodes that have been merged 
S[40, 45]


# cut the dendrogram at 4 clusters
cls <- cutree(cc, k = 4)

# add cluster information to the nodes data frame
nodes <- mutate(nodes, cluster = cls)

# print the nodes data frame
nodes


# output the names of terrorists in the first cluster
filter(nodes, cluster == 1) %>% 
    select(name)

# for each cluster select the size of the cluster, the average node degree, and the average node strength and sorts by cluster size
group_by(nodes, cluster) %>%
  summarise(size = n(), 
            avg_degree = mean(degree),
            avg_strength = mean(strength)
            ) %>%
  arrange(-size)


# add cluster information to the network 
V(g)$cluster <- nodes$cluster

# visualize the original network with colored clusters
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight), show.legend=FALSE) + 
  geom_node_point(aes(color = factor(cluster))) +
  labs(color = "cluster")

# facet the network with respect to cluster attribute
ggraph(g, layout = "with_kk") + 
  geom_edge_link(aes(alpha = weight), show.legend=FALSE) + 
  geom_node_point(aes(color = factor(cluster))) +
  facet_nodes(~cluster, scales="free")  +
  labs(color = "cluster")


# convert igraph to visNetwork
data <- visNetwork::toVisNetworkData(g)

# print head of nodes and ties
head(data$nodes)
head(data$edges)

# visualize the network
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300)


# use the circle layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk")

# use the circle layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_in_circle")

# use the grid layout
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_on_grid")


# highlight nearest nodes and ties of the selected node
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(highlightNearest = TRUE) 


# select nodes by id 
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(nodesIdSelection = TRUE)

# set color to cluster and generate network data
V(g)$color = V(g)$cluster
data <- visNetwork::toVisNetworkData(g)

# select by group (cluster)
visNetwork::visNetwork(nodes = data$nodes, edges = data$edges, width = 300, height = 300) %>%
  visNetwork::visIgraphLayout(layout = "layout_with_kk") %>%
  visNetwork::visOptions(selectedBy = "group")

```
  
  
  
***
  
###_Data Privacy and Anaonymization in R_  
  
Chapter 1 - Introduction to Data Privacy  
  
Intro to Anonymization - Part I:  
  
* Need to implement better data privacy techniques - e.g., census data, healthcare data, etc.  
* Need to have data such as individualized health, but not in a manner that identifies specific individuals  
* Topics covered in this course will include  
	* Remove identifiers, synthesize data  
    * Laplace mechnaism for removing names  
    * Differential privacy and post-processing  
    * Release of data using the above techniques  
* Data sets will include White House salaries and male infertility data  
	* One basic technique is removing identifiers, such as replacing names with numbers  
    * Another basic technique is to round continuous values (such as to the nearest 1000)  
  
Intro to Anonymization - Part II:  
  
* Additional approaches include generalization and top/bottom coding  
	* Generalization creates larger buckets of data  
    * Top/bottom is about setting outliers back to a pre-defined top and bottom of the range  
* Additional dplyr functions of interest  
	* count() is used to find the number of observations for each distinct group  
    * whitehouse %>% count(Status)  
    * whitehouse %>% count(Status, Title, sort = TRUE)  # sort=TRUE sorts by descending n  
    * summarize_at() lets you get summary statistics for a key variable  
    * whitehouse %>% summarise_at(vars(Salary), sum)  # vars() holds the bare variables, while sum is the requested function  
    * whitehouse %>% summarise_at(vars(Salary), funs(mean, sd))  # funs() holds the list of functions that you want to apply  
  
Data Synthesis:  
  
* Fake datasets created based on sampling from a probability distribution  
* Goal is a fake dataset (by definition anaonymized) that is statistically similar to the real dataset  
	* For 1/0 data, sampling from the binomial distribution can work well  
    * For bell-shaped data, the normal or log-normal can often work well (though there can be issues with bounding)  
    * Hard-bounding is setting values to a proper max/min, while another approach is to discard the record and sample again  
  
Example code includes:  
```{r}

load("./RInputFiles/dataPriv.RData")


# Preview data
whitehouse

# Set seed
set.seed(42)

# Replace names with random numbers from 1 to 1000
whitehouse_no_names <- whitehouse %>%
    mutate(Name = sample(1:1000, nrow(.), replace=FALSE))

whitehouse_no_names


# Rounding Salary to the nearest ten thousand
whitehouse_no_identifiers <- whitehouse_no_names %>%
    mutate(Salary = round(Salary, -4))

whitehouse_no_identifiers


# Convert the salaries into three categories
whitehouse.gen <- whitehouse %>%
    mutate(Salary = ifelse(Salary < 50000, 0, 
                           ifelse(Salary >= 50000 & Salary < 100000, 1, 2)))

whitehouse.gen


# Bottom Coding
whitehouse.bottom <- whitehouse %>%
    mutate(Salary = pmax(Salary, 45000))

# Filter Results
whitehouse.bottom %>%
    filter(Salary <= 45000)


# View fertility data
fertility

# Number of participants with Surgical_Intervention and Diagnosis
fertility %>%
    summarise_at(vars(Surgical_Intervention, Diagnosis), sum)

# Mean and Standard Deviation of Age
fertility %>%
    summarise_at(vars(Age), funs(mean, sd))

# Counts of the Groups in High_Fevers
fertility %>%
    count(High_Fevers)

# Counts of the Groups in Child_Disease
fertility %>%
    count(Child_Disease, Accident_Trauma)

# Find proportions
fertility %>%
    summarise_at(vars(Accident_Trauma, Surgical_Intervention), mean)


# Set seed
set.seed(42)

# Generate Synthetic data
accident <- rbinom(100, 1, prob=0.440)
surgical <- rbinom(100, 1, prob=0.510)


# Square root Transformation of Salary
whitehouse.salary <- whitehouse %>%
    mutate(Salary = sqrt(Salary))

# Calculate the mean and standard deviation
stats <- whitehouse.salary %>%
    summarize(mean(Salary), sd(Salary))

stats


# Generate Synthetic data
set.seed(42)
salary_transformed <- rnorm(nrow(whitehouse), mean=279, sd=71.8)

# Power transformation
salary_original <- salary_transformed ** 2

# Hard bound
salary <- ifelse(salary_original < 0, 0, salary_original)

```
  
  
  
***
  
Chapter 2 - Introduction to Differential Privacy  
  
Differential Privacy - quantification of privacy loss via a privacy budget:  
  
* The worst-case scenario is that no assumptions are made about data intruders  
	* If an individual is from a small group, their data may be 100% available by looking at statistics in aggregate and statistics for the group that excludes them (everyone else)  
* The privacy budget is defined using epsilon - smaller numbers mean that less information will be made available  
* The general concept is to look at a dataset that includes the segment the individual is in, and a dataset that includes all other segments  
	* The answer sent back to the query will have noise added to it depending on the privacy budget  
* Basically, the differential privacy algorithm finds the most "unique" person in the dataset, and then decides how much noise to add based on how identifiable they are by attribute  
  
Global Sensitivity - usual decision-making factor for differential privacy:  
  
* The global sensitivity of a query is the most a variable could change based on removing one individual  
	* By definition, count queries always have a global sensitivity of 1 (exclude 1 individual)  
    * Therefore, proportion queries always have a global sensitity of 1/n  
    * Mean queries always have a global sensitivity of (max - min) / n  
    * Variance queries always have a global sensitivity of (max - min)^2 / n  
* The global sensitivity and the epsilon work together to determine the amount of noise  
	* Measures like median are not very sensitive to outliers, and thus very little noise needs to be added  
    * Measures like maximum are very sensitive to outliers (e.g., Bill Gates income), and thus very little noise needs to be added  
  
Laplace Mechanism - adds noise based on the Laplace distribution with mean 0 and parameters global sensitivity and privacy budget:  
  
* fertility %>% summarise_at(vars(Child_Disease), sum)  
* library(smoothmest)  # has function rdoublex(draws, mean, shaping) - set draws=1, mean=true_mean, shaping=globalSensitivity / epsilon  
  
Example code includes:  
```{r}

# Number of observations
n <- nrow(fertility)

# Global sensitivity of counts
gs.count <- 1

# Global sensitivity of proportions
gs.prop <- 1/n


# Lower bound of Hours_Sitting
a <- 0

# Upper bound of Hours_Sitting
b <- 1

# Global sensitivity of mean for Hours_Sitting
gs.mean <- (b - a) / n

# Global sensitivity of proportions Hours_Sitting
gs.var <- (b - a)**2 / n


# How many participants had a Surgical_Intervention?
fertility %>%
   summarise_at(vars(Surgical_Intervention), sum)

# Set the seed
set.seed(42)

# Apply the Laplace mechanism
eps <- 0.1
smoothmest::rdoublex(1, 51, 1/eps)


# Proportion of Accident_Trauma
stats <- fertility %>%
   summarise_at(vars(Accident_Trauma), mean)

stats

# Set the seed
set.seed(42)

# Apply the Laplace mechanism
eps <- 0.1
smoothmest::rdoublex(1, 0.440, (1/n)/eps)


# Mean and Variance of Hours Sitting
fertility %>%
    summarise_at(vars(Hours_Sitting), funs(mean, var))

# Setup
set.seed(42)
eps <- 0.1

# Laplace mechanism to mean
smoothmest::rdoublex(1, 0.41, gs.mean/eps)

# Laplace mechanism to variance
smoothmest::rdoublex(1, 0.03, gs.var/eps)

```
  
  
  
***
  
Chapter 3 - Differentially Private Properties  
  
Sequential Composition - method to require that someone cannot find the real answer by just sending multiple queries:  
  
* Idea is that the privacy budget is divided by the number of queries you plan to send  
* For example, if a query will be made for mean and another query will be made for maximum, then epsilon needs to be divided by two  
  
Parallel Composition - method to account for queries to different parts of the database (no adjustment to epsilon needed):  
  
* Deciding between sequential and parallel is whether queries could be answered using completely different (MECE) splits of the dataset  
  
Post-processing:  
  
* When new queries can be answered using data that has already been privatized, it can be synthesized to a noisy answer to this new query  
	* The privacy budget need not be adjusted in this case  
    * For example, if there are three groups, can just add noise to two of the groups and let the third group be total minus these two groups  
  
Impossible and inconsistent answers:  
  
* Bounding can be introduced, such as making all negative numbers zero or anything greater than the total to the total  
	* rdoublex(1, 12, gs.count / eps) %>% round() %>% max(0)  # lower bound is zero  
    * normalized <- (smoking/sum(smoking)) * (nrow(fertility))  # upper bound is the size of the dataset  
  
Example code includes:  
```{r}

# Set Value of Epsilon
eps <- 0.1 / 2

# Number of observations
n <- nrow(fertility)

# Lower bound of Age
a <- 0

# Upper bound of Age
b <- 1

# GS of counts for Diagnosis
gs.count <- 1

# GS of mean for Age
gs.mean <- (b-a)/n


# Number of Participants with abnormal diagnosis
stats1 <- fertility %>% 
    summarize_at(vars(Diagnosis), sum)

stats1

# Mean of age
stats2 <- fertility %>%
    summarize_at(vars(Age), mean)

stats2


# Set seed
set.seed(42)

# Laplace mechanism to the count of abnormal diagnosis
smoothmest::rdoublex(1, 12, gs.count/eps)

# Laplace mechanism to the mean of age
smoothmest::rdoublex(1, 0.67, gs.mean/eps)


# Set Value of Epsilon
eps <- 0.1

# Mean of Age per diagnosis level 
fertility %>%
  group_by(Diagnosis) %>%
  summarise_at(vars(Age), mean)


# Set the seed
set.seed(42)

# Laplace mechanism to the mean age of participants with an abnormal diagnoisis
smoothmest::rdoublex(1, 0.71, gs.mean/eps)

# Laplace mechanism to the mean age of participants with a normal diagnoisis
smoothmest::rdoublex(1, 0.66, gs.mean/eps)


# Set Value of Epsilon
eps <- 0.5/3

# GS of Counts
gs.count <- 1

# Number of participants in each of the four seasons
fertility %>%
    group_by(Diagnosis) %>%
    summarise_at(vars(Age), mean)

# Set the seed
set.seed(42)

# Laplace mechanism to the number of participants who were evaluated in the winter, spring, and summer
winter <- smoothmest::rdoublex(1, 28, gs.count / eps) %>%
    round()

spring <- smoothmest::rdoublex(1, 37, gs.count / eps) %>%
    round()

summer <- smoothmest::rdoublex(1, 4, gs.count / eps) %>%
    round()

# Post-process based on previous queries
fall <- nrow(fertility) - winter - spring - summer


# Set Value of Epsilon
eps <- 0.01

# GS of counts
gs.count <- 1

# Number of Participants with Child_Disease
fertility %>%
    summarise_at(vars(Child_Disease), sum)

# Apply the Laplace mechanism
set.seed(42)
lap_childhood <- smoothmest::rdoublex(1, 87, gs.count / eps) %>%
    round()

# Total number of observations in fertility
max_value <- nrow(fertility)

# Bound the value such that the noisy answer does not exceed the total number of observations
ifelse(lap_childhood > max_value, max_value, lap_childhood)


# Set the seed
set.seed(42)

# Apply the Laplace mechanism
fever1 <- smoothmest::rdoublex(1, 9, gs.count/eps) %>%
    max(0)
fever2 <- smoothmest::rdoublex(1, 63, gs.count/eps) %>%
    max(0)
fever3 <- smoothmest::rdoublex(1, 28, gs.count/eps) %>%
    max(0)

fever <- c(fever1, fever2, fever3)

# Normalize noise 
fever_normalized <- (fever/sum(fever)) * (nrow(fertility))

# Round the values
round(fever_normalized)

```
  
  
  
***
  
Chapter 4 - Differentially Private Data Synthesis  
  
Laplace Sanitizer - basic way to generate "noisy" categorical data:  
  
* Takes advantage of parallel - if the data can be binned or placed in a contingency table, assumes no more need to divide the privacy budget  
	* Since the data is queries as a histogram, it can be considered disjoint (non-overlapping) and thus parallel composition  
* Can generate data using rep() for a single vector  
  
Parametric Approaches:  
  
* Sampling from a binomial distribution (where appropriate), with a known proportion that has been modified by Laplace differential privacy guarantee  
* Sampling from a normal or log-normal distribution (where appropriate), with a known mean and variance that has been modified by Laplace differential privacy guarantee  
  
Wrap up:  
  
* Basics of anonymyzing data, such as removing names  
* Basics of modifying data such as generalizing to categorical data  
* Basics of generating synthetic data using rbinom() and rnorm()  
* Basics of privacy budgets, global sensitivities, and the Laplace mechanism  
* Basics of differential privacy, such as sequential (split epsilon) or parallel (including through binning or continegnecy tables)  
* Basics of the Laplace sanitizer for both categorical data (rbinom) and continuous data (rnorm)  
* Next steps include managing data gaps, incorrect statistics distributions with hard bounding, etc.  
	* Local differential privacy (Apple) and probabilistic differential privacy (US census)  
    * Techniques specific to GPS data or PCA  
  
Example code includes:  
```{r}

# Set Value of Epsilon
eps <- 0.1

# GS of Counts
gs.count <- 1

# Number of participants in each season
fertility %>%
    count(Season)


# Set the seed
set.seed(42)

# Apply the Laplace mechanism 
winter <- smoothmest::rdoublex(1, 28, gs.count/eps) %>% max(0)
spring <- smoothmest::rdoublex(1, 37, gs.count/eps) %>% max(0)
summer <- smoothmest::rdoublex(1, 4, gs.count/eps) %>% max(0)
fall <- smoothmest::rdoublex(1, 31, gs.count/eps) %>% max(0)


# Store noisy results
seasons <- c(winter = winter, spring = spring, summer = summer, fall = fall)

# Normalizing seasons
seasons_normalized <- (seasons/sum(seasons)) * nrow(fertility)

# Round the values
round(seasons_normalized)

# Generate synthetic data for winter
rep(-1, 29)

# Generate synthetic data for spring
rep(-0.33, 38)

# Generate synthetic data for summer
rep(0.33, 0)

# Generate synthetic data for fall
rep(1, 33)


# Calculate proportions
fertility %>%
    summarise_at(vars(Accident_Trauma, Surgical_Intervention), mean)

# Number of Observations
n <- nrow(fertility)

# Set Value of Epsilon
eps <- 0.1

# GS of Proportion
gs.prop <- (1/n)


# Apply the Laplace mechanism
set.seed(42)
smoothmest::rdoublex(1, 0.44, gs.prop/eps)
smoothmest::rdoublex(1, 0.51, gs.prop/eps)

# Generate Synthetic data
set.seed(42)
accident <- rbinom(n, 1, 0.46)
surgical <- rbinom(n, 1, 0.54)


# Set Value of Epsilon
eps <- 0.1 / 2

# Number of observations
n <- nrow(fertility)

# Upper and lower bounds of age
a <- 0
b <- 1

# GS of mean and variance for age
gs.mean <- (b-a) / n
gs.var <- (b-a)**2 / n


# Mean and Variance of Age
fertility %>%
    summarise_at(vars(Age), funs(mean, var))

# Apply the Laplace mechanism
set.seed(42)
smoothmest::rdoublex(1, 0.67, gs.mean/eps)
smoothmest::rdoublex(1, 0.01, gs.var/eps)


# Generate Synthetic data
set.seed(42)
age <- rnorm(n, mean=0.71, sd=sqrt(0.07))

# Hard Bounding the data
age[age < 0] <- 0
age[age > 1] <- 1

```
  
  
  
*** 
  
###_Marketing Analytics in R: Statistical Modeling_  
  
Chapter 1 - Modeling Customer Lifetime Value with Linear Regression  
  
Introduction - Verena from INWT Statistics (consultancy in marketing analytics):  
  
* Customer Lifetime Value (CLV) is the expected value of forecasted customer value to the company  
	* CLV is based on margin, and needs to use current information to predict future margins  
    * Customers predicted to have higher CLV can then be targeted  
* Can inspect the data without seeing attributes using str(clvData1, give.attr = FALSE)  
* Can derive correlations using corrplot  
	* library(corrplot)  
    * clvData1 %>% select(nOrders, nItems, ... ,margin, futureMargin) %>% cor() %>% corrplot()  
  
Simple linear regression - one predictor variable to predict one response variable:  
  
* Can run linear regressions using basic stats modules  
	* simpleLM <- lm(futureMargin ~ margin, data = clvData1)  
    * summary(simpleLM)  
* Can plot previous margin vs. current margin, including a linear regression (smooth)  
	* ggplot(clvData1, aes(margin, futureMargin)) + geom_point() + geom_smooth(method = lm, se = FALSE) + xlab("Margin year 1") + ylab("Margin year 2")  
* Several conditions must apply for linear regression to be the best method  
	* Linear relationship between x and y  
    * No measurement error in x (weak exogeneity)  
    * Independence of errors  
    * Expectation of errors is 0  
    * Constant variance of prediction errors (homoscedasticity)  
    * Normality of errors  
  
Multiple linear regression:  
  
* Omitted variable bias is when a variable not in the regression is correlated with both the predictor and the response variables  
	* Simpson's Paradox is an example - upward sloping becomes downward sloping after properly splitting on the extra variable  
* Multicollinearity is a threat to a linear regression - leads to unstable regression coefficients, with associated under-reporting of standard errors  
	* rms::vif(myLMModel)  # above 5 is concerning, above 10 almost always needs to be addressed  
  
Model validation, fit, and prediction:  
  
* The R-squared is the proportion of variance in the depedent variable that is explained by the regression  
* Can look at the p-value of the F-test to assess the overall statistical significance of the model  
* There is a risk of over-fitting, when the model is overly complex and learns artifacts of the training data rather than genuine patterns  
	* Can use stats::AIC() or MASS::stepAIC(), with the goal being to minimize AIC (needs to be models of the same data)  
    * AIC(multipleLM2)  
* Can predict outputs automatically, such as with  
	* predMargin <- predict(multipleLM2, newdata = clvData2)  
  
Example code includes:  
```{r}

salesData <- readr::read_csv("./RInputFiles/salesData.csv")

# Structure of dataset
str(salesData, give.attr = FALSE)

# Visualization of correlations
salesData %>% select_if(is.numeric) %>%
  select(-id) %>%
  cor() %>%
  corrplot::corrplot()

# Frequent stores
ggplot(salesData) +
    geom_boxplot(aes(x = mostFreqStore, y = salesThisMon))

# Preferred brand
ggplot(salesData) +
    geom_boxplot(aes(x = preferredBrand, y = salesThisMon))


# Model specification using lm
salesSimpleModel <- lm(salesThisMon ~ salesLast3Mon, data = salesData)

# Looking at model summary
summary(salesSimpleModel)


# Estimating the full model
salesModel1 <- lm(salesThisMon ~ . -id, data = salesData)

# Checking variance inflation factors
car::vif(salesModel1)

# Estimating new model by removing information on brand
salesModel2 <- lm(salesThisMon ~ . -id -preferredBrand -nBrands, data = salesData)

# Checking variance inflation factors
car::vif(salesModel2)


salesData2_4 <- readr::read_csv("./RInputFiles/salesDataMon2To4.csv")

# getting an overview of new data
summary(salesData2_4)

# predicting sales
predSales5 <- predict(salesModel2, newdata = salesData2_4)

# calculating mean of future sales
mean(predSales5)

```
  
  
  
***
  
Chapter 2 - Logistic Regression for Churn Prevention  
  
Churn prevention in online marketing:  
  
* Objective is to predict the likelihood of a customer repeating their business, assessed using logistic regression  
	* Model the log-odds (defined as log (P(Y=1) / P(Y=0))) as a linear function of the inputs  
    * Convert the log-odds to odds (defined as P(Y=1) / P(Y=0)) by exponentiation  
    * Convert the odds to a probability of churning, using odds / (1 + odds)  
* Can begin with basic data exploration  
	* ggplot(churnData, aes(x = returnCustomer)) + geom_histogram(stat = "count")  
  
Modeling and model selection:  
  
* The logit model can be run using the GLM provided in R  
	* logitModelFull <- glm(returnCustomer ~ title + newsletter + websiteDesign + ..., family = binomial, churnData)  
* Interpreting the coefficients is not easy - they are related to the log-odds  
	* Can exponentiate the coefficients to get their impact on the odds  
    * Can then interpret that greater than 1 means "more likely, all else equal"  
* Can use MASS::stepAIC() to help refine the modeling  
	* library(MASS)  
    * logitModelNew <- stepAIC(logitModelFull, trace = 0)  
    * summary(logitModelNew)  
    * Produces a model with fewer variables and a lower AIC  
  
In-sample model fit and thresholding:  
  
* There are three types of pseudo-R-squared statistics available for the results of logistical regression  
	* McFadden: R-squared = 1 - L(null) / L(full)  
    * Cox-Snell: R-squared = 1 - (L(null) / L(full)) ** (2/n)  
    * Nagelkerke: R-squared = [1 - (L(null) / L(full)) ** (2/n)] / [1 - L(null) ** (2/n)]  
    * Generally, anything above 0.2 is reasonably good  
    * descr::LogRegR2(logitModelNew)  
    * library(SDMTools)  
    * churnData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)  # get the prediction probabilities  
    * data %>% select(returnCustomer, predNew) %>% tail()  
    * confMatrixNew <- confusion.matrix(churnData$returnCustomer, churnData$predNew, threshold = 0.5)  # this is the version from SDMTools  
* Can give different weights to the different errors (false negatives, false positives, etc.)  
	* Can instead look at a payoff, defined based on scalars for the various quadrants  
  
Out-of-sample validation and cross validation:  
  
* Begin by randomly splitting data in to training (roughly two-thirds) and holding back the remainder for validation (roughly one-third)  
	* set.seed(534381)  
    * churnData$isTrain <- rbinom(nrow(churnData), 1, 0.66)  
    * train <- subset(churnData, churnData$isTrain == 1)  
    * test <- subset(churnData, churnData$isTrain == 0)  
    * test$predNew <- predict(logitTrainNew, type = "response", newdata = test)  # make predictions only on the test dataset  
* Cross-validation is an even more powerful tool for assessing out-of-sample error  
	* Split the data in to k subsets, and run the model k times with k-1 training data and the last subset used as the validation data  
    * Acc03 <- function(r, pi = 0) {  
    *   cm <- confusion.matrix(r, pi, threshold = 0.3)  
    *   acc <- sum(diag(cm)) / sum(cm) return(acc)  
    * }  
    * set.seed(534381)  
    * boot::cv.glm(churnData, logitModelNew, cost = Acc03, K = 6)$delta  
* Can continually tweak the model to see if transforms, variable additions, etc., might tend to improve the out-of-sample error rate  
  
Example code includes:  
```{r}

defaultData <- readr::read_delim("./RInputFiles/defaultData.csv", delim=";")

# Summary of data
summary(defaultData)

# Look at data structure
str(defaultData, give.attr=FALSE)

# Analyze the balancedness of dependent variable
ggplot(defaultData, aes(x = PaymentDefault)) +
  geom_histogram(stat = "count") 


# Build logistic regression model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
                   age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + 
                   billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + 
                   payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, 
                family = "binomial", data = defaultData)

# Take a look at the model
summary(logitModelFull)

# Take a look at the odds
coefsexp <- coef(logitModelFull) %>% exp() %>% round(2)
coefsexp


# The old (full) model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
                   age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + 
                   billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + 
                   payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, 
                 family = binomial, defaultData)

#Build the new model
logitModelNew <- MASS::stepAIC(logitModelFull, trace=0) 

#Look at the model
summary(logitModelNew) 

# Save the formula of the new model (it will be needed for the out-of-sample part) 
formulaLogit <- as.formula(summary(logitModelNew)$call)
formulaLogit


# Make predictions using the full Model
defaultData$predFull <- predict(logitModelFull, type = "response", na.action = na.exclude)

# Construct the in-sample confusion matrix
confMatrixModelFull <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                                  defaultData$predFull, 
                                                  threshold = 0.5
                                                  )
confMatrixModelFull

# Calculate the accuracy for the full Model
accuracyFull <- sum(diag(confMatrixModelFull)) / sum(confMatrixModelFull)
accuracyFull


# Calculate the accuracy for 'logitModelNew'
# Make prediction
defaultData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)

# Construct the in-sample confusion matrix
confMatrixModelNew <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                                 defaultData$predNew, 
                                                 threshold = 0.5
                                                 )
confMatrixModelNew

# Calculate the accuracy...
accuracyNew <- sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew)
accuracyNew

# and compare it to the full model's accuracy
accuracyFull
accuracyNew


# Prepare data frame with threshold values and empty payoff column
payoffMatrix <- data.frame(threshold = seq(from = 0.1, to = 0.5, by = 0.1), payoff = NA) 
payoffMatrix
 
for(i in 1:length(payoffMatrix$threshold)) {
  # Calculate confusion matrix with varying threshold
  confMatrix <- SDMTools::confusion.matrix(defaultData$PaymentDefault, 
                                           defaultData$predNew, 
                                           threshold = payoffMatrix$threshold[i]
                                           )
  # Calculate payoff and save it to the corresponding row
  payoffMatrix$payoff[i] <- confMatrix[1, 1]*250 + confMatrix[1, 2]*(-1000)
}
payoffMatrix


# Split data in train and test set
set.seed(534381) 
defaultData$isTrain <- rbinom(nrow(defaultData), 1, 0.66)
train <- subset(defaultData, isTrain == 1)
test <- subset(defaultData, isTrain  == 0)

logitTrainNew <- glm(formulaLogit, family = binomial, data = train) # Modeling
test$predNew <- predict(logitTrainNew, type = "response", newdata = test) # Predictions

# Out-of-sample confusion matrix and accuracy
confMatrixModelNew <- SDMTools::confusion.matrix(test$PaymentDefault, test$predNew, threshold = 0.3) 
sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew) # Compare this value to the in-sample accuracy


# Accuracy function
costAcc <- function(r, pi = 0) {
  cm <- SDMTools::confusion.matrix(r, pi, threshold = 0.3)
  acc <- sum(diag(cm)) / sum(cm)
  return(acc)
}

# Cross validated accuracy for logitModelNew
set.seed(534381)
boot::cv.glm(defaultData, logitModelNew, cost = costAcc, K = 6)$delta[1]

```
  
  
  
***
  
Chapter 3 - Modeling Time to Reorder with Survival Analysis  
  
Survival Analysis Introduction:  
  
* Often have "censored" data, meaning that the customer journeys are not yet complete  
	* Random Type I Right censoring is the most common - a point can only be observed if it has occurred before time X, and it is otherwise unknowable (but known that they have not yet churned)  
    * Can plot histograms of whether someone has churned depending on the length of time  
    * plotTenure <- dataSurv %>% mutate(churn = churn %>% factor(labels = c("No", "Yes"))) %>%  
    *   ggplot() + geom_histogram(aes(x = tenure, fill = factor(churn))) + facet_grid( ~ churn) +           
    *   theme(legend.position = "none")  
* Survival analysis attempts to estimate when something will happen (churn, second order, renewal, etc.)  
  
Survival curve analysis by Kaplan-Meier:  
  
* Begin by creating a new object containing the survival attribute  
	* cbind(dataSurv %>% select(tenure, churn), surv = Surv(dataSurv$tenure, dataSurv$churn)) %>% head(10)  
* The survival function is the probability of "no event" in cumulative by time t  
	* The hazard function is the cumulative probability of "event" by time t  
    * The "hazard rate" is the probability of the event happening in a small time, provided that it has not yet happened  
* The Kaplan-Meier analysis can be used to estimate survival  
	* fitKM <- survival::survfit(Surv(dataSurv$tenure, dataSurv$churn) ~ 1, type = "kaplan-meier")  
    * print(fitKM)  # gives a few rough summary statistics  
    * plot(fitKM) # survival curve with confidence interval  
    * fitKMstr <- survfit(Surv(tenure, churn) ~ Partner, data = dataSurv)  # add covariates, such as ~ Partner rather than ~1 as in the baseline  
  
Cox PH model with constant covariates:  
  
* Model definition: cannot parse to ISO - see Excel notes  
	* Predictors are lineary and multiplicatively related to the hazard function, lambda  
    * Relative hazard function needs to remain constant over time  
* Fitting a survival model in R  
	* library(rms)  
    * units(dataSurv$tenure) <- "Month"  
    * dd <- datadist(dataSurv)  
    * options(datadist = "dd")  
    * fitCPH1 <- cph(Surv(tenure, churn) ~ gender + SeniorCitizen + Partner + Dependents + StreamMov + PaperlessBilling + PayMeth + MonthlyCharges, data = dataSurv, x = TRUE, y = TRUE, surv = TRUE, time.inc = 1)  
    * Coefficient interpretation is relatively similar to logistic regression - exp(fitCPH1$coefficients) - can simplify the coefficients be making them multiplicative (1.00 is no impact)  
    * survplot(fitCPH1, MonthlyCharges, label.curves = list(keys = 1:5))  # plots the survival probabilities based on varying 1 variable, assuming other variables constant  
    * survplot(fitCPH1, Partner)  # covariate with partner, plotted  
    * plot(summary(fitCPH1), log = TRUE)  # visualizing the hazard ratios  
  
Checking model assumptions and making predictions:  
  
* Can again use the Cox PH function  
	* testCPH1 <- cox.zph(fitCPH1)  
    * print(testCPH1)  # if p < 0.05, can reject the assumption that the predictor meets the proportional hazard assumption  
    * plot(testCPH1, var = "Partner=Yes")  
    * plot(testCPH1, var = "MonthlyCharges")  
    * This test is conservative and sensitive to the number of observations  
* If the PH (proportional hazard) assumptions are violated, can correct for this using  
	* fitCPH2 <- cph(Surv(tenure, churn) ~ MonthlyCharges + SeniorCitizen + Partner + Dependents + StreamMov + Contract, stratum = "gender = Male", data = dataSurv, x = TRUE, y = TRUE, surv = TRUE)  
    * rms::validate(fitCPH1, method = "crossvalidation", B = 10, pr = FALSE)  # pr=FALSE means only print at the end; R2 is the R-squared corrected by cross-validation  
* Can then assess probabilities for the event to occur  
	* oneNewData <- data.frame(gender = "Female", SeniorCitizen = "Yes", Partner = "No", Dependents = "Yes", StreamMov = "Yes", PaperlessBilling = "Yes", PayMeth = "BankTrans(auto)", MonthlyCharges = 37.12)  
    * str(survest(fitCPH1, newdata = oneNewData, times = 3))  
    * plot(survfit(fitCPH1, newdata = oneNewData))  
    * print(survfit(fitCPH1, newdata = oneNewData))  
  
Example code includes:  
```{r}

survData <- readr::read_delim("./RInputFiles/survivalDataExercise.csv", delim=",")


dataNextOrder <- survData %>%
    select(daysSinceFirstPurch, boughtAgain)

# Look at the head of the data
head(dataNextOrder)

# Plot a histogram
ggplot(dataNextOrder) +
  geom_histogram(aes(x = daysSinceFirstPurch, fill = factor(boughtAgain))) +
  facet_grid( ~ boughtAgain) + # Separate plots for boughtAgain = 1 vs. 0
  theme(legend.position = "none") # Don't show legend


# Create survival object
survObj <- survival::Surv(dataNextOrder$daysSinceFirstPurch, dataNextOrder$boughtAgain)

# Look at structure
str(survObj)


# Compute and print fit
fitKMSimple <- survival::survfit(survObj ~ 1)
print(fitKMSimple)

# Plot fit
plot(fitKMSimple, conf.int = FALSE, xlab = "Time since first purchase", 
     ylab = "Survival function", main = "Survival function"
     )


dataNextOrder <- survData %>%
    select(daysSinceFirstPurch, boughtAgain, voucher)

# Compute fit with categorical covariate
fitKMCov <- survival::survfit(survObj ~ voucher, data = dataNextOrder)

# Plot fit with covariate and add labels
plot(fitKMCov, lty = 2:3, xlab = "Time since first purchase", 
     ylab = "Survival function", main = "Survival function"
     )
legend(90, .9, c("No", "Yes"), lty = 2:3)


dataNextOrder <- survData

# Determine distributions of predictor variables
dd <- rms::datadist(dataNextOrder)
options(datadist = "dd")

# Compute Cox PH Model and print results
fitCPH <- rms::cph(survival::Surv(daysSinceFirstPurch, boughtAgain) ~ 
                       shoppingCartValue + voucher + returned + gender, data = dataNextOrder, 
                   x = TRUE, y = TRUE, surv = TRUE
                   )
print(fitCPH)

# Interpret coefficients
exp(fitCPH$coefficients)

# Plot result summary
plot(summary(fitCPH), log = TRUE)


# Check proportional hazard assumption and print result
testCPH <- survival::cox.zph(fitCPH)
print(testCPH)

# Plot time-dependent beta
plot(testCPH, var = "gender=male")

# Validate model
rms::validate(fitCPH, method = "crossvalidation", B = 10, dxy = TRUE, pr = FALSE)


# Create data with new customer
newCustomer <- data.frame(daysSinceFirstPurch = 21, shoppingCartValue = 99.9, gender = "female", 
                          voucher = 1, returned = 0, stringsAsFactors = FALSE
                          )

# Make predictions
pred <- survival::survfit(fitCPH, newdata = newCustomer)
print(pred)
plot(pred)

# Correct the customer's gender
newCustomer2 <- newCustomer
newCustomer2$gender <- "male"

# Redo prediction
pred2 <- survival::survfit(fitCPH, newdata = newCustomer2)
print(pred2)

```
  
  
  
***
  
Chapter 4 - Reducing Dimensionality with Principal Component Analysis  
  
PCA for CRM Data - address mutlicollinearity and data volume issues in the raw CRM data:  
  
* PCA reduces a large number of correlated variables to a smaller number of uncorrelated (orthogonal) variables  
* PCA can also help with creating an index, such as using the first component of the PCA  
* All variables must be either continuous or binary prior to running the PCA analysis  
	* dataCustomers %>% cor() %>% corrplot()  # plot the initial correlations  
  
PCA Computation:  
  
* Need to manage for variance, otherwise high-variance variables will be over-represented in the PCA  
	* lapply(dataCustomers, var)  
    * dataCustomers <- dataCustomers %>% scale() %>% as.data.frame()  
    * pcaCust <- prcomp(dataCustomers)  
    * pcaCust$sdev %>% round(2)  # standard deviations by component  
    * pcaCust$sdev ^ 2 %>% round(2)  # variances, also known as eigenvalues, by component give a good sense for relative importance (relative ratio is percent of variance explained)  
    * round(pcaCust$rotation[, 1:6], 2)  # correlations between original variables and principal components (can use these to give descriptive names to components)  
* Values of the observations are the weightings for the PC to make up the underlying data  
	* sum(dataCustomers[1,] * pcaCust$rotation[,1])  # Value on 1st component for 1st customer  
    * pcaCust$x[1:5, 1:6]  # first 5 customers and first 6 component loadings (weightings)  
  
PCA Model Specification:  
  
* Need to decide on how many components to keep - balance size of data vs. reconstruction of original data  
	* Can set a minimum requirement for percentage of variance explained (such as 70%)  
    * summary(pcaCust)  # will show cumulatives also  
    * Can use the Kaiser-Guttman criteria, which keeps only components with an eigenvalue of 1 (since 1 is the average)  
    * Can also draw a scree plot to see the variances (eigenvalues) in descending order - look for an elbow  
    * screeplot(pcaCust, type = "lines")  
    * Generally, use a few different techniques, and pick a number that is "in the range"  
* The biplot can help to show how the data map on to the principal components  
	* biplot(pcaCust, choices = 1:2, cex = 0.7)  # will show PC1 and PC2, with arrows for the various features and how they map on them  
  
Principal components in a regression analysis:  
  
* PCA can help to solve the multi-collinearity problem in a regression  
	* dataCustComponents <- cbind(dataCustomers[, "customerSatis"], pcaCust$x[, 1:6]) %>% as.data.frame  
    * mod2 <- lm(customerSatis ~ ., dataCustComponents)  
    * vif(mod2)  # by construction, these will all be 1, since the principal components are orthogonal  
* Factor analysis is another dimension-reduction technique, sometimes confused with PCA  
	* Factor analysis theorizes that latent constructs (e.g., intelligence) which cannot be directly measured are influencing the observed variables  
    * Factor analysis is often used in questionnaires - factor analysis can investigate where multiple questions really just measure one thing  
    * In contrast, with PCA, the features are actually being combined to model the data  
  
Wrap up:  
  
* Logistic regression for churn  
* Survival analysis to prevent churn  
* Principal component analysis (PCA) to reduce multicollinearity  
  
Example code includes:  
```{r}

load("./RInputFiles/newsData.RData")

rawData <- newsData
newsData <- newsData[, c('n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'num_keywords', 'is_weekend', 'kw_avg_min', 'kw_avg_avg', 'kw_avg_max', 'average_token_length', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words', 'avg_positive_polarity', 'avg_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity')]


# Overview of data structure:
str(newsData, give.attr = FALSE)

# Correlation structure:
newsData %>% cor() %>% corrplot::corrplot()


# Standardize data
newsData <- newsData %>% scale() %>% as.data.frame()

# Compute PCA
pcaNews <- newsData %>% prcomp()

# Eigenvalues
pcaNews$sdev**2


# Screeplot:
screeplot(pcaNews, type = "lines")

# Cumulative explained variance:
summary(pcaNews)

# Kaiser-Guttmann (number of components with eigenvalue larger than 1):
sum(pcaNews$sdev > 1)


# Print loadings of the first six components
pcaNews$rotation[, 1:6] %>% round(2)

pcaNews %>% biplot(choices=1:2, cex = 0.5)


# Predict log shares with all original variables
logShares <- rawData %>%
    select(shares) %>%
    mutate(logShares=log(1+shares)) %>%
    pull(logShares) %>%
    scale()

newsData <- newsData %>%
    cbind(logShares)

mod1 <- lm(logShares ~ ., data = newsData)

# Create dataframe with log shares and first 6 components
dataNewsComponents <- cbind(logShares = newsData[, "logShares"], pcaNews$x[, 1:6]) %>%
  as.data.frame()

# Predict log shares with first six components
mod2 <- lm(logShares ~ ., data = dataNewsComponents)

# Print adjusted R squared for both models
summary(mod1)$adj.r.squared
summary(mod2)$adj.r.squared

```
  
  
  
***
  
###_Interactive Maps with leaflet in R_  
  
Chapter 1 - Setting Up Interactive Web Maps  
  
Introduction to leaflet - open-source JavaScript library that makes interactive, mobile-friendly maps:  
  
* Objective for this course is to build up to an interactive map of 4-year colleges, including incorporation type (public, private, etc.)  
	* Additionally, labels that occur when hovering  
* Leaflet builds maps using tiles, which join many smaller maps together  
	* library(leaflet)  
    * leaflet() %>% addTiles()  # zooming and scrolling lead to new tiles being shown  
* In Chapter 1, will use multiple tile types to create maps of the DataCamp HQ in Belgium and Boston  
	* leaflet() %>% addProviderTiles("CartoDB") %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  
  
Map tiles - over 100 pre-canned maps that are available as bases:  
  
* Selecting a base map - consider the intended purpose of the map, and ensure that the maps selected meet that purpose  
	* Instructor has a preference for gray-scale maps (for ease of seeing other data)  
* The base maps are stored as "providers" - most are available for immediate use, but a few require registration  
	* names(providers)  # get all the available providers  
    * names(providers)[str_detect(names(providers), "OpenStreetMap")]  # all from OpenStreetMap  
    * leaflet() %>% # addTiles() addProviderTiles("OpenStreetMap.BlackAndWhite")  # replace the default with the BW OpenStreetMap  
  
Setting the default map view:  
  
* Can load the map centered on a specific point and with a requested zoom level - coomon to use ggmap::geocode()  
	* ggmap::geocode("350 5th Ave, New York, NY 10118")   # will return the lat-lon where possible (uses google API unless source="dsk" is chosen)  
* Can use either setView() or fitBounds()  
	* leaflet() %>% addTiles() %>% setView(lng = -73.98575, lat = 40.74856, zoom = 13)  # setView picks a lat/lon and zoom  
    * leaflet() %>% addTiles() %>% fitBounds( lng1 = -73.910, lat1 = 40.773, lng2 = -74.060, lat2 = 40.723)  # fitBounds defines a rectangle  
* Can limit user controls such as panning and zooming  
	* leaflet(options = leafletOptions(dragging = FALSE, minZoom = 14, maxZoom = 18)) %>% addProviderTiles("CartoDB") %>% setView(lng = -73.98575, lat = 40.74856, zoom = 18)  
    * dragging=FALSE removes the ability to pan  
    * maxZoom and minZoom limit the options for zooming  
    * leaflet() %>% addTiles() %>% setView(lng = -73.98575, lat = 40.74856, zoom = 18) %>% setMaxBounds(lng1 = -73.98575, lat1 = 40.74856, lng2 = -73.98575, lat2 = 40.74856)  
    * setMaxBounds() limits the user to the boundaries that you pre-specify  
* For more information, can go to  
	* http://leafletjs.com/reference-1.3.0.html  
    * https://rstudio.github.io/leaflet/  
  
Plotting DataCamp HQ:  
  
* Location markers are a common addition, managed using addMarkers()  
	* leaflet() %>% addTiles() %>% addMarkers(lng = -73.98575, lat = 40.74856)  
    * If single vectors are passed to lng and lat, then a single blue pin will be placed and the map will be centered/zoomed there  
    * dc_hq <- tibble( hq = c("DataCamp - NYC", "DataCamp - Belgium"), lon = c(-73.98575, 4.717863), lat = c(40.74856, 50.881363))  
    * leaflet() %>% addTiles() %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat)  
    * When the tibble is passed, then the map will be zoomed/centered such that all the pins can be displayed
dc_hq %>% leaflet() %>% addTiles() %>% addMarkers()   
    * The functions will seek a lat and lon column from the piped in data (dc_hq in this case), and pass along a note that they were used  
* Pop-ups are a common way to provide additional information about a marker  
	* leaflet() %>% addTiles() %>% addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  # markers, with popup enabled on clicking  
    * leaflet() %>% addTiles() %>% addPopups(lng = dc_hq$lon, lat = dc_hq$lat, popup = dc_hq$hq)  # popups instead of markers  
* Leaflets can be stored as objects (similar to ggplot2), with additions and prints and whatnot called later  
  
Example code includes:  
```{r}

# Load the leaflet library
library(leaflet)

# Create a leaflet map with default map tile using addTiles()
leaflet() %>%
    addTiles()


# Print the providers list included in the leaflet library
providers

# Print only the names of the map tiles in the providers list 
names(providers)

# Use str_detect() to determine if the name of each provider tile contains the string "CartoDB"
str_detect(names(providers), "CartoDB")

# Use str_detect() to print only the provider tile names that include the string "CartoDB"
names(providers)[str_detect(names(providers), "CartoDB")]


# Change addTiles() to addProviderTiles() and set the provider argument to "CartoDB"
leaflet() %>% 
    addProviderTiles("CartoDB")

# Create a leaflet map that uses the Esri provider tile 
leaflet() %>% 
    addProviderTiles("Esri")

# Create a leaflet map that uses the CartoDB.PositronNoLabels provider tile
leaflet() %>% 
    addProviderTiles("CartoDB.PositronNoLabels")


# Map with CartoDB tile centered on DataCamp's NYC office with zoom of 6
leaflet()  %>% 
    addProviderTiles("CartoDB")  %>% 
    setView(lng = -73.98575, lat = 40.74856, zoom = 6)


dc_hq <- tibble::tibble(hq=c("NYC", "Belgium"), lon=c(-73.98575, 4.71786), lat=c(40.7486, 50.8814))
dc_hq

# Map with CartoDB.PositronNoLabels tile centered on DataCamp's Belgium office with zoom of 4
leaflet() %>% 
    addProviderTiles("CartoDB.PositronNoLabels") %>% 
    setView(lng = dc_hq$lon[2], lat = dc_hq$lat[2], zoom = 4)


leaflet(options = leafletOptions(
                    # Set minZoom and dragging 
                    minZoom = 12, dragging = TRUE))  %>% 
  addProviderTiles("CartoDB")  %>% 
  # Set default zoom level 
  setView(lng = dc_hq$lon[2], lat = dc_hq$lat[2], zoom = 14) %>% 
  # Set max bounds of map 
  setMaxBounds(lng1 = dc_hq$lon[2] + 0.05, 
               lat1 = dc_hq$lat[2] + .05, 
               lng2 = dc_hq$lon[2] - 0.05, 
               lat2 = dc_hq$lat[2] - .05) 


# Plot DataCamp's NYC HQ
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = dc_hq$lon[1], lat = dc_hq$lat[1])

# Plot DataCamp's NYC HQ with zoom of 12    
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = -73.98575, lat = 40.74856)  %>% 
    setView(lng = -73.98575, lat = 40.74856, zoom = 12)    

# Plot both DataCamp's NYC and Belgium locations
leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addMarkers(lng = dc_hq$lon, lat = dc_hq$lat)


# Store leaflet hq map in an object called map
map <- leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    # add hq column of dc_hq as popups
    addMarkers(lng = dc_hq$lon, lat = dc_hq$lat, 
               popup = dc_hq$hq
               )

# Center the view of map on the Belgium HQ with a zoom of 5  
map_zoom <- map %>% 
      setView(lat = 50.881363, lng = 4.717863, zoom = 5)

# Print map_zoom
map_zoom

```
  
  
  
***
  
Chapter 2 - Plotting points  
  
Introduction to IPEDS Data:  
  
* Can clear the boundaries of a map, while keeping everything else (data and the like) constant  
	* m %>% clearBounds()  # kills the bounds layers  
    * m %>% clearBounds() %>% clearMarkers()  # kills the markers layers  
* The IPEDS data is the Integrated Post-Secondary Education dataset - this course uses a subset consisting of 4-year colleges  
	* Goal is to create a subset of the IPEDS data consisting of the ~300 colleges in California  
    * Can then plot and color-code the California colleges  
  
Mapping California colleges:  
  
* Clustered markers are poorly shown by pins due to obscuring  
* A nice alternative is to use circle markers, which have much less tendency for overlaps  
	* maine_colleges_map %>% clearMarkers() %>% addCircleMarkers(data = maine, radius = 3)  
    * maine_colleges_map %>% addCircleMarkers( data = maine_colleges, radius = 4, color = "red", popup = ~name)  # custom color and radius while maintaining popups  
  
Labels and pop-ups:  
  
* Can use piping as well as the tilde, which allows for referring to key variables in the piped in data  
	* ipeds %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers( lng = ~lng, lat = ~lat, popup = ~name, color = "#FF0000")  
    * Colors can be specified using hexadecimal, as shown in the example above - can find these using google and color sliders  
* Can build better popups using pipes and tildes  
	* addCircleMarkers(popup = ~paste0(name, "-", sector_label)  
    * addCircleMarkers(popup = ~paste0("<b>",name,"</b>","<br/>",sector_label))  # enhanced with html tags  
* Labels provide similar information as pop-ups, but require only a hover rather than a click  
	* ipeds %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(label = ~name, radius = 2)  
  
Color coding colleges:  
  
* Can include differential colors depending on a variables that has been piped in using colorFactor()  
	* OR <- ipeds %>% filter(state == "OR")  
    * pal <- colorFactor(palette = c("red", "blue", "#9b4a11"), levels = c("Public", "Private", "For-Profit"))  # create the color palette for future use  
    * oregon_colleges <- OR %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(radius = 2, color = ~pal(sector_label), label = ~name)  # apply as pal()  
    * oregon_colleges %>% addLegend(position = "bottomright", pal = pal, values = c("Public", "Private", "For-Profit"))  # add to legend  
* Can instead color based on a numeric value using colorNumeric()  
	* admit <- admit %>% filter(!is.na(rate), rate < 50, rate > 0)  # filer for rates that exist and are between 0 and 50  
    * pal <- colorNumeric(palette = "Reds", domain = c(1:50), reverse = TRUE)  # reverse=TRUE flips the gradient so that lower admit rates are darker red  
    * admit_map <- admit %>% leaflet() %>% addProviderTiles("CartoDB") %>% addCircleMarkers(radius = 4, color = ~pal(rate), label = ~name) %>% addLegend(title = "Admit Rate", pal = pal, values = c(1:50), position = "bottomright")  
* Can use RColorBrewer for default color palettes  
	* library(RColorBrewer)  
    * display.brewer.all()  
  
Example code includes:  
```{r}

# Remove markers, reset bounds, and store the updated map in the m object
map <- map %>%
    clearMarkers() %>% 
    clearBounds()

# Print the cleared map
map


ipedsRaw <- readr::read_csv("./RInputFiles/ipeds.csv")


# Remove colleges with missing sector information
ipeds <- 
    ipedsRaw %>% 
    tidyr::drop_na()

# Count the number of four-year colleges in each state
ipeds %>% 
    group_by(state)  %>% 
    count()

# Create a list of US States in descending order by the number of colleges in each state
ipeds  %>% 
    group_by(state)  %>% 
    count()  %>% 
    arrange(desc(n))

# Create a dataframe called `ca` with data on only colleges in California
ca <- ipeds %>%
    filter(state == "CA")

map <- leaflet() %>% 
    addProviderTiles("CartoDB")

# Use `addMarkers` to plot all of the colleges in `ca` on the `m` leaflet map
map %>%
    addMarkers(lng = ca$lng, lat = ca$lat)


la_coords <- data.frame(lat = 34.05223, lon = -118.2437) 

# Center the map on LA 
map %>% 
    addMarkers(data = ca) %>% 
    setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 12)

# Set the zoom level to 8 and store in the m object
map_zoom <-
    map %>%
    addMarkers(data = ca) %>%
    setView(lat = la_coords$lat, lng = la_coords$lon, zoom = 8)

map_zoom


# Clear the markers from the map 
map2 <- map %>% clearMarkers()

# Use addCircleMarkers() to plot each college as a circle
map2 %>%
    addCircleMarkers(lng = ca$lng, lat = ca$lat)

# Change the radius of each circle to be 2 pixels and the color to red
map2 %>% 
    addCircleMarkers(lng = ca$lng, lat = ca$lat, radius = 2, color = "red")


# Add circle markers with popups for college names
map %>%
    addCircleMarkers(data = ca, radius = 2, popup = ~name)

# Change circle color to #2cb42c and store map in map_color object
map_color <- map %>% 
    addCircleMarkers(data = ca, radius = 2, color = "#2cb42c", popup = ~name)

# Print map_color
map_color


# Clear the bounds and markers on the map object and store in map2
map2 <- map %>% 
    clearBounds() %>% 
    clearMarkers()

# Add circle markers with popups that display both the institution name and sector
map2 %>% 
    addCircleMarkers(data = ca, radius = 2, 
                     popup = ~paste0(name, "<br/>", sector_label)
                     )

# Make the institution name in each popup bold
map2 %>% 
    addCircleMarkers(data = ca, radius = 2, 
                     popup = ~paste0("<b>", name, "</b>", "<br/>", sector_label)
                     )


# Add circle markers with labels identifying the name of each college
map %>% 
    addCircleMarkers(data = ca, radius = 2, label = ~name)

# Use paste0 to add sector information to the label inside parentheses 
map %>% 
    addCircleMarkers(data = ca, radius = 2, label = ~paste0(name, " (", sector_label, ")"))


# Make a color palette called pal for the values of `sector_label` using `colorFactor()`  
# Colors should be: "red", "blue", and "#9b4a11" for "Public", "Private", and "For-Profit" colleges, respectively
pal <- colorFactor(palette = c("red", "blue", "#9b4a11"), 
                   levels = c("Public", "Private", "For-Profit")
                   )

# Add circle markers that color colleges using pal() and the values of sector_label
map2 <- map %>% 
        addCircleMarkers(data = ca, radius = 2, 
                         color = ~pal(sector_label), 
                         label = ~paste0(name, " (", sector_label, ")")
                         )

# Print map2
map2


# Add a legend that displays the colors used in pal
map2 %>% 
    addLegend(pal = pal, values = c("Public", "Private", "For-Profit"))

# Customize the legend
map2 %>% 
    addLegend(pal = pal, 
              values = c("Public", "Private", "For-Profit"),
              # opacity of .5, title of Sector, and position of topright
              opacity = 0.5, title = "Sector", position = "topright"
              )

```
  
  
  
***
  
Chapter 3 - Groups, Layers, Extras  
  
Leaflet Extras Package:  
  
* The leaflet.extras package provides some nice extensibility to the baseline leaflet package  
	* leaflet() %>% addTiles() %>% addSearchOSM()  # searching open-source-maps (magnifying glass icon with search box)  
    * leaflet() %>% addTiles() %>% addSearchOSM() %>% addReverseSearchOSM()  # can also use geocode to find a click, as requested by addReverseSearchOSM()  
    * leaflet() %>% addTiles() %>% addSearchOSM() %>% addReverseSearchOSM() %>% addResetMapButton()  # can click "reset" to return to the default view  
  
Overlay Groups - ability to control the segments that are displayed on the map:  
  
* One option is to segment the data in advance, then to add as layers using addCircleMarkers  
	* ca_public <- ipeds %>% filter(sector == "Public", state == "CA")  
    * m %>% addCircleMarkers( data = ca_public, group = "Public")  
* After creating multiple calls for addCircleMarkers(), each with group=, can then activate the grouping  
	* addLayersControl( overlayGroups = c("Public", "Private", "For-Profit"))  
* Since the layers are stacked, the order in which they are added matters (they layer/stack on top of each other)  
  
Base Groups - can provide multiple options for toggling (only one may be selected at a time):  
  
* Need to call addProviderTiles() once for each layer that is an option, then activate using addLayersControl()  
	* a <- leaflet() %>% addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>% addProviderTiles("Esri", group = "Esri")   
    * a %>% addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), position = "topleft")  
* Can be handy to try a few different base groups during exploratory analysis, to find the base that best matches the rest of the analysis  
* Basic four-step process for building up the base groups includes  
	* leaflet() %>% # initialize leaflet map  
    * addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>% addProviderTiles("Esri", group = "Esri") %>% # add basemaps with groups  
    * addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>% addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Private") %>% addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "For-Profit") %>% # add marker layer for each sector with corresponding group name  
    * addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), overlayGroups = c("Public", "Private", "For-Profit")) # add layer controls for base and overlay groups  
  
Pieces of Flair:  
  
* Can customize ths search function using leaflet.extra capability  
	* ca_public <- ipeds %>% filter(sector_label == "Public", state == "CA")  
    * ca_public %>% leaflet() %>% addProviderTiles("Esri") %>% addCircleMarkers(radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>% addSearchFeatures(targetGroups = 'Public', options = searchFeaturesOptions(zoom = 10))  # will filter the search on Public data, with a specified zoom  
* Can cluster the colleges to improve readability of the maps  
	* ipeds %>% leaflet() %>% addTiles() %>% addCircleMarkers(radius = 2, color = ~pal(sector_label), clusterOptions = markerClusterOptions())  # many colleges in one circle  
  
Example code includes:  
```{r}

library(leaflet.extras)
library(htmltools)

leaflet() %>%
  addTiles() %>% 
  addSearchOSM() %>% 
  addReverseSearchOSM() 


m2 <- ipeds %>% 
    leaflet() %>% 
    # use the CartoDB provider tile
    addProviderTiles("CartoDB") %>% 
    # center on the middle of the US with zoom of 3
    setView(lat = 39.8282, lng = -98.5795, zoom=3)

# Map all American colleges 
m2 %>% 
    addCircleMarkers() 


# Create data frame called public with only public colleges
public <- filter(ipeds, sector_label == "Public")  

# Create a leaflet map of public colleges called m3 
m3 <- leaflet() %>% 
    addProviderTiles("CartoDB") %>% 
    addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Public"
                     )

m3


# Create data frame called private with only private colleges
private <- filter(ipeds, sector_label == "Private")  

# Add private colleges to `m3` as a new layer
m3 <- m3 %>% 
    addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Private"
                     ) %>% 
    addLayersControl(overlayGroups = c("Public", "Private"))

m3


# Create data frame called profit with only for-profit colleges
profit <- filter(ipeds, sector_label == "For-Profit")  

# Add for-profit colleges to `m3` as a new layer
m3 <- m3 %>% 
    addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label),   group = "For-Profit"
                     )  %>% 
    addLayersControl(overlayGroups = c("Public", "Private", "For-Profit"))  

# Center the map on the middle of the US with a zoom of 4
m4 <- m3 %>%
    setView(lat = 39.8282, lng = -98.5795, zoom = 4) 
        
m4


leaflet() %>% 
  # Add the OSM, CartoDB and Esri tiles
  addTiles(group = "OSM") %>% 
  addProviderTiles("CartoDB", group = "Carto") %>% 
  addProviderTiles("Esri", group = "Esri") %>% 
  # Use addLayersControl to allow users to toggle between basemaps
  addLayersControl(baseGroups = c("OSM", "Carto", "Esri"))


m4 <- leaflet() %>% 
    addTiles(group = "OSM") %>% 
    addProviderTiles("CartoDB", group = "Carto") %>% 
    addProviderTiles("Esri", group = "Esri") %>% 
    addCircleMarkers(data = public, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label),  group = "Public"
                     ) %>% 
    addCircleMarkers(data = private, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "Private"
                     )  %>% 
    addCircleMarkers(data = profit, radius = 2, label = ~htmlEscape(name), 
                     color = ~pal(sector_label), group = "For-Profit"
                     )  %>% 
    addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), 
                     overlayGroups = c("Public", "Private", "For-Profit")
                     ) %>% 
    setView(lat = 39.8282, lng = -98.5795, zoom = 4) 

m4


ipeds %>% 
    leaflet() %>% 
    addTiles() %>% 
    # Sanitize any html in our labels
    addCircleMarkers(radius = 2, label = ~htmlEscape(name), 
                     # Color code colleges by sector using the `pal` color palette 
                     color = ~pal(sector_label), 
                     # Cluster all colleges using `clusterOptions` 
                     clusterOptions = markerClusterOptions()
                     ) 

```
  
  
  
***
  
Chapter 4 - Plotting Polygons  
  
Spatial Data - ability to plot polygons rather than points:  
  
* Polygons have many points, and are stored in SPDF (Spatial Polygons Data Frame) with 5 slots  
	* data - one observation per polygon  
    * polygons - coordinates to plot each polygon  
    * plotOrder - order for plotting  
    * bbox - rectangle containing all the polygons  
    * proj4string - coordinate reference system (CRS)  
    * All accessed using the @ symbol  
* Can join from the data component of the SPDF, accessed using @  
	* hp@data <- shp@data %>% left_join(nc_income, by = c("GEOID10" = "zipcode"))  
    * shp@polygons[[1]] %>% leaflet() %>% addPolygons()  # can plot a single polygon  
  
Mapping Polygons - can pipe SPDF in to a series of leaflet calls:  
  
* The basic polygon plotting method using leaflet() may produce shape boundaries that are too thick  
	* shp %>% leaflet() %>% addTiles() %>% addPolygons()  
    * weight - thickness of lines  
    * color - color of lines  
    * label - information shown on hover  
    * highlight - options to highlight polygon on hover  
* The refined plotting approach adds customization for better readability  
	* shp %>% leaflet() %>% addTiles() %>% addPolygons(weight = 1, color = "grey", label = ~paste0("Total Income: " dollar(income)), highlight = highlightOptions(weight = 3, color = "red", bringToFront = TRUE))  
* Can color numeric data when plotting polygons  
	* colorNumeric - maps continuous data to interpolated palettes  
    * colorBin - colors based on cut function  
    * colorQuantile - colors based on quantile  
    * nc_pal <- colorNumeric(palette = "Blues", domain = high_inc@data$mean_income)  
    * nc_pal <- colorBin(palette = "YlGn", bins = 5, domain = high_inc@data$mean_income)  
    * nc_pal <- colorQuantile(palette = "YlGn", n = 4, domain = high_inc@data$mean_income)  
* Example of coloring using colorNumeric()  
	* nc_pal <- colorNumeric("Blues", domain = high_inc@data$mean_income)  
    * previewColors(pal = nc_pal, values = c(seq(100000, 600000, by = 100000)))  # explore sample values  
    * shp %>% leaflet() %>% # addTiles() %>% addPolygons(weight = 1, fillOpacity = 1, color = ~nc_pal(mean_income), label = ~paste0("Mean Income: ", dollar(mean_income)), highlight = highlightOptions(weight = 3, color = "red", bringToFront = TRUE))  
* Sometimes need to log-transform skewed data for better displays  
  
Putting Everything Together:  
  
* Leaflet and htmlwidgets for base maps and coloring  
* Base and overlay groups to enhance interactivity  
* Features available in the leaflet.extras function  
* Can piece together a full map that includes both polygons and circle markers  
	* leaflet() %>% addTiles(group = "OSM") %>% addProviderTiles("CartoDB", group = "Carto") %>%           
	*   addProviderTiles("Esri", group = "Esri") %>%  
    *   addPolygons(data = shp, weight = 1, fillOpacity = .75, color = ~nc_pal(log(mean_income)), label = ~paste0("Mean Income: ", dollar(mean_income)), group = "Mean Income") %>%  
    *   addCircleMarkers(data = nc_public, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Public") %>%  
    *   addCircleMarkers(data = nc_private, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "Private") %>%  
    *   addCircleMarkers(data = nc_profit, radius = 2, label = ~htmlEscape(name), color = ~pal(sector_label), group = "For-Profit") %>%  
    * addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), overlayGroups = c("Public", "Private", "For-Profit", "Mean Income"))  
* Can also save a map for future use  
	* m <- leaflet() %>% addTiles() %>% addMarkers( data = ipeds, clusterOptions = markerClusterOptions())%>% addPolygons(data = shp)  
    * library(htmlwidgets)  
    * saveWidget(m, file="myMap.html")  # saves the file as html  
  
Wrap up - additional resources:  
  
* RStudio's leaflet website: https://rstudio.github.io/leaflet/  
* Leaflet extras: https://github.com/bhaskarvk/leaflet.extras  
* JavaScript library: http://leafletjs.com/  
  
Example code includes:  
```{r}


load("./RInputFiles/nc_zips.Rda")
load("./RInputFiles/wealthiest_zips.Rda")
nc_income <- readr::read_csv("./RInputFiles/mean_income_by_zip_nc.csv")
str(nc_income, give.attr = FALSE)


# Print a summary of the `shp` data
summary(shp)

# Print the class of `shp`
class(shp)

# Print the slot names of `shp`
slotNames(shp)


# Glimpse the data slot of shp
glimpse(shp@data)

# Print the class of the data slot of shp
class(shp@data)

# Print GEOID10
shp@data$GEOID10
shp@data$GEOID10 <- as.integer(as.character(shp@data$GEOID10))
str(shp@data$GEOID10)


# Glimpse the nc_income data
glimpse(nc_income)

# Summarise the nc_income data
summary(nc_income)

# Left join nc_income onto shp@data and store in shp_nc_income
shp_nc_income <- shp@data %>% 
                left_join(nc_income, by = c("GEOID10" = "zipcode"))

# Print the number of missing values of each variable in shp_nc_income
shp_nc_income %>%
  summarise_all(funs(sum(is.na(.))))


shp <- merge(shp, shp_nc_income, by=c("GEOID10", "ALAND10"))


# map the polygons in shp
shp %>% 
    leaflet() %>% 
    addTiles() %>% 
    addPolygons()

# which zips were not in the income data?
shp_na <- shp[is.na(shp$mean_income),]

# map the polygons in shp_na
shp_na %>% 
    leaflet() %>% 
    addTiles() %>% 
    addPolygons()


# summarise the mean income variable
summary(shp$mean_income)

# subset shp to include only zip codes in the top quartile of mean income
high_inc <- shp[!is.na(shp$mean_income) & shp$mean_income > 55917,]

# map the boundaries of the zip codes in the top quartile of mean income
high_inc %>%
  leaflet() %>%
  addTiles() %>%
  addPolygons()


dollar <- function (x, negative_parens=TRUE, prefix="$", suffix="") {
    # KLUGE to make this work . . . 
    needs_cents <- function(...) { FALSE }
    if (length(x) == 0) 
        return(character())
    x <- plyr::round_any(x, 0.01)
    if (needs_cents(x, largest_with_cents)) {
        nsmall <- 2L
    }
    else {
        x <- plyr::round_any(x, 1)
        nsmall <- 0L
    }
    negative <- !is.na(x) & x < 0
    if (negative_parens) {
        x <- abs(x)
    }
    amount <- format(abs(x), nsmall = nsmall, trim = TRUE, big.mark = ",", scientific = FALSE, digits = 1L)
    if (negative_parens) {
        paste0(ifelse(negative, "(", ""), prefix, amount, suffix, ifelse(negative, ")", ""))
    }
    else {
        paste0(prefix, ifelse(negative, "-", ""), amount, suffix)
    }
}


# create color palette with colorNumeric()
nc_pal <- colorNumeric("YlGn", domain = high_inc@data$mean_income)

high_inc %>%
  leaflet() %>%
  addTiles() %>%
  # set boundary thickness to 1 and color polygons blue
  addPolygons(weight = 1, color = ~nc_pal(mean_income),
              # add labels that display mean income
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              # highlight polygons on hover
              highlight = highlightOptions(weight = 5, color = "white",
              bringToFront = TRUE))


# Create a logged version of the nc_pal color palette
nc_pal <- colorNumeric("YlGn", domain = log(high_inc@data$mean_income))

# apply the nc_pal
high_inc %>%
  leaflet() %>%
  addProviderTiles("CartoDB") %>%
  addPolygons(weight = 1, color = ~nc_pal(log(mean_income)), fillOpacity = 1,
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlightOptions = highlightOptions(weight = 5, color = "white", bringToFront = TRUE))


# Print the slot names of `wealthy_zips`
slotNames(wealthy_zips)

# Print a summary of the `mean_income` variable
summary(wealthy_zips$mean_income)

# plot zip codes with mean incomes >= $200k
wealthy_zips %>% 
  leaflet() %>% 
  addProviderTiles("CartoDB") %>% 
  addPolygons(weight = 1, fillOpacity = .7, color = "Green",  group = "Wealthy Zipcodes", 
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlightOptions = highlightOptions(weight = 5, color = "white", bringToFront = TRUE))


# Add polygons using wealthy_zips
final_map <- m4 %>% 
   addPolygons(data = wealthy_zips, weight = 1, fillOpacity = .5, color = "Grey",  group = "Wealthy Zip Codes", 
              label = ~paste0("Mean Income: ", dollar(mean_income)),
              highlight = highlightOptions(weight = 5, color = "white", bringToFront = TRUE)) %>% 
    # Update layer controls including "Wealthy Zip Codes"
    addLayersControl(baseGroups = c("OSM", "Carto", "Esri"), 
                         overlayGroups = c("Public", "Private", "For-Profit", "Wealthy Zip Codes"))

# Print and explore your very last map of the course!
final_map

```
  
  
  
***
  
###_Support Vector Machines in R_  
  
Chapter 1 - Introduction  
  
Sugar content of soft drinks:  
  
* Course covers Support Vector Machines (SVM), including visualization, mechanics, situations where they work best, etc.  
	* Will stick with binary classification for this course  
* For a 1-dimensional dataset, the clusters can be separated by choosing a "separating boundary" (decision boundary)  
* Margins are the distances between the decision boundary and the closest point  
	* The best decision boundary is considered to be the decision boundary that maximizes the margin (more robust to noise)  
    * The SVM tries to find the decision boundary that maximizes the margin in n-dimensions  
  
Generating a linearly separable dataset  
  
* Can use runif to generate random data that is unifotm from 0 to 1  
	* n <- 200  
    * set.seed(42)  
    * df <- data.frame(x1 = runif(n), x2 = runif(n))  
* Can define the points with x1 < x2 as class A and the points with x1 > x2 as class B  
	* Can also create a margin by filtering out points where abs(x1-x2) is below a user-specified threshold  
  
Example code includes:  
```{r}

df <- data.frame(sample=1:25, 
                 sugar_content=c(10.9, 10.9, 10.6, 10, 8, 8.2, 8.6, 10.9, 10.7, 8, 7.7, 7.8, 8.4, 11.5, 11.2, 8.9, 8.7, 7.4, 10.9, 10, 11.4, 10.8, 8.5, 8.2, 10.6)
                 )
str(df)

#print variable names
names(df)

#build plot
plot_ <- ggplot(data = df, aes(x = sugar_content, y = c(0))) + 
    geom_point() + 
    geom_text(label = df$sugar_content, size = 2.5, vjust = 2, hjust = 0.5)

#display plot
plot_


#The maximal margin separator is at the midpoint of the two extreme points in each cluster.
mm_separator <- (8.9 + 10)/2


#create data frame
separator <- data.frame(sep = c(mm_separator))

#add ggplot layer 
plot_ <- plot_ + 
  geom_point(data = separator, x = separator$sep, y = c(0), color = "blue", size = 4)

#display plot
plot_


#set seed
set.seed(42)

#set number of data points. 
n <- 600

#Generate data frame with two uniformly distributed predictors lying between 0 and 1.
df <- data.frame(x1 = runif(n), x2 = runif(n))

#classify data points depending on location
df$y <- factor(ifelse(df$x2 - 1.4*df$x1 < 0, -1, 1), levels = c(-1, 1))


#set margin
delta <- 0.07

# retain only those points that lie outside the margin
df1 <- df[abs(1.4*df$x1 - df$x2) > delta, ]

#build plot
plot_margins <- ggplot(data = df1, aes(x = x1, y = x2, color = y)) + geom_point() + 
    scale_color_manual(values = c("red", "blue")) + 
    geom_abline(slope = 1.4, intercept = 0)+
    geom_abline(slope = 1.4, intercept = delta, linetype = "dashed") +
    geom_abline(slope = 1.4, intercept = -delta, linetype = "dashed")
 
#display plot 
plot_margins

```
  
  
  
***
  
Chapter 2 - Support Vector Classifiers - Linear Kernels  
  
Linear Support Vector Machines:  
  
* Can split the data from the previous chapter (perfectly separable) in to train/test on an 80-20 basis  
	* set.seed() = 1  
    * df[, "train"] <- ifelse(runif(nrow(df))<0.8,1,0)  
    * trainset <- df[df$train==1,]  
    * testset <- df[df$train==0,]  
    * trainColNum <- grep("train", names(trainset))  
    * trainset <- trainset[,-trainColNum]  
    * testset <- testset[,-trainColNum]  
* Decision boundaries have many shapes-types (called kernels) such as lines, polynomials, etc.  
* For this chapter, will use e1071::svm(), a function with many options  
	* formula, data, type ("C-classification" for classification), kernel ("linear" for this chapter), cost/gamma (tuning parameters, which will be left at the defaults for now), scale (boolean telling whether to scale the data in advance - FALSE makes for easier plotting, but typically would be set to TRUE in the real-world)  
* Example of running e1071::svm()  
	* library(e1071)  
    * svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear", scale = FALSE)  
    * svm_model  
    * svm_model$index  # indices of the support vectors  
    * svm_model$SV  # support vector coordinates  
    * svm_model$rho  # negative y-intercept of the decision boundary  
    * svm_model$coefs  # weighting coefficients of support vectors (magnitude is importance, side is which part of boundary)  
    * pred_train <- predict(svm_model,trainset)  
    * pred_test <- predict(svm_model,testset)  
  
Visualizing Linear SVM:  
  
* Can begin by plotting the training data, distinguished by color  
	* p <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + geom_point() + scale_color_manual(values = c("red","blue"))  
    * df_sv <- trainset[svm_model$index,]  
    * p <- p + geom_point(data = df_sv, aes(x = x1, y = x2), color = "purple", size = 4, alpha = 0.5)  
    * p  
* The support vectors tend to be close to the decision boundary - in fact, they are defined as points that "support" the boundary  
* Goal is to extract the slope and coefficients from the model (not stored in the model object)  
	* w <- t(svm_model$coefs) %*% svm_model$SV  
    * slope_1 <- -w[1]/w[2]  
    * intercept_1 <- svm_model$rho/w[2]  
    * p <- p + geom_abline(slope = slope_1, intercept = intercept_1)  
    * p <- p + geom_abline(slope = slope_1, intercept = intercept_1-1/w[2], linetype = "dashed") + geom_abline(slope = slope_1, intercept = intercept_1+1/w[2], linetype = "dashed")  
    * p  
* There are several properties observed in the plot  
	* The boundary is supported by the support vectors  
    * The boundary is "soft", which allows for uncertainty in location/shape of the boundary  
    * Can also use e1071::plot(x=myModel, data=myData) to plot the function  
  
Tuning Linear SVM:  
  
* Can tweak the cost parameter to change the size of the soft boundary for the SVM  
	* Higher costs lead to harder (smaller, narrower) decision boundaries, with fewer support vectors  
    * The implication is that raising the cost can be a good idea if the data are known to be linearly separable  
  
Multi-class problems:  
  
* SVM can manage classification problems with 3+ target types also - using the example iris data  
	* p <- ggplot(data = iris, aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point()  
    * p  
* The SVM at core is a binary classifier, but can be used in a multi-class setting  
	* Have a model for each of the choose(m, 2) possible combinations, and use majority voting on the outputs (ties broken by random)  
    * This method is called the "one against one" classification, and it is automatically included in e1071  
    * svm_model<- svm(Species ~ ., data = trainset, type = "C-classification", kernel = "linear")  # all run automatically  
  
Example code includes:  
```{r}


dfOld <- df
delta <- 0.07
df <- df[abs(1.4*df$x1 - df$x2) > delta, ]


#split train and test data in an 80/20 proportion
df[, "train"] <- ifelse(runif(nrow(df))<0.8, 1, 0)

#assign training rows to data frame trainset
trainset <- df[df$train == 1, ]
#assign test rows to data frame testset
testset <- df[df$train == 0, ]

#find index of "train" column
trainColNum <- grep("train", names(df))

#remove "train" column from train and test dataset
trainset <- trainset[, -trainColNum]
testset <- testset[, -trainColNum]


library(e1071)

#build svm model, setting required parameters
svm_model<- svm(y ~ ., 
                data = trainset, 
                type = "C-classification", 
                kernel = "linear", 
                scale = FALSE)


#list components of model
names(svm_model)

#list values of the SV, index and rho
svm_model$SV
svm_model$index
svm_model$rho

#compute training accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)

#compute test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)


#build scatter plot of training dataset
scatter_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + 
    scale_color_manual(values = c("red", "blue"))
 
#add plot layer marking out the support vectors 
layered_plot <- 
    scatter_plot + geom_point(data = trainset[svm_model$index, ], aes(x = x1, y = x2), color = "purple", size = 4, alpha = 0.5)

#display plot
layered_plot


#calculate slope and intercept of decision boundary from weight vector and svm model
w <- c(x1=6.55241, x2=-4.73278)  # calculated manually outside of this module
slope_1 <- -w[1]/w[2]
intercept_1 <- svm_model$rho/w[2]

#build scatter plot of training dataset
scatter_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + scale_color_manual(values = c("red", "blue"))
#add decision boundary
plot_decision <- scatter_plot + geom_abline(slope = slope_1, intercept = intercept_1) 
#add margin boundaries
plot_margins <- plot_decision + 
 geom_abline(slope = slope_1, intercept = intercept_1 - 1/w[2], linetype = "dashed")+
 geom_abline(slope = slope_1, intercept = intercept_1 + 1/w[2], linetype = "dashed")
#display plot
plot_margins


#build svm model
svm_model<- 
    svm(y ~ ., data = trainset, type = "C-classification", 
        kernel = "linear", scale = FALSE)

#plot decision boundaries and support vectors
plot(x = svm_model, data = trainset)


#build svm model, cost = 1
svm_model_1 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 1,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_1

#build svm model, cost = 100
svm_model_100 <- svm(y ~ .,
                   data = trainset,
                   type = "C-classification",
                   cost = 100,
                   kernel = "linear",
                   scale = FALSE)

#print model details
svm_model_100


# Create the base train_plot
train_plot <- ggplot(data = trainset, aes(x = x1, y = x2, color = y)) + 
    geom_point() + scale_color_manual(values = c("red", "blue"))
w_1 <- c(x1=6.55241, x2=-4.73278)  # calculated manually outside of this module
w_100 <- c(x1=18.3097, x2=-13.09972)  # calculated manually outside of this module
intercept_1 <- -0.005515526  # calculated outside of this module
intercept_100 <- 0.001852543  # calculated outside of this module
slope_1 <- -w_1[1]/w_1[2]
slope_100 <- -w_100[1]/w_100[2]


#add decision boundary and margins for cost = 1 to training data scatter plot
train_plot_with_margins <- train_plot + 
    geom_abline(slope = slope_1, intercept = intercept_1) +
    geom_abline(slope = slope_1, intercept = intercept_1 - 1/w_1[2], linetype = "dashed")+
    geom_abline(slope = slope_1, intercept = intercept_1 + 1/w_1[2], linetype = "dashed")

#display plot
train_plot_with_margins

#add decision boundary and margins for cost = 100 to training data scatter plot
train_plot_with_margins <- train_plot_with_margins + 
    geom_abline(slope = slope_100, intercept = intercept_100, color = "goldenrod") +
    geom_abline(slope = slope_100, intercept = intercept_100 - 1/w_100[2], linetype = "dashed", color = "goldenrod")+
    geom_abline(slope = slope_100, intercept = intercept_100 + 1/w_100[2], linetype = "dashed", color = "goldenrod")

#display plot 
train_plot_with_margins


svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear", scale = FALSE)

#compute training accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)

#compute test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot
plot(svm_model, trainset)


data(iris)
nTrials <- 100
accuracy <- numeric(nTrials)

#calculate accuracy for n distinct 80/20 train/test partitions
for (i in 1:nTrials){ 
    iris[, "train"] <- ifelse(runif(nrow(iris))<0.8, 1, 0)
    trainColNum <- grep("train", names(iris))
    trainset <- iris[iris$train == 1, -trainColNum]
    testset <- iris[iris$train == 0, -trainColNum]
    svm_model <- svm(Species~ ., data = trainset, 
                     type = "C-classification", kernel = "linear")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$Species)
}

#mean accuracy and standard deviation
mean(accuracy) 
sd(accuracy)

```
  
  
  
***
  
Chapter 3 - Polynomial Kernels  
  
Generating radially separable datasets:  
  
* The goal is to generate 2D points (again uniformly distributed on x1 and x2 using runif)  
* Can then define a value for whether the points are within x of the center  
	* radius <- 0.7  
    * radius_squared <- radius^2  
    * df$y <- factor(ifelse(df$x1^2 + df$x2^2 < radius_squared, -1, 1), levels = c(-1,1))  
    * p <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + geom_point() + scale_color_manual(values = c("-1" = "red","1" = "blue"))   
    * p  
* Can add a circular boundary  
	* circle <- function(x1_center, x2_center, r, npoint = 100){ 
	*   #angular spacing of 2*pi/npoint between points  
	*   theta <- seq(0,2*pi,length.out = npoint)  
	*   x1_circ <- x1_center + r * cos(theta)  
	*   x2_circ <- x2_center + r * sin(theta)  
	*   return(data.frame(x1c = x1_circ, x2c = x2_circ))  
	*   }  
    * boundary <- circle(x1_center = 0, x2_center = 0, r = radius)  
    * p <- p + geom_path(data = boundary, aes(x = x1c, y = x2c), inherit.aes = FALSE)  
  
Linear SVM on radially separable datasets:  
  
* The linear SVM will perform poorly on the radially separable dataset  
	* svm_model<- svm(y ~ ., data=trainset, type="C-classification", kernel="linear")  
    * svm_model  
    * pred_test <- predict(svm_model,testset)  
    * plot(svm_model,trainset)  # all points are classified as 1  
  
Kernel trick - devise a mathematical transformation that makes the data linearly separable:  
  
* For a circles could map X1 = x1**2 and X2 = x2**2, where X1 + X2 = 0.49 (which is linearly separable)  
* The polynomial kernel has a degree (e.g., 1 for linear, 2 for quadratic, etc.) and tuning parameters gamma and coef0  
	* The kernel also uses u dot v where u and v are vectors belonging to the dataset  
    * (gamma * (u dot v) + coef0) ** degree  
* Applying the quadratic kernel to the circular data from above  
	* svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)  
    * plot(svm_model, trainset)  
  
Tuning SVM:  
  
* Set a search range for each parameter, typically as a sequence of variable (e.g., in multiples of 10)  
* For each combination of parameters, build an SVM and assess the out-of-sample accuracy - can become computationally intensive, though  
	* tune_out <- tune.svm(x = trainset[,-3], y = trainset[,3], type = "C-classification", kernel = "polynomial", degree = 2, cost = 10^(-1:2), gamma = c(0.1,1,10), coef0 = c(0.1,1,10))  
    * tune_out$best.parameters$cost  
    * tune_out$best.parameters$gamma  
    * tune_out$best.parameters$coef0  
    * svm_model <- svm(y~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2, cost = tune_out$best.parameters$cost, gamma = tune_out$best.parameters$gamma, coef0 = tune_out$best.parameters$coef0)  
  
Example code includes:  
```{r}

#set number of variables and seed
n <- 400
set.seed(1)

#Generate data frame with two uniformly distributed predictors, x1 and x2
df <- data.frame(x1 = runif(n, min = -1, max = 1), x2 = runif(n, min = -1, max = 1))

#We want a circular boundary. Set boundary radius 
radius <- 0.8
radius_squared <- radius^2

#create dependent categorical variable, y, with value -1 or 1 depending on whether point lies
#within or outside the circle.
df$y <- factor(ifelse(df$x1**2 + df$x2**2 < radius_squared, -1, 1), levels = c(-1, 1))


#build scatter plot, distinguish class by color
scatter_plot <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    geom_point() +
    scale_color_manual(values = c("red", "blue"))

#display plot
scatter_plot


inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]


#default cost mode;
svm_model_1 <- svm(y ~ ., data = trainset, type = "C-classification", cost = 1, kernel = "linear")

#training accuracy
pred_train <- predict(svm_model_1, trainset)
mean(pred_train == trainset$y)

#test accuracy
pred_test <- predict(svm_model_1, testset)
mean(pred_test == testset$y)

#cost = 100 model
svm_model_100 <- svm(y ~ ., data = trainset, type = "C-classification", cost = 100, kernel = "linear")

#accuracy
pred_train <- predict(svm_model_100, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model_100, testset)
mean(pred_test == testset$y)


#print average accuracy and standard deviation
accuracy <- rep(NA, 100)
set.seed(2)

#comment
for (i in 1:100){
    df[, "train"] <- ifelse(runif(nrow(df))<0.8, 1, 0)
    trainset <- df[df$train == 1, ]
    testset <- df[df$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


#transform data
df1 <- data.frame(x1sq = df$x1^2, x2sq = df$x2^2, y = df$y)

#plot data points in the transformed space
plot_transformed <- ggplot(data = df1, aes(x = x1sq, y = x2sq, color = y)) + 
    geom_point()+ guides(color = FALSE) + 
    scale_color_manual(values = c("red", "blue"))

#add decision boundary and visualize
plot_decision <- plot_transformed + geom_abline(slope = -1, intercept = 0.64)
plot_decision


# Still want to use the old (non-squared) data
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
df$train <- NULL
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]

svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)

#measure training and test accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot
plot(svm_model, trainset)


#tune model
tune_out <- 
    tune.svm(x = trainset[, -3], y = trainset[, 3], 
             type = "C-classification", 
             kernel = "polynomial", degree = 2, cost = 10^(-1:2), 
             gamma = c(0.1, 1, 10), coef0 = c(0.1, 1, 10))

#list optimal values
tune_out$best.parameters$cost
tune_out$best.parameters$gamma
tune_out$best.parameters$coef0


#Build tuned model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", 
                 kernel = "polynomial", degree = 2, 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma, 
                 coef0 = tune_out$best.parameters$coef0)

#Calculate training and test accuracies
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model
plot(svm_model, trainset)

```
  
  
  
***
  
Chapter 4 - Radial Basis Kernel Functions  
  
Generating complex datasets:  
  
* The RBF kernel is highly flexible, can fit complex boundaries, and is common in the real-world  
* Can generate complex data by using different distributions for x and y  
	* n <- 600  
    * set.seed(42)  
    * df <- data.frame(x1 = rnorm(n, mean = -0.5, sd = 1), x2 = runif(n, min = -1, max = 1))  
* The decision boundary can then be two circles that just barely touch at the origin  
	* radius <- 0.7  
    * radius_squared <- radius^2  
    * center_1 <- c(-0.7,0)  
    * center_2 <- c(0.7,0)  
    * df$y <- factor(ifelse( (df$x1-center_1[1])^2 + (df$x2-center_1[2])^2 < radius_squared| (df$x1-center_2[1])^2 + (df$x2-center_2[2])^2 < radius_squared, -1,1), levels = c(-1,1))  
    * p <- ggplot(data = df, aes(x = x1, y = x2, color = y)) + geom_point() + guides(color = FALSE) + scale_color_manual(values = c("red","blue"))  
    * p  
* Can then build linear, polynomial, and RBF kernels to model the data  
  
Motivating the RBF kernel:  
  
* Neither the linear kernel nor the polynomial kernel will work well for the dataset as described  
* Can use the heuristic that points near each other probably belong to the same class (similar to kNN)  
	* The kernel should have a maximum at (a, b), and should decay as you move away from (a, b)  
    * The rate of decay, all else equal should be the same in all directions, with a tunable gamma  
    * As good fortune has it, the exponential exp(-gamma * r) has all of these properties  
    * rbf <- function(r, gamma) exp(-gamma*r)  
    * ggplot(data.frame(r = c(-0, 10)), aes(r)) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.2), aes(color = "0.2")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.4), aes(color = "0.4")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.6), aes(color = "0.6")) +  
    *   stat_function(fun = rbf, args = list(gamma = 0.8), aes(color = "0.8")) +  
    *   stat_function(fun = rbf, args = list(gamma = 1), aes(color = "1")) +  
    *   stat_function(fun = rbf, args = list(gamma = 2), aes(color = "2")) +  
    *   scale_color_manual("gamma", values = c("red","orange","yellow", "green","blue","violet")) +  
    *   ggtitle("Radial basis function (gamma=0.2 to 2)")  
  
The RBF kernel simulates some of the principles of kNN using exponential decay:  
  
* The RBF kernel can be built using pre-set R commands  
	* svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "radial")  
* The predicted decision boundary will no longer be linear, and can be refined through tuning  
	* tune_out <- tune.svm(x = trainset[,-3], y = trainset[,3], gamma = 5*10^(-2:2), cost = c(0.01,0.1,1,10,100), type = "C-classification", kernel = "radial")  
    * tune_out$best.parameters$cost  
    * tune_out$best.parameters$gamma  
    * svm_model <- svm(y~ ., data=trainset, type="C-classification", kernel="radial", cost=tune_out$best.parameters$cost, gamma=tune_out$best.parameters$gamma)  
  
Example code includes:  
```{r}

#number of data points
n <- 1000

#set seed
set.seed(1)

#create dataframe
df <- data.frame(x1 = rnorm(n, mean = -0.5, sd = 1), x2 = runif(n, min = -1, max = 1))


#set radius and centers
radius <- 0.8
center_1 <- c(-0.8, 0)
center_2 <- c(0.8, 0)
radius_squared <- radius^2

#create binary classification variable
df$y <- factor(ifelse((df$x1-center_1[1])^2 + (df$x2-center_1[2])^2 < radius_squared |
                      (df$x1-center_2[1])^2 + (df$x2-center_2[2])^2 < radius_squared, -1, 1),
                      levels = c(-1, 1))


#create scatter plot
scatter_plot<- ggplot(data = df, aes(x = x1, y = x2, color = y)) + 
    geom_point() + 
    scale_color_manual(values = c("red", "blue"))
 
scatter_plot 


# Create 75/25 split for train/test
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]


#build model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "linear")

#accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model against testset
plot(svm_model, testset)


#build model
svm_model <- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)

#accuracy
pred_train <- predict(svm_model, trainset)
mean(pred_train == trainset$y)
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#plot model
plot(svm_model, trainset)


#create vector to store accuracies and set random number seed
accuracy <- rep(NA, 100)
set.seed(2)


# Create a dummy frame dfDum for use in the for loop
dfDum <- df

#calculate accuracies for 100 training/test partitions
for (i in 1:100){
    dfDum[, "train"] <- ifelse(runif(nrow(dfDum))<0.8, 1, 0)
    trainset <- dfDum[dfDum$train == 1, ]
    testset <- dfDum[dfDum$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "polynomial", degree = 2)
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


#create vector to store accuracies and set random number seed
accuracy <- rep(NA, 100)
set.seed(2)

#calculate accuracies for 100 training/test partitions
for (i in 1:100){
    dfDum[, "train"] <- ifelse(runif(nrow(dfDum))<0.8, 1, 0)
    trainset <- dfDum[dfDum$train == 1, ]
    testset <- dfDum[dfDum$train == 0, ]
    trainColNum <- grep("train", names(trainset))
    trainset <- trainset[, -trainColNum]
    testset <- testset[, -trainColNum]
    svm_model<- svm(y ~ ., data = trainset, type = "C-classification", kernel = "radial")
    pred_test <- predict(svm_model, testset)
    accuracy[i] <- mean(pred_test == testset$y)
}

#print average accuracy and standard deviation
mean(accuracy)
sd(accuracy)


# Re-create original 75/25 split for train/test
inTrain <- sample(1:nrow(df), round(0.75*nrow(df)), replace=FALSE)
trainset <- df[sort(inTrain), ]
testset <- df[-inTrain, ]

#tune model
tune_out <- tune.svm(x = trainset[, -3], y = trainset[, 3], 
                     gamma = 5*10^(-2:2), 
                     cost = c(0.01, 0.1, 1, 10, 100), 
                     type = "C-classification", kernel = "radial")
tune_out

#build tuned model
svm_model <- svm(y~ ., data = trainset, type = "C-classification", kernel = "radial", 
                 cost = tune_out$best.parameters$cost, 
                 gamma = tune_out$best.parameters$gamma)

#calculate test accuracy
pred_test <- predict(svm_model, testset)
mean(pred_test == testset$y)

#Plot decision boundary against test data
plot(svm_model, testset)

```
  
  
  
***
  
###_Experimental Design in R_  
  
Chapter 1 - Introduction to Experimental Design  
  
Introduction to experimental design:  
  
* Experiments start with a question in mind, then finding and analyzing data  
* This course will use open data, meaning that we do not know the original experimental design  
* Key conditions of an experiment include randomization, replication, and blocking  
  
Hypothesis testing:  
  
* The null hypothesis changes depending on the question of interest - "no effect" (two-sided) or "no positive effect" (one-sided) or etc.  
* Power is the probability that the test correctly reject the null hypothesis when the alternative hypothesis is true (target >= 80%)  
* The effect size is the standardized measure of the difference that you are trying to detect  
* Sample size is generally chosen so that the effect size can be measured at the required power  
* Example of using the power package for calculating the metrics  
	* library(pwr)  
    * pwr.anova.test(k = 3, n = 20, f = 0.2, sig.level = 0.05, power = NULL)  # one must be entered as NULL (this will be calculated) ; k groups with n per group and f effect size  
  
Example code includes:  
```{r}

# load the ToothGrowth dataset
data("ToothGrowth")

#perform a two-sided t-test
t.test(x = ToothGrowth$len, alternative = "two.sided", mu = 18)

#perform a t-test
ToothGrowth_ttest <- t.test(len ~ supp, data = ToothGrowth)

#tidy the t-test model object
broom::tidy(ToothGrowth_ttest)


#group by supp, dose, then examine how many observations in ToothGrowth there are by those groups
ToothGrowth %>% 
    group_by(supp, dose) %>% 
    summarize(n=n())

#create a boxplot with geom_boxplot()
ggplot(ToothGrowth, aes(x=as.factor(dose), y=len)) + 
    geom_boxplot()

#create the ToothGrowth_aov model object
ToothGrowth_aov <- aov(len ~ dose + supp, data = ToothGrowth)

#examine the model object with summary()
summary(ToothGrowth_aov)


#less than
t.test(x = ToothGrowth$len, alternative = "less", mu = 18)

#greater than
t.test(x = ToothGrowth$len, alternative = "greater", mu = 18)


#calculate power
pwr::pwr.t.test(n = 100, d = 0.35, sig.level = 0.10, type = "two.sample", 
                alternative = "two.sided", power = NULL
                )

#calculate sample size
pwr::pwr.t.test(n = NULL, d = 0.25, sig.level = 0.05, 
                type = "one.sample", alternative = "greater", power = 0.8
                )

```
  
  
  
***
  
Chapter 2 - Basic Experiments  
  
Single and Multiple Factor Experiments:  
  
* The ANOVA (Analysis of Variance) test allows for comparing means across 3-groups; is at least one mean different  
	* model_1 <- lm(y ~ x, data = dataset)  # first option is lm followed by aov  
    * anova(model_1)  # first option is lm followed by anova  
    * aov(y ~ x, data = dataset)  # second option is a straight call to aov  
* The multiple factor experiment includes additional potential explanatory variables  
	* model2 <- lm(y ~ x + r + s + t)  
    * anova(model2)  
* The Lending Club data is 890k x 75, and contains data from a lending company  
  
Model Validation:  
  
* EDA is an important step prior to modeling the data  
* Boxplots can be a helpful way to explore the data  
	* ggplot(data = lendingclub, aes(x = verification_status, y = funded_amnt)) + geom_boxplot()  
* ANOVA and other linear models generally assume that the residuals are normally distributed  
  
A/B Testing:  
  
* A/B tests are a type of controlled experiment with only two variants of something  
* Power and sample size are crucial to A/B testing, allowing for an understanding of the required size for a desired power and expected effect size  
  
Example code includes:  
```{r}

lendingclub <- readr::read_csv("./RInputFiles/lendclub.csv")


#examine the variables with glimpse()
glimpse(lendingclub)

#find median loan_amt, mean int_rate, and mean annual_inc with summarise()
lendingclub %>% summarise(median(loan_amnt), mean(int_rate), mean(annual_inc))

# use ggplot2 to build a bar chart of purpose
ggplot(data=lendingclub, aes(x = purpose)) + geom_bar()

#use recode() to create the new purpose_recode variable.
lendingclub$purpose_recode <- lendingclub$purpose %>% recode( 
        "credit_card" = "debt_related",
        "debt_consolidation" = "debt_related", 
        "medical" = "debt_related",
        "car" = "big_purchase", 
        "major_purchase" = "big_purchase", 
        "vacation" = "big_purchase",
        "moving" = "life_change", 
        "small_business" = "life_change", 
        "wedding" = "life_change",
        "house" = "home_related", 
        "home_improvement" = "home_related"
        )


#build a linear regression model, stored as purpose_recode_model
purpose_recode_model <- lm(funded_amnt ~ purpose_recode, data = lendingclub)

#look at results of purpose_recode_model
summary(purpose_recode_model)

#get anova results and save as purpose_recode_anova
purpose_recode_anova <- anova(purpose_recode_model)

# look at the class of purpose_recode_anova
class(purpose_recode_anova)


#Use aov() to build purpose_recode_aov
purpose_recode_aov <- aov(funded_amnt ~ purpose_recode, data = lendingclub)

#Conduct Tukey's HSD test to create tukey_output
tukey_output <- TukeyHSD(purpose_recode_aov)

#tidy tukey_output to make sense of the results
broom::tidy(tukey_output)


#Use aov() to build purpose_emp_aov
purpose_emp_aov <- aov(funded_amnt ~ purpose_recode + emp_length, data=lendingclub)

#print purpose_emp_aov to the console
purpose_emp_aov

#call summary() to see the p-values
summary(purpose_emp_aov)


#examine the summary of int_rate
summary(lendingclub$int_rate)

#examine int_rate by grade
lendingclub %>% 
    group_by(grade) %>% 
    summarise(mean = mean(int_rate), var = var(int_rate), median = median(int_rate))

#make a boxplot of int_rate by grade
ggplot(lendingclub, aes(x = grade, y = int_rate)) + geom_boxplot()

#use aov() to create grade_aov plus call summary() to print results
grade_aov <- aov(int_rate ~ grade, data = lendingclub)
summary(grade_aov)


#for a 2x2 grid of plots:
par(mfrow=c(2, 2))

#plot grade_aov
plot(grade_aov)

#back to defaults
par(mfrow=c(1, 1))

#Bartlett's test for homogeneity of variance
bartlett.test(int_rate ~ grade, data=lendingclub)


#use the correct function from pwr to find the sample size
pwr::pwr.t.test(n=NULL, d=0.2, sig.level=0.05, 
                type="two.sample", alternative="two.sided", power=0.8
                )


lc_A <- c(11976148, 1203719, 54998739, 5801830, 31587242, 7711391, 54494666, 57663583, 8967787, 21760921, 44765721, 8596988, 5794746, 59501253, 10578432, 36058744, 11727607, 357888, 51936863, 1178593, 57315811, 5705168, 46024211, 12947039, 57345207, 55299831, 28763037, 49763149, 20077511, 60216198, 12295190, 1570287, 61408414, 59121340, 32349527, 5773180, 26899704, 55412161, 2217935, 16462713, 9196065, 27802028, 40949245, 56007625, 56935379, 62187473, 20178048, 604912, 58533358, 652594, 44066849, 38942161, 6414816, 65617953, 51816492, 43489983, 6794967, 42345315, 59532019, 13107597, 63249029, 7371829, 12335467, 8560739, 7337238, 887484, 23493355, 41031080, 60537197, 12816159, 38446687, 51026618, 6374688, 18685270, 296645, 44439325, 4915968, 63449566, 25256236, 63407874, 36753301, 20728660, 7937228, 13058684, 636359, 50527238, 40450502, 1018943, 12438198, 3065732, 1510626, 5764344, 37840363, 27460227, 39751366, 5028066, 43956700, 56109033, 1412622, 44289534, 41770436, 49956562, 44409121, 47168726, 60953428, 52189251, 64281487, 51928150, 1002880, 4537354, 12605849, 477843, 6808167, 38629237, 33311208, 36109419, 58593881, 40362979, 440300, 9848361, 30656060, 15691500, 4375269, 15360849, 7077904, 66076532, 33350264, 4175651, 44006939, 21130605, 54098234, 53192890, 7371114, 12967808, 58061230, 34803392, 5544911, 28843825, 63244663, 38504887, 68565204, 1211255, 63427670, 56472411, 10548622, 43957279, 59313014, 5768723, 66210490, 25507112, 55472659, 61339767, 65684813, 45544639, 43710238, 46833245, 13028661, 13167268, 3064642, 62072249, 27631726, 65825964, 15540990, 64320858, 8605358, 17795606, 9894584, 543619, 2380700, 20959552, 57743104, 63917130, 38480348, 61393540, 19916851)
lc_A <- c(lc_A, 12528162, 7264617, 61480809, 36411752, 20139228, 21290880, 390228, 45584424, 17755019, 23413261, 15490914, 1254285, 875004, 24274579, 51006600, 11458143, 5125832, 37802077, 57327243, 41059894, 64978360, 58683523, 4290736, 40919379, 65029207, 7096004, 42285591, 7388784, 65914238, 46833088, 21221678, 62855006, 10557733, 44915714, 23083224, 67289213, 9746670, 349608, 66610322, 1595886, 3635144, 38419356, 9715410, 9726377, 621152, 23213635, 18685424, 65782663, 57304429, 20770003, 8865120, 58664359, 1454540, 42404539, 60952405, 61339308, 7367648, 11215938, 41207320, 23553299, 1681376, 7617266, 30485630, 10604792, 46044414, 63094909, 59189668, 10106916, 52058386, 17763104, 6396213, 8981232, 48070364, 10615808, 11956507, 38444903, 60216940, 58310439, 10099562, 7504691, 17533228, 62236540, 38626163, 55657128, 7728107, 42415348, 42454693, 4777573, 23834164, 25157042, 1339435, 50587486, 55998961, 32950014, 28422748, 492346, 50607472, 11335041, 4254623, 65058537, 5375256, 5646680, 44430975, 4054992, 55253292, 68375791, 16822421, 64978226, 59859214, 65424555, 10112206, 6908772, 67879649, 4794842, 31227479, 17423361, 64049774, 58624386, 14829134, 50233873, 44389635, 29684724, 452267, 43044890, 55942742, 19516366, 34443897, 57135665, 34392172, 17352839, 12896521, 40451807, 43255228, 40372428, 8568706, 68364520, 3486848, 40991148, 19196658, 8658538, 65885614, 38352455, 65674149, 1029473, 39290483, 47420355, 65364529, 32318884, 13115811, 48484348, 65975356, 56129109, 3378980, 31026386, 55231010, 41113253, 1480114, 51406116, 2445051, 8627441, 60942818, 55453270, 58573102, 25767158, 9655554, 49783137, 42273770, 32038806, 681948, 65059359, 48546050, 20169281, 68546780, 7065575, 46387142, 66180493, 58430918, 1390497, 41950574, 39888056, 11774847, 55308824, 51969105, 7936525, 5960208, 7700566, 14529825, 14688918, 43024566, 21110140, 55797803, 31236439, 6817136, 1467168, 36028128, 60781310, 66595886, 57548184, 3194733, 8589175, 1546517, 17654773, 40572454, 63284984, 5780985, 39660177, 64050493, 55081623, 51346675, 1235123, 65633931, 66390924, 17413278, 57950994, 55911330, 11814853, 31357211, 56038385, 40038565, 64400706, 35034758, 60296238, 6527713, 5685238, 1062701, 63406447, 64008930, 63476297, 5114652, 20060374, 10085133, 61328568, 9435001, 56057656, 49934674, 39661404, 19616499, 34342717, 46653815, 45614269, 59290211, 31296803, 50605437, 46928301, 58562582, 63879452, 65733359, 51086476, 40601201, 9845217, 29213549, 41227222, 7337659, 46517072, 38610653, 9694813, 21350102, 46716202, 50535150, 39729407, 22263578, 25987787, 64913590, 19636684, 59311687, 4295372, 571012, 20588847, 63424767, 1099384, 3810242, 5604591, 39760687, 43739869, 56019939, 51526987, 45494853, 4302122, 21009984, 66210827, 67255219, 46613149, 63345017, 43570211, 62002161, 2214708, 4234697, 51055338, 19647002, 28593783, 6804647, 40542044, 42263319, 4784593, 19636686, 44015285, 55697847, 5814660, 15409525, 2307393, 54404433, 15490230, 62245810, 64969544, 48120716, 41040511, 51176224, 6376426, 60386775, 826517, 27601385, 8185587, 28564285, 68613325, 58623041, 60941473, 1635691, 7729270, 46417835, 57285778, 55960993, 66510262, 60285691, 61902329, 68565071)


lc_B <- c(62012715, 49974687, 27570947, 63417796, 61449107, 12906517, 57074291, 21021086, 404854, 15139172, 46774978, 50486061, 4305577, 65783354, 48544529, 31667129, 36980133, 19117791, 3845908, 846821, 40381968, 64018601, 57184860, 49963980, 44142706, 6327771, 20811335, 67336862, 3628833, 31247310, 4764984, 1619549, 56492219, 67959628, 61672211, 1472227, 55268407, 13497237, 57538143, 43096178, 35723158, 226780, 2307012, 1210773, 50273799, 28903599, 50839792, 44916418, 9714937, 51876659, 3919804, 12968154, 54978278, 6938022, 53854432, 63350177, 39692948, 67216234, 22253060, 59099446, 46135199, 11717805, 48596572, 8475061, 61462130, 21480483, 2014943, 41430440, 43196143, 243173, 61543762, 66562164, 67878273, 41100627, 11915326, 28753020, 12617369, 59090559, 55583726, 31256585, 544537, 61430245, 1681767, 7670078, 38506546, 36500594, 31367711, 46694948, 2080069, 38457330, 54524836, 27651989, 63358477, 62002922, 8995111, 45694307, 61470409, 17933815, 27370082, 66612753, 1536521, 54948920, 57548472, 876991, 40127147, 57365210, 1904740, 3195692, 743529, 67408356, 8766184, 23643466, 51336378, 13397002, 3700020, 49935259, 38455198, 63506356, 11386690, 32479126, 6300017, 67427011, 63344398, 51366616, 727247, 59291548, 21551336, 8776003, 16111335, 1051513, 61973285, 60764833, 59190150, 25406927, 10138072, 61361677, 32279884, 63337618, 49933340, 30565592, 3217416, 61883095, 63436296, 58290318, 29884855, 50353289, 14699170, 67625637, 6815821, 2286867, 6274586, 17853756, 55948157, 6995898, 44126015, 66643915, 41338910, 8626219, 67858810, 38597465, 45884338, 565018, 46436141, 15259622, 6594706, 39479497, 5535388, 5855546, 48734782, 2896555, 67296211, 713979, 33110251, 8987918, 1224687, 5637315, 484473, 9814600, 29694710, 60902260, 25897153, 40705483, 1439301, 3055155, 26319992, 6245002, 66441896, 46427698, 36330836, 8915199, 46205024, 62459417, 3497439, 54888931, 30475522, 38998249, 12636103, 60536957)
lc_B <- c(lc_B, 27521279, 2365984, 361549, 43430210, 35843833, 9768308, 12705933, 59179388, 60830121, 67929084, 36138408, 854552, 8865548, 13096420, 23836169, 61502149, 1621627, 11426617, 48274995, 41123011, 7296181, 29635336, 30565882, 8145149, 46116481, 21119590, 43894290, 65866235, 44143687, 873468, 12419378, 26378681, 55140334, 56964922, 61682200, 14338072, 65047247, 57267246, 59581503, 41093708, 48524124, 513842, 1685090, 42723216, 60647576, 55341080, 9735578, 41110083, 30255415, 56010965, 63214550, 67828966, 671468, 38540004, 65107371, 18645038, 26017706, 660734, 573283, 9454644, 64017354, 617449, 7645594, 43286428, 55941273, 8636865, 31226902, 46194753, 6160505, 1412225, 65741544, 24084859, 58532795, 41880754, 45515321, 60585561, 65272380, 7937327, 1489732, 17553239, 7638498, 1473206, 38162164, 3355990, 15610681, 57025137, 6254978, 38162571, 52768311, 5938741, 58101279, 18895673, 30175739, 38222417, 55909312, 65663878, 6607837, 24725076, 61722475, 11895058, 28182084, 185962, 55259655, 16241080, 66602227, 5781939, 60801476, 6996130, 12346893, 65672013, 19076244, 1475379, 9056893, 59492895, 56864322, 60942704, 44015940, 62225220, 39739191, 66435524, 44199929, 59471139, 38547168, 6205030, 38615829, 6698930, 66514563, 1623685, 60545969, 46703319, 39739315, 12636426, 65364691, 16403147, 9204637, 19306532, 66270322, 65653692, 22313524, 59082682, 19796545, 10766253, 50436003, 49363132, 27600713, 44865530, 57763719, 47857115, 48535477, 65986020, 58603818, 42934257, 1167844, 66390187, 58281312, 63888770, 48596526, 67385135, 24775459, 55090096, 12347068, 37317537, 64007908, 1683908, 11976597, 41019342, 6855113, 7964638, 65701227, 44037648, 23133074, 9787718, 61389384, 38418035, 33130454, 13038119, 14639242, 38505864, 65725266, 62904623, 68513661, 36039498, 6538734, 51857455, 59139740, 64341225, 21430833, 55455899, 17795459, 65128493, 46428798, 43216120, 59199242, 50364311, 41079485, 27711293, 63218354, 65492649, 50819365, 40737432, 377507, 65736437, 61488876, 44886450, 31467727, 46651816, 11914779, 65352381, 24726593, 52989922, 43105128, 34322310, 8669148, 12795739, 38485516, 39559934, 4280915, 63437401, 7103037, 44946049, 15400322, 28583975, 59592185, 877645, 56019484, 3372858, 60556772, 19846532, 11658194, 6894823, 61414862, 52708301, 48806212, 12204849, 60863986, 3919883, 37661631, 47210580, 14689912, 23393084, 60961679, 6170889, 55191727, 14690280, 42415518, 65855022, 62156039, 38536464, 44603544, 63527328, 48182146, 25867085, 61952845, 4744682, 20110370, 65854766, 57722242, 11438361, 34111919, 53262232, 12247443, 64210396, 37630339, 41237564, 46722148, 65791211, 16882760, 7719304, 37622016, 3220774, 51906280, 12446784, 50064210, 57733299, 63437152, 38445791, 3730324, 56052115, 57354312, 58010576, 626701, 7224706, 64079786, 62167132, 8396526, 7625377, 12707224, 35084508, 56022111, 52027979, 43215589, 50425264, 59253209, 28312549, 67376619, 30795837, 43869662, 20849433, 55351366, 39549686, 22972745, 1025579)


# The specific member IDs in lc_A and lc_B are not in dataset lendingclub
lendingclub_ab <- lendingclub %>%
    mutate(Group=ifelse(member_id %in% lc_A, "A", ifelse(member_id %in% lc_B, "B", "C")))


# ggplot(lendingclub_ab, aes(x=Group, y=loan_amnt)) + geom_boxplot()

#conduct a two-sided t-test
# t.test(loan_amnt ~ Group, data=lendingclub_ab)


#build lendingclub_multi
# lendingclub_multi <-lm(loan_amnt ~ Group + grade + verification_status, data=lendingclub_ab)

#examine lendingclub_multi results
# broom::tidy(lendingclub_multi)

```
  
  
  
***
  
Chapter 3 - Randomized Complete (and Balanced Incomplete) Block Designs  
  
Intro to NHANES Dataset and Sampling:  
  
* NHANES is the National Health and Nutrition Examination Study, run once every 2 years in the US since the late 1990s (was run on different frequency since the 1960s)  
* NHANES individuals are sampled from a scheme to match the US demographics - upsampling of elderly and minorities for sufficient sample size for statistical conclusions  
* Two key types of sampling  
	* Probability sampling - probability is used to select the sample (will be covered in this course)  
    * Non-probability sampling - voluntary (whoever responds), convenience (whoever the researcher can find)  
* Many types of random sampling can be run in R  
	* Simple Random Sampling - sample()  
    * Stratified Sampling - dataset %>% group_by(strata_variable) %>% sample_n()  # sample a specified number of people inside each segment  
    * Cluster Sampling - cluster(dataset, cluster_var_name, number_to_select, method = "option")  # select everyone in each randomly select cluster  
    * Systematic Sampling - every 5th or 10th or etc. person (implemented by custom functions)  
    * Multi-Stage Sampling - combinations of 2+ of the above approaches in a sensible and structured manner  
  
Randomized Complete Block Designs (RCBD):  
  
* RCBD is run when there is a potential nuisance factor in the data that might otherwise impact the results and conclusions  
	* Randomized - treatment is assigned randomly inside each block  
    * Complete - each treatment is used the same number of times inside each block  
    * Block - experimental groups are blocked to be similar (differences within the group are expected to be lesser than differences across the groups)  
    * Design - the experiment  
    * "Block what you can, randomize what you cannot"  
* The library(agricolae) allows for drawing some of the experimental designs such as an RCBD  
	* library(agricolae)  
    * trt <- letters[1:4]  
    * rep <- 4  
    * design.rcbd <- design.rcbd(trt, r = rep, seed = 42, serie = 0)  # serie has to do with tagging of number blocks  
    * design.rcbd$sketch  
  
Balanced Incomplete Block Designs (BIBD):  
  
* Incomplete blocaks are when you cannot fully fit a treatment inside a block  
	* Balanced - each pair of treatments occur together in a block an equal number of times  
    * Incomplete - not every treatment will appear in every block  
    * Block - experimental groups are blocked to be similar (differences within the group are expected to be lesser than differences across the groups  
    * Design - the experiment  
* Suppose that t is the number of treatments, k is the number of treatments per block, and r is the number of replications  
	* lambda = r * (k - 1) / (t - 1)  
    * If lambda is a whole number, then a BIBD is possible; otherwise, it is not  
  
Example code includes:  
```{r}

nhanes_demo <- readr::read_csv("./RInputFiles/nhanes_demo.csv")
nhanes_medical <- readr::read_csv("./RInputFiles/nhanes_medicalconditions.csv")
nhanes_bodymeasures <- readr::read_csv("./RInputFiles/nhanes_bodymeasures.csv")
dummy_nhanes_final <- readr::read_csv("./RInputFiles/nhanes_final.csv")

#merge the 3 datasets you just created to create nhanes_combined
nhanes_combined <- list(nhanes_demo, nhanes_medical, nhanes_bodymeasures) %>%
  Reduce(function(df1, df2) inner_join(df1, df2, by="seqn"), .)


#fill in the dplyr code
nhanes_combined %>% group_by(mcq365d) %>% summarise(mean = mean(bmxwt, na.rm = TRUE))

#fill in the ggplot2 code
nhanes_combined %>% filter(ridageyr > 16) %>% 
  ggplot(aes(x=as.factor(mcq365d), y=bmxwt)) +
  geom_boxplot()


#filter out anyone less than 16
nhanes_filter <- nhanes_combined %>% filter(ridageyr > 16)

#use simputation & impute bmxwt to fill in missing values
nhanes_final <- simputation::impute_median(nhanes_filter, bmxwt ~ riagendr)

#recode mcq365d with ifelse() & examine with table()
nhanes_final$mcq365d <- ifelse(nhanes_final$mcq365d==9, 2, nhanes_final$mcq365d)
table(nhanes_final$mcq365d)


#use sample() to create nhanes_srs
nhanes_srs <- nhanes_final[sample(nrow(nhanes_final), 2500), ]

#create nhanes_stratified with group_by() and sample_n()
nhanes_stratified <- nhanes_final %>%
  group_by(riagendr) %>%
  sample_n(2000)
table(nhanes_stratified$riagendr)

#load sampling package and create nhanes_cluster with cluster()
nhanes_cluster <- sampling::cluster(nhanes_final, "indhhin2", 6, method = "srswor")


#use str() to view design.rcbd's criteria
str(agricolae::design.rcbd)

#build trt and rep
trt <- LETTERS[1:5]
rep <- 4

#Use trt and rep to build my.design.rcbd and view the sketch part of the object
my_design_rcbd <- agricolae::design.rcbd(trt, r=rep, seed = 42, serie=0)
my_design_rcbd$sketch


#make nhanes_final$riagendr a factor variable
nhanes_final$riagendr <- factor(nhanes_final$riagendr)

#use aov() to create nhanes_rcbd
nhanes_rcbd <- aov(bmxwt ~ mcq365d + riagendr, data=nhanes_final)

#check the results of nhanes_rcbd with summary()
summary(nhanes_rcbd)

#print the difference in weights by mcq365d and riagendr
nhanes_final %>% group_by(mcq365d, riagendr) %>% summarise(mean_wt = mean(bmxwt))


#set up the 2x2 plotting grid and then plot nhanes_rcbd
par(mfrow=c(2, 2))
plot(nhanes_rcbd)
par(mfrow=c(1, 1))

#run the code to view the interaction plots
with(nhanes_final, interaction.plot(mcq365d, riagendr, bmxwt))

#run the code to view the interaction plots
with(nhanes_final, interaction.plot(riagendr, mcq365d, bmxwt))


#create my_design_bibd_1
# my_design_bibd_1 <- design.bib(LETTERS[1:3], k = 4, r = 16, serie = 0, seed = 42)  # will throw an error

#create my_design_bibd_2
# my_design_bibd_2 <- design.bib(letters[1:2], k = 3, r = 5, serie = 0, seed = 42)  # will throw warning

#create my_design_bibd_3
my_design_bibd_3 <- agricolae::design.bib(letters[1:4], k = 4, r = 6, serie = 0, seed = 42)
my_design_bibd_3$sketch


lambda <- function(t, k, r){
  return((r*(k-1)) / (t-1))
}

#calculate lambda
lambda(4, 3, 3)


#build the data.frame
creatinine <- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22)
food <- as.factor(c("A", "C", "D", "A", "B", "C", "B", "C", "D", "A", "B", "D"))
color <- as.factor(rep(c("Black", "White", "Orange", "Spotted"), each = 3))
cat_experiment <- as.data.frame(cbind(creatinine, food, color))

#create cat_model & then wrong_cat_model and examine them with summary()
cat_model <- aov(creatinine ~ food + color, data=cat_experiment)
summary(cat_model)


#calculate lambda
lambda(3, 3, 2)

#create weightlift_model & examine results (variable does not exist in dataset)
# weightlift_model <- aov(bmxarmc ~ weightlift_treat + ridreth1, data=nhanes_final)
# summary(weightlift_model)

```
  
  
  
***
  
Chapter 4 - Latin Squares, Graeco-Latin Squares, Factorial Experiments  
  
Latin Squares have two blocking factors, assumed not to interact with each other or the treatment, and each with the same number of levels:  
  
* Latin squares can be analyzed just like an RCBD  
* In a Latin square, both the rows and the columns are the blocking factors  
* Can use nyc_scores dataset containing reading, writing, and math scores from all accredited high schools  
	* Goal is to assess the impact of a (fabricated) tutoring program on the scores by school  
  
Graeco-Latin Squares builds on Latin squares by adding an additional blocking factor:  
  
* Three blocking factors, all with the same number of levels, and assumed not to interact with each other or the treatment  
	* Greek letters added next to the Latin letters indicate the third blocking factors (can use Latin and numbers instead)  
    * All of the combinations occur only once (each letter once per row/column, and each number once per letter)  
  
Factorial Experiments - designs in which 2+ variables are crossed in an experiment, with each combination considered a factor:  
  
* Example of testing all combinations of high/low water and high/low light - each combination is tested, with TukeyHSD() applied after  
* This course will focus on 2^k factor experiments, meaning that each level has only a High/Low (or similar) possibility  
  
Next steps:  
  
* Many other types of factorial designs - do not all need to be 2**k, with many factor levels  
	* Might consider a fractional factorial design to minimize the analytical burden  
* Design should be a valued and integrated part of the process  
* There will always be some unmeasured confounders, but good design can help to reduce that noise  
  
Example code includes:  
```{r}

nyc_scores <- readr::read_csv("./RInputFiles/nyc_scores.csv")
glimpse(nyc_scores)


tEL <- c('PhD', 'BA', 'BA', 'MA', 'MA', 'PhD', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'College Student', 'Grad Student', 'MA', 'MA', 'MA', 'BA', 'MA', 'BA', 'MA', 'College Student', 'PhD', 'MA', 'MA', 'BA', 'MA', 'College Student', 'BA', 'PhD', 'Grad Student', 'MA', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'MA', 'Grad Student', 'BA', 'BA', 'College Student', 'Grad Student', 'College Student', 'BA', 'BA', 'PhD', 'BA', 'Grad Student', 'Grad Student', 'College Student', 'College Student', 'BA', 'PhD', 'College Student', 'PhD', 'PhD', 'PhD', 'College Student', 'Grad Student', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'MA', 'MA', 'College Student', 'Grad Student', 'MA', 'PhD', 'MA', 'College Student', 'MA', 'PhD', 'MA', 'College Student', 'College Student', 'Grad Student', 'PhD', 'MA', 'MA', 'Grad Student', 'MA', 'MA', 'Grad Student', 'PhD', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'PhD', 'BA', 'MA', 'Grad Student', 'BA', 'College Student', 'MA', 'College Student', 'Grad Student', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'Grad Student', 'College Student', 'College Student', 'College Student', 'MA', 'BA', 'Grad Student', 'Grad Student', 'MA', 'College Student', 'BA', 'Grad Student', 'MA', 'Grad Student', 'PhD', 'MA', 'MA', 'College Student', 'MA', 'College Student', 'PhD', 'College Student', 'MA', 'MA', 'MA', 'MA', 'College Student', 'MA', 'BA', 'MA', 'Grad Student', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'MA', 'College Student', 'MA', 'MA', 'BA', 'MA', 'College Student', 'Grad Student', 'College Student', 'MA', 'BA', 'MA', 'BA', 'College Student', 'Grad Student', 'Grad Student', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'BA', 'MA', 'BA', 'College Student', 'MA', 'BA', 'MA', 'Grad Student', 'MA', 'PhD', 'MA', 'BA', 'Grad Student', 'MA', 'BA', 'BA', 'MA', 'BA', 'College Student', 'BA', 'MA', 'MA', 'BA', 'MA', 'College Student', 'BA', 'Grad Student', 'MA', 'BA', 'MA', 'MA', 'MA', 'BA', 'College Student', 'College Student')
tEL <- c(tEL, 'BA', 'Grad Student', 'BA', 'BA', 'MA', 'Grad Student', 'BA', 'MA', 'BA', 'PhD', 'MA', 'MA', 'MA', 'BA', 'College Student', 'PhD', 'BA', 'Grad Student', 'BA', 'College Student', 'BA', 'MA', 'College Student', 'MA', 'College Student', 'Grad Student', 'College Student', 'MA', 'PhD', 'BA', 'PhD', 'Grad Student', 'BA', 'BA', 'MA', 'MA', 'BA', 'PhD', 'College Student', 'MA', 'BA', 'College Student', 'BA', 'MA', 'College Student', 'MA', 'College Student', 'BA', 'MA', 'BA', 'BA', 'MA', 'PhD', 'BA', 'MA', 'Grad Student', 'College Student', 'MA', 'College Student', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'Grad Student', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'Grad Student', 'Grad Student', 'MA', 'Grad Student', 'MA', 'BA', 'College Student', 'MA', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'College Student', 'College Student', 'College Student', 'College Student', 'College Student', 'PhD', 'MA', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'MA', 'MA', 'PhD', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'Grad Student', 'MA', 'MA', 'PhD', 'BA', 'BA', 'Grad Student', 'Grad Student', 'PhD', 'BA', 'BA', 'Grad Student', 'College Student', 'BA', 'College Student', 'MA', 'MA', 'MA', 'Grad Student', 'BA', 'BA', 'MA', 'Grad Student', 'PhD', 'BA', 'Grad Student', 'Grad Student', 'Grad Student', 'BA', 'MA', 'BA', 'College Student', 'College Student', 'Grad Student', 'MA', 'Grad Student', 'Grad Student', 'BA', 'BA', 'MA', 'College Student', 'BA', 'Grad Student', 'Grad Student', 'College Student', 'Grad Student', 'College Student', 'PhD', 'BA', 'MA', 'MA', 'BA', 'College Student', 'College Student', 'PhD', 'MA', 'BA', 'MA', 'MA', 'Grad Student', 'MA', 'PhD', 'MA', 'MA', 'Grad Student', 'College Student', 'MA', 'BA', 'BA', 'College Student', 'Grad Student', 'BA', 'MA', 'MA', 'Grad Student', 'BA', 'Grad Student', 'Grad Student', 'MA', 'PhD', 'Grad Student', 'Grad Student', 'MA', 'MA', 'PhD', 'College Student', 'College Student', 'MA', 'BA', 'MA', 'College Student', 'MA', 'PhD', 'BA', 'MA', 'College Student', 'PhD', 'PhD', 'College Student', 'MA', 'MA', 'MA', 'PhD', 'MA', 'BA', 'College Student', 'BA', 'BA', 'MA', 'MA', 'College Student', 'College Student', 'Grad Student', 'College Student', 'MA', 'MA', 'MA', 'Grad Student', 'MA', 'College Student', 'Grad Student', 'BA', 'Grad Student', 'BA', 'MA', 'College Student', 'MA')


nyc_scores <- nyc_scores %>%
    mutate(Teacher_Education_Level=tEL)
glimpse(nyc_scores)


#mean, var, and median of Math score
nyc_scores %>%
    group_by(Borough) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))

#mean, var, and median of Math score
nyc_scores %>%
    group_by(Teacher_Education_Level) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))

#mean, var, and median of Math score
nyc_scores %>%
    group_by(Borough, Teacher_Education_Level) %>% 
    summarise(mean = mean(Average_Score_SAT_Math, na.rm=TRUE),
              var = var(Average_Score_SAT_Math, na.rm=TRUE),
              median = median(Average_Score_SAT_Math, na.rm=TRUE))


# If we want to use SAT scores as our outcome, we need to examine their missingness
# First, look at the pattern of missingness using md.pattern() from the mice package
# There are 60 scores missing in each of the scores
# There are many R packages which help with more advanced forms of imputation, such as MICE, Amelia, mi, and more
# We will use the simputation andimpute_median() as we did previously

#examine missingness with md.pattern()
mice::md.pattern(nyc_scores)

#impute the Math, Writing, and Reading scores by Borough
nyc_scores_2 <- simputation::impute_median(nyc_scores, Average_Score_SAT_Math ~ Borough)

#convert Math score to numeric
nyc_scores_2$Average_Score_SAT_Math <- as.numeric(nyc_scores_2$Average_Score_SAT_Math)

#examine scores by Borough in both datasets, before and after imputation
nyc_scores %>% 
  group_by(Borough) %>% 
  summarise(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE))
nyc_scores_2 %>% 
  group_by(Borough) %>% 
  summarise(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE))


#design a LS with 5 treatments A:E then look at the sketch
my_design_lsd <- agricolae::design.lsd(LETTERS[1:5], serie=0, seed=42)
my_design_lsd$sketch


# To execute a Latin Square design on this data, suppose we want to know the effect of of our tutoring program, which includes one-on-one tutoring, two small groups, and an in and after school SAT prep class
# A new dataset nyc_scores_ls is available that represents this experiment. Feel free to explore the dataset in the console.

# We'll block by Borough and Teacher_Education_Level to reduce their known variance on the score outcome
# Borough is a good blocking factor because schools in America are funded partly based on taxes paid in each city, so it will likely make a difference on quality of education

lsID <- c('11X290', '10X342', '09X260', '09X412', '12X479', '14K478', '32K554', '14K685', '22K405', '17K382', '05M692', '02M427', '02M308', '03M402', '02M282', '30Q501', '26Q495', '24Q455', '29Q326', '25Q670', '31R450', '31R445', '31R080', '31R460', '31R455')
lsTP <- c('One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'Small Groups (4-6)', 'SAT Prep Class (school hours)')

nyc_scores_ls <- nyc_scores_2 %>%
    filter(School_ID %in% lsID) %>%
    mutate(Tutoring_Program=lsTP)


#build nyc_scores_ls_lm
nyc_scores_ls_lm <- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level,
                       data=nyc_scores_ls
                       )

#tidy the results with broom
nyc_scores_ls_lm %>% broom::tidy()

#examine the results with anova
nyc_scores_ls_lm %>% anova()


#create a boxplot of Math scores by Borough, with a title and x/y axis labels
ggplot(nyc_scores, aes(x=Borough, y=Average_Score_SAT_Math)) + 
  geom_boxplot() + 
  ggtitle("Average SAT Math Scores by Borough, NYC") + 
  xlab("Borough (NYC)") + 
  ylab("Average SAT Math Scores (2014-15)")


#create trt1 and trt2
trt1 <- LETTERS[1:5]
trt2 <- 1:5

#create my_graeco_design
my_graeco_design <- agricolae::design.graeco(trt1, trt2, serie=0, seed=42)

#examine the parameters and sketch
my_graeco_design$parameters
my_graeco_design$sketch


glsID <- c('09X241', '10X565', '09X260', '07X259', '11X455', '18K563', '23K697', '32K403', '22K425', '16K688', '02M135', '06M348', '02M419', '02M489', '04M495', '30Q502', '24Q530', '30Q555', '24Q560', '27Q650', '31R440', '31R064', '31R450', '31R445', '31R460')
glsTP <- c('SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'One-on-One', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'Small Groups (4-6)', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (2-3)', 'SAT Prep Class (school hours)', 'SAT Prep Class (after school)', 'One-on-One', 'Small Groups (4-6)')
glsHT <- c('Small Group', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Large Group', 'Individual', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Mix of Large Group/Individual', 'Mix of Small Group/Individual', 'Small Group', 'Large Group', 'Individual')


nyc_scores_gls <- nyc_scores_2 %>%
    filter(School_ID %in% glsID) %>%
    mutate(Tutoring_Program=glsTP, Homework_Type=glsHT)


#build nyc_scores_gls_lm
nyc_scores_gls_lm <- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level + Homework_Type, data=nyc_scores_gls)

#tidy the results with broom
nyc_scores_gls_lm %>% broom::tidy()

#examine the results with anova
nyc_scores_gls_lm %>% anova()


pctTHL <- c(1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2)
pctBHL <- c(2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1)
tP <- c('Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No')

nyc_scores <- nyc_scores %>%
    select(-Teacher_Education_Level) %>%
    mutate(Percent_Tested_HL=factor(pctTHL), Percent_Black_HL=factor(pctBHL), Tutoring_Program=factor(tP))


#build the boxplots for all 3 factor variables: tutoring program, pct black, pct tested
ggplot(nyc_scores, aes(x=Tutoring_Program, y=Average_Score_SAT_Math)) + 
    geom_boxplot()

ggplot(nyc_scores, aes(x=Percent_Black_HL, y=Average_Score_SAT_Math)) + 
    geom_boxplot()

ggplot(nyc_scores, aes(x=Percent_Tested_HL, y=Average_Score_SAT_Math)) + 
    geom_boxplot()


#create nyc_scores_factorial and examine the results
nyc_scores_factorial <- aov(Average_Score_SAT_Math ~ Percent_Tested_HL * Percent_Black_HL * Tutoring_Program, data=nyc_scores)
broom::tidy(nyc_scores_factorial)


#use shapiro.test() to test the outcome
shapiro.test(nyc_scores$Average_Score_SAT_Math)

#plot nyc_scores_factorial to examine residuals
par(mfrow = c(2, 2))
plot(nyc_scores_factorial)
par(mfrow = c(1, 1))

```
  
  
  
***
  
###_Structural Equation Modeling with lavaan in R_  
  
Chapter 1 - One-Factor Models  
  
Model Specification - Structural Equation Models (SEM) - explore relationships between variables:  
  
* Can confirm the structure of a developed model also  
* Two variable types - manifest (directly measured) which are represented by squares, and latent (abstract underlying phenomenon) represented as circles  
	* The manifest variables are assumed to be driven by the latent variables (such as intelligence)  
* Can set up an analysis in R using lavaan based on 1939 intelligence data  
	* library(lavaan)  
    * data(HolzingerSwineford1939)  
    * example model <- 'latent_variable =~ manifest_variable1 + manifest_variable2 + ...'  # latent_variable can have any name not in dataset, =~ is direction of prediction  
    * visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'  # x1, x2, x3, x7, x8, x9 are visual components inside the 1939 dataset  
  
Model Analysis:  
  
* Degrees of freedom are based on df = Possible Values - Estimated Values  
	* Possible Values = Manifest Variables * (Manifest Variables + 1) / 2  
    * Models need to have at least 3 manifest variables and df > 0  
    * Can use scaling and constraints to control degrees of freedom - managed inside lavaan but can modify defaults  
* Can run the models using lavaan in R  
	* visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'  
    * visual.fit <- cfa(model = visual.model, data = HolzingerSwineford1939)  # include the previously defined model and the data frame  
    * summary(visual.fit)  # basic information about the model  
    * The loadings (weightings) for each of the manifest variables will be shown, typically with the first coefficient set to 1 as per the scaling  
    * The variance estimates are also provided for each of the variables - should be positive, but can be negative (needs to be troubleshot)  
  
Model Assessment:  
  
* Standardized loadings measure the strength of the relationships between the manifest variables and the latent variables  
	* Can be measured based on the estimates, relative to the variable that was set as 1.00  
    * Can instead use the standardized solution based on the z-scores  
    * summary(visual.fit, standardized = TRUE)  # to get the standardized solution (Std.all column, with close to 1 being best; Std.lv being scaled like the loading variable)  
* The model fit measures how well the data fit the specified model  
	* Goodness of fit indices like the Comparative Fit Index or the Tucker Lewis Index - goal is closer to 1 and 0.9+  
    * Badness of fit indices like RMSE Approximation or Standardized Root Mean Square Residual (SRMR) - goal is lower and 0.1-  
    * summary(visual.fit, standardized = TRUE, fit.measures = TRUE)  # will show most common fit meaasures  
  
Example code includes:  
```{r}

#Load the lavaan library
library(lavaan)

#Look at the dataset
data(HolzingerSwineford1939, package="lavaan")
head(HolzingerSwineford1939[ , 7:15])

#Define your model specification
text.model <- "textspeed =~ x4 + x5 + x6 + x7 + x8 + x9"

#Analyze the model with cfa()
text.fit <- lavaan::cfa(model=text.model, data=HolzingerSwineford1939)

#Summarize the model
summary(text.fit)
summary(text.fit, standardized=TRUE)
summary(text.fit, fit.measures=TRUE)


#Look at the dataset
data(PoliticalDemocracy, package="lavaan")
head(PoliticalDemocracy)

#Define your model specification
politics.model <- "poldemo60 =~ y1 + y2 + y3 + y4"

#Analyze the model with cfa()
politics.fit <- lavaan::cfa(model = politics.model, data = PoliticalDemocracy)

#Summarize the model
summary(politics.fit, standardized=TRUE, fit.measures=TRUE)

```
  
  
  
***
  
Chapter 2 - Multi-Factor Models  
  
Multifactor Specification - exploring multiple latent relationships, and their relationships to each other:  
  
* Combining manifest variables that represent different latent variables often results in a model with poor fit  
* Can instead convert each of the manifest variables to the appropriate latent variable, for example  
	* visual.model <- 'visual =~ x1 + x2 + x3'  
    * visual.fit <- cfa(model = visual.model, data = HolzingerSwineford1939)   
    * speed.model <- 'speed =~ x7 + x8 + x9'  
    * speed.fit <- cfa(model = speed.model, data = HolzingerSwineford1939)  
* However, having too many models can lead to having zero degrees of freedom; constraints (such as same loading for x2/x3) are used to address this  
	* visual.model <- 'visual =~ x1 + a*x2 + a*x3'  # The a means that x2 and x3 will be set equal to each other, while a number rather than a would use that exact number  
* One larger model can sometimes better capture all the relationships  
	* twofactor.model <- 'visual =~ x1 + x2 + x3 \n speed =~ x7 + x8 + x9'  # adding them all at the same time (must have new line for new model)  
    * twofactor.fit <- cfa(model = twofactor.model, data = HolzingerSwineford1939)  
    * summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)  
  
Model Structure:  
  
* The two-factor model assumes there is a covariant relationship between the latent variables - basically, one latent variable can predict another  
* Can see the correlation between the standardized variables using the summary() function - technically shows R-squared  
	* =~ creates latent variables  
    * ~~ creates covariances between variables  
    * ~ creates direct prediction between variables  
    * if there is a newline followed by 'speed ~~ 0*visual' then speed will be assumed NOT to vary at all with visual  
    * if there is a newline followed by 'speed ~ visual' then there is assumed to be a direct relationship between these variables  
  
Modification Indices:  
  
* If a model has a poor fit, can examine the standardized solutions - desire is to see loading greater than 0.3  
* Model problems can often be identified by variances that are very high relative to the raw data  
* Modification indices can help show the improvement in the model when an additional index is added  
	* modificationindices(twofactor.fit, sort = TRUE)  
    * Output will be lhs op rhs (left-hand side, operator, right-hand-side) followed by mi (modification index, a form of chi-squared)  
    * Parameters should be added one at a time, and only if they "make theoretical sense"  
    * Take the desired path(s) and add them as new lines in the model  
  
Model Comparison:  
  
* Can create and save two models, then analyze both using the same cfa(), then use anova() to compare the models  
	* anova(twofactor.fit, twofactor.fit1)  
    * This is only useful for nested models that otherwise share the same variables  
* Can also compare the fit indices using more detailed criteria  
	* fitmeasures(twofactor.fit)  
    * AIC (lower is better, including more negative better than less negative)  
    * ECVI is the likelihood of replicating the model with the same sample size and population (lower is better)  
    * fitmeasures(twofactor.fit1, c("aic", "ecvi"))  
  
Example code includes:  
```{r}

#Create your text model specification
text.model <- 'text =~ x4 + x5 + x6'

#Analyze the model
text.fit <- cfa(model=text.model, data=HolzingerSwineford1939)

#Summarize the model
summary(text.fit, standardized = TRUE, fit.measures = TRUE)


#Update the model specification by setting two paths to the label a
text.model <- 'text =~ x4 + a*x5 + a*x6'

#Analyze the model
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)

#Summarize the model
summary(text.fit, standardized = TRUE, fit.measures = TRUE)


#Create a two-factor model of text and speed variables
twofactor.model <- 'text =~ x4 + x5 + x6
speed =~ x7 + x8 + x9'

#Previous one-factor model output
summary(text.fit, standardized = TRUE, fit.measures = TRUE)

#Two-factor model specification
twofactor.model <- 'text =~ x4 + x5 + x6
speed =~ x7 + x8 + x9'

#Use cfa() to analyze the model
twofactor.fit <- cfa(model=twofactor.model, data=HolzingerSwineford1939)

#Use summary() to view the fitted model
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)


#Load the library and data
data(epi, package="psych")

#Specify a three-factor model with one correlation set to zero
epi.model <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
extraversion ~~ 0*neuroticism'

#Run the model
epi.fit <- cfa(model = epi.model, data = epi)

#Examine the output 
summary(epi.fit, standardized = TRUE, fit.measures = TRUE)


#Specify a three-factor model where lying is predicted by neuroticism
epi.model <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
lying ~ neuroticism'


#Run the model
epi.fit <- cfa(model = epi.model, data = epi)

#Examine the output 
summary(epi.fit, standardized = TRUE, fit.measures = TRUE)

#Calculate the variance of V1
var(epi$V1, na.rm=TRUE)

#Examine the modification indices
modificationindices(epi.fit, sort=TRUE)


#Edit the model specification
epi.model1 <- 'extraversion =~ V1 + V3 + V5 + V8
neuroticism =~ V2 + V4 + V7 + V9
lying =~ V6 + V12 + V18 + V24
neuroticism =~ V3'

#Reanalyze the model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Summarize the updated model
summary(epi.fit1, standardized = TRUE, fit.measures = TRUE)


#Analyze the original model
epi.fit <- cfa(model = epi.model, data = epi)

#Analyze the updated model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Compare those models
anova(epi.fit, epi.fit1)


#Analyze the original model
epi.fit <- cfa(model = epi.model, data = epi)

#Find the fit indices for the original model
fitmeasures(epi.fit)[c("aic", "ecvi")]

#Analyze the updated model
epi.fit1 <- cfa(model = epi.model1, data = epi)

#Find the fit indices for the updated model
fitmeasures(epi.fit1)[c("aic", "ecvi")]

```
  
  
  
***
  
Chapter 3 - Troubleshooting Model Errors and Diagrams  
  
Heywood Cases on the Latent Variable:  
  
* Heywood cases (defined by Heywood in 1931) are cases where correlations (greater than 1) or variances (negative) are out of bounds  
* The lavaan package will throw a warning that the matrix of latent variables is "not positive definite"  
	* Usually occurs because one of the latent variables is really a combination of the others  
    * Can then identify the highly correlated variables, and collapse them in to a single equation (fewer factors or the like)  
  
Heywood Cases on the Manifest Variable (negative error variances):  
  
* Generally occur dur to a mis-specified (under-specified) model, small sample sizes, manifest variables on vastly different scales, etc.  
* The lavaan package will throw a warning that "model has not converged"  
	* summary(negative.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)  # rsquare can help to identify the issue; variance in each manifest variable should be (0, 1)  
* Can just freeze the variance of one of the wonky variables to its variance in the raw data  
	* negative.model <- 'latent1 =~ V1 + V2 + V3\nlatent2 =~ V4 + V5 + V6\nV2 ~~ 18.83833*V2'   # 18.84 is var(V2)  
  
Create Diagrams with semPaths():  
  
* The semPlot library allows for diagramming the fit models  
	* library(semPlot)  
    * twofactor.model <- 'text =~ x4 + x5 + x6\nspeed =~ x7 + x8 + x9'  
    * twofactor.fit <- cfa(model = twofactor.model, data = HolzingerSwineford1939)  
    * semPaths(object = twofactor.fit)  
* The double-headed arrows on the manifest variables are variances, and the double-headed arrows on the latent variables are covariances  
* There are many options for semPaths, and allow a few will be covered here  
	* semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1)  # std is standardized while par is parameters; edge.label.cex is the font size for the edges  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "circle")  # "tree" is the default for layouts  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "tree", rotation = 2)  # rotation can only be used for trees; 2 means left/right  
    * semPaths(object = twofactor.fit, whatLabels = "std", edge.label.cex = 1, layout = "tree", rotation = 2, what = "std", edge.color = "purple")  # what colors arrows by strength  
  
Example code includes:  
```{r}

badlatentdata <- readr::read_csv("./RInputFiles/badlatentdata.csv")
badvardata <- readr::read_csv("./RInputFiles/badvardata.csv")

adoptsurvey <- badlatentdata %>%
    select(-X1) %>%
    rename(pictures=V1, background=V2, loveskids=V3, energy=V4, wagstail=V5, playful=V6)

#Look at the data
str(adoptsurvey, give.attr=FALSE)
head(adoptsurvey)

#Build the model
adopt.model <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model = adopt.model, data = adoptsurvey)
lavInspect(adopt.fit, "cov.lv")
summary(adopt.fit, standardized=TRUE, fit.measures=TRUE)


#Edit the original model 
adopt.model <- 'goodstory =~ pictures + background + loveskids + energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model = adopt.model, data = adoptsurvey)

#Look for Heywood cases
summary(adopt.fit, standardized = TRUE, fit.measures = TRUE)



adoptsurvey <- badvardata %>%
    select(-X1) %>%
    rename(pictures=V1, background=V2, loveskids=V3, energy=V4, wagstail=V5, playful=V6)
str(adoptsurvey, give.attr=FALSE)
summary(adoptsurvey)


#Build the model
adopt.model <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful'

#Analyze the model
adopt.fit <- cfa(model=adopt.model, data=adoptsurvey)

#Summarize the model to view the negative variances
summary(adopt.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#View the variance of the problem manifest variable
var(adoptsurvey$wagstail)


#Update the model using 5 decimal places
adopt.model2 <- 'goodstory =~ pictures + background + loveskids
inperson =~ energy + wagstail + playful
wagstail~~23.07446*wagstail'

#Analyze and summarize the updated model
adopt.fit2 <- cfa(model = adopt.model2, data = adoptsurvey)
summary(adopt.fit2, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)


#Create a default picture
semPlot::semPaths(adopt.fit)

#Update the default picture
semPlot::semPaths(object = adopt.fit, layout="tree", rotation=2)

#Update the default picture
semPlot::semPaths(object = adopt.fit, layout = "tree", rotation = 2, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "blue"
                  )

```
  
  
  
***
  
Chapter 4 - Full Example and Extension  
  
Model WAIS-III IQ Scale:  
  
* WAIS-III is a four-factor model of intelligence, including verbal, working memory, perceptual organization, and processing speed  
	* Idea is that Verbal IQ drives verbal and working memory, Performance IQ drives perceptual and processing, and Verbal/Performance drive each other  
    * 4 latent variables, measured by 12 manifest variables, with 2 additional latent variables at a higher layer that drive the initial 4 latent variables  
  
Update WAIS-III Model:  
  
* Once the model is stable, can look for additional areas to further improve the model  
* Variables that are poor on loadings and are also high in variance should be further explored  
* Can also use modification indices to better understand and model the data  
	* modificationindices(wais.fit, sort = TRUE)  
  
Hierarchical Model of IQ:  
  
* One overall IQ that is the latent variable for all of the other latent variable  
	* wais.model3 <- 'verbalcomp =~ vocab + simil + inform + compreh  
    * workingmemory =~ arith + digspan + lnseq  
    * perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch  
    * simil ~~ inform  
    * general =~ verbalcomp + workingmemory + perceptorg'  # the general is a new latent variable, built from other latent variables  
* The updated model will often have the same fit indices (simply shifting parameters from covariances to loadings)  
  
Wrap Up:  
  
* Learned model syntax for lavaan (=~ for latent, ~~ for covariance/correlation, and ~ for prediction)  
* Learned to add constraints and troubleshoot Heywood cases  
* Learned one-factor, multi-factor, and hierarchical models  
  
Example code includes:  
```{r}

IQdata <- readr::read_csv("./RInputFiles/IQdata.csv")
glimpse(IQdata)
IQdata <- IQdata %>%
    select(-X1)
glimpse(IQdata)


#Build a four-factor model
wais.model <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason
processing =~ digsym + symbolsearch'

#Analyze the model
wais.fit <- cfa(model=wais.model, data=IQdata)

#Summarize the model
summary(wais.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)


#Edit the original model
wais.model <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch'

#Analyze the model
wais.fit <- cfa(model=wais.model, data=IQdata)

#Summarize the model
summary(wais.fit, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#Update the default picture
semPlot::semPaths(object = wais.fit, layout = "tree", rotation = 1, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "black"
                  )

#Examine modification indices 
modificationindices(wais.fit, sort = TRUE)


#Update the three-factor model
wais.model2 <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch
simil ~~ inform'

#Analyze the three-factor model where data is IQdata
wais.fit2 <- cfa(model=wais.model2, data=IQdata)

#Summarize the three-factor model 
summary(wais.fit2, standardized=TRUE, fit.measures=TRUE, rsquare=TRUE)

#Compare the models
anova(wais.fit, wais.fit2)


#View the fit indices for the original model
fitmeasures(wais.fit, c("aic", "ecvi"))

#View the fit indices for the updated model
fitmeasures(wais.fit2, c("aic", "ecvi"))


#Update the three-factor model to a hierarchical model
wais.model3 <- 'verbalcomp =~ vocab + simil + inform + compreh 
workingmemory =~ arith + digspan + lnseq
perceptorg =~ piccomp + block + matrixreason + digsym + symbolsearch
simil ~~ inform
general =~ verbalcomp + workingmemory + perceptorg'

#Analyze the hierarchical model where data is IQdata
wais.fit3 <- cfa(model = wais.model3, data = IQdata)

#Examine the fit indices for the old model
fitmeasures(wais.fit2, c("rmsea", "srmr"))

#Examine the fit indices for the new model
fitmeasures(wais.fit3, c("rmsea", "srmr"))


#Update the default picture
semPlot::semPaths(object = wais.fit3, layout = "tree", rotation = 1, whatLabels = "std", 
                  edge.label.cex = 1, what = "std", edge.color = "navy"
                  )

```
  
  
  
***
  
###_Working with Data in the Tidyverse_  
  
Chapter 1 - Explore Data  
  
Import data:  
  
* Begging steps of the pipeline include importing, tidying, and transforming (wrangling)  
* Focus of this course will be recatngular data including both columns (variables) and rows (observations)  
	* bakers  # 10x6 tibble  
    * tibbles are a special type of data frame - both store rectangular data in R  
* Can read the data using readr::read_csv()  
	* ?read_csv  
    * bakers <- read_csv("bakers.csv")  
    * bakers  # same 10x6 tibble  
  
Know data:  
  
* The bakeoff data includes three types of challenges - Signature, Technical, Showstopper  
* Tibble printing by default will cut off columns and just show the variables - glimpse from dplyr can help with visualizing  
	* glimpse(bakers_mini)  
    * library(skimr)  
    * skim(bakers_mini)  # skim provides statistics for every column depending on the variable types  
  
Count data - broken video that provides some code snippets:  
  
* bakers %>% distinct(series)  
* bakers %>% count(series)  
* bakers %>% group_by(series) %>% summarize(n = n())  
* bakers %>% count(aired_us, series)  
* bakers %>% count(aired_us, series) %>% mutate(prop_bakers = n/sum(n))  
* bakers %>% group_by(aired_us, series) %>% summarize(n = n()) %>% mutate(prop_bakers = n/sum(n))  
* bakers %>% count(aired_us, series) %>% count(aired_us)  
  
Example code includes:  
```{r}

# Read in "bakeoff.csv" as bakeoff
bakeoff <- readr::read_csv("./RInputFiles/bakeoff.csv")

# Print bakeoff
bakeoff


# Data set above is already OK - UNKNOWN are NA in CSV
# Filter rows where showstopper is UNKNOWN 
bakeoff %>% 
    filter(showstopper == "UNKNOWN")

# Edit to add list of missing values
bakeoff <- read_csv("./RInputFiles/bakeoff.csv", na = c("", "NA", "UNKNOWN"))

# Filter rows where showstopper is NA 
bakeoff %>% 
    filter(is.na(showstopper))


# Edit to filter, group by, and skim
bakeoff %>% 
  filter(!is.na(us_season)) %>% 
  group_by(us_season) %>%
  skimr::skim()
  

bakeoff %>% 
  distinct(result)

# Count rows by distinct results
bakeoff %>% 
  count(result)

# Count whether or not star baker
bakeoff %>% 
  count(result=="SB")


# Count the number of rows by series and episode
bakeoff %>%
  count(series, episode)

# Add second count by series
bakeoff %>% 
  count(series, episode) %>%
  count(series)


# Count the number of rows by series and baker
bakers_by_series <- 
  bakeoff %>%
  count(series, baker)

# Print to view
bakers_by_series

# Count again by series
bakers_by_series %>%
  count(series)

# Count again by baker
bakers_by_series %>%
  count(baker, sort=TRUE)


ggplot(bakeoff, aes(x=episode)) + 
    geom_bar() + 
    facet_wrap(~series)

```
  
  
  
***
  
Chapter 2 - Tame Data  
  
Cast column types:  
  
* Type-casting can be an important step in taming data  
* The readr package has options for col_type within the read_csv() function  
	* By default, all of the column types are guessed from the first 1,000 rows  
    * bakers_raw %>% dplyr::slice(1:4)  # look at the first 4 rows  
* Can convert a character to a number using parse_number()  
	* parse_number("36 years")  # will become 36  
    * bakers_tame <- read_csv(file = "bakers.csv", col_types = cols(age = col_number()) )  # col_number() will wrangle the age column to a numeric  
* Can also use the parse_date capability to manage datetime inputs  
	* parse_date("14 August 2012", format = "%d %B %Y")  
    * bakers <- read_csv("bakers.csv", col_types = cols( last_date_uk = col_date(format = "%d %B %Y") ))  # col_date() will wrangle last_date_uk to a datetime  
* There is always both a parse_* and a col_* for any given data type; can practive with parse_* then use col_* in the read-in  
  
Recode values:  
  
* The recode() function in dplyr can be used to recode values in the data  
	* young_bakers %>% mutate(stu_label = recode(student, `0` = "other", .default = "student"))  # 0 will become other, anything else will become student  
    * young_bakers %>% mutate(stu_label = recode(student, `0` = NA_character_, .default = "student"))  # create NA for a specific string  
    * young_bakers %>% mutate(student = na_if(student, 0))  # na_if will convert to NA if the condition(s) is met  
  
Select variables:  
  
* Can select just a subset of the variables using select  
* The select() function is powerful when you only need to work with a subset of the data  
	* young_bakers2 %>% select(baker, series_winner)  # keep these variables  
    * young_bakers2 %>% select(-technical_winner)  # drop these variables (signalled by the minus sign)  
* Can use helper functions inside the select() call  
	* young_bakers2 %>% select(baker, starts_with("series"))  
    * young_bakers2 %>% select(ends_with("winner"), baker)  
    * young_bakers2 %>% select(contains("bake"))  
* The filter() function works on rows rather than columns  
	* young_bakers2 %>% filter(series_winner == 1 | series_runner_up == 1)  
  
Tame variable names:  
  
* Can rename variables while selecting  
	* young_bakers3 %>% select(baker, tech_1 = tre1)  
    * young_bakers3 %>% select(baker, tech_ = tre1:tre3)  
    * young_bakers3 %>% select(baker, tech_ = starts_with("tr"), result_ = starts_with("rs"))  
* Within the rename call, it is not possible to use the helper functions  
	* young_bakers3 %>% rename(tech_1 = t_first, result_1 = r_first)  # new = old  
    * young_bakers3 %>% select(everything(), tech_ = starts_with("tr"), result_ = starts_with("rs"))  # everything first keeps all the column orders the same  
* Can also use the janitor package to help with cleaning variables  
	* young_bakers3 %>% janitor::clean_names()  
    * Converts to snake case (lower case with underscores)  
  
Example code includes:  
```{r}

# NOTE THAT THIS WILL THROW WARNINGS
# Try to cast technical as a number
desserts <- readr::read_csv("./RInputFiles/desserts.csv",
                      col_types = cols(
                        technical = col_number())
                     )

# View parsing problems
readr::problems(desserts)

# NOTE THAT THIS WILL FIX THE ERRORS
# Edit code to fix the parsing error 
desserts <- readr::read_csv("./RInputFiles/desserts.csv",
                      col_types = cols(
                        technical = col_number()),
                      na = c("", "NA", "N/A") 
                     )

# View parsing problems
readr::problems(desserts)


# Find format to parse uk_airdate 
readr::parse_date("17 August 2010", format = "%d %B %Y")

# Edit to cast uk_airdate
desserts <- readr::read_csv("./RInputFiles/desserts.csv", 
                     na = c("", "NA", "N/A"),
                     col_types = cols(
                       technical = col_number(),
                       uk_airdate = col_date("%d %B %Y")
                     ))

# Print by descending uk_airdate
desserts %>%
  arrange(desc(uk_airdate))


# Cast result a factor
desserts <- readr::read_csv("./RInputFiles/desserts.csv", 
                     na = c("", "NA", "N/A"),
                     col_types = cols(
                       technical = col_number(),
                       uk_airdate = col_date(format = "%d %B %Y"),
                       result = col_factor(levels=NULL)
                     ))
                    
# Glimpse to view
glimpse(desserts)


oldDesserts <- desserts
tempDesserts <- desserts %>%
    gather(key="type_ing", value="status", starts_with(c("showstopper")), starts_with(c("signature"))) %>%
    separate(type_ing, into=c("challenge", "ingredient"), sep="_") %>%
    spread(ingredient, status)
glimpse(tempDesserts)
desserts <- tempDesserts


# Count rows grouping by nut variable
desserts %>%
  count(nut, sort=TRUE)

# Recode filberts as hazelnuts
desserts <- desserts %>% 
  mutate(nut = recode(nut, "filbert" = "hazelnut"))

# Count rows again 
desserts %>% 
    count(nut, sort = TRUE)

# Edit code to recode "no nut" as missing
desserts <- desserts %>% 
  mutate(nut = recode(nut, "filbert" = "hazelnut", 
                           "no nut" = NA_character_))

# Count rows again 
desserts %>% 
    count(nut, sort = TRUE)


# Edit to recode tech_win as factor
desserts <- desserts %>% 
  mutate(tech_win = recode_factor(technical, `1` = 1,
                           .default = 0))

# Count to compare values                      
desserts %>% 
  count(technical == 1, tech_win)


ratings0 <- readr::read_csv("./RInputFiles/02.03_messy_ratings.csv")
str(ratings0, give.attr=FALSE)

ratings <- ratings0 %>%
    filter(series >= 3) %>%
    rename(day=day_of_week) %>%
    mutate(series=factor(series), 
           season_premiere=lubridate::mdy(season_premiere), 
           season_finale=lubridate::mdy(season_finale), 
           viewer_growth = (e10_viewers_7day - e1_viewers_7day)
           ) %>%
    select(-contains("uk_airdate"))


# Recode channel as dummy: bbc (1) or not (0)
ratings <- ratings %>% 
  mutate(bbc = recode_factor(channel, "Channel 4"=0, .default=1))

# Look at the variables to plot next
ratings %>% select(series, channel, bbc, viewer_growth)

# Make a filled bar chart
ggplot(ratings, aes(x = series, y = viewer_growth, fill = bbc)) +
  geom_col()


# Move channel to first column
ratings %>% 
  select(channel, everything())

# Edit to drop 7- and 28-day episode viewer data
ratings %>% 
  select(-ends_with("day"))

# Edit to move channel to first and drop episode viewer data
ratings %>% 
  select(-ends_with("day")) %>%
  select(channel, everything())


# Glimpse messy names
# glimpse(messy_ratings)

# Reformat to lower camelcase
# ratings <- messy_ratings %>%
#   clean_names(case="lower_camel")
    
# Glimpse cleaned names
# glimpse(ratings)

# Reformat to snake case
# ratings <- messy_ratings %>% 
#     clean_names("snake")

# Glimpse cleaned names
# glimpse(ratings)


# Select 7-day viewer data by series
viewers_7day <- ratings %>%
  select(series, contains("7day"))

# Glimpse
glimpse(viewers_7day)

# Adapt code to also rename 7-day viewer data
viewers_7day <- ratings %>% 
    select(series, viewers_7day_ = ends_with("7day"))

# Glimpse
glimpse(viewers_7day)


# Adapt code to drop 28-day columns; move 7-day to front
viewers_7day <- ratings %>% 
    select(viewers_7day_ = ends_with("7day"), everything(), -contains("28day"))

# Glimpse
glimpse(viewers_7day)


# Adapt code to keep original order
viewers_7day <- ratings %>% 
    select(everything(), -ends_with("28day"), viewers_7day_ = ends_with("7day"))

# Glimpse
glimpse(viewers_7day)

```
  
  
  
***
  
Chapter 3 - Tidy Your Data  
  
Introduction to Tidy Data:  
  
* Tidy data helps with producing good plots - allows for faceting and the like  
* Data can be tidy but not tame, and can be tame but not tidy  
	* In general, tidy data is long rather than wide  
    * As a result, tidy data tends to take up more space, but with the advantage of being easier to plot or analyze  
* Can automatically get counts summed to a specific level  
	* juniors_tidy %>% count(baker, wt = correct)  # variable wt will be the sum of correct  
    * ggplot(juniors_tidy, aes(baker, correct)) + geom_col()  # roughly the equivalent if plotting the data  
  
Gather:  
  
* Gathering is the process of converting data from wide to long  
	* gather(data, key, value, .)  
    * key is the new column containing the variable  
    * value is the new column contining the value  
    * The . are the columns to be gathered, with column name going to the key column and associated values going to the value column  
    * The key and value need to be quoted while the . can be passed bare (unquoted)  
  
Separate:  
  
* Sometimes, a column really contains two variable, for example when there is spice_trail or the like  
* The separate function requires at least three arguments  
	* data - the data frame  
    * col - the column that you want to separate (can be a bare variable name since it already exists in the data frame)  
    * into - quoted variables to be created, inside the c() function  
    * By default, the existing column col is replaced  
    * There is also an option for convert=TRUE where it will try to pick the best variable type (especially helpful when creating numbers)  
    * There is also the option for sep, where the defaults for separators can be over-ridden to better match the data  
  
Spread:  
  
* The spread function is designed to convert long data to wide data  
	* Spread can be considered a tool to tidy messy rows, where gather is a tool to tidy messy columns  
    * data - the data frame  
    * key - the key is the column that currently contains what should become the new columns  
    * value - value is the column that currently contains what should become the values in the new columns  
    * convert=TRUE will help with re-casting variable types (particularly helpful when numbers are being pulled out of a mixed character-number column (likely what drove the need to spread)  
  
Tidy multiple sets of data:  
  
* Sometimes, there are multiple data components to tidy, where the columns need to be fixed in several ways  
	* For example, score_1, guess_1, score_2, guess_2  
    * Ideal target would be to have trials (1, 2, 3) in one column, and with columns score and guess containing the variables  
* Example code for converting multiple columns simultaneously  
	* juniors_multi %>% gather(key = "key", value = "value", score_1:guess_3) %>% separate(key, into = c("var", "order"), convert = TRUE)  
    * juniors_multi %>% gather(key = "key", value = "value", score_1:guess_3) %>% separate(key, into = c("var", "order"), convert = TRUE) %>% spread(var, value)  
  
Example code includes:  
```{r}

ratings1 <- readr::read_csv("./RInputFiles/messy_ratings.csv")
oldRatings <- ratings
ratings <- ratings1
ratings1

# Plot of episode 1 viewers by series
ratings %>%
  ggplot(aes(x=series, y=e1)) + 
  geom_bar(stat="identity")
  
# Adapt code to plot episode 2 viewers by series
ggplot(ratings, aes(x = series, y = e2)) +
    geom_col() 


# Gather and count episodes
tidy_ratings <- ratings %>%
    gather(key = "episode", value = "viewers_7day", -series, 
           factor_key = TRUE, na.rm = TRUE) %>% 
    arrange(series, episode) %>% 
    mutate(episode_count = row_number())

# Plot viewers by episode and series
ggplot(tidy_ratings, aes(x = episode_count, y = viewers_7day, fill = as.factor(series))) +
    geom_col()


ratings2 <- readr::read_csv("./RInputFiles/messy_ratings2.csv")
ratings2$series <- as.factor(ratings2$series)
ratings2

# Gather 7-day viewers by episode (ratings2 already loaded)
week_ratings <- ratings2  %>% 
    select(series, ends_with("7day")) %>% 
    gather(episode, viewers_7day, ends_with("7day"), na.rm = TRUE, factor_key = TRUE)
    
# Plot 7-day viewers by episode and series
ggplot(week_ratings, aes(x = episode, y = viewers_7day, group = series)) +
    geom_line() +
    facet_wrap(~series)


# Edit to parse episode number
week_ratings <- ratings2 %>% 
    select(series, ends_with("7day")) %>% 
    gather(episode, viewers_7day, ends_with("7day"), na.rm = TRUE) %>% 
    separate(episode, into = "episode", extra = "drop") %>% 
    mutate(episode = parse_number(episode))
    
# Edit your code to color by series and add a theme
ggplot(week_ratings, aes(x = episode, y = viewers_7day, 
                         group = series, color = series)) +
    geom_line() +
    facet_wrap(~series) +
    guides(color = FALSE) +
    theme_minimal() 


week_ratings_dec <- week_ratings %>%
    mutate(viewers_7day=as.character(viewers_7day)) %>%
    separate(viewers_7day, into=c("viewers_millions", "viewers_decimal"), sep="\\.") %>%
    mutate(viewers_decimal=ifelse(is.na(viewers_decimal), ".", paste0(".", viewers_decimal))) %>%
    dplyr::arrange(series, episode)

# Unite series and episode
ratings3 <- week_ratings_dec %>% 
    unite("viewers_7day", viewers_millions, viewers_decimal)

# Print to view
ratings3


# Adapt to change the separator
ratings3 <- week_ratings_dec  %>% 
    unite(viewers_7day, viewers_millions, viewers_decimal, sep="")

# Print to view
ratings3


# Adapt to cast viewers as a number
ratings3 <- week_ratings_dec  %>% 
    unite(viewers_7day, viewers_millions, viewers_decimal, sep="") %>%
    mutate(viewers_7day = parse_number(viewers_7day))

# Print to view
ratings3


# Create tidy data with 7- and 28-day viewers
tidy_ratings_all <- ratings2 %>%
    gather(episode, viewers, ends_with("day"), na.rm = TRUE) %>% 
    separate(episode, into = c("episode", "days")) %>%  
    mutate(episode = parse_number(episode),
           days = parse_number(days)) 

# Adapt to spread counted values
tidy_ratings_all %>% 
    count(series, days, wt = viewers) %>%
    spread(key=days, value=n, sep="_")

# Fill in blanks to get premiere/finale data
tidy_ratings <- ratings %>%
    gather(episode, viewers, -series, na.rm = TRUE) %>%
    mutate(episode = parse_number(episode)) %>% 
    group_by(series) %>% 
    filter(episode == 1 | episode == max(episode)) %>% 
    ungroup()


# Recode first/last episodes
first_last <- tidy_ratings %>% 
  mutate(episode = recode(episode, `1` = "first", .default = "last")) 

# Fill in to make slope chart
ggplot(first_last, aes(x = episode, y = viewers, color = as.factor(series))) +
  geom_point() +
  geom_line(aes(group = series))

# Switch the variables mapping x-axis and color
ggplot(first_last, aes(x = series, y = viewers, color = episode)) +
  geom_point() + # keep
  geom_line(aes(group = series)) + # keep
  coord_flip() # keep

# Calculate relative increase in viewers
bump_by_series <- first_last %>% 
  spread(episode, viewers) %>%   
  mutate(bump = (last - first) / first)
  
# Fill in to make bar chart of bumps by series
ggplot(bump_by_series, aes(x = series, y = bump)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) # converts to %

```
  
  
  
***
  
Chapter 4 - Transform Your Data  
  
Complex recoding with case_when:  
  
* The case_when function allow for vectoizing multiple if-else-then statements  
	* The LHS must give (or be) a boolean  
    * The default value for else is NA  
* Example using ages of the baker data  
	* bakers %>% mutate(gen = if_else(between(birth_year, 1981, 1996), "millenial", "not millenial"))  # simple if statement (boundaries of between are inclusive)  
    * bakers %>% mutate(gen = case_when( between(birth_year, 1965, 1980) ~ "gen_x", between(birth_year, 1981, 1996) ~ "millenial" ))  # logical ~ result  
    * bakers %>% mutate(gen = case_when( between(birth_year, 1928, 1945) ~ "silent", between(birth_year, 1946, 1964) ~ "boomer", between(birth_year, 1965, 1980) ~ "gen_x", between(birth_year, 1981, 1996) ~ "millenial", TRUE ~ "gen_z" ))  
    * bakers %>% count(gen, sort = TRUE) %>% mutate(prop = n / sum(n))  
  
Factors:  
  
* The forcats package is made specifically for working with factors - all functions start with fct_  
* Converting to factors helps ensure the proper ordering of the data  
	* ggplot(bakers, aes(x = fct_rev(fct_infreq(gen)))) + geom_bar()  # reverse by infrequency (build from small to large)  # on-the-fly conversions inside ggplot  
    * bakers <- bakers %>% mutate(gen = fct_relevel(gen, "silent", "boomer", "gen_x", "millenial", "gen_z"))  # conversions of the raw dataset  
    * bakers %>% dplyr::pull(gen) %>% levels()  # check that this worked  
    * ggplot(bakers, aes(x = gen)) + geom_bar()  # will now be plotted in the desired order  
* Need to be careful of the proper treatment of factors  
	* ggplot(bakers, aes(x = gen, fill = series_winner)) + geom_bar()  # FAIL  
    * bakers <- bakers %>% mutate(series_winner = as.factor(series_winner))  
    * ggplot(bakers, aes(x = gen, fill = series_winner)) + geom_bar()  # WORKS  
    * ggplot(bakers, aes(x = gen, fill = as.factor(series_winner))) + geom_bar()  # ALSO WORKS  
  
Dates:  
  
* Can use lubridate for convenience functions such as ymd() or dmy(), with the output being ISO (YYYY-MM-DD)  
	* Can also include a vector of suspected dates  
    * dmy("17 August 2010")  # will work  
    * hosts <- tibble::tribble( ~host, ~bday, ~premiere, "Mary", "24 March 1935", "August 17th, 2010", "Paul", "1 March 1966", "August 17th, 2010")  
    * hosts <- hosts %>% mutate(bday = dmy(bday), premiere = mdy(premiere))  
* There are three aspects of timespans  
	* interval - time span bound by two real dates  
    * duration - exact number of seconds in an interval  
    * period - change in clock time of an interval  
    * hosts <- hosts %>% mutate(age_int = interval(bday, premiere))  # new variable age_int will be of type interval  
    * hosts %>% mutate(years_decimal = age_int / years(1), years_whole = age_int %/% years(1))  # years(1) is one year, so this is fractional and whole (floored) years  
  
Strings:  
  
* The separate function splits one column in to 2+ columns (for example "age, job" could become "age" and "job")  
	* series5 <- series5 %>% separate(about, into = c("age", "occupation"), sep = ", ")  
    * series5 <- series5 %>% separate(about, into = c("age", "occupation"), sep = ", ") %>% mutate(age = parse_number(age))  # numeric age. Dropping years  
* The stringr package makes working with strings in R easier (typically used within a mutate) - all functions start with str_  
	* series5 <- series5 %>% mutate(baker = str_to_upper(baker), showstopper = str_to_lower(showstopper))  
    * series5 %>% mutate(pie = str_detect(showstopper, "pie"))  # returns a boolean  
    * series5 %>% mutate(showstopper = str_replace(showstopper, "pie", "tart"))  # find and replace for strings  
    * series5 %>% mutate(showstopper = str_remove(showstopper, "pie"))  # remove "pie", though there may be trailing whitespace  
    * series5 %>% mutate(showstopper = str_remove(showstopper, "pie"), showstopper = str_trim(showstopper))  # trim whitespace at the beginning or end  
  
Final thoughts:  
  
* R using the tidyverse for analysis and presentation  
* Reading data using readr and analyzing using dplyr and ggplot2  
* Taming variable types, names, and values  
* Transforming data using stringr and lubridate  
* The "here" package can make working with file paths much easier  
  
Example code includes:  
```{r}

baker_results <- readr::read_csv("./RInputFiles/baker_results.csv")
messy_baker_results <- readr::read_csv("./RInputFiles/messy_baker_results.csv")
bakers <- baker_results
glimpse(bakers)


# Create skill variable with 3 levels
bakers <- bakers %>% 
  mutate(skill = case_when(
    star_baker > technical_winner ~ "super_star",
    star_baker < technical_winner ~ "high_tech",
    TRUE ~ "well_rounded"
  ))
  
# Filter zeroes to examine skill variable
bakers %>% 
  filter(star_baker==0 & technical_winner==0) %>% 
  count(skill)


# Add pipe to drop skill = NA
bakers_skill <- bakers %>% 
  mutate(skill = case_when(
    star_baker > technical_winner ~ "super_star",
    star_baker < technical_winner ~ "high_tech",
    star_baker == 0 & technical_winner == 0 ~ NA_character_,
    star_baker == technical_winner  ~ "well_rounded"
  )) %>% 
  drop_na(skill)
  
# Count bakers by skill
bakers_skill %>%
  count(skill)


# Cast skill as a factor
bakers <- bakers %>% 
  mutate(skill = as.factor(skill))

# Examine levels
bakers %>%
  pull(skill) %>%
  levels()


baker_dates <- bakers %>%
    select(series, baker, contains("date")) %>%
    mutate(last_date_appeared_us=as.character(last_date_us), 
           first_date_appeared_us=as.character(first_date_us)
           ) %>%
    rename(first_date_appeared_uk=first_date_appeared, last_date_appeared_uk=last_date_appeared) %>%
    select(-last_date_us, -first_date_us)
glimpse(baker_dates)


# Add a line to extract labeled month
baker_dates <- baker_dates %>% 
  mutate(last_date_appeared_us=lubridate::ymd(last_date_appeared_us), 
         last_month_us=lubridate::month(last_date_appeared_us, label=TRUE)
         )
         
ggplot(baker_dates, aes(x=last_month_us)) + geom_bar()


baker_time <- baker_dates %>%
    mutate(first_date_appeared_us=lubridate::ymd(first_date_appeared_us)) %>%
    select(-last_month_us)
glimpse(baker_time)

           
# Add a line to create whole months on air variable
baker_time <- baker_time  %>% 
  mutate(time_on_air = lubridate::interval(first_date_appeared_uk, last_date_appeared_uk),
         weeks_on_air = time_on_air / lubridate::weeks(1), 
         months_on_air = time_on_air %/% months(1)
         )

# Count rows
messy_baker_results %>% 
  count(position_reached)


# Add another mutate to replace "THIRD PLACE" with "RUNNER UP"and count
messy_baker_results <- messy_baker_results %>% 
  mutate(position_reached = str_to_upper(position_reached),
         position_reached = str_replace(position_reached, "-", " "), 
         position_reached = str_replace(position_reached, "THIRD PLACE", "RUNNER UP"))

# Count rows
messy_baker_results %>% 
  count(position_reached)


# Add a line to create new variable called student
bakers <- bakers %>% 
    mutate(occupation = str_to_lower(occupation), 
           student=str_detect(occupation, "student")
           )

# Find all students and examine occupations
bakers %>% 
  filter(student) %>%
  select(baker, occupation, student)

```
  
  
  
***
  
###_Modeling Data in the Tidyverse_  
  
Chapter 1 - Introduction to Modeling  
  
Background on modeling for explanation:  
  
* Generally, the model has y as a function of x plus epsilon, where y is the outcome of interest and x is a set of explanatory variables and epsilon is irreducible error  
	* The x can be either explanatory or predictive - depends on the purpose of the analysis  
* Example of explanation - can differences in teacher evaluation scores be explained by teacher attributes  
	* library(dplyr)  
    * library(moderndive)  
    * glimpse(evals)  # evals data is available in the moderndivw package (From the moderndive package for ModernDive.com:)  
    * ggplot(evals, aes(x = score)) + geom_histogram(binwidth = 0.25) + labs(x = "teaching score", y = "count")  # EDA on scores using histogram  
    * evals %>% summarize(mean_score = mean(score), median_score = median(score), sd_score = sd(score))  # summary statistics using dplyr::summarize  
  
Background on modeling for prediction:  
  
* House sales in King County USA in 2014-2015 (from Kaggle) based on features such as size, bedrooms, etc.  
	* glimpse(house_prices)  
    * ggplot(house_prices, aes(x = price)) + geom_histogram() + labs(x = "house price", y = "count")  
    * house_prices <- house_prices %>% mutate(log10_price = log10(price))  
    * house_prices %>% select(price, log10_price)  
    * ggplot(house_prices, aes(x = log10_price)) + geom_histogram() + labs(x = "log10 house price", y = "count")  # after transformation  
  
Modeling problem for explanation:  
  
* Typically, both the function that relates x and y and the function that generates the errors is unknwon  
	* Goal is to create a model that can generate y-hat by separating signal from noise  
* Can start by considering linear models, assessed as a starting point by examining a scatter plot  
	* ggplot(evals, aes(x = age, y = score)) + geom_point() + labs(x = "age", y = "score", title = "Teaching score over age")  
    * ggplot(evals, aes(x = age, y = score)) + geom_jitter() + labs(x = "age", y = "score", title = "Teaching score over age (jittered)")  
* Can further explore the data by looking at correlations among some or all of the potential explanatory variables  
	* evals %>% summarize(correlation = cor(score, age))  
  
Modeling problem for prediction:  
  
* For explanation, we care about the form of the function  
* For prediction, we care mainly that the function makes good predictions (even if it may not be easy to explain)  
	* house_prices %>% select(log10_price, condition) %>% glimpse()  # condition is a categorical variable saved as a factor  
    * ggplot(house_prices, aes(x = condition, y = log10_price)) + geom_boxplot() + labs(x = "house condition", y = "log10 price", title = "log10 house price over condition")  
* Means tend to be at the center of the linear modeling process  
	* house_prices %>% group_by(condition) %>% summarize(mean = mean(log10_price), sd = sd(log10_price), n = n())  
  
Example code includes:  
```{r}

data(evals, package="moderndive")
glimpse(evals)


# Plot the histogram
ggplot(evals, aes(x = age)) +
  geom_histogram(binwidth = 5) +
  labs(x = "age", y = "count")

# Compute summary stats
evals %>%
  summarize(mean_age = mean(age),
            median_age = median(age),
            sd_age = sd(age))


data(house_prices, package="moderndive")
glimpse(house_prices)


# Plot the histogram
ggplot(house_prices, aes(x = sqft_living)) +
  geom_histogram() +
  labs(x="Size (sq.feet)", y="count")

# Add log10_sqft_living
house_prices_2 <- house_prices %>%
  mutate(log10_sqft_living = log10(sqft_living))

# Plot the histogram  
ggplot(house_prices_2, aes(x = log10_sqft_living)) +
  geom_histogram() +
  labs(x = "log10 size", y = "count")


# Plot the histogram
ggplot(evals, aes(x=bty_avg)) +
  geom_histogram(binwidth=0.5) +
  labs(x = "Beauty score", y = "count")

# Scatterplot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "teaching score")

# Jitter plot
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_jitter() +
  labs(x = "beauty score", y = "teaching score")


# Compute correlation
evals %>%
  summarize(correlation = cor(score, bty_avg))


house_prices <- house_prices %>%
    mutate(log10_price=log10(price))

# View the structure of log10_price and waterfront
house_prices %>%
  select(log10_price, waterfront) %>%
  glimpse()

# Plot 
ggplot(house_prices, aes(x = waterfront, y = log10_price)) +
  geom_boxplot() +
  labs(x = "waterfront", y = "log10 price")


# Calculate stats
house_prices %>%
  group_by(waterfront) %>%
  summarize(mean_log10_price = mean(log10_price), n = n())
  
# Prediction of price for houses with view
10^(6.12)

# Prediction of price for houses without view
10^(5.66)

```
  
  
  
***
  
Chapter 2 - Modeling with Regression  
  
Explaining teaching score with age:  
  
* Can overlay a regression line to the scatter plot for a bivariate relationship  
	* ggplot(evals, aes(x = age, y = score)) + geom_point() + labs(x = "age", y = "score", title = "Teaching score over age") + geom_smooth(method = "lm", se = FALSE)  
* In simple linear regression, the assumption is that f(x) is B0 + B1*x  
    * The fitted model f-hat does not have an error term, since it is just the model prediction for a given value of x  
    * model_score_1 <- lm(score ~ age, data = evals)  
    * moderndive::get_regression_table(model_score_1)  
  
Predicting teaching score using age:  
  
* Can make predictions based on the existing regression line - f-hat can be used for both explanatory and predictive purposes  
* The residuals are the errors (predictive vs. actual values), and correspond to the epsilon of the general modeling framework  
    * On average, for linear regression, the residuals should average out to zero  
    * get_regression_points(model_score_1)  # gives y, x, y-hat, and residuals  
  
Explaining teaching score with gender:  
  
* Can extend the models to include categorical data, such as gender  
	* ggplot(evals, aes(x = score)) + geom_histogram(binwidth = 0.25) + facet_wrap(~gender) + labs(x = "score", y = "count")  
    * model_score_3 <- lm(score ~ gender, data = evals)  # will just give an overall mean and a change in mean vs. the first-level factor  
* Can also look at multi-level factors, such as rank (teacher type)  
	* evals %>% group_by(rank) %>% summarize(n = n())  
  
Predicting teaching score with gender:  
  
* Can use group means as part of the predictive approach - if only factor are used in the regression, there will be the same prediction for everyone who is in the same class(es)  
	* model_score_3_points <- get_regression_points(model_score_3)  
  
Example code includes:  
```{r}

# Plot 
ggplot(evals, aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "beauty score", y = "score") +
  geom_smooth(method = "lm", se = FALSE)

# Fit model
model_score_2 <- lm(score ~ bty_avg, data = evals)

# Output content
model_score_2

# Output regression table
moderndive::get_regression_table(model_score_2)

# Use fitted intercept and slope to get a prediction
y_hat <- 3.88 + 0.067 * 5
y_hat

# Compute residual y - y_hat
4.7 - y_hat


# Get regression table
moderndive::get_regression_table(model_score_2, digits = 5)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2) %>% 
  mutate(score_hat_2 = 3.88 + 0.0666 * bty_avg)

# Get all fitted/predicted values and residuals
moderndive::get_regression_points(model_score_2) %>% 
  mutate(residual_2 = score - score_hat)


ggplot(evals, aes(x=rank, y=score)) +
  geom_boxplot() +
  labs(x = "rank", y = "score")

evals %>%
  group_by(rank) %>%
  summarize(n = n(), mean_score = mean(score), sd_score = sd(score))


# Fit regression model
model_score_4 <- lm(score ~ rank, data = evals)

# Get regression table
moderndive::get_regression_table(model_score_4, digits = 5)

# teaching mean
teaching_mean <- 4.28

# tenure track mean
tenure_track_mean <- 4.28-0.13 

# tenure mean
tenure_mean <- 4.28-0.145


# Calculate predictions and residuals
model_score_4_points <- moderndive::get_regression_points(model_score_4)
model_score_4_points

# Plot residuals
ggplot(model_score_4_points, aes(x=residual)) +
  geom_histogram() +
  labs(x = "residuals", title = "Residuals from score ~ rank model")

```
  
  
  
***
  
Chapter 3 - Modeling with Multiple Regression  
  
Explaining house price with year and size:  
  
* Can incorporate 2+ explanatory / predictive variable using multiple regression  
	* house_prices %>% select(price, sqft_living, condition, waterfront) %>% glimpse()  
    * The log-10 transformation is helpful for this specific dataset (assume the code below for future examples in this course)  
    * house_prices <- house_prices %>% mutate( log10_price = log10(price), log10_sqft_living = log10(sqft_living) )  
* Exploring the relationship between mutliple variables - EDA and regression  
	* Can create a 3D plot with associated regression plane using plotly  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * get_regression_table(model_price_1, digits = 5))  
  
Predicting house price using year and size:  
  
* Can get the fitted values and exponentiate as needed, assessing the overall fit or lack thereof (sum-squared residuals) of the model  
	* get_regression_points(model_price_1, digits = 5)  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(squared_residuals))  # SSR  
  
Explaining house price with size and condition:  
  
* The EDA from previous chapters is repeated  
	* house_prices <- house_prices %>% mutate( log10_price = log10(price), log10_sqft_living = log10(sqft_living) )  
    * house_prices %>% group_by(condition) %>% summarize(mean = mean(log10_price), sd = sd(log10_price), n = n())  
* The parallel slopes model is lines where the slopes are the same but they have a different intercept (likely, coefficients of a categorical variable)  
	* model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_table(model_price_3, digits = 5)  
  
Predicting house price using size and condition:  
  
* Objective is to predict on new data (as opposed to checking our predictions on data where we already had the answer)  
	* model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_table(model_price_3)  
* Automating the housing price prediction process  
	* new_houses <- read_csv("new_houses.csv")  
    * new_houses  
    * get_regression_points(model_price_3, newdata = new_houses)  # moderndata form of predict() function  
    * get_regression_points(model_price_3, newdata = new_houses) %>% mutate(price_hat = 10^log10_price_hat)  
  
Example code includes:  
```{r}

# Create scatterplot with regression line
ggplot(house_prices, aes(x=bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)

# Remove outlier
house_prices_transform <- house_prices %>%
    filter(bedrooms < 33) %>%
    mutate(log10_sqft_living=log10(sqft_living))

# Create scatterplot with regression line
ggplot(house_prices_transform, aes(x = bedrooms, y = log10_price)) +
  geom_point() +
  labs(x = "Number of bedrooms", y = "log10 price") +
  geom_smooth(method = "lm", se = FALSE)


# Fit model
model_price_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data = house_prices_transform)

# Get regression table
moderndive::get_regression_table(model_price_2)

# Make prediction in log10 dollars
2.69 + 0.941 * log10(1000) - 0.033 * 3

# Make prediction dollars
10**(2.69 + 0.941 * log10(1000) - 0.033 * 3)

# Automate prediction and residual computation
moderndive::get_regression_points(model_price_2) %>%
    mutate(squared_residuals = residual**2) %>%
    summarize(sum_squared_residuals = sum(squared_residuals))


# Fit model
model_price_4 <- lm(log10_price ~ log10_sqft_living + waterfront, data = house_prices_transform)

# Get regression table
moderndive::get_regression_table(model_price_4)

# Prediction for House A
10**(2.96 + 0.825*2.9 + 0.322)

# Prediction for House B
10**(2.96 + 0.825*3.1 + 0)


# View the "new" houses
new_houses_2 <- tibble(log10_sqft_living=c(2.9, 3.1), waterfront=c(TRUE, FALSE))
new_houses_2

# Get predictions price_hat in dollars on "new" houses
moderndive::get_regression_points(model_price_4, newdata = new_houses_2) %>% 
  mutate(price_hat = 10**log10_price_hat)

```
  
  
  
***
  
Chapter 4 - Model Selection and Assessment  
  
Model selection and assessment:  
  
* Can use multiple models for the same data and compare  
	* model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
    * get_regression_points(model_price_3) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
  
Assessing model fit with R-squared:  
  
* The R-squared is a reasonable measure of model fit - R-squared = 1 - Var(Residuals) / Var(Y)  
	* Larger R-squared is suggestive of better fit, with values (typically) constrained between 0 and 1  
    * R-squared is the proportion of variation in the outcome model that can be explained using the model  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)  
    * get_regression_points(model_price_1) %>% summarize(r_squared = 1 - var(residual) / var(log10_price))  
    * model_price_3 <- lm(log10_price ~ log10_sqft_living + condition, data = house_prices)  
    * get_regression_points(model_price_3) %>% summarize(r_squared = 1 - var(residual) / var(log10_price))  
  
Assessing predictions with RMSE:  
  
* RMSE (Root Mean Squared Error) is a slight variation on RSS  
	* Where RSS is the sum-squared of the residuals, RMSE is the square root of the average of the residuals-squared  
    * model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = house_prices)   
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(sum_sq_residuals = sum(sq_residuals))  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals))  
    * get_regression_points(model_price_1) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals)) %>% mutate(rmse = sqrt(mse))  
* Cannot calculate RMSE on new data - predictions means that we do not know the actual values  
	* get_regression_points(model_price_3, newdata = new_houses) %>% mutate(sq_residuals = residual^2) %>% summarize(mse = mean(sq_residuals)) %>% mutate(rmse = sqrt(mse))  
    * The above code will crash out, since the residuals do not exist  
  
Validation set prediction framework:  
  
* Use two different datasets for modeling; a training set used for modeling, and a test set used for assessing likely out-of-sample errors  
	* house_prices_shuffled <- house_prices %>% sample_frac(size = 1, replace = FALSE)  # Randomly shuffle order of rows  
    * train <- house_prices_shuffled %>% slice(1:10000)  
    * test <- house_prices_shuffled %>% slice(10001:21613)  
    * train_model_price_1 <- lm(log10_price ~ log10_sqft_living + yr_built, data = train)  
* After having trained the model on the train data, can assess the fit using the test data  
	* get_regression_points(train_model_price_1, newdata = test)  
    * get_regression_points(train_model_price_1, newdata = test) %>% mutate(sq_residuals = residual^2) %>% summarize(rmse = sqrt(mean(sq_residuals)))  
  
Next steps:  
  
* Tidyverse ties together many of the packages that help with data wrangling and analysis  
* Can extend regressions to areas like polynomials and trees  
* "ModernDive" is a textbook on the tidyverse tools  
  
Example code includes:  
```{r}

# Model 2
model_price_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data = house_prices_transform)

# Calculate squared residuals
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals=residual**2) %>% 
    summarize(sum_sq_residuals=sum(sq_residuals))

# Model 4
model_price_4 <- lm(log10_price ~ log10_sqft_living + waterfront, data = house_prices_transform)

# Calculate squared residuals
moderndive::get_regression_points(model_price_4) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(sum_sq_residuals=sum(sq_residuals))


# Get fitted/values & residuals, compute R^2 using residuals
moderndive::get_regression_points(model_price_2) %>% 
    summarize(r_squared = 1 - var(residual) / var(log10_price))

# Get fitted/values & residuals, compute R^2 using residuals
moderndive::get_regression_points(model_price_4) %>% 
    summarize(r_squared = 1 - var(residual) / var(log10_price))


# Get all residuals, square them, take the mean and square root
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals)) %>% 
    mutate(rmse = sqrt(mse))

# MSE and RMSE for model_price_2
moderndive::get_regression_points(model_price_2) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))

# MSE and RMSE for model_price_4
moderndive::get_regression_points(model_price_4) %>% 
    mutate(sq_residuals = residual^2) %>% 
    summarize(mse = mean(sq_residuals), rmse = sqrt(mean(sq_residuals)))


# Set random number generator seed value for reproducibility
set.seed(76)

# Randomly reorder the rows
house_prices_shuffled <- house_prices_transform %>% 
    sample_frac(size = 1, replace = FALSE)

# Train/test split
train <- house_prices_shuffled %>% 
    slice(1:10000)
test <- house_prices_shuffled %>% 
    slice(10001:nrow(.))

# Fit model to training set
train_model_2 <- lm(log10_price ~ log10_sqft_living + bedrooms, data=train)


# Compute RMSE (train)
moderndive::get_regression_points(train_model_2) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(rmse = sqrt(mean(sq_residuals)))

# Compute RMSE (test)
moderndive::get_regression_points(train_model_2, newdata = test) %>% 
    mutate(sq_residuals = residual**2) %>% 
    summarize(rmse = sqrt(mean(sq_residuals)))

```
  
  
  
***
  
###_Analyzing Survey Data in R_  
  
Chapter 1 - Introduction to Survey Data  

What are survey weights?  
  
* Survey weights sometimes appear inside a dataset, to reflect potential over/under sampling  
	* Survey weights result from a complex survey design - number of points in the population represented by each entry in the sampling frame  
    * For example, average income would be the sum-product of weights and incomes divided by the sum of weights  
  
Specifying elements of the design in R:  
  
* Simple random sampling is when every member of the population is known and had an equal chance of being selected  
	* library(survey)  
    * srs_design <- svydesign(data = paSample, weights = ~wts, fpc=~N, id=~1)  # the ~ means that these are column names  
* Stratified sampling is when a simple random sample is taken from each of the strata (sub-units)  
	* For example, taking 100 people from every county in a state, so that county-level averages can be gathered  
    * stratified_design <- svydesign(data = paSample, id = ~1, weights = ~wts, strata = ~county, fpc = ~N)  
* Cluster sampling is when the population are grouped in to clusters, with a simple random sample of clusters selected, and with simple random samples taken within each selected cluster  
	* cluster_design <- svydesign(data = paSample, id = ~county + personid, fpc = ~N1 + N2, weights = ~wts) 
  
Visualizing impact of survey weights:  
  
* NHANES data - assessment of health of persons in the US, derived by a health check in a mobile doctor's office  
	* Stage 0 - stratified by geography and proportion minority  
    * Stage 1 - within strata, counties randomly selected (selection likelihood proportional to population)  
    * Stage 2 - within counties, city blocks randomly selected (selection likelihood proportional to population)  
    * Stage 3 - within city blocks, households randomly selected (based on demographics)  
    * Stage 4 - within households, people randomly selected  
* NHANES data are availabl through a package in R  
	* library(NHANES)  
    * dim(NHANESraw)  
    * summarize(NHANESraw, N_hat = sum(WTMEC2YR))  # sums to double the US population, due to having 4 years of data when desiring only 2 years of data  
    * NHANESraw <- mutate(NHANESraw, WTMEC4YR = WTMEC2YR/2)  # fix the double population issue  
    * NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR)  # id is the cluster (first-level), nest=TRUE is due to id being nested within strata (???)  
    * distinct(NHANESraw, SDMVPSU)  # only takes 3 values, since only 1-3 counties are selected  
  
Example code includes:  
```{r}

colTypes <- "FINLWT21 numeric _ FINCBTAX integer _ BLS_URBN integer _ POPSIZE integer _ EDUC_REF character _ EDUCA2 character _ AGE_REF integer _ AGE2 character _ SEX_REF integer _ SEX2 integer _ REF_RACE integer _ RACE2 integer _ HISP_REF integer _ HISP2 integer _ FAM_TYPE integer _ MARITAL1 integer _ REGION integer _ SMSASTAT integer _ HIGH_EDU character _ EHOUSNGC numeric _ TOTEXPCQ numeric _ FOODCQ numeric _ TRANSCQ numeric _ HEALTHCQ numeric _ ENTERTCQ numeric _ EDUCACQ integer _ TOBACCCQ numeric _ STUDFINX character _ IRAX character _ CUTENURE integer _ FAM_SIZE integer _ VEHQ integer _ ROOMSQ character _ INC_HRS1 character _ INC_HRS2 character _ EARNCOMP integer _ NO_EARNR integer _ OCCUCOD1 character _ OCCUCOD2 character _ STATE character _ DIVISION integer _ TOTXEST integer _ CREDFINX character _ CREDITB integer _ CREDITX character _ BUILDING character _ ST_HOUS integer _ INT_PHON character _ INT_HOME character _ "

ce <- readr::read_csv("./RInputFiles/ce.csv")
glimpse(ce)
ceColTypes <- ""
for (x in names(ce)) { ceColTypes <- paste0(ceColTypes, x, " ", class(ce[, x, drop=TRUE]), " _ ") }
all.equal(colTypes, ceColTypes)

# Construct a histogram of the weights
ggplot(data = ce, mapping = aes(x = FINLWT21)) +
    geom_histogram()

# In the next few exercises we will practice specifying sampling designs using different samples from the api dataset, located in the survey package
# The api dataset contains the Academic Performance Index and demographic information for schools in California
# The apisrs dataset is a simple random sample of schools from the api dataset
# Notice that pw contains the survey weights and fpc contains the total number of schools in the population

data(api, package="survey")
library(survey)

# Look at the apisrs dataset
glimpse(apisrs)

# Specify a simple random sampling for apisrs
apisrs_design <- svydesign(data = apisrs, weights = ~pw, fpc = ~fpc, id = ~1)

# Print a summary of the design
summary(apisrs_design)


# Now let's practice specifying a stratified sampling design, using the dataset apistrat
# The schools are stratified based on the school type stype where E = Elementary, M = Middle, and H = High School
# For each school type, a simple random sample of schools was taken

# Glimpse the data
glimpse(apistrat)

# Summarize strata sample sizes
apistrat %>%
  count(stype)

# Specify the design
strat_design <- svydesign(data = apistrat, weights = ~pw, fpc = ~fpc, id = ~1, strata = ~stype)

# Look at the summary information for the stratified design
summary(strat_design)


# Now let's practice specifying a cluster sampling design, using the dataset apiclus2
# The schools were clustered based on school districts, dnum
# Within a sampled school district, 5 schools were randomly selected for the sample
# The schools are denoted by snum
# The number of districts is given by fpc1 and the number of schools in the sampled districts is given by fpc2

# Glimpse the data
glimpse(apiclus2)

# Specify the design
apiclus_design <- svydesign(id = ~dnum + snum, data = apiclus2, weights = ~pw, fpc = ~fpc1 + fpc2)

#Look at the summary information stored for both designs
summary(apiclus_design)


# Construct histogram of pw
ggplot(data = apisrs, mapping = aes(x = pw)) + 
    geom_histogram()

# Construct histogram of pw
ggplot(data = apistrat, mapping = aes(x = pw)) + 
    geom_histogram()

# Construct histogram of pw
ggplot(data = apiclus2, mapping = aes(x = pw)) + 
    geom_histogram()



NHANESraw <- read.csv("./RInputFiles/NHANESraw.txt")
NHANESraw <- NHANESraw %>%
    mutate(WTMEC4YR=WTMEC2YR / 2)
names(NHANESraw)[1] <- "SurveyYr"
glimpse(NHANESraw)

#Create table of average survey weights by race
tab_weights <- NHANESraw %>%
  group_by(Race1) %>%
  summarize(avg_wt = mean(WTMEC4YR))

#Print the table
tab_weights


# The two important design variables in NHANESraw are SDMVSTRA, which contains the strata assignment for each unit, and SDMVPSU, which contains the cluster id within a given stratum
# Specify the NHANES design
NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, 
                           nest = TRUE, weights = ~WTMEC4YR
                           )

# Print summary of design
summary(NHANES_design)

# Number of clusters
NHANESraw %>%
  summarize(n_clusters = n_distinct(SDMVSTRA, SDMVPSU))

# Sample sizes in clusters
NHANESraw %>%
  count(SDMVSTRA, SDMVPSU) 

```
  
  
  
***
  
Chapter 2 - Exploring categorical data  
  
Visualizing categorical variables:  
  
* Can estimate distributions of race, including both the weighted and unweighted distributions  
	* tab_unw <- NHANESraw %>% group_by(Race1) %>% summarize(Freq = n()) %>% mutate(Prop = Freq/sum(Freq)) %>% arrange(desc(Prop))  
    * ggplot(data = tab_unw, mapping = aes(x = Race1, y = Prop)) + geom_col() + coord_flip() + scale_x_discrete(limits = tab_unw$Race1) # Labels layer omitted  
* Can convert back to the weighted frequencies  
	* tab_w <- svytable(~Race1, design = NHANES_design) %>% as.data.frame() %>% mutate(Prop = Freq/sum(Freq)) %>% arrange(desc(Prop))  
    * ggplot(data = tab_w, mapping = aes(x = Race1, y = Prop)) + geom_col() + coord_flip() + scale_x_discrete(limits = tab_w$Race1) # Labels layer omitted  
  
Exploring two categorical variables:  
  
* Can look at diabetes withing the NHANES data, using the syvtable() function  
	* svytable(~Diabetes, design = NHANES_design)  
    * tab_w <- svytable(~Race1 + Diabetes, design = NHANES_design)  # Race and Diabetes  
    * tab_w <- as.data.frame(tab_w)  # converts contingency table to frame  
    * ggplot(data = tab_w, mapping = aes(x = Race1, fill = Diabetes, y = Freq)) + geom_col() + coord_flip()  
    * ggplot(data = tab_w, mapping = aes(x = Race1, y = Freq, fill = Diabetes)) + geom_col(position = "fill") + coord_flip()  # stacked bars to 100%  
  
Inference for categorical variables:  
  
* Formal statistical tests for associations among categorical variables using chi-squared tests for association  
	* svychisq(~Race1 + Diabetes, design = NHANES_design, statistic = "Chisq")  
  
Example code includes:  
```{r}

# Specify the survey design
NHANESraw <- mutate(NHANESraw, WTMEC4YR = .5 * WTMEC2YR)
NHANES_design <- svydesign(data = NHANESraw, strata = ~SDMVSTRA, id = ~SDMVPSU, nest = TRUE, weights = ~WTMEC4YR)

# Determine the levels of Depressed
levels(NHANESraw$Depressed)

# Construct a frequency table of Depressed
tab_w <- svytable(~Depressed, design = NHANES_design)

# Determine class of tab_w
class(tab_w)

# Display tab_w
tab_w


# Add proportions to table
tab_w <- tab_w %>%
  as.data.frame() %>%
  mutate(Prop = Freq/sum(Freq))

# Create a barplot
ggplot(data = tab_w, mapping = aes(x = Depressed, y = Prop)) + 
  geom_col()


# Construct and print a frequency table
tab_D <- svytable(~Depressed, design = NHANES_design)
tab_D

# Construct and print a frequency table
tab_H <- svytable(~HealthGen, design = NHANES_design)
tab_H

# Construct and print a frequency table
tab_DH <- svytable(~Depressed + HealthGen, design = NHANES_design)
tab_DH


# Add conditional proportions to tab_DH
tab_DH_cond <- tab_DH %>%
    as.data.frame() %>%
    group_by(HealthGen) %>%
    mutate(n_HealthGen = sum(Freq), Prop_Depressed = Freq/n_HealthGen) %>%
    ungroup()

# Print tab_DH_cond
tab_DH_cond

# Create a segmented bar graph of the conditional proportions in tab_DH_cond
ggplot(data = tab_DH_cond, mapping = aes(x = HealthGen, y = Prop_Depressed, fill = Depressed)) + 
  geom_col() + 
  coord_flip() 


# We can also estimate counts with svytotal(). The syntax is given by:
# svytotal(x = ~interaction(Var1, Var2), design = design, na.rm = TRUE)
# For each combination of the two variables, we get an estimate of the total and the standard error


# Estimate the totals for combos of Depressed and HealthGen
tab_totals <- svytotal(x = ~interaction(Depressed, HealthGen), design = NHANES_design, na.rm = TRUE)

# Print table of totals
tab_totals

# Estimate the means for combos of Depressed and HealthGen
tab_means <- svymean(x = ~interaction(Depressed, HealthGen), design = NHANES_design, na.rm = TRUE)

# Print table of means
tab_means


# Run a chi square test between Depressed and HealthGen
svychisq(~Depressed + HealthGen, design = NHANES_design, statistic = "Chisq")

# Construct a contingency table
tab <- svytable(~Education + HomeOwn, design=NHANES_design)

# Add conditional proportion of levels of HomeOwn for each educational level
tab_df <- as.data.frame(tab) %>%
  group_by(Education) %>%
  mutate(n_Education = sum(Freq), Prop_HomeOwn = Freq/n_Education) %>%
  ungroup()

# Create a segmented bar graph
ggplot(data = tab_df, mapping = aes(x=Education, y=Prop_HomeOwn, fill=HomeOwn)) + 
  geom_col() + 
  coord_flip()

# Run a chi square test
svychisq(~Education + HomeOwn, 
    design = NHANES_design, 
    statistic = "Chisq")

```
  
  
  
***
  
Chapter 3 - Exploring quantitative data  
  
Summarizing quantitative data:  
  
* Can look at the physician health bad variable and summarize  
	* NHANESraw %>% filter(Age >= 12) %>% select(DaysPhysHlthBad)  # just the data  
    * svymean(x = ~DaysPhysHlthBad, design = NHANES_design, na.rm = TRUE)  # means of number of days feeling in bad heatlh  
    * svyquantile(x = ~DaysPhysHlthBad, design = NHANES_design, na.rm = TRUE, quantiles = 0.5)  # get the median (quantile 0.5) of the data  
* Can grab summaries by group using svyby with a function FUN provided  
	* svyby(formula = ~DaysPhysHlthBad, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, row.names = FALSE)  
    * svyby(formula = ~Age, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE)  
  
Visualizing quantitative data:  
  
* Can create bar graphs of the means  
	* out <- svyby(formula = ~DaysPhysHlthBad, by = ~SmokeNow, design = NHANES_design, FUN = svymean, na.rm = TRUE, keep.names = FALSE)  
    * ggplot(data = out, mapping = aes(x = SmokeNow, y = DaysPhysHlthBad)) + geom_col() + labs(y = "Monthly Average Number\n of Bad Health Days", x = "Smoker?")  
    * out <- mutate(out, lower = DaysPhysHlthBad - se, upper = DaysPhysHlthBad + se)  
* Create histograms of the data  
	* ggplot(data = out, mapping = aes(x = SmokeNow, y = DaysPhysHlthBad, ymin = lower, ymax = upper)) + geom_col(fill = "lightblue") + geom_errorbar(width = .5) + labs(y = "Monthly Average Number\n of Bad Health Days", x = "Smoker?")  
    * ggplot(data = NHANESraw, mapping = aes(x = DaysPhysHlthBad, weight = WTMEC4YR)) + geom_histogram(binwidth = 1, color = "white") + labs(x = "Number of Bad Health Days in a Month")  
* Create density plots of the data  
	* NHANESraw %>% filter(!is.na(DaysPhysHlthBad)) %>% mutate(WTMEC4YR_std = WTMEC4YR/sum(WTMEC4YR)) %>%  
    *     ggplot(mapping = aes(x = DaysPhysHlthBad, weight = WTMEC4YR_std)) + geom_density(bw = .6, fill = "lightblue") + labs(x = "Number of Bad Health Days in a Month")  
  
Inference for quantitative data:  
  
* May want to compare means across two groups in the data using a weighted 2-sample t-test  
	* The test statistic is the difference in means divided by the SE (standard error)  
    * svyttest(formula = DaysPhysHlthBad ~ SmokeNow, design = NHANES_design)  
  
Example code includes:  
```{r}

# Compute the survey-weighted mean
svymean(x = ~SleepHrsNight, design = NHANES_design, na.rm = TRUE)

# Compute the survey-weighted mean by Gender
svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, 
      FUN = svymean, na.rm = TRUE, keep.names = FALSE
      )

# Compute the survey-weighted quantiles
svyquantile(x = ~SleepHrsNight, design = NHANES_design, na.rm = TRUE, 
            quantiles = c(0.01, 0.25, 0.5, 0.75, .99)
            )

# Compute the survey-weighted quantiles by Gender
svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, FUN = svyquantile, 
      na.rm = TRUE, quantiles = c(0.5), keep.rows = FALSE, keep.var = FALSE
      )

# Compute the survey-weighted mean by Gender
out <- svyby(formula = ~SleepHrsNight, by = ~Gender, design = NHANES_design, 
             FUN = svymean, na.rm = TRUE, keep.names = FALSE
             )
             
# Construct a bar plot of average sleep by gender
ggplot(data = out, mapping = aes(x=as.factor(Gender), y=SleepHrsNight)) + 
    geom_col() + 
    labs(y="Average Nightly Sleep")

# Add lower and upper columns to out
out_col <- mutate(out, lower = SleepHrsNight - 2*se, upper = SleepHrsNight + 2*se)

# Construct a bar plot of average sleep by gender with error bars
ggplot(data = out_col, mapping = aes(x = Gender, y = SleepHrsNight, ymin = lower, ymax = upper)) + 
    geom_col(fill = "gold") + 
    labs(y = "Average Nightly Sleep") + 
    geom_errorbar(width = 0.7)  


# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 1, color = "white") + 
    labs(x = "Hours of Sleep")

# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 0.5, color = "white") + 
    labs(x = "Hours of Sleep")

# Create a histogram with a set binwidth
ggplot(data = NHANESraw, mapping = aes(x=SleepHrsNight, weight=WTMEC4YR)) + 
    geom_histogram(binwidth = 2, color = "white") + 
    labs(x = "Hours of Sleep")


# Density plot of sleep faceted by gender
NHANESraw %>% 
    filter(!is.na(SleepHrsNight), !is.na(Gender)) %>%
    group_by(Gender) %>%
    mutate(WTMEC4YR_std = WTMEC4YR/sum(WTMEC4YR)) %>%
    ggplot(mapping = aes(x = SleepHrsNight, weight = WTMEC4YR_std)) + 
        geom_density(bw = 0.6,  fill = "gold") +
        labs(x = "Hours of Sleep") + 
        facet_wrap(~Gender, labeller = "label_both")


# Run a survey-weighted t-test
svyttest(formula = SleepHrsNight ~ Gender, design = NHANES_design)

# Find means of total cholesterol by whether or not active 
out <- svyby(formula = ~TotChol, by = ~PhysActive, design = NHANES_design, 
             FUN = svymean, na.rm = TRUE, keep.names = FALSE
             )

# Construct a bar plot of means of total cholesterol by whether or not active 
ggplot(data = out, mapping = aes(x=PhysActive, y=TotChol)) + 
    geom_col()

# Run t test for difference in means of total cholesterol by whether or not active
svyttest(formula = TotChol ~ PhysActive, design = NHANES_design)

```
  
  
  
***
  
Chapter 4 - Modeling quantitative data  
  
Visualization with scatter plots:  
  
* Can look at head circumference compared to age (only captured for babies) using a scatterplot  
	* babies <- filter(NHANESraw, AgeMonths <= 6) %>% select(AgeMonths, HeadCirc)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc)) + geom_point()  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc)) + geom_jitter(width = 0.3, height = 0)  # width jitter but no height jitter  
* Can use weighting to extrapolate the scatter plot to the entire population  
	* ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, size = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(size = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, size = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0, alpha = 0.3) + guides(size = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, color = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(color = FALSE)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE)  
  
Visualizing trends:  
  
* Survey-weighted lines of best fit can be added using the geom_smooth() in ggplot2, with the weight= provided as an aestehtic to the geom_smooth()  
	* ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE) + geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR))  
* Can also graph the best fit trendlines split by a categorical variable  
	* babies <- filter(NHANESraw, AgeMonths <= 6) %>% select(AgeMonths, HeadCirc, WTMEC4YR, Gender)  
    * ggplot(data = babies, mapping = aes(x = AgeMonths, y = HeadCirc, alpha = WTMEC4YR, color = Gender)) + geom_jitter(width = 0.3, height = 0) + guides(alpha = FALSE) + geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR))  
  
Modeling survey data:  
  
* Can use the regression equations directly to predict values for a new data point  
	* mod <- svyglm(HeadCirc ~ AgeMonths, design = NHANES_design)  
    * summary(mod)  
* The standard errors are an assessment of the likely errors between the estimated regression line and the true regression line  
  
More complex modeling:  
  
* Can extend the simple regression to a multiple regression in a parallel slopes model  
	* mod <- svyglm(HeadCirc ~ AgeMonths + Gender, design = NHANES_design)  
* Can also extend the simple regression to a multiple regression with different slopes  
  
Wrap up:  
  
* Packages survey, dplyr, and ggplot2  
* Survey fundamentals - clusters, strata, weights, svydesign(), etc.  
* Categorical data, svytable(), svychisq()  
* Quantiative data, svymean(), svytotal(), svyby(), svyquantile(), svyttest()  
* Modeling trends, svyglm()  
  
Example code includes:  
```{r}

# Create dataset with only 20 year olds
NHANES20 <- filter(NHANESraw, Age == 20)

# Construct scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight)) + 
    geom_point(alpha = 0.3) + 
    guides(size = FALSE)

# Construct bubble plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, size=WTMEC4YR)) + 
    geom_point(alpha = 0.3) + 
    guides(size = FALSE)

# Construct a scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, color=WTMEC4YR)) + 
    geom_point() + 
    guides(color = FALSE)

# Construct a scatter plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, alpha=WTMEC4YR)) + 
    geom_point() + 
    guides(alpha = FALSE)


# Add gender to plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, size=WTMEC4YR, color=Gender)) + 
    geom_point(alpha=0.3) + 
    guides(size = FALSE)

# Add gender to plot
ggplot(data = NHANES20, mapping = aes(x=Height, y=Weight, alpha=WTMEC4YR, color=Gender)) + 
    geom_point() + 
    guides(alpha = FALSE)


# Bubble plot with linear of best fit
ggplot(data = NHANESraw, mapping = aes(x = Height, y = Weight, size=WTMEC4YR)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight=WTMEC4YR))

# Add quadratic curve and cubic curve
ggplot(data = NHANESraw, mapping = aes(x = Height, y = Weight, size = WTMEC4YR)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR)) +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), formula = y ~ poly(x, 2), color = "orange") +
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight = WTMEC4YR), formula = y ~ poly(x, 3), color = "red")


# Add survey-weighted trend lines to bubble plot
ggplot(data = NHANES20, mapping = aes(x = Height, y = Weight, size = WTMEC4YR, color = Gender)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 2)

# Add non-survey-weighted trend lines
ggplot(data = NHANES20, mapping = aes(x = Height, y = Weight, size = WTMEC4YR, color = Gender)) + 
  geom_point(alpha = 0.1) + 
  guides(size = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, linetype = 2) + 
  geom_smooth(method = "lm", se = FALSE, mapping = aes(weight=WTMEC4YR))


# Subset survey design object to only include 20 year olds
NHANES20_design <- subset(NHANES_design, Age == 20)

# Build a linear regression model
mod <- svyglm(Weight ~ Height, design = NHANES20_design)

# Print summary of the model
summary(mod)


# Build a linear regression model same slope
mod1 <- svyglm(Weight ~ Height + Gender, design = NHANES20_design)

# Print summary of the same slope model
summary(mod1)

# Build a linear regression model different slopes
mod2 <- svyglm(Weight ~ Height*Gender, design = NHANES20_design)

# Print summary of the different slopes model
summary(mod2)


# Plot BPDiaAve and BPSysAve by Diabetes and include trend lines
drop_na(NHANESraw, Diabetes) %>% 
    ggplot(mapping = aes(x=BPDiaAve, y=BPSysAve, size=WTMEC4YR, color=Diabetes)) + 
    geom_point(alpha = 0.2) +  
    guides(size = FALSE) + 
    geom_smooth(method="lm", se = FALSE, mapping = aes(weight=WTMEC4YR))

# Build simple linear regression model
mod1 <- svyglm(BPSysAve ~ BPDiaAve, design = NHANES_design)

# Build model with different slopes
mod2 <- svyglm(BPSysAve ~ BPDiaAve*Diabetes, design = NHANES_design)

# Summarize models
summary(mod1)
summary(mod2)

```
  
  
  
***
  
###_Inference for Catgeorical Data_  
  
Chapter 1 - Inference for a Single Parameter  
  
General Social Survey:  
  
* Categorical data are where the data are categories rather than numbers, which is prevalent in the General Social Survey (GSS)  
	* Several thousand people are surveyed, with a goal of drawing inferences about the population from the sample  
    * Can grab the "gss" dataframe from the tidyverse package  
* Can generate an approximate error by using mean +/- 2*SE  
* The bootstrap can be a valuable way to assess the standard errors - calculate the sample statistic within each replicate, and calculate its distribution  
	* library(infer)  
    * boot <- gss2016 %>% specify(response=happy, success="HAPPY") %>% generate(reps=500, type="bootstrap") %>% calculate(stat="prop")  
  
CI interpretations:  
  
* In classicial statistical inference, there is assumed to be a fix but unknown population parameter that is being estimated by way of sampling  
* A 95% CI means that 95% of the intervals formed from random samples would include the true population parameter  
  
Approximation shortcut:  
  
* Standard errors tend to increase when the sample size is small or the probability is close to 50%  
* The normal distribution (bell curve) can be a useful approximation for a large sample size - the normal becomes the sampling distribution  
	* SE = sqrt( p * (1-p) / n )  
    * n * p and n * (1-p) should both be greater than or equal to 10  
  
Example code includes:  
```{r}

load("./RInputFiles/gss.RData")
glimpse(gss)


# Subset data from 2016
gss2016 <- gss %>%
  filter(year == 2016)

gss2016 %>% count(consci)
gss2016 <- gss2016 %>%
    mutate(old_consci=consci, 
           consci=fct_other(fct_recode(old_consci, "High"="A GREAT DEAL"), keep="High", other_level="Low")
           )
gss2016 %>% count(consci)

# Plot distribution of consci
ggplot(gss2016, aes(x = consci)) +
  geom_bar()

# Compute proportion of high conf
p_hat <- gss2016 %>%
  summarize(p = mean(consci == "High", na.rm = TRUE)) %>%
  pull()


# Load the infer package
library(infer)

# Create single bootstrap data set
b1 <- gss2016 %>%
    specify(response = consci, success = "High") %>%
    generate(reps = 1, type = "bootstrap")

# Plot distribution of consci
ggplot(b1, aes(x = consci)) +
  geom_bar()

# Compute proportion with high conf
b1 %>%
  summarize(p = mean(consci == "High")) %>%
  pull()


# Create bootstrap distribution for proportion that favor
boot_dist <- gss2016 %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500) %>%
  calculate(stat = "prop", success = "High", na.rm = TRUE)

# Plot distribution
ggplot(boot_dist, aes(x=stat)) +
  geom_density()

# Compute estimate of SE
SE <- boot_dist %>%
  summarize(se = sd(stat)) %>%
  pull()

# Create CI
c(p_hat - 2*SE, p_hat + 2*SE)


# Two new smaller data sets have been created for you from gss2016: gss2016_small, which contains 50 observations, and gss2016_smaller which contains just 10 observations

id50 <- c(6, 98, 2673, 1435, 1535, 525, 2784, 1765, 163, 1859, 2497, 1780, 184, 575, 2781, 2310, 1677, 2478, 1226, 2350, 1139, 1635, 1350, 1809, 1842, 1501, 1502, 2610, 2456, 49, 56, 2167, 2401, 2002, 2343, 2012, 860, 2557, 1147, 1119, 2449, 695, 1511, 666, 1595, 1094, 2643, 769, 1263, 2426)
id10 <- c(1609, 1342, 2066, 2710, 1809, 503, 1889, 486, 1469, 6)

gss2016_small <- gss2016 %>%
    filter(id %in% id50)
gss2016_smaller <- gss2016 %>%
    filter(id %in% id10)

# Create bootstrap distribution for proportion
boot_dist_small <- gss2016_small %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "prop")

# Compute estimate of SE
SE_small_n <- boot_dist_small %>%
  summarize(se = sd(stat)) %>%
  pull()

# Create bootstrap distribution for proportion
boot_dist_smaller <- gss2016_smaller %>%
  specify(response = consci, success = "High") %>%
  generate(reps = 500, type = "bootstrap") %>%
  calculate(stat = "prop")

# Compute estimate of SE
SE_smaller_n <- boot_dist_smaller %>%
  summarize(se = sd(stat)) %>%
  pull()

c(SE_small_n, SE_smaller_n)


# Create bootstrap distribution for proportion that have hardy any
boot_dist <- gss2016 %>%
  specify(response=consci,  success = "Low") %>%
  generate(reps=500, type="bootstrap") %>%
  calculate(stat = "prop", na.rm = TRUE)

# Compute estimate of SE
SE_low_p <- boot_dist %>%
    summarize(se = sd(stat)) %>%
    pull()


# Compute p-hat and n
p_hat <- gss2016_small %>% 
    summarize(p = mean(consci == "High", na.rm=TRUE)) %>%
    pull()
n <- nrow(gss2016_small)

# Check conditions
p_hat * n >= 10
(1 - p_hat) * n >= 10

# Calculate SE
SE_approx <- sqrt(p_hat * (1 - p_hat) / n)

# Form 95% CI
c(p_hat - 2 * SE_approx, p_hat + 2 * SE_approx)

```
  
  
  
***
  
Chapter 2 - Proportions (Testing and Power)  
  
Hypothesis test for a proportion:  
  
* The hypothesis test for a proportion looks at what sort of p-hat would be observed if p held a specific value  
	* The hypothesize() function prior to generate() sets out the hypothesis in question  
* Suppose that analysis is being run on whether people favor capital punishment  
	* null <- gss2016 %>% specify(response=cappun, success="FAVOR") %>% hypothesize(null="point", p=0.5) %>% generate(reps=500, type="simulate") %>% calculate(stat="prop")  
    * null %>% summarize(mean(stat > p_hat)) %>% pull() * 2  # The times 2 is for a two-sided test  
  
Intervals for differences:  
  
* Can also look at differences in proportions, for example men vs. women belief in afterlife  
* Can generate null data by rewording the null hypothesis to "there is no association between belief in the afterlife and gender" - enables test by permutation  
	* gss2016 %>% specify(response=postlife, explanatory=sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=1, type="permute")  
    * gss2016 %>% specify(postlife ~ sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=1, type="permute")  # can use formula notation; same command as above, but simplified  
    * null <- gss2016 %>% specify(postlife ~ sex, success="YES") %>% hypothesize(null="independence") %>% generate(reps=500, type="permute")  %>% calculate(stat="diff in props", order=c("FEMALE", "MALE")) # Full command  
    * null %>% summarize(mean(stat > d_hat)) %>% pull() * 2  
  
Statistical errors:  
  
* Type I errors - probability of rejecting a true null hypothesis - will happen with probability alpha  
* Type II errors - probability of not rejecting a false null hypothesis  - will happen with probability beta, meaning the test has power 1-beta  
  
Example code includes:  
```{r}

# Construct plot
ggplot(gss2016, aes(x = postlife)) + 
    geom_bar()

# Compute and save proportion that believe
p_hat <- gss2016 %>%
    summarize(mean(postlife == "YES", na.rm = TRUE)) %>%
    pull()

# Generate one data set under H0
sim1 <- gss2016 %>%
    specify(response = postlife, success = "YES") %>%
    hypothesize(null = "point", p = 0.75) %>%
    generate(reps = 1, type = "simulate")

# Construct plot
ggplot(sim1, aes(x=postlife)) +
    geom_bar()

# Compute proportion that believe
sim1 %>%
    summarize(mean(postlife == "YES")) %>%
    pull()


# Generate null distribution
null <- gss2016 %>%
    specify(response = postlife, success = "YES") %>%
    hypothesize(null = "point", p = .75) %>%
    generate(reps = 100, type = "simulate") %>%
    calculate(stat = "prop")

# Visualize null distribution
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = p_hat, color = "red")

# Compute the two-tailed p-value
null %>%
    summarize(mean(stat > p_hat)) %>%
    pull() * 2


# Plot distribution
ggplot(gss2016, aes(x = sex, fill = cappun)) +
    geom_bar(position = "fill")
  
# Compute two proportions
p_hats <- gss2016 %>%
    group_by(sex) %>%
    summarize(mean(cappun == "FAVOR", na.rm = TRUE)) %>%
    pull()

# Compute difference in proportions
d_hat <- diff(p_hats)


# Create null distribution
null <- gss2016 %>%
    specify(cappun ~ sex, success = "FAVOR") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "diff in props", order = c("FEMALE", "MALE"))
  
# Visualize null
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, col = "red")
  
# Compute two-tailed p-value
null %>%
    summarize(mean(stat < d_hat)) %>%
    pull() * 2


# Create the bootstrap distribution
boot <- gss2016 %>%
    specify(cappun ~ sex, success="FAVOR") %>%
    generate(reps=500, type="bootstrap") %>%
    calculate(stat = "diff in props", order = c("FEMALE", "MALE"))
  
# Compute the standard error
SE <- boot %>%
    summarize(sd(stat)) %>%
    pull()
  
# Form the CI (lower, upper)
c( d_hat - 2*SE, d_hat + 2*SE )


gssmod <- gss2016 %>%
    mutate(coinflip=sample(c("heads", "tails"), size=nrow(.), replace=TRUE))
table(gssmod$coinflip)

# Find difference in props
p_hats <- gssmod %>%
    group_by(coinflip) %>%
    summarize(mean(cappun == "FAVOR", na.rm = TRUE)) %>%
    pull()

# Compute difference in proportions
d_hat <- diff(p_hats)

# Form null distribution
null <- gssmod %>%
    specify(cappun ~ coinflip, success = "FAVOR") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "diff in props", order = c("heads", "tails"))

ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, color = "red")


# Set alpha
alpha <- 0.05

# Find cutoffs
upper <- null %>%
    summarize(quantile(stat, probs = c(1-alpha/2))) %>%
    pull()
lower <- null %>%
    summarize(quantile(stat, probs = alpha/2)) %>%
    pull()
  
# Visualize cutoffs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = d_hat, color = "red") +
    geom_vline(xintercept = lower, color = "blue") +
    geom_vline(xintercept = upper, color = "blue")

# check if inside cutoffs
d_hat %>%
    between(lower, upper)

```
  
  
  
***
  
Chapter 3 - Comparing Many Parameters (Independence)  
  
Contingency tables:  
  
* Can look at bivariate relationships, such as political party affiliation vs. opinions on military spending  
	* The broom package can help in movements to/from contingency tables, by keeping things cleaner  
    * tab <- gss2016 %>% select(natarms, party) %>% table()  
    * tab %>% broom::tidy() %>% uncount(Freq)  
  
Chi-squared test statistic:  
  
* Can use Chi-squared to look at dependence of variables  
* Can create a contingency table O of the observations and a contingency table E of the expected distribution if there is pure independence  
	* Can then look at (O-E)**2 / E, and sum up to get the overall Chi-squared distribution  
    * Hypothesis tests can then assess how extreme a given Chi-squared may be  
  
Alternative method - chi-squared test statistic:  
  
* The Chi-squared statistic is derived from the Chi-squared distribution, which is specified solely by the number of degrees of freedom  
	* The degrees of freedom are (nRows - 1) * (nCols - 1)  
    * pchisq(chi_obs_spac, df=4)  # gives the likelihood of actual being less than, can use 1-pchisq() for the amount that is greater (the p-value of interest  
* Generally, need to have 5+ counts per cell, and to only use chi-squared for df=2+ (for df=1, can just compare proportions using the normal distribution)  
  
Intervals for chi-squared:  
  
* Can remove the hypothesize() call and use bootstrap() instead, but there is no real meaning to a Chi-squared in the absence of a null hypothesis  
* It is very unlikely that you would ever see a confidence interval attached to a Chi-squared interval  
  
Example code includes:  
```{r}

# Exclude "other" party
gss_party <- gss2016 %>%
    mutate(party=fct_collapse(partyid, 
                              "D"=c("STRONG DEMOCRAT", "NOT STR DEMOCRAT"), 
                              "R"=c("NOT STR REPUBLICAN", "STRONG REPUBLICAN"),
                              "I"=c("IND,NEAR DEM", "INDEPENDENT", "IND,NEAR REP"),
                              "O"="OTHER PARTY"
                              )
           ) %>%
    filter(!is.na(party), party != "O") %>%
    droplevels()

# Bar plot of proportions
gss_party %>%
    ggplot(aes(x = party, fill = natspac)) +
    geom_bar(position = "fill")
  
# Bar plot of counts
gss_party %>%
    ggplot(aes(x=party, fill = natspac)) +
    geom_bar()


# Create table of natspac and party
O <- gss_party %>%
    select(natspac, party) %>%
    table()

# Convert table back to tidy df
O %>%
    broom::tidy() %>%
    uncount(n)


# Create one permuted data set
perm_1 <- gss_party %>%
    specify(natarms ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1, type = "permute")
  
# Visualize permuted data
ggplot(perm_1, aes(x = party, fill = natarms)) +
    geom_bar()

# Make contingency table
tab <- perm_1 %>%
    ungroup() %>%
    select(natarms, party) %>%
    table()
  
# Compute chi-squared stat
(chi_obs_arms <- chisq.test(tab)$statistic)

(chi_obs_spac <- chisq.test(gss_party$natspac, gss_party$party)$statistic)

# Create null
null <- gss_party %>%
    specify(natspac ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 100, type = "permute") %>%
    calculate(stat = "Chisq")

# Visualize H_0 and obs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_spac, color = "red")

# Create null
null <- gss_party %>%
    specify(natarms ~ party) %>%
    hypothesize(null = "independence") %>%
    generate(reps = 100, type = "permute") %>%
    calculate(stat = "Chisq")

# Visualize H_0 and obs
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_arms, color = "red")


# create bar plot
gss2016 %>%
    ggplot(aes(x = region, fill = happy)) +
    geom_bar(position = "fill") +
    coord_flip()

# create table
tab <- gss2016 %>%
    select(happy, region) %>%
    table()
  
# compute observed statistic
(chi_obs_stat <- chisq.test(tab)$statistic)


# generate null distribution
null <- gss2016 %>%
    mutate(happy=fct_other(happy, keep=c("VERY HAPPY"))) %>%
    specify(happy ~ region, success = "VERY HAPPY") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 500, type = "permute") %>%
    calculate(stat = "Chisq")

# plot null(s)
ggplot(null, aes(x = stat)) +
    geom_density() +
    geom_vline(xintercept = chi_obs_stat) +
    stat_function(fun = dchisq, args = list(df = (9-1)*(2-1)), color = "blue")

# permutation p-value
null %>% 
    summarize(mean(stat > chi_obs_stat)) %>% 
    pull()

# approximation p-value
1 - pchisq(chi_obs_stat, df = (9-1)*(2-1))

```
  
  
  
***
  
Chapter 4 - Comparing Many Parameters (Goodness of Fit)  
  
Case Study: Election Fraud:  
  
* Election fraud has many meanings; this course will focus on altering vote totals  
* Benford's Law applies when looking at broad collections of data, and considering only the first digit  
	* The law proposed that 30.1% of the first digits should be 1, with decreases as the numbers increase  
    * The basic idea is that the 1's always happen first (get to the 100s before any other x00s)  
* Can look at the 2009 Iranian election, and assess in comparison to Benford's Law  
  
Goodness of Fit:  
  
* Desire to assess whether the voter data is well aligned with Benford's law - Chi-squared is a good statistic for this  
	* chisq.test(myTab, p=myProbNull)  
* Can simulate the null hypothesis, for example by using  
	* gss2016 %>% specify(response=party) %>% hypothesize(null="point", p=p_uniform) %>% generate(reps=1, type="simulate")  
  
Now to the US:  
  
* Comparison to the US election in Iowa in 2016  
* Can look at county-level data  
  
Wrap-Up:  
  
* Could have rejected the null hypothesis even when it is true - typically 5%  
* More fundamental errors could be at play, such as assuming the first digit should follow Benford's Law  
	* Population of world cities tend to fit Benford's Law criteria (uniform distribution, consistency of logs, etc.)  
* Techniques for carrying out inference on categorical data - confidence intervals, hypothesis tests, Chi-squared tests for independence, goodness of fit of distributions  
* All tests follow specify-hypohteize-generate-calculate  
  
Example code includes:  
```{r}

iran <- readr::read_csv("./RInputFiles/iran.csv")
glimpse(iran)


# Compute candidate totals
totals <- iran %>%
  summarize(ahmadinejad = sum(ahmadinejad),
            rezai = sum(rezai),
            karrubi = sum(karrubi),
            mousavi = sum(mousavi))

# Plot totals
totals %>%
  gather(key = "candidate", value = "votes") %>%
  ggplot(aes(x = candidate, y = votes)) +
  geom_bar(stat = "identity")
  
# Cities won by #2
iran %>%
  group_by(province) %>%
  summarize(ahmadinejad = sum(ahmadinejad),
            mousavi = sum(mousavi)) %>%
  mutate(mousavi_win = mousavi > ahmadinejad) %>%
  filter(mousavi_win)


# Print get_first
get_first <- function(x) {
    substr(as.character(x), 1, 1) %>%
      as.numeric() %>%
      as.factor()
}

# Create first_digit
iran2 <- iran %>%
  mutate(first_digit = get_first(total_votes_cast))
  
# Construct barchart
iran2 %>%
  ggplot(aes(x=first_digit)) +
  geom_bar()


# Tabulate the counts of each digit
tab <- iran2 %>%
  select(first_digit) %>%
  table()

# Compute observed stat
p_benford <- c(0.301029995663981, 0.176091259055681, 0.1249387366083, 0.0969100130080564, 0.0791812460476248, 0.0669467896306132, 0.0579919469776867, 0.0511525224473813, 0.0457574905606751)
names(p_benford) <- 1:9
p_benford[9] <- 1 - sum(p_benford[-9])
sum(p_benford)
chi_obs_stat <- chisq.test(tab, p = p_benford)$stat

# Form null distribution
null <- iran2 %>%
  specify(response=first_digit) %>%
  hypothesize(null = "point", p = p_benford) %>%
  generate(reps=500, type = "simulate") %>%
  calculate(stat = "Chisq")


# plot both nulls
ggplot(null, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = chi_obs_stat) + 
  stat_function(fun = dchisq, args = list(df = 9-1), color = "blue")

# permutation p-value
null %>%
  summarize(mean(stat > chi_obs_stat)) %>%
  pull()

# approximation p-value
pchisq(chi_obs_stat, df=9-1, lower.tail=FALSE)


iowa <- readr::read_csv("./RInputFiles/iowa.csv")
glimpse(iowa)

# Get R+D county totals
iowa2 <- iowa %>%
  filter(candidate == "Hillary Clinton / Tim Kaine" | candidate == "Donald Trump / Mike Pence") %>%
  group_by(county) %>%
  summarize(dem_rep_votes = sum(votes, na.rm = TRUE)) 

# Add first_digit
iowa3 <- iowa2 %>%
  mutate(first_digit = get_first(dem_rep_votes))

# Construct bar plot
iowa3 %>%
  ggplot(aes(x=first_digit)) + 
  geom_bar()


# Tabulate the counts of each digit
tab <- iowa3 %>%
  select(first_digit) %>%
  table()

# Compute observed stat
chi_obs_stat <- chisq.test(tab, p = p_benford)$stat

# Form null distribution
null <- iowa3 %>%
  specify(response = first_digit) %>%
  hypothesize(null = "point", p = p_benford) %>%
  generate(reps = 500, type = "simulate") %>%
  calculate(stat = "Chisq")
  
# Visualize null
ggplot(null, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = chi_obs_stat)

```
  
  
  
***
  
###_Building Dashboards with flexdashboard_  
  
Chapter 1 - Dashboard Layouts  
  
Introduction:  
  
* Dashboards are a collection of components in a single display - graphs, text, tables, widgets, etc.  
* The flexdashboard is an R package that allows for using R Markdown to create a dashboard  
	* Can include all the power of R  
    * Can combine with Shiny for reactive elements  
* Course will include capabilities of flexdashboard, decision as to whether to incorporate Shiny, and potential extensions  
  
Anatomy of flexdashboard:  
  
* Within R Markdown, the header controls the type of document created during knitting  
	* output: flexdashboard::flex_dashboard (will create the flesdashboard output)  
* The flexdashboard is made up of charts, with each chart denoted by ### ChartName  
	* The succeeding lines can then be R code, similar to other R Markdown processes  
* By default, all of the charts will stack in a single column, though multiple columns can also be declared  
	* Columns are created using 14+ dashes, with everything underneath contained in that column  
    * Can give a specific name and specify options for each of the columns  
* Can start in Rstudio using File - New File - R Markdown - From Template - flexdashboard  
* Course data will include bicycle sharing data from San Francisco  
  
Layout basics:  
  
* Columns can be of variable width by using data-width= such that they add up to 1000  
* Can create by rows rather than columns using orientation:rows underneath flexdashboard::flex_dashboard:  
	* Can use data-height to vary the row heights  
    * Can use certical_layout: scroll as an option to allow for scrolling rather than forcing everything on to one page (this is considered poor dashboard design, though)  
  
Advanced layouts:  
  
* Options for extending the dashboard include tabsets  
	* Column {.tabset} - will apply the tabset to every chart in that column  
* Can also extend by using pages, where columns and rows are children of their respective pages  
	* Sixteen (16) or more equal signs under a Page title specify a call to the page  
    * Can add Page xxx {data-navmenu=yyy} to specify that the page xxx should belong to the navmenu yyy  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 2 - Data Visualization for Dashboards  
  
Graphs:  
  
* The easiest way to add a graph is to include it as part of a code snippet in flexdashboard  
* Graphs will flex and resize to stay in the container  
* Sometimes, may want to set the figure width and height to match the aspect ratio of the target device as closely as possible  
	* {r, fig.width=10, fig.height=5}  
    * Downside #1 - trial and error needed  
    * Downside #2 - need to adjust every time charts are added  
    * Downside #3 - graphs are no longer responsive to user inputs  
  
Web-Friendly Visualizations:  
  
* Web-friendly packages include plotly, highcharter, dygraphs, rbokeh, ggvis  
* The plotly calls are helpful since they are closely linked to ggplot2  
	* library(plotly)  
    * ggplotly(my_ggplot)  # my_ggplot is a ggplot2 object  
  
htmlwidgets:  
  
* htmlwidgets are a framework that connects R with Javascript (web-friendly and well-suited to dashboards)  
	* Learn more at: http://htmlwidgets.org  
* The leaflet package allows for adding interactive maps  
	* library(leaflet)  
    * leaflet() %>% addTiles() %>% addMarkers(lng = data_df$longitude, lat = data_df$latitude)  
    * leaflet(data_df) %>% addTiles() %>% addMarkers()  # leaflet called on a data frame  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 3 - Dashboard Components  
  
Highlighting Single Values:  
  
* Gauges can be helpful for values in a defined range, such as 0%-100%  
	* gauge(value = pct_subscriber_trips, min = 0, max = 100)  # basics for creating a gauge include the value, the min, and the max  
    * gauge(value = pct_subscriber_trips, min = 0, max = 100, sectors = gaugeSectors( success = c(90, 100), warning = c(70, 89), danger = c(0, 69) ), symbol = '%')  # additional features  
* Value boxes can be helpful for values that do not fall in a pre-defined range  
	* valueBox(prettyNum(num_trips, big.mark = ","), caption = "Total Daily Trips", icon = "fa-bicycle")  # font-awesome bicycles  
* Both gauges and value boxes can be linked  
	* valueBox(prettyNum(num_trips, big.mark = ','), caption = 'Total Daily Trips', icon = 'fa-bicycle', href = '#trip-raw-data')  # href makes the caption linked and clickable  
  
Dashboard Tables:  
  
* The kable function from knitr is one of the easiest ways to create a table - but, not very well tuned to html  
	* library(knitr)  
    * kable(my_data_df)  
* The DT package is better suited to making responsive tables  
	* library(DT)  
    * datatable(my_data_df)  
    * datatable(my_data_df, rownames = FALSE)  # eliminate row numbering  
    * datatable(my_data_df, rownames = FALSE, options = list(pageLength = 15))  # most options are set by way of a list; note the contrast to rownames  
    * datatable( my_data_df, rownames = FALSE, extensions = 'Buttons', options = list( dom = 'Bfrtip', buttons = c('copy', 'csv', 'excel', 'pdf', 'print') ) )  # buttons for extract  
  
Text for Dashboards:  
  
* Captions (notes) are a common way to add text to a chart, and are added using a greater than sign with text; there must be an empty line between the end of the chunk and the caption  
* Another way to provide more context is with the storyboard format  
	* Presents one chart at a time in a specified order, where the user controls the navigation speed between the charts  
    * Good format for content that runs in order  
    * Requires storyboard: true in the yaml header in the flexdashboard::flex_dashboard items  
    * Within the story, the ### signal the next page of the story, and should have descriptive text  
    * Can add commentary using the triple asterisk (***) which needs to be AFTER the R chunk and separated by at least one space  
* Can also mix in storyboard on some pages but not on others (requires leaving this out of the yaml header)  
	* Add {.storyboard} to the end of the page description  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
Chapter 4 - Adding Interactivity with Shiny  
  
Incorporating Shiny into Dashboards:  
  
* Incorporating Shiny is optional but can mak the dashboards even more interactive  
	* Shiny is interactive and lightweight, though at the expense of greater complication and hosting challenges  
* Even after incorporating Shiny, the flexdashboard document is still an interactive R Markdown document  
* runtime:shiny in the yaml header will make the flexdashboard in to a Shiny App  
  
Reactive Dataframe Pattern:  
  
* Creating a narrow sidebar using  
	* Column {data-width=200 .sidebar}  
* Widgets can be use inside of an R chunk, like any other dashboard component  
	* sliderInput("duration_slider", label="Select maximum trip duration to display (in minutes): ", min=0, max=120, value=15, step=5, dragRange=TRUE)  
    * show_trips_df <- reactive({ trips_df %>% filter(duration_sec <= input$duration_slider * 60) })  
    * To call the reactice data frame later, use show_trips_df()  
* Output from the reactive needs to be encloses in the appropriate render*() function  
	* renderLeaflet({ show_trips_df() %>% . %>% leaflet() %>% . })  # no need for the output call like there would be in a typical Shiny document  
* Five key steps for the reactive data frame pattern  
	* Create a sidebar column  
    * Add user inputs to the sidebar - *Input() Shiny widgets  
    * Make a data frame that uses the inputs, called later using ()  
    * Replace the dataframe in the dashboard component code with the reactive version  
    * Wrap with the appropriate rendering function render*()  
  
Customized Inputs for Charts:  
  
* Can have a reactive component impact everything, as per the example worked through in the previous section  
* May also want to have sliders that only impact a single object  
	* Putting these together in the same loaction can cause headaches due to the need to work with layouts  
* Example code to implement includes  
	* fillCol(height=600, flex=c(NA, 1), inputPanel(sliderInput("my_input", .)), plotOutput("my_plot", height="100%"))  # flex is the flexible height for the components  
    * output$my_plot <- renderPlot({ . })  
* Can use a global shortcut  
	* Can put all of the charts that are driven by the same slider on to the same page  
    * Can put the sidebar as a class with its own page, followed by all the other pages, to have the same sidebar drive all of the pages  
  
Wrap-up:  
  
* Additional resources available through Rstudio and htmlwidgets.org (information about all html widgets available in R)  
	* The highcharter can be helpful - high quality charts with some interactivity  
* Can use shinydashboard for just Shiny if R Markdown and flexdasboard are not needed  
  
Example code includes (not added due to need for separate dashboard file):  
```{r}


```
  
  
  
***
  
###_Network Analysis in R: Case Studies_  
  
Chapter 1 - Exploring Graphs Through Time  
  
Exploring Data Set:  
  
* Daily snapshots of items purchased together (co-purchases) from Amazon in 2003  
	* There is the to and from that will make up the graph, plus associated metadata  
    * Desire to look only at a single data, and only the from-to data (assuming a directional graph)  
* Can look at a smaller subset of the graph, for example  
	* sg <- induced_subgraph(amzn_g, 1:500)  
    * sg <- delete.vertices(sg, degree(sg) == 0)  
    * plot(sg, vertex.label = NA, edge.arrow.width = 0, edge.arrow.size = 0, margin = 0, vertex.size = 2)  
* Can count the number of diad (2-connect) and triad (3-connect) in the data  
	* null, asymmetric, and mutual are the potential results for diads  
    * Three-digit codes are used to reflect the 16 potential triad states - #Bi/#Assym/#Uncon - plus a letter D, U, and C  
  
Exploring Temporal Structure:  
  
* Dataset has 4 days worth of data - can build from the earliest date to the latest date  
* Can create a list to hold the igraphs at each time period, loop over the times, and then plot them  
	* A handful of vertices may be important and interesting across time  
  
Example code includes:  
```{r}

library(igraph)
amzn_g <- read.graph("./RInputFiles/amzn_g.gml", format=c("gml"))
amzn_g


# Perform dyad census
dc <- dyad_census(amzn_g)

# Perform triad census
tc <- triad_census(amzn_g)

# Find the edge density
ed <- edge_density(amzn_g)

# Output values
print(dc)
print(tc)
print(ed)


# Calculate transitivity
transitivity(amzn_g)

# Calculate reciprocity
amzn_rp <- reciprocity(amzn_g)

# Simulate our outputs
nv <- gorder(amzn_g)
ed <- edge_density(amzn_g)
rep_sim <- rep(NA, 1000)

# Simulate 
for(i in 1:1000){
  rep_sim[i] <- reciprocity(erdos.renyi.game(nv, ed, "gnp", directed = TRUE))
}

# Compare
quantile(rep_sim, c(0.25, .5, 0.975))
print(amzn_rp)


# Get the distribution of in and out degrees
table(degree(amzn_g, mode = "in"))
table(degree(amzn_g, mode = "out"))

# Find important products based on the ratio of out to in and look for extremes
imp_prod <- V(amzn_g)[degree(amzn_g, mode = "out") > 3 & degree(amzn_g, mode = "in") < 3]

## Output the vertices
print(imp_prod)


ipFrom <- c(1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 11163, 32129, 32129, 32129, 32129, 32129, 32129, 32129, 38131, 38131, 38131, 38131, 38131, 38131, 45282, 45282, 45282, 45282, 52831, 52831, 52831, 52831, 52831, 52831, 52831, 52831, 53591, 53591, 53591, 53591, 53591, 53591, 53591, 53591, 56427, 56427, 56427, 56427, 59706, 59706, 59706, 59706, 59706, 59706, 59706, 59706, 62482, 62482, 62482, 62482, 62482, 62482, 67038, 67038, 67038, 67038, 71192, 71192, 71192, 71192, 71192, 77957, 77957, 77957, 77957, 77957, 77957, 103733, 103733, 103733, 103733, 103733, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 117841, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 123808, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 132757, 144749, 144749, 144749, 144749, 144749, 144749, 144749, 170830, 170830, 170830, 170830, 170830, 170830, 177282, 177282, 177282, 177282, 177282, 177282, 177432, 177432, 177432, 177432, 177432, 177432, 177432, 184526, 184526, 184526, 184526, 184526, 191825, 191825, 191825, 191825, 191825, 215668, 215668, 215668, 221085, 221085, 221085, 221085, 221085, 231604, 231604, 231604, 231604, 231604, 231604, 239014, 239014, 239014, 239014, 239014, 242693, 242693, 242693, 242693, 242693, 257621, 257621, 257621, 257621, 261587, 261587, 261587, 261587, 261587, 261587, 261657, 261657, 261657, 261657, 261657, 261657)
ipTo <- c(190, 1366, 2679, 4023, 1625, 1627, 7529, 1272, 1628, 1630, 1631, 11124, 15360, 20175, 10626, 20970, 10776, 11164, 11166, 5955, 8719, 11164, 23842, 23843, 24115, 15312, 23329, 32127, 80473, 44848, 44849, 44850, 38133, 31084, 33711, 10920, 20178, 20179, 87093, 2134, 2136, 4119, 9995, 36524, 64698, 64700, 52833, 120083, 120085, 120086, 36689, 12340, 113789, 32094, 51015, 1898, 10076, 15800, 61488, 63836, 63837, 63838, 8882, 59708, 59711, 26982, 59708, 69497, 69498, 69499, 69500, 23349, 62480, 58926, 58928, 64118, 52271, 71190, 71380, 75384, 9762, 57876, 43543, 43546, 98488, 77951, 77953, 116842, 103732, 103734, 103735, 103728, 124733, 117842, 117843, 117845, 117842, 117843, 117845, 117842, 117842, 117843, 117845, 59267, 89503, 89506, 156, 190, 105428, 184973, 195785, 195787, 132753, 132754, 132755, 52563, 132755, 132756, 132759, 132762, 126757, 132754, 132755, 132756, 189269, 265886, 43155, 80519, 159667, 82479, 152760, 136747, 65216, 114684, 114686, 114687, 117132, 132667, 81755, 109198, 109199, 109202, 144124, 75023, 216449, 139527, 149146, 152038, 177428, 177430, 177428, 177430, 56930, 61658, 207112, 250755, 250756, 56930, 141148, 191036, 147084, 245110, 175959, 177376, 177377, 88463, 103641, 115111, 165118, 228427, 43553, 76706, 78278, 131353, 75725, 119146, 12615, 15740, 229533, 151325, 237568, 239545, 239546, 239547, 110872, 215593, 60310, 60312, 133398, 44502, 261582, 261590, 261599, 271593, 261584, 261588, 261649, 261653, 261654, 261658, 261662, 105814)
ipGroupFrom <- factor(c('DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'Video', 'Video', 'Video', 'Video', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD', 'DVD'), levels=c("DVD", "Video"))
ipSRFrom <- c(30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 16, 16, 16, 16, 16, 16, 16, 37, 37, 37, 37, 37, 37, 26, 26, 26, 26, 14, 14, 14, 14, 14, 14, 14, 14, 16, 16, 16, 16, 16, 16, 16, 16, 10, 10, 10, 10, 1, 1, 1, 1, 1, 1, 1, 1, 19, 19, 19, 19, 19, 19, 10, 10, 10, 10, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 27, 27, 27, 27, 27, 27, 27, 10, 10, 10, 10, 10, 10, 6, 6, 6, 6, 6, 6, 19, 19, 19, 19, 19, 19, 19, 25, 25, 25, 25, 25, 3, 3, 3, 3, 3, 8, 8, 8, 27, 27, 27, 27, 27, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 26, 26, 26, 26, 26, 15, 15, 15, 15, 8, 8, 8, 8, 8, 8, 26, 26, 26, 26, 26, 26)
ipSRTo <- c(5, 2, 18, 20, 12, 6, 8, 14, 16, 4, 18, 20, 3, 6, 14, 5, 3, 3, 4, 3, 13, 3, 5, 9, 18, 17, 8, 2, 8, 9, 16, 9, 24, 11, 25, 6, 9, 3, 21, 1, 5, 2, 24, 2, 6, 6, 8, 18, 7, 4, 20, 6, 22, 13, 10, 19, 4, 22, 7, 7, 9, 7, 11, 21, 12, 17, 21, 5, 7, 2, 1, 26, 6, 14, 2, 17, 4, 13, 12, 6, 8, 13, 4, 7, 1, 7, 9, 15, 19, 6, 20, 0, 19, 14, 18, 11, 14, 18, 11, 14, 14, 18, 11, 16, 1, 5, 3, 5, 6, 22, 5, 20, 10, 29, 9, 22, 9, 12, 10, 9, 12, 29, 9, 12, 13, 6, 23, 6, 18, 10, 18, 6, 9, 11, 8, 8, 19, 12, 10, 9, 8, 14, 1, 7, 10, 13, 18, 6, 6, 4, 6, 4, 4, 22, 5, 8, 4, 4, 13, 11, 3, 4, 21, 22, 8, 18, 1, 6, 5, 5, 4, 8, 6, 12, 6, 3, 13, 8, 10, 1, 1, 22, 12, 18, 19, 5, 18, 31, 8, 13, 10, 14, 25, 4, 19, 17, 5, 21, 3, 1, 19, 10)
ipTRFrom <- c(290, 290, 290, 290, 290, 290, 290, 290, 290, 290, 290, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 73, 73, 73, 73, 73, 73, 73, 294, 294, 294, 294, 294, 294, 43, 43, 43, 43, 5, 5, 5, 5, 5, 5, 5, 5, 13, 13, 13, 13, 13, 13, 13, 13, 28, 28, 28, 28, 1, 1, 1, 1, 1, 1, 1, 1, 110, 110, 110, 110, 110, 110, 7, 7, 7, 7, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 25, 25, 25, 25, 25, 25, 25, 2, 2, 2, 2, 2, 2, 12, 12, 12, 12, 12, 12, 111, 111, 111, 111, 111, 111, 111, 294, 294, 294, 294, 294, 0, 0, 0, 0, 0, 0, 0, 0, 243, 243, 243, 243, 243, 43, 43, 43, 43, 43, 43, 15, 15, 15, 15, 15, 483, 483, 483, 483, 483, 1, 1, 1, 1, 12, 12, 12, 12, 12, 12, 2, 2, 2, 2, 2, 2)
ipTRTo <- c(19, 2, 22, 105, 22, 1, 6, 55, 40, 21, 47, 13, 0, 42, 14, 51, 2, 4, 0, 2, 41, 4, 0, 19, 21, 63, 5, 0, 2, 4, 63, 63, 7, 1, 8, 11, 134, 134, 12, 5, 10, 3, 58, 1, 6, 2, 27, 39, 2, 18, 87, 12, 218, 2, 30, 17, 0, 41, 13, 9, 3, 2, 13, 8, 10, 1, 8, 1, 0, 7, 1, 167, 63, 28, 0, 6, 1, 10, 4, 0, 2, 0, 5, 2, 3, 2, 2, 12, 24, 45, 21, 0, 8, 2, 21, 20, 2, 21, 20, 2, 2, 21, 20, 14, 6, 6, 3, 19, 13, 88, 4, 9, 6, 0, 19, 54, 19, 6, 9, 1, 2, 0, 19, 6, 3, 13, 46, 29, 6, 1, 15, 1, 4, 18, 28, 5, 15, 21, 10, 12, 3, 5, 4, 3, 8, 5, 0, 0, 5, 0, 5, 0, 1, 221, 1, 13, 3, 1, 7, 40, 5, 0, 8, 37, 67, 48, 0, 6, 1, 25, 1, 69, 0, 55, 3, 0, 5, 5, 2, 13, 0, 44, 53, 9, 4, 5, 13, 212, 3, 3, 1, 3, 8, 0, 3, 12, 11, 10, 5, 0, 49, 42)
ipTitleFrom <- c(16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 13, 13, 13, 13, 13, 13, 13, 30, 30, 30, 30, 30, 30, 11, 11, 11, 11, 26, 26, 26, 26, 26, 26, 26, 26, 5, 5, 5, 5, 5, 5, 5, 5, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22, 22, 22, 18, 18, 18, 18, 18, 18, 25, 25, 25, 25, 14, 14, 14, 14, 14, 12, 12, 12, 12, 12, 12, 21, 21, 21, 21, 21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 27, 27, 27, 27, 27, 27, 27, 27, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 29, 29, 29, 29, 29, 29, 29, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 15, 15, 15, 15, 15, 15, 15, 30, 30, 30, 30, 30, 7, 7, 7, 7, 7, 6, 6, 6, 9, 9, 9, 9, 9, 17, 17, 17, 17, 17, 17, 10, 10, 10, 10, 10, 24, 24, 24, 24, 24, 19, 19, 19, 19, 3, 3, 3, 3, 3, 3, 28, 28, 28, 28, 28, 28)
ipNames <- c('Attraction', 'Barbara The Fair With The Silken Hair', 'Cannibal Apocalypse', "DJ Qbert's Wave Twisters", 'David and Lisa', 'Def Comedy Jam  Vol. 13', 'Detroit Lions 2001 NFL Team Video', 'Donnie McClurkin: Live in London and More', 'El Hombre Sin Sombra (Hollow Man)', 'Gladiator', 'Kindergarten Cop', "Kingsley's Meadow - Wise Guy", "Lady & The Tramp II - Scamp's Adventure", 'Lojong - Transforming the Mind (Boxed Set)', 'Menace II Society', 'Merlin', 'Modern Times', 'Murder by Numbers (Full Screen Edition)', 'Nancy Drew: A Haunting We Will Go', 'Princess Nine - Triple Play (Vol. 3)', 'Secret Agent AKA Danger Man  Set 2', 'Seguire Tus Pasos', 'Selena Remembered', 'Seven (New Line Platinum Series)', 'Sheba  Baby', 'Slaughter', 'The Complete Guide to Medicine Ball Training', 'The Gambler', 'The Getaway', 'The Sum of All Fears')
ip_df <- data.frame(X=1:202, 
                    from=ipFrom, 
                    to=ipTo, 
                    salesrank.from=ipSRFrom, 
                    salesrank.to=ipSRTo, 
                    totalreviews.from=ipTRFrom, 
                    totalreviews.to=ipTRTo, 
                    group.from=ipGroupFrom, 
                    title.from=factor(ipNames[ipTitleFrom], levels=ipNames)
                    )


# Create a new graph
ip_g <- graph_from_data_frame(ip_df %>% select(from, to), directed = TRUE)

# Add color to the edges based on sales rank, blue is higer to lower, red is lower to higher
E(ip_g)$rank_flag <- ifelse(ip_df$salesrank.from <= ip_df$salesrank.to, "blue", "red")

# Plot and add a legend
plot(ip_g, vertex.label = NA, edge.arrow.width = 1, edge.arrow.size = 0, 
    edge.width = 4, margin = 0, vertex.size = 4, 
    edge.color = E(ip_g)$rank_flag, vertex.color = "black" )
legend("bottomleft", legend = c("Lower to Higher Rank", "Higher to Lower Rank"), 
       fill = unique(E(ip_g)$rank_flag ), cex = .7)


# Get a count of out degrees for all vertices
# deg_ct <- lapply(time_graph, function(x){return(degree(x, mode = "out") )})

# Create a dataframe starting by adding the degree count
# deg_df <- data.frame(ct = unlist(deg_ct))

# Add a column with the vertex names 
# deg_df$vertex_name <- names(unlist(deg_ct))

# Add a time stamp 
# deg_df$date <- ymd(rep(d, unlist(lapply(time_graph, function(x){length(V(x))}))))

# See all the vertices that have more than three out degrees
# lapply(time_graph, function(x){return(V(x)[degree(x, mode = "out") > 3])})

# Create a dataframe to plot of three important vertices
# vert_df <- deg_df %>% filter(vertex_name %in% c(1629, 132757, 117841))

# Draw the plot to see how they change through time
# ggplot(vert_df, aes(x = date, y = ct, group = vertex_name, colour = vertex_name)) + geom_path()


# Calculate clustering and reciprocity metrics
# trans <- unlist(lapply(all_graphs, FUN=transitivity))
# rp <- unlist(lapply(all_graphs, FUN=reciprocity))

# Create daaframe for plotting
# met_df <- data.frame("metric" = c(trans, rp))

# Repeat the data
# met_df$date <- rep(ymd(d), 2)

# Sort and then Repeat the metric labels
# met_df$name <- sort(rep(c("clustering", "reciprocity"), 4))

# Plot
# ggplot(met_df, aes(x= date, y= metric, group = name, colour = name)) + geom_path()

```
  
  
  
***
  
Chapter 2 - Talk About R on Twitter  
  
Creating retweet graphs:  
  
* Data is several days of tweets from #rstats - want to use retweets (starts with RT) to form the network  
	* raw_tweets <- read.csv("datasets/rstatstweets.csv", stringsAsFactors = F)  
    * all_sn <- unique(raw_tweets$screen_name)  
    * rt_g <- graph.empty()  
    * rt_g <- rt_g + vertices(all_sn)  
    * for(i in 1:dim(raw_tweets)[1]){  
    *     rt_name <- find_rt(raw_tweets$tweet_text[i])  
    *     if(!is.null(rt_name)){  
    *         if(!rt_name %in% all_sn){   
    *             rt_g <- rt_g + vertices(rt_name)  
    *         }  
    *     rt_g <- rt_g + edges(c(raw_tweets$screen_name[i], rt_name))  
    *     }  
    * }  
    * sum(degree(rt_g) == 0)  
    * rt_g <- simplify(rt_g)  
    * rt_g <- delete.vertices(rt_g, degree(rt_g) == 0)  
  
Building mentions graphs:  
  
* Tweets that mention someone can be a reply or a callout  
* There is more complexity than with the retweets, since there is no common format to a mention such as "starts with RT"  
    * ment_g <- graph.empty()  
    * ment_g <- ment_g + vertices(all_sn)  
    * for(i in 1:dim(raw_tweets)[1]) {  
    *     ment_name <- mention_ext(raw_tweets$tweet_text[i])  
    *     if(length(ment_name) > 0 ) {  
    *         for(j in ment_name) {  
    *             if(!j %in% all_sn) {  
    *                 ment_g <- ment_g + vertices(j)  
    *             }  
    *         ment_g <- ment_g + edges(c(raw_tweets$screen_name[i], j))  
    *         }  
    *     }  
    * }  
* The mentions graph is significantly different, with many more small conversations shown by way of a sub-graph  
  
Finding communities:  
  
* Communities are natural way to think of graphs - people who talk much more to each other than to the full network  
	* ment_edg <- cluster_edge_betweenness(as.undirected(ment_g))  
    * ment_eigen <- cluster_leading_eigen(as.undirected(ment_g))  
    * ment_lp <- cluster_label_prop(as.undirected(ment_g))  
    * length(ment_edg)  
    * table(sizes(ment_edg))  
* Can compare similarities within community structures  
	* compare(ment_edg, ment_eigen, method = 'vi')  # "vi" is "variance information"  
    * compare(ment_eigen, ment_lp, method = 'vi')  
    * compare(ment_lp, ment_edg, method = 'vi')  
* Can also plot the community structures  
	* lrg_eigen <- as.numeric(names(ment_eigen[which(sizes(ment_eigen) >45)]))  
    * eigen_sg <- induced.subgraph(ment_g, V(ment_g)[ eigen %in% lrg_eigen])  
* plot(eigen_sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2,  
*      coords = layout_with_fr(ment_sg), margin = 0, vertex.size = 6,  
*      vertex.color = as.numeric(as.factor(V(eigen_sg)$eigen))  
*      )  
  
Example code includes:  
```{r cache=TRUE}

rt_g <- read.graph("./RInputFiles/rt_g.gml", format=c("gml"))
rt_g


# Calculate the number of nodes
gsize(rt_g)

# Calculate the number of edges
gorder(rt_g)

# Calculate the density
graph.density(rt_g)

# Create the plot
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.4, vertex.size = 3)


# Set the default color to black
V(rt_g)$color <- "black"

# Set the color of nodes that were retweeted just once to blue
V(rt_g)[degree(rt_g, mode = "in") == 1]$color <- "blue"

# Set the color of nodes that were retweeters just once to green 
V(rt_g)[degree(rt_g, mode = "out") == 1 ]$color <- "green"

# Plot the network
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, 
    edge.arrow.size = 0.25, 
    vertex.size = 4, vertex.color = V(rt_g)$color)


# Set the default color to black
V(rt_g)$color <- "black"

# Set the color of nodes that were retweeted just once to blue
V(rt_g)[degree(rt_g, mode = "in") == 1 & degree(rt_g, mode = "out") == 0]$color <- "blue"

# Set the color of nodes that were retweeters just once to green 
V(rt_g)[degree(rt_g, mode = "in") == 0 & degree(rt_g, mode = "out") == 1 ]$color <- "green"

# Plot the network
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, 
    edge.arrow.size = 0.25, 
    vertex.size = 4, vertex.color = V(rt_g)$color)


# Calculate betweenness
rt_btw <- igraph::betweenness(rt_g, directed = TRUE)

# Plot histogram
hist(rt_btw, breaks = 2000, xlim = c(0, 1000), main = "Betweenness")

# Calculate eigen centrality
rt_ec <- eigen_centrality(rt_g, directed = TRUE)

# Plot histogram
hist(rt_ec$vector, breaks = 100, xlim = c(0, .2), main = "Eigen Centrality")


# Get top 1% of vertices by eigen centrality
top_ec <- rt_ec$vector[rt_ec$vector > quantile(rt_ec$vector, .99)]

# Get top 1% of vertices by betweenness
top_btw <- rt_btw[rt_btw > quantile(rt_btw, .99)]

# Make a nice data frame to print, with three columns, Rank, Betweenness, and Eigencentrality
most_central <- as.data.frame(cbind(1:length(top_ec), names(sort(top_btw, decreasing = T)), 
                                    names(sort(top_ec, decreasing = T))
                                    )
                              )

# Set column names
colnames(most_central) <- c("Rank", "Betweenness", "Eigen Centrality")

# Print out the data frame
print(most_central)


# Transform rt_btw and add as centrality
V(rt_g)$cent <-  log(rt_btw+2)

# Visualize
plot(rt_g, vertex.label = NA, edge.arrow.width = .2,
     edge.arrow.size = 0.0,
     vertex.size = unlist(V(rt_g)$cent), vertex.color = "red")

# Create subgraph 
rt_sub <-induced_subgraph(rt_g, V(rt_g)[V(rt_g)$cent >= quantile(V(rt_g)$cent, 0.99 )])

# Plot subgraph
plot(rt_sub, vertex.label = NA, edge.arrow.width = .2,
     edge.arrow.size = 0.0,
     vertex.size = unlist(V(rt_sub)$cent), vertex.color = "red")


ment_g <- read.graph("./RInputFiles/ment_g.gml", format=c("gml"))
ment_g


rt_ratio <- degree(rt_g, mode="in") / (degree(rt_g, mode="out"))
ment_ratio <- degree(ment_g, mode="in") / (degree(ment_g, mode="out"))

# Create a dataframe to plot with ggplot
ratio_df <- data.frame(io_ratio = c(ment_ratio, rt_ratio))
ratio_df["graph_type"] <- c(rep("Mention", length(ment_ratio)), rep("Retweet", length(rt_ratio)) )
ratio_df_filtered <- ratio_df %>% filter(!is.infinite(io_ratio) & io_ratio > 0)

# Plot the graph
ggplot(ratio_df, aes(x = io_ratio , fill= graph_type, group = graph_type)) +
  geom_density(alpha = .5) +
  xlim(0, 10)
 
# Check the mean and median of each ratio
ratio_df %>% group_by(graph_type) %>% summarise(m_ratio = mean(io_ratio))
ratio_df %>% group_by(graph_type) %>% summarise(med = median(io_ratio))
ratio_df %>% filter(io_ratio != +Inf) %>% group_by(graph_type) %>% summarise(m_ratio = mean(io_ratio))
ratio_df %>% filter(io_ratio != +Inf) %>% group_by(graph_type) %>% summarise(med = median(io_ratio))


# Plot mention graph 
plot(ment_g, vertex.label = NA, edge.arrow.width = .8,
     edge.arrow.size = 0.2,
     margin = 0,
     vertex.size = 3)

# Find the assortivity of each graph
assortativity_degree(rt_g, directed = TRUE)
assortativity_degree(ment_g, directed = TRUE)

# Find the reciprocity of each graph
reciprocity(rt_g) 
reciprocity(ment_g)


# Get size 3 cliques
clq_list <- cliques(ment_g, min = 3, max = 3)

# Convert to a dataframe and filter down to just revodavid cliques
clq_df <- data.frame(matrix(names(unlist(clq_list)), nrow = length(clq_list), byrow = T))
rev_d <- clq_df %>% filter(X1 == "revodavid" | X2 == "revodavid" | X3 == "revodavid") %>% droplevels()

# Create empty graph and build it up
clq_g_empty <- graph.empty()
clq_g <- clq_g_empty + vertices(unique(unlist(rev_d)))
for(i in 1:dim(rev_d)[1]){
  clq_g <- clq_g + edges(rev_d[i, 1], rev_d[i, 2])
  clq_g <- clq_g + edges(rev_d[i, 2], rev_d[i, 3])
  clq_g <- clq_g + edges(rev_d[i, 1], rev_d[i, 3])}

# Trim graph and plot using `simplify()`
clq_g_trimmed <- as.undirected(simplify(clq_g))
plot(clq_g_trimmed)


# Find the communities
rt_fgc <-  cluster_fast_greedy(as.undirected(rt_g))
rt_info <- cluster_infomap(as.undirected(rt_g))
rt_clust <- cluster_louvain(as.undirected(rt_g))

# Compare all the communities
compare(rt_fgc, rt_clust, method = 'vi')
compare(rt_info, rt_clust, method = 'vi')
compare(rt_fgc, rt_info, method = 'vi')

# Test membership of the same users
fgc_test <- which(names(membership(rt_fgc)) %in% c("bass_analytics", "big_data_flow"))
membership(rt_fgc)[fgc_test]

info_test <- which(names(membership(rt_info)) %in% c("bass_analytics", "big_data_flow"))
membership(rt_info)[info_test]


# The crossing() function in igraph will return true if a particular edge crosses communities
# This is useful when we want to see certain vertices that are bridges between communities

# Assign cluster membership to each vertex in rt_g using membership()
V(rt_g)$clust <- membership(rt_clust)

# Assign crossing value to each edge
E(rt_g)$cross <- crossing(rt_clust, rt_g)

# Plot the whole graph (this is probably a mess)
plot(rt_g, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2, 
     coords = layout_with_fr(rt_g), margin = 0, vertex.size = 3, 
     vertex.color = V(rt_g)$clust, edge.color = E(rt_g)$cross+1)

# Create a subgraph with just a few communities greater than 50 but less than 90 in size
mid_comm <- as.numeric(names(sizes(rt_clust)[sizes(rt_clust) > 50 & sizes(rt_clust) < 90 ]))
rt_sg <- induced.subgraph(rt_g, V(rt_g)[ clust %in% mid_comm ])

# Plot the subgraph
plot(rt_sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.2, 
     coords = layout_with_fr(rt_sg), margin = 0, vertex.size = 3, 
     vertex.color = V(rt_sg)$clust, edge.color = E(rt_sg)$cross+1)

```
  
  
  
***
  
Chapter 3 - Bike Sharing in Chicago  
  
Creating our graph from raw data:  
  
* Dataset based on Chicago Divvy bike sharing, from freely available data  
	* bike_dat <- read.csv("/Users/edmundhart/wkspace/courses-case-studies-network-r/datasets/bike2_test3.csv", stringsAsFactors = F)  
    * trip_df <- bike_dat %>% group_by(from_station_id, to_station_id) %>% summarise(weights = n())  
    * head(trip_df)  
    * trip_g <- graph_from_data_frame(trip_df[, 1:2])  
    * E(trip_g)$weight <- trip_df$weights  
    * gsize(trip_g)  
    * gorder(trip_g)  
* Can create a sub-graph and run some initial explorations on the network - will notice that there are many loops (trips where to/from is the same)  
	* sg <- induced_subgraph(trip_g, 1:12)  
    * plot(sg, vertex.label = NA, edge.arrow.width = .8, edge.arrow.size = 0.6, margin = 0, vertex.size = 6, edge.width = log(E(sg)$weight+2))  
  
Compare Graph Distance vs. Geographic Distance:  
  
* Graphs do not always reflect the geography well; graph distance may or may not be related to geographic distance  
* Can get graph distances using built in functions  
	* farthest_vertices(trip_g_simp)  
    * get_diameter(trip_g_simp)  
* Can use geographic coding (lat/lon) to find the geographic distances  
	* library(geosphere)  
    * st_to <- bike_dat %>% filter(from_station_id == 336 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    * st_from <- bike_dat %>% filter(from_station_id == 340 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    * farthest_dist <- distm(st_from, st_to, fun = distHaversine)  
    * bike_dist <- function(station_1, station_2, divy_bike_df){  
    *     st1 <- divy_bike_df %>% filter(from_station_id == station_1 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    *     st2 <- divy_bike_df %>% filter(from_station_id == station_2 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)  
    *     farthest_dist <- distm(st1, st2, fun = distHaversine)  
    *     return(farthest_dist)  
    * }
  
Connectivity:  
  
* Can be measured either for vertex or edges - how many need to be removed to create 2 distinct graphs  
	* rand_g <- erdos.renyi.game(10, .4, "gnp", directed = F)  
    * plot(rand_g)  
    * vertex_connectivity(rand_g)  
    * edge_connectivity(rand_g)  
    * min_cut(rand_g, value.only = F)  # more information about the connectivity  
* Can then run comparisons between random graphs and bike-sharing graphs  
	* nv <- gorder(trip_g_ud)  
    * ed <- edge_density(trip_g_ud)  
    * graph_vec <- rep(NA, 1000)  
    * for(i in 1:1000){ w1 <- erdos.renyi.game(nv, ed, "gnp", directed = T) ; graph_vec[i]<- edge_connectivity(w1) }  
  
Example code includes:  
```{r}

bike_dat <- readr::read_csv("./RInputFiles/divvy_bike_sample.csv")
glimpse(bike_dat)

# Create trip_df_subs
trip_df_subs <- bike_dat %>% 
  filter(usertype == "Subscriber") %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_subs <- graph_from_data_frame(trip_df_subs[, 1:2])

# Add edge weights
E(trip_g_subs)$weights <- trip_df_subs$weights / sum(trip_df_subs$weights)

# Now work the same code and filter it down to non-subs
trip_df_non_subs <- bike_dat %>% 
  filter(usertype == "Customer") %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_non_subs <- graph_from_data_frame(trip_df_non_subs[, 1:2])

# Add edge weights
E(trip_g_non_subs)$weights <- trip_df_non_subs$weights / sum(trip_df_non_subs$weights)

# Now let's compare these graphs
gsize(trip_g_subs)
gsize(trip_g_non_subs)


# Create the subgraphs
sg_sub <- induced_subgraph(trip_g_subs, 1:12)
sg_non_sub <- induced_subgraph(trip_g_non_subs, 1:12)

# Plot sg_sub
plot(sg_sub, vertex.size = 20, edge.arrow.width = .8, edge.arrow.size = 0.4, 
     margin = 0, edge.width = E(sg_sub)$weights*10000, main = "Subscribers")

# Plot sg_non_sub
plot(sg_non_sub, vertex.size = 20, edge.arrow.width = .8, edge.arrow.size = 0.4, 
     margin = 0, vertex.size = 10, edge.width = E(sg_non_sub)$weights*10000, 
     main = "Customers")


bike_dist <- function(station_1, station_2, divy_bike_df){ 
   st1 <- divy_bike_df %>% filter(from_station_id == station_1 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)
   st2 <- divy_bike_df %>% filter(from_station_id == station_2 ) %>% sample_n(1) %>% select(from_longitude, from_latitude)
   farthest_dist <- geosphere::distm(st1, st2, fun = geosphere::distHaversine)
   return(farthest_dist)
}


# See the diameter of each graph
get_diameter(trip_g_subs)
get_diameter(trip_g_non_subs)

# Find the farthest vertices
farthest_vertices(trip_g_subs)
farthest_vertices(trip_g_non_subs)

# See how far apart each one is and compare the distances
bike_dist(200, 298, bike_dat)
bike_dist(116, 281, bike_dat)


# Create trip_df
trip_df <- bike_dat %>% 
  group_by(from_station_id, to_station_id) %>% 
  summarise(weights = n())

# Create igraph object
trip_g_df <- graph_from_data_frame(trip_df[, 1:2])

# Add edge weights
E(trip_g_df)$weights <- trip_df$weights / sum(trip_df$weights)


trip_g_simp <- simplify(trip_g_df, remove.multiple=FALSE)
trip_g_simp


# Find the degree distribution
trip_out <- degree(trip_g_simp, mode = "out")
trip_in <- degree(trip_g_simp, mode = "in")

# Create a data frame for easier filtering 
trip_deg <- data.frame(cbind(trip_out, trip_in))
trip_deg$station_id <- names(trip_out)
trip_deg_adj <- trip_deg %>% mutate(ratio = trip_out / trip_in)

# Filter out rarely traveled to stations
trip_deg_filter <- trip_deg_adj %>% filter(trip_out > 10) %>% filter(trip_in > 10) 

# Plot histogram
hist(trip_deg_filter$ratio)

# See which stations were the most skewed using which.min() and which.max()
trip_deg_filter %>% slice(which.min(ratio))
trip_deg_filter %>% slice(which.max(ratio))


# If the weights are the same across all stations, then an unweighted degree ratio would work
# But if we want to know how many bikes are actually flowing, we need to consider weights
# The weighted analog to degree distribution is strength
# We can calculate this with the strength() function, which presents a weighted degree distribution based on the weight attribute of a graph's edges

# Calculate the weighted in and out degrees
trip_out_w <- strength(trip_g_simp, mode = "out")
trip_in_w <- strength(trip_g_simp, mode = "in")

# Create a data frame for easier filtering 
trip_deg_w <- data.frame(cbind(trip_out_w, trip_in_w))
trip_deg_w$station_id <- names(trip_out_w)
trip_deg_w_adj <- trip_deg_w %>% mutate(ratio = trip_out_w / trip_in_w)

# Filter out rarely traveled to stations
trip_deg_w_filter <- trip_deg_w_adj %>% filter(trip_out_w > 10) %>% filter(trip_in_w > 10) 

# Plot histogram of ratio
hist(trip_deg_w_filter$ratio)

# See which stations were the most skewed using which.min() and which.max()
trip_deg_w_filter %>% slice(which.min(ratio))
trip_deg_w_filter %>% slice(which.max(ratio))


latlong <- data.frame(from_longitude=c(-87.656495, -87.660996, -87.6554864, -87.642746, -87.67328, -87.661535, -87.623727, -87.668745, -87.65103, -87.666507, -87.666611), 
                      from_latitude=c(41.858166, 41.869417, 41.8694821, 41.880422, 41.87501, 41.857556, 41.864059, 41.857901, 41.871737, 41.865234, 41.891072)
                      )

# Create a sub graph of the least traveled graph 275
g275 <- make_ego_graph(trip_g_simp,  1, nodes = "275", mode= "out")[[1]]

# Plot graph with geographic coordinates
plot(g275, layout = as.matrix(latlong), vertex.label.color = "blue", vertex.label.cex = .6,
     edge.color = 'black', vertex.size = 15, edge.arrow.size = .1,
     edge.width = E(g275)$weight, main = "Lat/Lon Layout")

# Plot graph without geographic coordinates
plot(g275, vertex.label.color = "blue", vertex.label.cex = .6,
     edge.color = 'black', vertex.size = 15, edge.arrow.size = .1,
     edge.width = E(g275)$weight,
     main = "Default Layout")


# Eigen centrality weighted
ec_weight <- eigen_centrality(trip_g_simp, directed = T, weights = NULL)

# Eigen centrality unweighted
ec_unweight <- eigen_centrality(trip_g_simp, directed = T, weights = NA)

# Closeness weighted
close_weight <- closeness(trip_g_simp, weights = NULL)

# Closeness unweighted
close_unweight <- closeness(trip_g_simp, weights = NA)

# Output nicely with cbind()
cbind(c(
  names(V(trip_g_simp))[which.min(ec_weight$vector)],
  names(V(trip_g_simp))[which.min(close_weight)],
  names(V(trip_g_simp))[which.min(ec_unweight$vector)],
  names(V(trip_g_simp))[which.min(close_unweight)]
  ), c("Weighted Eigen Centrality", "Weighted Closeness", "Unweighted Eigen Centrality", "Unweighted Closeness")
)


trip_g_ud <- as.undirected(trip_g_simp)
trip_g_ud


# Find the minimum number of cuts using min_cut()
ud_cut <- min_cut(trip_g_ud, value.only = FALSE)

# Print the vertex with the minimum number of cuts
print(ud_cut$partition1)

# Make an ego graph
g<- make_ego_graph(trip_g_ud, 1, nodes = "281")[[1]]
plot(g, edge.color = 'black', edge.arrow.size = .1)

# Print the value
print(ud_cut$value)

# Print cut object
print(ud_cut$cut)

far_stations <- c("231", "321")
close_stations <- c("231", "213")

# Compare the output of close and far vertices
stMincuts(trip_g_simp, far_stations[1], far_stations[2])$value
stMincuts(trip_g_simp, close_stations[1], close_stations[2])$value


# Find the actual value
clust_coef <- transitivity(trip_g_simp, type = "global")

# Get randomization parameters using gorder() and edge_density()
nv <- gorder(trip_g_simp)
ed <- edge_density(trip_g_simp)

# Create an empty vector to hold output of 300 simulations
graph_vec <- rep(NA, 300)

# Calculate clustering for random graphs
for(i in 1:300){
  graph_vec[i]<- transitivity(erdos.renyi.game(nv, ed, "gnp", directed = T), type = "global")
}

# Plot a histogram of the simulated values
hist(graph_vec, xlim = c(.35, .6), main = "Unweighted clustering randomization")

# Add a line with the true value
abline(v = clust_coef, col = "red")


# Find the mean local weighted clustering coeffecient
m_clust <- mean(transitivity(trip_g_simp, type = "weighted"))
nv <- gorder(trip_g_simp)
ed <- edge_density(trip_g_simp)
graph_vec <- rep(NA, 100)

for(i in 1:100){
  g_temp <- erdos.renyi.game(nv, ed, "gnp", directed = T)
  # Sample existing weights and add them to the random graph
  E(g_temp)$weight <- sample(x = E(trip_g_simp)$weights, size = gsize(g_temp), replace = TRUE)
  graph_vec[i]<- mean(transitivity(g_temp, type = "weighted"))
}

# Plot a histogram of the simulated values
hist(graph_vec, xlim = c(.35, .7), main = "Unweighted clustering randomization")

# Add a line with the true value
abline(v = m_clust ,col = "red")

```
  
  
  
***
  
Chapter 4 - Other Ways to Visualize Graph Data  
  
Other packages for plotting graphs:  
  
* Base plotting in igraph is good for quick visualizations, but other libraries can make great plots simply  
* Can use the ggplot syntax with ggnet, for example  
	* library(ggnetwork)  
    * library(igraph)  
    * library(GGally)  
    * library(intergraph)  
* The basic igraph plotting is as follows  
	* rand_g <- erdos.renyi.game(30, .15, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * plot(rand_g)  
* The basic ggnet plotting requires use of asNetwork on the core graph  
	* net <- asNetwork(rand_g)  
    * ggnet2(net)  
* The basic ggnetwork plotting is more similar to ggplot2  
	* gn <- ggnetwork(rand_g)  
    * g <- ggplot(gn, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges() + geom_nodes() + theme_blank()  
    * plot(g)  
* Where an igraph might require voluminous code for extensions, the ggnet or ggnetwork can be extended in a much simpler manner  
	* ggnet2(net, node.size = "cent", node.color = "comm", edge.size = .8, color.legend = "Community Membership", color.palette = "Spectral", edge.color = c("color", "gray88"), size.cut = T, size.legend = "Centrality")  
    * g <- ggplot(gn, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(aes(color = as.factor(comm))) + geom_nodes(aes(color = as.factor(comm), size = cent)) + theme_blank() + guides(color = guide_legend(title = "Community"), size = guide_legend(title = "Centrality"))  
    * plot(g)  
  
Interactive visualizations:  
  
* Can use R to create interactive javascript plots for outputs to html or similar  
* Examples can be built on a simple random graph  
	* library(ggiraph)  
    * library(htmlwidgets)  
    * library(networkD3)  
    * rand_g <- erdos.renyi.game(30, .12, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * V(rand_g)$cent <- betweenness(rand_g)  
* Can plot using ggplot2 and ggiraph  
	* g <- ggplot(ggnetwork(rand_g), aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(color = "black") + geom_nodes(aes(size = cent)) + theme_blank() + guides(size = guide_legend(title = "Centrality"))  
    * my_gg <- g + geom_point_interactive(aes(tooltip = round(cent, 2)), size = 2)  # Create ggiraph object  
    * ggiraph(code = print(my_gg))  # Display ggiraph object  
* Can also customize the ggiraph call using valid CSS, for example using  
	* hover_css = "cursor:pointer;fill:red;stroke:red;r:5pt"  
    * data_id = round(cent, 2)), size = 2)  
    * ggiraph(code = print(my_gg), hover_css = hover_css, tooltip_offx = 10, tooltip_offy = -10)  
* Can also plot with networkD3 which is easy to use but does not allow for much customization  
	* nd3 <- igraph_to_networkD3(rand_g)  
    * simpleNetwork(nd3$links)  
* Can add complexity to the D3 plot by specifying information about the nodes and edges to be plotted insinde forceNetwork()  
	* nd3$nodes$group = V(rand_g)$comm  
    * nd3$nodes$cent = V(rand_g)$cent  
    * forceNetwork(Links = nd3$links, Nodes = nd3$nodes, Source = 'source', Target = 'target', NodeID = 'name', Group = 'group', Nodesize = 'cent', legend = T, fontSize = 20)  
  
Alternative visualizations:  
  
* Hairball plots are designed to maximize spacing between vertices, but as plots get large the information is hard if not impossible to interpret  
* One potential solution is the hive plot  
	* library(HiveR)  
    * rand_g <- erdos.renyi.game(18, .3, "gnp", directed = T)  
    * plot(rand_g, vertex.size = 7)  # standard igraph plot  
    * rand_g_df <- as.data.frame(get.edgelist(rand_g))  
    * rand_g_df$weight <- 1  
    * rand_hive <- edge2HPD(edge_df = rand_g_df)  
    * rand_hive$nodes$axis <- sort(rep(1:3, 6))  
    * rand_hive$nodes$radius <- as.double(rep(1:6, 3))  
    * plotHive(rand_hive, method="abs", bkgnd="white")  
* Can modify hive plots by way of either nodes or edges  
	* # Setting location of each node  
    * rand_hive$nodes$axis <- sort(rep(1:3, 6))  
    * rand_hive$nodes$radius <- as.double(rep(1:6, 3))  
    * # Add weights to each edge  
    * rand_hive$edges$weight <- as.double(rpois(length(rand_hive$edges$weight), 5))  
    * # Add color based on edge origination  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 1:6] <- 'red'  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 7:12] <- 'blue'  
    * rand_hive$edges$color[rand_hive$edges$id1 %in% 13:18] <- 'green'  
    * # Plot  
    * plotHive(rand_hive, method = "abs", bkgnd = "white")  
* Another alternative is the biofabric plot  
	* # Create random graph  
    * rand_g <- erdos.renyi.game(10, .3, "gnp", directed = F)  
    * rand_g <- simplify(rand_g)  
    * # Add names to vertices  
    * V(rand_g)$name <- LETTERS[1:length(V(rand_g))]  
    * # Create biofabric plot  
    * biofbc <- bioFabric(rand_g)  
    * bioFabric_htmlwidget(biofbc)  
  
Example code includes:  
```{r cache=TRUE}

verts <- c(1185, 3246, 1684, 3634, 3870, 188, 2172, 3669, 2267, 1877, 3931, 1862, 2783, 2351, 423, 3692, 1010, 173, 1345, 3913, 3646, 2839, 2624, 4072, 2685, 2901, 2227, 2431, 1183, 602, 3937, 3688, 2823, 3250, 101, 1951, 3097, 884, 1299, 945, 583, 1691, 1687, 1504, 622, 566, 949, 1897, 1083, 3491, 187, 1799, 3249, 496, 2280, 840, 519, 3060, 4115, 1520, 2700, 385, 1558, 1113, 3303, 1818, 3283, 3291, 3218, 1781, 3055, 2547, 2874, 3, 1923, 890, 1536, 2477, 1422, 449, 984, 2697, 1686, 3181, 415, 1754, 3972, 3600, 3573, 706, 527, 2631, 1383, 2644, 1290, 756, 3147, 377, 4109, 2056, 2411, 1337, 1963, 3833, 1939, 4030, 4111, 2442, 1647, 590, 3749, 1208, 244, 3796, 2886, 570, 2199, 3818, 2342, 1618, 2591, 1279, 1230, 878, 1476, 3930, 616, 364, 567, 2753, 2470, 3554, 2683, 2938, 2077, 2629, 3273, 3131, 3900, 1749, 1240, 1629, 42, 731, 3350, 919, 950, 305, 976, 2906, 3363, 1974, 1539, 978, 441, 1546, 4110, 860, 1762, 864, 1989, 1401, 2572, 1482, 1406, 2110, 2926, 874, 1631, 1050, 2488, 726, 3408, 2946, 2636, 2437, 1468, 2089, 3447, 2292, 3308, 1231, 2788, 1043, 2339, 1893, 3935, 2220, 3589, 3544, 1077, 1263, 4114, 2434, 3679, 1831, 1596, 2585, 598, 2246, 936, 3770, 2355, 2017, 1576, 3445, 1425, 1128, 668, 674, 1884, 989, 845, 2634, 4068, 2736, 1374, 3922, 3202, 3583, 1102, 3746, 2838, 2674, 206, 3966, 1860, 2180, 2717, 3562, 2405, 1666, 2107, 228, 1014, 1543, 768, 3229, 594, 3117, 2121, 2568, 666, 2454, 1209, 2807, 1545, 3753, 3744, 2812, 995, 858, 2293, 1034, 2053, 3034, 650, 1562, 1821, 3351, 3572, 3402, 2600, 3663, 1991, 2222, 1296, 1338, 78, 1936, 3352, 25, 278, 632, 2962, 2826, 3734, 1792, 286, 2491, 2912, 4028, 1522, 863, 223, 1518, 249, 866, 210, 2567, 1140, 386, 276, 3368, 2885, 3122, 3754, 396, 379, 3051, 2996, 36, 2973, 4106, 2404, 1834, 3920, 32, 1724, 1876, 1484, 1769, 2715, 211, 1350, 3054, 3178, 904, 1346, 3256, 3243, 1124, 559, 2672, 394, 128, 3790, 133, 1283, 3468, 3934, 1085, 2794, 3157, 1190, 1864, 2638, 2426, 2435, 3696, 1567, 451, 1987, 850, 1836, 1397, 3710, 1465, 865, 2350, 515, 3645, 1940, 614, 2341, 3711, 2516, 3914, 1216, 3140, 541, 725, 3369, 1157, 1364, 2943, 3947, 67, 1525, 1812, 1582, 1285, 4117, 1705, 1999, 3608, 2899, 782, 1155, 3632, 2187, 2844, 1393, 2873, 2008, 3412, 692, 1053, 355, 785, 3643, 1105, 2706, 2927, 393, 893, 1007, 4021, 439, 3687, 3667, 510, 3365, 2141, 1469, 1671, 2623, 307, 1259, 2526, 1176, 3083, 798, 1845, 1023, 712, 3520, 1191, 1771, 104, 2025, 2382, 2204, 3784, 3292, 2313, 1119, 1433, 593, 3182, 3516, 2079, 1215, 3673, 3831, 2257, 399, 1793, 366, 3690, 1041, 2147, 2690, 609, 3184, 2603, 2793, 540, 1315, 2471, 1922, 3792, 882, 214, 867, 3261, 3816, 2737, 3990, 457, 3566, 1595, 1697, 605, 2138, 990, 841, 2524, 1033, 2958, 343, 2998, 1559, 2756, 2414, 1620, 2285, 2, 791, 2566, 783, 2961, 1120, 2500, 3390, 421, 464, 2463, 4056, 3029, 3525, 256, 1668, 2544, 316, 3598, 917, 180, 2485, 2848, 1280, 1326, 1039, 290, 1321, 644)
verts <- c(verts, 1937, 1820, 3733, 1232, 1677, 298, 3102, 1427, 2653, 619, 1639, 2774, 226, 2934, 1084, 1312, 1123, 135, 1865, 2440, 3245, 92, 3551, 1088, 3370, 2467, 1604, 2928, 142, 2648, 1250, 2970, 1918, 983, 2866, 328, 2976, 3653, 2692, 4099, 291, 3819, 2864, 1375, 1169, 732, 2031, 3166, 1888, 2092, 2372, 1887, 1816, 58, 170, 3306, 3903, 715, 2312, 2323, 1404, 3824, 1942, 3142, 1964, 3214, 2084, 1502, 3366, 2513, 1464, 66, 2007, 1735, 3109, 2876, 3021, 1301, 3089, 535, 996, 3916, 3451, 2057, 1858, 215, 3417, 424, 312, 3103, 1791, 1189, 3149, 113, 835, 2415, 794, 3636, 612, 2816, 514, 2889, 1162, 1313, 2210, 339, 3850, 3481, 2047, 2739, 3124, 2643, 3428, 155, 3161, 3027, 2711, 1317, 148, 1273, 956, 2969, 1265, 1063, 3899, 3945, 1597, 2543, 363, 767, 3322, 2618, 2850, 1454, 2066, 2778, 3534, 1339, 314, 2174, 2589, 297, 3932, 2132, 2612, 3180, 1649, 1966, 2552, 3581, 3148, 196, 1741, 1213, 2924, 3936, 406, 3631, 813, 259, 3230, 543, 2233, 599, 70, 1797, 3607, 975, 1448, 2022, 2777, 696, 1581, 1542, 2523, 2457, 2857, 3046, 3272, 1891, 3681, 586, 1644, 871, 137, 2176, 1849, 480, 972, 1996, 565, 330, 1466, 1217, 2888, 889, 80, 3487, 1143, 2157, 3594, 3747, 634, 1463, 2150, 1775, 2247, 2484, 1658, 1309, 24, 13, 3383, 367, 1423, 2439, 2522, 3637, 2064, 3639, 4046, 2078, 3676, 3506, 1413, 2964, 2192, 3130, 4078, 1069, 2720, 3344, 1090, 5, 3848, 501, 167, 3915, 3787, 4049, 3986, 233, 2343, 3196, 3918, 4063, 537, 242, 3809, 1648, 1662, 2986, 124, 685, 1726, 4087, 1932, 3999, 1910, 484, 489, 1382, 2289, 2189, 3067, 2722, 2262, 2702, 429, 839, 1109, 1361, 2123, 4058, 3959, 2735, 52, 2183, 2707, 1538, 678, 63, 943, 3047, 3108, 1806, 730, 1628, 2664, 1355, 345, 932, 1201, 861, 3861, 1214, 403, 156, 3429, 3210, 3355, 1583, 2479, 3508, 164, 2299, 3320, 2923, 2562, 460, 4013, 417, 1947, 1853, 2272, 1027, 1997, 3266, 2449, 250, 1486, 177, 1118, 3644, 14, 2538, 3836, 2368, 3349, 1879, 2310, 3413, 4032, 319, 3155, 2413, 3842, 3724, 1802, 3319, 2940, 31, 773, 426, 1067, 2374, 3240, 2335, 4010, 3398, 3096, 392, 245, 2898, 4026, 138, 2109, 1526, 2011, 881, 512, 372, 1650, 3373, 3659, 552, 2474, 1712, 3786, 2185, 43, 3406, 2890, 3504, 348, 2982, 2186, 481, 4018, 3048, 1360, 962, 838, 720, 1826, 4011, 2161, 1763, 2617, 2447, 65, 1227, 3938, 2569, 3662, 1746, 2742, 4020, 2148, 1643, 2450, 4093, 3905, 230, 3401, 168, 2779, 1847, 1006, 3074, 1894, 1702, 1229, 3704, 2586, 3595, 1163, 3661, 2230, 3236, 1111, 1770, 438, 2504, 2828, 651, 2456, 1900, 3050, 506, 1674, 3477, 2766, 76, 3606, 3630, 1237, 3617, 295, 3512, 1286, 3623, 3495, 964, 3407, 494, 3629, 140, 1178, 3045, 2041, 194, 3852, 3800, 1605, 1420, 1968, 442, 3570, 1796, 1729, 369, 2401, 1507, 2462, 145, 2580, 848, 4043, 3443, 2979, 22, 3727, 1316, 1437, 3450, 3590, 3465, 3188, 2373, 432, 3425, 3449, 1356, 273, 700, 1789, 1251, 1767, 3998, 2005, 1222, 2214, 340)

# Create subgraph of rt_g
rt_samp <- induced_subgraph(rt_g, verts)

# Convert from igraph using asNetwork()
net <- intergraph::asNetwork(rt_samp)

# Plot using igraph
plot(rt_samp, vertex.label = NA, edge.arrow.size = 0.2, edge.size = 0.5, 
     vertex.color = "black", vertex.size = 1
     )

# Plot using ggnet2
GGally::ggnet2(net, node.size = 1, node.color = "black", edge.size = .4)


# Raw plot of rt_samp using ggnetwork()
library(ggnetwork)
library(GGally)

ggplot(ggnetwork(rt_samp, arrow.gap = .01) , aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), color = "black") +
    geom_nodes(size = 4) 

# Prettier plot of rt_samp using ggnetwork()
ggplot(ggnetwork(rt_samp, arrow.gap = .01),aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), color = "black", curvature = .2) +
    geom_nodes(size = 4) + theme_blank()

# NEED TO FIX!
rt_keys <- sort(table(vertex_attr(rt_g)$clust), decreasing=TRUE)
# rt_drops <- names(rt_keys)[11:length(rt_keys)]
# vt_drops <- which(vertex_attr(rt_g)$clust %in% rt_drops)
# rt_use <- delete_vertices(rt_g, vt_drops)
rt_use <- induced_subgraph(rt_g, which(V(rt_g)$clust %in% names(rt_keys[1:10])))

# Convert to a network object
net <- intergraph::asNetwork(rt_use)
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral"
       )

# Now remove the centrality legend by setting size to false in the guide() function
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral"
       ) + 
    guides( size = FALSE)

# Add edge colors
ggnet2(net, node.size = "cent", node.color = "clust", edge.size = .1, 
       color.legend = "Community Membership", color.palette = "Spectral", 
       edge.color = c("color", "gray88")) +
  guides( size = FALSE)


# NEED TO CREATE rt_g_smaller!
# Basic plot where we set parameters for the plots using geom_edegs() and geom_nodes()
# ggplot(ggnetwork(rt_g_smaller, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
#   geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), curvature = .2, color = "black") + 
#   geom_nodes(size = 4, aes(color = comm)) + 
#   theme_blank()

# Added guide legend, changed line colors, added size 
# ggplot(ggnetwork(rt_g_smaller, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
#   geom_edges(arrow = arrow(length = unit(6, "pt"), type = "closed"), curvature = .2, lwd = .3, aes(color=comm)) +
#   geom_nodes(aes(color = comm, size = cent)) + 
#   theme_blank() +  
#   guides(color = guide_legend(title = "Community"), size = guide_legend(title = "Centrality"))


# NEED TO FIX!
# Add betweenness centrality using betweenness()
V(trip_g_simp)$cent <- igraph::betweenness(trip_g_simp)

# Create a ggplot object with ggnetwork to render using ggiraph
g <- ggplot(ggnetwork(trip_g_simp, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(size = cent)) + 
    theme_blank() 
plot(g)

# Create ggiraph object and assign the tooltip to be interactive
my_gg <- g + ggiraph::geom_point_interactive(aes(tooltip = round(cent, 2), 
                                                 data_id = round(cent, 2)
                                                 ), size = 2
                                             ) 

# Define some hover css so the cursor turns red
hover_css = "cursor:pointer;fill:red;stroke:red;r:3pt"
# ggiraph::ggiraph(code = print(my_gg), hover_css = hover_css, tooltip_offx = 10, tooltip_offy = -10)


# Add community membership as a vertex attribute using the cluster_walktrap algorithm
V(rt_g)$comm <- membership(cluster_walktrap(rt_g))

# Create an induced_subgraph
rt_sub_g <- induced_subgraph(rt_g, which(V(rt_g)$comm %in% 10:13))

# Plot to see what it looks like without an interactive plot using ggnetwork
ggplot(ggnetwork(rt_sub_g, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(color = as.factor(comm))) + 
    theme_blank()  

# Convert to a networkD3 object
# nd3 <- igraph_to_networkD3(rt_sub_g)

# Assign grouping factor as community membership
# nd3$nodes$group = V(rt_sub_g)$comm

# Render your D3.js graph
# forceNetwork(Links = nd3$links, Nodes = nd3$nodes, Source = 'source', 
#              Target = 'target', NodeID = 'name', Group = 'group', legend = T, fontSize = 20
#              )

# Convert  trip_df to hive object using edge2HPD()
# bike_hive <- edge2HPD(edge_df =  as.data.frame(trip_df))

# Assign to trip_df edgecolor using our custom function
# trip_df$edgecolor <- dist_gradient(trip_df$geodist)

# Calculate centrality with betweenness()
# bike_cent <- betweenness(trip_g)

# Add axis and radius based on longitude and radius
# bike_hive$nodes$radius<- ifelse(bike_cent > 0, bike_cent, runif(1000, 0, 3))

# Set axis as integers and axis colors to black
# bike_hive$nodes$axis <- as.integer(dist_stations$axis)
# bike_hive$axis.cols <- rep("black", 3)

# Set the edge colors to a heatmap based on trip_df$edgecolor
# bike_hive$edges$color <- trip_df$edgecolor
# plotHive(bike_hive, method = "norm", bkgnd = "white")


# Add community membership as a vertex attribute
V(rt_g)$comm <- membership(cluster_walktrap(rt_g))

# Create a subgraph
rt_sub_g <- induced_subgraph(rt_g, which(V(rt_g)$comm %in% 10:15))

# Plot to see what it looks like without an interactive plot
ggplot(ggnetwork(rt_sub_g, arrow.gap = .01), aes(x = x, y = y, xend = xend, yend = yend)) + 
    geom_edges(color = "black") + 
    geom_nodes(aes(color = as.factor(comm)))+ theme_blank() +
    theme(legend.position = "none")

# Make a Biofabric plot htmlwidget
# rt_bf <- bioFabric(rt_sub_g)
# bioFabric_htmlwidget(rt_bf)


# Create a dataframe of start and end latitude and longitude and add weights
# ll_to_plot <- bike_dat %>% group_by(from_station_id, to_station_id, from_latitude, 
#                                     from_longitude, to_latitude, to_longitude, usertype
#                                     ) %>% 
#     summarise(weight = n())

# Create a base map with station points with ggmap()
# ggmap(chicago) + 
#     geom_segment(data = ll_to_plot, aes(x = from_longitude, y = from_latitude, 
#                                         xend = to_longitude, yend = to_latitude, 
#                                         colour = usertype, size = weight
#                                         ), alpha = .5
#                )

```
  
  
  
***
  
###_Fundamentals of Bayesian Analysis in R:_  
  
Chapter 1 - What is Bayesian Analysis?  
  
Introduction:  
  
* British team spearheaded by Turing found ways to decrypt German communications in 1941  
	* Key to Turing's success was the use of Bayesian methods, which wree not very widely used  
* Bayesian inference is "A method for figuring out unobservable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be"  
	* The unknown was the configuration of the wheels in the encryption machine - British already knew what a given wheel configuration would produce  
    * Turing worked backwards to figure out the probable configuration of the wheels from the messages that he had received  
* Bayesian analysis is flexible and can be problem-specific and customized to a specific dataset and analysis need  
  
Bayesian data analysis - named for Thomas Bayes from the early-mid 1700s:  
  
* Bayesian data analysis is about probabilistic inference for learning from data and drawing conclusions  
* For this specific course, probability will be a statement about the certainty (p=1 means certain yes, p=0 means certain no), though there are other definitions that can be used  
	* Probability need not be only about yes/no statements, and can be associated to distributions such as "amount of rainfall tomorrow"  
    * "The role of probability distributions in Bayesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has been learned from data."  
* Example of using a Bayesian approach to patients  
	* prop_model(data) is a function that has been created to plot out probabilities vs. p  
    * The data is a vector of successes and failures represented by 1s and 0s  
    * There is an unknown underlying proportion of success  
    * If data point is a success is only affected by this proportion  
    * Prior to seeing any data, any underlying proportion of success is equally likely  
    * The result is a probability distribution that represents what the model knows about the underlying proportion of success  
  
Samples and posterior samples:  
  
* Prior probability distribution is a distribution PRIOR to updating with some data; for example, equally probable that p falls anywhere between 0 and 1  
* Posterior probability distribution is a distribution AFTER updating with what is learned by seeing some data  
* Can be valuable to have a vector of potential outcomes, appropriately weighted by the likelihood of each of the outcomes  
  
Chapter wrap-up:  
  
* Can draw conclusions about the probabilities based on even a small sample of data observed  
* Next chapters will cover mechanics of Bayesian inference in more detail  
  
Example code includes:  
```{r}

prop_model <- function(data = c(), prior_prop = c(1, 1), n_draws = 10000) {
    data <- as.logical(data)
    proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
    data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 20)))

    post_curves <- map_dfr(data_indices, function(i) {
        value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", "Failure"))
        label <- paste0("n=", i)
        probability <- dbeta(proportion_success, prior_prop[1] + sum(data[seq_len(i)]), 
                             prior_prop[2] + sum(!data[seq_len(i)])
                             )
        probability <- probability / max(probability)
        data_frame(value, label, proportion_success, probability)
        }
    )
    post_curves$label <- fct_rev(factor(post_curves$label, levels =  paste0("n=", data_indices )))
    post_curves$value <- factor(post_curves$value, levels = c("Prior", "Success", "Failure"))
  
    p <- ggplot(post_curves, aes(x = proportion_success, y = label, height = probability, fill = value)) +
        ggridges::geom_density_ridges(stat="identity", color = "white", 
                                      alpha = 0.8, panel_scaling = TRUE, size = 1
                                      ) +
        scale_y_discrete("", expand = c(0.01, 0)) +
        scale_x_continuous("Underlying proportion of success") +
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", 
                          drop = FALSE, labels =  c("Prior   ", "Success   ", "Failure   ")
                          ) +
        #ggtitle(paste0("Binomial model - Data: ", sum(data),  " successes, " , sum(!data), " failures"))  +
        theme_light(base_size = 18) +
        theme(legend.position = "top")
    print(p)
  
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data)))
}


# Define data and run prop_model
data = c(1, 0, 0, 1)
prop_model(data)


# Define data and run prop_model
data = c(1, 0, 0, 1)
prop_model(data)


data = c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
posterior <- prop_model(data)
head(posterior)
hist(posterior, breaks = 30, xlim = c(0, 1), col = "palegreen4")

# Get some more information about posterior
median(posterior)
quantile(posterior, c(0.05, 0.95))
sum(posterior > 0.07) / length(posterior)

```
  
  
  
***
  
Chapter 2 - How Does Bayesian Inference Work?  
  
Parts needed for Bayesian inference:  
  
* Bayesian inference requires priors (what is known before seeing data), generative model, and data  
	* The generative model is a formula or computer expression that can generate simulated data based on provided input parameters  
    * For example, could assume that there is a proportion of zombies cured and a number of zombies treated and then simulate data based on these  
  
Using a generative model:  
  
* The binomial distribution function can be very helpful as a generative model for summing probabilities of single 1/0 events  
* Typically in data analysis, we know the outcome and want to figure out the likely parameters in our generative model (that is the Bayesian inference)  
  
Repressing uncertainty with priors:  
  
* The prior reflects our certainty/uncertainty in the parameters prior to running the analysis  
	* Example could be a uniform distribution from (a, b)  
    * proportion_clicks <- runif(n = 6, min = 0.0, max = 1.0)  # sample 6 values that are between 0 and 1 with every number being equally likely  
    * n_clicks <- rbinom(n = 6, size = 100, proportion_clicks)  # rbinom will vectorize over proportion_clicks  
  
Bayesian models and conditioning:  
  
* The Bayesian model is based on the generative model and the prior  
	* prior <- data.frame(proportion_clicks, n_visitors)  # joint PDF over proportion_clicks and n_visitors  
* Can then condition on the observed data and assess the joint PDF for the implications on the distribution of the underlying proportion  
	* "Bayesian inference is conditioning on data, in order to learn about parameter values."  
  
Chapter wrap-up:  
  
* Used the binomial model as an assumed generative function and a uniform distribution as the prior probabilities  
* Calculated joint probability distributions and then found a probability distribution based on a known outcome (data)  
* The posterior can then be used as the prior for future analyses, and can be repeated indefinitely  
* Bayesian machinery from simple cases can be extended to more complex cases  
	* Just need a generative model and an assumption for the prior  
    * Need a computational model that can scale easily  
  
Example code includes:  
```{r}

# Generative zombie drug model
# Parameters
prop_success <- 0.42
n_zombies <- 100
# Simulating data
data <- c()
for(zombie in 1:n_zombies) {
  data[zombie] <- runif(1, min = 0, max = 1) < prop_success
}
data <- as.numeric(data)
data
data_counts <- sum(as.numeric(data))
data_counts


# Try out rbinom
rbinom(n = 1, size = 100, prob = 0.42)

# Try out rbinom
rbinom(n = 200, size = 100, prob = 0.42)


# Fill in the parameters
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n_samples, size = n_ads_shown, prob = proportion_clicks)

# Visualize the results
hist(n_visitors)


# Update proportion_clicks
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n = n_samples, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_samples, size = n_ads_shown, prob = proportion_clicks)

# Visualize the results
hist(n_visitors)
hist(proportion_clicks)


# Create prior
prior <- data.frame(proportion_clicks, n_visitors)
head(prior)

# Create posterior
posterior <- prior[prior$n_visitors==13, ]
hist(posterior$proportion_clicks)


prior <- posterior
head(prior)
prior$n_visitors <- rbinom(nrow(prior), size=100, prob=prior$proportion_clicks)
hist(prior$n_visitors)
mean(prior$n_visitors >= 5)

```
  
  
  
***
  
Chapter 3 - Why Use Bayesian Data Analysis?  
  
Four good things with Bayes:  
  
* Many good tools exist for Bayesian analysis, and those will be covered in Chapter 5  
* The main reasons for Bayesian analysis are the flexibility and power  
	* You can include information sources such as expertise in addition to the data  
    * You can make any comparisons between groups or data sets  
    * You can use the results of Bayesian analysis for Decision Analysis  
    * You can change the underlying statistical model  
* Background information, common knowledge, or expertise can be incorporated in to the prior  
	* Can also exclude all such information by assuming a uniform distribution for the prior  
    * The beta distribution can be useful for a proportion, and is set based on alpha and beta  
  
Contrasts and comparisons:  
  
* The more data, the more likely that the posterior is informed by the data than by the prior  
* There is often a benefit to comparing groups - for example, two different treatments or two different campaigns  
	* Can compare the posterior distributions from the two groups, which is fairly simple since each distribution is contained in a vector  
    * posterior$prop_diff <- posterior$video_prop - posterior$text_prop  
  
Decision analysis:  
  
* Can calculate median, mean, credible interval, likelihood of more extreme than a certain parameter, etc.  
* The results of a Bayesian analysis can be used for Decision analysis  
* Can add dimensions such as cost and revenue, then compare profitability  
	* video_cost <- 0.25  
    * text_cost <- 0.05  
    * visitor_spend <- 2.53  
    * posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost  
    * posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost  
    * posterior$profit_diff <- posterior$video_profit - posterior$text_profit  
  
Change anything and everything:  
  
* There can be large uncertainty as to the outcomes, particularly if the data sizes are small and the key metrics are close to the critical parameter  
* Can switch out the generating functions and re-run the approach - example of a banner that is pay-per-day rather than pay-per-impression  
	* One option is to split each day in to 1440 minutes and assume a probability of success (1/0), which has drawbacks  
    * In the limiting case of smaller and smaller slices, the Poisson distribution is created - has only a single parameter, the expected value of successes  
    * n_clicks <- rpois(n = 100000, labmda = 20)  
  
Bayes is optimal, kind of . . .   
  
* Bayesian analysis can be useful in many ways as shown in this chapter  
* Bayesian analysis is (kind of) optimal, provided that the generative model is perfectly precise and accurate  
	* In the world defined by the model the Bayesian approach is optimal  
    * However, no statistical model can ever be optimal in the real world  
  
Example code includes:  
```{r}

# Draw from the beta distribution
beta_sample <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)

# Explore the results
hist(beta_sample)

# Draw from the beta distribution
beta_sample <- rbeta(n = 10000, shape1 = 100, shape2 = 100)

# Explore the results
hist(beta_sample)

# Draw from the beta distribution
beta_sample <- rbeta(n = 10000, shape1 = 100, shape2 = 20)

# Explore the results
hist(beta_sample)


n_draws <- 100000
n_ads_shown <- 100

# Update proportion_clicks
proportion_clicks <- rbeta(n_draws, shape1 = 5, shape2 = 95)
n_visitors <- rbinom(n_draws, size = n_ads_shown, prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 13, ]

# Plots the prior and the posterior in the same plot
par(mfcol = c(2, 1))
hist(prior$proportion_clicks, 
     xlim = c(0, 0.25))
hist(posterior$proportion_clicks, 
     xlim = c(0, 0.25))
# Reset mfcol below

# Define parameters
n_draws <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_draws, size = n_ads_shown, prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)

# Create posteriors
posterior_video <- prior[prior$n_visitors == 13, ]
posterior_text <- prior[prior$n_visitors == 6, ]

# Visualize posteriors
hist(posterior_video$proportion_clicks, xlim = c(0, 0.25))
hist(posterior_text$proportion_clicks, xlim = c(0, 0.25))


posterior <- data.frame(video_prop = posterior_video$proportion_clicks[1:4000], 
                        text_prop  = posterior_text$proportion_click[1:4000]
                        )
    
# Create prop_diff
posterior$prop_diff <- posterior$video_prop - posterior$text_prop

# Plot your new column
hist(posterior$prop_diff)

# Explore prop_diff
median(posterior$prop_diff)
mean(posterior$prop_diff > 0)


visitor_spend <- 2.53
video_cost <- 0.25
text_cost <- 0.05

posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost
posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost
head(posterior)
hist(posterior$video_profit)
hist(posterior$text_profit)


posterior$profit_diff <- posterior$video_profit - posterior$text_profit
head(posterior)
hist(posterior$profit_diff)
median(posterior$profit_diff)
mean(posterior$profit_diff < 0)


x <- rpois(n = 10000, lambda = 3)
hist(x)

x <- rpois(n = 10000, lambda = 11.5)
hist(x)

x <- rpois(n = 10000, lambda = 11.5)
mean(x >= 15)


n_draws <- 100000
n_ads_shown <- 100
mean_clicks <- runif(n_draws, min = 0, max = 80)
n_visitors <- rpois(n_draws, lambda=mean_clicks)
                     
prior <- data.frame(mean_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 19, ]

hist(prior$mean_clicks)
hist(posterior$mean_clicks)

# Reset to default
par(mfcol = c(1, 1))

```
  
  
  
***
  
Chapter 4 - Bayesian Inference with Bayes' Theorem  
  
Probability rules:  
  
* The computation method used so far does not scale well, but there are alternatives  
* Bayesian statistics is a hot research error, and there are many methods to get to the same results in a faster method  
* Probability is defined as a statement of certainty/uncertainty between 0 and 1 (may be a distribution or an allocation of probabilities over all possible values)  
* Conditional probabilities are often of interest - P(A | B) is the probability of A given that B has occurred  
	* Can also get a conditional probability distribution  
* Sometimes probabilities can be summed - when they are exclusive and the goal is to get 1 of them  
* Sometimes probabilities can be multiplied - when they are independent and the goal is to get all of them  
  
Calculating likelihoods:  
  
* Can simulate or calculate probabilities - simulate and count with a common generative model and small dataset, or calculate using an optimized formula  
* For common distributions, can use the density functions such as dbinom() to get the key probabilities in many cases  
	* dbinom(13, size = 100, prob = 0.1) + dbinom(14, size = 100, prob = 0.1)  # probability of getting 13 or 14 successes in 100 trials with success 0.1 per trial  
    * n_visitors = seq(0, 100, by = 1)  
    * probability <- dbinom(n_visitors, size = 100, prob = 0.1)  
* For continuous distributions such as the uniform, there is no specific probability of any given value  
	* Probability densities are returned instead, and can be viewed as the relative probabilities  
  
Bayesian calculation:  
  
* Conditioning on the observed data is at the core of Bayesian inference  
* Example of converting previous simulations to precise calculations  
	* n_ads_shown <- 100  
    * n_visitors <- seq(0, 100, by = 1)  # full potential range of visitors based on ads  
    * proportion_clicks <- seq(0, 1, by = 0.01)  # fine grid of values even if it does not fully sample the given space  
    * pars <- expand.grid(proportion_clicks = proportion_clicks, n_visitors = n_visitors)  # all possible combinations of n_visitors and proportion_clicks  
    * pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)  # prior is of uniform density  
    * pars$likelihood <- dbinom(pars$n_visitors, size = n_ads_shown, prob = pars$proportion_clicks)  # likelihood given the rows in pars  
    * pars$probability <- pars$likelihood * pars$prior  # unscaled posterior probability  
    * pars$probability <- pars$probability / sum(pars$probability)  # normalized posterior probability  
    * pars <- pars[pars$n_visitors == 13, ]  # filter on the 13 observed clicks  
    * pars$probability <- pars$probability / sum(pars$probability)  # normalize remaining probs to 1  
  
Bayes theorem:  
  
* An example of Bayes' theorem is provided above by the multiplication of the prior and the probability  
	* pars$probability <- pars$likelihood * pars$prior  
    * pars$probability <- pars$probability / sum(pars$probability)  
    * This is an example of p(Param | Data) = P(Data | Param) * P(Param Before Seeing Data) / sum-of-all-numerators  
* The grid approximation technique was used above - cannot ever get all parameters for a continuous distribution  
* Can use mathematical notations where = is a point parameter and ~ is follows a specific distribution  
  
Example code includes:  
```{r}

prob_to_draw_ace <- 4 / 52
prob_to_draw_four_aces <- (4 / 52) * (3 / 51) * (2 / 50) * (1 / 49)


n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n = 99999, 
    size = n_ads_shown, prob = proportion_clicks)
prob_13_visitors <- sum(n_visitors == 13) / length(n_visitors)
prob_13_visitors

prob_13_visitors <- dbinom(x=13, size=n_ads_shown, prob=proportion_clicks)
prob_13_visitors


n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- 0:n_ads_shown
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
plot(x=n_visitors, y=prob, type="h")

n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 13
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)
prob
plot(x=proportion_clicks, y=prob, type="h")


n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- seq(0, 100, by = 1)
pars <- expand.grid(proportion_clicks = proportion_clicks,
                    n_visitors = n_visitors)
pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)
pars$likelihood <- dbinom(pars$n_visitors, 
    size = n_ads_shown, prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability)
pars_conditioned <- pars[pars$n_visitors==6, ]
pars_conditioned$probability <- pars_conditioned$probability / sum(pars_conditioned$probability)
plot(x=pars_conditioned$proportion_clicks, y=pars_conditioned$probability, type="h")


# Simplify slightly for a known result of 6
n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 6
pars <- expand.grid(proportion_clicks = proportion_clicks,
                    n_visitors = n_visitors)
pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)
pars$likelihood <- dbinom(pars$n_visitors, 
    size = n_ads_shown, prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability)
plot(pars$proportion_clicks, pars$probability, type = "h")

```
  
  
  
***
  
Chapter 5 - More Parameters, Data, and Bayes  
  
Temperature in a normal lake:  
  
* Example of having some water data temperature for a given day  
	* temp <- c(19, 23, 20, 17, 23)  
    * The normal distribution could be a good candidate for the generative function in this case - parameters with mu and sigma  
    * rnorm(n = , mean = , sd = )  
    * dnorm(x = , mean = , sd = )  
    * like <- dnorm(x = temp, mean = 20, sd = 2)  # likelihood of our observed temperatures given a mean of 20 and a standard deviation of 2  
    * prod(like)  # joint likelihood of probabilities  
    * log(like)  # addresses the problem of likelihoods being so small that computer precision becomes an issue  
  
Bayesian model of water temperature:  
  
* Can define priors such as sigma ~Uniform(0, 10) and mean ~ N(18, 5)  
* Can then run an additional grid-approximation exercise  
	* temp <- c(19, 23, 20, 17, 23)  
    * mu <- seq(8, 30, by = 0.5)  
    * sigma <- seq(0.1, 10, by = 0.3)  
    * pars <- expand.grid(mu = mu, sigma = sigma))  
    * pars$mu_prior <- dnorm(pars$mu, mean = 18, sd = 5)  
    * pars$sigma_prior <- dunif(pars$sigma, min = 0, max = 10)  
    * pars$prior <- pars$mu_prior * pars$sigma_prior  
    * for(i in 1:nrow(pars)) {  
    *     likelihoods <- dnorm(temp, pars$mu[i], pars$sigma[i])  
    *     pars$likelihood[i] <- prod(likelihoods)  
    * }  
    * pars$probability <- pars$likelihood * pars$prior  
    * pars$probability <- pars$probability / sum(pars$probability)  
  
Beach party implications of water temperatures:  
  
* Suppose that there is aminimum water temperature for holding a beach party and that we want the probability of exceedence of this temperature  
* It is helpful to create a frame for further analysis; random sampling, weighted by probability, can help  
	* sample_indices <- sample( 1:nrow(pars), size = 10000, replace = TRUE, prob = pars$probability)  # draw some random samples proportional to each row's probability  
    * pars_sample <- pars[sample_indices, c("mu", "sigma")]  
    * hist(pars_sample$mu, 30)  
    * pred_temp <- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma)  
    * hist(pred_temp, 30)  
    * sum(pred_temp >= 18) / length(pred_temp )  
  
Practical tool (BEST):  
  
* Models are often available off-the-shelf which can save time; one example is BEST by John Kruschke  
	* BEST assumes that data come from a t-distribution (more or less a normal distribution with the degrees of freedom added)  
    * BEST estimates standard deviation, mean, and degrees of freedom  
    * BEST uses MCMC (Markov chain Monte Carlo)  
* Can fit the data directly using BEST  
	* library(BEST)  
    * iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)  
    * fit <- BESTmcmc(iq)  
    * fit  # note that nu is the degrees of freedom  
    * plot(fit)  
  
Wrap and up and next steps:  
  
* Bayesian inference as a technique for modeling uncertainty - data, generative model, and prior probability distributions  
* Can use sampling and grid approximation to calculate probabilities and distributions  
	* Under-the-hood, used MCMC as implemented by way of BEST  
* Additional areas for exploration include full application of Bayesian approaches to time series, deep learning, and the like  
* Can also add more advanced computational models  
  
Example code includes:  
```{r}

mu <- 3500
sigma <- 600

weight_distr <- rnorm(n = 100000, mean = mu, sd = sigma)
hist(weight_distr, xlim = c(0, 6000), col = "lightgreen")


mu <- 3500
sigma <- 600

weight <- seq(0, 6000, by=100)
likelihood <- dnorm(weight, mean=mu, sd=sigma)

plot(x=weight, y=likelihood, type="h")


# The IQ of a bunch of zombies
iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
# Defining the parameter grid
pars <- expand.grid(mu = seq(0, 150, length.out = 100), 
                    sigma = seq(0.1, 50, length.out = 100))
# Defining and calculating the prior density for each parameter combination
pars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 100)
pars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)
pars$prior <- pars$mu_prior * pars$sigma_prior
# Calculating the likelihood for each parameter combination
for(i in 1:nrow(pars)) {
  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])
  pars$likelihood[i] <- prod(likelihoods)
}
# Calculating the probability of each parameter combination
pars$probability <- pars$likelihood * pars$prior / sum(pars$likelihood * pars$prior)
lattice::levelplot(probability ~ mu * sigma, data = pars)


head(pars)
sample_indices <- sample( nrow(pars), size = 10000,
    replace = TRUE, prob = pars$probability)
head(sample_indices)
pars_sample <- pars[sample_indices, c("mu", "sigma")]
hist(pars_sample$mu)
quantile(pars_sample$mu, c(0.025, 0.5, 0.975))


head(pars_sample)
pred_iq <- rnorm(10000, mean = pars_sample$mu, sd = pars_sample$sigma)
hist(pred_iq)
mean(pred_iq >= 60)


# The IQ of zombies on a regular diet and a brain based diet.
iq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)
iq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
mean(iq_brains) - mean(iq_regular)

# Need to load http://www.sourceforge.net/projects/mcmc-jags/files for rjags (called by BEST)
# library(BEST)
# best_posterior <- BESTmcmc(iq_brains, iq_regular)
# plot(best_posterior)

```
  
  
  
***
  
###_Categorical Data in the Tidyverse_  
  
Chapter 1 - Introduction to Factor Variables  
  
Introduction to qualitative variables:  
  
* Identifying and inspecting categorical data, using the forcats package, effective visualization  
* Qualitative data in this course will include categorical data and ordianl data  
	* Each have a fixed and known set of possible values - ordinal adds that there is an ordering, though no specific meaning such that 1 vs 2 and 2 vs 3 may be different distances  
* Categorical data is generally best converted to factors iff there is a finite set of potential values  
* Can check for factors using is.factor()  
  
Understanding your qualitative variables:  
  
* Data is from the Kaggle 2017 data science survey  
* High-level summaries include category names and levels; converting to factors can be valuable  
	* multipleChoiceResponses %>% mutate_if(is.character, as.factor)  # nice function!  Run the mutate only if the first condition (is.character) holds  
    * nlevels(multipleChoiceResponses$LearningDataScienceTime)  # number of levels  
    * levels(multipleChoiceResponses$LearningDataScienceTime)  # level names  
    * multipleChoiceResponses %>% summarise_if(is.factor, nlevels)  # run summarize only for the factors  
  
Making better plots:  
  
* Can use forcats::fct_reorder() to reorder data for plotting - fct_reorder(factor, orderingCriteria)  
	* ggplot(WorkChallenges) + geom_point(aes(x = fct_reorder(question, perc_problem), y = perc_problem))  
* Can use forcats::fct_infreq() to order based on frequency and reverse the order using forcats::fct_rev() as needed  
	* ggplot(multiple_choice_responses) + geom_bar(aes(x = fct_infreq(CurrentJobTitleSelect))  
    * ggplot(multiple_choice_responses) + geom_bar(aes(x = fct_rev(fct_infreq(CurrentJobTitleSelect))))  
  
Example code includes:  
```{r}

multiple_choice_answers <- readr::read_csv("./RInputFiles/smc_with_js.csv")

# Print out the dataset
glimpse(multiple_choice_answers)

# Check if CurrentJobTitleSelect is a factor
is.factor(multiple_choice_answers$CurrentJobTitleSelect)


# mutate() and summarise() in dplyr both have variants where you can add the suffix if, all, or at to change the operation
# mutate_if() applies a function to all columns where the first argument is true
# mutate_all() applies a function to all columns
# mutate_at() affects columns selected with a character vector or select helpers (e.g. mutate_at(c("height", "weight"), log))

# Change all the character columns to factors
responses_as_factors <- multiple_choice_answers %>%
    mutate_if(is.character, as.factor)

# Make a two column dataset with variable names and number of levels
number_of_levels <- responses_as_factors %>%
    summarise_all(nlevels) %>%
    gather(variable, num_levels)


# dplyr has two other functions that can come in handy when exploring a dataset
# The first is top_n(x, var), which gets us the first x rows of a dataset based on the value of var
# The other is pull(), which allows us to extract a column and take out the name, leaving only the value(s) from the column

# Select the 4 rows with the highest number of levels
number_of_levels %>%
    top_n(4, num_levels)
    
# How many levels does CurrentJobTitleSelect have? 
number_of_levels %>%
    filter(variable=="CurrentJobTitleSelect") %>%
    pull(num_levels)

# Get the names of the levels of CurrentJobTitleSelect
responses_as_factors %>%
    pull(CurrentJobTitleSelect) %>%
    levels()


# Make a bar plot
ggplot(multiple_choice_answers, aes(x=FormalEducation)) + 
    geom_bar() + 
    coord_flip()

# Make a bar plot
ggplot(multiple_choice_answers, aes(x=fct_rev(fct_infreq(FormalEducation)))) + 
    geom_bar() + 
    coord_flip()


multiple_choice_answers %>%
  filter(!is.na(Age) & !is.na(FormalEducation)) %>%
  group_by(FormalEducation) %>%
  summarize(mean_age = mean(Age)) %>%
  ggplot(aes(x = fct_reorder(FormalEducation, mean_age), y = mean_age)) + 
    geom_point() + 
    coord_flip()

```
  
  
  
***
  
Chapter 2 - Manipulating Factor Variables  
  
Reordering factors:  
  
* Can order by frequency or by another variable for pure categorical variables  
* For ordinal variables, typically is best to order by the implied order inside the ordinal variable - can manually enter using fct_relevel()  
	* ggplot(aes(nlp_frequency, x = fct_relevel(response, "Rarely", "Sometimes", "Often", "Most of the time"))) + geom_bar()  
    * nlp_frequency %>% pull(response) %>% levels()  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time")) %>% pull(response) %>% levels()  # This moves Often and Most of the time to the front, leaving others alone  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time", after = 2)) %>% pull(response) %>% levels()  # move these to after 2  
    * nlp_frequency %>% mutate(response = fct_relevel(response, "Often", "Most of the time", after = Inf) %>% pull(response) %>% levels()  # move this to the end  
  
Renaming factor levels:  
  
* Can convert names for factor levels using forcats::fct_recode()  
	* levels(flying_etiquette$middle_arm_rest_three)  # get the initial levels  
    * ggplot(flying_etiquette, aes(x = fct_infreq(middle_arm_rest_three))) + geom_bar() + coord_flip() + labs(x = "Arm rest opinions")  # labels are very wordy, graph is compressed  
    * flying_etiquette %>% mutate(middle_arm_rest_three = fct_recode(middle_arm_rest_three,   
    *     "Other" = "Other (please specify)", "Everyone should share" = "The arm rests should be shared",  
    *     "Aisle and window people" = "The people in the aisle and window seats get both arm rests",   
    *     "Middle person" = "The person in the middle seat gets both arm rests",  
    *     "Fastest person" = "Whoever puts their arm on the arm rest first")  
    * ) %>%   
    * count(middle_arm_rest_three)  
  
Collapsing factor levels:  
  
* Can collapse factor levels using forcats::fct_collapse()  
	* flying_etiquette %>% mutate(height = fct_collapse(height, under_5_3 = c("Under 5 ft.", "5'0\"", "5'1\"", "5'2\""), over_6_1 = c("6'1\"", "6'2\"", "6'3\"", "6'4\"", "6'5\"", "6'6\" and above"))) %>% pull(height) %>% levels()  
* Can collapse factor levels to other using forcats::fct_other()  
	* flying_etiquette %>% mutate(new_height = fct_other(height, keep = c("6'4\"", "5'1\""))) %>% count(new_height)  # will make everything other than keep in to Other  
    * flying_etiquette %>% mutate(new_height = fct_other(height, drop = c("Under 5 ft.", "5'0\"", "5'1\"", "5'2\"", "5'3\""))) %>% pull(new_height) %>% levels()  # will move the drop items to Other  
    * flying_etiquette %>% mutate(new_height = fct_lump(height, prop = .08)) %>% count(new_height)  # anything less than a proportion of 0.08 will be moved to other  
    * flying_etiquette %>% mutate(new_height = fct_lump(height, n = 3)) %>% count(new_height)  # keep the top 3 categories  
  
Example code includes:  
```{r}

multiple_choice_responses <- multiple_choice_answers

# Print the levels of WorkInternalVsExternalTools
levels(multiple_choice_responses$WorkInternalVsExternalTools)

# Reorder the levels from internal to external 
mc_responses_reordered <- multiple_choice_responses %>%
    mutate(WorkInternalVsExternalTools = fct_relevel(WorkInternalVsExternalTools, 
                                                     c('Entirely internal', 'More internal than external',
                                                       'Approximately half internal and half external', 
                                                       'More external than internal', 'Entirely external',
                                                       'Do not know'
                                                       )
                                                     )
           )

# Make a bar plot of the responses
ggplot(mc_responses_reordered, aes(x=WorkInternalVsExternalTools)) + 
    geom_bar() + 
    coord_flip()


multiple_choice_responses %>%
    # Move "I did not complete any formal education past high school" and "Some college/university study without earning a bachelor's degree" to the front
    mutate(FormalEducation = fct_relevel(FormalEducation, c("I did not complete any formal education past high school", "Some college/university study without earning a bachelor's degree"))) %>%
    # Move "Doctoral degree" to be the sixth level
    mutate(FormalEducation = fct_relevel(FormalEducation, after=6, "Doctoral degree")) %>%
    # Move "I prefer not to answer" to be the last level.
    mutate(FormalEducation = fct_relevel(FormalEducation, after=Inf, "I prefer not to answer")) %>%
    # Examine the new level order
    pull(FormalEducation) %>%
    levels()


# make a bar plot of the frequency of FormalEducation
ggplot(multiple_choice_responses, aes(x=FormalEducation)) + 
    geom_bar()


multiple_choice_responses %>%
    # rename levels
    mutate(FormalEducation = fct_recode(FormalEducation, "High school" ="I did not complete any formal education past high school", "Some college" = "Some college/university study without earning a bachelor's degree")) %>%
    # make a bar plot of FormalEducation
    ggplot(aes(x=FormalEducation)) + 
    geom_bar()


multiple_choice_responses %>%
    # Create new variable, grouped_titles, by collapsing levels in CurrentJobTitleSelect
    mutate(grouped_titles = fct_collapse(CurrentJobTitleSelect, 
        "Computer Scientist" = c("Programmer", "Software Developer/Software Engineer"), 
        "Researcher" = "Scientist/Researcher", 
        "Data Analyst/Scientist/Engineer" = c("DBA/Database Engineer", "Data Scientist", 
                                              "Business Analyst", "Data Analyst", 
                                              "Data Miner", "Predictive Modeler"))) %>%
    # Turn every title that isn't now one of the grouped_titles into "Other"
    mutate(grouped_titles = fct_other(grouped_titles, 
                             keep = c("Computer Scientist", 
                                     "Researcher", 
                                     "Data Analyst/Scientist/Engineer"))) %>% 
    # Get a count of the grouped titles
    count(grouped_titles)


multiple_choice_responses %>%
  # remove NAs of MLMethodNextYearSelect
  filter(!is.na(MLMethodNextYearSelect)) %>%
  # create ml_method, which lumps all those with less than 5% of people into "Other"
  mutate(ml_method = fct_lump(MLMethodNextYearSelect, prop=0.05)) %>%
  # print the frequency of your new variable in descending order
  count(ml_method, sort=TRUE)


multiple_choice_responses %>%
  # remove NAs 
  filter(!is.na(MLMethodNextYearSelect)) %>%
  # create ml_method, retaining the 5 most common methods and renaming others "other method" 
  mutate(ml_method = fct_lump(MLMethodNextYearSelect, 5, other_level="other method")) %>%
  # print the frequency of your new variable in descending order
  count(ml_method, sort=TRUE)

```
  
  
  
***
  
Chapter 3 - Creating Factor Variables  
  
Examining common themed variables:  
  
* Tidy data has each row as an observation and each column as a variable (generally, moving from wide to long)  
	* multipleChoiceResponses %>% select(contains("WorkChallengeFrequency")) %>% gather(work_challenge, frequency)  
    * work_challenges <- multipleChoiceResponses %>% select(contains("WorkChallengeFrequency")) %>% gather(work_challenge, frequency) %>%  
    *     mutate(work_challenge = str_remove(work_challenge, "WorkChallengeFrequency"))  # will remove the string "WorkChallengeFrequency" from column work_challenge  
* Can also convert the variables to 0/1 and then use for statistical summaries  
	* work_challenges %>% filter(!is.na(frequency)) %>% mutate(frequency = if_else( frequency %in% c("Most of the time", "Often"), 1, 0) ) %>%  
    * group_by(work_challenge) %>% summarise(perc_problem = mean(frequency))  
  
Tricks of ggplot2:  
  
* Initial plots may not look so good, for example  
	* ggplot(job_titles_by_perc, aes(x = CurrentJobTitleSelect,, y = perc_w_title)) + geom_point()  
* Can angle the tick axes for better readability  
	* ggplot(job_titles_by_perc, aes(x = CurrentJobTitleSelect, y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
* Can reorder by oreder of popularity using fct_reorder  
	* ggplot(job_titles_by_perc, aes(x = fct_reorder(CurrentJobTitleSelect, perc_w_title), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
    * ggplot(job_titles_by_perc, aes(x = fct_rev(fct_reorder(CurrentJobTitleSelect, perc_w_title)), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
* Can add axis labels and titles using labs()  
	* g <- ggplot(job_titles_by_perc, aes(x = fct_rev(fct_reorder(CurrentJobTitleSelect, perc_w_title)), y = perc_w_title)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))  
    * g <- g + labs(x = "Job Title", y = "Percent with title")  
* Can change the y-axis ticks to be explicit percentages  
	* g + scale_y_continuous(labels = scales::percent_format())  
  
Changing and creating variables with dplyr::case_when():  
  
* Suppose that there is a simple vector to be recoded using simple business rules  
	* x <- 1:20  
    * case_when(x %% 15 == 0 ~ "fizz buzz", x %% 3 == 0 ~ "fizz", x %% 5 == 0 ~ "buzz", TRUE ~ as.character(x) )  # TRUE is the all others  
* Conditions evaluate from first to last, and the first matching condition is acted on for that value  
	* moods %>% mutate(action = case_when( mood == "happy" & status == "know it" ~ "clap your hands", mood == "happy" & status == "do not know it" ~ "stomp your feet", mood == "sad" ~ "look at puppies", TRUE ~ "jump around")  
  
Example code includes:  
```{r}

learning_platform_usefulness <- multiple_choice_responses %>%
  # select columns with LearningPlatformUsefulness in title
  select(contains("LearningPlatformUsefulness")) %>%
  # change data from wide to long
  gather(learning_platform, usefulness) %>%
  # remove rows where usefulness is NA
  filter(!is.na(usefulness)) %>%
  # remove "LearningPlatformUsefulness" from each string in `learning_platform 
  mutate(learning_platform = str_remove(learning_platform, "LearningPlatformUsefulness"))


learning_platform_usefulness %>%
  # change dataset to one row per learning_platform usefulness pair with number of entries for each
  count(learning_platform, usefulness) %>%
  # use add_count to create column with total number of answers for that learning_platform
  add_count(learning_platform, wt=n) %>%
  # create a line graph for each question with usefulness on x-axis and percentage of responses on y
  ggplot(aes(x = usefulness, y = n/nn, group = learning_platform)) + 
  geom_line() + 
  facet_wrap(~ learning_platform)


avg_usefulness <- learning_platform_usefulness %>%
    # If usefulness is "Not Useful", make 0, else 1 
    mutate(usefulness = ifelse(usefulness=="Not Useful", 0, 1)) %>%
    # Get the average usefulness by learning platform 
    group_by(learning_platform) %>%
    summarize(avg_usefulness = mean(usefulness))

# Make a scatter plot of average usefulness by learning platform 
ggplot(avg_usefulness, aes(x=learning_platform, y=avg_usefulness)) + 
    geom_point()

ggplot(avg_usefulness, aes(x = learning_platform, y = avg_usefulness)) + 
    geom_point() + 
    # rotate x-axis text by 90 degrees
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    # rename y and x axis labels
    labs(x="Learning Platform", y="Percent finding at least somewhat useful") + 
    # change y axis scale to percentage
    scale_y_continuous(labels = scales::percent)

ggplot(avg_usefulness, 
       aes(x = fct_rev(fct_reorder(learning_platform, avg_usefulness)), y = avg_usefulness)
       ) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    labs(x = "Learning Platform", y = "Percent finding at least somewhat useful") + 
    scale_y_continuous(labels = scales::percent)


# Check the min age
min(multiple_choice_responses$Age, na.rm=TRUE)
# Check the max age
max(multiple_choice_responses$Age, na.rm=TRUE)
sum(is.na(multiple_choice_responses$Age))


multiple_choice_responses %>%
    # Eliminate any ages below 10 and above 90
    filter(between(Age, 10, 90)) %>%
    # Create the generation variable based on age
    mutate(generation=case_when(
      between(Age, 10, 22) ~ "Gen Z", 
      between(Age, 23, 37) ~ "Gen Y", 
      between(Age, 38, 52) ~ "Gen X", 
      between(Age, 53, 71) ~ "Baby Boomer", 
      between(Age, 72, 90) ~ "Silent"
    )) %>%
    # Get a count of how many answers in each generation
    count(generation)


multiple_choice_responses %>%
    # Filter out people who selected Data Scientist as their Job Title
    filter(!is.na(CurrentJobTitleSelect) & CurrentJobTitleSelect != "Data Scientist")  %>%
    # Create a new variable, job_identity
    mutate(job_identity = case_when(
        CurrentJobTitleSelect == "Data Analyst" & DataScienceIdentitySelect == "Yes" ~ "DS analysts", 
        CurrentJobTitleSelect == "Data Analyst" & DataScienceIdentitySelect %in% c("No", "Sort of (Explain more)") ~ "NDS analyst", 
        CurrentJobTitleSelect != "Data Analyst" & DataScienceIdentitySelect == "Yes" ~ "DS non-analysts", 
        TRUE ~ "NDS non analysts")
        ) %>%
    mutate(JobSat=case_when(
        is.na(JobSatisfaction) ~ NA_integer_,
        JobSatisfaction == "I prefer not to share" | JobSatisfaction == "NA" ~ NA_integer_, 
        JobSatisfaction == "1 - Highly Dissatisfied" ~ 1L, 
        JobSatisfaction == "10 - Highly Satisfied" ~ 10L, 
        TRUE ~ as.integer(JobSatisfaction))) %>%
    # Get the average job satisfaction by job_identity, removing NAs
    group_by(job_identity) %>%
    summarize(avg_js = mean(JobSat, na.rm=TRUE))

```
  
  
  
***
  
Chapter 4 - Case Study on Flight Etiquette  
  
Case study introduction:  
  
* Recreation of 538 dataset on flying etiquette  
* Need to begin by converting variable types and tidying the data and selecting key columns  
	* wide_data %>% mutate_if(is.character, as.factor)  
    * wide_data %>% gather(column, value)  
    * wide_data %>% select(contains("favorite"))  
  
Data preparation and regex:  
  
* Names will need to be changed to something more succinct for plotting  
	* gathered_data %>% distinct(response_var)  
* Regular expressions can be used in any computing language to find instances of general patterns  
	* str_detect("happy", ".")  # the . Will match anything  
    * str_detect("happy", "h.")  # TRUE  
    * str_detect("happy", "y.")  # FALSE, since nothing follows the y  
    * str_remove(string, ".*the ")  # will remove everything up to and including the followed by a space  
  
Recreating the plot:  
  
* The labs() command allows for both a caption and a subtitle  
	* ggplot(mtcars, aes(disp, mpg)) + geom_point() + labs(x = "x axis label", y = "y axis label", title = "My title", subtitle = "and a subtitle", caption = "even a caption!")  
* The geom_text() layer allows for adding the specific numbers to the plot  
	* initial_plot + geom_text(aes(label = round(mean_mpg)))  
    * initial_plot + geom_text(aes(label = round(mean_mpg), y = mean_mpg + 2))  # fix the issue where the text is on top of the tip of the bar  
* Can use the theme layer to modify the non-data layers of the plot  
	* initial_plot + geom_text(aes(label = round(mean_mpg), y = mean_mpg + 2)) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())  # get rid of x and y ticks  
  
End of course recap:  
  
* Basic forcats functions  
* Tidyverse functions  
* ggplot2 tricks  
* Recreating the 538 plot for airplane rudeness behaviors  
  
Example code includes:  
```{r}

flying_etiquette <- read.csv("./RInputFiles/flying-etiquette.csv", stringsAsFactors = FALSE)
names(flying_etiquette) <- 
    stringr::str_replace_all(stringr::str_replace_all(names(flying_etiquette), "\\.", " "), "  ", " ")
names(flying_etiquette) <- stringr::str_trim(names(flying_etiquette))
names(flying_etiquette)[2:22] <- paste0(names(flying_etiquette)[2:22], "?")
names(flying_etiquette) <- stringr::str_replace_all(names(flying_etiquette), "itrude", "it rude")
glimpse(flying_etiquette)


gathered_data <- flying_etiquette %>%
    mutate_if(is.character, as.factor) %>%
    filter(`How often do you travel by plane?` != "Never") %>%
    # Select columns containing "rude"
    select(contains("rude")) %>%
    # Change format from wide to long
    gather(response_var, value)


rude_behaviors <- gathered_data %>%
    mutate(response_var = str_replace(response_var, '.*rude to ', '')) %>%
    mutate(response_var = str_replace(response_var, 'on a plane', '')) %>%
    mutate(rude = if_else(value %in% c("No, not rude at all", "No, not at all rude"), 0, 1)) %>%
    # Create perc_rude, the percent considering each behavior rude
    group_by(response_var) %>%
    summarize(perc_rude=mean(rude))

rude_behaviors


# Create an ordered by plot of behavior by percentage considering it rude
initial_plot <- ggplot(rude_behaviors, aes(x=fct_reorder(response_var, perc_rude), y=perc_rude)) +
geom_col()

# View your plot
initial_plot


titled_plot <- initial_plot + 
    # Add the title, subtitle, and caption
    labs(title = "Hell Is Other People In A Pressurized Metal Tube",
         subtitle = "Percentage of 874 air-passenger respondents who said action is very or somewhat rude",
         caption = "Source: SurveyMonkey Audience", 
         # Remove the x- and y-axis labels
         x="",
         y=""
         ) 

titled_plot


flipped_plot <- titled_plot + 
    # Flip the axes
    coord_flip() + 
    # Remove the x-axis ticks and labels
    theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank())

flipped_plot + 
    # Add labels above the bar with the perc value
    geom_text(aes(label = paste0(round(100*perc_rude), "%"), y = perc_rude + .03), 
              position = position_dodge(0.9), vjust = 1)

```
  
  
  
***
  
###_Bayesian Modeling with RJAGS_  
  
Chapter 1 - Introduction to Bayesian Modeling  
  
Prior model:  
  
* Goals include foundational Bayesian models such as Beta-Binomial, Normal-Normal, and Bayesian regression  
	* Define, compile, simulate using RJAGS  
    * Conduct Bayesian posterior inference using RJAGS  
* Example of election poll - there is some uncertainty around the polling figures that have been released  
	* An additional poll might tend to update the prior model that was built using the previous elections data  
    * Bayesian posterior models are a powerful means of combining priors and data  
* The prior model depends on some specific assumptions and notations  
	* Suppose that p is the percentage of people who support you - assumed to be a random variable between 0 and 1  
    * The prior model for p is p ~ Beta(45, 55)  
    * The beta model can be tuned from pessimism Beta(1, 5) to complete uncertainty Beta(1, 1)  
  
Data and likelihood:  
  
* Suppose that a candidate running for election does a small poll and finds 6 of 10 plan to vote for them  
	* Can integrate even a small data sample like this with assumptions and priors  
    * Assumptions might include that voters are independent and p is a global probability that any given voter supports you  
    * Then, X ~ Bin(n, p) which is to say that X can be defined as the number of n polled voters that support you (assuming per above global probability p)  
* There is a dependence between X, n, and p, thus the data can help to assess how likely each of the values of p may be given that you observed X in n  
	* The likelihood function is the likelihood of observing X given that p takes on a specific value  
  
Posterior model:  
  
* The prior and the likelihood can be integrated to form the posterior  
	* The prior is knowledge that exists before data and the likelihood is what exists based on the data  
    * Multiply prior and likelihood and then scale so probabilities add to 1  
* The RJAGS package combines R with JAGS (Just Another Gibbs Sampler) - requires downloading JAGS and then loading rjags  
* Can define the model within RJAGS, for example  
	* vote_model <- "model{  
    *     # Likelihood model for X
    *     X ~ dbin(p, n)  # order of n and p are reversed (known difference for RJAGS)
    *     # Prior model for p
    *     p ~ dbeta(a, b)
    * }"  
    * vote_jags_A <- jags.model(textConnection(vote_model), data = list(a = 45, b = 55, X = 6, n = 10), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))  
    * vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)  # variable.names is the variable of interest, which is p in this case  
    * plot(vote_sim, trace = FALSE)  
  
Example code includes:  
```{r eval=FALSE}

# Make sure you have installed JAGS-4.x.y.exe (for any x >=0, y>=0) from http://www.sourceforge.net/projects/mcmc-jags/files

# Sample 10000 draws from Beta(45,55) prior
prior_A <- rbeta(n = 10000, shape1 = 45, shape2 = 55)

# Store the results in a data frame
prior_sim <- data.frame(prior_A)

# Construct a density plot of the prior sample
ggplot(prior_sim, aes(x = prior_A)) + 
    geom_density()    


# Sample 10000 draws from the Beta(1,1) prior
prior_B <- rbeta(n = 10000, shape1 = 1, shape2 = 1)    

# Sample 10000 draws from the Beta(100,100) prior
prior_C <- rbeta(n = 10000, shape1 = 100, shape2 = 100)

# Combine the results in a single data frame
prior_sim <- data.frame(samples = c(prior_A, prior_B, prior_C),
        priors = rep(c("A","B","C"), each = 10000))

# Plot the 3 priors
ggplot(prior_sim, aes(x = samples, fill = priors)) + 
    geom_density(alpha = 0.5)


# Define a vector of 1000 p values    
p_grid <- seq(0, 1, length.out=1000)

# Simulate 1 poll result for each p in p_grid   
poll_result <- rbinom(1000, 10, prob=p_grid)

# Create likelihood_sim data frame
likelihood_sim <- data.frame(p_grid, poll_result)    

# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result)) + 
    ggridges::geom_density_ridges()


# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result, fill = poll_result==6)) + 
    ggridges::geom_density_ridges()

# Keep the polls with X = 6    
likelihood_sim_6 <- likelihood_sim %>%     
    filter(poll_result==6)    

# Construct a density plot of the remaining p_grid values
ggplot(likelihood_sim_6, aes(x = p_grid)) + 
    geom_density() + 
    lims(x = c(0,1))


# DEFINE the model
vote_model <- "model{
    # Likelihood model for X
    X ~ dbin(p, n)
    
    # Prior model for p
    p ~ dbeta(a, b)
}"

# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE)


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))


# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)

# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))

```
  
  
  
***
  
Chapter 2 - Bayesian Models and Markov Chains  
  
Normal-Normal Model:  
  
* Example of reaction times in a sleep deprivation study  
	* Y(i) ~ N(m, s**2) meaning change in reaction time for subject I, assumed to be normally distributed as N(m, s**2)  
    * Prior information is that normal reaction time is 250 and is expected to increase by 0-150 after sleep deprivation (scale of the prior)  
    * The prior for the mean might then be defined as 50 ms increase in reaction time with a standard deviation of 25  
    * The prior for the standard deviation might then be uniform on 0-200  
* Overall formulation of the sleep study model includes  
	* Y(i) ~ N(m, s**2)  
    * m ~ N(50, 25**2)  
    * s ~ Unif(0, 200)  
  
Simulating Normal-Normal in RJAGS:  
  
* Posterior insights are based on the product of the prior and the data, which can be simulated using RJAGS  
	* sleep_model <- "model{  
    *     # Likelihood model for Y[i]
    *     for(i in 1:length(Y)) {
    *         Y[i] ~ dnorm(m, s^(-2))  # requires precision which can be defined as the inverse of sigma-squared  
    *     }  
    *     # Prior models for m and s  
    *     m ~ dnorm(50, 25^(-2))  # requires precision which can be defined as the inverse of sigma-squared  
    *     s ~ dunif(0, 200)  
    * }"  
    * sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))  
    * sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)  
  
Markov chains:  
  
* The RJAGS approach is approximating parameters based on Monte Carlo Markov Chains (MCMC)  
	* The goal of RJAGS is to use Markov chains to estimate (approximate) posteriors that would otherwise be too complicated to model  
* Each iteration of a Markov chain depends on the previous iteration (the iterations are not entirely random or independent)  
	* Over time, the Markov chain explores the sample space, but the exploration is often over a smaller range for smaller intervals (there are auto-corelations)  
    * The overall distribution of the Markov chain mimics a random sampling drawn from the posterior distribution  
  
Markov chain diagnostics and reproducibility:  
  
* The trace plots indicate the longitudinal behavior of the chain while the density plots indicate the distribution of the chain  
* Questions about what makes for a good chain (convergence, trials needed, etc.)  
	* Stability is good - long-run trends should be stabilized  
    * Multiple parallel chains should return very similar results with similar features (should not be overly dependent on RNG/seed)  
    * summary(sleep_sim)  # provides key Markov chain diagnostics  
* Generally, problems with Markov chains can be addressed with longer chains (more iterations)  
* Helpful to set the seed and RNG in the model for reproducibility  
	* inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)  # example inside jags.model()  
  
Example code includes:  
```{r eval=FALSE}

# Take 10000 samples from the m prior
prior_m <- rnorm(10000, 50, 25)

# Take 10000 samples from the s prior    
prior_s <- runif(10000, 0, 200)

# Store samples in a data frame
samples <- data.frame(prior_m, prior_s)

# Density plots of the prior_m & prior_s samples    
ggplot(samples, aes(x = prior_m)) + 
    geom_density()
ggplot(samples, aes(x = prior_s)) + 
    geom_density()


# Check out the first 6 rows of sleep_study
head(sleep_study)

# Define diff_3
sleep_study <- sleep_study %>%
  mutate(diff_3=day_3-day_0)

# Histogram of diff_3    
ggplot(sleep_study, aes(x = diff_3)) + 
    geom_histogram(binwidth = 20, color = "white")

# Mean and standard deviation of diff_3    
sleep_study %>%
  summarize(mean(diff_3), sd(diff_3))


# DEFINE the model    
sleep_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m, s^(-2))
    }

    # Prior models for m and s
    m ~ dnorm(50, 25^(-2))
    s ~ dunif(0, 200)
}"    

# COMPILE the model
sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))    

# SIMULATE the posterior    
sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# PLOT the posterior    
plot(sleep_sim, trace = FALSE)    


# Let m be the average change in reaction time after 3 days of sleep deprivation
# In a previous exercise, you obtained an approximate sample of 10,000 draws from the posterior model of m
# You stored the resulting mcmc.list object as sleep_sim which is loaded in your workspace:
# In fact, the sample of m values in sleep_sim is a dependent Markov chain, the distribution of which converges to the posterior
# You will examine the contents of sleep_sim and, to have finer control over your analysis, store the contents in a data frame

# Check out the head of sleep_sim
head(sleep_sim)

# Store the chains in a data frame
sleep_chains <- data.frame(sleep_sim[[1]], iter = 1:10000)

# Check out the head of sleep_chains
head(sleep_chains)


# NOTE: The 10,000 recorded Iterations start after a "burn-in" period in which samples are discarded
# Thus the Iterations count doesn't start at 1!

# Use plot() to construct trace plots of the m and s chains
plot(sleep_sim, density=FALSE)

# Use ggplot() to construct a trace plot of the m chain
ggplot(sleep_chains, aes(x = iter, y = m)) + 
    geom_line()

# Trace plot the first 100 iterations of the m chain
ggplot(dplyr::filter(sleep_chains, iter<=100), aes(x = iter, y = m)) + geom_line()

# Note that the longitudinal behavior of the chain appears quite random and that the trend remains relatively constant
# This is a good thing - it indicates that the Markov chain (likely) converges quickly to the posterior distribution of m


# Use plot() to construct density plots of the m and s chains
plot(sleep_sim, trace=FALSE)

# Use ggplot() to construct a density plot of the m chain
ggplot(sleep_chains, aes(x = m)) + 
    geom_density()

# Density plot of the first 100 values in the m chain
ggplot(dplyr::filter(sleep_chains, iter<=100), aes(x = m)) + 
    geom_density()


# COMPILE the model
sleep_jags_multi <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), n.chains=4)   

# SIMULATE the posterior    
sleep_sim_multi <- coda.samples(model = sleep_jags_multi, variable.names = c("m", "s"), n.iter = 1000)

# Check out the head of sleep_sim_multi
head(sleep_sim_multi)

# Construct trace plots of the m and s chains
plot(sleep_sim_multi, density=FALSE)


# The mean of the m Markov chain provides an estimate of the posterior mean of m
# The naive standard error provides a measure of the estimate's accuracy.

# Suppose your goal is to estimate the posterior mean of m within a standard error of 0.1 ms
# If the observed naive standard error exceeds this target, no problem!
# You can simply run a longer chain


# SIMULATE the posterior    
sleep_sim_1 <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 1000)

# Summarize the m and s chains of sleep_sim_1
summary(sleep_sim_1)

# RE-SIMULATE the posterior    
sleep_sim_2 <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# Summarize the m and s chains of sleep_sim_2
summary(sleep_sim_2)


# COMPILE the model
sleep_jags <- jags.model(textConnection(sleep_model), data = list(Y = sleep_study$diff_3), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)) 

# SIMULATE the posterior    
sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m", "s"), n.iter = 10000)

# Summarize the m and s chains of sleep_sim
summary(sleep_sim)

```
  
  
  
***
  
Chapter 3 - Bayesian Inference and Prediction
  
Simple Bayesian Regression Model:  
  
* The simple Bayesian regression lays the ground work for more complicated modeling built on it  
* Suppose that the goal is to model human weights, and that they are N(m, s**2)  
	* Y(i) ~ N(m(i), s**2)  # m(i) is an average weight that depends on height  
    * m(i) = a + b*X(i) where X(i) is the height of individual i  
* Can specify Bayesian model with priors  
	* a = intercept ~ N(0, 200**2)  
    * b = slope (expected to be positive) ~ N(1, 0.5**2)  
    * s = residual standard deviation ~ Unif(0, 20)  
  
Bayesian Regression in RJAGS:  
  
* The basic lm() regression will give the parameters based on linear regression  
* Can instead define the Bayesian linear regression for RJAGS  
	* Within RJAGS, [i] means that it varies for each subject, i  
    * Within RJAGS, <- means there is an exact mathematical formula that does not need to be estimated  
    * weight_model <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b * X[i]  
    *     }  
    *     # Prior models for a, b, s  
    *     a ~ dnorm(0, 200^(-2))  
    *     b ~ dnorm(1, 0.5^(-2))  
    *     s ~ dunif(0, 20)  
    * }"  
    * weight_jags <- jags.model(textConnection(weight_model), data = list(X = bdims$hgt, Y = bdims$wgt), inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 2018))  
    * weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 10000)  
* Options for addressing the Markov chain instability include  
	* Standardize the height predictor  
    * Increase the chain length  
  
Posterior estimation and inference:  
  
* Bayesian regression using RJAGS provided estimates for the slope and intercept parameters  
* The posterior densities can be summarized for better communication - for example, point estimates based on means  
* Can also plot the lines corresponding to each pair of slope, intercepts - can further create "credible intervals"  
	* The 95% credible intervals are the middle 95% of the densities for each of the parameters (slope and intercept)  
* Can also assess frequencies of exceedence for parameters  
	* table(weight_chains$b > 1.1)  
  
Posterior prediction:  
  
* Based on simulations, the posterior mean trend was estimated  
	* Can use the final coefficients to make estimates about the population  
    * Could instead model the regression based on each set of coefficients included in the chain, including calculating the credible interval  
* Rather than using the regression to find means, the goal may be to predict an individual  
	* Plug in the data as per finding the mean  
    * The residual standard deviation (s) can then be used inside chain, using all of the a, b, s, data  
  
Example code includes:  
```{r eval=FALSE}

# Note the 3 parameters in the model of weight by height: intercept a, slope b, & standard deviation s
# In the first step of your Bayesian analysis, you will simulate the following prior models for these parameters: a ~ N(0, 200^2), b ~ N(1, 0.5^2), and s ~ Unif(0, 20)

# Take 10000 samples from the a, b, & s priors
prior_a <- rnorm(10000, 0, 200)
prior_b <- rnorm(10000, 1, 0.5)
prior_s <- runif(10000, 0, 20)

# Store samples in a data frame
samples <- data.frame(prior_a, prior_b, prior_s, set=1:10000)

# Construct density plots of the prior samples    
ggplot(samples, aes(x = prior_a)) + 
    geom_density()
ggplot(samples, aes(x = prior_b)) + 
    geom_density()
ggplot(samples, aes(x = prior_s)) + 
    geom_density()


# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE)) 

# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>% 
    mutate(height = rnorm(600, 170, 10)) %>% 
    mutate(weight = rnorm(600, prior_a + prior_b*height, prior_s))

# Plot the simulated data & regression model for each parameter set
ggplot(prior_simulation, aes(x = height, y = weight)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE, size = 0.75) + 
    facet_wrap(~ set)


# The bdims data set from the openintro package is loaded in your workspace
# bdims contains physical measurements on a sample of 507 individuals, including their weight in kg (wgt) and height in cm (hgt)

# Construct a scatterplot of wgt vs hgt
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point()

# Add a model smooth
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE)
    
# Obtain the sample regression model
wt_model <- lm(wgt ~ hgt, data = bdims)

# Summarize the model
summary(wt_model)


# DEFINE the model    
weight_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m[i], s^(-2))
        m[i] <- a + b * X[i]
    }

    # Prior models for a, b, s
    a ~ dnorm(0, 200^(-2))
    b ~ dnorm(1, 0.5^(-2))
    s ~ dunif(0, 20)
}"

# COMPILE the model
weight_jags <- jags.model(textConnection(weight_model), data = list(X=bdims$hgt, Y=bdims$wgt), 
                  inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

# COMPILE the model
weight_jags <- jags.model(textConnection(weight_model), data = list(Y = bdims$wgt, X = bdims$hgt), 
                          inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989))

# SIMULATE the posterior    
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 1000)

# PLOT the posterior    
plot(weight_sim)


# A 100,000 iteration RJAGS simulation of the posterior, weight_sim_big, is in your workspace along with a data frame of the Markov chain output:
head(weight_chains, 2)

# The posterior means of the intercept & slope parameters, a & b, reflect the posterior mean trend in the relationship between weight & height
# In contrast, the full posteriors of a & b reflect the range of plausible parameters, thus posterior uncertainty in the trend
# You will examine the trend and uncertainty in this trend below
# The bdims data are in your workspace

# Summarize the posterior Markov chains
summary(weight_sim_big)

# Calculate the estimated posterior mean of b
mean(weight_chains$b)

# Plot the posterior mean regression model
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red")

# Visualize the range of 20 posterior regression models
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = weight_chains$a[1:20], slope = weight_chains$b[1:20], color = "gray", size = 0.25)


# Summarize the posterior Markov chains
summary(weight_sim_big)

# Calculate the 95% posterior credible interval for b
quantile(weight_chains$b, c(0.025, 0.975))

# Calculate the 90% posterior credible interval for b
quantile(weight_chains$b, c(0.05, 0.95))

# Mark the 90% credible interval 
ggplot(weight_chains, aes(x = b)) + 
    geom_density() + 
    geom_vline(xintercept = quantile(weight_chains$b, c(0.05, 0.95)), color = "red")


# Mark 1.1 on a posterior density plot for b
ggplot(weight_chains, aes(x=b)) + 
    geom_density() + 
    geom_vline(xintercept = 1.1, color = "red")

# Summarize the number of b chain values that exceed 1.1
table(weight_chains$b > 1.1)

# Calculate the proportion of b chain values that exceed 1.1 
mean(weight_chains$b > 1.1)


# Calculate the trend under each Markov chain parameter set
weight_chains <- weight_chains %>% 
    mutate(m_180 = a + b*180)

# Construct a posterior density plot of the trend
ggplot(weight_chains, aes(x = m_180)) + 
    geom_density() 

# Calculate the average trend
mean(weight_chains$m_180)

# Construct a posterior credible interval for the trend
quantile(weight_chains$m_180, c(0.025, 0.975))


# Simulate 1 prediction under the first parameter set
rnorm(1, mean=weight_chains$m_180[1], sd=weight_chains$s[1])

# Simulate 1 prediction under the second parameter set
rnorm(1, mean=weight_chains$m_180[2], sd=weight_chains$s[2])

# Simulate & store 1 prediction under each parameter set
weight_chains <- weight_chains  %>% 
    mutate(Y_180=rnorm(nrow(weight_chains), mean=m_180, sd=s))

# Print the first 6 parameter sets & predictions
head(weight_chains)


# Construct a density plot of the posterior predictions
ggplot(weight_chains, aes(x=Y_180)) + 
    geom_density() + 
    geom_vline(xintercept = quantile(weight_chains$Y_180, c(0.025, 0.975)), color = "red")

# Construct a posterior credible interval for the prediction
quantile(weight_chains$Y_180, c(0.025, 0.975))

# Visualize the credible on a scatterplot of the data
ggplot(bdims, aes(x=hgt, y=wgt)) + 
    geom_point() + 
    geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red") + 
    geom_segment(x = 180, xend = 180, y = quantile(weight_chains$Y_180, c(0.025)), yend = quantile(weight_chains$Y_180, c(0.975)), color = "red")

```
  
  
  
***
  
Chapter 4 - Multivariate and Generalized Linear Models  
  
Bayesian regression with categorical predictor:  
  
* Can incorporate categorical predictors in to the Bayesian regressions  
* Example of usage of a rail-trail in MA  
	* Y[i] ~ N(m[i], s**2) where [i] is the day - assumed to have varying means but constant standard deviations  
    * X[i] is a 1/0 variable where 1 is for weekdays and 0 is for weekends  
    * m[i] = a + b*X[i], meaning that a is the typical weekend volume and a+b is the typical weekday volume  
    * The prior will be a ~ N(400, 100**2) and b ~ N(0, 200**2) and s ~ Unif(0, 200)  
* Can then define the model using RJAGS  
	* rail_model_1 <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b[X[i]]  
    *     }  
    *     # Prior models for a, b, s  
    *     a ~ dnorm(400, 100^(-2))  
    *     s ~ dunif(0, 200)  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 200^(-2))  
    * }"  
    * Note that b[1] <- 0 is because we want m[i] = a for the reference level; b[2], the second level, is what we want to model  
  
Multivariate Bayesian regression:  
  
* Bayesian models can be generalized to multivariate models, for example  
	* Y[i] ~ N(m[i], s**2) where [i] is the day - assumed to have varying means but constant standard deviations  
    * X[i] is a 1/0 variable where 1 is for weekdays and 0 is for weekends  
    * Z[i] is the high temperatue on day [i] in degrees F  
    * m[i] = a + b*X[i] + c*Z[i]  
    * a ~ N(0, 200**2), b ~ N(0, 200**2), c ~ N(0, 20**2), s ~ Unif(0, 200)  
* Can then define and simulate this model using RJAGS  
	* rail_model_2 <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dnorm(m[i], s^(-2))  
    *         m[i] <- a + b[X[i]] + c * Z[i]  
    *     }  
    *     # Prior models for a, b, c, s  
    *     a ~ dnorm(0, 200^(-2))  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 200^(-2))  
    *     c ~ dnorm(0, 20^(-2))  
    *     s ~ dunif(0, 200)  
    * }"  
  
Bayesian Poisson regression:  
  
* Can generalize regression techniques to non-normalized settings, such as Poisson regressions  
* Bicycle riders per day might better be modeled as a Poisson - should be discrete and non-negative, for example  
	* Y ~ Pois(lambda[i])  
    * log(lambda[i]) = a + b * X[i] + c * Z[i]  
    * a ~ N(0, 200**2)  
    * b ~ N(0, 2**2)  
    * c ~ N(0, 2**2)  
* Can then define the model within RJAGS  
	* poisson_model <- "model{  
    *     # Likelihood model for Y[i]  
    *     for(i in 1:length(Y)) {  
    *         Y[i] ~ dpois(l[i])  
    *         log(l[i]) <- a + b[X[i]] + c*Z[i]  
    *     }  
    *     # Prior models for a, b, c  
    *     a ~ dnorm(0, 200^(-2))  
    *     b[1] <- 0  
    *     b[2] ~ dnorm(0, 2^(-2))  
    *     c ~ dnorm(0, 2^(-2))  
    * }"  
* Caveat for the Poisson is the mean and variance should be roughly equal; might accept some imperfections of dispersions  
  
Wrap up:  
  
* Bayesian modeling has grown in popularity along with computing resources  
* RJAGS allows for defining, compiling, and simulating Bayesian models  
* Intutive posterior inference, including credible intervals  
* Generalizing from normal models to Poisson models  
  
Example code includes:  
```{r eval=FALSE}

# Confirm that weekday is a factor variable
is.factor(RailTrail$weekday)

# Construct a density plot of volume by weekday
ggplot(RailTrail, aes(x = volume, fill = weekday)) + 
    geom_density(alpha = 0.5)

# Calculate the mean volume on weekdays vs weekends
RailTrail %>%
  group_by(weekday) %>%
  summarize(mean(volume))


# DEFINE the model    
rail_model_1 <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
      Y[i] ~ dnorm(m[i], s^(-2))
      m[i] <- a + b[X[i]]
    }
    
    # Prior models for a, b, s
    a ~ dnorm(400, 100^(-2))
    b[1] <- 0
    b[2] ~ dnorm(0, 200^(-2))
    s ~ dunif(0, 200)
}"

# COMPILE the model
rail_jags_1 <- jags.model(textConnection(rail_model_1), 
    data = list(Y=RailTrail$volume, X=RailTrail$weekday),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
    )  

# COMPILE the model
rail_jags_1 <- jags.model(textConnection(rail_model_1), data = list(Y = RailTrail$volume, X = RailTrail$weekday), 
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10))

# SIMULATE the posterior    
rail_sim_1 <- coda.samples(model = rail_jags_1, variable.names = c("a", "b", "s"), n.iter = 10000)

# Store the chains in a data frame
rail_chains_1 <- data.frame(rail_sim_1[[1]])

# PLOT the posterior    
plot(rail_sim_1)


# Posterior probability that typical volume is lower on weekdays
mean(rail_chains_1$'b.2.' < 0)

# Construct a chain of values for the typical weekday volume
rail_chains_1 <- rail_chains_1 %>% 
    mutate(weekday_mean = a + b.2.)

# Construct a density plot of the weekday chain
ggplot(rail_chains_1, aes(x=weekday_mean)) +
  geom_density()

# 95% credible interval for typical weekday volume
quantile(rail_chains_1$weekday_mean, c(0.025, 0.975))


# Construct a plot of volume by hightemp & weekday
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point()

# Construct a sample model
rail_lm <- lm(volume ~ weekday + hightemp, data=RailTrail)

# Summarize the model
summary(rail_lm)

# Superimpose sample estimates of the model lines
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point() + 
    geom_abline(intercept = coef(rail_lm)["(Intercept)"], slope = coef(rail_lm)["hightemp"], color = "red") +
    geom_abline(intercept = sum(coef(rail_lm)[c("(Intercept)", "weekdayTRUE")]), slope = coef(rail_lm)["hightemp"], color = "turquoise3")


# DEFINE the model    
rail_model_2 <- "model{
  # Likelihood model for Y[i]
  for(i in 1:length(Y)){
    Y[i] ~ dnorm(m[i], s^(-2))
    m[i] <- a + b[X[i]] + c * Z[i]
  }
    
  # Prior models for a, b, c, s
  a ~ dnorm(0, 200^(-2))
  b[1] <- 0
  b[2] ~ dnorm(0, 200^(-2))
  c ~ dnorm(0, 20^(-2))
  s ~ dunif(0, 200)
}"

# COMPILE the model
rail_jags_2 <- jags.model(textConnection(rail_model_2), 
                          data = list(Y=RailTrail$volume, X=RailTrail$weekday, Z=RailTrail$hightemp),
                          inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
                          )

# SIMULATE the posterior    
rail_sim_2 <- coda.samples(model = rail_jags_2, variable.names = c("a", "b", "c", "s"), n.iter = 10000)

# Store the chains in a data frame
rail_chains_2 <- data.frame(rail_sim_2[[1]])

# PLOT the posterior    
plot(rail_sim_2)


# Summarize the posterior Markov chains
summary(rail_sim_2)

# Plot the posterior mean regression models
ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point() + 
    geom_abline(intercept = mean(rail_chains_2[, "a"]), slope = mean(rail_chains_2[, "c"]), color = "red") + 
    geom_abline(intercept = mean(rail_chains_2[, "a"]) + mean(rail_chains_2[, "b.2."]), slope = mean(rail_chains_2[, "c"]), color = "turquoise3")
  
# Posterior probability that typical volume is lower on weekdays
mean(rail_chains_2$'b.2.' < 0)


# DEFINE the model    
poisson_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dpois(l[i])
        log(l[i]) <- a + b[X[i]] + c * Z[i]
    }

    # Prior models for a, b, c
    a ~ dnorm(0, 200^(-2))
    b[1] <- 0
    b[2] ~ dnorm(0, 2^(-2))
    c ~ dnorm(0, 2^(-2))
}" 

# COMPILE the model
poisson_jags <- jags.model(textConnection(poisson_model), 
                           data = list(Y=RailTrail$volume, X=RailTrail$weekday, Z=RailTrail$hightemp),
                           inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
                           )

# SIMULATE the posterior    
poisson_sim <- coda.samples(model = poisson_jags, variable.names = c("a", "b", "c"), n.iter = 10000)

# Store the chains in a data frame
poisson_chains <- data.frame(poisson_sim[[1]])

# PLOT the posterior    
plot(poisson_sim)


# Summarize the posterior Markov chains
summary(poisson_sim)

# Plot the posterior mean regression models
ggplot(RailTrail, aes(x = hightemp, y = volume, color = weekday)) + 
    geom_point() + 
    stat_function(fun = function(x){exp(5.01352 + 0.01426 * x)}, color = "red") + 
    stat_function(fun = function(x){exp(5.01352 - 0.12800 + 0.01426 * x)}, color = "turquoise3")


# Calculate the typical volume on 80 degree weekends & 80 degree weekdays
poisson_chains <- poisson_chains %>% 
    mutate(l_weekend=exp(a + c*80)) %>% 
    mutate(l_weekday=exp(a + b.2. + c*80))

# Construct a 95% CI for typical volume on 80 degree weekend
quantile(poisson_chains$l_weekend, c(0.025, 0.975))

# Construct a 95% CI for typical volume on 80 degree weekday
quantile(poisson_chains$l_weekday, c(0.025, 0.975))


# Simulate weekend & weekday predictions under each parameter set
poisson_chains <- poisson_chains %>% 
    mutate(Y_weekend=rpois(nrow(poisson_chains), l_weekend)) %>% 
    mutate(Y_weekday=rpois(nrow(poisson_chains), l_weekday))
    
# Print the first 6 sets of parameter values & predictions
head(poisson_chains)

# Construct a density plot of the posterior weekday predictions
ggplot(poisson_chains, aes(x=Y_weekday)) +
  geom_density()

# Posterior probability that weekday volume is less 400
mean(poisson_chains$Y_weekday < 400)

```
  
  
  
***
  
###_Parallel Programming in R_  
  
Chapter 1 - Can I run my application in parallel?  
  
Partitioning problems in to independent pieces:  
  
* Course contents include  
	* Methods of parallel programming and R packages for support  
    * The parallel package in R  
    * Packages foreach and future.apply  
    * Random numbers and reproducibility  
* Programs can be partitioned either by tasks (e.g., birth model, death model, migration model) or by data (chunks of data passed to a routine, such as rowSums to a matrix)  
	* Many independent tasks are referred to as "embarassingly parallel", and this is common to statistical simulations  
  
Models of parallel computing:  
  
* Available hardware drives the ability to split components - # CPU, Memory (including shared memory or distributed memory)  
	* Message passing software runs on distributed memory and allows for fully independent processes  
    * Shared memory allows for easier passing of data  
* Programming paradigms include master-worker and map-reduce (Hadoop or Scala or the like)  
* This course will cover the master-worker model  
	* The master process creates processes for the workers and then compiles the results that the workers return  
  
R packages for parallel computing:  
  
* The R core package parallel allows for code to be independent of other packages  
* Can instead work with iotools and sparklyr for working with the map-reduce process  
	* Further, pbdR allows for many parallel approaches within R  
* The master-worker paradigms can be implemented using foreach, future.apply, snow, snowFT, snowfall, future (currently under active development, more modern)  
* The parallel package can be used for basic parallel tasks  
	* ncores <- parallel::detectCores(logical = FALSE)  
    * cl <- parallel::makeCluster(ncores)  
    * parallel::clusterApply(cl, x = ncores:1, fun = rnorm)  # x is passed to the workers in order, so worker 1 will get ronorn(ncores)  
    * parallel::stopCluster(cl)  
  
Example code includes:  
```{r}

extract_words <- function(book_name) {
    # extract the text of the book
    text <- subset(austen_books(), book == book_name)$text
    # extract words from the text and convert to lowercase
    str_extract_all(text, boundary("word")) %>% unlist %>% tolower
}

janeausten_words <- function() {
    # Names of the six books contained in janeaustenr
    books <- austen_books()$book %>% unique %>% as.character
    # Vector of words from all six books
    words <- sapply(books, extract_words) %>% unlist
    words
}

austen_books <- function () 
{
    books <- list('Sense & Sensibility' = janeaustenr::sensesensibility, 
                  'Pride & Prejudice' = janeaustenr::prideprejudice, 
                  'Mansfield Park' = janeaustenr::mansfieldpark, 
                  'Emma' = janeaustenr::emma, 
                  'Northanger Abbey' = janeaustenr::northangerabbey, 
                  'Persuasion' = janeaustenr::persuasion
                  )
    ret <- data.frame(text = unlist(books, use.names = FALSE), stringsAsFactors = FALSE)
    ret$book <- factor(rep(names(books), sapply(books, length)))
    ret$book <- factor(ret$book, levels = unique(ret$book))
    structure(ret, class = c("tbl_df", "tbl", "data.frame"))
}

max_frequency <- function(letter, words, min_length = 1) {
    w <- select_words(letter, words = words, min_length = min_length)
    frequency <- table(w)    
    frequency[which.max(frequency)]
}

select_words <- function(letter, words, min_length = 1) {
    min_length_words <- words[nchar(words) >= min_length]
    grep(paste0("^", letter), min_length_words, value = TRUE)
}

# Vector of words from all six books
words <- janeausten_words()

# Most frequent "a"-word that is at least 5 chars long
max_frequency(letter = "a", words = words, min_length = 5)

# Partitioning
result <- lapply(letters, FUN=max_frequency,
                 words = words, min_length = 5) %>% unlist()

# barplot of result
barplot(result, las = 2)


replicates <- 50
sample_size <- 10000

# Function that computes mean of normal random numbers
myfunc <- function(n, ...) mean(rnorm(n, ...))

# Init result, set seed & repeat the task sequentially
result <- rep(NA, replicates)
set.seed(123)
for(iter in 1:replicates) result[iter] <- myfunc(sample_size)

# View result
hist(result)

# Use sapply() with different distribution parameters
hist(sapply(rep(sample_size, replicates), FUN=myfunc, mean = 10, sd = 5))


# We'll now introduce a demographic model to be used throughout the course. It projects net migration rates via an AR(1) model, rate(t+1) -  = ?(rate(t) -) + error with variance s2
# An MCMC estimation for the USA resulted in 1000 samples of parameters , ? and s
# The task is to project the future distribution of migration rates

ar1_trajectory <- function(est, rate0, len = 15) {
    trajectory <- rep(NA, len)
    rate <- rate0
    for (time in seq_len(len)) {
        trajectory[time] <- ar1(est, r = rate)
        rate <- trajectory[time]
    }
    trajectory
}

ar1 <- function(est, r) {
    est['mu'] + est['phi'] * (r - est['mu']) + 
        rnorm(1, sd = est['sigma'])
}

ar1_block <- function(id, rate0 = 0.015, traj_len = 15, block_size = 10) {
    trajectories <- matrix(NA, nrow = block_size, ncol = traj_len)
    for (i in seq_len(block_size)) 
        trajectories[i,] <- ar1_trajectory(unlist(ar1est[id, ]), rate0 = rate0, len = traj_len)
    trajectories
}

show_migration <- function(trajs) {
    df <- data.frame(time = seq(2020, by = 5, len = ncol(trajs)),
                     migration_rate = apply(trajs, 2, median),
                     lower = apply(trajs, 2, quantile, 0.1),
                     upper = apply(trajs, 2, quantile, 0.9)
                    )
    g <- ggplot(df, aes(x = time, y = migration_rate)) + 
        geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey70") + 
        geom_line()
    print(g)
}


# Simulate from multiple rows of the estimation dataset
ar1_multblocks <- function(ids, ...) {
    trajectories <- NULL
    for (i in seq_along(ids)) {
        trajectories <- rbind(trajectories, ar1_block(ids[i], ...))
    }
    trajectories
}

ar1est <- data.frame(mu=c(0.0105, 0.0185, 0.022, 0.0113, 0.0144, 0.0175, -9e-04, 0.0093, 0.0111, -9e-04, -0.0024, 0.0086, 0.012, 0.0161, 0.0043, 0.0175, 0.0118, 0.0019, 0.0116, 0.0048, 0.0154, 0.0137, 0.0168, 0.0191, 0.0108, -0.0037, 0.0135, 0.0203, -0.0042, 0.0097, 0.0209, 0.0034, 0.0113, 0.0102, 0.0094, -0.0012, 0.008, 0.0082, 0.0123, 0.0175, 0.0054, -0.0087, 0.0161, 0.0155, 0.0126, 0.0181, 0.014, -0.0135, -0.0095, 0.0142, 0.011, 0.0194, 0.0149, 0.0115, 0.0129, -0.0124, 0.0116, 0.0136, 0.0161, 0.005, 0.0165, -0.0079, 0.0129, -0.0016, -7e-04, 0.0243, 0.0193, -0.004, 0.0145, 0.0078, 0.0156, 0.001, 0.0032, 0.0069, 0.0146, 0.0164, 0.0113, 0.0116, 0.0182, 0.0167, -0.0031, 0.0168, 0.0137, 0.012, -0.0212, -0.0092, 0.019, 0.0167, -0.0021, 0.0156, 0.0173, 0.0148, -0.0036, 0.0168, 0.0179, 0.0086, 0.0131, 0.015, 0.0106, 0.0132, 0.0119, 0.0156, 0.0159, 0.0256, 0.0071, 0.0163, 0.0107, 0.0139, 0.0228, 0.0139, 0.0117, 0.0133, 0.0127, -0.0162, 0.0115, 0.0095, 0.0183, 0.0183, -6e-04, 0.0177, 0.0145, 0.0041, 0.0143, 0.0135, -0.0078, 0.0036, 0.015, 0.018, 0.0158, 0.0054, -0.0204, 0.0193, 0.0051, 0.0144, 0.0129, 0.0134, 0.0116, 0.0102, 0.0203, 0.0154, 0.0106, 0.0184, 0.0096, -0.0032, 0.0143, 0.0158, 0.0093, 0.0159, 0.0112, 0.0106, 0.0075, 0.0133, 0.0171, 0.0133, 0.0139, 0.0167, 0.0131, -0.0078, 0.0135, 0.0145, 0.0104, 8e-04, 0.0205, 0.0046, 0.011, 0.0148, 0.0202, 8e-04, 0.0211, 0.0135, -8e-04, -0.0104, -0.0027, 0.0094, 0.0179, -0.0101, 0.0156, 0.0155, 0.014, 0.0149, 0.0165, 0.0168, 0.0155, 0.0136, 0.0156, 0.0149, 0.0191, 0.0176, 0.0094, -0.0076, 0.0162, 0.0143, 0.0182, 0.0102, 0.015, -0.0292, 0.0063, -0.0028, 0.0163, 0.015), 
                     sigma=c(0.0081, 0.0053, 0.0069, 0.0075, 0.0082, 0.006, 0.0101, 0.011, 0.0064, 0.0066, 0.0095, 0.0057, 0.0078, 0.005, 0.0076, 0.0064, 0.0067, 0.0049, 0.0086, 0.0067, 0.0063, 0.0054, 0.0063, 0.0077, 0.0072, 0.0074, 0.0067, 0.0047, 0.0125, 0.0069, 0.0052, 0.0073, 0.0063, 0.0072, 0.0086, 0.0079, 0.009, 0.006, 0.0077, 0.0061, 0.0082, 0.0072, 0.0054, 0.0056, 0.0072, 0.0085, 0.0064, 0.0058, 0.0064, 0.0084, 0.0075, 0.006, 0.0048, 0.0068, 0.0065, 0.0082, 0.0072, 0.0056, 0.0056, 0.0055, 0.0054, 0.0059, 0.0064, 0.0069, 0.0073, 0.0071, 0.0057, 0.0062, 0.0086, 0.0062, 0.0054, 0.0052, 0.0066, 0.0076, 0.0046, 0.0056, 0.0066, 0.0077, 0.0074, 0.0061, 0.0056, 0.0065, 0.0069, 0.0084, 0.0058, 0.007, 0.0074, 0.0077, 0.0081, 0.0083, 0.0054, 0.0057, 0.0076, 0.0119, 0.0056, 0.0078, 0.005, 0.0073, 0.0075, 0.0054, 0.0085, 0.011, 0.0063, 0.0056, 0.009, 0.0069, 0.008, 0.0063, 0.007, 0.0059, 0.0064, 0.006, 0.0103, 0.0085, 0.006, 0.0076, 0.0054, 0.0066, 0.0056, 0.0071, 0.0079, 0.007, 0.0085, 0.0075, 0.007, 0.0085, 0.006, 0.0067, 0.006, 0.0074, 0.0098, 0.0066, 0.0058, 0.0075, 0.0064, 0.0059, 0.0103, 0.0055, 0.0053, 0.0068, 0.0057, 0.009, 0.0118, 0.0096, 0.0085, 0.0075, 0.0078, 0.0041, 0.0056, 0.008, 0.0071, 0.006, 0.0046, 0.0061, 0.007, 0.0061, 0.0066, 0.0075, 0.0094, 0.0072, 0.008, 0.0064, 0.0079, 0.0068, 0.0069, 0.0058, 0.0056, 0.0057, 0.0065, 0.006, 0.0073, 0.0067, 0.0068, 0.0071, 0.0048, 0.0071, 0.0063, 0.0051, 0.0079, 0.0042, 0.0048, 0.0066, 0.0072, 0.0058, 0.0057, 0.0083, 0.0063, 0.0057, 0.0103, 0.0096, 0.0067, 0.0051, 0.0075, 0.0064, 0.0069, 0.007, 0.007, 0.0074, 0.0056, 0.006), 
                     phi=c(0.42, 0.3509, 0.8197, 0.5304, 0.1491, 0.3675, 0.9687, 0.7877, 0.7114, 0.9435, 0.9634, 0.9189, 0.4758, 0.5738, 0.8016, 0.0509, 0.8281, 0.8168, 0.7442, 0.9347, 0.1699, 0.3566, 0.8388, 0.7724, 0.7474, 0.7834, 0.6661, 0.5162, 0.9025, 0.5306, 0.6912, 0.7625, 0.8289, 0.6985, 0.9188, 0.9639, 0.3178, 0.7288, 0.4129, 0.2196, 0.9304, 0.9697, 0.193, 0.1474, 0.3111, 0.8844, 0.7386, 0.9674, 0.9983, 0.4863, 0.9338, 0.7999, 0.4696, 0.5078, 0.5141, 0.9958, 0.6404, 0.2886, 0.4171, 0.9856, 0.3261, 0.9713, 0.682, 0.7686, 0.8577, 0.9481, 0.6057, 0.934, 0.3161, 0.9414, 0.8349, 0.8325, 0.8913, 0.7726, 0.7327, 0.1403, 0.8144, 0.7506, 0.225, 0.4884, 0.9052, 0.2891, 0.1652, 0.7612, 0.9403, 0.9865, 0.4107, 0.6518, 0.893, 0.4981, 0.72, 0.3366, 0.8437, 0.2551, 0.7753, 0.5, 0.7857, 0.7107, 0.5643, 0.2887, 0.9621, 0.2384, 0.414, 0.86, 0.6917, 0.4946, 0.2325, 0.3419, 0.9219, 0.2706, 0.717, 0.2327, 0.7541, 0.9692, 0.5838, 0.9346, 0.4739, 0.3219, 0.9634, 0.3046, 0.9913, 0.8485, 0.3071, 0.0373, 0.9183, 0.7935, 0.0039, 0.5968, 0.3654, 0.595, 0.9712, 0.2745, 0.6027, 0.7441, 0.7641, 0.3582, 0.3397, 0.7748, 0.8188, 0.0604, 0.5076, 0.2856, 0.6859, 0.6705, 0.0326, 0.8749, 0.2596, 0.1138, 0.6072, 0.4, 0.9241, 0.612, 0.2375, 0.2495, 0.0661, 0.3234, 0.7651, 0.8581, 0.4818, 0.7303, 0.7458, 0.8925, 0.2861, 0.982, 0.0791, 0.2474, 0.4326, 0.8757, 0.5288, 0.6476, 0.8473, 0.9098, 0.9562, 0.8464, 0.5444, 0.9738, 0.706, 0.0795, 0.391, 0.3167, 0.3311, 0.5681, 0.27, 0.9046, 0.2299, 0.2299, 0.085, 0.4002, 0.7443, 0.9865, 0.7028, 0.9016, 0.6092, 0.2367, 0.5402, 0.9401, 0.8013, 0.993, 0.2473, 0.6414)
                     )
str(ar1est)


# Generate trajectories for all rows of the estimation dataset
trajs <- ar1_multblocks(seq_along(nrow(ar1est)), rate0 = 0.015,  block_size = 10, traj_len = 15)

# Show results
show_migration(trajs)


# Load package
library(parallel)

# How many physical cores are available?
ncores <- detectCores(logical = FALSE)

# Create a cluster
cl <- makeCluster(ncores)

# Process rnorm in parallel
clusterApply(cl, 1:ncores, fun = rnorm, mean = 10, sd = 2)

# Evaluate partial sums in parallel
part_sums <- clusterApply(cl, x = c(1, 51), fun = function(x) sum(x:(x + 49)))

# Total sum
total <- sum(unlist(part_sums))

# Check for correctness
total == sum(1:100)

# Stop the cluster
stopCluster(cl)


# Create a cluster and set parameters
cl <- makeCluster(2)
replicates <- 50
sample_size <- 10000

# Parallel evaluation
means <- clusterApply(cl, x = rep(sample_size, replicates), fun = myfunc)
                
# View results as histogram
hist(unlist(means))

```
  
  
  
***
  
Chapter 2 - The parallel package  
  
Cluster basics:  
  
* The parallel package consists of two parts - snow (Tuerney) and multicore (Urbanek)  
	* The snow can work on any operating system  
    * The multicore works on most systems but not on Windows  
* Supported backends for snow are managed automatically by the parallel package  
	* cl <- makeCluster(ncores, type = "PSOCK")  # default socket communication, works on all OS, all clusters start with a completely empty environment  
    * cl <- makeCluster(ncores, type = "FORK")  # all OS except Windows, all workers are complete copies of the master environment  
    * cl <- makeCluster(ncores, type = "MPI")  # interface provided by Rmpi and may be more efficient on machines where MPI is enabled  
  
Core of parallel:  
  
* The main processing functions are clusterApply and clusterApplyLB ("load balanced")  
* The wrapper functions include parApply, parLapply, parSapply, parRapply (rows of a matrix), parCapply (columns of a matrix)  
	* Further, parLapplyLB, parSapplyLB are wrappers on the clusterApplyLB data  
* Example of using clusterApply for work on a pre-defined cluster cl  
	* clusterApply(cl, x = arg.sequence, fun = myfunc)  # each element of x is passed to myfunc (length of x is the total number of tasks)  
* There are several overheads involved in creating parallel processing - starting/stopping clusters, messages sent between nodes/masters, size of messages  
	* Communications between master and workers is expensive, so long worker times are preferred in general  
    * The overheads may sometimes be so significant as to make parallel processing more time-consuming than serial processing  
  
Initialization of nodes:  
  
* Cluster nodes typically start with a clean, empty environment (default for sockets)  
* Repeated communications with the workers is expensive  
	* clusterApply(cl, rep(1000, n), rnorm, sd = 1:1000)  # master needs to send the vector to all the clusters (big overhead)  
* Good practice is to initialize workers at the beginning with everything that stays constant and/or is time consuming  
	* Sending static data or datasets, loading libraries, evaluating global functions, etc.  
* The clusterCall() will call the same function with the same arguments on all the nodes  
	* cl <- makeCluster(2)  
    * clusterCall(cl, function() library(janeaustenr))  # will be loaded in all the clusters  
    * clusterCall(cl, function(i) emma[i], 20)  # will call the 20th element of emma from janeausten  
* The clusterEvalQ() will evaluate a literal expression on all nodes  
	* cl <- makeCluster(2)  
    * clusterEvalQ(cl, { library(janeaustenr) ; library(stringr) ; get_books <- function() austen_books()$book %>% unique %>% as.character })  # all books in the package  
    * clusterCall(cl, function(i) get_books()[i], 1:3)  # function get_books is available in the environment due to the above  
* The clusterExport() will export objects from master to the workers  
	* books <- get_books()  
    * cl <- makeCluster(2)  
    * clusterExport(cl, "books")  # The books object is passed quoted  
    * clusterCall(cl, function() print(books))  # books will be available since it was passed by clusterExport()  
  
Subsetting data:  
  
* Each task is applied to a different data chunk; these can be made available to the worker in various ways  
	* Random numbers on the fly  
    * Arguments  
    * Chunking on the workers side  
* Example of random numbers being created on the fly by the workers (reproducibility covered in later chapters)  
	* myfunc <- function(n, ...) mean(rnorm(n, ...))  
    * clusterApply(cl, rep(1000, 20), myfunc, sd = 6)  
* Example of chunking the data on the master side and then passing the data to workers as an argument  
	* Incorporated in to parApply() by default  
    * cl <- makeCluster(4)  
    * mat <- matrix(rnorm(12), ncol=4)  
    * parCapply(cl, mat, sum)  # splits the matrix by column and passes to worker  
    * unlist(clusterApply(cl, as.data.frame(mat), sum))  # needs to be converted to data.frame first for clusterApply()  
* Example of chunking data on the worker side (each pre-populated with the full data, chunking on the worker side) - saves communication time  
	* n <- 100  
    * M <- matrix(rnorm(n * n), ncol = n)  
    * clusterExport(cl, "M")  
    * mult_row <- function(id) apply(M, 2, function(col) sum(M[id,] * col))  
    * clusterApply(cl, 1:n, mult_row) %>% do.call(rbind, .)  
  
Example code includes:  
```{r}

# Load parallel and create a cluster
library(parallel)
cl <- makeCluster(4)

# Investigate the cl object and its elements
typeof(cl)
length(cl)
typeof(cl[[3]])
cl[[3]]$rank

# What is the process ID of the workers
clusterCall(cl, Sys.getpid)

# Stop the cluster
stopCluster(cl)


# Define ncores and a print function
ncores <- 2
print_ncores <- function() print(ncores)

# Create a socket and a fork clusters
# cl_sock <- makeCluster(ncores, type = "PSOCK")
# cl_fork <- makeCluster(ncores, type = "FORK")  # this is possible only on OS other than Windows

# Evaluate the print function on each cluster
# clusterCall(cl_sock, print_ncores)  # this will fail since the socket has no knowledge of the main environment
# clusterCall(cl_fork, print_ncores)

# Change ncores and evaluate again
# ncores <- 4
# clusterCall(cl_fork, print_ncores)  # the fork is only of the original environment, so these clusters will still think the answer is 2


# In this exercise, you will take the simple embarrassingly parallel application for computing mean of random numbers (myfunc()) from the first chapter, and implement two functions:
# One that runs the application sequentially, mean_seq(), and one that runs it in parallel, mean_par()
# Both functions have three arguments, n (sample size), repl (number of replicates) and ... (passed to myfunc())
# Function mean_par() assumes a cluster object cl to be present in the global environment

# Function to run repeatedly
myfunc <- function(n, ...) mean(rnorm(n, ...))

# Sequential solution
mean_seq <- function(n, repl, ...) {
    res <- rep(NA, repl)
    for (it in 1:repl) res[it] <- myfunc(n, ...)
    res
}

# Parallel solution
mean_par <- function(n, repl, ...) {
    res <- clusterApply(cl, x = rep(n, repl), fun = myfunc, ...)
    unlist(res)
}


# Load packages 
library(parallel)
library(microbenchmark)

# Create a cluster
cl <- makeCluster(2)

# Compare run times
microbenchmark(mean_seq(3000000, repl = 4), 
               mean_par(3000000, repl = 4),
               mean_seq(100, repl = 100), 
               mean_par(100, repl = 100),
               times = 1, unit = "s")
# Stop cluster               
stopCluster(cl)


# Load extraDistr on master
library(extraDistr)

# Define myrdnorm 
myrdnorm <- function(n, mean = 0, sd = 1) 
    rdnorm(n, mean = mean, sd = sd)

# Run myrdnorm in parallel - should fail
# res <- clusterApply(cl, rep(1000, 20), myrdnorm, sd = 6)   # will error out


# Load extraDistr on all workers
cl <- makeCluster(2)
clusterEvalQ(cl, library(extraDistr))

# Run myrdnorm in parallel again and show results
res <- clusterApply(cl, rep(1000, 20), myrdnorm, sd = 6)
hist(unlist(res))


# myrdnorm that uses global variables
myrdnorm <- function(n) rdnorm(n, mean = mean, sd = sd)

# Initialize workers 
clusterEvalQ(cl, {
    library(extraDistr)
    mean=10
    sd=5
    })
    
# Run myrdnorm in parallel and show results
res <- clusterApply(cl, rep(1000, 100), myrdnorm)

# View results
hist(unlist(res))


# Set global objects on master
mean <- 20
sd <- 10

# Export global objects to workers
clusterExport(cl, c("mean", "sd"))

# Load extraDistr on workers
clusterEvalQ(cl, library(extraDistr))

# Run myrdnorm in parallel and show results
res <- clusterApply(cl, rep(1000, 100), myrdnorm)
hist(unlist(res))


select_words <- function(letter, words, min_length = 1) {
    min_length_words <- words[nchar(words) >= min_length]
    grep(paste0("^", letter), min_length_words, value = TRUE)
}

# Export "select_words" to workers
clusterExport(cl, "select_words")

# Split indices for two chunks
ind <- splitIndices(length(words), 2)

# Find unique words in parallel
result <- clusterApply(cl, x = list(words[ind[[1]]], words[ind[[2]]]),  
            function(w, ...) unique(select_words("v", w, ...)), 
            min_length = 10)
            
# Show vectorized unique results
unique(unlist(result))


# Earlier you defined a function ar1_multblocks() that takes a vector of row identifiers as argument and generates migration trajectories using the corresponding rows of the parameter set ar1est
# ar1_multblocks() depends on ar1_block() which in turns depends on ar1_trajectory()
# These functions along with the cluster object cl of size 4, function show_migration(), the dataset ar1est (reduced to 200 rows) and packages parallel and ggplot2 are available in your workspace

ar1_block <- function(id, rate0 = 0.015, traj_len = 15, block_size = 10) {
    trajectories <- matrix(NA, nrow = block_size, ncol=traj_len)
    for (i in seq_len(block_size)) 
        trajectories[i,] <- ar1_trajectory(unlist(ar1est[id, ]), rate0 = rate0, len = traj_len)
    trajectories
}

ar1_trajectory <- function(est, rate0, len = 15) {
    ar1 <- function(est, r) {
        # simulate one AR(1) value
        est['mu'] + est['phi'] * (r - est['mu']) + 
        rnorm(1, sd = est['sigma'])
    }
    trajectory <- rep(NA, len)
    rate <- rate0
    for (time in seq_len(len)) {
        trajectory[time] <- ar1(est, r = rate)
        rate <- trajectory[time]
    }
    trajectory
}

ar1_multblocks <- function(ids, ...) {
    trajectories <- NULL
    for (i in seq_along(ids))
        trajectories <- rbind(trajectories, ar1_block(ids[i], ...))
    trajectories
}

# Export data and functions
clusterExport(cl, c("ar1est", "ar1_block", "ar1_trajectory"))

# Process ar1_multblocks in parallel
res <- clusterApply(cl, 1:nrow(ar1est), ar1_multblocks)

# Combine results into a matrix and show results
trajs <- do.call(rbind, res)
show_migration(trajs)


# The object res returned by clusterApply() in the previous exercise is also in your workspace, now called res_prev
res_prev <- res

# Split task into 5 chunks
ind <- splitIndices(nrow(ar1est), 5)

# Process ar1_multblocks in parallel
res <- clusterApply(cl, ind, ar1_multblocks)

# Dimensions of results 
(res_dim <- c(length(res), nrow(res[[1]])))
(res_prev_dim <- c(length(res_prev), nrow(res_prev[[1]])))

stopCluster(cl)

```
  
  
  
***
  
Chapter 3 - foreach, future.apply, and Load Balancing  
  
foreach:  
  
* The looping construct can be applied using the foreach package (Calaway and Weston), similar to previous examples  
* The foreach makes it possible to run parallel processing for loops - code can be written the same way for both sequential and parallel applications  
* The basic syntax is foreach(.) %do% .  
	* library(foreach)  
    * foreach(n = rep(5, 3)) %do% rnorm(n)  
    * foreach(n = rep(5, 3), m = 10^(0:2)) %do% rnorm(n, mean = m)  # can pass arguments  
    * The foreach() call will return a value - in the example above, this would be a list  
* Can combine results using post-processing arguments - the .combine argument  
	* foreach(n = rep(5, 3), .combine = rbind) %do% rnorm(n)  # rbind of the three lists  
    * foreach(n = rep(5, 3), .combine = '+') %do% rnorm(n)  # sum across the three lists  
* Can also use list comprehensions with the foreach() function - using the %:% operator  
	* foreach(x = sample(1:1000, 10), .combine = c) %:% when(x %% 3 == 0 || x %% 5 == 0) %do% x  
  
foreach and parallel backends:  
  
* The most popular backend is the doParallel() call for parallel foreach() processing  
	* Other backends include doFuture() using the future package and doSEQ() which allows switching between parallel and sequential  
* The doParallel package by Calaway et al is an interface between foreach and parallel, and requires initialization with registerDoParallel()  
	* library(doParallel)  
    * registerDoParallel(cores = 3)  # uses multicore for Unix and snow for Windows  
    * cl <- makeCluster(3)  # can make your own clusters and pass them also  
    * registerDoParallel(cl)  # passing the cl object rather than cores (will default to snow since that is makeCluster() default  
* Examples of converting a sequential loop to a parallel loop  
	* library(foreach)  
    * foreach(n = rep(5, 3)) %do% rnorm(n)  
    * library(doParallel)  
    * cl <- makeCluster(3)  
    * registerDoParallel(cl)  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  # note the conversion to %dopar% which is what engages the parallel processing  
* The doFuture package (Bengtsson) is built on top of the future package  
	* The central idea is that there is a future plan for how foreach should work behind the scenes - sequential, cluster, multicore, multiprocess, etc.  
    * Other packages are available from future.batchtools  
    * library(doFuture)  
    * registerDoFuture()  
    * plan(cluster, workers = 3)  # using the cluster plan  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  # same use of foreach, with the cluster plan applied  
    * plan(multicore)  # using the multicore plan  
    * foreach(n = rep(5, 3)) %dopar% rnorm(n)  
  
future and future.apply - packages that are continually under development:  
  
* The future package intends to have a uniform API for sequential and parallel processing  
* The future construct is for an expression that may be used in the future  
	* Example in plain R - x <- mean(rnorm(n, 0, 1)) ; y <- mean(rnorm(n, 10, 5)) ; print(c(x, y))  
    * Example in implicit futures - x %<-% mean(rnorm(n, 0, 1)) ; y %<-% mean(rnorm(n, 10, 5)) ; print(c(x, y))  
    * Example in explicit futures - x <- future(mean(rnorm(n, 0, 1))) ; y <- future(mean(rnorm(n, 10, 5))) ; print(c(value(x), value(y)))  
    * Values can be managed asynchronously, meaning that the next line can start running while the current line is still in process  
* The same code can then be run either in parallel or sequentially - sequential example  
	* plan(sequential)  
    * x %<-% mean(rnorm(n, 0, 1))  
    * y %<-% mean(rnorm(n, 10, 5))  
    * print(c(x, y))  
* The same code can then be run either in parallel or sequentially - parallel example  
	* plan(multicore)  
    * x %<-% mean(rnorm(n, 0, 1))  
    * y %<-% mean(rnorm(n, 10, 5))  
    * print(c(x, y))  
* The package future.apply is a higher level API for all the apply packages in R using futures  
	* Sibling to foreach  
    * functions include future_lapply(),future_sapply(),future_apply()  
* Example of using future.apply()  
	* lapply(1:10, rnorm)  # base R  
    * plan(sequential)  
    * future_lapply(1:10, rnorm)  # future.apply implementation of base R above  
    * plan(cluster, workers=4)  
    * future_lapply(1:10, rnorm)  # future.apply implementation of base R above, now run in parallel  
* Separating the plan from the processing allows for processing across many systems  
	* Single CPU uses sequential, cluster plan for many cores, etc.  
  
Load balancing and scheduling:  
  
* There can be significant node waiting times if the master waits for all workers to finish before assigning new tasks  
	* The clusterApplyLB() is designed to speed up processing by sending new tasks to workers as soon as they finish their old tasks  
* Communication overhead can also be a big problem when the tasks are small  
	* Can instead give the workers many tasks at once, and have them communicate with the master only at the start and the finish  
    * Drawback is that idle workers will not be available to help busy workers  
* Can chunk in parallel using the splitIndices() function  
	* splitIndices(10, 2)  
    * clusterApply(cl, x = splitIndices(10, 2), fun = sapply, "*", 100)  # multiply all numbers by 100  
    * Can also use parApply with the chunk.size option in R 3.5+  
    * foreach(s = isplitVector(1:10, chunks = 2)) %dopar% sapply(s, "*", 100)  # itertools functions to implement in foreach  
    * future_sapply(1:10, `*`, 100, future.scheduling = 1)  # 1 chunk per worker  
    * future_sapply(1:10, `*`, 100, future.scheduling = FALSE)  # 1 chunk per task  
  
Example code includes:  
```{r}

# Recall the first chapter where you found the most frequent words from the janeaustenr package that are of certain minimum length
result <- lapply(letters, max_frequency, words = words, min_length = 5) %>% 
    unlist
# In this exercise, you will implement the foreach construct to solve the same problem
# The janeaustenr package, a vector of all words from the included books, words, and a function max_frequency() for finding the results based on a given starting letter are all available in your workspace

# Load the package
library(foreach)

# foreach construct
result <- foreach(l = letters, .combine=c) %do% max_frequency(l, words=words, min_length=5)
                
# Plot results 
barplot(result, las = 2)


# Specifically, your job is to modify the code so that the maximum frequency for the first half of the alphabet is obtained for words that are two and more characters long, while the frequency corresponding to the second half of the alphabet is derived from words that are six and more characters long
# Note that we are using an alphabet of 26 characters

# foreach construct and combine into vector
result <- foreach(l = letters, n = rep(c(2, 6), each=13), .combine = c) %do% 
    max_frequency(l, words=words, min_length=n)
          
# Plot results
barplot(result, las = 2)


# Register doParallel with 2 cores
doParallel::registerDoParallel(cores=2)

# Parallel foreach loop
res <- foreach(r = rep(1000, 100), .combine = rbind, 
            .packages = "extraDistr") %dopar% myrdnorm(r)
            
# Dimensions of res
dim_res <- dim(res)


# So far you learned how to search for the most frequent word in a text sequentially using foreach()
# In the course of the next two exercises, you will implement the same task using doParallel and doFuture for parallel processing and benchmark it against the sequential version
# The sequential solution is implemented in function freq_seq() (type freq_seq in your console to see it)
# It iterates over a global character vector chars and calls the function max_frequency() which searches within a vector of words, while filtering for minimum word length
# All these objects are preloaded, as is the doParallel package
# Your job now is to write a function freq_doPar() that runs the same code in parallel via doParallel

freq_seq <- function(min_length = 5)
    foreach(l = letters, .combine = c) %do% 
        max_frequency(l, words = words, min_length = min_length)

# Function for doParallel foreach
freq_doPar <- function(cores, min_length = 5) {
    # Register a cluster of size cores
    doParallel::registerDoParallel(cores=cores)
    
    # foreach loop
    foreach(l=letters, .combine=c, 
            .export = c("max_frequency", "select_words", "words"),
            .packages = c("janeaustenr", "stringr")) %dopar%
        max_frequency(l, words=words, min_length=min_length)
}

# Run on 2 cores
freq_doPar(cores=2)


# Now your job is to create a function freq_doFut() that accomplishes the same task as freq_doPar() but with the doFuture backend
# Note that when using doFuture, arguments .packages and .export in foreach() are not necessary, as the package deals with the exports automatically
# You will then benchmark these two functions, together with the sequential freq_seq()
# All the functions from the last exercise are available in your workspace
# In addition, the packages doFuture and microbenchmark are also preloaded
# To keep the computation time low, the global chars vector is set to the first six letters of the alphabet only

cores <- 2
min_length <- 5

# Error in tweak.function(strategy, ..., penvir = penvir) : 
# Trying to use non-future function 'survival::cluster': function (x)  { ... }
# For solution see https://github.com/HenrikBengtsson/future/issues/152

# Function for doFuture foreach
freq_doFut <- function(cores, min_length = 5) {
    # Register and set plan
    doFuture::registerDoFuture()
    future::plan(future::cluster, workers=cores)

    # foreach loop
    foreach(l = letters, .combine = c) %dopar%
        max_frequency(l, words = words, min_length = min_length)
}

# Benchmark
microbenchmark(freq_seq(min_length),
               freq_doPar(cores, min_length),
               freq_doFut(cores, min_length),
               times = 1)

# It is straight forward to swap parallel backends with foreach
# In this small example, you might not see any time advantage in running it in parallel
# In addition, doFuture is usually somewhat slower than doParallel
# This is because doFuture has a higher computation overhead
# We encourage you to test these frameworks on more time-consuming applications where an overhead become negligible relative to the overall processing time

extract_words_from_text <- function(text) {
    str_extract_all(text, boundary("word")) %>% 
        unlist %>% 
        tolower
}

# Main function
freq_fapply <- function(words, chars=letters, min_length=5) {
    unlist(future.apply::future_lapply(chars, FUN=max_frequency, words = words, min_length = min_length))
}

obama <- readLines("./RInputFiles/obama.txt")
obama_speech <- paste(obama[obama != ""], collapse=" ")

# Extract words and call freq_fapply
words <- extract_words_from_text(obama_speech)
res <- freq_fapply(words)

# Plot results
barplot(res, las = 2)


# Now imagine you are a user of the fictional package from the previous exercise
# At home you have a two-CPU Mac computer, and at work you use a Linux cluster with two 16-CPU computers, called "oisin" and "oscar"
# Your job is to write a function for each of the hardware that calls freq_fapply() while taking advantage of all available CPUs
# For the cluster, you set workers to a vector of computer names corresponding to the number of CPUs, i.e. 16 x "oisin" and 16 x "oscar"
# For a one-CPU environment, we have created a function fapply_seq()

# fapply_seq <- function(...) {
#     future::plan(strategy="sequential") 
#     freq_fapply(words, letters, ...)
# }

# multicore function
# fapply_mc <- function(cores=2, ...) {
#     plan(strategy="multicore", workers=cores)
#     freq_fapply(words, letters, ...)
# }

# cluster function
# fapply_cl <- function(cores=NULL, ...) {
#     # set default value for cores
#     if(is.null(cores))
#         cores <- rep(c("oisin", "oscar"), each = 16)
#         
#     # parallel processing
#     plan(strategy="cluster", workers=cores)
#     freq_fapply(words, letters, ...)
# }


# Note: Multicore does not work on Windows. We recommend using the 'multiprocess' or 'cluster' plan on Windows computers.

# Microbenchmark
# microbenchmark(fapply_seq = fapply_seq(),
#                fapply_mc_2 = fapply_mc(cores=2), 
#                fapply_mc_10 = fapply_mc(cores=10),
#                fapply_cl = fapply_cl(cores=2), 
#                times = 1)

# Which is the slowest?
# slowest1 <- "fapply_cl"


# This is because for a small number of tasks a sequential code can run faster than a parallel version due to the parallel overhead
# The cluster plan has usually the largest overhead and thus, should be used only for larger number of tasks
# The multicore may be more efficient when the number of workers is equal to the number of cores
# It uses shared memory, and thus is faster than cluster


# In your workspace there is a vector tasktime containing simulated processing times of 30 tasks (generated using runif())
# There is also a cluster object cl with two nodes
# Your job is to apply the function Sys.sleep() to tasktime in parallel using clusterApply() and clusterApplyLB() and benchmark them
# The parallel and microbenchmark packages are loaded
# We also provided functions for plotting cluster usage plots called plot_cluster_apply() and plot_cluster_applyLB()
# Both functions use functionality from the snow package

tasktime <- c(0.1328, 0.1861, 0.2865, 0.4541, 0.1009, 0.4492, 0.4723, 0.3304, 0.3146, 0.031, 0.1031, 0.0884, 0.3435, 0.1921, 0.3849, 0.2489, 0.3588, 0.496, 0.1901, 0.3887, 0.4674, 0.1062, 0.3259, 0.0629, 0.1337, 0.1931, 0.0068, 0.1913, 0.4349, 0.1702)

# plot_cluster_apply <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::clusterApply(cl, x, fun)),
#             title = "Cluster usage of clusterApply")

# plot_cluster_applyLB <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::clusterApplyLB(cl, x, fun)),
#             title = "Cluster usage of clusterApplyLB")

# Benchmark clusterApply and clusterApplyLB
# microbenchmark(
#     clusterApply(cl, tasktime, Sys.sleep),
#     clusterApplyLB(cl, tasktime, Sys.sleep),
#     times = 1
# )

# Plot cluster usage
# plot_cluster_apply(cl, tasktime, Sys.sleep)
# plot_cluster_applyLB(cl, tasktime, Sys.sleep)


# Now we compare the results from the previous exercise with ones generated using parSapply(), which represents here an implementation that groups tasks into as many chunks as there are workers available
# You first explore its cluster usage plot, using the function plot_parSapply() we defined for you
# We generated a version of the tasktime vector, called bias_tasktime that generates very uneven load
# Your job is to compare the run times of parSapply() with clusterApplyLB() applied to bias_tasktime

# plot_parSapply <- function(cl, x, fun) 
#     plot(snow::snow.time(snow::parSapply(cl, x, fun)),
#             title = "Cluster usage of parSapply")

# bias_tasktime <- c(1, 1, 1, 0.1, 0.1, 0.1, 1e-04, 1e-04, 1e-04, 0.001, 1)

# Plot cluster usage for parSapply
# plot_parSapply(cl, tasktime, Sys.sleep)

# Microbenchmark
# microbenchmark(
#     clusterApplyLB(cl, bias_tasktime, Sys.sleep),
#     parSapply(cl, bias_tasktime, Sys.sleep),
#     times = 1
# )

# Plot cluster usage for parSapply and clusterApplyLB
# plot_cluster_applyLB(cl, bias_tasktime, Sys.sleep)
# plot_parSapply(cl, bias_tasktime, Sys.sleep)

```
  
  
  
***
  
Chapter 4 - Random Numbers and Reproducibility  
  
Are results reproducible?  
  
* Many statistical numbers require the use of random numbers - MCMC, boot, simulation, sample, rnorm, etc.  
	* Typically, would use a set.seed() to initialize the RNG to a known state  
* Setting the seed typically does not guarantee reproducibility of parallel processing  
	* library(parallel)  
    * cl <- makeCluster(2)  
    * set.seed(1234)  
    * clusterApply(cl, rep(3, 2), rnorm)  
    * set.seed(1234)  # only sets the RNG in the master node; thus not sent to the workers  
    * clusterApply(cl, rep(3, 2), rnorm)  # will get different sets of results 
* Can instead set the RNG for each of the workers  
	* clusterEvalQ(cl, set.seed(1234))  
    * clusterApply(cl, rep(3, 2), rnorm)  # both clusters will give the identicla results  
* There is another common and not recommended method of generating random numbers in parallel code - gives statistical properties that are not desirable  
	* for (i in 1:2) {  
    *     set.seed(1234)  
    *     clusterApply(cl, sample(1:10000000, 2), set.seed)  
    *     print(clusterApply(cl, rep(3, 2), rnorm))  
    * }  
  
Parallel random number generators:  
  
* A good RNG should comply with certain parameters - long period > 2**100, good structural properties in high dimensions, reproducible  
	* These parameters should hold in a distributed environment also  
* RNG streams with period 2**291 and seeds 2**127 steps apart is available based on research by L'Ecuyer  
	* Allows for each part of a parallel process to have reproducible streams with the proper properties  
    * Available in R through rlecuyer and rstream  
    * Also available in R Code using RNGkind("L'Ecuyer-CMRG")  
* The L'Ecuyer generator is the default when using parallel but needs to be initialized with a seed for cluster cl  
	* clusterSetRNGStream(cl, iseed = 1234)  # initializes a reproducible and independent seed for each of the workers  
* Reproducibility in parallel depends on the task flow - certain conditions need to apply  
	* Runs on clusters of the same size (different number of workers means different random numbers)  
    * Cannot use load balancing (balancing loads means non-deterministic scheduling and assignment)  
  
Reproducibility in foreach and future.apply:  
  
* Can achieve reproducibility in foreach using doRNG - one stream per task  
	* library(doRNG)  
    * library(doParallel)  
    * registerDoParallel(cores = 3)  
    * set.seed(1)  
    * res1 <- foreach(n = rep(2, 5), .combine = rbind) %dorng% rnorm(n)  
    * set.seed(1)  
    * res2 <- foreach(n = rep(2, 5), .combine = rbind) %dorng% rnorm(n)  
    * identical(res1, res2)  # TRUE  
* Can also use doRNG by way of %dopar%  
	* library(doRNG)  
    * library(doParallel)  
    * registerDoParallel(cores = 3)  
    * registerDoRNG(1)  # 1 is the seed  
    * res3 <- foreach(n = rep(2, 5), .combine = rbind) %dopar% rnorm(n)  
    * set.seed(1)  
    * res4 <- foreach(n = rep(2, 5), .combine = rbind) %dopar% rnorm(n)  
    * c(identical(res1, res3), identical(res2, res4))  # TRUE TRUE  
* Can also use independent streams in future.apply  
	* library(future.apply)  
    * plan(sequential)  
    * res5 <- future_lapply(1:5, FUN = rnorm, future.seed = 1234)  
    * plan(multiprocess)  
    * res6 <- future_lapply(1:5, FUN = rnorm, future.seed = 1234)  
    * identical(res5, res6)  # TRUE  
  
Next steps:  
  
* The parallel package is the baseline for many other packages  
	* Often not reproducible  
* The foreach package with doParallel and doFuture can be reproducible using doRNG  
* The future.apply package has an inutitive apply-like syntax and is always reproducible if future.seed is set  
* General best practices include  
	* Minimize overhead and master-worker communication frequencies  
    * Use scheduling and load balancing through effective use of chunking  
    * Be careful that parallel overhead can actually make tasks longer if the tasks are all small and the communication takes more time than the calculations  
  
Example code includes:  
```{r}

# In addition to the code in the previous exercise, we also created a FORK cluster for you.

# cl.fork <- makeCluster(2, type = "FORK")

# Your job is to register the two cluster objects with the preloaded doParallel package and compare results obtained with parallel foreach
# How do the results differ in terms of reproducibility?

library(doParallel)
cl.sock <- makeCluster(2, type = "PSOCK")
registerDoParallel(cl.sock)
set.seed(100)
foreach (i = 1:2) %dopar% rnorm(3)


# Register and use cl.sock
registerDoParallel(cl.sock)
replicate(2, {
    set.seed(100)
    foreach(i = 1:2, .combine = rbind) %dopar% rnorm(3)
    }, simplify = FALSE
)

# Register and use cl.fork
# registerDoParallel(cl.fork)
# replicate(2, {
#     set.seed(100)
#     foreach(i = 1:2, .combine = rbind) %dopar% rnorm(3)
#     }, simplify = FALSE
# )


# Create a cluster
cl <- makeCluster(2)

# Check RNGkind on workers
clusterCall(cl, RNGkind)

# Set the RNG seed on workers
clusterSetRNGStream(cl, iseed=100)

# Check RNGkind on workers
clusterCall(cl, RNGkind)


# Now you are ready to make your results reproducible
# You will use the simple embarrassingly parallel application for computing a mean of random numbers (myfunc) which we parallelized in the second chapter using clusterApply()
# The parallel package, myfunc() , n (sample size, set to 1000) and repl (number of replicates, set to 5) are available in your workspace
# You will now call clusterApply() repeatedly to check if results can be reproduced, without and with initializing the RNG

n <- 1000
repl <- 5

# Create a cluster of size 2
cl <- makeCluster(2)

# Call clusterApply three times
for(i in 1:3)
    print(unlist(clusterApply(cl, rep(n, repl), myfunc)))

# Create a seed object
seed <- 1234

# Repeatedly set the cluster seed and call clusterApply()
for(i in 1:3) {
    clusterSetRNGStream(cl, iseed = seed)
    print(unlist(clusterApply(cl, rep(n, repl), myfunc)))
}


# Create two cluster objects, of size 2 and 4
cl2 <- makeCluster(2)
cl4 <- makeCluster(4)

# Set seed on cl2 and call clusterApply
clusterSetRNGStream(cl2, iseed = seed)
unlist(clusterApply(cl2, rep(n, repl), myfunc))

# Set seed on cl4 and call clusterApply
clusterSetRNGStream(cl4, iseed = seed)
unlist(clusterApply(cl4, rep(n, repl), myfunc))


# Register doParallel and doRNG
library(doRNG)
registerDoParallel(cores = 2)
doRNG::registerDoRNG(seed)

# Call ar1_block via foreach
mpar <- foreach(r=1:5) %dopar% ar1_block(r)

# Register sequential backend, set seed and run foreach
registerDoSEQ()
set.seed(seed)
mseq <- foreach(r=1:5) %dorng% ar1_block(r)

# Check if results identical
identical(mpar, mseq)


# You are able to reproduce sequential and parallel applications! Remember to always use %dorng% if you use the doSEQ backend
# Also note that by default on the Linux DataCamp server, registerDoParallel() creates a FORK cluster if a number of cores is passed to it
# As a result, there was no need to export any functions to workers, as they were copied from the master
# On a different platform, the .export option may be needed


# Set multiprocess plan 
future::plan(strategy="multiprocess", workers = 2)

# Call ar1_block via future_lapply
mfpar <- future.apply::future_lapply(1:5, FUN=ar1_block, future.seed=seed)

# Set sequential plan and repeat future_lapply
future::plan(strategy="sequential")
mfseq <- future.apply::future_lapply(1:5, FUN=ar1_block, future.seed=seed)

# Check if results are identical
identical(mfpar, mfseq)


rm(mean)
rm(sd)

```
  
  
  
***
  
###_Marketing Analytics in R: Choice Modeling_  
  
Chapter 1 - Quickstart Guide  
  
Why choice?  
  
* Choice modeling (and conjoint) is a common and popular tool used in marketing  
	* Linear regression is about predicting a number based on features  
    * Frequently, though, we want to make a choose from a selection of objects (picking a show, purchasing a car, etc.)  
* Multinomial logit (logistic regressions) work well with choice data  
* Choice models can be helpful for feature selection, pricing, trade-offs between quality/cost, etc.  
  
Inspecting choice data:  
  
* Choice data does not fit in to normal predictive modeling formats  
	* For regression, data are typically one row per observation  
    * For choice datasets, data are typically stacked in to a few rows, where each row describes an alternative (rather than an observation), with a flag for which option was chosen  
* May want to count up the number of choices made (total or proportions)  
	* xtabs(choice ~ price, data=sportscar)  
  
Fitting and interpreting a choice model:  
  
* Fitting choice models is similar to fitting linear models  
	* my_model <- lm(y ~ x1 + x2 + x3, data=lm_data)  
    * library(mlogit)  # multinomial logit is needed rather than GLM  
    * mymodel <- mlogit(choice ~ feature1 + feature2 + feature3, data = choice_data)  # data must be choice data, including both ques, alt, and choice columns  
    * summary(mymodel)  
* Coefficients of magnitude greater than 1 are of very high impact in the decisions (rule of thumb)  
  
Using choice models to make decisions:  
  
* Can use the choice models to predict market shares  
    * predict_mnl(model, products)  # for mlogit models - written by instructor  
  
Example code includes:  
```{r eval=FALSE}

# Unload conflicting namespaces
unloadNamespace("rms")
unloadNamespace("quantreg")
unloadNamespace("MatrixModels")

unloadNamespace("lmerTest")
unloadNamespace("semPlot")
unloadNamespace("rockchalk")
unloadNamespace("qgraph")
unloadNamespace("sem")
unloadNamespace("mi")
unloadNamespace("arm")
unloadNamespace("mice")
unloadNamespace("mitml")
unloadNamespace("jomo")
unloadNamespace("arm")
unloadNamespace("jomo")
unloadNamespace("lme4")


# load the mlogit library 
library(mlogit)


scLong <- read.csv("./RInputFiles/sportscar_choice_long.csv")
scWide <- read.csv("./RInputFiles/sportscar_choice_wide.csv")
sportscar <- scLong
sportscar$alt <- as.factor(sportscar$alt)
sportscar$seat <- as.factor(sportscar$seat)
sportscar$price <- as.factor(sportscar$price)
sportscar$choice <- as.logical(sportscar$choice)
sportscar <- sportscar %>% rename(resp.id=resp_id)
sportscar$key <- rep(1:2000, each=3)
row.names(sportscar) <- paste(sportscar$key, sportscar$alt, sep=".")
sportscar <- mlogit.data(sportscar, shape="long", choice="choice", alt.var="alt")
str(sportscar)

# Create a table of chosen sportscars by transmission type
chosen_by_trans <- xtabs(choice ~ trans, data = sportscar)

# Print the chosen_by_trans table to the console
chosen_by_trans

# Plot the chosen_by_price object
barplot(chosen_by_trans)


# Crashes out due to issue with class "family" in MatrixModels and lme4
m1 <- mlogit(choice ~ seat + trans + convert + price, data=sportscar, seed=10)

# fit a choice model using mlogit() and assign the output to m1
# m1 <- mlogit::mlogit(choice ~ seat + trans + convert + price, 
#                      data=sportscar, 
#                      chid.var="key", 
#                      alt.var="alt", 
#                      choice="choice", 
#                      seed=10
#                      )

# summarize the m1 object to see the output of the choice model
summary(m1)


predict_mnl <- function(model, products) {
  # model: mlogit object returned by mlogit()
  # data: a data frame containing the set of designs for which you want to 
  #       predict shares.  Same format at the data used to estimate model. 
  data.model <- model.matrix(update(model$formula, 0 ~ .), data = products)[,-1]
  utility <- data.model%*%model$coef
  share <- exp(utility)/sum(exp(utility))
  cbind(share, products)
}

# inspect products
products <- data.frame(seat=factor("2", levels=c("2", "4", "5")), 
                       trans=factor(rep(c("manual", "auto"), each=2), levels=c("auto", "manual")), 
                       convert=factor(rep(c("no", "yes"), times=2), levels=c("no", "yes")), 
                       price=factor("35", levels=c("30", "35", "40"))
                       )
str(products)

# use predict_mnl to predict share for products
shares <- predict_mnl(m1, products)

# print the shares to the console
shares


barplot(shares$share, ylab="Predicted Market Share", 
        names.arg=c("Our Car", "Comp 1", "Comp 2", "Comp 3"))

```
  
  
  
***
  
Chapter 2 - Managing and Summarizing Choice Data  
  
Assembling choice data:  
  
* Choices made in the wild (revealed preference data) can be analyzed using the transaction record and product set available  
* Can instead run a survey with hypothetical decision making (conjoint)  
* Sometimes, data are provided in wide format, with sets of columns describing the choices  
  
Converting from wide to long:  
  
* Often helpful to convert the wide format data to long format instead  
	* sportscar <- reshape(sportscar_wide, direction="long", varying = list(seat=5:7, trans=8:10, convert=11:13, price=14:16), v.names = c("seat", "trans", "convert", "price"), timevar="alt")  # column labels are given in v.names; timevar="alt" means make an alt column  
    * new_order <- order(sportscar$resp_id, sportscar$ques, sportscar$alt)  
    * sportscar <- sportscar[new_order,]  # ordered by question  
    * sportscar$choice <- sportscar$choice == sportscar$alt  # make a boolean rather than an integer  
  
Choice data in two files:  
  
* Can receive choice data from two separate files - alternatives and choices  
	* sportscar <- merge(sportscar_choices, sportscar_alts, by=c("resp_id", "ques"))  
  
Visualizing choce data:  
  
* Data in long format can be summarized and visualized  
	* xtabs(~ trans, data = sportscar)  # just get the totals by transmission  
    * xtabs(~ trans + choice, data = sportscar)  # COUNT of transmission by choice  
    * xtabs(choice ~ trans, data=sportscar)  # SUM of choice by trans  
    * plot(xtabs(~ trans + choice, data=sportscar))  # mosaic plot  
    * plot(xtabs(~ trans + segment + choice, data=sportscar))  # mosaic plot split primarily by trans, then with segment and choice  
  
Designing a conjoint survey:  
  
* Conjoint surveys are popular for product design - can include any number of features  
	* Begin picking attributes of interest (commonly 8-10) and levels of interest (commonly 2-5 per level)  
* Need to decide which product profiles to show to which users and which questions  
* Can create a random design in R  
	* attribs <- list(Type=c("Milk", "Dark", "White"), Brand=c("Cadbury", "Toblerone", "Kinder"), Price=5:30/10)  
    * all_comb <- expand.grid(attribs)  
    * for (i in 1:100) {  
    *     rand_rows <- sample(1:nrow(all_comb), size=12*3)  
    *     rand_alts <- all_comb[rand_rows, ]  
    *     choc_survey[choc_survey$Subject==i, 4:6] <- rand_alts  
    * }  
* Can code the survey in html or use a platform like Google or Survey Monkey  
	* Can also use a firm like Qualtrics  
  
Example code includes:  
```{r}

chLong <- read.csv("./RInputFiles/chocolate_choice_long.csv")
chWide <- read.csv("./RInputFiles/chocolate_choice_wide.csv")
chocolate_wide <- chWide

# Look at the head() of chocolate_wide
head(chocolate_wide)

# Use summary() to see which brands and types are in chocolate_wide
summary(chocolate_wide)


# use reshape() to change the data from long to wide 
chocolate <- reshape(data= chocolate_wide , direction="long", 
                     varying = list(Brand=3:5, Price=6:8, Type=9:11), 
                     v.names=c("Brand", "Price", "Type"), timevar="Alt")
                     
# use head() to confirm that the data has been properly transformed
head(chocolate)


# Create the new order for the chocolate data frame
new_order <- order(chocolate$Subject, chocolate$Trial, chocolate$Alt)

# Reorder the chocolate data frame to the new_order
chocolate <- chocolate[new_order,]

# Look at the head() of chocolate to see how it has been reordered
head(chocolate)


# Use head(chocolate) and look at the Selection variable. 
head(chocolate)

# Transform the Selection variable to a logical indicator
chocolate$Selection <- chocolate$Alt == chocolate$Selection

# Use head(chocolate) to see how the Selection variable has changed. Now it is logical.
head(chocolate)


choc_choice <- chocolate %>%
    filter(Selection==TRUE) %>%
    mutate(Selection=Alt) %>%
    select(Subject, Trial, Response_Time, Selection)
choc_alts <- chocolate %>%
    select(Subject, Trial, Alt, Brand, Price, Type)

str(choc_choice)
str(choc_alts)


# Merge choc_choice and choc_alts
choc_merge <- merge(choc_choice, choc_alts, by=c("Subject", "Trial"))

# Convert Selection to a logical variable
choc_merge$Selection <- choc_merge$Selection == choc_merge$Alt

# Inspect chocolate_merge using head
head(choc_merge)


# Use xtabs to count up how often each Type is chosen
counts <- xtabs(~ Type + Selection, data=chocolate)

# Plot the counts
plot(counts, cex = 1.5)


# Modify this code to count up how many times each **Brand** is chosen
counts <- xtabs(~ Brand + Selection, data=chocolate)

# Plot the counts
plot(counts, cex = 1.5)


# Use xtabs to count up how often each Price is chosen
counts <- xtabs(~ Price + Selection, data=chocolate)

# Plot the counts
plot(counts, cex=0.6)

```
  
  
  
***
  
Chapter 3 - Building Choice Models  
  
Choice models - under the hood:  
  
* The multimonial logit model begins with a linear equation for utility which drives probabilities  
	* v1 <- alpha * 4 + beta * 100  # value  
    * v2 <- alpha * 5 + beta * 150  
    * v2 <- alpha * 2 + beta * 175  
    * u1 <- v1 + error1  # utility  
    * u2 <- v2 + error2  
    * u3 <- v3 + error3  
    * choice <- which.max(c(u1, u2, u3))  
    * p1 <- exp(v1) / ( exp(v1) + exp(v2) + exp(v3) )  
    * p2 <- exp(v2) / ( exp(v1) + exp(v2) + exp(v3) )  
    * p3 <- exp(v3) / ( exp(v1) + exp(v2) + exp(v3) )  
* We want to estimate the parameters that best calculate the v  
	* m1 <- mlogit(choice ~ 0 + seat + price, data=sportscar, print.level=3)  # find parameters that maximize likelihood, print all iterations as per print.level=3  
    * summary(m1)  
* The mlogit requires a specific data format  
	* sportscar <- mlogit.data(sportscar.df, shape="long", choice="choice", varying=5:8, alt.var="alt")  # what was chose, what attributes vary, and what is the alternative number column  
  
Interpreting choice model parameters:  
  
* Coefficients for each attribute are multiplied by the level of that attribute to create the v1/v2/v3  
* Factor variables are converted to numbers in the model.matrix process - need to understand what is the zero level  
	* head(model.matrix(m2))  
    * head(sportscar)  
* May want to convert to factors even if the data are numeric  
	* sportscar$seat <- as.factor(sportscar$seat)  
    * m3 <- mlogit(choice ~ 0 + seat + trans + convert + price, data=sportscar)  
    * summary(m3)  
* Can make price response non-linear (factor) or keep linear and assume Willingness to Pay  
	* coef(m3)  
    * coef(m3)/-coef(m3)[5]  # assumes that price is the 5th element of the coef vector  
  
Intercepts and interactions:  
  
* There is no intercept in the v1, v2, v3, since adding the same constant to each of them cancels out once the exponentials are taken  
	* The intercept is not identified, and so fixing the intercept using ~ 0 + is preferred (model will just pick a random one otherwise)  
* An interaction term is based on the multiplication of several terms from the original dataset  
	* m4 <- mlogit(choice ~ 0 + seat + trans + convert + price + trans:price, data=sportscar)  # interaction between transmission and price  
    * m4 <- mlogit(choice ~ 0 + seat + trans*convert + price, data=sportscar)  # same command as above, less typing  
* Interpreting the standard errors along with the z-values and p-values is similar to any other regression  
* Can also add segments, as long as they interact with at least one of the attributes  
	* m5 <- mlogit(choice ~ 0 + seat + convert + trans + price:segment, data=sportscar)  
  
Predicting shares:  
  
* Share predictions can be a good way to communicate preferences  
* Begin by creating vectors of attributes of interest  
	* price <- c(35, 30)  
    * seat <- factor(c(2, 2), levels=c(2,4,5))  
    * trans <- factor(c("manual", "auto"), levels=c("auto", "manual"))  
    * convert <- factor(c("no", "no"), levels=c("no", "yes"))  
    * segment <- factor(c("basic", "basic"), levels=c("basic", "fun", "racer"))  
    * prod <- data.frame(seat, trans, convert, price, segment)  
* Use a model prediction to make the share predictions  
	* m5 <- mlogit(choice ~ 0 + seat + convert + trans + price:segment, data=sportscar)  
    * prod.coded <- model.matrix(update(m5$formula, 0 ~ .), data = prod)[,-1]  
    * v <- prod.coded %*% m5$coef  
    * p <- exp(u) / sum(exp(u))  
    * cbind(p, prod)  
    * predict_mnl <- function(model, products) {  
    *     data.model <- model.matrix(update(model$formula, 0 ~ .), data = products)[,-1]  
    *     utility <- data.model%*%model$coef  
    *     share <- exp(utility)/sum(exp(utility))  
    *     cbind(share, products)  
    * }  
    * shares <- predict_mnl(m5, products)  
    * barplot(shares$share, horiz = TRUE, col="tomato2", xlab = "Predicted Market Share", names.arg = c("Our Sportscar", "Competitor 1"))  
  
Example code includes:  
```{r eval=FALSE}

# use mlogit.data() to convert chocolate to mlogit.data
chocolate_df <- mlogit.data(chocolate, shape = "long",
                            choice = "Selection", alt.var = "Alt", 
                            varying = 6:8)
                         
# use str() to confirm that chocolate is an mlogit.data object
str(chocolate_df)


# Fit a model with mlogit() and assign it to choc_m1
choc_m1 <- mlogit(Selection ~ Brand + Type + Price, data=chocolate_df, print.level=3)

# Summarize choc_m1 with summary()
summary(choc_m1)


# modify the call to mlogit to exclude the intercept
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data = chocolate_df, print.level=3)

# summarize the choc_m2 model
summary(choc_m2)


# compute the wtp by dividing the coefficient vector by the negative of the price coefficient
coef(choc_m2) / -coef(choc_m2)["Price"]


# change the Price variable to a factor in the chocolate data
chocolate$Price <- as.factor(chocolate$Price)

# fit a model with mlogit and assign it to choc_m3
choc_m3 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)

# inspect the coefficients
summary(choc_m3)


# likelihood ratio test comparing two models
lrtest(choc_m2, choc_m3)


# add the formula for mlogit
choc_m4 <- mlogit(Selection ~ 0 + Brand + Type + Price + Brand:Type, data=chocolate)

# use summary to see the coefficients
summary(choc_m4)


# add the formula for mlogit
choc_m5 <- mlogit(Selection ~ 0 + Brand + Type + Price + Price:Trial, data=chocolate)

# use summary to see the outputs
summary(choc_m5)


# add the formula for mlogit
choc_m5 <- mlogit(Selection ~ 0 + Brand + Type + Price + Price:Trial, data=chocolate)

# use summary to see the outputs
summary(choc_m5)


predict_mnl <- function(model, products) {
  data.model <- model.matrix(update(model$formula, 0 ~ .), 
                             data = products)[,-1]
  utility <- data.model%*%model$coef
  share <- exp(utility)/sum(exp(utility))
  cbind(share, products)
}

# modify the code below so that the segement is set to "racer" for both alternatives
price <- c(35, 30)
seat <- factor(c(2, 2), levels=c(2,4,5))
trans <- factor(c("manual", "auto"), levels=c("auto", "manual"))
convert <- factor(c("no", "no"), levels=c("no", "yes"))
segment <- factor(c("racer", "racer"), levels=c("basic", "fun", "racer"))
prod <- data.frame(seat, trans, convert, price, segment)

# predict shares for the "racer" segment
predict_mnl(model=m5, products=prod)


# fit the choc_m2 model
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)

# create a data frame with the Ghiradelli products
Brand <- factor(rep("Ghirardelli", 5), level = levels(chocolate$Brand))
Type <- levels(chocolate$Type)
Price <- 3   # treated as a number in choc_m2
ghir_choc <- data.frame(Brand, Type, Price)

# predict shares
predict_mnl(model=choc_m2, products=ghir_choc)


# compute and save the share prediction 
shares <- predict_mnl(choc_m2, ghir_choc)

# make a barplot of the shares
barplot(shares$share, 
        horiz = TRUE, col="tomato2",
        xlab = "Predicted Market Share", 
        main = "Shares for Ghiradelli chocolate bars at $3 each", 
        names.arg = levels(chocolate$Type)
        )

```
  
  
  
***
  
Chapter 4 - Hierarchical Choice Models  
  
What is a hierarchical choice model?  
  
* Hierarchical choice models (random coefficient models) account for differences in preferences across entities (heterogeneity)  
* An assumption is made that each individual is pulled from a distribution  
	* for (i in 1:n_resp) {  
    *     beta[i] <- mvrnorm(1, beta_0, Sigma)  # random normal vector  
    *     for (j in 1:n_task[i]) {  
    *         X <- X[X$resp == i & X$task == j, ]  
    *         u <- X %*% beta[i]  
    *         p[i,] <- exp(u) / sum(exp(u))  
    *     }  
    * }  
* Can fit the hierarchical model using the mlogit() function  
	* sportscar <- mlogit.data(sportscar, choice="choice", shape="long", varying=5:8, alt.var="alt", id.var = "resp_id")  
    * m7 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, rpar = c(price = "n"), panel = TRUE)  # rpar=c(price="n") will nornmally distribute price parameter across people  
    * summary(m7)  
    * plot(m7)  # plotting function for mlogit objects  
  
Heterogeneity in preferences for other features:  
  
* Might have heterogeneity in choices on many other attributes, including factor data  
* A different manner of coding factors can work better for hierarchical models  
	* Effects coding has -1, 0, 1 so that the factors are relative to average rather than relative to the first factor  
    * contrasts(sportscar$seat) <- contr.sum(levels(sportscar$seat))  # stores effects coding with the data frame  
    * dimnames(contrasts(sportscar$seat))[[2]] <- levels(sportscar$seat)[1:2]  # improve readability  
* Can make all of the coefficients heterogeneous  
	* my_rpar <- c("n", "n", "n", "n", "n")  # make them all normal  
    * m3 <- mlogit(choice ~ 0 + seat + trans + convert + price, data=sportscar)  # get coefficient names  
    * names(my_rpar) <- names(m3$coefficients)  # assign them to my_rpar  
    * m8 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, panel = TRUE, rpar = my_rpar)  # fit the model  
    * plot(m8, par=c("seat4", "seat5"))  
    * -sum(m8$coef[1:2])  # can get the 2-seat coefficient since it no longer needs to be 0  
  
Predicting shares with hierarchical models:  
  
* Can predict shares with a hierarchical model, including those where decision-making preferences may be correlated  
	* m10 <- mlogit(choice ~ 0 + seat + trans + convert + price, data = sportscar, rpar = myrpar, panel=TRUE, correlation = TRUE)  
    * cor.mlogit(m10)  
    * mean <- m10$coef[1:5] # hard coded  
    * Sigma <- cov.mlogit(m10)  
    * share <- matrix(NA, nrow=1000, ncol=nrow(prod.coded))  
    * for (i in 1:1000) {  
    *     coef <- mvrnorm(1, mu=mean, Sigma=Sigma)  
    *     utility <- prod.coded %*% coef  
    *     share[i,] <- exp(utility) / sum(exp(utility))  
    * }  
    * cbind(colMeans(share), prod)  
* Niche product shares tend to increase when heterogeneity is included - each element of the niche may be a small preference, but correlated with preference for other elements of the niche  
  
Wrap up:  
  
* Decisions need to be made in building a choice model - attributes, numeric vs. factors, hierarchical, distributions, nesting, etc.  
* Can start with basic models and build out as needed  
	* Inspect the data - investigate a few choices to confirm understanding of the data  
    * Run the model and inspect the standard errors - if too high, simplify  
    * Heterogeneity is a good idea whenever the decisions are being made by humans  
  
Example code includes:  
```{r eval=FALSE}

# Determine the number of subjects in chocolate$Subjects
length(levels(chocolate$Subject))


# add id.var input to mlogit.data call
chocolate <- mlogit.data(chocolate, choice = "Selection", shape="long", 
                         varying=6:8, alt.var = "Alt", id.var = "Subject"
                         )
                         
# add rpar and panel inputs to mlogit call
choc_m6 <- mlogit(Selection ~ 0 + Brand + Type + Price, data = chocolate, 
                  rpar = c(Price="n"), panel=TRUE)

# plot the model
plot(choc_m6)


# set the contrasts for Brand to effects code
contrasts(chocolate$Brand) <- contr.sum(levels(chocolate$Brand))
dimnames(contrasts(chocolate$Brand))[[2]] <- levels(chocolate$Brand)[1:4]
contrasts(chocolate$Brand)

# set the contrasts for Type to effects code
contrasts(chocolate$Type) <- contr.sum(levels(chocolate$Type))
dimnames(contrasts(chocolate$Type))[[2]] <- levels(chocolate$Type)[1:4]
contrasts(chocolate$Type)


# create my_rpar vector
choc_m2 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate)
my_rpar <- rep("n", length(choc_m2$coef))
names(my_rpar) <- names(choc_m2$coef)
my_rpar

# fit model with random coefficients
choc_m7 <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate, rpar=my_rpar, panel=TRUE)


# print the coefficients 
choc_m7$coef[5:8]

# compute the negative sum of those coefficients
-sum(choc_m7$coef[5:8])


# Extract the mean parameters and assign to mean
mean <- choc_m8$coef[1:9]   

# Extract the covariance parameters and assign to Sigma
Sigma <- cov.mlogit(choc_m8) 

# Create storage for individual draws of share
share <- matrix(NA, nrow=1000, ncol=nrow(choc_line_coded))

# For each draw (person)
for (i in 1:1000) { 
  # Draw a coefficient vector
  coef <- mvrnorm(1, mu=mean, Sigma=Sigma)
  # Compute utilities for those coef
  utility <- choc_line_coded %*% coef
  # Compute probabilites according to logit formuila
  share[i,] <- exp(utility) / sum(exp(utility))
}  

# Average the draws of the shares
cbind(colMeans(share), choc_line)

```
  
  
  
***
  
###_Single-Cell RNA-Seq Workflows in R_  
  
Chapter 1 - What Is RNA Single-Cell RNA-Seq?  
  
Background and utility:  
  
* Can get gene expression data at the cellular level - allows for better resolution of gene expressions  
	* Previous methods would get a mix of gener expressions across many cells - better for averages and distributions, but cannot identify a specific observation or cell type  
    * Implications are significant for personalized medicine and related areas  
* The data structure from the lab is a matrix - geners are rows and cells are columns  
	* ATCG counts by intersection  
    * Gene-level covariates  
    * Cell-level covariates  
* There are many zeroes - gene not expressed in cell, or dropouts (technical errors)  
  
Typical workflow:  
  
* There has been an exponential scaling in the ability to extract RNA data from 2009 - 2018  
	* Full-range technologies try to capture the full RNA sequence  
    * Tide-based (?) technologies try to capture the two ends of an RNA technology  
* Quality control is the process of removing problematic cells - library size and cell coverage metrics  
	* Library size is the total number of reads assigned to each cell  
    * Coverage is the total number of cells with reads assigned for that gene  
* Workflows then include normalization, dimensionality reduction, clustering, DE analysis (biomarkers with differential expression)  
  
Load, create, and access data:  
  
* The SingleCellExperiment (SCE) class is an S4 class for storing data from single-cell experiments  
	* source("https://bioconductor.org/biocLite.R")  
    * biocLite("SingleCellExperiment")  
    * library(SingleCellExperiment)  
    * counts <- matrix(rpois(8, lambda = 10), ncol = 2, nrow = 4)  
    * rownames(counts) <- c("Lamp5", "Fam19a1", "Cnr1", "Rorb")  
    * colnames(counts) <- c("SRR2140028", "SRR2140022")  
    * counts  
* Can create SCE data using the constructor  
	* sce <- SingleCellExperiment(assays = list(counts = counts), rowData = data.frame(gene = rownames(counts)), colData = data.frame(cell = colnames(counts)))  
* Can create SCE data using coercion  
	* se <- SummarizedExperiment(assays = list(counts = counts))  
    * sce <- as(se, "SingleCellExperiment")  
* Can apply process to a real dataset  
	* library(scRNAseq); data(allen)  # subset of mouse visual coretex data  
  
Example code includes:  
```{r eval=FALSE}

# head of count matrix
counts[1:3, 1:3]

# count of specific gene and cell
alignedReads <- counts['Cnr1', "SRR2140055"]

# overall percentage of zero counts 
pZero <- mean(counts == 0)

# cell library size
libsize <- colSums(counts)


# find cell coverage
coverage <- colMeans(counts > 0)
cell_info$coverage <- coverage

# load ggplot2
library(ggplot2)

# plot cell coverage
ggplot(cell_info, aes(x = names, y = coverage)) + 
  geom_point() +
  ggtitle('Cell Coverage') + 
  xlab('Cell Name') + 
  ylab('Coverage')


# mean of GC content
gc_mean <- mean(gene_info$gc)

# standard deviation of GC content
gc_sd <- sd(gene_info$gc)

# boxplot of GC content 
boxplot(gene_info$gc, main = 'Boxplot - GC content', ylab = 'GC content')


# batch
batch <- cell_info$batch

# patient
patient <- cell_info$patient

# nesting of batch within patient
batch_patient <- table(batch = batch, patient = patient)

# explore batch_patient
batch_patient


# load SingleCellExperiment
library(SingleCellExperiment)

# create a SingleCellExperiment object
sce <- SingleCellExperiment(assays = list(counts = counts ),
                            rowData = data.frame(gene_names = rownames(counts)),
                            colData = data.frame(cell_names = colnames(counts)))


# create a SummarizedExperiment object from counts
se <- SummarizedExperiment(assays = list(counts = counts))

# create a SingleCellExpression object from se
sce <- as(se, "SingleCellExperiment")


# create SingleCellExperiment object
sce <- as(allen, "SingleCellExperiment")

# cell information
cell_info <- colData(sce)

# size factors
sizeFactors(sce) <- colSums(assay(sce))

```
  
  
  
***
  
Chapter 2 - Quality Control and Normalization  
  
Quality Control:  
  
* Need to remove low quality cells and genes - identify first  
* Tung dataset includes three replicates  
	* sce  
    * library(scater)  
    * sce <- calculateQCMetrics(sce, feature_controls = list(ERCC = isSpike(sce, "ERCC"))  
* ERCC spiking genes are used for quality control - filter out cells with improper ratios (usually too much due to dead or stressed or the like) of "spiking"  
* Key functions used in the exercises include  
	* calculateQCMetrics()  
    * counts()  
    * rowSums()  
    * grepl()  
    * isSpike()  
    * plot(density(x))  
    * abline()  
  
Quality Control (continued):  
  
* Can filter based on library sizes using  
	* threshold <- 20000  
    * plot(density(sce$total_counts), main = "Density - total_counts")  
    * abline(v = threshold)  
    * keep <- (sce$total_counts > threshold)  
    * table(keep)  
* Can look at plots of the data  
	* scater::plotPhenoData(sce, aes_string(x = "total_counts", y = "total_counts_ERCC", colour = "batch"))  
* Can then filter based on data that do not meet key criteria in the plot  
	* keep <- (sce$batch != "NA19098.r2")  
    * table(keep)  
    * filter_genes <- apply(counts(sce), 1, function(x) length(x[x >= 2] >= 2)  # keep genes with counts of 2+ in at least 2+ cells  
    * table(filter_genes)  
  
Normalization:  
  
* Want to group cells based on their gene expression profiles; target with different drugs, for example  
* Technical artifacts can be introduced in the data  
	* Batch effect is a common problem with the data - cells may cluster by batch, even if they are from the same entity  
    * Can normalize based on dividing by library size (multiplied if needed to get count per million - CPM)  
* Exercise will use the following functions  
	* plotPCA()  
    * reducedDim(sce, "PCA")[, 1:2]  
    * computeSumFactors()  
    * sizeFactors()  
    * assays()  
    * normalize()  
    * plotRLE()  
  
Example code includes:  
```{r eval=FALSE}

# remove genes with only zeros
nonZero <- counts(sce) > 0
keep <- rowSums(nonZero) > 0
sce_2 <- sce[keep, ]

# spike-ins ERCC
isSpike(sce_2, "ERCC") <- grepl("^ERCC-", rownames(sce_2))


# load scater
library(scater)

# calculate QCs
sce <- calculateQCMetrics(sce, feature_controls = list(ERCC = isSpike(sce, "ERCC")))

# explore coldata of sce
colData(sce)


# set threshold
threshold <- 20000

# plot density
plot(density(sce@colData$total_counts), main = 'Density - total_counts')
abline(v = threshold)

# keep cells
keep <- sce@colData$total_counts > threshold

# tabulate kept cells
table(keep)


# set threshold
threshold <- 6000

# plot density
plot(density(sce$total_features), main = 'Density - total_features')
abline(v=threshold)

# keep cells
keep <- sce$total_features > threshold

# tabulate kept cells
table(keep)


#extract cell data into a data frame
cDataFrame <- as.data.frame(colData(sce))

# plot cell data
ggplot(cDataFrame, aes(x = total_counts, y = total_counts_ERCC, col = batch)) + 
  geom_point()

# keep cells
keep <- sce$batch != "NA19098.r2"

# tabulate kept cells
table(keep)


# load SingleCellExperiment
library(SingleCellExperiment)

# filter genes
filter_genes <- apply(counts(sce), 1, function(x){
  length(x[x > 1]) > 1
})

# tabulate the results of filter_genes
table(filter_genes)


# PCA raw counts
plotPCA(sce, exprs_values = "counts",
    colour_by = "batch", shape_by = "individual")

# PCA log counts
plotPCA(sce, exprs_values = "logcounts_raw",
        colour_by = "batch", shape_by = "individual")


#find first 2 PCs
pca <- reducedDim(sce, "PCA")[, 1:2]

#create cdata
cdata <- data.frame(PC1 = pca[, 1],
                    libsize = sce$total_counts,
                    batch = sce$batch)

#plot pc1 versus libsize
ggplot(cdata, aes(x = PC1, y = libsize, col = batch)) +
  geom_point()


# load scran
library(scran)

# find size factors
sce <- computeSumFactors(sce)

# display size factor histogram
hist(sizeFactors(sce))


# view assays
assays(sce)

# normalize sce
normalized_sce <- normalize(sce)

# view new assay for normalized logcounts
assays(normalized_sce)

```
  
  
  
***
  
Chapter 3 - Visualization and Dimensionality Reduction  
  
Mouse Epithelium Dataset:  
  
* Goal is to reduce the number of dimensions (from number of genes to something much smaller)  
* Mouse olfactory cell dataset - epithelium stem cell differentiation  
	* Dimensionlaity reduction makes for smaller data with preservation of signal much more so than noise  
  
Visualization:  
  
* Can visualize datasets using dimensionality reduction through any of several methods - PCA, tSNE, ZIFA, ZINB-WaVE  
	* plotPCA(sce, exprs_values = "logcounts", shape_by = "Batch", colour_by = "publishedClusters")  # los help reduce bias towards highly expressed genes  
    * plotTSNE(sce, exprs_values = "logcounts", shape_by = "Batch", colour_by = "publishedClusters", perplexity = 5)  # perplexity is a guess about the kNN parameter  
  
Dimensionality Reduction:  
  
* Can find the most variable genes using magrittr  
	* library(magrittr)  
    * vars <- assay(sce) %>% log1p %>% rowVars  
    * names(vars) <- rownames(sce)  
    * vars <- sort(vars, decreasing = TRUE)  
    * head(vars)  
    * sce_sub <- sce[names(vars[1:50]),]  
    * sce_sub  
* Can run dimensionality reduction using PCA  
	* logcounts <- log1p(assay(sce_sub))  
    * pca <- prcomp(t(logcounts))  
    * reducedDims(sce_sub) <- SimpleList(PCA = pca$x)  
    * sce_sub  
    * head(reducedDim(sce_sub, "PCA")[, 1:2])  
* Can then plot the PCA components  
	* pca <- reducedDim(sce_sub, "PCA")[, 1:2]  
    * col <- colData(sce)[, c("publishedClusters", "batch")]  
    * df <- cbind(pca, col)  
    * ggplot(df, aes(x = PC1, y = PC2, col = publishedClusters, shape = batch)) +  
    *     geom_point()  
  
Example code includes:  
```{r eval=FALSE}

# find dimensions
mydims <- dim(sce)

# extract cell and gene names
cellNames <- colnames(sce)
geneNames <- rownames(sce)


# cell data
cData <- colData(sce)

#print column names
colnames(cData)

# table batch & clusters
cData <- cData[, c('Batch', 'publishedClusters')]

#tabulate cData
table(cData)


# load scater
library(scater)

# plot pc1 and pc2 counts
plotPCA(
    object = sce,
    exprs_values = "counts",
    shape_by = "Batch",
    colour_by = "publishedClusters"
)


# explore initial assays
assays(sce)

# create log counts
logcounts <- log1p(assays(sce)$counts)

# add log counts
assay(sce, 'logcounts') <- logcounts
assays(sce)

# pca log counts
plotPCA(object = sce, exprs_values = "logcounts",
    shape_by = "Batch", colour_by = "publishedClusters")


# default tSNE
plotTSNE(
    sce,
    exprs_values = "counts",
    shape_by = "publishedClusters",
    colour_by = "Batch",
    perplexity = 5
)


# gene variance 
vars <- assay(sce) %>% log1p() %>% rowVars() 

#rename vars
names(vars) <- rownames(sce)

#sort vars
vars_2 <- sort(vars, decreasing = TRUE)
head(vars_2)

# subset sce 
sce_sub <- sce[names(vars[1:50]), ]
sce_sub


# log counts
logcounts <- log1p(assays(sce_sub)$counts)

# transpose
tlogcounts <- t(logcounts)

# perform pca
pca <- prcomp(tlogcounts)

# store pca matrix in sce
reducedDims(sce_sub) <- SimpleList(PCA = pca$x)
head(reducedDim(sce_sub, "PCA")[, 1:2])


# Extract PC1 and PC2 and create a data frame
pca <- reducedDim(sce_sub, "PCA")[, 1:2]
col_shape <- data.frame(publishedClusters = colData(sce)$publishedClusters, Batch = factor(colData(sce)$Batch))
df <- cbind(pca, col_shape)

# plot PC1, PC2
ggplot(df, aes(x = PC1, y = PC2, 
            colour = publishedClusters, 
            shape = Batch)) + 
  geom_point()

```
  
  
  
***
  
Chapter 4 - Cell Clustering and Differential Expression Analysis  
  
Clustering methods for scRNA-Seq:  
  
* Continuing to use the mouse epithelium dataset - cells color coded by cluster as per previous chapters  
* One of the goals of clustering is to group cells with similar gene expression, allowing for finding patterns in gene expression  
	* Hierarchical clustering  
    * k-means clustering  
* Challenges include setting the number of clusters, scalability to large datasets, etc.  
* Can begin by creating the Seurat object  
	* library(Seurat)  
    * library(SingleCellExperiment)  
    * seuset <- CreateSeuratObject(  
    *     raw.data = assay(sce),  
    *     normalization.method = "LogNormalize",  
    *     scale.factor = 10000,  
    *     meta.data = as.data.frame(colData(sce))  
    * )  
    * seuset <- ScaleData(object = seuset)  
    * seuset  
* Can then perform clustering on the seuset object  
	* seuset <- FindClusters( object = seuset, reduction.type = "pca", dims.use = 1:10, resolution = 1.8, print.output = FALSE )  
    * PCAPlot( object = seuset, group.by = "ident", pt.shape = "publishedClusters" )  
  
Differential expression analysis:  
  
* Differential expression (DE) analysis is to find differential expression of genes in various cells  
	* Methods include SCDE, MAST, edgeR, DESeq2, etc.  
* Can fit a MAST model using function zlm  
	* library(MAST)  
    * zlm <- zlm(~ celltype + cngeneson, sce)   
    * summary <- summary(zlm, doLRT = "celltype9")  
    * summary  
    * fit <- summary$datatable  
    * fit <- merge(fit[contrast=='celltype9' & component=='H', .(primerid, `Pr(>Chisq)`)],      fit[contrast=='celltype9' & component=='logFC', .(primerid, coef)], by='primerid')  
    * fit[, padjusted:=p.adjust(`Pr(>Chisq)`, 'fdr')]  
    * res = data.frame(gene = fit$primerid, pvalue = fit[,'Pr(>Chisq)'], padjusted = fit$padj, logFC = fit$coef)  
    * head(res)  
  
Visualization of DE genes:  
  
* Visualization is typically the final step of single-cell analysis  
* The volcano plot looks at fold-change and p-values simultaneously  
	* ggplot(res, aes(x=logFC, y=-log10(padjusted), color=mostDE)) + geom_point() +  
    *     ggtitle("Volcano") + xlab("log2 FC") + ylab("-log10 adjusted p-value")  
* Can also look at results of DE using a heatmap  
	* library(NMF)  
    * norm <- assay(sce[mostDE, ], "logcounts")  
    * norm <- as.matrix(norm)  
    * aheatmap(norm, annCol = colData(sce)$publishedClusters)  
* Course covered the typical workflow for analysis of single-cell RNA sequencing data  
	* Normalization  
    * Dimensionality reduction  
    * Clustering  
    * Differential expression analysis  
  
Example code includes:  
```{r eval=FALSE}

# load Seurat
library(Seurat)

#create seurat object
seuset <- CreateSeuratObject(
    raw.data = assay(sce),
    normalization.method = "LogNormalize", 
    scale.factor = 10000,
    meta.data = as.data.frame(colData(sce))
)

# scale seuset object
scaled_seuset <- ScaleData(object = seuset)


# perform pca
seuset <- RunPCA(
    object = seuset, 
    pc.genes = rownames(seuset@raw.data), 
    do.print = FALSE
)
# plot pca
PCAPlot(object = seuset,
        pt.shape = 'Batch',
        group.by = 'publishedClusters')


# load MAST
library(MAST)

# SingleCellAssay object 
sca

# fit zero-inflated regression 
zlm <- zlm(~ celltype + cngeneson, sca) 

# summary with likelihood test ratio
summary <- summary(zlm, doLRT = "celltype9")


# get summary table
fit <- summary$datatable

# pvalue df
pvalue <- fit[contrast == 'celltype9' & component == 'H', .(primerid, `Pr(>Chisq)`)]
  
# logFC df
logFC <- fit[contrast == 'celltype9' & component == 'logFC', .(primerid, coef)]

# pvalues and logFC
fit <- merge(pvalue, logFC, by = 'primerid')


# adjusted pvalues
fit[, padjusted:=p.adjust(`Pr(>Chisq)`, 'fdr')]

# result table
res <- data.frame(gene = fit$primerid,
                 pvalue = fit[,'Pr(>Chisq)'],
                 padjusted = fit$padj,
                 logFC = fit$coef)


# most DE 
res <- res[order(res$padjusted), ]
mostDE <- res$gene[1:20]
res$mostDE <- res$gene %in% mostDE

# volcano plot
ggplot(res, aes(x=logFC, y=-log10(padjusted), color=mostDE)) +
  geom_point() +
  ggtitle("Volcano plot") +
  xlab("log2 fold change") + 
  ylab("-log10 adjusted p-value")


# load NMF
library(NMF)

# normalize log counts
norm <- assay(sce[mostDE, ], "logcounts")
mat <- as.matrix(norm)

# heatmap
aheatmap(mat, annCol = colData(sce)$publishedClusters)

```
  
  
  
***
  
###_Differential Expression Analysis in R with limma_  
  
Chapter 1 - Differential Expression Analysis  
  
Differential expression analysis:  
  
* Analysis of data from functional genomics experiments  
* Example of having treated cells with phenotypes A and B  
	* The features can be genes or proteins or other molecular features of the cell - proxy for relative abundance (such as RNA)  
    * Upregulated - higher expression level  
    * Downregulated - lower expression level  
* Objectives are to look for novelty (genes that play an unexpected role), context (interpreting relevance of gene behaviors), systems level understanding (simultaneous allows for looking at pathways)  
* Many caveats to the analysis - study design is very important  
  
Differential expression data:  
  
* Data will be from the breast cancer data and CLL data - testing for differences in two groups of people within each  
	* x - Expression matrix  
    * f - Feature data - genes or proteins  
    * p - Phenotype data - description of each of the samples  
* Can begin by looking at the boxplot for a single gene  
	* boxplot(x[1, ] ~ p[, "er"], main = f[1, "symbol"])  
  
ExpressionSet class:  
  
* Data management can become precarious, especially when filtering and subsetting  
* Object-oriented programming can help - the class can hold the data, and has methods/functions that work specially on objectes of that class  
	* Accessors (getters) get the data  
    * Setters modify the stored data  
    * source("https://bioconductor.org/biocLite.R")  
    * biocLite("Biobase")  
    * library(Biobase)  
    * eset <- ExpressionSet(assayData = x, phenoData = AnnotatedDataFrame(p), featureData = AnnotatedDataFrame(f))  
* Can access data from an ExpressionSet object  
	* x <- exprs(eset)  
    * f <- fData(eset)  
    * p <- pData(eset)  
    * eset_sub <- eset[1000, 1:10]  
    * boxplot(exprs(eset)[1, ] ~ pData(eset)[, "er"], main = fData(eset)[1, "symbol"])  
  
The limma package:  
  
* Advantages of the limma package include replacing boiler-plate code and improved inference by sharing across genes (do not assume full independene - all from same experiment)  
	* Method used is empirical Bayes - convenience and better statistics (especially for smaller data sets)  
    * Good functions for pre-processing and post-processing  
* Specifying a linear model - Y = B0 + B1 * X1 + epsilon where Y is the expression level of the gene, B0 is the mean in ER- tumors, and B1 is the mean difference in expression in ER+ tumors  
	* model.matrix(~<explanatory>, data = <data frame>)  
    * design <- model.matrix(~er, data = pData(eset))  
    * colSums(design)  
* Can then test the design matrix using the standard limma pipeline  
	* library(limma)  
    * fit <- lmFit(eset, design)  
    * fit <- eBayes(fit)  
    * results <- decideTests(fit[, "er"])  
    * summary(results)  # -1 will be downregulated and +1 will be upregulated  
  
Example code includes:  
```{r eval=FALSE}

# Create a boxplot of the first gene in the expression matrix
boxplot(x[1, ] ~ p[, "Disease"], main = f[1, "symbol"])


# Load package
library(Biobase)

# Create ExpressionSet object
eset <- ExpressionSet(assayData = x,
                      phenoData = AnnotatedDataFrame(p),
                      featureData = AnnotatedDataFrame(f))

# View the number of features (rows) and samples (columns)
dim(eset)


# Subset to only include the 1000th gene (row) and the first 10 samples
eset_sub <- eset[1000, 1:10]

# Check the dimensions of the subset 
dim(eset_sub)

# Create a boxplot of the first gene in eset_sub
boxplot(exprs(eset_sub)[1, ] ~ pData(eset_sub)[, "Disease"],
        main = fData(eset_sub)[1, "symbol"])


# Create design matrix for leukemia study
design <- model.matrix(~Disease, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Calculate the t-statistics
fit <- eBayes(fit)

# Summarize results
results <- decideTests(fit[, "Diseasestable"])
summary(results)

```
  
  
  
***
  
Chapter 2 - Flexible Models for Common Study Designs  
  
Flexible linear models:  
  
* The models can be extended, for example to Y = B0 + B1*X1 + B2*X2 + eps  
	* This model is known as a treatment-contrast model  
* Can instead use a group-means parameterization - Y = B1*X1 + B2*X2 + eps  
	* Can then test whether B1-B2 == 0 since the intercept was exluded  
    * design <- model.matrix(~0 + er, data = pData(eset))  
    * cm <- limma::makeContrasts(status = erpositive - ernegative, levels = design)  # erpositive-ernegative means multiply erpositive by 1 and multiply ernegative by -1  
    * fit <- lmFit(eset, design)  # per above  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  # for the contrasts method  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
    * summary(results)  
  
Studies with more than two groups:  
  
* Dataset for this example has groups with leukemia types ALL, AML, CML - 20172x36 (12 leukemias of each type)  
* Desire to build a group-means model - Y = B1*X1 + B2*X2 + B3*X3 + eps  
	* design <- model.matrix(~0 + type, data = pData(eset))  
    * cm <- limma::makeContrasts(AMLvALL = typeAML - typeALL, CMLvALL = typeCML - typeALL, CMLvAML = typeCML - typeAML, levels = design)  
    * fit <- lmFit(eset, design)  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
* Exercises will look at gene expressions of stem cells grown in states of hypoxia  
  
Factorial experimental design:  
  
* Factorial designs look at every combination of experimental variables - for example, if there is a 2x2, then there would be 4 combinations examined  
* Example of 2x2 study in plants of types col, vte2 for temperatures of high, low - 11871 x 12  
* Can run the group-means model with the zero intercept; need to first create the type-temperature variable using paste  
	* group <- with(pData(eset), paste(type, temp, sep = "."))  
    * group <- factor(group)  # records the unique levels  
    * design <- model.matrix(~0 + group)  
    * colnames(design) <- levels(group)  
* May want to assess the interaction effect (difference in impact of temperature by type) as well as the direct effects (impact of temperature on a specific type, impact for same temperature across types)  
	* cm <- makeContrasts(type_normal = vte2.normal - col.normal, type_low = vte2.low - col.low, temp_vte2 = vte2.low - vte2.normal, temp_col = col.low - col.normal, interaction = (vte2.low - vte2.normal) - (col.low - col.normal), levels = design)  
    * fit <- lmFit(eset, design)  
    * fit2 <- contrasts.fit(fit, contrasts = cm)  
    * fit2 <- eBayes(fit2)  
    * results <- decideTests(fit2)  
  
Example code includes:  
```{r eval=FALSE}

# Create design matrix with no intercept
design <- model.matrix(~0 + Disease, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(status = Diseaseprogres. - Diseasestable, levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)


# Create design matrix with no intercept
design <- model.matrix(~0 + oxygen, data = pData(eset))

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(ox05vox01 = oxygenox05 - oxygenox01,
                    ox21vox01 = oxygenox21 - oxygenox01,
                    ox21vox05 = oxygenox21 - oxygenox05,
                    levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)


# Create single variable
group <- with(pData(eset), paste(type, water, sep = "."))
group <- factor(group)

# Create design matrix with no intercept
design <- model.matrix(~0 + group)
colnames(design) <- levels(group)

# Count the number of samples modeled by each coefficient
colSums(design)


# Load package
library(limma)

# Create a contrasts matrix
cm <- makeContrasts(type_normal = nm6.normal - dn34.normal,
                    type_drought = nm6.drought - dn34.drought,
                    water_nm6 = nm6.drought - nm6.normal,
                    water_dn34 = dn34.drought - dn34.normal,
                    interaction = (nm6.drought - nm6.normal) - (dn34.drought - dn34.normal),
                    levels = design)

# View the contrasts matrix
cm


# Load package
library(limma)

# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)

```
  
  
  
***
  
Chapter 3 - Pre-processing and post-processing  
  
Normalizing and filtering:  
  
* Need to convert raw data to analysis-ready data - generic approach for first-pass to new dataset  
	* Log transformation  
    * Quantile normalization  
    * Filtering  
* Can start with visualization of densities using the limma package  
	* limma::plotDensities(eset, legend = FALSE)  
    * exprs(eset) <- log(exprs(eset))  # log transform  
    * limma::plotDensities(eset, legend = FALSE)  
* Want to remove technical artifacts using quantile normalization  
	* exprs(eset) <- normalizeBetweenArrays(exprs(eset))  
    * limma::plotDensities(eset, legend = FALSE)  
    * abline(v = 5)  # from visualization, 5 may be a cutoff where the data should be kept  
    * keep <- rowMeans(exprs(eset)) > 5  
    * eset <- eset[keep, ]  
    * plotDensities(eset, legend = FALSE)  
  
Accounting for technical batch effects:  
  
* Technical batch effects are artifacts arising from differences in experiments - true for all experiment types, including functional genomics  
	* Need to balance variables of interest across batches - cannot just take type a from batch a and type b from type b and see if there are differences  
    * PCA and other dimension reduction techniques can help identify technical batch effects  
    * limma::plotMDS(eset, labels = pData(eset)[, "time"], gene.selection = "common")  
* Removing batch effects is also possible in limma  
	* exprs(eset) <- limma::removeBatchEffect(eset, batch = pData(eset)[, "batch"], covariates = pData(eset)[, "rin"])  
    * limma::plotMDS(eset, labels = pData(eset)[, "time"], gene.selection = "common")  
* For statistical analysis, it is better to include batch as a coefficient for analysis rather than to run the remove batch effect process  
  
Visualizing results:  
  
* Can inspect the results and visualize using limma  
	* results <- decideTests(fit2)  
    * topTable(fit2, number = 3)  
    * stats <- topTable(fit2, number = nrow(fit2), sort.by = "none")  
* Under the null hypothesis of no impact, the p-values should be uniformly distributed  
	* hist(runif(10000))  
    * hist(stats[, "P.Value"])  # should have many values near zero if there is an actual impact  
* Can examine results using a Volcano plot  
	* volcanoplot(fit2, highlight = 5, names = fit2$genes[, "symbol"])  
  
Enrichment testing:  
  
* Can use curated biological databases as a reference point  
* Can use the Fisher's exact test  
	* fisher.test(matrix(c(10, 100, 90, 900), nrow = 2))  
* Can also test for KEGG (reference set) enrichment, which requires a common ID  
	* entrez <- fit2$genes[, "entrez"]  
    * enrich_kegg <- kegga(fit2, geneid = entrez, species = "Hs")  # Hs is homo sapiens  
    * topKEGG(enrich_kegg, number = 3)  
* Can also test for GO (reference set) enrichment  
	* enrich_go <- goana(fit2, geneid = entrez, species = "Hs")  
    * topGO(enrich_go, ontology = "BP", number = 3)  
  
Example code includes:  
```{r eval=FALSE}

# Load package
library(limma)

# View the distribution of the raw data
plotDensities(eset, legend = FALSE)

# Log tranform
exprs(eset) <- log(exprs(eset))
plotDensities(eset, legend = FALSE)

# Quantile normalize
exprs(eset) <- normalizeBetweenArrays(exprs(eset))
plotDensities(eset, legend = FALSE)


# Load package
library(limma)

# View the normalized gene expression levels
plotDensities(eset, legend = FALSE); abline(v = 5)

# Determine the genes with mean expression level greater than 5
keep <- rowMeans(exprs(eset)) > 5
sum(keep)

# Filter the genes
eset <- eset[keep, ]
plotDensities(eset, legend = FALSE)


# Load package
library(limma)

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")

# Plot principal components labeled by batch
plotMDS(eset, labels = pData(eset)[, "batch"], gene.selection = "common")


# Load package
library(limma)

# Remove the batch effect
exprs(eset) <- removeBatchEffect(eset, batch = pData(eset)[, "batch"])

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")

# Plot principal components labeled by batch
plotMDS(eset, labels = pData(eset)[, "batch"], gene.selection = "common")


# Obtain the summary statistics for every gene
stats <- topTable(fit2, number = nrow(fit2), sort.by = "none")

# Plot a histogram of the p-values
hist(stats[, "P.Value"])


# Create a volcano plot. Highlight the top 5 genes
volcanoplot(fit2, highlight = 5, names = fit2$genes$symbol)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched KEGG Pathways
enrich_kegg <- kegga(fit2, geneid = entrez, species = "Hs")

# View the top 20 enriched KEGG pathways
topKEGG(enrich_kegg, number=20)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched GO categories
enrich_go <- goana(fit2, geneid = entrez, species = "Hs")

# View the top 20 enriched GO Biological Processes
topGO(enrich_go, ontology = "BP", number=20)

```
  
  
  
***
  
Chapter 4 - Case Study: Effect of Doxorubicin Treatment  
  
Pre-process data:  
  
* Doxorubicin is a commonly-prescribed cancer drug, with a strong side effect of cariotoxicity  
* Hypothesis for the MOA is that top2B is involved, and was tested in mice  
	* Wild mice tested against top2b null mice, each given DOX and a placebo  
    * The eset data is 29532 x 12 (3 replicates per combination of 2x2 factors)  
* Begin by inspecting and then pre-processing the data  
	* plotDensities(eset, group = pData(eset)[, "genotype"], legend = "topright")  
* Prep-processing steps include log-transform, quantile transform, and filter  
  
Model the data:  
  
* Can look at the main clusters of mice vs. treatments (top2b null seem to cluster together)  
	* plotMDS(eset, labels = pData(eset)[, "genotype"], gene.selection = "common")  
    * plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")  
* Follow the model.matrix process to run a differential expression analysis  
	* Contrasts will include wild to dox, top2b to dox, interaction effect of wild vs. top2b to dox  
    * Can also run hypothesis tests using the limma pipeline and a Venn diagram  
  
Inspect the results:  
  
* Initial results seem to support the hypothesis that top2b is the main vector for cardiotoxicity  
* The limma function topTable can be run, with a sepcified contrast  
	* coef = "dox_wt"  
    * coef = "dox_top2b"  
    * coef = "interaction"  
* Can use the volcanoplot for x-axis of log-fold change and y-axis for the log-odds of differential expression  
	* kegga and topKEGG functions  
    * species = "Mm"  # common house mouse  
  
Wrap up:  
  
* Pre-processing and visualization of gene data  
* Principal components analysis and plotMDS()  
* Fitting a group-means model for more interpretable contrasts  
* Investigation of p-values for uniformity vs. skew for low p-values  
* Use of volcanoplots  
* Testing for enrichment of differentially expressed genes  
  
Example code includes:  
```{r eval=FALSE}

# Log transform
exprs(eset) <- log(exprs(eset))
plotDensities(eset,  group = pData(eset)[, "genotype"], legend = "topright")

# Quantile normalize
exprs(eset) <- normalizeBetweenArrays(exprs(eset))
plotDensities(eset,  group = pData(eset)[, "genotype"], legend = "topright")

# Determine the genes with mean expression level greater than 0
keep <- rowMeans(exprs(eset)) > 0
sum(keep)

# Filter the genes
eset <- eset[keep, ]
plotDensities(eset, group = pData(eset)[, "genotype"], legend = "topright")


# Find the row which contains Top2b expression data
top2b <- which(fData(eset)["symbol"] == "Top2b")

# Plot Top2b expression versus genotype
boxplot(exprs(eset)[top2b, ] ~ pData(eset)[, "genotype"], main = fData(eset)[top2b, ])


# Plot principal components labeled by genotype
plotMDS(eset, labels = pData(eset)[, "genotype"], gene.selection = "common")

# Plot principal components labeled by treatment
plotMDS(eset, labels = pData(eset)[, "treatment"], gene.selection = "common")


# Create single variable
group <- with(pData(eset), paste(genotype, treatment, sep = "."))
group <- factor(group)

# Create design matrix with no intercept
design <- model.matrix(~0 + group)
colnames(design) <- levels(group)

# Count the number of samples modeled by each coefficient
colSums(design)


# Create a contrasts matrix
cm <- makeContrasts(dox_wt = wt.dox - wt.pbs,
                    dox_top2b = top2b.dox - top2b.pbs,
                    interaction = (top2b.dox - top2b.pbs) - (wt.dox - wt.pbs),
                    levels = design)

# View the contrasts matrix
cm


# Fit the model
fit <- lmFit(eset, design)

# Fit the contrasts
fit2 <- contrasts.fit(fit, contrasts = cm)

# Calculate the t-statistics for the contrasts
fit2 <- eBayes(fit2)

# Summarize results
results <- decideTests(fit2)
summary(results)

# Create a Venn diagram
vennDiagram(results)


# Obtain the summary statistics for the contrast dox_wt
stats_dox_wt <- topTable(fit2, coef = "dox_wt", number = nrow(fit2), sort.by = "none")
# Obtain the summary statistics for the contrast dox_top2b
stats_dox_top2b <- topTable(fit2, coef = "dox_top2b", number = nrow(fit2), sort.by = "none")
# Obtain the summary statistics for the contrast interaction
stats_interaction <- topTable(fit2, coef = "interaction", number = nrow(fit2), sort.by = "none")

# Create histograms of the p-values for each contrast
hist(stats_dox_wt[, "P.Value"])
hist(stats_dox_top2b[, "P.Value"])
hist(stats_interaction[, "P.Value"])


# Extract the gene symbols
gene_symbols <- fit2$genes[, "symbol"]

# Create a volcano plot for the contrast dox_wt
volcanoplot(fit2, coef = "dox_wt", highlight = 5, names = gene_symbols)

# Create a volcano plot for the contrast dox_top2b
volcanoplot(fit2, coef = "dox_top2b", highlight = 5, names = gene_symbols)

# Create a volcano plot for the contrast interaction
volcanoplot(fit2, coef = "interaction", highlight = 5, names = gene_symbols)


# Extract the entrez gene IDs
entrez <- fit2$genes[, "entrez"]

# Test for enriched KEGG Pathways for contrast dox_wt
enrich_dox_wt <- kegga(fit2, coef = "dox_wt", geneid = entrez, species = "Mm")

# View the top 5 enriched KEGG pathways
topKEGG(enrich_dox_wt, number = 5)

# Test for enriched KEGG Pathways for contrast interaction
enrich_interaction <- kegga(fit2, coef = "interaction", geneid = entrez, species = "Mm")

# View the top 5 enriched KEGG pathways
topKEGG(enrich_interaction, number = 5)

```
  
  
  
***
  
###_Interactive Data Visualization with bokeh_  
  
Chapter 1 - rbokeh Introduction  
  
Getting started with rbokeh:  
  
* rbokeh is the R interface to the Python Bokeh plot package - interactive and informative for end users  
* Data manipulation and pre-processing are needed, with tidyverse being integral for this course  
* Can run an example using the gapminder dataset  
	* library(gapminder)  
    * data_2002 <- gapminder %>% filter(year == 2002)  
    * gapminder_mod <- gapminder %>% mutate(pop_millions = pop/10^6)  
  
Layers for rbokeh:  
  
* The rbokeh plot is initialized using figure() with layers added using the pipe operator (note the contrast with the plus used in ggplot2)  
	* data_rwanda <- gapminder %>% filter(country == "Rwanda")  
    * figure() %>% ly_lines(x = year, y = lifeExp, data = data_rwanda)  
    * figure() %>% ly_lines(x = data_rwanda$year, y = data_rwanda$lifeExp)  # same output, but with different axis labels  
* Can look at the ggplot2::economics dataset  
	* plot_pop <- figure() %>% ly_lines(x = date, y = pop, data = economics)  
    * plot_pop  
  
Layers for rbokeh (continued):  
  
* There are many layers in rbokeh, and they all begin with ly_...()  
* Example for creating a one-layer plot  
	* dat_1982 <- gapminder %>% filter(year == 1982)  
    * figure() %>% ly_points(x = gdpPercap, y = lifeExp, data = dat_1982)  
* Example for creating a multi-layer plot - note that the data argument can be added to figure, and will inherit to the succeeding ly_() ommands  
	* data_oceania <- gapminder %>% filter(continent == "Oceania")  
    * figure(data = data_oceania, legend_location = "bottom_right") %>% ly_lines(x = year, y = gdpPercap , color = country) %>% ly_points(x = year, y = gdpPercap, color = country)  
  
Example code includes:  
```{r eval=FALSE}

## load rbokeh, gapminder and dplyr libraries
library(rbokeh)
library(gapminder)
library(dplyr)


## explore gapminder dataset 
str(gapminder)

## filter gapminder data by year 1982
dat_1982 <- gapminder %>% filter(year == 1982)


## plot life expectancy Vs GDP per Capita using data_1982
figure(legend_location = "bottom_right", title = "Life Expectancy Vs. GDP per Capita in 1982") %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_1982, 
              color = continent, hover = c(continent, country, pop)
              )


## filter the dataset for the continent Africa and and year 1967
data_africa <- gapminder %>% 
  filter(year==1967, continent=="Africa")
  
## view data_africa
data_africa


## plot life expectancy Vs GDP per Capita using data_africa   
figure(legend_location = "bottom_right",
       title = "Life Expectancy Vs. GDP per Capita in Africa - 1967"
       ) %>% 
       ly_points(x = gdpPercap, y = lifeExp, data = data_africa, hover = c(country, pop))


## add a new column with gdp in millions
gapminder_mill <- gapminder %>% 
  mutate(gdp_millions = gdpPercap * pop / 10^6)
  
## view the first 6 entries in gapminder after adding  gdp_millions
head(gapminder_mill)

## extract the entries for "Rwanda"
data_rwanda <- gapminder_mill %>% 
  filter(country=="Rwanda")

## explore data_rwanda
data_rwanda


## plot gdp over time
figure(data = data_rwanda) %>% 
    ly_lines(x = year, y = gdp_millions, width = 2)


## explore the economics dataset
data(economics)
str(economics)

## pass vectors to x & y
figure() %>%
  ly_lines(x = economics$date, y = economics$pce)

## pass columns names and dataframe
figure() %>%
  ly_lines(x = date, y = pce, data = economics)


## plot unemployment rate  versus time and change the default `ylab`
figure(ylab = "unemployment %") %>%
  ly_lines(x=date, y=100*unemploy/pop, data=economics)


dat_1992 <- gapminder %>%
    filter(year==1992)
str(dat_1992)

## plot lifeExp Vs. gdpPercap using rbokeh
plot_1992<- figure(legend_location = "bottom_right") %>%
  ly_points(x=gdpPercap, y=lifeExp, color=continent, data=dat_1992) 

## show the plot            
plot_1992


data_countries <- gapminder %>%
    filter(country %in% c("United Kingdom", "Australia", "Canada", "United States", "New Zealand"))
str(data_countries)

figure(data = data_countries, legend="top_left") %>% 
  ly_lines(x = year, y = gdpPercap , color = country) %>% 
  ly_points(x=year, y=gdpPercap, color=country)


data_countries <- gapminder %>% 
    filter(country %in% c("China", "India"))

## create a line plot with lifeExp vs. year 
fig_countries <- figure(legend="top_left") %>% 
  ly_lines(x=year, y=lifeExp, color=country, data=data_countries)


## View fig_countries
fig_countries

## modify fig_countries by adding a points layer with gdpPercap vs. year 
fig_countries %>% 
  ly_points(x=year, y=lifeExp, color=country, data=data_countries)

```
  
  
  
***
  
Chapter 2 - rbokeh Aesthetic Attributes and Figure Options  
  
Plot and Managed Attributes (Part I):  
  
* Can use aestehtic to modify areas like color, transparency, line type, shape, and the like  
	* figure(legend_location = "bottom_right", title = "Life Expectancy Vs. GDP per Capita in 1992" ) %>% ly_points(x = gdpPercap, y = lifeExp, data = dat_1992, color = continent)  
* Can use the Human Development Index (HDI) data from UNDP  
	* hdi_countries <- hdi_data %>% filter(country %in% c("Hungary", "Bulgaria", "Poland"))  
    * fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% ly_lines(x = year, y = human_development_index, color = country) %>% ly_points(x = year, y = human_development_index, color = country)  
* Can have varying color attributes by function - ly_points() has both fill_color() and line_color() which will both inherit from color  
	* Can make the fill_color explicit and set its alpha (default is 0.5) explicitly also  
    * fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% ly_points(x = year, y = human_development_index, fill_color = country, fill_alpha = 1) %>% ly_lines(x = year, y = human_development_index, color = country)  
* Can add a custom color palette also  
	* fig_col %>% set_palette(discrete_color = pal_color(c("#3182bd", "#31a354", "#de2d26")))  
  
Plot and Managed Attributes (Part II):  
  
* Bechdel dataset - movie data on finances and exclusion of women  
	* Bechdel criteria is a movie where two+ women have a discussion that is not about a male character  
    * figure() %>% ly_points(x = budget_2013, y = intgross_2013, data = dat_90_13)  # has an over-plotting problem and needs a log-transform  
    * figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13)  # log transform helps with a lot (but not all) of the over-plotting  
    * figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13, alpha = 0.4, size = 5)  # improves readability  
* May want to change the line widths for many countries  
	* hdi_countries <- hdi_data %>% filter(country %in% c("Rwanda", "Kenya", "Botswana"))  
    * figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country)  
    * Can use the width parameter to control the line width in ly_lines()  
    * (WRONG FROM VIDEO) figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country, size = 3)  
    * figure(title = "Human Development Index over Time", legend = "bottom_right") %>% ly_lines(x = year, y = human_development_index, data = hdi_countries, color = country, width = 3)  
  
Hover Info and Figure Options:  
  
* Can combine the HDI and the CPI (corruption perception index)  
	* The hover() argument added to the ly_points() can allow for hovering - see below  
    * figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = c(country, cpi_rank))  
* Can customize the hover commands using hover= where the @ means to place a variable from the frame at that point  
	* figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = "CPI Rank: @cpi_rank")  
    * Can also use basic html such as <b></b> and <br></br>  
    * figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015") %>% ly_points(x = corruption_perception_index, y = human_development_index, data = hdi_cpi_2015, color = continent, size = 7, hover = "<b>@country</b><br><b>CPI Rank</b>: @cpi_rank")  
* Can further add axis limits to the bokeh plots  
	* hdi_cpi_scatter <- figure(legend_location = "bottom_right", title = "CPI versus HDI - 2015", ylim = c(0, 1), xlab = "CPI", ylab = "HDI", theme = bk_ggplot_theme()) %>% ly_points(x = corruption_perception_index_score, y = human_development_index, data = hdi_cpi_data, color = continent, size = 7)  
  
Example code includes:  
```{r eval=FALSE}

hdiRaw <- read.csv("./RInputFiles/Human Development Index (HDI).csv", skip=1)
str(hdiRaw)
hdi_data <- hdiRaw %>% 
    gather(key="year", value="human_development_index", -Country, -`HDI.Rank..2017.`) %>%
    mutate(country=str_trim(as.character(Country)), year=as.integer(str_sub(year, 2))) %>%
    filter(year %in% 1990:2105) %>%
    select(country, year, human_development_index)
str(hdi_data)

## extract "Namibia" and "Botswana" entries from hdi_data
hdi_countries <- hdi_data %>% 
    filter(country %in% c("Namibia", "Botswana"))
  
## plot human_development_index versus year
fig_col <- figure(data = hdi_countries, legend_location = "bottom_right") %>% 
    ly_lines(x = year, y = human_development_index, color = country) %>% 
    ly_points(x = year, y = human_development_index, 
              fill_color = "white", fill_alpha = 1,
              line_color = country, line_alpha = 1,
              size = 4
              )

## view plot 
fig_col


## use a custom palette with colors "green", "red"
fig_col %>% 
  set_palette(discrete_color = pal_color(c("green", "red")))

## define custom palette   
custom_pal <- pal_color(c("#c51b8a", "#31a354"))

## use custom_pal yp modify fig_col
fig_col %>% 
    set_palette(discrete_color=custom_pal)


## explore bechdel dataset using str
data(bechdel, package="fivethirtyeight")
str(bechdel)

## extract entries between 1980 - 2013
dat_80_13 <- bechdel %>% 
  filter(between(year, 1980, 2013))

dat_80_13 <- dat_80_13 %>% 
  mutate(roi_total = intgross_2013 / budget_2013) 
  
## plot
figure() %>% 
  ly_points(x=log(budget_2013), y=log(roi_total), data=dat_80_13)

## plot log(roi_total) versus log(budget_2013)
figure() %>% 
  ly_points(x=log(budget_2013), y=log(roi_total), size=5, line_alpha=0, fill_alpha=0.3, data=dat_80_13)


## filter the data by country = Syrian Arab Republic
hdi_countries <- hdi_data %>% 
  filter(country %in% c("Syrian Arab Republic", "Morocco"))

## change the color and line width
figure(title = "Human Development Index over Time", legend = "bottom_right") %>% 
    ly_lines(x=year, y=human_development_index, color=country, width=3, data=hdi_countries)


# explore hdi_cpi_data dataset
# str(hdi_cpi_2015)

## add multiple values as hover info (country, cpi_rank)
# figure(legend_location = "bottom_right") %>% 
#     ly_points(x=corruption_perception_index, y=human_development_index, color=continent, hover=c(country, cpi_rank), size=6, data=hdi_cpi_2015)


## modify the figure theme 
# figure(title = "Corruption Perception Index Vs. Human Development Index 2015",
#        legend_location = "bottom_right", xgrid = FALSE, ygrid = FALSE, 
#        xlab = "CPI", ylab = "HDI", theme=bk_ggplot_theme()) %>% 
#     ly_points(x = corruption_perception_index, y = human_development_index, 
#               data = hdi_cpi_2015, color = continent, size = 6, hover = c(country, cpi_rank)
#               )

```
  
  
  
***
  
Chapter 3 - Data Manipulation for Visualization and More rbokeh Layers  
  
Data Formats:  
  
* The proper data format for plotting can make rbokeh much easier  
* Frequently need to transform data from long format to wide format for easier plotting - tidyr will help (inverse of the gather function)  
	* hdi_cpi_wide <- hdi_cpi_long %>% spread(key = index, value = value)  
* May also want to transform data from wide format to long format, for example if time is a column  
	* hdi_data_long <- hdi_data_wide %>% gather(key = year, value = human_development_index, - country)  # year will become a new column, -country means leave country as its own column  
  
More rbokeh Layers:  
  
* Can create a scatter plot with a regression line as an added layer  
	* dat_90_13 <- bechdel %>% filter(between(year, 1990, 2013))  
    * p_scatter <- figure() %>% ly_points(x = log(budget_2013), y = log(intgross_2013), data = dat_90_13, size = 5, alpha = 0.4)  
    * lin_reg <- lm(log(intgross_2013) ~ log(budget_2013), data = dat_90_13)  
    * summary(lin_reg)  
    * p_scatter %>% ly_abline(lin_reg)  # plots with the abline of the regression (the regression line) to the figure  
  
Interaction Tools:  
  
* Can use the interaction tool to pan, zoom, reset, and the like  
	* figure(tools=c("pan", "wheel_zoom", "box_zoom", "reset", "save", "help"), toolbar_location="right")  
* Tools can be any of "pan", "wheel_zoom", "box_zoom", "resize", "crosshair", "box_select", "lasso_select", "reset", "save", "help"  
	* Location can be any of 'above', 'below', 'left', 'right', NULL (remove the toolbar)  
* Example of customizing the available tools  
	* figure(tools = c("pan", "wheel_zoom", "box_zoom"), toolbar_location = "above", legend_location = "bottom_right", ylim = c(0, 100)) %>% ly_points(x = gdpPercap, y = lifeExp, data = gapminder_2002, color = continent, size = 6, alpha = 0.7)  
* Can create a plot and then use the widget2png tool to convert to PNG  
	* plot_scatter <- figure(title = "Life Expectancy Vs. GDP per Capita in 2002", legend_location = "bottom_right") %>% ly_points(x = gdpPercap, y = lifeExp, data = gapminder_2002)  
    * widget2png(p = plot_scatter, file = "plot_scatter.png")  
* Can also save as html  
	* rbokeh2html(fig = plot_scatter, file = "plot_scatter_interactive.html")  
    * browseURL("plot_scatter_interactive.html")  
  
Example code includes:  
```{r eval=FALSE}

ctry <- c('Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia', 'Austria', 'Bahrain', 'Bangladesh', 'Belgium', 'Benin', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Eritrea', 'Ethiopia', 'Finland', 'France', 'Gabon', 'Gambia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kenya', 'Kuwait', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Malaysia', 'Mali', 'Mauritania', 'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Rwanda', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Sierra Leone', 'Singapore', 'Slovenia', 'South Africa', 'Spain', 'Sri Lanka', 'Sudan', 'Sweden', 'Switzerland', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'United Kingdom', 'United States', 'Uruguay', 'Zambia', 'Zimbabwe', 'Afghanistan', 'Albania', 'Algeria', 'Angola', 'Argentina', 'Australia', 'Austria', 'Bahrain', 'Bangladesh', 'Belgium', 'Benin', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Costa Rica', 'Croatia', 'Cuba', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Eritrea', 'Ethiopia', 'Finland', 'France', 'Gabon', 'Gambia', 'Germany', 'Ghana', 'Greece', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jordan', 'Kenya', 'Kuwait', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Madagascar', 'Malawi', 'Malaysia', 'Mali', 'Mauritania', 'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Norway', 'Oman', 'Pakistan', 'Panama', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Romania', 'Rwanda', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Sierra Leone', 'Singapore', 'Slovenia', 'South Africa', 'Spain', 'Sri Lanka', 'Sudan', 'Sweden', 'Switzerland', 'Thailand', 'Togo', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Uganda', 'United Kingdom', 'United States', 'Uruguay', 'Zambia', 'Zimbabwe')
ctryCode <- c('AFG', 'ALB', 'DZA', 'AGO', 'ARG', 'AUS', 'AUT', 'BHR', 'BGD', 'BEL', 'BEN', 'BIH', 'BWA', 'BRA', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR', 'CAN', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 'CRI', 'HRV', 'CUB', 'CZE', 'DNK', 'DJI', 'DOM', 'ECU', 'EGY', 'SLV', 'ERI', 'ETH', 'FIN', 'FRA', 'GAB', 'GMB', 'DEU', 'GHA', 'GRC', 'GTM', 'GIN', 'GNB', 'HTI', 'HND', 'HUN', 'ISL', 'IND', 'IDN', 'IRQ', 'IRL', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KEN', 'KWT', 'LBN', 'LSO', 'LBR', 'LBY', 'MDG', 'MWI', 'MYS', 'MLI', 'MRT', 'MUS', 'MEX', 'MNG', 'MON', 'MAR', 'MOZ', 'MMR', 'NAM', 'NPL', 'NLD', 'NZL', 'NIC', 'NER', 'NGA', 'NOR', 'OMN', 'PAK', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'ROM', 'RWA', 'STP', 'SAU', 'SEN', 'SCG', 'SLE', 'SGP', 'SVN', 'ZAF', 'ESP', 'LKA', 'SDN', 'SWE', 'CHE', 'THA', 'TGO', 'TTO', 'TUN', 'TUR', 'UGA', 'GBR', 'USA', 'URY', 'ZMB', 'ZWE', 'AFG', 'ALB', 'DZA', 'AGO', 'ARG', 'AUS', 'AUT', 'BHR', 'BGD', 'BEL', 'BEN', 'BIH', 'BWA', 'BRA', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR', 'CAN', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 'CRI', 'HRV', 'CUB', 'CZE', 'DNK', 'DJI', 'DOM', 'ECU', 'EGY', 'SLV', 'ERI', 'ETH', 'FIN', 'FRA', 'GAB', 'GMB', 'DEU', 'GHA', 'GRC', 'GTM', 'GIN', 'GNB', 'HTI', 'HND', 'HUN', 'ISL', 'IND', 'IDN', 'IRQ', 'IRL', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KEN', 'KWT', 'LBN', 'LSO', 'LBR', 'LBY', 'MDG', 'MWI', 'MYS', 'MLI', 'MRT', 'MUS', 'MEX', 'MNG', 'MON', 'MAR', 'MOZ', 'MMR', 'NAM', 'NPL', 'NLD', 'NZL', 'NIC', 'NER', 'NGA', 'NOR', 'OMN', 'PAK', 'PAN', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'ROM', 'RWA', 'STP', 'SAU', 'SEN', 'SCG', 'SLE', 'SGP', 'SVN', 'ZAF', 'ESP', 'LKA', 'SDN', 'SWE', 'CHE', 'THA', 'TGO', 'TTO', 'TUN', 'TUR', 'UGA', 'GBR', 'USA', 'URY', 'ZMB', 'ZWE')
regn <- c('AP', 'ECA', 'MENA', 'SSA', 'AME', 'AP', 'WE/EU', 'MENA', 'AP', 'WE/EU', 'SSA', 'ECA', 'SSA', 'AME', 'WE/EU', 'SSA', 'SSA', 'AP', 'SSA', 'AME', 'SSA', 'SSA', 'AME', 'AP', 'AME', 'SSA', 'AME', 'WE/EU', 'AME', 'WE/EU', 'WE/EU', 'SSA', 'AME', 'AME', 'MENA', 'AME', 'SSA', 'SSA', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'WE/EU', 'SSA', 'WE/EU', 'AME', 'SSA', 'SSA', 'AME', 'AME', 'WE/EU', 'WE/EU', 'AP', 'AP', 'MENA', 'WE/EU', 'MENA', 'WE/EU', 'AME', 'AP', 'MENA', 'SSA', 'MENA', 'MENA', 'SSA', 'SSA', 'MENA', 'SSA', 'SSA', 'AP', 'SSA', 'SSA', 'SSA', 'AME', 'AP', 'ECA', 'MENA', 'SSA', 'AP', 'SSA', 'AP', 'WE/EU', 'AP', 'AME', 'SSA', 'SSA', 'WE/EU', 'MENA', 'AP', 'AME', 'AME', 'AME', 'AP', 'WE/EU', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'MENA', 'SSA', 'ECA', 'SSA', 'AP', 'WE/EU', 'SSA', 'WE/EU', 'AP', 'MENA', 'WE/EU', 'WE/EU', 'AP', 'SSA', 'AME', 'MENA', 'ECA', 'SSA', 'WE/EU', 'AME', 'AME', 'SSA', 'SSA', 'AP', 'ECA', 'MENA', 'SSA', 'AME', 'AP', 'WE/EU', 'MENA', 'AP', 'WE/EU', 'SSA', 'ECA', 'SSA', 'AME', 'WE/EU', 'SSA', 'SSA', 'AP', 'SSA', 'AME', 'SSA', 'SSA', 'AME', 'AP', 'AME', 'SSA', 'AME', 'WE/EU', 'AME', 'WE/EU', 'WE/EU', 'SSA', 'AME', 'AME', 'MENA', 'AME', 'SSA', 'SSA', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'WE/EU', 'SSA', 'WE/EU', 'AME', 'SSA', 'SSA', 'AME', 'AME', 'WE/EU', 'WE/EU', 'AP', 'AP', 'MENA', 'WE/EU', 'MENA', 'WE/EU', 'AME', 'AP', 'MENA', 'SSA', 'MENA', 'MENA', 'SSA', 'SSA', 'MENA', 'SSA', 'SSA', 'AP', 'SSA', 'SSA', 'SSA', 'AME', 'AP', 'ECA', 'MENA', 'SSA', 'AP', 'SSA', 'AP', 'WE/EU', 'AP', 'AME', 'SSA', 'SSA', 'WE/EU', 'MENA', 'AP', 'AME', 'AME', 'AME', 'AP', 'WE/EU', 'WE/EU', 'WE/EU', 'SSA', 'SSA', 'MENA', 'SSA', 'ECA', 'SSA', 'AP', 'WE/EU', 'SSA', 'WE/EU', 'AP', 'MENA', 'WE/EU', 'WE/EU', 'AP', 'SSA', 'AME', 'MENA', 'ECA', 'SSA', 'WE/EU', 'AME', 'AME', 'SSA', 'SSA')
cnt <- c('Asia', 'Europe', 'Africa', 'Africa', 'Americas', 'Oceania', 'Europe', 'Asia', 'Asia', 'Europe', 'Africa', 'Europe', 'Africa', 'Americas', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Americas', 'Africa', 'Africa', 'Americas', 'Asia', 'Americas', 'Africa', 'Americas', 'Europe', 'Americas', 'Europe', 'Europe', 'Africa', 'Americas', 'Americas', 'Africa', 'Americas', 'Africa', 'Africa', 'Europe', 'Europe', 'Africa', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Africa', 'Africa', 'Americas', 'Americas', 'Europe', 'Europe', 'Asia', 'Asia', 'Asia', 'Europe', 'Asia', 'Europe', 'Americas', 'Asia', 'Asia', 'Africa', 'Asia', 'Asia', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Asia', 'Africa', 'Africa', 'Africa', 'Americas', 'Asia', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Asia', 'Europe', 'Oceania', 'Americas', 'Africa', 'Africa', 'Europe', 'Asia', 'Asia', 'Americas', 'Americas', 'Americas', 'Asia', 'Europe', 'Europe', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Europe', 'Africa', 'Asia', 'Europe', 'Africa', 'Europe', 'Asia', 'Africa', 'Europe', 'Europe', 'Asia', 'Africa', 'Americas', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Americas', 'Africa', 'Africa', 'Asia', 'Europe', 'Africa', 'Africa', 'Americas', 'Oceania', 'Europe', 'Asia', 'Asia', 'Europe', 'Africa', 'Europe', 'Africa', 'Americas', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Americas', 'Africa', 'Africa', 'Americas', 'Asia', 'Americas', 'Africa', 'Americas', 'Europe', 'Americas', 'Europe', 'Europe', 'Africa', 'Americas', 'Americas', 'Africa', 'Americas', 'Africa', 'Africa', 'Europe', 'Europe', 'Africa', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Africa', 'Africa', 'Americas', 'Americas', 'Europe', 'Europe', 'Asia', 'Asia', 'Asia', 'Europe', 'Asia', 'Europe', 'Americas', 'Asia', 'Asia', 'Africa', 'Asia', 'Asia', 'Africa', 'Africa', 'Africa', 'Africa', 'Africa', 'Asia', 'Africa', 'Africa', 'Africa', 'Americas', 'Asia', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Asia', 'Europe', 'Oceania', 'Americas', 'Africa', 'Africa', 'Europe', 'Asia', 'Asia', 'Americas', 'Americas', 'Americas', 'Asia', 'Europe', 'Europe', 'Europe', 'Africa', 'Africa', 'Asia', 'Africa', 'Europe', 'Africa', 'Asia', 'Europe', 'Africa', 'Europe', 'Asia', 'Africa', 'Europe', 'Europe', 'Asia', 'Africa', 'Americas', 'Africa', 'Europe', 'Africa', 'Europe', 'Americas', 'Americas', 'Africa', 'Africa')
idx <- rep(c("corruption_perception_index", "human_development_index"), each=121)
cpiRk <- c(166, 88, 88, 163, 106, 13, 16, 50, 139, 15, 83, 76, 29, 76, 69, 76, 150, 150, 130, 10, 145, 147, 23, 83, 83, 136, 40, 50, 56, 38, 1, 98, 102, 106, 88, 72, 154, 102, 3, 23, 98, 123, 11, 56, 58, 123, 139, 158, 158, 111, 50, 13, 76, 88, 161, 18, 32, 61, 69, 18, 45, 139, 55, 123, 61, 83, 161, 123, 111, 54, 95, 111, 45, 111, 72, 61, 88, 111, 147, 45, 130, 9, 1, 130, 98, 136, 5, 60, 117, 72, 130, 88, 95, 29, 28, 58, 43, 66, 48, 61, 71, 119, 7, 34, 61, 37, 83, 165, 4, 6, 76, 106, 72, 76, 66, 139, 11, 16, 21, 76, 150, 166, 88, 88, 163, 106, 13, 16, 50, 139, 15, 83, 76, 29, 76, 69, 76, 150, 150, 130, 10, 145, 147, 23, 83, 83, 136, 40, 50, 56, 38, 1, 98, 102, 106, 88, 72, 154, 102, 3, 23, 98, 123, 11, 56, 58, 123, 139, 158, 158, 111, 50, 13, 76, 88, 161, 18, 32, 61, 69, 18, 45, 139, 55, 123, 61, 83, 161, 123, 111, 54, 95, 111, 45, 111, 72, 61, 88, 111, 147, 45, 130, 9, 1, 130, 98, 136, 5, 60, 117, 72, 130, 88, 95, 29, 28, 58, 43, 66, 48, 61, 71, 119, 7, 34, 61, 37, 83, 165, 4, 6, 76, 106, 72, 76, 66, 139, 11, 16, 21, 76, 150)
vl <- c(0.479, 0.764, 0.745, 0.533, 0.827, 0.939, 0.893, 0.824, 0.579, 0.896, 0.485, 0.75, 0.698, 0.754, 0.794, 0.402, 0.404, 0.563, 0.518, 0.92, 0.352, 0.396, 0.847, 0.738, 0.727, 0.498, 0.776, 0.827, 0.775, 0.878, 0.925, 0.473, 0.722, 0.739, 0.691, 0.68, 0.42, 0.448, 0.895, 0.897, 0.697, 0.452, 0.926, 0.579, 0.866, 0.64, 0.414, 0.424, 0.493, 0.625, 0.836, 0.921, 0.624, 0.689, 0.649, 0.923, 0.899, 0.887, 0.73, 0.903, 0.742, 0.555, 0.8, 0.763, 0.497, 0.427, 0.716, 0.512, 0.476, 0.789, 0.442, 0.513, 0.781, 0.762, 0.735, 0.807, 0.647, 0.418, 0.556, 0.64, 0.558, 0.924, 0.915, 0.645, 0.353, 0.527, 0.949, 0.796, 0.55, 0.788, 0.693, 0.74, 0.682, 0.855, 0.843, 0.802, 0.498, 0.574, 0.847, 0.494, 0.776, 0.42, 0.925, 0.89, 0.666, 0.884, 0.766, 0.49, 0.913, 0.939, 0.74, 0.487, 0.78, 0.725, 0.767, 0.493, 0.91, 0.92, 0.795, 0.579, 0.516, 11, 36, 36, 15, 32, 79, 76, 51, 25, 77, 37, 38, 63, 38, 41, 38, 21, 21, 27, 83, 24, 22, 70, 37, 37, 26, 55, 51, 47, 56, 91, 34, 33, 32, 36, 39, 18, 33, 90, 70, 34, 28, 81, 47, 46, 28, 25, 17, 17, 31, 51, 79, 38, 36, 16, 75, 61, 44, 41, 75, 53, 25, 49, 28, 44, 37, 16, 28, 31, 50, 35, 31, 53, 31, 39, 44, 36, 31, 22, 53, 27, 84, 91, 27, 34, 26, 88, 45, 30, 39, 27, 36, 35, 63, 64, 46, 54, 42, 52, 44, 40, 29, 85, 60, 44, 58, 37, 12, 89, 86, 38, 32, 39, 38, 42, 25, 81, 76, 74, 38, 21)

hdi_cpi_data_long <- data.frame(country=ctry, year=2015L, country_code=ctryCode, cpi_rank=cpiRk, 
                                region=regn, continent=cnt, index=idx, value=vl,
                                stringsAsFactors = FALSE
                                )

## explore hdi_cpi_data_long
str(hdi_cpi_data_long)

## How many unique values are there in the index column?
unique(hdi_cpi_data_long$index)


## convert from long to wide
hdi_cpi_data_wide <- hdi_cpi_data_long %>% 
  spread(key=index, value=value)
  
## display the first 5 rows from hdi_cpi_data_wide
head(hdi_cpi_data_wide, 5)


## plot corruption_perception_index  versus human_development_index
figure(legend_location = "top_left") %>% 
    ly_points(x=human_development_index, y=corruption_perception_index, color=continent, alpha=0.7,
              hover=c(country, cpi_rank,corruption_perception_index, human_development_index), 
              data=hdi_cpi_data_wide
              )


## convert from wide to long
hdi_cpi_remake_long <- hdi_cpi_data_wide %>%
    gather(key="index", value="value", corruption_perception_index, human_development_index)
  
## display the first 5 rows of hdi_data_long
head(hdi_cpi_remake_long, 5)
all.equal(hdi_cpi_data_long, hdi_cpi_remake_long)


## explore the unique values in the movie_budget column
# unique(dat_90_13_long$movie_budget)

## spread the values in the `movie_budget` in two columns
# dat_90_13_wide <- dat_90_13_long %>% 
#   spread(key=movie_budget, value=value)
  
## View column names of dat_90_13_wide
# names(dat_90_13_wide)

## create a scatter plot with log(budget_2013) Vs log(intgross_2013) 
# p_scatter <- figure() %>%
#   ly_points(y=log(intgross_2013), x=log(budget_2013), size=4, alpha=0.5, data=dat_90_13_wide)
  
## View plot
# p_scatter

## fit a linear reg model
# lin_reg <- lm(log(intgross_2013) ~ log(budget_2013), data = dat_90_13)

## add the linear regression line layer to p_scatter
# p_scatter %>% 
#   ly_abline(lin_reg)


## extract entries for year 2007
dat_2007 <- gapminder %>% 
  filter(year == 2007)
dat_2002 <- gapminder %>% 
  filter(year == 2002)

## create scatter plot
figure(toolbar_location="above", legend_location="bottom_right") %>%
    ly_points(x=gdpPercap, y=lifeExp, color=continent, size=6, alpha=0.7, 
              data=dat_2007, hover=c(country, lifeExp, gdpPercap)
              )

figure(legend_location = "bottom_right", tools=c("resize", "save")) %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_2002, color = continent)

figure(legend_location = "bottom_right", tools=c("resize", "save"), toolbar_location=NULL) %>% 
    ly_points(x = gdpPercap, y = lifeExp, data = dat_2002, color = continent)

```
  
  
  
***
  
Chapter 4 - Grid Plots and Maps  
  
Intro to Grid Plots:  
  
* Example of dataset for TB by year by age group in the US - combine multiple figures in the same area  
	* figure() %>% ly_bar(x = year, y = count, data = tb_2534, color = gender, position = "stack")  # x should be a factor, default is stacked bars if position is missing (dodge or fill also available)  
* Can use the grid plot basics for multi-plotting  
	* fig_list <- list(bar_2534 = bar_2534, bar_3544 = bar_3544)  
    * grid_plot(fig_list, width = 1000, height = 500)  # will have different axis limits by default  
    * grid_plot(fig_list, width = 1000, height = 500, same_axes = TRUE)  # forces the axes to be on the same scale  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  # overrides the defaults of all in one row  
    * fig_list <- list(list(bar_1524 = bar_1524, bar_2534 = bar_2534), list(bar_3544 = bar_3544))  
    * grid_plot(fig_list, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  # list of plots plots one list per row, and NULL will place an empty plot in that column  
  
Facets with Grid Plots:  
  
* Facets can be helpful for slicing data, placing small batches of the data (segmented by factor) in a larger plot  
* Can start by creating a plot for each of the groups, though this is inefficient  
	* fig_list <- list(bar_1524 = bar_1524, bar_2534 = bar_2534, bar_3544 = bar_3544, bar_4554 = bar_4554, bar_5564 = bar_5564, bar_65 = bar_65)  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  
* Can instead use the split() function and a function for plotting to create the relevant lists  
	* tb_split_age <- split(tb, tb$age)  
    * plot_bar <- function(x){ figure() %>% ly_bar(y = count, year, data = x, color = gender, position = "dodge")}  
* Can instead use the lapply() functionality  
	* fig_list <- lapply(tb_split_age, plot_bar)  
    * grid_plot(fig_list, width = 900, height = 600, nrow = 2, same_axes = TRUE) %>% theme_axis("x", major_label_orientation = 90)  
  
rbokeh maps:  
  
* Can create interactive maps using rbokeh - transportation, density, population, and other areas where geography is key to understanding  
* NYC bike data available from the bikedata package  
* Can begin by initializing a map (appears to source Google Maps)  
	* ny_map <- gmap(lat = 40.73306, lng = -73.97351, zoom = 11)   
* Can use the map type argument to change the type of map - hybrid, satellite, road, terrain  
* Can also customize maps using gamp_style(), such as making the water blue  
	* ny_map <- gmap(lat = 40.73306, lng = -73.97351, zoom = 11, map_style = gmap_style("blue_water"))   
* Can then add points layers to the map and arrange in grids using grid_plot() as per previous chapters  
	* ny_map %>% ly_points(x = station_longitude, y = station_latitude, data = ny_bikedata_20170427, fill_color = start_count, line_alpha = 0, size = 8, hover = c(station_name, start_count))  
    * grid_plot(list(weekeend_April23 = map_weekend_20170423, weekday_April25 = map_weekday_20170425), width = 860, height = 420)  
  
Example code includes:  
```{r eval=FALSE}

tb <- data.frame(iso2="US", 
                 gender=rep(c("m", "f"), each=84),
                 year=factor(c(1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008)), 
                 age=c(1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 2534, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 3544, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 4554, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 5564, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65), 
                 count=c(355, 333, 330, 321, 331, 365, 320, 343, 365, 362, 383, 388, 414, 375, 876, 815, 701, 663, 616, 602, 613, 562, 526, 547, 535, 568, 490, 513, 1417, 1219, 1127, 1009, 1011, 906, 824, 813, 754, 728, 666, 659, 572, 495, 1121, 1073, 979, 1007, 930, 904, 876, 795, 828, 829, 767, 759, 744, 725, 742, 678, 679, 628, 601, 577, 524, 490, 487, 504, 499, 531, 533, 526, 1099, 1007, 944, 914, 801, 738, 649, 592, 650, 582, 624, 596, 562, 561, 280, 289, 269, 269, 232, 246, 239, 233, 277, 265, 241, 257, 257, 220, 579, 487, 449, 425, 391, 376, 410, 423, 353, 339, 348, 384, 338, 329, 499, 478, 447, 424, 394, 349, 346, 362, 310, 302, 276, 263, 260, 269, 285, 279, 254, 267, 245, 253, 247, 255, 269, 252, 242, 212, 225, 224, 202, 217, 201, 179, 244, 152, 176, 167, 169, 166, 161, 146, 135, 172, 591, 541, 514, 492, 444, 396, 389, 370, 354, 344, 322, 303, 308, 300), 
                 stringsAsFactors = FALSE
                 )
str(tb)
tb_2534 <- tb %>% filter(age==2534)
str(tb_2534)


## create a bar plot for age group tb_2534
bar_2534 <- figure() %>%
  ly_bar(x=year, y=count, color=gender, data=tb_2534, hover=TRUE)

## View figure
bar_2534


## create a bar plot for age group tb_2534 with % on the y-axis
bar_2534_percent <- figure(ylab = "share") %>% 
  ly_bar(x = year, y =  count,  tb_2534, color = gender, hover = TRUE,  position = "fill")
         
## View figure
bar_2534_percent


## create a list with bar_2534 and bar_2534_percent figures
fig_list <- list(bar_2534 = bar_2534, bar_2534_percent = bar_2534_percent)

## create a grid plot 
grid_plot(fig_list, width=1000, height=400)

## create a grid plot with same axes limits
grid_plot(figs = fig_list, width = 1000, height = 400, same_axes=TRUE)


plot_line <- function(x){
    figure() %>% 
        ly_lines(y =  count, year, data = x, color = age,  alpha = 1, width = 2)
}

## create two dataframes for female/male data           
tb_female <- tb %>% filter(gender=="f")
tb_male <- tb %>% filter(gender=="m")


## create two plots using plot_line
fig_female <- plot_line(tb_female)
fig_male <- plot_line(tb_male)

## create figure list
fig_list <- list(female = fig_female, male = fig_male)

## plot the two figures in a grid
grid_plot(fig_list, width=1000, height=600, same_axes=TRUE)


## split tb data by gender 
tb_split_gender <- split(tb, tb$gender)

## create a list of figures using lapply
fig_list <- lapply(tb_split_gender, FUN=plot_line)

## create a grid plot 
grid_plot(fig_list, width=1000, height=600, same_axes=TRUE)


## define a function to create a bar plot with the number of tb cases over time
plot_bar <- function(x){ 
    figure() %>% 
        ly_bar(y=count, x=year, data=x, color = gender, position = "dodge", hover=TRUE)
}

## split tb data by age
tb_split_age <- split(tb, tb$age)

## apply the function to the groups in tb_split_age
fig_list <- fig_list <- lapply(tb_split_age, plot_bar)

## create a grid plot 
grid_plot(fig_list, width=600, height=900, nrow=3, same_axes=TRUE) %>% 
    theme_axis("x", major_label_orientation = 90)


## initialize a map for NY center
# ny_map <- gmap(lat=40.73306, lng=-73.97351, zoom=11, map_style=gmap_style("blue_water"))
# ny_map


## filter ny_bikedata to get the entries for day "2017-04-25"
# ny_bikedata_20170425 <- ny_bikedata %>% filter(trip_date==as.Date("2017-04-25"))

## add a points layer to ny_map
# ny_map %>%
#     ly_points(y=station_latitude, x=station_longitude, 
#               size=8, fill_color=start_count, line_alpha=0, 
#               data=ny_bikedata_20170425, hover=c(station_name, start_count, end_count)
#               )

## create a names list with the two figures
# fig_list <- list(map_weekend=map_weekend_20170423, map_weekday=map_weekday_20170425)

## create a grid plot with the 2 maps
# grid_plot(fig_list, width=860, height=420)

```
  
  
  
***
  
###_A/B Testing in R_  
  
Chapter 1 - Mini Case Study in A/B Testing  
  
Introduction:  
  
* A/B testing is a powerful way to experiment with potential changes before implementing them  
	* Framework for testing new ideas to improve an existing design (often a website)  
* Hypothetical example - cat adoption website - could a change in home page improve conversion rate (clicks divided by views)?  
	* Question - does changing the photo improve conversion rate?  
    * Hypothesis - cat in hat will improve conversion rate  
    * Dependent variable - clicks  
    * Independent variable - homepage photo  
* Need to begin by assessing conversion rates in the current website  
	* click_data <- read_csv("click_data.csv")  
  
Baseline conversion rates:  
  
* Contnuning the previous example - hypothesis of cats in hats having "more" conversions  
	* Need to define "more" relative to some baseline - recent past, control group at same time, etc.  
    * click_data %>% summarize(conversion_rate = mean(clicked_adopt_today))  # mean from historical data  
    * click_data_sum <- click_data %>% group_by(lubridate::month(visit_date)) %>% summarize(conversion_rate = mean(clicked_adopt_today))  # mean by month from the historical data  
    * ggplot(click_data_sum, aes(x = `month(visit_date)`, y=conversion_rate)) + geom_point() + geom_line()  
  
Experimental design and power analysis:  
  
* Power analysis helps determine how many samples are needed (thus how long the experiment needs to run)  
	* Ideal is to run both conditions simultaneously - mitigate seasonality and other potential confounders  
    * Should know the planned test, baseline (control) value, and desired (test) value, as well as proportion of the data (typically 0.5), significance/alpha (typically 0.05), and power (typically 0.8)  
* Can use the powerMediation package to assess the power - note the function returns the total sample size, so each group is divided by 2  
	* library(powerMediation)  
    * total_sample_size <- SSizeLogisticBin(p1 = 0.2, p2 = 0.3, B = 0.5, alpha = 0.05, power = 0.8)  # note the function returns the total sample size, so each group is divided by 2  
  
Example code includes:  
```{r}


# Read in data
click_data <- readr::read_csv("./RInputFiles/click_data.csv")
click_data


# Find oldest and most recent date
min(click_data$visit_date)
max(click_data$visit_date)

# Calculate the mean conversion rate by day of the week
click_data %>%
  group_by(weekdays(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))

# Calculate the mean conversion rate by week of the year
click_data %>%
  group_by(lubridate::week(visit_date)) %>%
  summarize(conversion_rate = mean(clicked_adopt_today))


# Compute conversion rate by week of the year
click_data_sum <- click_data %>%
    mutate(weekOfYear = lubridate::week(visit_date)) %>%
    group_by(weekOfYear) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Build plot
ggplot(click_data_sum, aes(x = `weekOfYear`, y = conversion_rate)) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = scales::percent)


# Compute and look at sample size for experiment in August
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.54, p2 = 0.64, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# Compute and look at sample size for experiment in August with 5% increase
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.54, p2 = 0.59, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size

```
  
  
  
***
  
Chapter 2 - Mini Case Study in A/B Testing - Part II  
  
Analyzing Results:  
  
* Can analyze the experiment data from the previous design - available in a new dataset  
	* experiment_data <- read_csv("experiment_data.csv")  
    * experiment_data %>% group_by(condition) %>% summarize(conversion_rate = mean(clicked_adopt_today))  
    * experiment_data_sum <- experiment_data %>% group_by(visit_date, condition) %>% summarize(conversion_rate = mean(clicked_adopt_today))  
    * ggplot(experiment_data_sum, aes(x = visit_date, y = conversion_rate, color = condition, group = condition)) + geom_point() + geom_line()  
* Can further assess the statistical significance of the outcomes  
	* glm(clicked_adopt_today ~ condition, family = "binomial", data = experiment_data) %>% broom::tidy()  
  
Designing follow-up experiments:  
  
* Can continue to refine and test new hypotheses, but typically still with one step at a time  
	* Experiments need to be unique, and each with their own control group  
    * Attempt to avoid confounding variables; hard to explain real-world outcomes if there were many changes at the same time  
  
Pre-follow-up-experiment assumptions:  
  
* Control conditions for seasonal products can be especially challenging - careful not to choose times that already have very extreme conversion rates  
  
Follow-up experiment assumptions:  
  
* May want to look at the differences in conversion rate by month  
	* eight_month_checkin_data_sum <- eight_month_checkin_data %>%  
    *     mutate(month_text = month(visit_date, label = TRUE)) %>% group_by(month_text, condition) %>%  
    *     summarize(conversion_rate = mean(clicked_adopt_today))  
    * eight_month_checkin_data_diff <- eight_month_checkin_data_sum %>%  
    *     spread(condition, conversion_rate) %>%  
    *     mutate(condition_diff = cat_hat - no_hat)  
    * mean(eight_month_checkin_data_diff$condition_diff)  
    * sd(eight_month_checkin_data_diff$condition_diff)  
  
Example code includes:  
```{r}

experiment_data <- read_csv("./RInputFiles/experiment_data.csv")
experiment_data
followup_experiment_data <- read_csv("./RInputFiles/eight_month_checkin_data.csv")
followup_experiment_data


# Group and summarize data
experiment_data_clean_sum <- experiment_data %>%
    group_by(condition, visit_date) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Make plot of conversion rates over time
ggplot(experiment_data_clean_sum, aes(x = visit_date, y = conversion_rate, 
                                      color = condition, group = condition
                                      )
       ) + 
    geom_point() +
    geom_line()


# View summary of results
experiment_data %>% 
    group_by(condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
experiment_results <- glm(clicked_adopt_today ~ condition, family = "binomial", 
                          data = experiment_data
                          ) %>%
    broom::tidy()
experiment_results


# Run logistic regression power analysis
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.39, p2 = 0.59, B = 0.5, 
                                                      alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# View conversion rates by condition
followup_experiment_data %>%
    group_by(condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Run logistic regression
followup_experiment_results <- glm(clicked_adopt_today ~ condition, family = "binomial",
                                   data = followup_experiment_data
                                   ) %>%
    broom::tidy()
followup_experiment_results


# Compute monthly summary
eight_month_checkin_data_sum <- followup_experiment_data %>%
    mutate(month_text = lubridate::month(visit_date, label = TRUE)) %>%
    group_by(month_text, condition) %>%
    summarize(conversion_rate = mean(clicked_adopt_today))

# Plot month-over-month results
ggplot(eight_month_checkin_data_sum, aes(x = month_text, y = conversion_rate, 
                                         color = condition, group = condition
                                         )
       ) +
    geom_point() +
    geom_line()


# Plot monthly summary
ggplot(eight_month_checkin_data_sum, aes(x = month_text, y = conversion_rate,
                                         color = condition, group = condition
                                         )
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
    labs(x = "Month", y = "Conversion Rate")


# Compute difference over time
# no_hat_data_diff <- no_hat_data_sum %>% 
#     spread(year, conversion_rate) %>% 
#     mutate(year_diff = `2018` - `2017`)
# no_hat_data_diff

# Compute summary statistics
# mean(no_hat_data_diff$year_diff, na.rm = TRUE)
# sd(no_hat_data_diff$year_diff, na.rm = TRUE)


# Run power analysis for logistic regression
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.49, p2 = 0.64, B = 0.5, 
                                                      alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# View summary of data
# followup_experiment_data_sep %>% 
#     group_by(condition) %>% 
#     summarize(conversion_rate=mean(clicked_adopt_today))

# Run logistic regression
# followup_experiment_sep_results <- glm(clicked_adopt_today ~ condition,
#                                        family = "binomial",
#                                        data = followup_experiment_data_sep
#                                        ) %>%
#     broom::tidy()
# followup_experiment_sep_results

```
  
  
  
***
  
Chapter 3 - Experimental Design in A/B Testing  
  
A/B Testing Research Questions:  
  
* A/B testing combined experimental design and statistics - core building block basic principles  
* Any experimental design that compares two ideas can be run as an A/B test - conversion rates, engagement with a web site, drop-off rates, total amount of time or money spent  
* Example of looking at time spent on a websit  
	* str(viz_website_2017)  
    * viz_website_2017 %>% summarize(mean(time_spent_homepage_sec))  
    * viz_website_2017 %>% group_by(month(visit_date)) %>% summarize(mean(time_spent_homepage_sec))  
  
Assumptions and types of A/B testing:  
  
* Example of changing text in a website title, then checking the implications  
* Can look at within-group experiments (everyone sees both) or between-group experiments (everyone sees one or the other)  
	* The within experiment will often have better power, while the between experiment is easier to run when people may only interact once  
    * The between experiment needs to be appropriately random, so that whether the person sees A/B is not linked to other attributes of the person  
* There are several types of A/B testing  
	* A/B testing is test and control  
    * A/A testing is to verify that the control process is working well - should be no significant effects  
    * A/B/N testing is a control conditions with any number of test conditions - seems exciting and fast, but has more challenging statistics and requires more data points  
  
Confounding variables?  
  
* Confounding variables are elements of the environment that can confound your ability to find the real effect of A/B  
	* Sometimes the confounder is internal to the experiment - examples of word length/novelty being the real driver rather than the specific work chosen  
    * Sometimes the confounder is external to the experiment - examples of differing demographics by month, with the demographics having been a key driver of outcomes  
  
Side effects:  
  
* Side effects are unintended effects of a change that you made  
    * Example of changing from tools to tips if it changed the page loading times  
* Side effects can include load times and information "above the fold" (what a person sees without doing any scrolling)  
  
Example code includes:  
```{r eval=FALSE}

# Compute summary by month
viz_website_2017 %>%
    group_by(month(visit_date)) %>%
    summarize(article_conversion_rate = mean(clicked_article))


# Compute 'like' click summary by month
viz_website_2017_like_sum <- viz_website_2017 %>%
    mutate(month = month(visit_date, label = TRUE)) %>%
    group_by(month) %>%
    summarize(like_conversion_rate = mean(clicked_like))

# Plot 'like' click summary by month
ggplot(viz_website_2017_like_sum,
       aes(x = month, y = like_conversion_rate, group = 1)
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Plot comparison of 'like'ing and 'sharing'ing an article
ggplot(viz_website_2017_like_share_sum,
       aes(x = month, y = conversion_rate, color = action, group = action)
       ) +
    geom_point() +
    geom_line() +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Compute conversion rates for A/A experiment
viz_website_2018_01_sum <- viz_website_2018_01 %>%
    group_by(condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))
viz_website_2018_01_sum

# Plot conversion rates for two conditions
ggplot(viz_website_2018_01_sum, aes(x = condition, y = like_conversion_rate)) +
    geom_bar(stat = "identity") +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Run logistic regression
aa_experiment_results <- glm(clicked_like ~ condition, family = "binomial", data = viz_website_2018_01) %>%
    broom::tidy()
aa_experiment_results


# Compute 'like' conversion rate by week and condition
viz_website_2018_02 %>%
    mutate(week = week(visit_date)) %>%
    group_by(week, condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))

# Compute 'like' conversion rate by if article published and condition
viz_website_2018_02 %>%
    group_by(article_published, condition) %>%
    summarize(like_conversion_rate = mean(clicked_like))


# Plot 'like' conversion rates by date for experiment
ggplot(viz_website_2018_02_sum,
       aes(x = visit_date, y = like_conversion_rate, color = condition,
           linetype = article_published, group = interaction(condition, article_published)
           )
       ) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = as.numeric(as.Date("2018-02-15"))) +
    scale_y_continuous(limits = c(0, 0.3), labels = percent)


# Compute 'like' conversion rate and mean pageload time by day
viz_website_2018_03_sum <- viz_website_2018_03 %>%
    group_by(visit_date, condition) %>%
    summarize(mean_pageload_time = mean(pageload_time), like_conversion_rate = mean(clicked_like))

# Plot effect of 'like' conversion rate by pageload time
ggplot(viz_website_2018_03_sum, aes(x = mean_pageload_time, y = like_conversion_rate, color = condition)) +
    geom_point()


# Plot 'like' conversion rate by day
ggplot(viz_website_2018_03_sum, aes(x = visit_date, y = like_conversion_rate, color = condition,
                                    linetype = pageload_delay_added, 
                                    group = interaction(condition, pageload_delay_added)
                                    )
       ) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = as.numeric(as.Date("2018-03-15"))) +
    scale_y_continuous(limits = c(0, 0.3), labels = percent)

```
  
  
  
***
  
Chapter 4 - Statistical Analyses in A/B Testing  
  
Power analyses:  
  
* Generally, the goal of a power analysis is to determine the sample size - dependent on alpha, power (1 minus beta), and effect size  
	* Effect size is often defined as the difference in the two groups divided by the standard deviation of the groups  
* The t-test is often used for significance, and can be planned using the library(pwr)  
	* pwr.t.test(power = 0.8, sig.level = 0.05, d = 0.6)  # will return the number of data points needed for power 0.8, alpha 0.05, effect size 0.6  
    * pwr.t.test(power = 0.8, sig.level = 0.05, d = 0.2)  # effect size change of ~3x drives sample size change of ~9x (delta-effect-size-squared)  
  
Statistical tests:  
  
* Logistic regression and t-tests are both common statistical methods used for A/B testing  
	* viz_website_2018_01 <- read_csv("viz_website_2018_01.csv")  
    * aa_experiment_results <- t.test(time_spent_homepage_sec ~ condition, data = viz_website_2018_01)  
* Linear regression can be thought of as an extension of t-tests with more than 2 levels per variable  
	* However, for an A/B test with only 2-levels, you will get the same results  
  
Stopping rules and sequential analysis:  
  
* Stopping rules are procedures that allow for interim analysis (peaks in to the data) - also known as "sequential analysis"  
	* Can stop because the experiment worked, stop because the experiment failed, or continue experiment  
    * The p-value needs to be adjusted lower to account for the multiple peaks at the data  
    * Need to be very careful to prevent p-hacking by creating the stopping rules and points in advance  
* The library(gsDesign) can help with running sequrntial analysis in R  
	* library(gsDesign)  
    * seq_analysis <- gsDesign(k = 4, test.type = 1, alpha = 0.05, beta = 0.2, sfu = "Pocock")  # k=4 looks, test.type=1 is similar to one-sided test, alpha is significance, beta is 1-power so beta=0.2 is power=0.8, sfu is the spending function  
* Can then figure out the sample sizes using resource-based approaches  
	* max_n <- 1000  
    * max_n_per_group <- max_n / 2  
    * stopping_points <- max_n_per_group * seq_analysis$timing  
  
Multivariate testing:  
  
* Sometmes want to make comparisons that account for multiple changes  
	* multivar_results <- lm(time_spent_homepage_sec ~ word_one data = viz_website_2018_05) %>% tidy()  # single variable  
    * multivar_results <- lm(time_spent_homepage_sec ~ word_one * word_two, data = viz_website_2018_05) %>% tidy()  # full interaction effects  
* The default R order for regressions is to use the lowest alphanumeric as the baseline level - can modify this pre-regression though  
	* multivar_results <- viz_website_2018_05 %>% mutate(word_one = factor(word_one, levels = c("tips", "tools"))) %>% mutate(word_two = factor(word_two, levels = c("better", "amazing"))) %>% lm(time_spent_homepage_sec ~ word_one * word_two, data = .) %>% tidy()  
  
A/B Testing Recap:  
  
* Introduction to the basic concepts of A/B testing  
* New Ideas -> Experiments -> Statistical Analysis -> Implement Winners -> Repeat  
  
Example code includes:  
```{r eval=FALSE}

# Run power analysis for logistic regression
total_sample_size <- powerMediation::SSizeLogisticBin(p1 = 0.17, p2 = 0.27, 
                                                      B = 0.5, alpha = 0.05, power = 0.8
                                                      )
total_sample_size


# Run power analysis for t-test
sample_size <- pwr::pwr.t.test(d = 0.3, sig.level = 0.05, power = 0.8)
sample_size


# Run logistic regression
ab_experiment_results <- glm(clicked_like ~ condition, family = "binomial", data = viz_website_2018_04) %>%
    broom::tidy()
ab_experiment_results


# Run t-test
ab_experiment_results <- t.test(time_spent_homepage_sec ~ condition, data = viz_website_2018_04)
ab_experiment_results


# Run sequential analysis
seq_analysis_3looks <- gsDesign::gsDesign(k = 3, test.type = 1, 
                                          alpha = 0.05, beta = 0.2, sfu = "Pocock"
                                          )
seq_analysis_3looks


# Fill in max number of points and compute points per group and find stopping points
max_n <- 3000
max_n_per_group <- max_n / 2
stopping_points <- max_n_per_group * seq_analysis_3looks$timing
stopping_points


# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>% 
    group_by(word_one, word_two) %>% 
    summarize(mean_time_spent_homepage_sec = mean(time_spent_homepage_sec))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum, aes(x = word_one, y = mean_time_spent_homepage_sec, fill = word_two)) + 
    geom_bar(stat = "identity", position = "dodge")


# Compute summary values for four conditions
viz_website_2018_05_sum <- viz_website_2018_05 %>% 
    group_by(word_one, word_two) %>% 
    summarize(like_conversion_rate = mean(clicked_like))

# Plot summary values for four conditions
ggplot(viz_website_2018_05_sum, aes(x = word_one, y = like_conversion_rate, fill = word_two)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_y_continuous(limits = c(0, 1), labels = percent)


# Organize variables and run logistic regression
viz_website_2018_05_like_results <- viz_website_2018_05 %>%
    mutate(word_one = factor(word_one, levels = c("tips", "tools"))) %>%
    mutate(word_two = factor(word_two, levels = c("better", "amazing"))) %>%
    glm(clicked_like ~ word_one * word_two, family = "binomial", data = .) %>%
    broom::tidy()
viz_website_2018_05_like_results

```
  
  
  
***
  
###_Mixture Models in R_  
  
Chapter 1 - Introduction to Mixture Models  
  
Introduction to Model-Based Clustering:  
  
* Mixture models are a tool for model-based clustering (partitioning and segmentation)  
	* Objective for clusters to be homogenous within and heterogenous across  
* Common techniques include k-means (assign to nearest centers) and hierarchical (connect based on sililarity) and probabilistic model-based approaches (mixture models)  
	* gender <- read.csv("gender.csv")  
    * ggplot(gender, aes(x = Weight, y = BMI)) + geom_points()  
* May be helpful to have probabilities by gender rather than a hard cutoff for male vs. female  
	* This is the core of the mixture model - assumes an underlying probability distribution, with the outcome being the combination of these distributions  
  
Gaussian Distribution:  
  
* There are packages for fitting mixture models in R - mixtools (no Poisson), bayesmix (Bayesian), EMCluster (Gaussian only), flexmix (covered in this course)  
* The Gaussain distribution frequently plays a role in the mixture model  
	* Defined by the mean and standard deviation  
    * rnorm(n, mean, sd)  # sample from the Gaussian of mean and sd, taking n samples  
    * The mean is typically estimated as the sample mean, and the sd is typically estimated as root-mean-squared-delta-from-mean - sd()  
    * ggplot(data = population_sample) + geom_histogram(aes(x = x, y = ..density..)) + stat_function(geom = "line", fun = dnorm, args = list(mean = mean_estimate, sd = standard_deviation_estimate))  
  
Gaussian Mixture Models (GMM):  
  
* Can imagine two Gaussian distributions, and pick from each randomly with 50/50 probability  
	* number_of_obs <- 500  
    * coin <- sample(c(0,1), size = number_of_obs, replace = TRUE, prob = c(0.5, 0.5))  # can change the coin to be non-50/50 for other mixture simulations  
    * gauss_1 <- rnorm(n = number_of_obs, mean = 5, sd = 2)  
    * gauss_2 <- rnorm(n = number_of_obs)  
    * mixture_simulation <- ifelse(coin, gauss_1, gauss_2)  
    * head(cbind(coin, gauss_1, gauss_2, mixture_simulation))  
    * mixture_simulation <- data.frame(x = mixture_simulation)  
    * ggplot(mixture_simulation) + geom_histogram(aes(x = x, ..density..), bins = 40)  
* Can also create mixtures of 3+ underlying Gaussian  
	* proportions <- sample(c(0, 1, 2), number_of_obs, replace = TRUE, prob = c(1/3, 1/3, 1/3))  
    * gauss_3 <- rnorm(n = number_of_obs, mean = 10, sd = 1)  
    * mixture_simulation <- data.frame(x = ifelse(proportions == 0, gauss_1, ifelse(proportions == 1, gauss_2, gauss_3)))  
    * ggplot(mixture_simulation) + geom_histogram(aes(x = x, ..density..), bins = 40)  
  
Example code includes:  
```{r}

gender <- readr::read_csv("./RInputFiles/gender.csv")
glimpse(gender)

# Have a look to gender (before clustering)
head(gender)

# Scatterplot with probabilities
gender %>% 
  ggplot(aes(x = Weight, y = BMI, col = probability))+
  geom_point(alpha = 0.5)


# Set seed
set.seed(1313)

# Simulate a Gaussian distribution
simulation <- rnorm(n = 500, mean = 5, sd = 4)

# Check first six values
head(simulation)

# Estimation of the mean
mean_estimate <- mean(simulation)
mean_estimate

# Estimation of the standard deviation
standard_deviation_estimate <- sd(simulation)
standard_deviation_estimate

# Transform the results to a data frame
simulation <- data.frame(x = simulation)

# Plot the sample with the estimated curve
ggplot(simulation) + 
  geom_histogram(aes(x = x, y = ..density..)) + 
  stat_function(geom = "line", fun = dnorm,
                args = list(mean = mean_estimate, 
                sd = standard_deviation_estimate))


# Estimation of the mean
mean_estimate <- gender %>% 
  pull(Weight) %>% 
  mean()
mean_estimate

# Estimation of the standard deviation
sd_estimate <- gender %>% 
  pull(Weight) %>% 
  sd()
sd_estimate

# Plot the sample with the estimated curve
gender %>% 
  ggplot() + 
  geom_histogram(aes(x = Weight, y = ..density..), bins = 100) + 
  stat_function(geom = "line", fun = dnorm,
                args = list(mean = mean_estimate, sd = sd_estimate))


# Create coin object
coin <- sample(c(0, 1), size = 500, replace = TRUE, prob = c(0.2, 0.8))

# Sample from two different Gaussian distributions
mixture <- ifelse(coin == 1, rnorm(n = 500, mean = 5, sd = 2), rnorm(n = 500))

# Check the first elements
head(mixture)


# Transform into a data frame
mixture <- data.frame(x = mixture)

# Create histogram especifiying that is a density plot
mixture %>% ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 50)


number_observations <- 1000

# Create the assignment object
assignments <- sample(c(0, 1 , 2), size = number_observations, replace = TRUE, prob = c(0.3, 0.4, 0.3))

# Simulate the GMM with 3 distributions
mixture <- data.frame(
    x = ifelse(assignments == 1, rnorm(n = number_observations, mean = 5, sd = 2), 
               ifelse(assignments == 2, 
                      rnorm(n = number_observations, mean = 10, sd = 1), 
                      rnorm(n = number_observations)
                      )
               )
    )

# Plot the mixture
mixture %>% 
    ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 50)

```
  
  
  
***
  
Chapter 2 - Structure of Mixture Models and Parameter Estimation  
  
Structure of Mixture Models:  
  
* Three questions to be answered for clustering with mixture models  
	* Suitable probability distribution - depends on domain expertise  
    * Number of clusters - domain expertise or testing to see what best satisfies criteria  
    * Parameters and estimates - based on Expectation Maximization (EM) Algorithm  
* Example of the gender dataset - bivariate Gaussian, with two clusters, and mu/sigma/proportion for each  
* Example of handwritten digits (3 vs. 6) - Bernoulli distributions with two clusters, mean probability of being 1 for every dot  
* Example of crime types in Chicago - Poisson distribution with six clusters (crime types), average and proportion of crimes  
  
Parameter Estimation:  
  
* Suppose that we have an assumption of 2 clusters each from a Gaussian distribution and with each distribution having the same sigma  
	* If the probabilities are known, try to estimate the means  
    * If the means are known, try to estimate the probabilities  
    * means_estimates <- data_with_probs %>% summarise(mean_red = sum(x * prob_red) / sum(prob_red), mean_blue = sum(x * prob_blue) / sum(prob_blue))  
    * proportions_estimates <- data_with_probs %>% summarise(proportion_red = mean(prob_red), proportion_blue = 1 - proportion_red)  
    * data %>% mutate(prob_from_red = 0.3 * dnorm(x, mean = 3), prob_from_blue = 0.7 * dnorm(x,mean = 5), prob_red = prob_from_red/(prob_from_red + prob_from_blue), prob_blue = prob_from_blue/(prob_from_red + prob_from_blue)) %>%  
    *     select(x, prob_red, prob_blue) %>% head()  
  
EM Algorithm:  
  
* Can begin by making nave assumptions about the distributions, for later refinement  
	* means_init <- c(1, 2)  
    * props_init <- c(0.5, 0.5)  
* Can then run a first iteration of the probabilities - the expectations  
	* means_estimates <- data_with_probs %>% summarise(mean_red = sum(x * prob_red) / sum(prob_red), mean_blue = sum(x * prob_blue) / sum(prob_blue)) %>% as.numeric()  
    * props_estimates <- data_with_probs %>% summarise(proportion_red = mean(prob_red), proportion_blue = 1- proportion_red) %>% as.numeric()  
* Basically, the iterations continue  
	* Iteration 0: Initial Parameters -> Estimate Probabilities (1)  
    * Iteration 1: Estimated Probabilities (1) -> Estimated Parameters (2) -> Estimated Probabilities (2)  
    * Iteration 2: Estimated Probabilities (2) -> Estimated Parameters (3) -> Estimated Probabilities (3)  
    * Etc.  
* Can translate the iterative process in to a function that is called by way of a for loop  
	* expectation <- function(data, means, proportions){  
    *     data <- data %>%  
    *         mutate(prob_from_red = proportions[1] * dnorm(x, mean = means[1]),  
    *                prob_from_blue = proportions[2] * dnorm(x, mean = means[2]),  
    *                prob_red = prob_from_red/(prob_from_red + prob_from_blue),  
    *                prob_blue = prob_from_blue/(prob_from_red + prob_from_blue)
    *                ) %>%  
    *         select(x, prob_red, prob_blue)  
    *     return(data)  
    * }  
    * for(i in 1:10){  
    *     new_values <- maximization(expectation(data, means_init, props_init))  
    *     means_init <- new_values[[1]]  
    *     props_init <- new_values[[2]]  
    *     cat(c(i, means_init, proportions_init),"\n")  
    * }  
  
Example code includes:  
```{r}

digits <- readr::read_csv("./RInputFiles/digits.csv")
dim(digits)

digitData <- digits[, 1:256]
digitKey <- digits[, 257:266]

# keep a subset of 4 and 8
digitUse <- rowSums(digitKey[, c(5, 9)]==1)
digData <- digitData[digitUse, ]
digKey <- digitKey[digitUse, ]

show_digit <- function(arr256, col=gray(4:1/4), ...) {
    arr256 <- as.numeric(arr256)
    image(matrix(arr256, nrow=16)[,16:1],col=col,...)
}

# Dimension
# broom::glance(digits)

# Apply `glimpse` to the data
glimpse(digitData)

# Digit in row 50
show_digit(digitData[50, ])

# Digit in row 100
show_digit(digitData[100, ])


gaussian_sample_with_probs <- data.frame(
    x = c(54.5, 7.7, 55.9, 27.9, 4.6, 59.9, 6.4, 60.5, 32.6, 21.3, 0.5, 8.9, 70.7, 49.3, 40.1, 43, 8.1, 62.9, 56, 54.4, 42.5, 46.1, 58.3, 61.7, -11.6, 10.8, 27.5, 12.2, 67.7, -5.6, 13.3, 62.7, 37.2, 41.4, 47.4, 54.2, 31, 60.2, 69.9, 33.8, 25.4, 21.9, 17.9, 61.5, 49.8, 37.9, 55.8, 14.1, 53.3, 45.6, 44.7, 14.2, -5.7, 10.9, 63.7, -6.5, 50.3, 61.4, 35.1, -3.7, 68.4, -6.2, 64, 24.4, 65.7, 59.7, 52.7, 27.2, 17.5, 22.6, 14.7, 22.1, 61.5, 55.6, 62.6, 5.6, 52.3, 8, 25.4, 48.8, 58.4, 6.2, 52.3, 6.6, 64, 43, 60.6, 33.5, 45.8, 2.5, 63, 58.2, 50.9, 22.1, 36.5, 27.1, 61.4, 56.3, 63.5, 55.6, 53.8, 31.9, 30.7, 15.6, 14.8, 44.4, 51.9, 61.4, 11.8, 51.3, 58.6, 45.4, 8.3, 41.5, 52.7, 9.1, 60.8, 40.2, 20.5, 40.2, 59.2, 36.7, 47.5, 12.2, 7.7, 56.2, -13.2, 6, 58.7, 43.7, 67.3, 53.6, 37.6, 54.3, 37.7, 51.9, 10.5, 42, 24, -0.7, 53.1, 27.4, 57.2, 37.3, 28.6, 13.5, 35.2, 22.7, 35.8, 66.9, 45.9, 45.9, 56.7, 55.6, 58.3, 3.2, 45.9, 59.5, 50.8, 43.7, 42.8, 4.7, 29.5, 50.9, 7.8, 44.3, 53.6, 57, 57.8, 47.3, 56.8, 51.1, 27.7, 44.9, 33, 44, 42.1, 38, 52.3, 44, 28.1, 52.7, 53.6, 4.7, 42.1, 40.8, 5, 8, 49.1, 67.5, 16.2, 11.2, 14.6, 32.8, 61.3, 49.8, 51.5, 54.5, 51.6, 45.8, 55.9, 7.4, -10.2, 41.9, 27.4, 45.1, 17.7, 37.5, 53.5, 25.7, 18.1, 13.4, 40.5, 13.3, 2, 49.8, 66.7, 34.7, 11.4, 42.1, 54.4, 48.3, 38.3, 17.4, 48.2, 48.4, 57.4, 54.5, 13.6, 52.3, -0.1, 12.8, 29.3, 45.6, 62.3, 49.2, 32.6, 38.4, 15, 6.1, 12.2, 5.8, 17.7, 20.7, 43.6, 52.3, 42.4, 64.6, 34.3, 9.5, 3.6, 37.2, 45.7, 56.9, 67, 48.7, -3.1, 50.1, 45.4, 54.4, 38.1, 10.8, 7.4, 50.5, 24.7, 11.4, 59.5, 43.9, 4.4, 53.7, 41.9, 60.2, 49.5, 11.6, 51.1, 69.1, 46.2, 35.5, 15, -6.4, 59.9, 57.3, 49.1, 55.5, 55.6, 43.9, 52.5, 46.4, 5.8, 55.3, 22.2, 42.7, 51.3, 40.1, 62.1, 62.2, 48.8, 6.1, 0.6, 19.6, 36.8, 48, 33.8, 52.8, 66.6, 30.2, 45.9, 5.9, 52.7, 49.7, 37.7, 10.4, 60.1, 35.8, 62.1, 35, 38.7, 13.3, -4.9, 30.6, 55.9, 23.7, 12.6, 45.7, 38.1, 9.9, 39.6, 46.3, -3.5, 31.2, 8.3, -8.1, 31.4, 65.7, 10.7, 5.5, 54.4, 51.8, 59.8, 50.3, 45.1, 8.5, 15.3, 3.2, 19.3, 40.8, 48.4, 30.1, 32.7, 12.7, 59.2, 51.4, 55.3, 58.9, -19, 61.9, 30.3, 77.2, 39.8, 31.3, 23.1, 56, 41.9, 0.5, 33.4, 36.6, 54.4, 12.4, 16.4, 24.4, -2.4, 30.9, 56.4, 12.5, 65.2, 10, -1.7, 45.7, 49.5, 45.3, 17.5, 29, -8.7, 51.7, 17.3, 20.2, 14.6, 47.6, 55.3, 50.2, 4.1, 47.5, 71, 13.2, 75.4, 6.2, 53, 54.2, 40.6, 55.1, 67.4, 45, 47.3, 44.2, 8.4, 46.1, 48.7, 8.3, 40.4, 63, 49, 2.8, 50.4, 17.7, 40.4, 41.1, 56.6, 37.3, -0.1, 62.5, 47.7, 62.1, 16.6, 33.3, 4.1, 61, 49.4, 44.1, 18.7, -1.3, 42.1, -11.8, 40.6, 45.6, 14.9, 51.9, 57.4, 41.3, 59.2, 58.6, 50.5, -3.9, -0.6, 11.5, 54.5, 57.1, 46.2, 51.9, 58.2, 51.6, 50.3, 64.2, 8.3, 49, 42, 43.7, 53.4, 6.5, 36.6, -18.2, 41.8, -6.8, 35, 46.8, 43.8, 60.6, -11.3, 18.5, 0.3, 40.2, 73.3, 58.2, 43.9, 22.2, 12.8, 6.7, 36.3, 51.8, 33.6, 71, 56.8, 26, 43.3, 37.4, 60, 17.2, -10.3, 43.9, 69, 38.7, 57.9, 40.2, 48.6, 57.7, 45.8, 56.2, 7.3, 32.1, 41.2, 39.1), 
    prob_cluster1=c(0, 1, 0, 0.552, 1, 0, 1, 0, 0.158, 0.947, 1, 1, 0, 0, 0.01, 0.003, 1, 0, 0, 0, 0.004, 0.001, 0, 0, 1, 0.999, 0.591, 0.999, 0, 1, 0.998, 0, 0.03, 0.006, 0.001, 0, 0.268, 0, 0, 0.107, 0.773, 0.933, 0.985, 0, 0, 0.023, 0, 0.997, 0, 0.001, 0.002, 0.997, 1, 0.999, 0, 1, 0, 0, 0.065, 1, 0, 1, 0, 0.834, 0, 0, 0, 0.626, 0.988, 0.912, 0.996, 0.928, 0, 0, 0, 1, 0, 1, 0.773, 0, 0, 1, 0, 1, 0, 0.003, 0, 0.118, 0.001, 1, 0, 0, 0, 0.926, 0.038, 0.631, 0, 0, 0, 0, 0, 0.201, 0.286, 0.994, 0.996, 0.002, 0, 0, 0.999, 0, 0, 0.001, 1, 0.005, 0, 1, 0, 0.009, 0.961, 0.009, 0, 0.036, 0, 0.999, 1, 0, 1, 1, 0, 0.002, 0, 0, 0.025, 0, 0.024, 0, 0.999, 0.004, 0.855, 1, 0, 0.604, 0, 0.028, 0.484, 0.997, 0.062, 0.909, 0.05, 0, 0.001, 0.001, 0, 0, 0, 1, 0.001, 0, 0, 0.002, 0.003, 1, 0.398, 0, 1, 0.002, 0, 0, 0, 0.001, 0, 0, 0.576, 0.001, 0.138, 0.002, 0.004, 0.022, 0, 0.002, 0.533, 0, 0, 1, 0.004, 0.007, 1, 1, 0, 0, 0.993, 0.999, 0.996, 0.147, 0, 0, 0, 0, 0, 0.001, 0, 1, 1, 0.005, 0.606, 0.001, 0.987, 0.026, 0, 0.754, 0.985, 0.998, 0.008, 0.998, 1, 0, 0, 0.076, 0.999, 0.004, 0, 0, 0.019, 0.988, 0, 0, 0, 0, 0.997, 0, 1, 0.998, 0.417, 0.001, 0, 0, 0.162, 0.018, 0.995, 1, 0.998, 1, 0.987, 0.956, 0.002, 0, 0.004, 0, 0.087, 0.999, 1, 0.03, 0.001, 0, 0, 0, 1, 0, 0.001, 0, 0.021, 0.999, 1, 0, 0.82, 0.999, 0, 0.002, 1, 0, 0.005, 0, 0, 0.999, 0, 0, 0.001, 0.057, 0.995, 1, 0, 0, 0, 0, 0, 0.002, 0, 0.001, 1, 0, 0.925, 0.003, 0, 0.009, 0, 0, 0, 1, 1, 0.971, 0.035, 0, 0.104, 0, 0, 0.329, 0.001, 1, 0, 0, 0.024, 0.999, 0, 0.05, 0, 0.067, 0.016, 0.998, 1, 0.298, 0, 0.87, 0.998, 0.001, 0.021, 0.999, 0.012, 0.001, 1, 0.247, 1, 1, 0.233, 0, 0.999, 1, 0, 0, 0, 0, 0.001, 1, 0.995, 1, 0.975, 0.007, 0, 0.34, 0.157, 0.998, 0, 0, 0, 0, 1, 0, 0.325, 0, 0.011, 0.246, 0.895, 0, 0.005, 1, 0.12, 0.037, 0, 0.998, 0.992, 0.834, 1, 0.273, 0, 0.998, 0, 0.999, 1, 0.001, 0, 0.001, 0.987, 0.446, 1, 0, 0.989, 0.964, 0.996, 0, 0, 0, 1, 0, 0, 0.998, 0, 1, 0, 0, 0.008, 0, 0, 0.001, 0.001, 0.002, 1, 0.001, 0, 1, 0.008, 0, 0, 1, 0, 0.986, 0.008, 0.006, 0, 0.028, 1, 0, 0, 0, 0.991, 0.128, 1, 0, 0, 0.002, 0.98, 1, 0.004, 1, 0.008, 0.001, 0.996, 0, 0, 0.006, 0, 0, 0, 1, 1, 0.999, 0, 0, 0.001, 0, 0, 0, 0, 0, 1, 0, 0.004, 0.002, 0, 1, 0.037, 1, 0.005, 1, 0.068, 0.001, 0.002, 0, 1, 0.981, 1, 0.009, 0, 0, 0.002, 0.925, 0.998, 1, 0.042, 0, 0.114, 0, 0, 0.725, 0.003, 0.027, 0, 0.989, 1, 0.002, 0, 0.016, 0, 0.009, 0, 0, 0.001, 0, 1, 0.189, 0.006, 0.014)
)

gaussian_sample_with_probs <- gaussian_sample_with_probs %>%
    mutate(prob_cluster2 = 1-prob_cluster1)
glimpse(gaussian_sample_with_probs)


# Estimation of the means
means_estimates <- gaussian_sample_with_probs %>% 
    summarise(mean_cluster1= sum(x*prob_cluster1)/sum(prob_cluster1),
              mean_cluster2 = sum(x*prob_cluster2)/sum(prob_cluster2)
              )
means_estimates

# Estimation of the proportions
props_estimates <- gaussian_sample_with_probs %>% 
    summarise(props_cluster1 = mean(prob_cluster1),
              props_cluster2 = mean(prob_cluster2)
              )
props_estimates

# Transform to a vector
means_estimates <- as.numeric(means_estimates)

# Plot histogram with means estimates
ggplot(gaussian_sample_with_probs) + geom_histogram(aes(x = x), bins = 100) +
    geom_vline(xintercept = means_estimates)


gaussian_sample <- data.frame(
    x=c(6.4, 5.9, 57.8, 52.6, 54.3, 52.3, 4.4, 49.1, -4, 12.7, 19.8, 51.8, 35.4, 17.1, 38.8, 44.1, 45.6, 7.9, 57.7, 51.1, 14.1, 36.6, 51.6, 4.1, -1.8, 55.1, 52.4, 54.4, 47.9, 36.6, 53.9, 15, 68.8, 8.3, 40.8, 39.3, 37.1, 12.7, 54.6, 34.1, 24.9, 58.5, 50.8, 48.6, 60, 52.1, 61.5, 6.9, 63, 63.5, 54.1, 37.7, 52.6, 49.1, 53.7, 13.4, 23.6, 45.5, 33.4, 46.4, 46.6, 56.1, 37.8, 44.1, 62.4, 12, 54.4, 31.6, -1, 9.4, 16, 53.4, 71.1, 8.9, 64.4, 55.9, 50.5, 57.2, 45.9, 18.5, 53.9, 12.5, 12.2, 1.5, 0.3, 40.1, 13.9, 53.2, 12.1, 57.2, 2.3, -2.6, 2.7, 59.6, 3, 10.3, 66.9, 57.3, 57.6, 9.1, 43.8, 51.1, 7.7, 13.4, 46.3, 57.5, 0.2, 1.9, 43.8, 53.9, 9.3, 45.5, 15.4, -3.2, -1.2, 40.5, 1.9, 14.5, -2, 3.4, 54.1, 2.9, 58.2, 49.5, 49.1, 60.2, 45.3, 59.7, 38, 22.4, 42.6, 53.6, 7.3, 43.9, 2.8, 66.5, 56.5, 44.4, 53.5, 40.6, 57.1, 43.8, -3.1, 47.3, 42.5, 50.8, -12, -12, 15.2, 43.8, 57.3, 32.2, 61.1, 15.1, 5.8, 24.7, 51.5, 7.7, -5.1, 63.1, 50.1, 39.9, 38.7, -5.2, 50.3, 49.1, 58.1, 31.3, 54.6, 39.1, 4.4, 60.5, 45.6, 59.7, 39.5, 60.6, 42.8, 49.5, 12.9, 47.2, 50, 11.4, 50.9, 57.3, 46.7, 35.6, 38.8, 56, -5.7, 50.5, 21.2, 45.9, 60.7, 22.1, 46.7, 12.5, 55.2, 48.4, 36.6, 54, 47, 50.3, 51.7, 11, 56, 42.4, 61.8, 45.6, 60.5, 40.6, 8.8, 21, 5.6, 68.2, 21.3, 11.5, 47.2, 26.4, 35.8, 25.4, 19.6, 56, 9.1, 63.4, 48.5, 3.2, 57.1, 52.7, 11.3, 16.3, 49, 46.5, 12.4, 9.6, 45.5, 55.3, 72.9, 8.1, -3.8, 53.8, 34.1, 45.7, 56.3, 44, 23.4, 57.2, 0.5, 33.2, 63.4, 37.3, 57.3, 52.7, 9.7, 51.9, 39.4, 63.7, 23.3, 39.9, -0.5, 41.6, 11.3, 48, 38.2, 54.2, 41.3, 30.6, 55.2, 48.9, 34.4, 16.2, 45.7, 10.1, 42.7, 12.2, 39.5, 14.1, 64.9, 53.1, 50.4, 47, 58.5, 50.8, 43.9, 56.8, 12.6, 44.5, 54.6, 8.9, 15.5, 50.2, 4.8, 52.8, 14.4, 33.7, 5.4, -0.2, 19.8, 51, 59.4, -8.2, 10.4, 47.8, 31.2, 41.4, 9.4, -3.2, 21.1, 44.7, 22.9, 11.5, 49.6, 26.7, 11.5, 35.2, 9.4, 44.8, 63.1, 8.5, 21, 30.9, 16.1, 54.4, 53.4, 9.7, 49.8, 45.6, -3, 53, 43.4, 43.4, 43.9, 56.6, 33.5, 55.1, 54.4, 62.8, 37.9, 35.1, 8.6, 7.1, 46.1, 6.1, 27, -9.9, 6.4, 44.6, 49, 46, 42.4, 9.5, 47.1, 51.3, -4.7, 14, 64.8, 38, 33.6, -0.4, 53.5, 40.3, 47.2, 58.5, 45.4, 2.5, 52.9, 47.4, 56.1, 17.7, 3.9, 30.7, 44.6, 42.4, 55.4, 47.1, 11.5, 50.7, 47.6, 11.3, 45.1, 44.2, 46.6, 36.9, 47.4, 54.6, -2, 50.7, 63.6, 58.9, 7.6, -3.1, 31.1, 44.9, 55.7, 16.6, 64.3, 27.1, 23, 48.7, -0.8, 23.6, 72.8, 11.9, 57.3, 25.4, 47.1, 9.4, 57.6, 39.6, 25.3, 31.2, 52.4, 51.1, 1.6, 76.5, 50.7, 34.2, 7.6, 25.4, 11.7, 53.5, 17.5, 53.7, 61.2, 49.9, 48.8, 40.8, 61.2, 16.4, 48.6, 7.5, -2, 64.2, 26.2, 11.2, 3.2, -4.3, 37.9, 47.7, 26.3, 58, 66.9, 59.1, 35.8, 14.2, 53, 60.3, 63.3, 53.6, 47.6, 57.1, 37, 47.6, 61.6, 52.7, 0.8, 50.5, 48.1, -3.4, 53.6, 35.7, 49.8, 2.7, 59.9, 36.5, 63.6, 53.3, 3.8, 20.2, 19.7, 20.7, 45.6, 39.8, 37.2, 38.6, 12.4, 56.3, 59.6, 10.5, 11, -6.8, 58.8, 49.5, -3.6, 51.1, 53.1, 46, 57.9, 15.2, -2.3, 22.9, 32.8, 37.6, 52, 77.5, 2.2, 9.5, 40.4, 48.5, 27.2, 37.4)
)
str(gaussian_sample)


# Create data frame with probabilities
gaussian_sample_with_probs <- gaussian_sample %>% 
    mutate(prob_from_cluster1 = 0.35 * dnorm(x, mean = 10, sd = 10),
           prob_from_cluster2 = 0.65 * dnorm(x, mean = 50, sd = 10),
           prob_cluster1 = prob_from_cluster1/(prob_from_cluster1 + prob_from_cluster2),
           prob_cluster2 = prob_from_cluster2/(prob_from_cluster1 + prob_from_cluster2)) %>%
    select(x, prob_cluster1, prob_cluster2) 
head(gaussian_sample_with_probs)


expectation <- function(data, means, proportions, sds){
  # Estimate the probabilities
  exp_data <- data %>% 
      mutate(prob_from_cluster1 = proportions[1] * dnorm(x, mean = means[1], sd = sds[1]),
             prob_from_cluster2 = proportions[2] * dnorm(x, mean = means[2], sd = sds[2]),
             prob_cluster1 = prob_from_cluster1/(prob_from_cluster1 + prob_from_cluster2),
             prob_cluster2 = prob_from_cluster2/(prob_from_cluster1 + prob_from_cluster2)) %>%
      select(x, prob_cluster1, prob_cluster2)
    # Return data with probabilities
  return(exp_data)
}

maximization <- function(data_with_probs){
    means_estimates <- data_with_probs %>%
        summarise(mean_1 = sum(x * prob_cluster1) / sum(prob_cluster1),
                  mean_2 = sum(x * prob_cluster2) / sum(prob_cluster2)
                  ) %>% 
        as.numeric()
    props_estimates <- data_with_probs %>% 
        summarise(proportion_1 = mean(prob_cluster1), proportion_2 = 1 - proportion_1) %>% 
        as.numeric()
    list(means_estimates, props_estimates)   
}


means_init <- c(0, 100)
props_init <- c(0.5, 0.5)

# Iterative process
for(i in 1:10){
    new_values <- maximization(expectation(gaussian_sample, means_init, props_init, c(10, 10)))
    means_init <- new_values[[1]]
    props_init <- new_values[[2]]
    cat(c(i, means_init, props_init), "\n")
}


fun_gaussian <- function(x, mean, proportion){
    proportion * dnorm(x, mean, sd = 10)
}

means_iter10 <- means_init
props_iter10 <- props_init

gaussian_sample %>% ggplot() + 
    geom_histogram(aes(x = x, y = ..density..), bins = 200) +
    stat_function(geom = "line", fun = fun_gaussian, 
                  args = list(mean = means_iter10[1], proportion = props_iter10[1])
                  ) +
    stat_function(geom = "line", fun = fun_gaussian,
                  args = list(mean = means_iter10[2], proportion = props_iter10[2])
                  )

```
  
  
  
***
  
Chapter 3 - Mixture of Gaussians with flexmix  
  
Univariate Gaussian Mixture Models:  
  
* Gaussian mixture models are formed by Gaussian distributions, which can have many potential parameters  
	* Simplest version is the univariate Gaussian, such as the BMI dataset with no labels for gender  
* Example of using the gender dataset for segmenting assuming that the data are not labeled by gender  
	* Weight should be a Gaussian - continuous, not linked to integer values, etc.  
    * Histogram looks like two Gaussians, so begin with the assumption of 2 univariate Gaussians with a resulting 2 segments  
    * Parameters can initially be estimated with a baseline mean, sd, and proportion (6 total parameters)  
    * The EM algorithm, implemented in flexmix, can then help to simplify the calculations  
  
Univariate Gaussian Mixture Models with flexmix:  
  
* Can begin by checking what the most suitable distributions might be  
	* gender %>% ggplot(aes(x = Weight)) + geom_histogram(bins = 100)  
    * Univariate Gaussian with 2 clusters  
* Can use the flexmix::flexmix(formula, data, k, models, control, ...) to make the estimates  
	* formula - describes the model to be fit (often variable ~ 1)  
    * data - data frame  
    * k - number of clusters  
    * models - distribution to be considered, such as FLXMCnorm1 for the univariate Gaussian  
    * control - maximum iterations and tolerance  
* Example of fitting to a dataset using flexmix  
	* fit_mixture <- flexmix(Weight ~ 1, data = gender, k = 2, model = FLXMCnorm1(), control = list(tol = 1e-15, verbose = 1, iter = 1e4))  
    * proportions <- prior(fit_mixture)  # proportions from the model  
    * parameters(fit_mixture)  # all the parameters  
    * comp_1 <- parameters(fit_mixture, component = 1)  # just the parameters for component 1  
    * posterior(fit_mixture) %>% head()  # probability of belonging to each cluster by observation  
    * posterior(fit_mixture) %>% head()  # assignment to the cluster with maximum probability  
    * table(gender$Gender, clusters(fit_mixture))  
  
Bivariate Gaussian Mixture Models with flexmix:  
  
* May want to use additional variables to improve the clustering - for example, using both height and weight from the gender dataset  
	* Bivariate Gaussian with 2 clusters, and needing to estimate additional means and standard deviations (which need to also be between the variables)  
* The flexmix library implements the Bivariate Gaussian distribution, which is conceptually like  
	* There would be two means - variable 1 and variable 2  
    * There would be a 2x2 covariance matrix - sd1, sd2, cov(1, 2), cov(1, 2) - really, three terms since the off-diagonals will be equal  
  
Bivariate Gaussian Mixture Models with flexmix:  
  
* Example of a covariance matrix without cross-terms  
	* fit_without_cov <- flexmix(cbind(Weight, BMI) ~ 1, k = 2, data = gender, model = FLXMCmvnorm(diag = TRUE), control = list(tolerance = 1e-15, iter.max = 1000))  # cbind() is because there are two variables; diag=TRUE means no covariance  
    * proportions <- prior(fit_without_cov)  
    * parameters(fit_without_cov)  
    * comp_1 <- parameters(fit_without_corr, component=1)  
    * comp_2 <- parameters(fit_without_corr, component=2)  
    * mean_comp_1 <- comp_1[1:2]  
    * mean_comp_2 <- comp_2[1:2]  
* Can then visualize the resulting clusters  
	* library(ellipse)  
    * ellipse_comp_1 <- ellipse(x = covariance_comp_1, centre = mean_comp_1, npoints = nrow(gender))  
    * ellipse_comp_2 <- ellipse(x = covariance_comp_2, centre = mean_comp_2, npoints = nrow(gender))  
    * gender %>%  
    *     ggplot(aes(x = Weight, y = BMI)) + geom_point() +  
    *         geom_path(data = data.frame(ellipse_comp_1), aes(x=x,y=y), col = "red") +  
    *         geom_path(data = data.frame(ellipse_comp_2), aes(x=x,y=y), col = "blue"  
* Need to include joint variability to improve the modeling  
	* fit_with_cov <- flexmix(cbind(Weight, BMI) ~ 1, k = 2, data = gender, model = FLXMCmvnorm(diag = FALSE), control = list(tolerance = 1e-15, iter.max = 1000))  # cbind() is because there are two variables; diag=FALSE means covariance  
  
Example code includes:  
```{r eval=FALSE}

xExample <- c(7.3, 58.7, 9.7, 16.9, 6.3, 35.1, 33.5, 61.3, 28.3, 24.3, 58.6, 13.1, 58.7, 34, 29.1, 46.4, 54.6, 5.9, 30.6, 27.9, 27.5, -5.3, 37.6, 9.1, 44.5, 57.5, 30.5, 5, 51.9, 33.6, 37.4, 28.8, 47.9, 5.4, 64.1, 45.1, 41, 36.3, 28.2, 33.8, 9.8, 57.4, 48.4, 58.3, 27.7, 38.4, 36.4, 66.9, 30.7, 34.3, 25.9, 48.5, 52, 0.3, 45.3, 31.9, 21.6, 36.6, 29, 13.2, 41.5, 8.2, 46.6, 30.6, 48.6, 5.6, 39.3, 30.5, 34.2, 61.5, 4.2, 71.3, 42.5, 32.7, 54.4, 19.2, 13.3, 40.3, 72, 21.8, 49.5, 38.7, 9.6, 49.6, 32, 30.9, 28.6, 30.1, 29.8, 67.9, 60.8, 55, 34.6, 32.8, 11.9, 50.5, 32.1, 13.7, 48.6, 32.6, 9.1, 27.6, 35.6, 28.3, 15.1, 54.7, 30.8, 22.2, 27.5, 49.3, 56, 26.1, 57.2, 46.4, 50.3, 43.6, 51.8, 47.5, 15.5, 60.2, 63.6, 45.3, 14.1, 42.1, 31.4, 42.4, 61.7, 60.1, 27.7, 55.9, 3.3, 18.7, 58.1, 46, 14, 41.7, 28.9, 29.1, 56.9, 32.3, -0.8, 29.4, 27.3, 33.5, 39.1, 13.9, 28.7, 29.4, 10.3, 44.3, 57.1, 76, 49.4, 44.9, 23.2, 53.9, 33.6, 32.7, 30, 57, 63.6, 32.9, 8.6, 26.5, 26, 53.3, 40.8, 30.1, 10.5, 47.2, 30.2, 49.3, 52.4, 48.8, 51.4, 40.7, 33.8, 45.7, 28.1, 13.2, 28.4, 31.7, 30, 29.6, 49.5, 35, 62, 51.9, 39, 15.4, 59.1, 54.8, 9.2, 9.7, 35.4, 32.9, 31.3, 30.4, 64.4, 63.4, 32.9, 40.6, 37.5, 52.3, 35.3, 8.1, 6.4, 26.2, 29.2, 29.7, 27.8, 35.2, 34.1, 29.8, 49, 65.6, -1.1, 28.6, 33.7, 48.1, 45.7, 30.3, 32.7, 64.5, 29.8, 52.5, 48.4, 48.8, 26.4, 37.4, 33.2, 46.1, 29.5, -0.9, 49.8, 34.1, 48.9, 12.5, 36.6, 22.1, 57.3, 9.5, 9.4, 58.5, 50.2, 45.3, 25.3, 27.4, 4.5, 58.5, 63.4, 48.7, 42.6, 33, 47.9, 30.3, 54.9, 7.9, 50.2, 11.2, 59.7, 46.5, 57.5, 26.9, 28.5, 29.7, 52.5, 16.9, 29.8, 28.6, 31.2, 65.3, 1.7, 31.4, 52.5, 5.1, 66.1, 51.5, 9.5, 9.8, 41.6, 0.3, 10.4, 15.5, 34.8, 27.5, 43.6, 31.4, 46.3, 4.6, 45.8, 49.2, 10.7, 48.1, 7.3, 33.4, 10.7, 53.4, 28.9, 51.1, 52.4, 55.9, 56.8, 47.2, 46.8, 30.8, 60.3, 53.6, 30.9, 70.8, 11.2, 7.5, 55.8, 14.3, 25.8, 14.5, 30.9, 60.8, 26.8, 16.5, 31.4, 26.6, 10.6, 53.4, 33.1, 33.1, 46.3, 8.2, 56, 14.1, 25.5, 59.6, 61.9, 58.6, 63.1, 47.7, 30.5, 42.4, 56.2, 17, 13.4, 34.4, 1.1, 18.4, 63.9, 38.6, 15, 30.1, 23.9, 5.9, 53.8, 18.2, 22.7, 45.7, 29.2, 8.4, 52.5, 42, 28.7, 61.7, 35.4, 32.5, 5.5, 6.8, 60.1, 29.4, 31.5, 2.3, 28.3, 29.6, 34.9, 33.2, 28.9, 33.9, 51, 35.4, 52.3, 60, 27.1, 24.7, 57.7, 32.7, 52.5, 66.3, 37.8, 46.3, 38.1, 30.6, 55.6, 44.9, 28.4, 28.9, 19, 7.7, 9.4, 36, 49.9, 42.2, 28.2, 11.5, 52.4, 46.3, 52.4, 27.4, 15.6, 62.3, 51.7, 41.6, 6.2, 10.5, 14.7, 30.4, 23.9, 58.7, 36.1, 47.6, 31.2, 29.1, 60.1, 18, 30, 56.5, 42.7, 27.1, 45.5, 36.6, 46.4, 25.9, 15.4, 31.6, 3.3, 33.6, 63.3, 57.1, 32.3, 11.8, 32.9, 47.2, 31.2, 49.3, 61.7, 11.5, 9.7, 49.6, 45.7, 16.1, 27.4, 22.8, 8.5, 56.2, 26, 45.7, 29, 34.6, 29.4, 3.9, 45.7, 31.7, 52.6, 40.2, 35.5, 5.8, 56.4, 49.5, 30.6, 40.2, 20.8, 43.9, 32.1, 40.8, 45.6, 32.8, 7.4, 27.5, 29.4, 50.8, 43.9, 36.8, 5.5, 61.5, 41.5, 47.5, 13.9, 30.1, 67.3, 27.1, 50.8, 37.4, 28, 25, 37.1, 49.3, 25.3, 26.9, 34.9, 51.8, 33.9, 34.7, 44.2, 10.1, 71.3, 47.5, 23.4, 45.7, 49.4, 32.6, 6.9, 67.8, 56.8, 41.9, 50.7, 31.5, 55, 14.2, 34.8, 26.2, 25.8, 64, 63.8, 56.4, 42.1, 29.5, 49.4, 30.2, 16.2, 30, -0.2, 30.7, 29.6, 57, 41.5, 6.4, 9.7, 47.1, 19.4, 39.8)
xExample <- c(xExample, 32.9, 53.6, 8.4, 32.8, 63.1, 58.4, 7.5, 26, 41.8, 29, 36.9, 41.5, 39.5, 14.1, 27.4, 14.9, 48.4, 34.8, 72.8, 36.9, 27.8, 27.6, 6.1, 43.8, 36.9, 58.5, 55.1, 45.2, 2.6, 20.4, 59, 60.6, 57.7, 29.8, 60.2, 36.9, 29, 28, 46.5, 55, 29.6, 52.6, 38, 45.3, 5.7, 44.8, 35.3, 56.1, 30.3, 32.4, 56.9, 30.8, 44.8, 62.8, 46.1, 57.2, 50.5, 46.4, 37.6, 29.9, 8.6, 35.5, 47.4, 27.2, 36.4, 33.1, 29.4, 25.8, 46, 27.6, 45.7, 32.3, 12.8, 49.8, 13.7, 65.3, 48.5, 39.6, 4, 32.1, 49.6, 44, 74.5, 31, 52.6, 33.3, 56.8, 11.4, 33.7, 34.3, 25.8, 39.8, 7.3, 33.6, 7.9, 49.6, 52.6, 36.5, 43, 14.7, 43.5, 37, 50.8, 46.5, 46.9, 25.4, 32.7, 48.4, 40.3, 45.9, 51.3, 24, 48.3, 39.5, 21.2, 48.1, 56.9, 32.3, 10.2, 9.3, 40.3, 52.8, 34.5, 32.4, 30.1, 10.8, -3.8, 30.4, 58.2, 57.3, 48.9, 36.1, 46.2, 69, 67.8, 58.5, 41.9, 29.6, 51.7, 39.4, 50.8, 29.2, 56.1, 54.4, 17.2, 57.5, 54.1, 48.6, -0.9, 56.3, 27.7, 58.8, 57, 44.1, 6.3, 4.1, 35.9, 60.2, 44.1, 53.9, 33.3, 35.4, 32.1, 56, 56.8, 30.1, 43.1, 64.6, 27.7, 30.7, 53, 66, 29.1, 45, 12.3, 41.3, 54.7, 45.3, 13.3, 9.7, -2, 29.1, 29.5, 31.3, 29.2, 13.8, 26.7, 7.4, 36.8, 42.6, 54.7, 51.3, 42.6, 18, 34, 44.1, 53.6, 44.7, 28.9, 64.9, 60, 66.6, 32.9, 15.5, 37.6, 8.3, 28.5, 16.2, 39.7, 25.9, 8.8, 30.9, 9.9, 39.3, 66.4, 62.4, 53.8, 9.3, 44.7, 50.4, 57.8, 29, 50.1, 28.5, 62.9, 16.3, 54, 45.4, 60.6, 9, 7.7, 64.2, 54.4, 53.3, 45.5, 38, 5.2, 61.7, 10.8, 4.3, 24.8, 26.5, 32.2, 4.5, 49.3, 3.9, 39.6, 26.8, 36.3, 65.1, 59.6, 61.3, 30.1, 65.5, 55.8, 48.2, 49.8, 11.2, 64.2, 29, 44.6, 59.9, 12.6, 51.8, 14.5, 28.8, 49.8, 30.4, 42.7, 2.8, 31.1, 29.2, 27.4, 49.9, 28.2, 59.5, 28.7, 9.4, 30.2, 33.3, 30, 26, 65.1, 55.9, 30.5, 61.1, 50.3, 31.3, 58.2, 41.3, 33.4, 14.8, 51.2, 40.8, 34.1, 33.7, 29.4, 56, 26.4, 30.7, 55.1, 49.7, 37.7, 56.9, 38.5, 28.8, 50.3, 45.7, 13.2, 32.8, 30.5, 30.6, 61.5, 57.7, 33.6, 24.6, 53.9, 36.1, 37.4, 55.5, 27.4, 44.2, 15.4, 56.3, 28.1, 28.8, 67.6, 17.7, 48.5, 57.5, 33.7, 12.9, 19.5, 30.6, 56.8, 75.4, 26, 32.3, 28.3, 10.7, 9, 66.5, 51.6, 30.2, 46, 44.1, 53, 33.9, 28.4, 53.1, 42.3, 55.2, 42.4, 9.4, 36.3, 26.6, 41.2, 33, 42.1, 27, 25.4, 53.8, 56.7, 22.2, 29.5, 30.9, 9.3, 30.4, 48.1, 30.9, 28.4, 38.6, 28.8, 52, 16.5, 64.3, 56.1, 51.4, 50.2, 30.1, 67.3, 62.3, 12.9, 27.9, 38.9, 29.3, 17.4, 30, 62.5, 40.5, 48, 31.9, 54.7, 27.4, 28.2, 46.6, 14, 61.9, 59.4, 65.4, 30.2, 28.9, 35.4, 55.8, 51.4, 47.8, 34, 56.2, 26.5, 30.2, 8.4, 10.9, 63.9, 41.9, 31.3, 52.8, 36, 45.4, -2, 57.3, 80.3, 41, 13.8, 31.9, 33.8, 48.5, 16.7, 29.5, 6.7, 42.1, 32.2, 45.7, 18.9, 30.5, 30.9, 40.2, 14.6, 41.2, 27, 6.1, 34.9, 57.5, 30.1, 56.6, 62.4, 11.5, 25.7, 14.8, 28.2, 43.5, 37.7, 32.1, 44.4, 56.2, 7.6, 29.4, 63.4, 53, 14.6, 50.1, 62.6, 29.3, 33.5, 52.7)
mix_assign <- c(1, 2, 1, 1, 1, 2, 2, 2, 3, 3, 2, 1, 2, 2, 3, 2, 2, 1, 3, 3, 3, 1, 2, 1, 2, 2, 3, 1, 2, 3, 2, 3, 2, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 1, 2, 3, 1, 3, 3, 1, 2, 1, 2, 3, 2, 1, 2, 3, 3, 2, 1, 2, 2, 3, 2, 1, 1, 2, 2, 3, 2, 2, 1, 2, 3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 1, 2, 3, 1, 2, 2, 1, 3, 2, 3, 1, 2, 3, 1, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 3, 2, 2, 2, 3, 2, 1, 1, 2, 2, 1, 2, 3, 3, 2, 3, 1, 3, 3, 2, 2, 1, 3, 3, 1, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 2, 2, 3, 1, 3, 3, 2, 2, 3, 1, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 1, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 2, 1, 1, 3, 3, 3, 3, 3, 3, 2, 2, 2, 1, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 1, 2, 3, 2, 1, 3, 3, 2, 1, 1, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 1, 2, 2, 2, 3, 3, 3, 2, 1, 3, 3, 2, 2, 1, 3, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 3, 3, 2, 3, 2, 1, 2, 2, 1, 2, 1, 3, 1, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 1, 1, 2, 1, 3, 1, 3, 2, 3, 1, 3, 3, 1, 2, 3, 2, 2, 1, 2, 1, 3, 2, 2, 2, 2, 2, 3, 2, 2, 1, 1, 3, 1, 1, 2, 3, 1, 3, 3, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 2, 3, 1, 1, 2, 3, 2, 1, 3, 3, 2, 3, 3, 3, 2, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 3, 1, 1, 1, 2, 2, 2, 3, 1, 2, 2, 2, 3, 1, 2, 2, 2, 1, 1, 1, 3, 3, 2, 3, 2, 3, 2, 2, 1, 3, 2, 2, 3, 2, 3, 2, 3, 1, 3, 1, 3, 2, 2, 3, 1, 3, 2, 3, 2, 2, 1, 1, 2, 2, 1, 3, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 2, 2, 2, 3, 1, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 1, 3, 3, 2, 2, 2, 1, 2, 2, 2, 1, 3, 2, 3, 2, 2, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 2, 1, 2, 2, 3, 2, 2, 3, 1, 2, 2, 2, 2, 3, 2, 1, 3, 3, 2, 2, 2, 2, 2, 3, 2, 3, 1, 3, 1, 3, 3, 2, 2, 1, 1, 2, 2, 2, 3, 2, 1, 3, 2, 2, 1, 3, 2, 3, 2, 2, 2, 1)
mix_assign <- c(mix_assign, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 3, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 1, 2, 1, 2, 2, 2, 1, 3, 2, 2, 2, 3, 2, 3, 2, 1, 3, 2, 3, 2, 1, 2, 1, 2, 2, 3, 2, 1, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 2, 1, 2, 2, 3, 1, 1, 2, 2, 3, 3, 3, 1, 1, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 1, 2, 3, 2, 2, 2, 1, 1, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 1, 2, 2, 2, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 3, 2, 2, 2, 3, 1, 2, 1, 3, 1, 2, 3, 1, 3, 1, 3, 2, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 3, 3, 3, 1, 2, 1, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 1, 2, 3, 2, 2, 1, 2, 1, 3, 2, 3, 2, 1, 3, 3, 3, 2, 3, 2, 3, 1, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 3, 1, 2, 2, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 1, 2, 3, 3, 2, 1, 2, 2, 2, 1, 1, 3, 2, 2, 3, 3, 3, 1, 1, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 1, 3, 3, 2, 3, 2, 3, 3, 2, 2, 2, 3, 2, 1, 3, 2, 3, 3, 2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 2, 1, 3, 2, 2, 1, 3, 2, 2, 2, 3, 2, 3, 3, 2, 1, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3, 1, 1, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 1, 3, 3, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 1, 3, 2, 3, 2, 2, 1, 3, 1, 3, 2, 2, 3, 2, 2, 1, 3, 2, 2, 2, 2, 2, 3, 3, 2)

mix_example <- data.frame(x=xExample, assignment=mix_assign)
str(mix_example)

library(flexmix)
set.seed(1515)

fit_mix_example <- flexmix(x ~ 1, data = mix_example, k = 3, model = FLXMCnorm1(), 
                           control = list(tolerance = 1e-15, verbose = 1, iter = 1e4)
                           )

proportions <- prior(fit_mix_example)
comp_1 <- parameters(fit_mix_example, component = 1)
comp_2 <- parameters(fit_mix_example, component = 2)
comp_3 <- parameters(fit_mix_example, component = 3)


fun_prop <- function(x, mean, sd, proportion){
    proportion * dnorm(x = x, mean = mean, sd = sd)
}

ggplot(mix_example) + 
    geom_histogram(aes(x = x, y = ..density..)) + 
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_1[1], sd = comp_1[2], proportion = proportions[1])
                  ) +
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_2[1], sd = comp_2[2], proportion = proportions[2])
                  ) +
    stat_function(geom = "line", fun = fun_prop, 
                  args = list(mean = comp_3[1], sd = comp_3[2], proportion = proportions[3])
                  )


# Explore the first assignments
head(clusters(fit_mix_example))

# Explore the first real labels
head(mix_example$assignment)

# Create frequency table
table(mix_example$assignment, clusters(fit_mix_example))


genderData <- readr::read_csv("./RInputFiles/gender.csv")
str(genderData)

set.seed(1313)
fit_with_covariance <- flexmix(cbind(Weight, BMI) ~ 1, data = genderData, k = 2, 
                               model = FLXMCmvnorm(diag = FALSE), 
                               control = list(tolerance = 1e-15, iter.max = 1000)
                               )

# Get the parameters
comp_1 <- parameters(fit_with_covariance, component = 1)
comp_2 <- parameters(fit_with_covariance, component = 2)

# The means
mean_comp_1 <- comp_1[1:2]
mean_comp_1
mean_comp_2 <- comp_2[1:2]
mean_comp_2

# The covariance matrices
covariance_comp_1 <- matrix(comp_1[3:6], nrow = 2)
covariance_comp_1
covariance_comp_2 <- matrix(comp_2[3:6], nrow = 2)
covariance_comp_2


# Create ellipse curve 1
ellipse_comp_1 <- ellipse::ellipse(x = covariance_comp_1, centre = mean_comp_1, npoints = nrow(genderData))
head(ellipse_comp_1)

# Create ellipse curve 2
ellipse_comp_2 <- ellipse::ellipse(x = covariance_comp_2, centre = mean_comp_2, npoints = nrow(genderData))
head(ellipse_comp_2)


# Plot the ellipses
genderData %>% 
    ggplot(aes(x = Weight, y = BMI)) + geom_point()+
    geom_path(data = data.frame(ellipse_comp_1), aes(x=x,y=y), col = "red") +
    geom_path(data = data.frame(ellipse_comp_2), aes(x=x,y=y), col = "blue")

# Check the assignments
table(genderData$Gender, clusters(fit_with_covariance))

```
  
  
  
***
  
Chapter 4 - Mixture Models Beyond Gaussians  
  















