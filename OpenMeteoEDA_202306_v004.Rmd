---
title: "Open Meteo Weather Exploration"
author: "davegoblue"
date: "2024-06-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache.lazy=FALSE)
```

## Background
Open-Meteo maintains an [API for historical weather](https://open-meteo.com/en/docs/historical-weather-api) that allows for non-commercial usage of historical weather data maintained by the website.

This file builds on _v001, _v002, and _v003 to run exploratory analysis on some historical weather data.

## Functions and Libraries
The exploration process uses tidyverse, ranger, several generic custom functions, and several functions specific to Open Meteo processing. First, tidyverse, ranger, and the generic functions are loaded:  
```{r}

library(tidyverse) # tidyverse functionality is included throughout
library(ranger) # predict() does not work on ranger objects unless ranger has been called

source("./Generic_Added_Utility_Functions_202105_v001.R") # Basic functions

```
  
Next, specific functions written in _v001 are copied:  
```{r}

# Helper function for reading a partial CSV file
partialCSVRead <- function(loc, firstRow=1L, lastRow=+Inf, col_names=TRUE, ...) {
    
    # FUNCTION arguments
    # loc: file location
    # firstRow: first row that is relevant to the partial file read (whether header line or data line)
    # last Row: last row that is relevant to the partial file read (+Inf means read until last line of file)
    # col_names: the col_names parameter passed to readr::read_csv
    #            TRUE means header=TRUE (get column names from file, read data starting on next line)
    #            FALSE means header=FALSE (auto-generate column names, read data starting on first line)
    #            character vector means use these as column names (read data starting on first line)
    # ...: additional arguments passed to read_csv

    # Read the file and return
    # skip: rows to be skipped are all those prior to firstRow
    # n_max: maximum rows read are lastRow-firstRow, with an additional data row when col_names is not TRUE
    readr::read_csv(loc, 
                    col_names=col_names,
                    skip=firstRow-1, 
                    n_max=lastRow-firstRow+ifelse(isTRUE(col_names), 0, 1), 
                    ...
                    )
    
}


# Get the break points for gaps in a vector (e.g., 0, 3, 5:8, 20 has break points 0, 3, 5, 20 and 0, 3, 8, 30)
vecGaps <- function(x, addElements=c(), sortUnique=TRUE) {
    
    if(length(addElements)>0) x <- c(addElements, x)
    if(isTRUE(sortUnique)) x <- unique(sort(x))
    list("starts"=c(x[is.na(lag(x)) | x-lag(x)>1], +Inf), 
         "ends"=x[is.na(lead(x)) | lead(x)-x>1]
         )
    
}


# Find the break points in a single file
flatFileGaps <- function(loc) {

    which(stringr::str_length(readLines(loc))==0) %>% vecGaps(addElements=0)
    
}


# Read all relevant data as CSV with header
readMultiCSV <- function(loc, col_names=TRUE, ...) {

    gaps <- flatFileGaps(loc)
    
    lapply(seq_along(gaps$ends), 
           FUN=function(x) partialCSVRead(loc, 
                                          firstRow=gaps$ends[x]+1, 
                                          lastRow=gaps$starts[x+1]-1, 
                                          col_names=col_names, 
                                          ...
                                          )
           )
    
}


# Create URL with specified parameters for downloading data from Open Meteo
openMeteoURLCreate <- function(mainURL="https://archive-api.open-meteo.com/v1/archive", 
                               lat=45, 
                               lon=-90, 
                               startDate=paste(year(Sys.Date())-1, "01", "01", sep="-"), 
                               endDate=paste(year(Sys.Date())-1, "12", "31", sep="-"), 
                               hourlyMetrics=NULL, 
                               dailyMetrics=NULL,
                               tz="GMT", 
                               ...
                               ) {
    
    # Create formatted string
    fString <- paste0(mainURL, 
                      "?latitude=", 
                      lat, 
                      "&longitude=", 
                      lon, 
                      "&start_date=", 
                      startDate, 
                      "&end_date=", 
                      endDate
                      )
    if(!is.null(hourlyMetrics)) fString <- paste0(fString, "&hourly=", hourlyMetrics)
    if(!is.null(dailyMetrics)) fString <- paste0(fString, "&daily=", dailyMetrics)
    
    # Return the formatted string
    paste0(fString, "&timezone=", stringr::str_replace(tz, "/", "%2F"), ...)
    
}


# Helper function to simplify entry of parameters for Open Meteo download requests
helperOpenMeteoURL <- function(cityName=NULL,
                               lat=NULL,
                               lon=NULL,
                               hourlyMetrics=NULL,
                               hourlyIndices=NULL,
                               hourlyDesc=tblMetricsHourly,
                               dailyMetrics=NULL,
                               dailyIndices=NULL,
                               dailyDesc=tblMetricsDaily,
                               startDate=NULL, 
                               endDate=NULL, 
                               tz=NULL,
                               ...
                               ) {
    
    # Convert city to lat/lon if lat/lon are NULL
    if(is.null(lat) | is.null(lon)) {
        if(is.null(cityName)) stop("\nMust provide lat/lon or city name available in maps::us.cities\n")
        cityData <- maps::us.cities %>% tibble::as_tibble() %>% filter(name==cityName)
        if(nrow(cityData)!=1) stop("\nMust provide city name that maps uniquely to maps::us.cities$name\n")
        lat <- cityData$lat[1]
        lon <- cityData$long[1]
    }
    
    # Get hourly metrics by index if relevant
    if(is.null(hourlyMetrics) & !is.null(hourlyIndices)) {
        hourlyMetrics <- hourlyDesc %>% slice(hourlyIndices) %>% pull(metric)
        hourlyMetrics <- paste0(hourlyMetrics, collapse=",")
        cat("\nHourly metrics created from indices:", hourlyMetrics, "\n\n")
    }
    
    # Get daily metrics by index if relevant
    if(is.null(dailyMetrics) & !is.null(dailyIndices)) {
        dailyMetrics <- dailyDesc %>% slice(dailyIndices) %>% pull(metric)
        dailyMetrics <- paste0(dailyMetrics, collapse=",")
        cat("\nDaily metrics created from indices:", dailyMetrics, "\n\n")
    }
    
    # Use default values from OpenMeteoURLCreate() for startDate, endDate, and tz if passed as NULL
    if(is.null(startDate)) startDate <- eval(formals(openMeteoURLCreate)$startDate)
    if(is.null(endDate)) endDate <- eval(formals(openMeteoURLCreate)$endDate)
    if(is.null(tz)) tz <- eval(formals(openMeteoURLCreate)$tz)
    
    # Create and return URL
    openMeteoURLCreate(lat=lat,
                       lon=lon, 
                       startDate=startDate, 
                       endDate=endDate, 
                       hourlyMetrics=hourlyMetrics, 
                       dailyMetrics=dailyMetrics, 
                       tz=tz,
                       ...
                       )
    
}


# Read JSON data returned from Open Meteo
readOpenMeteoJSON <- function(js, mapDaily=tblMetricsDaily, mapHourly=tblMetricsHourly) {
    
    # FUNCTION arguments: 
    # js: JSON list returned by download from Open-Meteo
    # mapDaily: mapping file for daily metrics
    # mapHourly: mapping file for hourly metrics
    
    # Get the object and names
    jsObj <- jsonlite::read_json(js, simplifyVector = TRUE)
    nms <- jsObj %>% names()
    cat("\nObjects in JSON include:", paste(nms, collapse=", "), "\n\n")
    
    # Set default objects as NULL
    tblDaily <- NULL
    tblHourly <- NULL
    tblUnitsDaily <- NULL
    tblUnitsHourly <- NULL
    
    # Get daily and hourly as tibble if relevant
    if("daily" %in% nms) tblDaily <- jsObj$daily %>% tibble::as_tibble() %>% omProcessDaily()
    if("hourly" %in% nms) tblHourly <- jsObj$hourly %>% tibble::as_tibble() %>% omProcessHourly()
    
    # Helper function for unit conversions
    helperMetricUnit <- function(x, mapper, desc=NULL) {
        if(is.null(desc)) 
            desc <- as.list(match.call())$x %>% 
                deparse() %>% 
                stringr::str_replace_all(pattern=".*\\$", replacement="")
        x %>% 
            tibble::as_tibble() %>% 
            pivot_longer(cols=everything()) %>% 
            left_join(mapper, by=c("name"="metric")) %>% 
            mutate(value=stringr::str_replace(value, "\u00b0", "deg ")) %>% 
            mutate(metricType=desc) %>% 
            select(metricType, everything())
    }
    
    # Get the unit descriptions
    if("daily_units" %in% nms) tblUnitsDaily <- helperMetricUnit(jsObj$daily_units, mapDaily)
    if("hourly_units" %in% nms) tblUnitsHourly <- helperMetricUnit(jsObj$hourly_units, mapHourly)
    if(is.null(tblUnitsDaily) & !is.null(tblUnitsHourly)) tblUnits <- tblUnitsHourly
    else if(!is.null(tblUnitsDaily) & is.null(tblUnitsHourly)) tblUnits <- tblUnitsDaily
    else if(!is.null(tblUnitsDaily) & !is.null(tblUnitsHourly)) 
        tblUnits <- bind_rows(tblUnitsHourly, tblUnitsDaily)
    else tblUnits <- NULL
    
    # Put everything else together
    tblDescription <- jsObj[setdiff(nms, c("hourly", "hourly_units", "daily", "daily_units"))] %>%
        tibble::as_tibble()
    
    # Return the list objects
    list(tblDaily=tblDaily, tblHourly=tblHourly, tblUnits=tblUnits, tblDescription=tblDescription)
    
}


# Return Open meteo metadata in prettified format
prettyOpenMeteoMeta <- function(df, extr="tblDescription") {
    if("list" %in% class(df)) df <- df[[extr]]
    for(name in names(df)) {
        cat("\n", name, ": ", df %>% pull(name), sep="")
    }
    cat("\n\n")
}


# Process Open Meteo daily data
omProcessDaily <- function(tbl, extr="tblDaily") {
    if("list" %in% class(tbl)) tbl <- tbl[[extr]]
    tbl %>% mutate(date=lubridate::ymd(time)) %>% select(date, everything())
}


# Process Open meteo hourly data
omProcessHourly <- function(tbl, extr="tblHourly") {
    if("list" %in% class(tbl)) tbl <- tbl[[extr]]
    tbl %>% 
        mutate(origTime=time, 
               time=lubridate::ymd_hm(time), 
               date=lubridate::date(time), 
               hour=lubridate::hour(time)
               ) %>% 
        select(time, date, hour, everything())
}


# Simple predictive model for categorical variable
simpleOneVarPredict <- function(df, 
                                tgt, 
                                prd, 
                                dfTest=NULL,
                                nPrint=30, 
                                showPlot=TRUE, 
                                returnData=TRUE
                                ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble with key elements (training data set)
    # tgt: target variable
    # prd: predictor variable
    # dfTest: test dataset for applying predictions
    # nPrint: maximum number of lines of confusion matrix to print
    #         0 means do not print any summary statistics
    # showPlot: boolean, should overlap plot be created and shown?
    
    # Counts of predictor to target variable
    dfPred <- df %>%
        group_by(across(all_of(c(prd, tgt)))) %>%
        summarize(n=n(), .groups="drop") %>%
        arrange(across(all_of(prd)), desc(n)) %>%
        group_by(across(all_of(prd))) %>%
        mutate(correct=row_number()==1, predicted=first(get(tgt))) %>%
        ungroup()

    # Confusion matrix and accuracy
    dfConf <- dfPred %>%
        group_by(across(all_of(c(tgt, "correct")))) %>%
        summarize(n=sum(n), .groups="drop") %>%
        pivot_wider(id_cols=tgt, names_from=correct, values_from=n, values_fill=0) %>%
        mutate(n=`TRUE`+`FALSE`, 
               pctCorrect=`TRUE`/n, 
               pctNaive=1/(nrow(.)), 
               lift=pctCorrect/pctNaive-1
               )
    
    # Overall confusion matrix
    dfConfAll <- dfConf %>%
        summarize(nMax=max(n), across(c(`FALSE`, `TRUE`, "n"), sum)) %>%
        mutate(pctCorrect=`TRUE`/n, 
               pctNaive=nMax/n, 
               lift=pctCorrect/pctNaive-1, 
               nBucket=length(unique(dfPred[[prd]]))
               )
    
    # Print confusion matrices
    if(nPrint > 0) {
        cat("\nAccuracy by target subgroup (training data):\n")
        dfConf %>% print(n=nPrint)
        cat("\nOverall Accuracy (training data):\n")
        dfConfAll %>% print(n=nPrint)
    }
    
    # Plot of overlaps
    if(isTRUE(showPlot)) {
        p1 <- dfPred %>%
            group_by(across(c(all_of(tgt), "predicted", "correct"))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            ggplot(aes(x=get(tgt), y=predicted)) + 
            labs(x="Actual", 
                 y="Predicted", 
                 title=paste0("Training data - Actual vs. predicted ", tgt), 
                 subtitle=paste0("(using ", prd, ")")
                 ) + 
            geom_text(aes(label=n)) + 
            geom_tile(aes(fill=correct), alpha=0.25)
        print(p1)
    }
    
    # Create metrics for test dataset if requested
    if(!is.null(dfTest)) {
        # Get maximum category from training data
        mostPredicted <- count(dfPred, predicted, wt=n) %>% slice(1) %>% pull(predicted)
        # Get mapping of metric to prediction
        dfPredict <- dfPred %>% 
            group_by(across(all_of(c(prd, "predicted")))) %>% 
            summarize(n=sum(n), .groups="drop")
        # Create predictions for test data
        dfPredTest <- dfTest %>%
            select(all_of(c(prd, tgt))) %>%
            left_join(select(dfPredict, -n)) %>%
            replace_na(list(predicted=mostPredicted)) %>%
            group_by(across(all_of(c(prd, tgt, "predicted")))) %>%
            summarize(n=n(), .groups="drop") %>%
            mutate(correct=(get(tgt)==predicted))
        # Create confusion statistics for test data
        dfConfTest <- dfPredTest %>%
            group_by(across(all_of(c(tgt, "correct")))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            pivot_wider(id_cols=tgt, names_from=correct, values_from=n, values_fill=0) %>%
            mutate(n=`TRUE`+`FALSE`, 
                   pctCorrect=`TRUE`/n, 
                   pctNaive=1/(nrow(.)), 
                   lift=pctCorrect/pctNaive-1
                   )
        # Overall confusion matrix for test data
        dfConfAllTest <- dfConfTest %>%
            summarize(nMax=max(n), across(c(`FALSE`, `TRUE`, "n"), sum)) %>%
            mutate(pctCorrect=`TRUE`/n, 
                   pctNaive=nMax/n, 
                   lift=pctCorrect/pctNaive-1, 
                   nBucket=length(unique(dfConfTest[[prd]]))
               )
        # Print confusion matrices
        if(nPrint > 0) {
            cat("\nAccuracy by target subgroup (testing data):\n")
            dfConfTest %>% print(n=nPrint)
            cat("\nOverall Accuracy (testing data):\n")
            dfConfAllTest %>% print(n=nPrint)
            }
    } else {
        dfPredTest <- NULL
        dfConfTest <- NULL
        dfConfAllTest <- NULL
        
    }
    
    # Return data if requested
    if(isTRUE(returnData)) list(dfPred=dfPred, 
                                dfConf=dfConf, 
                                dfConfAll=dfConfAll, 
                                dfPredTest=dfPredTest, 
                                dfConfTest=dfConfTest, 
                                dfConfAllTest=dfConfAllTest
                                )
    
}


# Fit a single predictor to a single categorical variable
simpleOneVarFit <- function(df, 
                            tgt, 
                            prd, 
                            rankType="last", 
                            naMethod=TRUE
                            ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble with key elements (training data set)
    # tgt: target variable
    # prd: predictor variable
    # rankType: method for breaking ties of same n, passed to base::rank as ties.method=
    # naMethod: method for handling NA in ranks, passed to base::rank as na.last=
    
    # Counts of predictor to target variable, and associated predictions
    df %>%
        group_by(across(all_of(c(prd, tgt)))) %>%
        summarize(n=n(), .groups="drop") %>%
        arrange(across(all_of(prd)), desc(n), across(all_of(tgt))) %>%
        group_by(across(all_of(prd))) %>%
        mutate(rankN=n()+1-rank(n, ties.method=rankType, na.last=naMethod)) %>%
        arrange(across(all_of(prd)), rankN) %>%
        ungroup()

}


# Create categorical predictions mapper
simpleOneVarMapper <- function(df, tgt, prd) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame or tibble from SimpleOneVarFit()
    # tgt: target variable
    # prd: predictor variable
    
    # Get the most common actual results
    dfCommon <- df %>% count(across(all_of(tgt)), wt=n, sort=TRUE)
    
    # Get the predictions
    dfPredictor <- df %>%
        group_by(across(all_of(prd))) %>%
        filter(row_number()==1) %>%
        select(all_of(c(prd, tgt))) %>%
        ungroup()
    
    list(dfPredictor=dfPredictor, dfCommon=dfCommon)
    
}


# Map the categorical predictions to unseen data
simpleOneVarApplyMapper <- function(df, 
                                    tgt,
                                    prd, 
                                    mapper, 
                                    mapperDF="dfPredictor", 
                                    mapperDefault="dfCommon",
                                    prdName="predicted"
                                    ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame containing prd for predicting tgt
    # tgt: target variable in df
    # prd: predictor variable in df
    # mapper: mapping list from sinpleOneVarMapper()
    # mapperDF: element that can be used to merge mappings
    # mapperDefault: element that can be used for NA resulting from merging mapperDF
    # prdName: name for the prediction variable
    
    # Extract the mapper and default value
    vecRename <- c(prdName) %>% purrr::set_names(tgt)
    dfMap <- mapper[[mapperDF]] %>% select(all_of(c(prd, tgt))) %>% colRenamer(vecRename=vecRename)
    chrDefault <- mapper[[mapperDefault]] %>% slice(1) %>% pull(tgt)
    
    # Merge mappings to df
    df %>%
        left_join(dfMap, by=prd) %>%
        replace_na(list("predicted"=chrDefault))
    
}


# Create confusion matrix data for categorical predictions
simpleOneVarConfusionData <- function(df, 
                                      tgtOrig,
                                      tgtPred, 
                                      otherVars=c(),
                                      weightBy="n"
                                      ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame from simpleOneVarApplyMapper()
    # tgtOrig: original target variable name in df
    # tgtPred: predicted target variable name in df
    # otherVars: other variables to be kept (will be grouping variables)
    # weightBy: weighting variable for counts in df (NULL means count each row of df as 1)
    
    # Confusion matrix data creation
    df %>%
        group_by(across(all_of(c(tgtOrig, tgtPred, otherVars)))) %>%
        summarize(n=if(!is.null(weightBy)) sum(get(weightBy)) else n(), .groups="drop") %>%
        mutate(correct=get(tgtOrig)==get(tgtPred))
    
}


# Print and plot confusion matrix for categorical predictions
simpleOneVarConfusionReport <- function(df, 
                                        tgtOrig,
                                        tgtPred, 
                                        otherVars=c(), 
                                        printConf=TRUE,
                                        printConfOrig=printConf, 
                                        printConfPred=printConf,
                                        printConfOverall=printConf, 
                                        plotConf=TRUE, 
                                        plotDesc="",
                                        nBucket=NA, 
                                        predictorVarName="", 
                                        returnData=FALSE
                                        ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame from simpleOneVarConfusionData()
    # tgtOrig: original target variable name in df
    # tgtPred: predicted target variable name in df
    # otherVars: other variables to be kept (will be grouping variables) - NOT IMPLEMENTED
    # printConf: boolean, should confusion matrix data be printed? Applies to all three
    # printConfOrig: boolean, should confusion data be printed based on original target variable?
    # printConfPred: boolean, should confusion data be printed based on predicted target variable?
    # printConfOverall: boolean, should overall confusion data be printed?
    # plotConf: boolean, should confusion overlap data be plotted?
    # plotDesc: descriptive label to be included in front of plot title
    # nBucket: number of buckets used for prediction (pass from previous data)
    # predictorVarName: variable name to be included in chart description
    # returnData: boolean, should the confusion matrices be returned?
    
    # Confusion data based on original target variable
    if(isTRUE(printConfOrig) | isTRUE(returnData)) {
        dfConfOrig <- df %>%
            group_by(across(all_of(c(tgtOrig)))) %>%
            summarize(right=sum(n*correct), wrong=sum(n)-right, n=sum(n), .groups="drop") %>%
            mutate(pctRight=right/n, pctNaive=n/(sum(n)), lift=pctRight/pctNaive-1)
    }

    # Confusion data based on predicted target variable
    if(isTRUE(printConfPred) | isTRUE(returnData)) {
        dfConfPred <- df %>%
            group_by(across(all_of(c(tgtPred)))) %>%
            summarize(right=sum(n*correct), wrong=sum(n)-right, n=sum(n), .groups="drop") %>%
            mutate(pctRight=right/n)
    }

    # Overall confusion data
    if(isTRUE(printConfOverall) | isTRUE(returnData)) {
        maxNaive <- df %>%
            group_by(across(all_of(tgtOrig))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            arrange(desc(n)) %>%
            slice(1) %>%
            pull(n)
        dfConfOverall <- df %>%
            summarize(right=sum(n*correct), wrong=sum(n)-right, n=sum(n), .groups="drop") %>%
            mutate(maxN=maxNaive, pctRight=right/n, pctNaive=maxN/n, lift=pctRight/pctNaive-1, nBucket=nBucket)
    }
    
    # Confusion report based on original target variable
    if(isTRUE(printConfOrig)) {
        cat("\nConfusion data based on original target variable:", tgtOrig, "\n")
        dfConfOrig %>%
            print(n=50)
    }

    # Confusion report based on predicted target variable
    if(isTRUE(printConfPred)) {
        cat("\nConfusion data based on predicted target variable:", tgtPred, "\n")
        dfConfPred %>%
            print(n=50)
    }
    
    # Overall confusion matrix
    if(isTRUE(printConfOverall)) {
        cat("\nOverall confusion matrix\n")
        dfConfOverall %>%
            print(n=50)
    }
    
    # Plot of overlaps
    if(isTRUE(plotConf)) {
        p1 <- df %>%
            group_by(across(all_of(c(tgtOrig, tgtPred, "correct")))) %>%
            summarize(n=sum(n), .groups="drop") %>%
            ggplot(aes(x=get(tgtOrig), y=get(tgtPred))) + 
            labs(x="Actual", 
                 y="Predicted", 
                 title=paste0(plotDesc, "Actual vs. predicted ", tgtOrig), 
                 subtitle=paste0("(using ", predictorVarName, ")")
                 ) + 
            geom_text(aes(label=n)) + 
            geom_tile(aes(fill=correct), alpha=0.25)
        print(p1)
    }
    
    # Return data if requested
    if(isTRUE(returnData)) list(dfConfOrig=dfConfOrig, dfConfPred=dfConfPred, dfConfOverall=dfConfOverall)
    
}


# Process for chaining predictor, applier, and confusion matrix for categorical variables
simpleOneVarChain <- function(df,
                              tgt,
                              prd,
                              mapper=NULL, 
                              rankType="last", 
                              naMethod=TRUE, 
                              printReport=TRUE, 
                              plotDesc="",
                              returnData=TRUE, 
                              includeConfData=FALSE
                              ) {

    # FUNCTION ARGUMENTS:
    # df: data frame or tibble with key elements (training or testing data set)
    # tgt: target variable
    # prd: predictor variable
    # mapper: mapping file to be applied for predictions (NULL means create from simpleOneVarApply())
    # rankType: method for breaking ties of same n, passed to base::rank as ties.method=
    # naMethod: method for handling NA in ranks, passed to base::rank as na.last=    
    # printReport: boolean, should the confusion report data and plot be printed?
    # plotDesc: descriptive label to be included in front of plot title
    # returnData: boolean, should data elements be returned?
    # includeConfData: boolean, should confusion data be returned?
    
    # Create the summary of predictor-target-n
    dfFit <- simpleOneVarFit(df, tgt=tgt, prd=prd, rankType=rankType, naMethod=naMethod)     

    # Create the mapper if it does not already exist
    if(is.null(mapper)) mapper <- simpleOneVarMapper(dfFit, tgt=tgt, prd=prd)
    
    # Apply mapper to data
    dfApplied <- simpleOneVarApplyMapper(dfFit, tgt=tgt, prd=prd, mapper=mapper)

    # Create confusion data
    dfConfusion <- simpleOneVarConfusionData(dfApplied, tgtOrig=tgt, tgtPred="predicted")
    
    # Create confusion report if requested
    if(isTRUE(printReport) | isTRUE(includeConfData)) {
        dfConfReport <- simpleOneVarConfusionReport(df=dfConfusion, 
                                                    tgtOrig=tgt, 
                                                    tgtPred="predicted", 
                                                    nBucket=length(unique(dfApplied[[prd]])), 
                                                    predictorVarName=prd, 
                                                    printConf=printReport, 
                                                    plotConf=printReport,
                                                    plotDesc=plotDesc,
                                                    returnData=includeConfData
                                                    )
    }
    
    # Return data if requested
    if(isTRUE(returnData)) {
        ret <- list(dfFit=dfFit, mapper=mapper, dfApplied=dfApplied, dfConfusion=dfConfusion)
        if(isTRUE(includeConfData)) ret<-c(ret, list(dfConfData=dfConfReport))
        ret
    }
    
}


# Adds a train-test component for single variable predictions
simpleOneVarTrainTest <- function(dfTrain,
                                  dfTest,
                                  tgt,
                                  prd,
                                  rankType="last", 
                                  naMethod=TRUE, 
                                  printReport=FALSE, 
                                  includeConfData=TRUE, 
                                  returnData=TRUE
                              ) {

    # FUNCTION ARGUMENTS:
    # dfTrain: data frame or tibble with key elements (training data set)
    # dfTest: data frame or tibble with key elements (testing data set)
    # tgt: target variable
    # prd: predictor variable
    # rankType: method for breaking ties of same n, passed to base::rank as ties.method=
    # naMethod: method for handling NA in ranks, passed to base::rank as na.last=    
    # printReport: boolean, should the confusion report data and plot be printed?
    # includeConfData: boolean, should confusion data be returned?
    # returnData: boolean, should data elements be returned?
    
    # Fit the training data
    tmpTrain <- simpleOneVarChain(df=dfTrain, 
                                  tgt=tgt, 
                                  prd=prd,
                                  rankType=rankType,
                                  naMethod=naMethod,
                                  printReport=printReport,
                                  plotDesc="Training data: ",
                                  returnData=TRUE,
                                  includeConfData=includeConfData
                                  )
    
    # Fit the testing data
    tmpTest <- simpleOneVarChain(df=dfTest, 
                                 tgt=tgt, 
                                 prd=prd,
                                 mapper=tmpTrain$mapper,
                                 rankType=rankType,
                                 naMethod=naMethod,
                                 printReport=printReport,
                                 plotDesc="Testing data: ",
                                 returnData=TRUE,
                                 includeConfData=includeConfData
                                 )
    
    # Return data if requested
    if(isTRUE(returnData)) list(tmpTrain=tmpTrain, tmpTest=tmpTest)
    
}


# Plot the means by cluster and variable for a k-means object
plotClusterMeans <- function(km, nrow=NULL, ncol=NULL, scales="fixed") {

    # FUNCTION ARGUMENTS
    # km: object returned by stats::kmeans(...)
    # nrow: number of rows for faceting (NULL means default)
    # ncol: number of columns for faceting (NULL means default)
    # scales: passed to facet_wrap as scales=scales
    
    # Assess clustering by dimension
    p1 <- km$centers %>%
        tibble::as_tibble() %>%
        mutate(cluster=row_number()) %>%
        pivot_longer(cols=-c(cluster)) %>%
        ggplot(aes(x=fct_reorder(name, 
                                 value, 
                                 .fun=function(a) ifelse(length(a)==2, a[2]-a[1], diff(range(a)))
                                 ), 
                   y=value
                   )
               ) + 
        geom_point(aes(color=factor(cluster))) + 
        scale_color_discrete("Cluster") + 
        facet_wrap(~factor(cluster), nrow=nrow, ncol=ncol, scales=scales) +
        labs(title=paste0("Cluster means (kmeans, centers=", nrow(km$centers), ")"), 
             x="Metric", 
             y="Cluster mean"
             ) + 
        geom_hline(yintercept=median(km$centers), lty=2) +
        coord_flip()
    print(p1)
    
}


# Plot percentage by cluster
plotClusterPct <- function(df, km, keyVars, nRowFacet=1, printPlot=TRUE) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame initially passed to stats::kmeans(...)
    # km: object returned by stats::kmeans(...)
    # keyVars: character vector of length 1 (y-only, x will be cl) or length 2 (x, y, cl will facet)
    # nRowFacet: number of rows for facetting (only relevant if length(keyVars) is 2)
    # printPlot: boolean, should plot be printed? (if not true, plot will be returned)
    
    # Check length of keyVars
    if(!(length(keyVars) %in% c(1, 2))) stop("\nArgument keyVars must be length-1 or length-2\n")
    
    p1 <- df %>%
        mutate(cl=factor(km$cluster)) %>%
        group_by(across(c(all_of(keyVars), "cl"))) %>%
        summarize(n=n(), .groups="drop") %>%
        group_by(across(all_of(keyVars))) %>%
        mutate(pct=n/sum(n)) %>%
        ungroup() %>%
        ggplot() + 
        scale_fill_continuous(low="white", high="green") + 
        labs(title=paste0("Percentage by cluster (kmeans with ", nrow(km$centers), " centers)"), 
             x=ifelse(length(keyVars)==1, "Cluster", keyVars[1]), 
             y=ifelse(length(keyVars)==1, keyVars[1], keyVars[2])
             )
    if(length(keyVars)==1) p1 <- p1 + geom_tile(aes(fill=pct, x=cl, y=get(keyVars[1])))
    if(length(keyVars)==2) {
        p1 <- p1 + 
            geom_tile(aes(fill=pct, x=get(keyVars[1]), y=get(keyVars[2]))) + 
            facet_wrap(~cl, nrow=nRowFacet)
    }
    
    if(isTRUE(printPlot)) print(p1)
    else return(p1)
    
}


# Run k-means (or use passed k-means object) and plot centers and percentages of observations
runKMeans <- function(df, 
                      km=NULL,
                      vars=NULL, 
                      centers=2, 
                      nStart=1L, 
                      iter.max=10L, 
                      seed=NULL, 
                      plotMeans=FALSE,
                      nrowMeans=NULL,
                      plotPct=NULL, 
                      nrowPct=1, 
                      returnKM=is.null(km)
                      ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame for clustering
    # km: k-means object (will shut off k-means processing and run as plot-only)
    # vars: variables to be used for clustering (NULL means everything in df)
    # centers: number of centers
    # nStart: passed to kmeans
    # iter.max: passed to kmeans
    # seed: seed to be set (if NULL, no seed is set)
    # plotMeans: boolean, plot variable means by cluster?
    # nrowMeans: argument passed as nrow for faceting rows in plotClusterMeans() - NULL is default ggplot2
    # plotPct: list of character vectors to be passed sequentially as keyVars to plotClusterPct()
    #          NULL means do not run
    #          pctByCluster=list(c("var1"), c("var2", "var3")) will run plotting twice
    # nrowPct: argument for faceting number of rows in plotClusterPct()
    # returnKM: boolean, should the k-means object be returned?
    
    # Set seed if requested
    if(!is.null(seed)) set.seed(seed)
    
    # Get the variable names if passed as NULL
    if(is.null(vars)) vars <- names(df)
    
    # Run the k-means process if the object has not been passed
    if(is.null(km)) {
        km <- df %>%
            select(all_of(vars)) %>% 
            kmeans(centers=centers, iter.max=iter.max, nstart=nStart)
    }

    # Assess clustering by dimension if requested
    if(isTRUE(plotMeans)) plotClusterMeans(km, nrow=nrowMeans)
    if(!is.null((plotPct))) 
        for(ctr in 1:length(plotPct)) 
            plotClusterPct(df=df, km=km, keyVars=plotPct[[ctr]], nRowFacet=nrowPct)
    
    # Return the k-means object
    if(isTRUE(returnKM)) return(km)
    
}


# Assign points to closest center of a passed k-means object
assignKMeans <- function(km, df, returnAllDistanceData=FALSE) {
    
    # FUNCTION ARGUMENTS:
    # km: a k-means object
    # df: data frame or tibble
    # returnAllDistanceData: boolean, should the distance data and clusters be returned?
    #                        TRUE returns a data frame with distances as V1, V2, ..., and cluster as cl
    #                        FALSE returns a vector of cluster assignments as integers
    
    # Select columns from df to match km
    df <- df %>% select(all_of(colnames(km$centers)))
    if(!all.equal(names(df), colnames(km$centers))) stop("\nName mismatch in clustering and frame\n")
    
    # Create the distances and find clusters
    distClust <- sapply(seq_len(nrow(km$centers)), 
                        FUN=function(x) sqrt(rowSums(sweep(as.matrix(df), 
                                                           2, 
                                                           t(as.matrix(km$centers[x,,drop=FALSE]))
                                                           )**2
                                                     )
                                             )
                        ) %>% 
        as.data.frame() %>% 
        tibble::as_tibble() %>% 
        mutate(cl=apply(., 1, which.min))
    
    # Return the proper file
    if(isTRUE(returnAllDistanceData)) return(distClust)
    else return(distClust$cl)
    
}


```

As well, specific functions from _v002 and _v003 are copied:  
```{r}

runSimpleRF <- function(df, yVar, xVars=NULL, ...) {

    # FUNCTION ARGUMENTS:
    # df: data frame containing observations
    # yVar: variable to be predicted (numeric for regression, categorical for classification)
    # xVars: predictor variables (NULL means everything in df except for yVar)
    # ...: other arguments passed to ranger::ranger
    
    # Create xVars if passed as NULL
    if(is.null(xVars)) xVars <- setdiff(names(df), yVar)
    
    # Simple random forest model
    ranger::ranger(as.formula(paste0(yVar, "~", paste0(xVars, collapse="+"))), 
                   data=df[, c(yVar, xVars)], 
                   ...
                   )
    
}

plotRFImportance <- function(rf, 
                             impName="variable.importance", 
                             divBy=1000, 
                             plotTitle=NULL, 
                             plotData=TRUE, 
                             returnData=!isTRUE(plotData)
                             ) {
    
    # FUNCTION ARGUMENTS:
    # rf: output list from random forest with an element for importance
    # impName: name of the element to extract from rf
    # divBy: divisor for the importance variable
    # plotTitle: title for plot (NULL means use default)
    # plotData: boolean, should the importance plot be created and printed?
    # returnData: boolean, should the processed data be returned?
    
    # Create title if not provided
    if(is.null(plotTitle)) plotTitle <- "Importance for simple random forest"

    # Create y-axis label
    yAxisLabel="Variable Importance"
    if(!isTRUE(all.equal(divBy, 1))) yAxisLabel <- paste0(yAxisLabel, " (", divBy, "s)")
    
    # Create variable importance
    df <- rf[[impName]] %>% 
        as.data.frame() %>% 
        purrr::set_names("imp") %>% 
        rownames_to_column("metric") %>% 
        tibble::as_tibble() 
    
    # Create and print plot if requested
    if(isTRUE(plotData)) {
        p1 <- df %>%
            ggplot(aes(x=fct_reorder(metric, imp), y=imp/divBy)) + 
            geom_col(fill="lightblue") + 
            labs(x=NULL, y=yAxisLabel, title=plotTitle) +
            coord_flip()
        print(p1)
    }
    
    # Return data if requested
    if(isTRUE(returnData)) return(df)
    
}

predictRF <- function(rf, df, newCol="pred", predsOnly=FALSE) {
    
    # FUNCTION ARGUMENTS:
    # rf: a trained random forest model
    # df: data frame for adding predictions
    # newCol: name for new column to be added to df
    # predsOnly: boolean, should only the vector of predictions be returned?
    #            if FALSE, a column named newCol is added to df, with df returned

    # Performance on holdout data
    preds <- predict(rf, data=df)$predictions
    
    # Return just the predictions if requested otherwise add as final column to df
    if(isTRUE(predsOnly)) return(preds)
    else {
        df[newCol] <- preds
        return(df)
    }
    
}

# Update for continuous variables
reportAccuracy <- function(df, 
                           trueCol, 
                           predCol="pred", 
                           reportAcc=TRUE, 
                           rndReport=2, 
                           useLabel="requested data",
                           returnAcc=!isTRUE(reportAcc), 
                           reportR2=FALSE
                           ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame containing actual and predictions
    # trueCol: column containing true value
    # predCol: column containing predicted value
    # reportAcc: boolean, should accuracy be reported (printed to output)?
    # rndReport: number of significant digits for reporting (will be converted to percentage first)
    # useLabel: label for data to be used in reporting
    # returnAcc: boolean, should the accuracy be returned 
    #            return value is not converted to percentage, not rounded
    # reportR2: boolean, should accuracy be calculated as R-squared?
    #           (default FALSE measures as categorical)
    
    # Continuous or categorical reporting
    if(isTRUE(reportR2)) {
        tc <- df %>% pull(get(trueCol))
        pc <- df %>% pull(get(predCol))
        mseNull <- mean((tc-mean(tc))**2)
        msePred <- mean((tc-pc)**2)
        r2 <- 1 - msePred/mseNull
        if(isTRUE(reportAcc)) 
            cat("\nR-squared of ", 
                useLabel, 
                " is: ", 
                round(100*r2, rndReport), 
                "% (RMSE ",
                round(sqrt(msePred), 2), 
                " vs. ", 
                round(sqrt(mseNull), 2),
                " null)\n", 
                sep=""
                )
        acc <- c("mseNull"=mseNull, "msePred"=msePred, "r2"=r2)
    } else {
        acc <- mean(df[trueCol]==df[predCol])
        if(isTRUE(reportAcc)) 
            cat("\nAccuracy of ", useLabel, " is: ", round(100*acc, rndReport), "%\n", sep="")    
    }
    
    # Return accuracy statistic if requested
    if(isTRUE(returnAcc)) return(acc)
    
}

# Update for automated rounding
plotConfusion <- function(df, 
                          trueCol, 
                          predCol="pred", 
                          useTitle=NULL,
                          useSub=NULL, 
                          plotCont=FALSE, 
                          rndTo=NULL,
                          rndBucketsAuto=100,
                          nSig=NULL,
                          refXY=FALSE
                          ) {
    
    # FUNCTION ARGUMENTS:
    # df: data frame containing actual and predictions
    # trueCol: column containing true value
    # predCol: column containing predicted value
    # useTitle: title to be used for chart (NULL means create from trueCol)
    # useSub: subtitle to be used for chart (NULL means none)
    # plotCont: boolean, should plotting assume continuous variables?
    #           (default FALSE assumes confusion plot for categorical variables)
    # rndTo: every number in x should be rounded to the nearest rndTo
    #        NULL means no rounding (default)
    #        -1L means make an estimate based on data
    # rndBucketsAuto: integer, if rndTo is -1L, about how many buckets are desired for predictions?
    # nSig: number of significant digits for automatically calculated rounding parameter
    #       (NULL means calculate exactly)
    # refXY: boolean, should a reference line for y=x be included? (relevant only for continuous)
    
    # Create title if not supplied
    if(is.null(useTitle)) useTitle <- paste0("Predicting ", trueCol)

    # Function auto-round returns vector as-is when rndTo is NULL and auto-rounds when rndTo is -1L
    df <- df %>%
        mutate(across(all_of(c(trueCol, predCol)), 
                      .fns=function(x) autoRound(x, rndTo=rndTo, rndBucketsAuto=rndBucketsAuto, nSig=nSig)
                      )
               )
    
    # Create base plot (applicable to categorical or continuous variables)
    # Use x as true and y as predicted, for more meaningful geom_smooth() if continuous
    # Flip coordinates if categorical
    p1 <- df %>%
        group_by(across(all_of(c(trueCol, predCol)))) %>%
        summarize(n=n(), .groups="drop") %>%
        ggplot(aes(y=get(predCol), x=get(trueCol))) + 
        labs(y="Predicted", x="Actual", title=useTitle, subtitle=useSub)
        
    # Update plot as appropriate
    if(isTRUE(plotCont)) {
        p1 <- p1 +
            geom_point(aes(size=n), alpha=0.5) + 
            scale_size_continuous("# Obs") +
            geom_smooth(aes(weight=n), method="lm")
        if(isTRUE(refXY)) p1 <- p1 + geom_abline(slope=1, intercept=0, lty=2, color="red")
    } else {
        p1 <- p1 + 
            geom_tile(aes(fill=n)) + 
            geom_text(aes(label=n), size=2.5) +
            coord_flip() +
            scale_fill_continuous("", low="white", high="green")
    }
    
    # Output plot
    print(p1)
    
}

runFullRF <- function(dfTrain, 
                      yVar, 
                      xVars, 
                      dfTest=dfTrain,
                      useLabel="test data",
                      useSub=NULL, 
                      isContVar=FALSE,
                      rndTo=NULL,
                      rndBucketsAuto=100,
                      nSig=NULL,
                      refXY=FALSE,
                      makePlots=TRUE,
                      plotImp=makePlots,
                      plotConf=makePlots,
                      returnData=FALSE, 
                      ...
                      ) {
    
    # FUNCTION ARGUMENTS:
    # dfTrain: training data
    # yVar: dependent variable
    # xVars: column(s) containing independent variables
    # dfTest: test dataset for applying predictions
    # useLabel: label to be used for reporting accuracy
    # useSub: subtitle to be used for confusion chart (NULL means none)
    # isContVar: boolean, is the variable continuous? (default FALSE means categorical)
    # rndTo: every number in x should be rounded to the nearest rndTo
    #        NULL means no rounding (default)
    #        -1L means make an estimate based on data
    # rndBucketsAuto: integer, if rndTo is -1L, about how many buckets are desired for predictions?
    # nSig: number of significant digits for automatically calculated rounding parameter
    #       (NULL means calculate exactly)    
    # refXY: boolean, should a reference line for y=x be included? (relevant only for continuous)
    # makePlots: boolean, should plots be created for variable importance and confusion matrix?
    # plotImp: boolean, should variable importance be plotted? (default is makePlots)
    # plotConf: boolean, should confusion matrix be plotted? (default is makePlots)
    # returnData: boolean, should data be returned?
    # ...: additional parameters to pass to runSimpleRF(), which are then passed to ranger::ranger()

    # 1. Run random forest using impurity for importance
    rf <- runSimpleRF(df=dfTrain, yVar=yVar, xVars=xVars, importance="impurity", ...)

    # 2. Create, and optionally plot, variable importance
    rfImp <- plotRFImportance(rf, plotData=plotImp, returnData=TRUE)

    # 3. Predict on test dataset
    tstPred <- predictRF(rf=rf, df=dfTest)

    # 4. Report on accuracy (updated for continuous or categorical)
    rfAcc <- reportAccuracy(tstPred, 
                            trueCol=yVar, 
                            rndReport=3, 
                            useLabel=useLabel, 
                            reportR2=isTRUE(isContVar),
                            returnAcc=TRUE
                            )

    # 5. Plot confusion data (updated for continuous vs. categorical) if requested
    if(isTRUE(plotConf)) {
        plotConfusion(tstPred, 
                      trueCol=yVar, 
                      useSub=useSub, 
                      plotCont=isTRUE(isContVar), 
                      rndTo=rndTo, 
                      rndBucketsAuto=rndBucketsAuto,
                      nSig=nSig,
                      refXY=refXY
                      )
    }
    
    #6. Return data if requested
    if(isTRUE(returnData)) return(list(rf=rf, rfImp=rfImp, tstPred=tstPred, rfAcc=rfAcc))
    
}

runPartialImportanceRF <- function(dfTrain, 
                                   yVar, 
                                   dfTest,
                                   impDB=dfImp,
                                   nImp=+Inf,
                                   otherX=c(),
                                   isContVar=TRUE, 
                                   useLabel=keyLabel, 
                                   useSub=stringr::str_to_sentence(keyLabel), 
                                   rndTo=NULL,
                                   rndBucketsAuto=50,
                                   nSig=NULL,
                                   refXY=FALSE,
                                   makePlots=FALSE, 
                                   returnElem=c("rfImp", "rfAcc")
                                   ) {
    
    # FUNCTION ARGUMENTS
    # dfTrain: training data
    # yVar: y variable in dfTrain
    # dfTest: test data
    # impDB: tibble containing variable importance by dependent variable
    # nImp: use the top nImp variables by variable importance
    # otherX: include these additional x variables
    # isContVar: boolean, is this a continuous variable (regression)? FALSE means classification
    # useLabel: label for description
    # useSub: label for plot
    # rndTo: controls the rounding parameter for plots, passed to runFullRF 
    #        (NULL means no rounding)
    #        -1L means make an estimate based on underlying data
    # rndBucketsAuto: integer, if rndTo is -1L, about how many buckets are desired for predictions?
    # nSig: number of significant digits for automatically calculated rounding parameter
    #       (NULL means calculate exactly)    
    # refXY: controls the reference line parameter for plots, passed to runFullRF
    # makePlots: boolean, should plots be created?
    # returnElem: character vector of list elements to be returned

    runFullRF(dfTrain=dfTrain, 
              yVar=yVar, 
              xVars=unique(c(impDB %>% filter(n<=nImp, src==yVar) %>% pull(metric), otherX)), 
              dfTest=dfTest, 
              isContVar = isContVar, 
              useLabel=useLabel, 
              useSub=useSub, 
              rndTo=rndTo,
              rndBucketsAuto=rndBucketsAuto,
              nSig=nSig,
              refXY=refXY,
              makePlots=makePlots,
              returnData=TRUE
              )[returnElem]
    
}

autoRound <- function(x, rndTo=-1L, rndBucketsAuto=100, nSig=NULL) {

    # FUNCTION ARGUMENTS
    # x: vector to be rounded
    # rndTo: every number in x should be rounded to the nearest rndTo
    #        NULL means no rounding
    #        -1L means make an estimate based on data (default)
    # rndBucketsAuto: integer, if rndTo is -1L, about how many buckets are desired for predictions?
    # nSig: number of significant digits for automatically calculated rounding parameter
    #       (NULL means calculate exactly)
    
    # If rndTo is passed as NULL, return x as-is
    if(is.null(rndTo)) return(x)
    
    # If rndTo is passed as -1L, make an estimate for rndTo
    if(isTRUE(all.equal(-1L, rndTo))) {
        # Get the number of unique values in x
        nUq <- length(unique(x))
        # If the number of unique values is no more than 150% of rndToBucketsAuto, return as-is
        if(nUq <= 1.5*rndBucketsAuto) return(x)
        # Otherwise, calculate a value for rndTo
        rndTo <- diff(range(x)) / rndBucketsAuto
        # Truncate to requested number of significant digits
        if(!is.null(nSig)) rndTo <- signif(rndTo, digits=nSig)
    }
    
    # Return the rounded vector if it was not already returned
    return(round(x/rndTo)*rndTo)

}


autoPartialImportance <- function(dfTrain, 
                                  dfTest, 
                                  yVar, 
                                  isContVar,
                                  impDB=dfImp,
                                  impNums=c(1:10, 16, 25, nrow(filter(dfImp, src==yVar)))
                                  ) {
    
    # FUNCTION ARGUMENTS:
    # dfTrain: training data
    # dfTest: test (holdout) data
    # yVar: dependent variable
    # isContVar: boolean, is this a contnuous variable (R-2) or categorical variable (accuracy)?
    # impDB: tibble containing sorted variable importances by predictor
    # impNums: vector of number of variables to run (each element in vector run)
    
    # Accuracy on holdout data
    tblRPI <- tibble::tibble(nImp=impNums, 
                             rfAcc=sapply(impNums, 
                                          FUN=function(x) {y <- runPartialImportanceRF(dfTrain=dfTrain, 
                                                                                       yVar=yVar, 
                                                                                       dfTest=dfTest, 
                                                                                       isContVar=isContVar, 
                                                                                       impDB=impDB, 
                                                                                       nImp=x, 
                                                                                       makePlots=FALSE
                                                                                       )[["rfAcc"]]
                                                           if(isTRUE(isContVar)) y <- y["r2"]
                                                           y
                                                           }
                                          )
                             )
    print(tblRPI)

    # Plot of holdout accuracy/r-squared vs. number of variables
    # if(isTRUE(isContVar)) tblRPI <- tblRPI %>% mutate(rfAcc=r2)
    if(isTRUE(isContVar)) prtDesc <- "R-squared" else prtDesc <- "Accuracy"
    p1 <- tblRPI %>%
        select(nImp, rfAcc) %>%
        bind_rows(tibble::tibble(nImp=0, rfAcc=0)) %>%
        ggplot(aes(x=nImp, y=rfAcc)) + 
        geom_line() + 
        geom_point() + 
        labs(title=paste0(prtDesc, " on holdout data vs. number of predictors"), 
             subtitle=paste0("Predicting ", yVar),
             y=paste0(prtDesc, " on holdout data"), 
             x="# Predictors (selected in order of variable importance in full model)"
             ) + 
        lims(y=c(0, 1)) + 
        geom_hline(data=~filter(., rfAcc==max(rfAcc)), aes(yintercept=rfAcc), lty=2)
    print(p1)
    
    return(tblRPI)
    
}


runNextBestPredictor <- function(varsRun, 
                                 xFix, 
                                 yVar, 
                                 isContVar,
                                 dfTrain,
                                 dfTest=dfTrain, 
                                 useLabel="predictions based on training data applied to holdout dataset",
                                 useSub=stringr::str_to_sentence(keyLabel_v3), 
                                 makePlots=FALSE
                                 ) {
    
    # FUNCTION ARGUMENTS:
    # varsRun: variables to be run as potential next-best predictors
    # xFix: variables that are already included in every test of next-best
    # yVar: dependent variable of interest
    # isContVar: boolean, is yvar continuous?
    # dfTrain: training data
    # dfTest: test data
    # useLabel: descriptive label
    # useSub: subtitle description
    # makePlots: boolean, should plots be created for each predictor run?
    
    vecAcc <- sapply(varsRun, FUN=function(x) {
        y <- runFullRF(dfTrain=dfTrain, 
                  yVar=yVar, 
                  xVars=c(xFix, x),
                  dfTest=dfTest, 
                  useLabel=useLabel, 
                  useSub=useSub,
                  isContVar=isContVar,
                  makePlots=makePlots,
                  returnData=TRUE
                  )[["rfAcc"]]
        if(isTRUE(isContVar)) y[["r2"]] else y
        }
        )

    vecAcc %>% 
        as.data.frame() %>% 
        purrr::set_names("rfAcc") %>% 
        rownames_to_column("pred") %>% 
        tibble::tibble() %>%
        arrange(desc(rfAcc)) %>%
        print(n=40)
    
    vecAcc

}


getNextBestVar <- function(x, returnTbl=FALSE, n=if(isTRUE(returnTbl)) +Inf else 1) {
    
    # FUNCTION ARGUMENTS:
    # x: named vector of accuracy or r-squared
    # returnTbl: boolean, if TRUE convert to tibble and return, if FALSE return vector of top-n predictors 
    # n: number of predictrs to return (+Inf will return the full tibble or vector)
    
    tbl <- vecToTibble(x, colNameName="pred") %>%
        arrange(-value) %>%
        slice_head(n=n)
    if(isTRUE(returnTbl)) return(tbl)
    else return(tbl %>% pull(pred))
    
}


newCityPredict <- function(rf, 
                           dfTest, 
                           trueCol, 
                           isContVar=FALSE,
                           reportR2=isTRUE(isContVar), 
                           plotCont=isTRUE(isContVar), 
                           reportAcc=TRUE, 
                           rndReport=2, 
                           useLabel="requested data",
                           useTitle=NULL,
                           useSub=NULL, 
                           rndTo=NULL,
                           rndBucketsAuto=100,
                           nSig=NULL,
                           refXY=FALSE, 
                           returnData=TRUE
                           ) {
    
    # FUNCTION ARGUMENTS:
    # rf: The existing "ranger" model OR a list containing element "rf" that has the existing "ranger" model
    # dfTest: the new dataset for predictions
    # trueCol: column containing true value
    # isContVar: boolean, is the variable continuous? (default FALSE means categorical)
    # reportR2: boolean, should accuracy be calculated as R-squared?
    #           (FALSE measures as categorical)
    # plotCont: boolean, should plotting assume continuous variables?
    #           (FALSE assumes confusion plot for categorical variables)
    # reportAcc: boolean, should accuracy be reported (printed to output)?
    # rndReport: number of significant digits for reporting (will be converted to percentage first)
    # useLabel: label for data to be used in reporting
    # useTitle: title to be used for chart (NULL means create from trueCol)
    # useSub: subtitle to be used for chart (NULL means none)
    # rndTo: every number in x should be rounded to the nearest rndTo
    #        NULL means no rounding (default)
    #        -1L means make an estimate based on data
    # rndBucketsAuto: integer, if rndTo is -1L, about how many buckets are desired for predictions?
    # nSig: number of significant digits for automatically calculated rounding parameter
    #       (NULL means calculate exactly)
    # refXY: boolean, should a reference line for y=x be included? (relevant only for continuous)
    # returnData: boolean, should a list be returned containing tstPred and rfAcc?
    
    # Get the ranger data
    if(!("ranger" %in% class(rf))) {
        if(!("rf" %in% names(rf))) {
            stop("\nERROR: rf must be of class 'ranger' OR a list with element 'rf' that is of class 'ranger")
        }
        rf <- rf[["rf"]]
    }
    if(!("ranger" %in% class(rf)))
        stop("\nERROR: rf must be of class 'ranger' OR a list with element 'rf' that is of class 'ranger")
    
    # Predict on new dataset
    tstPred <- predictRF(rf=rf, df=dfTest)

    # Report on accuracy
    rfAcc <- reportAccuracy(tstPred, 
                            trueCol=trueCol, 
                            reportAcc=reportAcc,
                            rndReport=rndReport, 
                            useLabel=useLabel, 
                            reportR2=reportR2,
                            returnAcc=TRUE
                            )

    # Plot confusion data
    plotConfusion(tstPred, 
                  trueCol=trueCol, 
                  useTitle=useTitle,
                  useSub=useSub, 
                  plotCont=plotCont, 
                  rndTo=rndTo,
                  rndBucketsAuto=rndBucketsAuto,
                  nSig=nSig,
                  refXY=refXY
                  )
    
    # Return data if requested
    if(isTRUE(returnData)) return(list(tstPred=tstPred, rfAcc=rfAcc))
    
}


```

Key mapping tables for available metrics are also copied:  
```{r}

hourlyMetrics <- "temperature_2m,relativehumidity_2m,dewpoint_2m,apparent_temperature,pressure_msl,surface_pressure,precipitation,rain,snowfall,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,shortwave_radiation,direct_radiation,direct_normal_irradiance,diffuse_radiation,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m,windgusts_10m,et0_fao_evapotranspiration,weathercode,vapor_pressure_deficit,soil_temperature_0_to_7cm,soil_temperature_7_to_28cm,soil_temperature_28_to_100cm,soil_temperature_100_to_255cm,soil_moisture_0_to_7cm,soil_moisture_7_to_28cm,soil_moisture_28_to_100cm,soil_moisture_100_to_255cm"
dailyMetrics <- "weathercode,temperature_2m_max,temperature_2m_min,apparent_temperature_max,apparent_temperature_min,precipitation_sum,rain_sum,snowfall_sum,precipitation_hours,sunrise,sunset,windspeed_10m_max,windgusts_10m_max,winddirection_10m_dominant,shortwave_radiation_sum,et0_fao_evapotranspiration"

hourlyDescription <- "Air temperature at 2 meters above ground\nRelative humidity at 2 meters above ground\nDew point temperature at 2 meters above ground\nApparent temperature is the perceived feels-like temperature combining wind chill factor, relative humidity and solar radiation\nAtmospheric air pressure reduced to mean sea level (msl) or pressure at surface. Typically pressure on mean sea level is used in meteorology. Surface pressure gets lower with increasing elevation.\nAtmospheric air pressure reduced to mean sea level (msl) or pressure at surface. Typically pressure on mean sea level is used in meteorology. Surface pressure gets lower with increasing elevation.\nTotal precipitation (rain, showers, snow) sum of the preceding hour. Data is stored with a 0.1 mm precision. If precipitation data is summed up to monthly sums, there might be small inconsistencies with the total precipitation amount.\nOnly liquid precipitation of the preceding hour including local showers and rain from large scale systems.\nSnowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent\nTotal cloud cover as an area fraction\nLow level clouds and fog up to 2 km altitude\nMid level clouds from 2 to 6 km altitude\nHigh level clouds from 6 km altitude\nShortwave solar radiation as average of the preceding hour. This is equal to the total global horizontal irradiation\nDirect solar radiation as average of the preceding hour on the horizontal plane and the normal plane (perpendicular to the sun)\nDirect solar radiation as average of the preceding hour on the horizontal plane and the normal plane (perpendicular to the sun)\nDiffuse solar radiation as average of the preceding hour\nWind speed at 10 or 100 meters above ground. Wind speed on 10 meters is the standard level.\nWind speed at 10 or 100 meters above ground. Wind speed on 10 meters is the standard level.\nWind direction at 10 or 100 meters above ground\nWind direction at 10 or 100 meters above ground\nGusts at 10 meters above ground of the indicated hour. Wind gusts in CERRA are defined as the maximum wind gusts of the preceding hour. Please consult the ECMWF IFS documentation for more information on how wind gusts are parameterized in weather models.\nET0 Reference Evapotranspiration of a well watered grass field. Based on FAO-56 Penman-Monteith equations ET0 is calculated from temperature, wind speed, humidity and solar radiation. Unlimited soil water is assumed. ET0 is commonly used to estimate the required irrigation for plants.\nWeather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details. Weather code is calculated from cloud cover analysis, precipitation and snowfall. As barely no information about atmospheric stability is available, estimation about thunderstorms is not possible.\nVapor Pressure Deificit (VPD) in kilopascal (kPa). For high VPD (>1.6), water transpiration of plants increases. For low VPD (<0.4), transpiration decreases\nAverage temperature of different soil levels below ground.\nAverage temperature of different soil levels below ground.\nAverage temperature of different soil levels below ground.\nAverage temperature of different soil levels below ground.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths.\nAverage soil water content as volumetric mixing ratio at 0-7, 7-28, 28-100 and 100-255 cm depths."
dailyDescription <- "The most severe weather condition on a given day\nMaximum and minimum daily air temperature at 2 meters above ground\nMaximum and minimum daily air temperature at 2 meters above ground\nMaximum and minimum daily apparent temperature\nMaximum and minimum daily apparent temperature\nSum of daily precipitation (including rain, showers and snowfall)\nSum of daily rain\nSum of daily snowfall\nThe number of hours with rain\nSun rise and set times\nSun rise and set times\nMaximum wind speed and gusts on a day\nMaximum wind speed and gusts on a day\nDominant wind direction\nThe sum of solar radiaion on a given day in Megajoules\nDaily sum of ET0 Reference Evapotranspiration of a well watered grass field"

# Create tibble for hourly metrics
tblMetricsHourly <- tibble::tibble(metric=hourlyMetrics %>% str_split_1(","), 
                                   description=hourlyDescription %>% str_split_1("\n")
                                   )
tblMetricsHourly %>% 
    print(n=50)

# Create tibble for daily metrics
tblMetricsDaily <- tibble::tibble(metric=dailyMetrics %>% str_split_1(","), 
                                  description=dailyDescription %>% str_split_1("\n")
                                   )
tblMetricsDaily

```

A function is written to process saved data for later use:  
```{r}

formatOpenMeteoJSON <- function(x, 
                                glimpseData=TRUE, 
                                addVars=FALSE, 
                                addExtract="tblHourly", 
                                showStats=addVars
                                ) {
    
    # FUNCTION ARGUMENTS:
    # x: Saved json file for passage to readOpenMeteoJSON
    # glimpseData: boolean, should a glimpse of the file and metadata be shown?
    # addVars: boolean, should variables be added for later processing?
    # addExtract: list elemented to be extracted (relevant only for addVars=TRUE)
    # showStats: boolean, should counts of key elements be shown (relevant only for addVars=TRUE)

    # Read file
    lst <- readOpenMeteoJSON(x)
    
    # Show a glimpse if requested
    if(isTRUE(glimpseData)) {
        print(lst)
        prettyOpenMeteoMeta(lst)
    }
    
    # If no variables to be added, return the file
    if(!isTRUE(addVars)) return(lst)
    
    # Add statistics
    df <- lst[[addExtract]] %>%
        mutate(year=year(date), 
               month=factor(month.abb[lubridate::month(date)], levels=month.abb), 
               hour=lubridate::hour(time), 
               fct_hour=factor(hour), 
               tod=ifelse(hour>=7 & hour<=18, "Day", "Night"), 
               doy=yday(date),
               season=case_when(month %in% c("Mar", "Apr", "May") ~ "Spring", 
                                month %in% c("Jun", "Jul", "Aug") ~ "Summer", 
                                month %in% c("Sep", "Oct", "Nov") ~ "Fall", 
                                month %in% c("Dec", "Jan", "Feb") ~ "Winter", 
                                TRUE~"typo"
                                ), 
               todSeason=paste0(season, "-", tod), 
               tod=factor(tod, levels=c("Day", "Night")), 
               season=factor(season, levels=c("Spring", "Summer", "Fall", "Winter")), 
               todSeason=factor(todSeason, 
                                levels=paste0(rep(c("Spring", "Summer", "Fall", "Winter"), each=2), 
                                              "-", 
                                              c("Day", "Night")
                                              )
                                ),
               across(where(is.numeric), .fns=function(x) round(100*percent_rank(x)), .names="pct_{.col}")
               )
    
    # Show counts if requested
    if(isTRUE(showStats)) {
        # Glimpse file
        glimpse(df)
        # Counts of day-of-year/month
        p1 <- df %>% 
            count(doy, month) %>% 
            ggplot(aes(y=doy, x=month)) + 
            geom_boxplot(aes(weight=n), fill="lightblue") + 
            labs(title="Observations by day-of-year and month", x=NULL, y="Day of Year")
        print(p1)
        # Counts of year/month
        p2 <- df %>% 
            count(year, month) %>% 
            ggplot(aes(y=factor(year), x=month)) + 
            geom_tile(aes(fill=n)) + 
            geom_text(aes(label=n), size=3) + 
            scale_fill_continuous("# Records", low="white", high="green") + 
            labs(title="Records by year and month", x=NULL, y=NULL)
        print(p2)
        # Counts of todSeason-season-tod, hour-fct_hour-tod, and month-season
        df %>% count(todSeason, season, tod) %>% print()
        df %>% count(hour, fct_hour, tod) %>% print(n=30)
        df %>% count(month, season) %>% print()
    }
    
    # Return the file
    df
    
}


```

Core daily datasets are loaded:  
```{r cache=TRUE}

# Read daily JSON file
nycOMDaily <- formatOpenMeteoJSON("testOM_daily_nyc.json")
laxOMDaily <- formatOpenMeteoJSON("testOM_daily_lax.json")
chiOMDaily <- formatOpenMeteoJSON("testOM_daily_chi.json")
houOMDaily <- formatOpenMeteoJSON("testOM_daily_hou.json")

```
  
Processed hourly data for NYC and LA are loaded:  
```{r cache=TRUE}

# Read hourly JSON file (NYC and LA)
nycTemp <- formatOpenMeteoJSON("testOM_hourly_nyc.json", addVars=TRUE)
laxTemp <- formatOpenMeteoJSON("testOM_hourly_lax.json", addVars=TRUE)

```

Processed hourly data for Chicago and Houston are loaded:  
```{r cache=TRUE}

# Read hourly JSON file (CHI and HOU)
chiTemp <- formatOpenMeteoJSON("testOM_hourly_chi.json", addVars=TRUE)
houTemp <- formatOpenMeteoJSON("testOM_hourly_hou.json", addVars=TRUE)

```

An integrated set of all-city test and train data is created:  
```{r, fig.height=9, fig.width=9}

# Bind all the data frames
allCity <- list("NYC"=nycTemp, 
                "LA"=laxTemp, 
                "Chicago"=chiTemp, 
                "Houston"=houTemp
                ) %>%
    bind_rows(.id="src")

# Create the index for training data
set.seed(24061512)
idxTrain <- sample(1:nrow(allCity), size = round(0.7*nrow(allCity)), replace=FALSE)

# Add test-train flag to full dataset
allCity <- allCity %>%
    mutate(tt=ifelse(row_number() %in% idxTrain, "train", "test"), 
           fct_src=factor(src))
allCity

# Review counts by year
allCity %>% 
    count(year, src, tt) %>% 
    pivot_wider(id_cols=c("src", "tt"), names_from="year", values_from="n")

```

Distributions of several key variables are explored:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyVars <- c('temperature_2m', 
             'relativehumidity_2m', 
             'dewpoint_2m', 
             'shortwave_radiation', 
             'vapor_pressure_deficit', 
             'soil_temperature_28_to_100cm', 
             'soil_temperature_100_to_255cm', 
             'soil_moisture_28_to_100cm', 
             'soil_moisture_100_to_255cm'
             )

allCity %>%
    colSelector(vecSelect=c("src", keyVars)) %>%
    pivot_longer(cols=-c(src)) %>%
    ggplot(aes(x=src, y=value)) + 
    geom_boxplot(aes(fill=src)) + 
    facet_wrap(~name, scales="free_y") + 
    labs(x=NULL, y=NULL, title="Distribution of Key Metrics by City") + 
    scale_fill_discrete(NULL)

```

In addition, pair plots by city are create for several combinations of variables:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyVars <- c('pressure_msl', 
             'surface_pressure', 
             'soil_temperature_100_to_255cm', 
             'soil_moisture_100_to_255cm'
             )

for(intCtr in 1:(length(keyVars)-1)) {
    for(intCtr2 in (intCtr+1):length(keyVars)) {
        p1 <- allCity %>%
            mutate(across(c("pressure_msl", "surface_pressure", "soil_temperature_100_to_255cm"), 
                          .fns=function(x) round(x*2)/2
                          ), 
                   soil_moisture_100_to_255cm=round(soil_moisture_100_to_255cm, 2)
                   ) %>%
            colSelector(vecSelect=c("src", keyVars[c(intCtr, intCtr2)])) %>%
            group_by(across(c("src", keyVars[c(intCtr, intCtr2)]))) %>%
            summarize(n=n(), .groups="drop") %>%
            ungroup() %>%
            ggplot(aes(x=get(keyVars[intCtr]), y=get(keyVars[intCtr2]))) + 
            geom_point(aes(color=src, size=n), alpha=0.25) + 
            labs(title="Distribution of Key Metrics by City", x=keyVars[intCtr], y=keyVars[intCtr2]) + 
            scale_size_continuous("# Obs")
        print(p1)
    }
}

```
  
The cities are well differentiated by several combinations, particularly surface pressure vs. MSL pressure

A full random forest model is run for predicting city using LA, NYC, and Chicago:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Create set of relevant training variables
varsTrain <- allCity %>%
    select(starts_with("pct")) %>%
    names() %>%
    str_replace(pattern="pct_", replacement="")
varsTrain
keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
keyCities <- c("NYC", "LA", "Chicago")

rfCity <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022, src %in% keyCities), 
                     yVar="fct_src", 
                     xVars=varsTrain, 
                     dfTest=allCity %>% filter(tt=="test", year==2022, src %in% keyCities), 
                     useLabel=keyLabel, 
                     useSub=stringr::str_to_sentence(keyLabel), 
                     returnData=TRUE
                     )

```

Prediction accuracy is 100%, as expected given the significant differentiation. Houston is assessed for the city it is "most similar" to:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

predictRF(rfCity$rf, df=allCity %>% filter(tt=="test", year==2022)) %>%
    plotConfusion(trueCol="fct_src", useSub=NULL, plotCont=FALSE)

```
  
Based on predictors in the three-city random forest, Houston is most similar to NYC. The full random forest model is updated, including Houston:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyCities <- c("NYC", "LA", "Chicago", "Houston")
rfCity <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022, src %in% keyCities), 
                    yVar="fct_src", 
                    xVars=varsTrain, 
                    dfTest=allCity %>% filter(tt=="test", year==2022, src %in% keyCities), 
                    useLabel=keyLabel, 
                    useSub=stringr::str_to_sentence(keyLabel), 
                    returnData=TRUE
                    )

```

Even with the similarities between NYC and Houston, there is sufficient differentiation in the predictors to drive 100% accuracy

A model is created to predict temperature for two cities:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyCities <- c("NYC", "Chicago")
keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
rfTemp2m <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022, src %in% keyCities), 
                      yVar="temperature_2m", 
                      xVars=c(varsTrain[!str_detect(varsTrain, "^temp|ature$")]), 
                      dfTest=allCity %>% filter(tt=="test", year==2022, src %in% keyCities), 
                      useLabel=keyLabel, 
                      useSub=stringr::str_to_sentence(keyLabel), 
                      isContVar=TRUE,
                      rndTo=-1L,
                      refXY=TRUE,
                      returnData=TRUE
                      )

```
  
Temperature predictions on holdout data for NYC and Chicago have R-squared over 99%. The model is applied to data from Houston and LA:  
```{r, fig.height=9, fig.width=9}

# Temperature predictions for LA
predTempLA <- predictRF(rfTemp2m$rf, df=allCity %>% filter(tt=="test", year==2022, src=="LA"))
reportAccuracy(predTempLA, trueCol="temperature_2m", reportR2=TRUE, useLabel="LA temperature predictions")
plotConfusion(predTempLA, trueCol="temperature_2m", plotCont=TRUE, rndTo=0.5, refXY=TRUE, useSub="LA")

# Temperature predictions for Houston
predTempHOU <- predictRF(rfTemp2m$rf, df=allCity %>% filter(tt=="test", year==2022, src=="Houston"))
reportAccuracy(predTempHOU, trueCol="temperature_2m", reportR2=TRUE, useLabel="Houston temperature predictions")
plotConfusion(predTempHOU, trueCol="temperature_2m", plotCont=TRUE, rndTo=0.5, refXY=TRUE, useSub="Houston")

```

Predictions for two cities not included in the original model have ~95% R-squared. Houston being relatively similar to NYC has higher R-squared than LA

Function runFullRF() is updated to allow for using an existing model with new data:  
```{r, fig.height=9, fig.width=9}

runFullRF <- function(dfTrain, 
                      yVar, 
                      xVars, 
                      useExistingRF=NULL,
                      dfTest=dfTrain,
                      useLabel="test data",
                      useSub=NULL, 
                      isContVar=FALSE,
                      rndTo=NULL,
                      rndBucketsAuto=100,
                      nSig=NULL,
                      refXY=FALSE,
                      makePlots=TRUE,
                      plotImp=makePlots,
                      plotConf=makePlots,
                      returnData=FALSE, 
                      ...
                      ) {
    
    # FUNCTION ARGUMENTS:
    # dfTrain: training data
    # yVar: dependent variable
    # xVars: column(s) containing independent variables
    # useExistingRF: an existing RF model, meaning only steps 3-5 are run (default NULL means run all steps)
    # dfTest: test dataset for applying predictions
    # useLabel: label to be used for reporting accuracy
    # useSub: subtitle to be used for confusion chart (NULL means none)
    # isContVar: boolean, is the variable continuous? (default FALSE means categorical)
    # rndTo: every number in x should be rounded to the nearest rndTo
    #        NULL means no rounding (default)
    #        -1L means make an estimate based on data
    # rndBucketsAuto: integer, if rndTo is -1L, about how many buckets are desired for predictions?
    # nSig: number of significant digits for automatically calculated rounding parameter
    #       (NULL means calculate exactly)    
    # refXY: boolean, should a reference line for y=x be included? (relevant only for continuous)
    # makePlots: boolean, should plots be created for variable importance and confusion matrix?
    # plotImp: boolean, should variable importance be plotted? (default is makePlots)
    # plotConf: boolean, should confusion matrix be plotted? (default is makePlots)
    # returnData: boolean, should data be returned?
    # ...: additional parameters to pass to runSimpleRF(), which are then passed to ranger::ranger()

    # Create the RF and plot importances, unless an RF is passed
    if(is.null(useExistingRF)) {
        # 1. Run random forest using impurity for importance
        rf <- runSimpleRF(df=dfTrain, yVar=yVar, xVars=xVars, importance="impurity", ...)

        # 2. Create, and optionally plot, variable importance
        rfImp <- plotRFImportance(rf, plotData=plotImp, returnData=TRUE)
    }
    else {
        rf <- useExistingRF
        rfImp <- NA
    }

    # 3. Predict on test dataset
    tstPred <- predictRF(rf=rf, df=dfTest)

    # 4. Report on accuracy (updated for continuous or categorical)
    rfAcc <- reportAccuracy(tstPred, 
                            trueCol=yVar, 
                            rndReport=3, 
                            useLabel=useLabel, 
                            reportR2=isTRUE(isContVar),
                            returnAcc=TRUE
                            )

    # 5. Plot confusion data (updated for continuous vs. categorical) if requested
    if(isTRUE(plotConf)) {
        plotConfusion(tstPred, 
                      trueCol=yVar, 
                      useSub=useSub, 
                      plotCont=isTRUE(isContVar), 
                      rndTo=rndTo, 
                      rndBucketsAuto=rndBucketsAuto,
                      nSig=nSig,
                      refXY=refXY
                      )
    }
    
    #6. Return data if requested
    if(isTRUE(returnData)) return(list(rf=rf, rfImp=rfImp, tstPred=tstPred, rfAcc=rfAcc))
    
}

```

Updated function runFullRF() is tested on LA and Houston:  
```{r, fig.height=9, fig.width=9}

# Temperature predictions for LA
runFullRF(yVar="temperature_2m", 
          useExistingRF=rfTemp2m$rf, 
          dfTest=allCity %>% filter(tt=="test", year==2022, src=="LA"), 
          useLabel="LA temperature predictions", 
          useSub="LA", 
          isContVar=TRUE,
          rndTo=0.5, 
          refXY=TRUE
          )

# Temperature predictions for Houston
runFullRF(yVar="temperature_2m", 
          useExistingRF=rfTemp2m$rf, 
          dfTest=allCity %>% filter(tt=="test", year==2022, src=="Houston"), 
          useLabel="Houston temperature predictions", 
          useSub="Houston", 
          isContVar=TRUE,
          rndTo=0.5, 
          refXY=TRUE
          )

```

A basic linear model can potentially drive better temperature predictions:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyCities <- c("NYC", "Chicago")
lmMiniTemp <- allCity %>% 
    filter(tt=="train", year<2022, src %in% keyCities) %>%
    select(t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m) %>%
    lm(t~rh+d+rh:d+1, data=.) 
summary(lmMiniTemp)

ggMiniTemp <- predict(lmMiniTemp, 
                      newdata=allCity %>% 
                          filter(tt=="test", year==2022, src %in% keyCities) %>% 
                          select(rh=relativehumidity_2m, d=dewpoint_2m)
                      ) %>% 
    mutate(select(allCity %>% filter(tt=="test", year==2022, src %in% keyCities), temperature_2m), 
           pred=., 
           err=pred-temperature_2m, 
           err2=err**2, 
           rnd5=round(temperature_2m/5)*5
           ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean))
ggMiniTemp

ggMiniTemp %>% 
    select(rnd5, temperature_2m, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "temperature_2m"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Temperature Using City Linear Model on Same City Holdout Data", 
         x="New city actual temperature (rounded to nearest 5)", 
         y="Average temperature for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```

Predictions can then be explored in cities not included in the original linear model, starting with Houston:  
```{r, fig.height=9, fig.width=9}

ggMiniTemp_hou <- predict(lmMiniTemp, 
                          newdata=allCity %>% 
                              filter(tt=="test", year==2022, src %in% c("Houston")) %>% 
                              select(rh=relativehumidity_2m, d=dewpoint_2m)
                          ) %>% 
    mutate(select(allCity %>% filter(tt=="test", year==2022, src %in% c("Houston")), temperature_2m), 
           pred=., 
           err=pred-temperature_2m, 
           err2=err**2, 
           rnd5=round(temperature_2m/5)*5
           ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean))
ggMiniTemp_hou

ggMiniTemp_hou %>% 
    summarize(mse=sum(n*err2)/sum(n)) %>% 
    mutate(rmse=sqrt(mse))

ggMiniTemp_hou %>% 
    select(rnd5, temperature_2m, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "temperature_2m"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Temperature Using City Linear Model on New City (Houston) Holdout Data", 
         x="New city (Houston) actual temperature (rounded to nearest 5)", 
         y="Average temperature for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```
  
The linear model is generally very accurate for Houston, with the exception of under-predicting the very highest temperatures. RMSE of temperature predictions is lowered to ~1 from ~1.5 observed using the random forest

Predictions are also explored in Los Angeles:  
```{r, fig.height=9, fig.width=9}

ggMiniTemp_lax <- predict(lmMiniTemp, 
                          newdata=allCity %>% 
                              filter(tt=="test", year==2022, src %in% c("LA")) %>% 
                              select(rh=relativehumidity_2m, d=dewpoint_2m)
                          ) %>% 
    mutate(select(allCity %>% filter(tt=="test", year==2022, src %in% c("LA")), temperature_2m), 
           pred=., 
           err=pred-temperature_2m, 
           err2=err**2, 
           rnd5=round(temperature_2m/5)*5
           ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean))
ggMiniTemp_lax

ggMiniTemp_lax %>% 
    summarize(mse=sum(n*err2)/sum(n)) %>% 
    mutate(rmse=sqrt(mse))

ggMiniTemp_lax %>% 
    select(rnd5, temperature_2m, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "temperature_2m"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Temperature Using City Linear Model on New City (LA) Holdout Data", 
         x="New city (LA) actual temperature (rounded to nearest 5)", 
         y="Average temperature for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```
  
The linear model is generally inaccurate for LA, consistently underestimating temperatures. RMSE of temperature predictions is raised to ~4 from ~2 observed using the random forest

Los Angeles is meaningfully different from NYC and Chicago on key predictors:  
```{r, fig.height=9, fig.width=9}

tmpPlotData <- allCity %>% 
    select(src, relativehumidity_2m, dewpoint_2m, temperature_2m) %>% 
    mutate(across(where(is.numeric), .fns=round)) %>% 
    count(src, relativehumidity_2m, dewpoint_2m, temperature_2m)

tmpPlotData %>%
    count(src, temperature_2m, dewpoint_2m, wt=n) %>%
    ggplot(aes(x=temperature_2m, y=dewpoint_2m)) + 
    geom_point(aes(color=src, size=n), alpha=0.2) + 
    geom_smooth(aes(color=src, weight=n), method="lm") +
    labs(title="T/D by city")

tmpPlotData %>%
    count(src, temperature_2m, relativehumidity_2m, wt=n) %>%
    ggplot(aes(x=temperature_2m, y=relativehumidity_2m)) + 
    geom_point(aes(color=src, size=n), alpha=0.1) + 
    geom_smooth(aes(color=src, weight=n), method="lm") +
    labs(title="T/RH by city")

```
  
Los Angeles is routinely hot and arid, while the other cities tend to be humid when they are hot. Data for an additional low-humidity city are downloaded, cached to avoid multiple hits to the server:  
```{r cache=TRUE}

# Hourly data download for Las Vegas, NV
testURLHourly <- helperOpenMeteoURL(cityName="Las Vegas NV", 
                                    hourlyIndices=1:nrow(tblMetricsHourly),
                                    startDate="2010-01-01", 
                                    endDate="2023-12-31", 
                                    tz="America/Los_Angeles"
                                    )
testURLHourly

# Download file
if(!file.exists("testOM_hourly_las.json")) {
    fileDownload(fileName="testOM_hourly_las.json", url=testURLHourly)
} else {
    cat("\nFile testOM_hourly_las.json already exists, skipping download\n")
}


# Daily data download for Las Vegas, NV
testURLDaily <- helperOpenMeteoURL(cityName="Las Vegas NV", 
                                   dailyIndices=1:nrow(tblMetricsDaily),
                                   startDate="2010-01-01", 
                                   endDate="2023-12-31", 
                                   tz="America/Los_Angeles"
                                   )
testURLDaily

# Download file
if(!file.exists("testOM_daily_las.json")) {
    fileDownload(fileName="testOM_daily_las.json", url=testURLDaily)
} else {
    cat("\nFile testOM_daily_las.json already exists, skipping download\n")
}

```

The daily and hourly datasets are loaded:  
```{r cache=TRUE}

# Read daily JSON file
lasOMDaily <- formatOpenMeteoJSON("testOM_daily_las.json")

# Read hourly JSON file
lasTemp <- formatOpenMeteoJSON("testOM_hourly_las.json", addVars=TRUE)

```

An integrated set of all-city test and train data is updated:  
```{r, fig.height=9, fig.width=9}

# Bind all the data frames
allCity <- list("NYC"=nycTemp, 
                "LA"=laxTemp, 
                "Chicago"=chiTemp, 
                "Houston"=houTemp, 
                "Vegas"=lasTemp
                ) %>%
    bind_rows(.id="src")

# Create the index for training data
set.seed(24070113)
idxTrain_v2 <- sample(1:nrow(allCity), size = round(0.7*nrow(allCity)), replace=FALSE)

# Add test-train flag to full dataset
allCity <- allCity %>%
    mutate(tt=ifelse(row_number() %in% idxTrain_v2, "train", "test"), 
           fct_src=factor(src))
allCity

# Review counts by year
allCity %>% 
    count(year, src, tt) %>% 
    pivot_wider(id_cols=c("src", "tt"), names_from="year", values_from="n")

```

Distributions of several key variables are explored:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyVars <- c('temperature_2m', 
             'relativehumidity_2m', 
             'dewpoint_2m', 
             'shortwave_radiation', 
             'vapor_pressure_deficit', 
             'soil_temperature_28_to_100cm', 
             'soil_temperature_100_to_255cm', 
             'soil_moisture_28_to_100cm', 
             'soil_moisture_100_to_255cm'
             )

allCity %>%
    colSelector(vecSelect=c("src", keyVars)) %>%
    pivot_longer(cols=-c(src)) %>%
    ggplot(aes(x=src, y=value)) + 
    geom_boxplot(aes(fill=src)) + 
    facet_wrap(~name, scales="free_y") + 
    labs(x=NULL, y=NULL, title="Distribution of Key Metrics by City") + 
    scale_fill_discrete(NULL)

```

Las Vegas stands out for especially low relative humidity (even relative to LA), as well as dry soil (similar to LA)

The scatter of temperature and dewpoint is also explored:  
```{r, fig.height=9, fig.width=9}

allCity %>% 
    select(t=temperature_2m, d=dewpoint_2m, src) %>% 
    mutate(across(.cols=where(is.numeric), .fns=function(x) round(x))) %>% 
    count(src, t, d) %>% 
    ggplot(aes(x=t, y=d)) + 
    geom_point(aes(size=n, color=src), alpha=0.5) + 
    geom_smooth(aes(color=src, weight=n), method="lm") +
    labs(x="Temperature (C)", y="Dewpoint (C)", title="Temperature vs. Dewpoint", subtitle="Hourly") + 
    scale_color_discrete(NULL) + 
    scale_size_continuous("# Obs")

allCity %>% 
    group_by(src) %>%
    summarize(cor_td=cor(temperature_2m, dewpoint_2m))

```

Las Vegas is similar to LA, with lower dewpoints. The more humid cities have 80%+ correlation between temperature and dewpoint, dropping to ~40% correlation in the drier cities

Models for predicting city (one with soil temperature, one without) are saved using data without Las Vegas, for application to the new Las Vegas data:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Run with all variables
rfCityFull <- runFullRF(allCity %>% 
                            mutate(fct_src=factor(src)) %>% 
                            filter(year<2022, tt=="train", src!="Vegas"), 
                        yVar="fct_src", 
                        xVars=varsTrain, 
                        dfTest=allCity %>% 
                            mutate(fct_src=factor(src)) %>% 
                            filter(year==2022, tt=="test", src!="Vegas"), 
                        isContVar=FALSE, 
                        returnData=TRUE
                        )
predictRF(rfCityFull$rf, df=allCity %>% filter(src=="Vegas")) %>% count(pred)

# Run without moisture variables
rfCityNoMoisture <- runFullRF(allCity %>% 
                                  mutate(fct_src=factor(src)) %>% 
                                  filter(year<2022, tt=="train", src!="Vegas"), 
                              yVar="fct_src", 
                              xVars=varsTrain[!grepl(pattern="moist", x=varsTrain)],
                              dfTest=allCity %>% 
                                  mutate(fct_src=factor(src)) %>% 
                                  filter(year==2022, tt=="test", src!="Vegas"), 
                              isContVar=FALSE, 
                              returnData=TRUE
                              )
predictRF(rfCityNoMoisture$rf, df=houTemp) %>% count(pred)

```

The previously trained random forest models overwhelmingly predict Las Vegas as Los Angeles (if soil moisture is included) or Houston (if soil moisture is excluded)

The linear approximation for estimating temperature based on dewpoint and relative humidity is applied:  
```{r, fig.height=9, fig.width=9}

ggMiniTempLAS <- predict(lmMiniTemp, 
                         newdata=allCity %>% 
                             filter(src=="Vegas", tt=="test", year==2022) %>%
                             select(rh=relativehumidity_2m, d=dewpoint_2m)
                         ) %>% 
    mutate(allCity %>% filter(src=="Vegas", tt=="test", year==2022) %>% select(temperature_2m), 
           pred=., 
           err=pred-temperature_2m, 
           err2=err**2, 
           rnd5=round(temperature_2m/5)*5
    ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean))
ggMiniTempLAS

ggMiniTempLAS %>% 
    summarize(mse=sum(n*err2)/sum(n)) %>% 
    mutate(rmse=sqrt(mse))

ggMiniTempLAS %>% 
    select(rnd5, temperature_2m, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "temperature_2m"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Temperature Using Old City Linear Model on New City Data", 
         x="New city actual temperature (rounded to nearest 5)", 
         y="Average temperature for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```

The linear approximation based on dewpoint and relative humidity is inaccurate for predicting temperatures in Las Vegas, consistent with Las Vegas having T/D trends very different from originally modeled cities, NYC and Chicago

Las Vegas is meaningfully different from NYC and Chicago on key predictors:  
```{r, fig.height=9, fig.width=9}

tmpPlotData <- allCity %>% 
    select(src, relativehumidity_2m, dewpoint_2m, temperature_2m) %>% 
    mutate(across(where(is.numeric), .fns=round)) %>% 
    count(src, relativehumidity_2m, dewpoint_2m, temperature_2m)

tmpPlotData %>%
    count(src, temperature_2m, dewpoint_2m, wt=n) %>%
    ggplot(aes(x=temperature_2m, y=dewpoint_2m)) + 
    geom_point(aes(color=src, size=n), alpha=0.2) + 
    geom_smooth(aes(color=src, weight=n), method="lm") +
    labs(title="T/D by city")

tmpPlotData %>%
    count(src, temperature_2m, relativehumidity_2m, wt=n) %>%
    ggplot(aes(x=temperature_2m, y=relativehumidity_2m)) + 
    geom_point(aes(color=src, size=n), alpha=0.1) + 
    geom_smooth(aes(color=src, weight=n), method="lm") +
    labs(title="T/RH by city")

```
  
The existing random forest model, trained on NYC and Chicago, is also tested on Las Vegas temperatures:  
```{r, fig.height=9, fig.width=9}

# Temperature predictions for Vegas
runFullRF(yVar="temperature_2m", 
          useExistingRF=rfTemp2m$rf, 
          dfTest=allCity %>% filter(tt=="test", year==2022, src=="Vegas"), 
          useLabel="Las Vegas temperature predictions", 
          useSub="Las Vegas", 
          isContVar=TRUE,
          rndTo=0.5, 
          refXY=TRUE
          )

```

The random forest is more accurate than the linear model in predicting temperatures in Las Vegas based on training data from other cities. RMSE is ~3 rather than the ~10 from the linear model application

All combinations of two variables are explored for predicting temperature on a smaller training dataset:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Train and test data
dfTrainTemp <- allCity %>% 
    filter(!(src %in% c("Vegas")), tt=="train", year<2022) %>% 
    mutate(fct_src=factor(src))
dfTestTemp <- allCity %>% 
    filter(!(src %in% c("Vegas")), tt=="test", year==2022) %>% 
    mutate(fct_src=factor(src))

# Variables to explore
possTempVars <- c(varsTrain[!str_detect(varsTrain, "^temp|ature$")], "month", "tod")

# Subsets to use
set.seed(24070815)
idxSmallTemp <- sample(1:nrow(dfTrainTemp), 5000, replace=FALSE)
mtxSmallTemp <- matrix(nrow=0, ncol=3)

for(idx1 in 1:(length(possTempVars)-1)) {
    for(idx2 in (idx1+1):length(possTempVars)) {
        r2SmallTemp <- runFullRF(dfTrain=dfTrainTemp[idxSmallTemp,], 
                                 yVar="temperature_2m", 
                                 xVars=possTempVars[c(idx1, idx2)], 
                                 dfTest=dfTestTemp, 
                                 useLabel=keyLabel, 
                                 useSub=stringr::str_to_sentence(keyLabel), 
                                 isContVar=TRUE,
                                 makePlots=FALSE,
                                 returnData=TRUE
                                 )[["rfAcc"]][["r2"]]
        mtxSmallTemp <- rbind(mtxSmallTemp, c(idx1, idx2, r2SmallTemp))
    }
}

```

Predictive success by metric is explored:  
```{r, fig.height=9, fig.width=9}

dfSmallR2Temp <- as.data.frame(mtxSmallTemp) %>% 
    purrr::set_names(c("idx1", "idx2", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possTempVars[idx1], var2=possTempVars[idx2], rn=row_number()) 
dfSmallR2Temp %>% arrange(desc(r2)) %>% select(var1, var2, r2) %>% print(n=20)

dfSmallR2Temp %>% 
    pivot_longer(cols=c(var1, var2)) %>% 
    group_by(value) %>% 
    summarize(across(r2, .fns=list("min"=min, "mu"=mean, "max"=max))) %>% 
    ggplot(aes(x=fct_reorder(value, r2_mu))) + 
    coord_flip() + 
    geom_point(aes(y=r2_mu)) + 
    geom_errorbar(aes(ymin=r2_min, ymax=r2_max)) + 
    lims(y=c(NA, 1)) + 
    geom_hline(yintercept=1, lty=2, color="red") +
    labs(title="R-squared in every 2-predictor model including self and one other", 
         subtitle="Predicting temperature", 
         y="Range of R-squared (min-mean-max)", 
         x=NULL
    )

dfSmallR2Temp %>% 
    arrange(desc(r2)) %>% 
    filter(var2!="soil_temperature_0_to_7cm", var1!="soil_temperature_0_to_7cm") %>% 
    select(var1, var2, r2) %>% 
    print(n=20)

```

Select combinations are explored using the full training dataset:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

possLargeVars <- c("dewpoint_2m", 
                   "vapor_pressure_deficit", 
                   "relativehumidity_2m", 
                   "soil_temperature_0_to_7cm"
                   )
possLargeVars
mtxLarge <- matrix(nrow=0, ncol=3)

for(idx1 in 1:(length(possLargeVars)-1)) {
    for(idx2 in (idx1+1):length(possLargeVars)) {
        r2LargeTemp <- runFullRF(dfTrain=dfTrainTemp[,], 
                                 yVar="temperature_2m", 
                                 xVars=possLargeVars[c(idx1, idx2)], 
                                 dfTest=dfTestTemp, 
                                 useLabel=keyLabel, 
                                 useSub=stringr::str_to_sentence(keyLabel), 
                                 isContVar=TRUE,
                                 makePlots=FALSE,
                                 returnData=TRUE
                                 )[["rfAcc"]][["r2"]]
        mtxLarge <- rbind(mtxLarge, c(idx1, idx2, r2LargeTemp))
    }
}

dfLargeR2Temp <- as.data.frame(mtxLarge) %>% 
    purrr::set_names(c("idx1", "idx2", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possLargeVars[idx1], var2=possLargeVars[idx2], rn=row_number()) 
dfLargeR2Temp %>% arrange(desc(r2)) %>% select(var1, var2, r2) %>% print(n=20)

```

A model using only dewpoint and vapor pressure deficit is run on one city, then applied to the other:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Train and test data
dfTrainTemp_v2 <- allCity %>% 
    filter(src %in% c("NYC"), tt=="train", year<2022) %>% 
    mutate(fct_src=factor(src))
dfTestTemp_v2 <- allCity %>% 
    filter(tt=="test", year==2022) %>% 
    mutate(fct_src=factor(src))

# Random forest for temperature using dewpoint and vapor pressure deficit
keyLabel <- "predictions based on NYC pre-2022 training data applied to each city in 2022 holdout dataset"
tmpPred_v2 <- runFullRF(dfTrain=dfTrainTemp_v2, 
                        yVar="temperature_2m", 
                        xVars=c("dewpoint_2m", "vapor_pressure_deficit"), 
                        dfTest=dfTestTemp_v2, 
                        useLabel=keyLabel, 
                        useSub=stringr::str_to_sentence(keyLabel), 
                        isContVar=TRUE,
                        makePlots=FALSE,
                        returnData=TRUE
                        )[["tstPred"]] %>%
    select(src, temperature_2m, pred) %>%
    group_by(src) %>%
    summarize(n=n(), 
              tss=sum((temperature_2m-mean(temperature_2m))**2), 
              rss=sum((temperature_2m-pred)**2), 
              r2=1-rss/tss, 
              rmse=sqrt(rss/n),
              berr=sqrt(tss/n)
              )
tmpPred_v2

```

The model trained on NYC performs well on Chicago, Houston, and LA, while missing significantly on Las Vegas

Patterns in dewpoint and vapor pressure deficit are explored:  
```{r, fig.height=9, fig.width=9}

dfPlot_v2 <- dfTestTemp_v2 %>% 
    select(src, vapor_pressure_deficit, dewpoint_2m) %>% 
    mutate(across(where(is.numeric), .fns=function(x) round(2*x)/2)) %>% 
    count(src, vapor_pressure_deficit, dewpoint_2m) 

dfPlot_v2 %>% 
    ggplot(aes(y=vapor_pressure_deficit, x=dewpoint_2m)) + 
    geom_point(aes(color=src, size=n), alpha=0.25) + facet_wrap(~src) + 
    scale_color_discrete(NULL)

# Overlap of NYC points by city
tmpNYC <- dfTrainTemp_v2 %>% 
    select(src, vapor_pressure_deficit, dewpoint_2m) %>% 
    mutate(across(where(is.numeric), .fns=function(x) round(2*x)/2)) %>% 
    count(src, vapor_pressure_deficit, dewpoint_2m) %>%
    filter(src=="NYC", n>=10) %>%
    mutate(inNYC=TRUE)

dfPlot_v2 %>%
    left_join(select(tmpNYC, vapor_pressure_deficit, dewpoint_2m, inNYC), 
              by=c("vapor_pressure_deficit", "dewpoint_2m")
              ) %>%
    mutate(inNYC=ifelse(is.na(inNYC), FALSE, inNYC)) %>% 
    ggplot(aes(y=vapor_pressure_deficit, x=dewpoint_2m)) + 
    geom_point(aes(color=inNYC, size=n), alpha=0.25) + facet_wrap(~src) + 
    scale_color_discrete("NYC training\nhas 10+ obs")

dfPlot_v2 %>%
    left_join(select(tmpNYC, vapor_pressure_deficit, dewpoint_2m, inNYC), 
              by=c("vapor_pressure_deficit", "dewpoint_2m")
              ) %>%
    mutate(inNYC=ifelse(is.na(inNYC), FALSE, inNYC)) %>%
    group_by(src) %>%
    summarize(meanNYC=sum(n*inNYC)/sum(n), n=sum(n), nObs=n())

```

Chicago and NYC are both very well-represented by the training data, while a majority of Las Vegas observations are largely or entirely absent from the training data

There are strong relationships among dewpoint, vapor pressure deficit, relative humidity, and temperature:  
```{r, fig.height=9, fig.width=9}

dfTestTemp_v2 %>% 
    select(src, vapor_pressure_deficit, dewpoint_2m, temperature_2m, relativehumidity_2m) %>% 
    mutate(across(c(dewpoint_2m, temperature_2m, relativehumidity_2m), .fns=function(x) round(x))) %>% 
    filter(dewpoint_2m %in% c(-10, 0, 10, 20)) %>% 
    ggplot(aes(x=vapor_pressure_deficit, y=temperature_2m)) + 
    geom_point(aes(color=factor(dewpoint_2m))) + 
    scale_color_discrete("Dewpoint")

dfTestTemp_v2 %>% 
    select(src, vapor_pressure_deficit, dewpoint_2m, temperature_2m, relativehumidity_2m) %>% 
    mutate(across(c(dewpoint_2m, temperature_2m, relativehumidity_2m), .fns=function(x) round(x))) %>% 
    filter(dewpoint_2m %in% c(-10, 0, 10, 20)) %>% 
    ggplot(aes(x=relativehumidity_2m, y=temperature_2m)) + 
    geom_point(aes(color=factor(dewpoint_2m))) + 
    scale_color_discrete("Dewpoint")

```
  
To better cover the predictor space, a model using only dewpoint and vapor pressure deficit is run on NYC and Vegas, then applied to the others:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Train and test data
dfTrainTemp_v3 <- allCity %>% 
    filter(src %in% c("NYC", "Vegas"), tt=="train", year<2022) %>% 
    mutate(fct_src=factor(src))
dfTestTemp_v3 <- allCity %>% 
    filter(tt=="test", year==2022) %>% 
    mutate(fct_src=factor(src))

# Random forest for temperature using dewpoint and vapor pressure deficit
keyLabel <- "predictions based on NYC/Vegas pre-2022 training data applied to each city in 2022 holdout dataset"
tmpPred_v3 <- runFullRF(dfTrain=dfTrainTemp_v3, 
                        yVar="temperature_2m", 
                        xVars=c("dewpoint_2m", "vapor_pressure_deficit"), 
                        dfTest=dfTestTemp_v3, 
                        useLabel=keyLabel, 
                        useSub=stringr::str_to_sentence(keyLabel), 
                        isContVar=TRUE,
                        makePlots=FALSE,
                        returnData=TRUE
                        )[["tstPred"]] %>%
    select(src, temperature_2m, pred) %>%
    group_by(src) %>%
    summarize(n=n(), 
              tss=sum((temperature_2m-mean(temperature_2m))**2), 
              rss=sum((temperature_2m-pred)**2), 
              r2=1-rss/tss, 
              rmse=sqrt(rss/n),
              berr=sqrt(tss/n)
              )
tmpPred_v3

```

The model trained on NYC and Vegas generally performs very well on all cities

Coverage of the temperature and humidity space by city is explored:  
```{r, fig.height=9, fig.width=9}

dfTestTemp_v2 %>% 
    select(src, vapor_pressure_deficit, dewpoint_2m, temperature_2m, relativehumidity_2m) %>% 
    mutate(across(c(dewpoint_2m, temperature_2m, relativehumidity_2m), .fns=function(x) round(x))) %>% 
    ggplot(aes(x=dewpoint_2m, y=temperature_2m)) + 
    geom_density2d(data=~filter(., src %in% c("NYC", "Vegas"))) +
    geom_point(data=~count(., src, temperature_2m, dewpoint_2m), 
               aes(color=src, size=n), 
               alpha=0.25
               ) + 
    scale_color_discrete(NULL) + 
    labs(title="Relationships between temperature and depoint", 
         subtitle="Contours from geom_density_2d() use only NYC and Las Vegas data"
         )

```

Modeling using NYC and Las Vegas data may not fully cover the coldest and driest portions of the Chicago space

The model using only NYC and Las Vegas is applied to Chicago, with accuracy explored by temperature:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Train and test data
dfTrainTemp_v3 <- allCity %>% 
    filter(src %in% c("NYC", "Vegas"), tt=="train", year<2022) %>% 
    mutate(fct_src=factor(src))
dfTestTemp_v3 <- allCity %>% 
    filter(tt=="test", year==2022) %>% 
    mutate(fct_src=factor(src))

# Random forest for temperature using dewpoint and vapor pressure deficit
keyLabel <- "predictions based on NYC/Vegas pre-2022 training data applied to each city in 2022 holdout dataset"
tmpPred_v3_df <- runFullRF(dfTrain=dfTrainTemp_v3, 
                           yVar="temperature_2m", 
                           xVars=c("dewpoint_2m", "vapor_pressure_deficit"), 
                           dfTest=dfTestTemp_v3, 
                           useLabel=keyLabel, 
                           useSub=stringr::str_to_sentence(keyLabel), 
                           isContVar=TRUE,
                           makePlots=FALSE,
                           returnData=TRUE
                           )[["tstPred"]] 

tmpPred_v3_df %>%
    select(src, temperature_2m, pred) %>%
    group_by(src) %>%
    summarize(n=n(), 
              tss=sum((temperature_2m-mean(temperature_2m))**2), 
              rss=sum((temperature_2m-pred)**2), 
              r2=1-rss/tss, 
              rmse=sqrt(rss/n),
              berr=sqrt(tss/n)
              )

```
  
```{r, fig.height=9, fig.width=9}

ggMiniTempCHI <- tmpPred_v3_df %>% 
    select(src, temperature_2m, pred) %>%
    filter(src=="Chicago") %>%
    mutate(err=pred-temperature_2m, 
           err2=err**2, 
           rnd5=round(temperature_2m/5)*5
    ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean)) %>%
    mutate(pcterr2=n*err2/sum(n*err2))
ggMiniTempCHI

ggMiniTempCHI %>% 
    summarize(mse=sum(n*err2)/sum(n)) %>% 
    mutate(rmse=sqrt(mse))

ggMiniTempCHI %>% 
    select(rnd5, temperature_2m, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "temperature_2m"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Temperature Using NYC/Vegas Random Forest Model on Chicago Data", 
         x="Chicago actual temperature (rounded to nearest 5)", 
         y="Average temperature for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```

As expected, predictions are excellent in the space covered by the training data and poor for the small number of very cold observations never seen in training. Around 60% of MSE in Chicago temperature predictions occurs in the 23 test data observations where temperature (rounded to nearest 5 degrees C) is -20C or colder

The model using only NYC and Las Vegas is applied to Houston, with accuracy explored by temperature:  
```{r, fig.height=9, fig.width=9}

ggMiniTempHOU <- tmpPred_v3_df %>% 
    select(src, temperature_2m, pred) %>%
    filter(src=="Houston") %>%
    mutate(err=pred-temperature_2m, 
           err2=err**2, 
           rnd5=round(temperature_2m/5)*5
    ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean)) %>%
    mutate(pcterr2=n*err2/sum(n*err2))
ggMiniTempHOU

ggMiniTempHOU %>% 
    summarize(mse=sum(n*err2)/sum(n)) %>% 
    mutate(rmse=sqrt(mse))

ggMiniTempHOU %>% 
    select(rnd5, temperature_2m, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "temperature_2m"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Temperature Using NYC/Vegas Random Forest Model on Houston Data", 
         x="Houston actual temperature (rounded to nearest 5)", 
         y="Average temperature for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```

As expected, predictions are excellent in the space covered by the training data and miss only with the very hottest observations never seen in training

The model using only NYC and Las Vegas is applied to Los Angeles, with accuracy explored by temperature:  
```{r, fig.height=9, fig.width=9}

ggMiniTempLA <- tmpPred_v3_df %>% 
    select(src, temperature_2m, pred) %>%
    filter(src=="LA") %>%
    mutate(err=pred-temperature_2m, 
           err2=err**2, 
           rnd5=round(temperature_2m/5)*5
    ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean)) %>%
    mutate(pcterr2=n*err2/sum(n*err2))
ggMiniTempLA

ggMiniTempLA %>% 
    summarize(mse=sum(n*err2)/sum(n)) %>% 
    mutate(rmse=sqrt(mse))

ggMiniTempLA %>% 
    select(rnd5, temperature_2m, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "temperature_2m"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Temperature Using NYC/Vegas Random Forest Model on LA Data", 
         x="Los Angeles actual temperature (rounded to nearest 5)", 
         y="Average temperature for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```

As expected, predictions are excellent since the entire LA space is covered by the training data

An approximate formula for relative humidity is assessed for resonance with the data:  
```{r, fig.height=9, fig.width=9}

# Approximate formula for relative humidity
# Source https://www.omnicalculator.com/physics/relative-humidity
calcRH <- function(t, d, c1=17.63, c2=243) {
    100 * exp((c1*d)/(c2+d)) / exp((c1*t)/(c2+t))
}

# Applied to sample data
dfTestTemp_v3 %>%
    select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m) %>%
    mutate(crh=calcRH(t, d)) %>%
    ggplot(aes(x=rh, y=crh)) + 
    geom_point(aes(color=src)) + 
    facet_wrap(~src) + 
    geom_smooth(method="lm") + 
    geom_abline(intercept=0, slope=1, lty=2) + 
    labs(x="Reported relative humidity", 
         y="Formula relative humidity", 
         title="Relative humidity by formula from temperature and dewpoint vs. reported in raw data") + 
    scale_color_discrete(NULL)

```

The formula is an exact match to the reported data, allowing the random forest to find the correct third value when given two of T, D, RH, provided that the training space also includes that combination

Example training data is created for all temperatures and dew points between -30 and 50 (rounded to the nearest 1), with RH calculated based on formula:  
```{r, fig.height=9, fig.width=9}

# Sample dataset
rhTrain <- expand.grid(t=seq(-30, 50, by=1), d=seq(-30, 50, by=1)) %>% 
    tibble::as_tibble() %>% 
    filter(d<=t) %>% 
    mutate(rh=calcRH(t, d))
rhTrain

# Training and testing (mtry=1)
rhOut <- rhTrain %>%
    bind_rows(.,.,.,.,.,.,.,.,.,.) %>% 
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("rh", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=1, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m), 
              rndTo=1, 
              returnData=TRUE
              )
rhOut <- rhOut$tstPred
rhOut

# Errors by city
rhOut %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by RH
rhOut %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=25)

```

Training data rounds temperature to the nearest degree and RH always rounds to the nearest percent, making temperature predictions commonly off by a fraction of a degree. The model is generally accurate, with the exception of very low relative humidities (rounding is much more impactful) and very high relative humidities (mtry=1 creates challenges since grid-based training data overweights some T/D combinations). 

The example training data is modified to be more consistent with T/D typically observed:  
```{r, fig.height=9, fig.width=9}

# Sample of T/D in cities
set.seed(24072114)
tdAll <- allCity %>%
    select(t=temperature_2m, d=dewpoint_2m) %>%
    slice(sample(1:nrow(.), round(nrow(.)/10), replace=TRUE)) %>%
    mutate(across(where(is.numeric), .fns=round)) %>%
    count(t, d)
tdAll

# Examples of real-world occurence
tdAll %>%
    ggplot(aes(x=d, y=t)) + 
    geom_point(aes(size=n), alpha=0.25) + 
    labs(title="Sample (10%) of 5-city temperature and dew points")

# Training and testing (mtry=1) weighted by real-world occurence
rhOut_wtd <- rhTrain %>%
    left_join(tdAll, by=c("t", "d")) %>%
    mutate(n=ifelse(is.na(n), 5, n+5)) %>%
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("rh", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=1, 
              case.weights="n",
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m), 
              rndTo=1, 
              returnData=TRUE
              )
rhOut_wtd <- rhOut_wtd$tstPred
rhOut_wtd

# Errors by city
rhOut_wtd %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by RH
rhOut_wtd %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=25)

```

The weighted training data performs slightly better for data points with high density, at the offset of somewhat worse performance for less commonly observed relative humidities

Example training data is expanded all temperatures and dew points between -50 and 50 (rounded to the nearest 1), with RH calculated based on formula:  
```{r, fig.height=9, fig.width=9}

# Sample dataset
rhTrain_ex <- expand.grid(t=seq(-50, 50, by=1), d=seq(-50, 50, by=1)) %>% 
    tibble::as_tibble() %>% 
    filter(d<=t) %>% 
    mutate(rh=calcRH(t, d))
rhTrain_ex

# Sample of T/D in cities from previous code section (frame 'tdAll')
# Training and testing (mtry=1) weighted by real-world occurence
rhOut_wtd_ex <- rhTrain_ex %>%
    left_join(tdAll, by=c("t", "d")) %>%
    mutate(n=ifelse(is.na(n), 5, n+5)) %>%
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("rh", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=1, 
              case.weights="n",
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m), 
              rndTo=1, 
              returnData=TRUE
              )
rhOut_wtd_ex <- rhOut_wtd_ex$tstPred
rhOut_wtd_ex

# Errors by city
rhOut_wtd_ex %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by RH
rhOut_wtd_ex %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=25)

# Errors by RH (plotted)
rhOut_wtd_ex %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    select(rh5, rmse, r2) %>%
    pivot_longer(cols=-c(rh5)) %>%
    ggplot(aes(x=rh5, y=value)) + 
    geom_point(aes(color=name)) + 
    facet_wrap(~name, ncol=1, scales="free_y") + 
    labs(title="R-squared and RMSE of temperature predictions by relative humidity", 
         x="Reported relative humidity (rounded to nearest 5)", 
         y=NULL
         ) + 
    scale_color_discrete(NULL)

```

The expanded training data improves prediction quality at very low temperatures. Predictions continue to be less accurate at very low, and very high, relative humidities

Rounding is a meaningful challenge for some temperature predictions given training data that rounds temperature and dewpoint to the nearest 1:  
```{r, fig.height=9, fig.width=9}

# Prediction error summaries (most of the significant errors occur when RH is 90+)
rhOut_wtd_ex %>% mutate(delta=abs(pred-t)) %>% summary()
rhOut_wtd_ex %>% mutate(delta=abs(pred-t)) %>% filter(delta>1.5) %>% summary()

# Sample dataset
rhTrain_hl <- expand.grid(t=seq(-25, 50, by=0.1), d=seq(-20, 20, by=10)) %>% 
    tibble::as_tibble() %>% 
    filter(d<=t) %>% 
    mutate(rh=calcRH(t, d), rndt=round(t), rndrh=round(rh))
rhTrain_hl

# Examples for rndt==d
rhTrain_hl %>%
    filter(rndt==d) %>%
    print(n=40)

# Examples for rndrh==1
rhTrain_hl %>%
    filter(rndrh<=5) %>%
    group_by(d, rndrh) %>%
    summarize(maxt=max(t), meant=mean(t), mint=min(t), n=n(), .groups="drop")

```

As temperature and dewpoint converge (high relative humidity), the same rounded value of temperature can occur with RH that spans as much as ~4%. Greater granularity in the training data may help address this. As relative humidity gets very low, a given dewpoint can be associated with over 5 degrees of temperature variation for the same rounded value of RH. Since the raw data has rounded RH, this may be a harder constraint, though extremely low relative humidity is uncommon so this may not be a major driver of overall RMSE

Training data is updated to include 0.2 degree granularity for temperature and dewpoint:  
```{r, fig.height=9, fig.width=9}

# Sample dataset
rhTrain_02 <- expand.grid(t=seq(-50, 50, by=0.2), d=seq(-50, 50, by=0.2)) %>% 
    tibble::as_tibble() %>% 
    filter(d<=t) %>% 
    mutate(rh=calcRH(t, d))
rhTrain_02

```
  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Training and testing (mtry=1) - NOT weighted by real-world occurence
rhOut_02 <- rhTrain_02 %>%
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("rh", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=1, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m), 
              rndTo=1, 
              returnData=TRUE
              )
rhOut_02 <- rhOut_02$tstPred
rhOut_02

# Errors by city
rhOut_02 %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by RH
rhOut_02 %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=25)

```

The model performs very well, with the exception of some remaining RMSE mainly for very high RH. Allowing both predictors (RH, D) to be used at the same time is forced using mtry=2:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

rhOut_02_mt2 <- rhTrain_02 %>%
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("rh", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=2, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m), 
              rndTo=1, 
              returnData=TRUE
    )
rhOut_02_mt2 <- rhOut_02_mt2$tstPred
rhOut_02_mt2

```
  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Errors by city
rhOut_02_mt2 %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by RH
rhOut_02_mt2 %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=25)

```

The model performs significantly better for very high RH, with the only meaningful errors at low RH where the impact of rounding (raw data RH is reported to the nearest percent) has the greatest impact

The process is run to predict RH based on temperature and dewpoint, starting with mtry=1:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

predRH_02_mt1 <- rhTrain_02 %>%
    runFullRF(dfTrain=., 
              yVar=c("rh"), 
              xVars=c("t", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=1, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m), 
              rndTo=1, 
              returnData=TRUE
    )
predRH_02_mt1 <- predRH_02_mt1$tstPred
predRH_02_mt1


# Errors by city
predRH_02_mt1 %>%
    group_by(src) %>%
    summarize(e2=mean((rh-pred)**2), mu=mean(rh-pred), n=n(), e2Base=mean((rh-mean(rh))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by RH
predRH_02_mt1 %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((rh-pred)**2), mu=mean(rh-pred), n=n(), e2Base=mean((rh-mean(rh))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=25)

```

The model is inaccurate at high relative humidities but otherwise predicts accurately RH consistent with the known formula

The process is updated to predict RH based on temperature and dewpoint with mtry=2:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

predRH_02_mt2 <- rhTrain_02 %>%
    runFullRF(dfTrain=., 
              yVar=c("rh"), 
              xVars=c("t", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=2, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, rh=relativehumidity_2m), 
              rndTo=1, 
              returnData=TRUE
    )
predRH_02_mt2 <- predRH_02_mt2$tstPred
predRH_02_mt2


# Errors by city
predRH_02_mt2 %>%
    group_by(src) %>%
    summarize(e2=mean((rh-pred)**2), mu=mean(rh-pred), n=n(), e2Base=mean((rh-mean(rh))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by RH
predRH_02_mt2 %>%
    mutate(rh5=round(rh/5)*5) %>%
    group_by(rh5) %>%
    summarize(e2=mean((rh-pred)**2), mu=mean(rh-pred), n=n(), e2Base=mean((rh-mean(rh))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=25)

```

The model is now accurate even at high relative humidities

An approximate formula for vapor pressure deficit is assessed for resonance with the data:  
```{r, fig.height=9, fig.width=9}

# Approximate formula for vapor pressure deficit
# Source https://pulsegrow.com/blogs/learn/vpd
calcVPD <- function(t, d, c1=610.78, c2=17.2694, c3=237.3) {
    # SVP (saturation vapor pressure) = 610.78 * exp(T * 17.2694 / (T + 237.3))
    # VPD = (1 - RH/100) * SVP
    # Formula produces VPD in Pa, divide by 1000 to convert to kPa
    (1 - calcRH(t, d)/100) * c1 * exp(t * c2 / (t + c3)) / 1000
}

# Applied to sample data
dfTestTemp_v3 %>%
    select(src, t=temperature_2m, d=dewpoint_2m, v=vapor_pressure_deficit) %>%
    mutate(cvpd=calcVPD(t, d)) %>%
    ggplot(aes(x=v, y=cvpd)) + 
    geom_point(aes(color=src)) + 
    facet_wrap(~src) + 
    geom_smooth(method="lm") + 
    geom_abline(intercept=0, slope=1, lty=2) + 
    labs(x="Reported vapor pressure deficit (kPa)", 
         y="Formula vapor pressure deficit (kPa)", 
         title="Vapor pressure deficit by formula from temperature and dewpoint vs. reported in raw data") + 
    scale_color_discrete(NULL)

```

The formula is a strong match to the reported data, which should allow the random forest to find the correct third value when given two of T, D, VPD (provided that training space also includes that combination)

Example training data is created for all temperatures and dew points between -50 and 50 (rounded to the nearest 1), with VPD calculated based on formula:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Sample dataset
rhTrainVPD <- expand.grid(t=seq(-50, 50, by=1), d=seq(-50, 50, by=1)) %>% 
    tibble::as_tibble() %>% 
    filter(d<=t) %>% 
    mutate(vpd=calcVPD(t, d))
rhTrainVPD

# Training and testing (mtry=1)
rhOutVPD <- rhTrainVPD %>%
    bind_rows(.,.,.,.,.,.,.,.,.,.) %>% 
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("vpd", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=1, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, vpd=vapor_pressure_deficit), 
              rndTo=1, 
              returnData=TRUE
              )
rhOutVPD <- rhOutVPD$tstPred
rhOutVPD

# Errors by city
rhOutVPD %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by VPD
rhOutVPD %>%
    mutate(vpd_rnd=ifelse(vpd<0.4, round(vpd*20)/20, ifelse(vpd<2, round(vpd*5)/5, round(vpd/1)*1))) %>%
    group_by(vpd_rnd) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=30)

```

Training data rounds temperature to the nearest degree, making temperature predictions commonly off by a fraction of a degree. The model is generally accurate, with the exception of very low/high vapor pressure deficits. 

The model is updated to use mtry=2:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Training and testing (mtry=2)
rhOutVPD_mt2 <- rhTrainVPD %>%
    bind_rows(.,.,.,.,.,.,.,.,.,.) %>% 
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("vpd", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=2, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, vpd=vapor_pressure_deficit), 
              rndTo=2, 
              returnData=TRUE
              )
rhOutVPD_mt2 <- rhOutVPD_mt2$tstPred
rhOutVPD_mt2

# Errors by city
rhOutVPD_mt2 %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by VPD
rhOutVPD_mt2 %>%
    mutate(vpd_rnd=ifelse(vpd<0.4, round(vpd*20)/20, ifelse(vpd<2, round(vpd*5)/5, round(vpd/1)*1))) %>%
    group_by(vpd_rnd) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=30)

```

The model is more accurate, particularly for very low vapor pressure deficits

Training data is updated to include 0.2 degree granularity for temperature and dewpoint:  
```{r, fig.height=9, fig.width=9}

# Sample dataset
rhTrainVPD_02 <- expand.grid(t=seq(-50, 50, by=0.2), d=seq(-50, 50, by=0.2)) %>% 
    tibble::as_tibble() %>% 
    filter(d<=t) %>% 
    mutate(vpd=calcVPD(t, d))
rhTrainVPD_02

```
  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Training and testing (mtry=2)
rhOutVPD_02_mt2 <- rhTrainVPD_02 %>%
    bind_rows(.,.,.,.,.,.,.,.,.,.) %>% 
    runFullRF(dfTrain=., 
              yVar=c("t"), 
              xVars=c("vpd", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=2, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, vpd=vapor_pressure_deficit), 
              rndTo=2, 
              returnData=TRUE
              )
rhOutVPD_02_mt2 <- rhOutVPD_02_mt2$tstPred
rhOutVPD_02_mt2

# Errors by city
rhOutVPD_02_mt2 %>%
    group_by(src) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by VPD
rhOutVPD_02_mt2 %>%
    mutate(vpd_rnd=ifelse(vpd<0.4, round(vpd*20)/20, ifelse(vpd<2, round(vpd*5)/5, round(vpd/1)*1))) %>%
    group_by(vpd_rnd) %>%
    summarize(e2=mean((t-pred)**2), mu=mean(t-pred), n=n(), e2Base=mean((t-mean(t))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=30)

```

Predictions become extremely accurate, at the expense of long run times

The model is run to predict VPD as f(T, D): 
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Training and testing (mtry=2)
predVPD_02_mt2 <- rhTrainVPD_02 %>%
    # bind_rows(.,.,.,.,.,.,.,.,.,.) %>%
    runFullRF(dfTrain=., 
              yVar=c("vpd"), 
              xVars=c("t", "d"), 
              isContVar=TRUE, 
              refXY=TRUE, 
              mtry=2, 
              dfTest=allCity %>%
                  filter(tt=="test") %>%
                  select(src, t=temperature_2m, d=dewpoint_2m, vpd=vapor_pressure_deficit), 
              rndTo=0.025, 
              returnData=TRUE
              )
predVPD_02_mt2 <- predVPD_02_mt2$tstPred
predVPD_02_mt2

# Errors by city
predVPD_02_mt2 %>%
    group_by(src) %>%
    summarize(e2=mean((vpd-pred)**2), mu=mean(vpd-pred), n=n(), e2Base=mean((vpd-mean(vpd))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base)

# Errors by VPD
predVPD_02_mt2 %>%
    mutate(vpd_rnd=ifelse(vpd<0.4, round(vpd*20)/20, ifelse(vpd<2, round(vpd*5)/5, round(vpd/1)*1))) %>%
    group_by(vpd_rnd) %>%
    summarize(e2=mean((vpd-pred)**2), mu=mean(vpd-pred), n=n(), e2Base=mean((vpd-mean(vpd))**2)) %>%
    mutate(rmse=sqrt(e2), r2=1-e2/e2Base, e2pct=n*e2/sum(n*e2)) %>%
    print(n=30)

```

Predictions are very accurate, as expected

A model is run to predict cloud cover, at first allowing the cloud subset data (low, mid, high):  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
rfCloudFull <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022), 
                      yVar="cloudcover", 
                      xVars=c(varsTrain[!str_detect(varsTrain, "cloudcover$")]), 
                      dfTest=allCity %>% filter(tt=="test", year==2022), 
                      useLabel=keyLabel, 
                      useSub=stringr::str_to_sentence(keyLabel), 
                      isContVar=TRUE,
                      rndTo=-1L,
                      refXY=TRUE,
                      returnData=TRUE
                      )

```

The model is effective at predicting cloud cover. Of interest, variable importance is highest for 'weathercode', a categorical variable improperly run as an integer. The interpretation of WMO weather codes is available at https://www.nodc.noaa.gov/archive/arc0021/0002199/1.1/data/0-data/HTML/WMO-CODE/WMO4677.HTM

The weather code and cloud cover variables are explored:  
```{r, fig.height=9, fig.width=9}

# Distribution of 'cloudcover'
allCity %>% 
    select(weathercode, cloudcover) %>% 
    mutate(weathercode=factor(weathercode)) %>% 
    ggplot(aes(x=cloudcover)) + 
    geom_histogram(bins=50, aes(y=after_stat(count)/sum(after_stat(count)))) + 
    labs(title="Distribution of cloud cover", y="Proportion of total observations")

# Distribution of 'weathercode'
allCity %>% 
    select(weathercode, cloudcover) %>% 
    mutate(weathercode=factor(weathercode)) %>% 
    ggplot(aes(x=weathercode)) + 
    geom_bar(aes(y=after_stat(count)/sum(after_stat(count)))) + 
    labs(title="Distribution of weather code", y="Proportion of total observations")

# Cloud cover boxplot by weather code
allCity %>% 
    select(weathercode, cloudcover) %>% 
    mutate(weathercode=factor(weathercode)) %>% 
    ggplot(aes(x=weathercode, y=cloudcover)) + 
    geom_boxplot(fill="lightblue") + 
    labs(title="Cloud cover by weather code", y="Cloud cover")

```

Weather code is strongly predictive of cloud cover, with codes 00 and 01 associated with few clouds and other clouds associated with many clouds

The model is run to predict cloud cover using only weather code as a factor:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
runFullRF(dfTrain=allCity %>% mutate(fct_wmo=factor(weathercode)) %>% filter(tt=="train", year<2022), 
          yVar="cloudcover", 
          xVars=c("fct_wmo"), 
          dfTest=allCity %>% mutate(fct_wmo=factor(weathercode)) %>% filter(tt=="test", year==2022), 
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=TRUE,
          rndTo=-1L,
          refXY=TRUE,
          returnData=FALSE
          )

```

The model drives over 90% R-squared, with RMSE falling from ~35 in the baseline to ~10 with predictions based solely on weather code

The model is run to predict cloud cover using only the three cloud cover (low, mid, high) predictors:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
runFullRF(dfTrain=allCity %>% mutate(fct_wmo=factor(weathercode)) %>% filter(tt=="train", year<2022), 
          yVar="cloudcover", 
          xVars=c(varsTrain[str_detect(varsTrain, pattern="cloudcover_")]), 
          dfTest=allCity %>% mutate(fct_wmo=factor(weathercode)) %>% filter(tt=="test", year==2022), 
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=TRUE,
          rndTo=-1L,
          refXY=TRUE,
          returnData=FALSE
          )

```

The model drives over 99% R-squared, with RMSE falling from ~35 in the baseline to ~3 with predictions based solely on cloud cover sub-types

All combinations of two variables are explored for predicting cloud cover on a smaller training dataset:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Train and test data
dfTrainCloud <- allCity %>% 
    filter(tt=="train", year<2022) %>% 
    mutate(fct_src=factor(src))
dfTestCloud <- allCity %>% 
    filter(tt=="test", year==2022) %>% 
    mutate(fct_src=factor(src))

# Variables to explore
possCloudVars <- c(varsTrain[!str_detect(varsTrain, "cover$")], "month", "tod")

# Subsets to use
set.seed(24080616)
idxSmallCloud <- sample(1:nrow(dfTrainCloud), 5000, replace=FALSE)
mtxSmallCloud <- matrix(nrow=0, ncol=3)

for(idx1 in 1:(length(possCloudVars)-1)) {
    for(idx2 in (idx1+1):length(possCloudVars)) {
        r2SmallCloud <- runFullRF(dfTrain=dfTrainCloud[idxSmallCloud,], 
                                  yVar="cloudcover", 
                                  xVars=possCloudVars[c(idx1, idx2)], 
                                  dfTest=dfTestCloud, 
                                  useLabel=keyLabel, 
                                  useSub=stringr::str_to_sentence(keyLabel), 
                                  isContVar=TRUE,
                                  makePlots=FALSE,
                                  returnData=TRUE
                                  )[["rfAcc"]][["r2"]]
        mtxSmallCloud <- rbind(mtxSmallCloud, c(idx1, idx2, r2SmallCloud))
    }
}

```

Predictive success by metric is explored:  
```{r, fig.height=9, fig.width=9}

dfSmallR2Cloud <- as.data.frame(mtxSmallCloud) %>% 
    purrr::set_names(c("idx1", "idx2", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possCloudVars[idx1], var2=possCloudVars[idx2], rn=row_number()) 
dfSmallR2Cloud %>% arrange(desc(r2)) %>% select(var1, var2, r2) %>% print(n=20)

dfSmallR2Cloud %>% 
    pivot_longer(cols=c(var1, var2)) %>% 
    group_by(value) %>% 
    summarize(across(r2, .fns=list("min"=min, "mu"=mean, "max"=max))) %>% 
    ggplot(aes(x=fct_reorder(value, r2_mu))) + 
    coord_flip() + 
    geom_point(aes(y=r2_mu)) + 
    geom_errorbar(aes(ymin=r2_min, ymax=r2_max)) + 
    lims(y=c(NA, 1)) + 
    geom_hline(yintercept=1, lty=2, color="red") +
    labs(title="R-squared in every 2-predictor model including self and one other", 
         subtitle="Predicting cloud cover", 
         y="Range of R-squared (min-mean-max)", 
         x=NULL
    )

dfSmallR2Cloud %>% 
    arrange(desc(r2)) %>% 
    filter(var2!="weathercode", var1!="weathercode") %>% 
    select(var1, var2, r2) %>% 
    print(n=20)

dfSmallR2Cloud %>% 
    filter(var2!="weathercode", var1!="weathercode") %>% 
    pivot_longer(cols=c(var1, var2)) %>% 
    group_by(value) %>% 
    summarize(across(r2, .fns=list("min"=min, "mu"=mean, "max"=max))) %>% 
    ggplot(aes(x=fct_reorder(value, r2_mu))) + 
    coord_flip() + 
    geom_point(aes(y=r2_mu)) + 
    geom_errorbar(aes(ymin=r2_min, ymax=r2_max)) + 
    lims(y=c(NA, 1)) + 
    geom_hline(yintercept=1, lty=2, color="red") +
    labs(title="R-squared in every 2-predictor model including self and one other", 
         subtitle="Predicting cloud cover (excluding variable paired with 'weathercode')", 
         y="Range of R-squared (min-mean-max)", 
         x=NULL
    )

```

Select combinations are explored using the full training dataset, with mtry=3:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

possLargeVars <- c("weathercode", 
                   "cloudcover_low", 
                   "cloudcover_mid", 
                   "cloudcover_high"
                   )
possLargeVars
mtxLargeCloud <- matrix(nrow=0, ncol=4)

for(idx1 in 1:(length(possLargeVars)-2)) {
    for(idx2 in (idx1+1):(length(possLargeVars)-1)) {
        for(idx3 in (idx2+1):(length(possLargeVars))) {
            r2LargeCloud <- runFullRF(dfTrain=dfTrainCloud[,], 
                                      yVar="cloudcover", 
                                      xVars=possLargeVars[c(idx1, idx2, idx3)], 
                                      dfTest=dfTestCloud, 
                                      useLabel=keyLabel, 
                                      useSub=stringr::str_to_sentence(keyLabel), 
                                      isContVar=TRUE,
                                      mtry=3,
                                      makePlots=FALSE,
                                      returnData=TRUE
                                      )[["rfAcc"]][["r2"]]
            mtxLargeCloud <- rbind(mtxLargeCloud, c(idx1, idx2, idx3, r2LargeCloud))
        }
    }
}

dfLargeR2Cloud <- as.data.frame(mtxLargeCloud) %>% 
    purrr::set_names(c("idx1", "idx2", "idx3", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possLargeVars[idx1], var2=possLargeVars[idx2], var3=possLargeVars[idx3], rn=row_number()) 
dfLargeR2Cloud %>% arrange(desc(r2)) %>% select(var1, var2, var3, r2) %>% print(n=20)

```

The three cloud cover subtypes, in combination, have almost perfect predictive power on overall cloud cover

A linear model is run for comparison:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

lmMiniCloud <- allCity %>% 
    filter(tt=="train", year<2022) %>%
    select(c=cloudcover, l=cloudcover_low, m=cloudcover_mid, h=cloudcover_high) %>%
    lm(c~l*m*h, data=.) 
summary(lmMiniCloud)

ggMiniCloud <- predict(lmMiniCloud, 
                      newdata=allCity %>% 
                          filter(tt=="test", year==2022) %>% 
                          select(c=cloudcover, l=cloudcover_low, m=cloudcover_mid, h=cloudcover_high)
                      ) %>% 
    mutate(select(allCity %>% filter(tt=="test", year==2022), cloudcover), 
           pred=., 
           err=pred-cloudcover, 
           err2=err**2, 
           rnd5=round(cloudcover/5)*5
           ) %>% 
    group_by(rnd5) %>% 
    summarize(n=n(), across(.cols=where(is.numeric), .fns=mean))
ggMiniCloud %>% print(n=25)

ggMiniCloud %>% 
    select(rnd5, cloudcover, pred) %>%
    pivot_longer(cols=-c(rnd5)) %>%
    ggplot(aes(x=rnd5, y=value)) + 
    geom_line(aes(group=name, 
                  color=c("pred"="Predicted Mean", "cloudcover"="Actual Mean")[name]
                  )
              ) + 
    labs(title="Actual vs. Predicted Cloud Cover Using Linear Model on Holdout Data", 
         x="Actual cloud cover (rounded to nearest 5)", 
         y="Average cloud cover for metric"
         ) + 
    scale_color_discrete("Metric") + 
    geom_abline(slope=1, intercept=0, lty=2)

```

The linear model generally makes strong predictions, though with generally lower accuracy on cloudier days. Distribution of errors is explored:  
```{r, fig.height=9, fig.width=9}

predict(lmMiniCloud, 
        newdata=allCity %>% 
            filter(tt=="test", year==2022) %>% 
            select(c=cloudcover, l=cloudcover_low, m=cloudcover_mid, h=cloudcover_high)
        ) %>% 
    mutate(select(allCity %>% filter(tt=="test", year==2022), cloudcover), 
           pred=., 
           err=pred-cloudcover, 
           err2=err**2, 
           rnd5=round(cloudcover/5)*5, 
           rndCat=case_when(cloudcover<10~"1) clear (<10)", 
                            cloudcover<50~"2) partly (10-50)", 
                            cloudcover<90~"3) mostly (50-90)", 
                            TRUE~"4) cloudy (>90)"
                            )
           ) %>%
    ggplot(aes(x=err)) + 
    geom_histogram(fill="lightblue") + 
    labs(title="Errors in linear model cloud cover prediction by amount of clouds", 
         x="Error (Predicted minus Actual)", 
         y="# Observations"
         ) + 
    facet_wrap(~rndCat, scales="free")

```

Predictions for the random forest model are also explored:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

rfSubCloudPred <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022), 
                            yVar="cloudcover", 
                            xVars=c(varsTrain[str_detect(varsTrain, pattern="cloudcover_")]), 
                            dfTest=allCity %>% filter(tt=="test", year==2022), 
                            useLabel=keyLabel, 
                            useSub=stringr::str_to_sentence(keyLabel), 
                            isContVar=TRUE,
                            mtry=3,
                            rndTo=-1L,
                            refXY=TRUE,
                            returnData=TRUE
                            )[["tstPred"]]
rfSubCloudPred

```

Distribution of errors from the random forest model is explored:  
```{r, fig.height=9, fig.width=9}

rfSubCloudPred %>% 
    mutate(err=pred-cloudcover, 
           err2=err**2, 
           rnd5=round(cloudcover/5)*5, 
           rndCat=case_when(cloudcover<10~"1) clear (<10)", 
                            cloudcover<50~"2) partly (10-50)", 
                            cloudcover<90~"3) mostly (50-90)", 
                            TRUE~"4) cloudy (>90)"
                            )
           ) %>%
    ggplot(aes(x=err)) + 
    geom_histogram(fill="lightblue") + 
    labs(title="Errors in linear model cloud cover prediction by amount of clouds", 
         x="Error (Predicted minus Actual)", 
         y="# Observations"
         ) + 
    facet_wrap(~rndCat, scales="free")

```

There are essentially no prediction errors at any level of overall cloudiness

The impact of varying mtry is also explored:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

sapply(1:3, FUN=function(mt) { 
    runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022), 
              yVar="cloudcover", 
              xVars=c(varsTrain[str_detect(varsTrain, pattern="cloudcover_")]), 
              dfTest=allCity %>% filter(tt=="test", year==2022), 
              useLabel=keyLabel, 
              useSub=stringr::str_to_sentence(keyLabel), 
              isContVar=TRUE,
              mtry=mt,
              rndTo=-1L,
              refXY=TRUE,
              makePlots=FALSE,
              returnData=TRUE
              )[["rfAcc"]][["r2"]]
    }
    )

```

With mtry=1 (single variable per tree), R-squared on the test data is slightly over 99%. With mtry=2 or mtry=3, R-squared on the test data is almost exactly 100%

A model is run to predict rain, at first allowing precipitation and snowfall as predictors:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
rfRainFull <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022), 
                      yVar="rain", 
                      xVars=c(varsTrain[!str_detect(varsTrain, "^rain$")]), 
                      dfTest=allCity %>% filter(tt=="test", year==2022), 
                      useLabel=keyLabel, 
                      useSub=stringr::str_to_sentence(keyLabel), 
                      isContVar=TRUE,
                      rndTo=-1L,
                      refXY=TRUE,
                      returnData=TRUE
                      )

```

The model is effective at predicting rain, primarily by leveraging highly associated predictors precipitation, and weather code. The model generally under-predicts high rainfall observations

A similar process is run using the linear model:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Eliminate diffuse radiation due to rank-deficiency
lmRainFull <- lm(rain ~ ., 
                 data=allCity %>% 
                     filter(tt=="train", year<2022) %>% 
                     select(all_of(varsTrain)) %>% 
                     select(-diffuse_radiation)
                 )
summary(lmRainFull)

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(pred=predict(lmRainFull, newdata=.)) %>%
    summarize(meModel=mean((pred-rain)**2), 
              meBase=mean((rain-mean(rain))**2), 
              r2=1-meModel/meBase, 
              rmse=sqrt(meModel)
              )

summary(lmRainFull)$coefficients %>% 
    as.data.frame() %>% 
    rownames_to_column("Variable") %>% 
    tibble::as_tibble() %>% 
    arrange(desc(abs(`t value`)))

```

The linear model has very strong explanatory and predictive power. Rain (mm) appears defined in the raw data as precipitation (mm) minus snowfall (cm) divided by 0.7:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Eliminate diffuse radiation due to rank-deficiency
lmRainTwo <- lm(rain ~ precipitation + snowfall, 
                 data=allCity %>% 
                     filter(tt=="train", year<2022) %>% 
                     select(all_of(varsTrain)) %>% 
                     select(-diffuse_radiation)
                 )
summary(lmRainTwo)

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(pred=predict(lmRainTwo, newdata=.)) %>%
    summarize(meModel=mean((pred-rain)**2), 
              meBase=mean((rain-mean(rain))**2), 
              r2=1-meModel/meBase, 
              rmse=sqrt(meModel)
              )

summary(lmRainTwo)$coefficients %>% 
    as.data.frame() %>% 
    rownames_to_column("Variable") %>% 
    tibble::as_tibble() %>% 
    arrange(desc(abs(`t value`)))

```

The random forest model is re-run to predict rain, using only precipitation and snowfall as predictors:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
rfRainTwo <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022), 
                       yVar="rain", 
                       xVars=c("precipitation", "snowfall"), 
                       dfTest=allCity %>% filter(tt=="test", year==2022), 
                       useLabel=keyLabel, 
                       useSub=stringr::str_to_sentence(keyLabel), 
                       isContVar=TRUE,
                       rndTo=-1L,
                       mtry=2,
                       refXY=TRUE,
                       returnData=TRUE
                       )[["tstPred"]]

```

The random forest model is similarly effective at predicting rain using precipitation and snowfall

All combinations of two variables are explored for predicting rain on a smaller training dataset:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Train and test data created previously (dfTrainCloud and dfTestCloud)
# Variables to explore
possRainVars <- c(varsTrain[!str_detect(varsTrain, "rain")], "month", "tod")

# Subsets to use
set.seed(24081818)
idxSmallRain <- sample(1:nrow(dfTrainCloud), 5000, replace=FALSE)
mtxSmallRain <- matrix(nrow=0, ncol=3)

for(idx1 in 1:(length(possRainVars)-1)) {
    for(idx2 in (idx1+1):length(possRainVars)) {
        r2SmallRain <- runFullRF(dfTrain=dfTrainCloud[idxSmallRain,], 
                                 yVar="rain", 
                                 xVars=possRainVars[c(idx1, idx2)], 
                                 dfTest=dfTestCloud, 
                                 useLabel=keyLabel, 
                                 useSub=stringr::str_to_sentence(keyLabel), 
                                 isContVar=TRUE,
                                 mtry=2,
                                 makePlots=FALSE,
                                 returnData=TRUE
                                 )[["rfAcc"]][["r2"]]
        mtxSmallRain <- rbind(mtxSmallRain, c(idx1, idx2, r2SmallRain))
    }
}

```

Predictive success by metric is explored:  
```{r, fig.height=9, fig.width=9}

dfSmallR2Rain <- as.data.frame(mtxSmallRain) %>% 
    purrr::set_names(c("idx1", "idx2", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possRainVars[idx1], var2=possRainVars[idx2], rn=row_number()) 
dfSmallR2Rain %>% arrange(desc(r2)) %>% select(var1, var2, r2) %>% print(n=20)

dfSmallR2Rain %>% 
    pivot_longer(cols=c(var1, var2)) %>% 
    group_by(value) %>% 
    summarize(across(r2, .fns=list("min"=min, "mu"=mean, "max"=max))) %>% 
    ggplot(aes(x=fct_reorder(value, r2_mu))) + 
    coord_flip() + 
    geom_point(aes(y=r2_mu)) + 
    geom_errorbar(aes(ymin=r2_min, ymax=r2_max)) + 
    lims(y=c(NA, 1)) + 
    geom_hline(yintercept=1, lty=2, color="red") +
    labs(title="R-squared in every 2-predictor model including self and one other", 
         subtitle="Predicting rain", 
         y="Range of R-squared (min-mean-max)", 
         x=NULL
    )

dfSmallR2Rain %>% 
    arrange(desc(r2)) %>% 
    filter(var2!="precipitation", var1!="precipitation") %>% 
    select(var1, var2, r2) %>% 
    print(n=20)

dfSmallR2Rain %>% 
    filter(var2!="precipitation", var1!="precipitation", var2!="weathercode", var1!="weathercode") %>% 
    pivot_longer(cols=c(var1, var2)) %>% 
    group_by(value) %>% 
    summarize(across(r2, .fns=list("min"=min, "mu"=mean, "max"=max))) %>% 
    ggplot(aes(x=fct_reorder(value, r2_mu))) + 
    coord_flip() + 
    geom_point(aes(y=r2_mu)) + 
    geom_errorbar(aes(ymin=r2_min, ymax=r2_max)) + 
    lims(y=c(NA, 1)) + 
    geom_hline(yintercept=1, lty=2, color="red") +
    labs(title="R-squared in every 2-predictor model including self and one other", 
         subtitle="Predicting cloud cover (excluding variable paired with 'precipitation' or 'weathercode')", 
         y="Range of R-squared (min-mean-max)", 
         x=NULL
    )

dfSmallR2Rain %>% 
    arrange(desc(r2)) %>% 
    filter(var2!="precipitation", var1!="precipitation", var2!="weathercode", var1!="weathercode") %>% 
    select(var1, var2, r2) %>% 
    print(n=20)

```

Precipitation and weather code are highly predictive of rainfall, but most other predictors drive zero or even negative R-squared when applied in the test data set

Select combinations are explored using the full training dataset:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

possLargeRain <- c("precipitation", 
                   "weathercode", 
                   "snowfall", 
                   "soil_moisture_0_to_7cm"
                   )
possLargeRain
mtxLargeRain <- matrix(nrow=0, ncol=3)

for(idx1 in 1:(length(possLargeRain)-1)) {
    for(idx2 in (idx1+1):length(possLargeRain)) {
        r2LargeRain <- runFullRF(dfTrain=dfTrainCloud[,], 
                                 yVar="rain", 
                                 xVars=possLargeRain[c(idx1, idx2)], 
                                 dfTest=dfTestCloud,
                                 useLabel=keyLabel, 
                                 useSub=stringr::str_to_sentence(keyLabel), 
                                 isContVar=TRUE,
                                 mtry=2,
                                 makePlots=FALSE,
                                 returnData=TRUE
                                 )[["rfAcc"]][["r2"]]
        mtxLargeRain <- rbind(mtxLargeRain, c(idx1, idx2, r2LargeRain))
    }
}

dfLargeR2Rain <- as.data.frame(mtxLargeRain) %>% 
    purrr::set_names(c("idx1", "idx2", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possLargeRain[idx1], var2=possLargeRain[idx2], rn=row_number()) 
dfLargeR2Rain %>% arrange(desc(r2)) %>% select(var1, var2, r2) %>% print(n=20)

```

In contrast to previous models, R2 for predicting rain is significantly improved by access to a much larger training dataset

A model is run to predict snowfall, at first allowing precipitation and rain as predictors:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
rfSnowFull <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022), 
                        yVar="snowfall", 
                        xVars=c(varsTrain[!str_detect(varsTrain, "^snowfall$")]), 
                        dfTest=allCity %>% filter(tt=="test", year==2022), 
                        useLabel=keyLabel, 
                        useSub=stringr::str_to_sentence(keyLabel), 
                        isContVar=TRUE,
                        rndTo=-1L,
                        refXY=TRUE,
                        returnData=TRUE
                        )

```

The model is reasonably effective at predicting snowfall, primarily by leveraging highly associated predictors precipitation and weather code. The model generally under-predicts high rainfall observations

A similar process is run using the linear model:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Eliminate diffuse radiation due to rank-deficiency
lmSnowFull <- lm(snowfall ~ ., 
                 data=allCity %>% 
                     filter(tt=="train", year<2022) %>% 
                     mutate(weathercode=factor(weathercode)) %>%
                     select(all_of(varsTrain)) %>% 
                     select(-diffuse_radiation)
                 )
summary(lmSnowFull)

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(weathercode=factor(weathercode)) %>%
    mutate(pred=predict(lmSnowFull, newdata=.)) %>%
    summarize(meModel=mean((pred-snowfall)**2), 
              meBase=mean((snowfall-mean(snowfall))**2), 
              r2=1-meModel/meBase, 
              rmse=sqrt(meModel)
              )

summary(lmSnowFull)$coefficients %>% 
    as.data.frame() %>% 
    rownames_to_column("Variable") %>% 
    tibble::as_tibble() %>% 
    arrange(desc(abs(`t value`)))

```

Even with many confounders, the linear model largely identifies that precipitation and rain predict snowfall. As well, the linear model identifies weather codes 71, 73, and 75 which each mean that snow is falling

The linear model has very strong explanatory and predictive power. Snowfall (cm) appears defined in the raw data as 7 * (precipitation (mm) minus rain (mm)):  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Best predictors only
lmSnowTwo <- lm(snowfall ~ precipitation + rain, 
                data=allCity %>% 
                    filter(tt=="train", year<2022) %>% 
                    select(all_of(varsTrain)) %>% 
                    select(-diffuse_radiation)
                )
summary(lmSnowTwo)

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(pred=predict(lmSnowTwo, newdata=.)) %>%
    summarize(meModel=mean((pred-snowfall)**2), 
              meBase=mean((snowfall-mean(snowfall))**2), 
              r2=1-meModel/meBase, 
              rmse=sqrt(meModel)
              )

```

As well, since 'weathercode' indicates whether it is snowing, the combination with precipitation has reasonable predictive power on snowfall:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Precipitation and weather code as factor
lmSnowWCP <- lm(snowfall ~ precipitation:weathercode, 
                data=allCity %>% 
                    mutate(weathercode=factor(weathercode)) %>%
                    filter(tt=="train", year<2022) %>% 
                    select(all_of(varsTrain)) %>% 
                    select(-diffuse_radiation)
                )
summary(lmSnowWCP)

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(weathercode=factor(weathercode)) %>%
    mutate(pred=predict(lmSnowWCP, newdata=.)) %>%
    summarize(meModel=mean((pred-snowfall)**2), 
              meBase=mean((snowfall-mean(snowfall))**2), 
              r2=1-meModel/meBase, 
              rmse=sqrt(meModel)
              )

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(weathercode=factor(weathercode)) %>%
    mutate(pred=predict(lmSnowWCP, newdata=.)) %>%
    mutate(across(.cols=where(is.numeric), .fns=function(x) autoRound(x, rndTo=0.01))) %>%
    count(snowfall, pred) %>%
    ggplot(aes(x=pred, y=snowfall)) + 
    geom_point(aes(size=n)) + 
    labs(title="Actual vs. Predicted Snowfall\n(linear model with precipitation and weather code)") + 
    geom_smooth(method="lm", aes(weight=n)) + 
    geom_abline(slope=1, intercept=0, lty=2, color="red")

```

The relationship between weathercode and precipitation/snow is explored:  
```{r, fig.height=9, fig.width=9}

allCity %>%
    mutate(weathercode=factor(weathercode)) %>%
    group_by(weathercode) %>%
    summarize(across(.cols=c("precipitation", "snowfall"), .fns=mean)) %>%
    pivot_longer(cols=-c(weathercode)) %>%
    ggplot(aes(x=weathercode, y=value)) + 
    geom_col(fill="lightblue") + 
    geom_text(aes(y=value/2, label=round(value, 1)), size=2.5) + 
    facet_wrap(~name, scales="free_y") + 
    labs(title="Average precipitation (mm) and snow (cm) by weathercode", 
         x=NULL, 
         y="Precip (mm) or Snowfall (cm)"
         )

allCity %>%
    filter(precipitation>0) %>%
    mutate(weathercode=factor(weathercode), pctSnow=snowfall/(0.7*precipitation)) %>%
    ggplot(aes(x=weathercode, y=pctSnow)) + 
    geom_boxplot(fill="lightblue") + 
    labs(title="Percent of precipitation as snowfall by weathercode", 
         x=NULL, 
         y="Snowfall (cm) divided by\n(0.7 * precipitation (mm))"
         )

```

While rain is sometimes falling during snow events, in general the precipitation falls mainly or enitrely as snow during weathercode 71, 73, and 75. There is no snowfall under other weathercode

A model is run to predict weathercode, at first allowing precipitation, rain, and snowfall as predictors:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
rfWCFull <- runFullRF(dfTrain=allCity %>% 
                          filter(tt=="train", year<2022) %>% 
                          mutate(weathercode=factor(weathercode)), 
                      yVar="weathercode", 
                      xVars=c(varsTrain[!str_detect(varsTrain, "^weathercode$")]), 
                      dfTest=allCity %>% 
                          filter(tt=="test", year==2022) %>% 
                          mutate(weathercode=factor(weathercode)), 
                      useLabel=keyLabel, 
                      useSub=stringr::str_to_sentence(keyLabel), 
                      isContVar=FALSE,
                      rndTo=-1L,
                      refXY=TRUE,
                      returnData=TRUE
                      )

```

Accuracy is extremely high, with weathercode being strongly linked to cloud cover, precipitation rate, and percentage of precipitation falling as snow

Relationships between precipitation, rain, snow, clouds, and weathercode are further explored:  
```{r, fig.height=9, fig.width=9}

# Precipitation type by weather code
allCity %>% 
    mutate(isSnow=snowfall>0, isRain=rain>0) %>% 
    group_by(isSnow, isRain, weathercode) %>% 
    summarize(n=n(), 
              across(.cols=c("rain", "snowfall", "precipitation", "cloudcover"), .fns=function(x) sum(x)), 
              .groups="drop") %>% 
    mutate(pType=case_when(isSnow & isRain~"SNRA", isSnow & !isRain~"SN", !isSnow & isRain~"RA", TRUE~"None"),
           musn=snowfall/n, 
           mura=rain/n, 
           mucc=cloudcover/n) %>% 
    ggplot(aes(y=pType, x=factor(weathercode))) + 
    geom_tile(aes(fill=n)) + 
    scale_fill_continuous(low="white", high="lightgreen") + 
    geom_text(aes(label=n), size=2.5) + 
    labs(title="Precipitation types by weather code", x="Weather code", y="Precipitation type")

# Cloud cover by weather code
allCity %>% 
    ggplot(aes(x=factor(weathercode), y=cloudcover)) + 
    geom_boxplot(fill="lightblue") + 
    labs(title="Cloud cover by weather code", x="Weather code", y="Cloud cover (%)")

# Rain by weather code
allCity %>% 
    ggplot(aes(x=factor(weathercode), y=rain)) + 
    geom_boxplot(fill="lightblue") + 
    labs(title="Rain by weather code", x="Weather code", y="Rain (mm)")

# Snow by weather code
allCity %>% 
    ggplot(aes(x=factor(weathercode), y=snowfall)) + 
    geom_boxplot(fill="lightblue") + 
    labs(title="Snow by weather code", x="Weather code", y="Snow (cm)")

# Table of results
allCity %>% 
    mutate(isSnow=snowfall>0, isRain=rain>0) %>% 
    group_by(isSnow, isRain, weathercode) %>% 
    summarize(n=n(), 
              maxcc=max(cloudcover), 
              mincc=min(cloudcover),
              across(.cols=c("rain", "snowfall", "precipitation", "cloudcover"), .fns=function(x) sum(x)),
              .groups="drop") %>% 
    mutate(pType=case_when(isSnow & isRain~"SNRA", isSnow & !isRain~"SN", !isSnow & isRain~"RA", TRUE~"None"),
           muprecip=precipitation/n, 
           musn=snowfall/n, 
           mura=rain/n, 
           mucc=cloudcover/n
           ) %>%
    select(-rain, -snowfall, -precipitation, -cloudcover) %>%
    select(weathercode, pType, isSnow, isRain, n, mincc, mucc, maxcc, everything())

```

Weather codes appear to be defined as:  
  
* 0 - no precipitation, cloud cover under 20%  
* 1 - no precipitation, cloud cover 20% to 50%  
* 2 - no precipitation, cloud cover 50% to 80%  
* 3 - no precipitation, cloud cover over 80%  
* 51 - very light drizzle, no snow  
* 53 - light drizzle, no snow  
* 55 - drizzle, no snow  
* 61 - light rain, no snow  
* 63 - rain, no snow  
* 65 - heavy rain, no snow  
* 71 - light snow, with or without rain  
* 73 - snow, with or without rain  
* 75 - heavy snow, with or without rain  
  
Not surprisingly, the random forest is effective at pulling apart very clean data splits like these

A model is run to predict weathercode, using only cloud cover, precipitation, rain, and snowfall as predictors, and with random forest defaults (mtry=2 for 4 predictors):  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022) %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("cloudcover", "precipitation", "rain", "snowfall"), 
          dfTest=allCity %>% filter(tt=="test", year==2022) %>% mutate(weathercode=factor(weathercode)), 
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          rndTo=-1L,
          refXY=TRUE,
          returnData=FALSE
          )

```

Accuracy remains over 99%, with main errors being cloudiness classification when there is zero precipitation

The model is updated using mtry=4 and mtry=1:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022) %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("cloudcover", "precipitation", "rain", "snowfall"), 
          dfTest=allCity %>% filter(tt=="test", year==2022) %>% mutate(weathercode=factor(weathercode)), 
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          rndTo=-1L,
          mtry=1,
          refXY=TRUE,
          returnData=FALSE
          )

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022) %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("cloudcover", "precipitation", "rain", "snowfall"), 
          dfTest=allCity %>% filter(tt=="test", year==2022) %>% mutate(weathercode=factor(weathercode)), 
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          rndTo=-1L,
          mtry=4,
          refXY=TRUE,
          returnData=FALSE
          )

```

The model performs with much lower accuracy for mtry=1 rather than mtry=2, but accuracy is essentially identical (over 99%) for mtry=2 and mtry=4. The main issue with mtry=1 is inability to classify non-precipitation days since snowfall, rain, and precipitation as stand-alones do not distinguish degree of cloudiness

All combinations of two variables are explored for predicting weathercode on a smaller training dataset:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Train and test data created previously (dfTrainCloud and dfTestCloud)
# Variables to explore
possWCVars <- c(varsTrain[!str_detect(varsTrain, "weathercode")], "month", "tod")

# Subsets to use
set.seed(24083015)
idxSmallWC <- sample(1:nrow(dfTrainCloud), 5000, replace=FALSE)
mtxSmallWC <- matrix(nrow=0, ncol=3)

for(idx1 in 1:(length(possWCVars)-1)) {
    for(idx2 in (idx1+1):length(possWCVars)) {
        r2SmallWC <- runFullRF(dfTrain=dfTrainCloud[idxSmallWC,] %>% mutate(weathercode=factor(weathercode)), 
                               yVar="weathercode", 
                               xVars=possWCVars[c(idx1, idx2)], 
                               dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)), 
                               useLabel=keyLabel, 
                               useSub=stringr::str_to_sentence(keyLabel), 
                               isContVar=FALSE,
                               mtry=2,
                               makePlots=FALSE,
                               returnData=TRUE
                               )[["rfAcc"]]
        mtxSmallWC <- rbind(mtxSmallWC, c(idx1, idx2, r2SmallWC))
    }
}

```

Accuracy by pairs of metrics is explored:  
```{r, fig.height=9, fig.width=9}

dfSmallR2WC <- as.data.frame(mtxSmallWC) %>% 
    purrr::set_names(c("idx1", "idx2", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possWCVars[idx1], var2=possWCVars[idx2], rn=row_number()) 
dfSmallR2WC %>% arrange(desc(r2)) %>% select(var1, var2, r2) %>% print(n=20)

dfSmallR2WC %>% 
    pivot_longer(cols=c(var1, var2)) %>% 
    group_by(value) %>% 
    summarize(across(r2, .fns=list("min"=min, "mu"=mean, "max"=max))) %>% 
    ggplot(aes(x=fct_reorder(value, r2_mu))) + 
    coord_flip() + 
    geom_point(aes(y=r2_mu)) + 
    geom_errorbar(aes(ymin=r2_min, ymax=r2_max)) + 
    lims(y=c(NA, 1)) + 
    geom_hline(yintercept=1, lty=2, color="red") +
    labs(title="Accuracy in every 2-predictor model including self and one other", 
         subtitle="Predicting weathercode", 
         y="Range of accuracy (min-mean-max)", 
         x=NULL
    )

dfSmallR2WC %>% 
    arrange(desc(r2)) %>% 
    filter(!str_detect(var2, "cloudcover"), !str_detect(var1, "cloudcover")) %>% 
    select(var1, var2, r2) %>% 
    print(n=20)

dfSmallR2WC %>% 
    filter(var2!="precipitation", 
           var1!="precipitation", 
           !str_detect(var2, "cloudcover"), 
           !str_detect(var1, "cloudcover")
           ) %>% 
    pivot_longer(cols=c(var1, var2)) %>% 
    group_by(value) %>% 
    summarize(across(r2, .fns=list("min"=min, "mu"=mean, "max"=max))) %>% 
    ggplot(aes(x=fct_reorder(value, r2_mu))) + 
    coord_flip() + 
    geom_point(aes(y=r2_mu)) + 
    geom_errorbar(aes(ymin=r2_min, ymax=r2_max)) + 
    lims(y=c(NA, 1)) + 
    geom_hline(yintercept=1, lty=2, color="red") +
    labs(title="Accuracy in every 2-predictor model including self and one other", 
         subtitle="Predicting weathercode (excluding variable paired with 'precipitation' or 'cloudcover')", 
         y="Range of accuracy (min-mean-max)", 
         x=NULL
    )

dfSmallR2WC %>% 
    arrange(desc(r2)) %>% 
    filter(!str_detect(var2, "rain|snow|precip"), 
           !str_detect(var1, "rain|snow|precip"),
           !str_detect(var2, "cloudcover"), 
           !str_detect(var1, "cloudcover")
           ) %>% 
    select(var1, var2, r2) %>% 
    print(n=20)

# Null accuracy would pick the most frequent observation
allCity %>% count(weathercode, sort=TRUE) %>% mutate(pct=n/sum(n))

```

Cloud cover and precipitation are highly predictive of weathercode, with most other variables having little explanatory power (accuracy near or even below the ~50% baseline for predicting everything as weathercode 0)

Select combinations are explored using the full training dataset:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

possLargeWC <- c("precipitation", "rain", "snowfall", "cloudcover")
possLargeWC
mtxLargeWC <- matrix(nrow=0, ncol=3)

for(idx1 in 1:(length(possLargeWC)-1)) {
    for(idx2 in (idx1+1):length(possLargeWC)) {
        r2LargeWC <- runFullRF(dfTrain=dfTrainCloud[,] %>% mutate(weathercode=factor(weathercode)), 
                               yVar="weathercode", 
                               xVars=possLargeWC[c(idx1, idx2)], 
                               dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)),
                               useLabel=keyLabel, 
                               useSub=stringr::str_to_sentence(keyLabel), 
                               isContVar=FALSE,
                               mtry=2,
                               makePlots=FALSE,
                               returnData=TRUE
                               )[["rfAcc"]]
        mtxLargeWC <- rbind(mtxLargeWC, c(idx1, idx2, r2LargeWC))
    }
}

dfLargeR2WC <- as.data.frame(mtxLargeWC) %>% 
    purrr::set_names(c("idx1", "idx2", "r2")) %>% 
    tibble::as_tibble() %>% 
    mutate(var1=possLargeWC[idx1], var2=possLargeWC[idx2], rn=row_number()) 
dfLargeR2WC %>% arrange(desc(r2)) %>% select(var1, var2, r2) %>% print(n=20)

```

Accuracy by type of weathercode is further explored for one subset (cloud cover and rain):  
```{r, fig.height=9, fig.width=9, cache=TRUE}

runFullRF(dfTrain=dfTrainCloud[,] %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("cloudcover", "rain"), 
          dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)),
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          mtry=2,
          makePlots=TRUE,
          returnData=TRUE
          )[["tstPred"]] %>%
    mutate(wcType=case_when(weathercode %in% c(0, 1, 2, 3)~"No Precip", 
                            weathercode %in% c(51, 53, 55, 61, 63, 65)~"Rain only", 
                            weathercode %in% c(71, 73, 75)~"Snow", 
                            TRUE~"Other"
                            )
           ) %>% 
    group_by(wcType) %>%
    summarize(acc=mean(weathercode==pred))

```

The model with cloud cover and rain as predictors is very accurate at predicting weathercode, with the exception of zero accuracy during snowfall (71, 73, 75)

Accuracy by type of weathercode is further explored for another subset (cloud cover and snow):  
```{r, fig.height=9, fig.width=9, cache=TRUE}

runFullRF(dfTrain=dfTrainCloud[,] %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("cloudcover", "snowfall"), 
          dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)),
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          mtry=2,
          makePlots=TRUE,
          returnData=TRUE
          )[["tstPred"]] %>%
    mutate(wcType=case_when(weathercode %in% c(0, 1, 2, 3)~"No Precip", 
                            weathercode %in% c(51, 53, 55, 61, 63, 65)~"Rain only", 
                            weathercode %in% c(71, 73, 75)~"Snow", 
                            TRUE~"Other"
                            )
           ) %>% 
    group_by(wcType) %>%
    summarize(acc=mean(weathercode==pred))

```

The model with cloud cover and snow as predictors is very accurate at predicting weathercode, with the exception of zero accuracy during rain events without snowfall (51, 53, 55, 61, 63, 65)

Accuracy by type of weathercode is further explored for a third subset (rain and snow):  
```{r, fig.height=9, fig.width=9, cache=TRUE}

runFullRF(dfTrain=dfTrainCloud[,] %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("rain", "snowfall"), 
          dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)),
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          mtry=2,
          makePlots=TRUE,
          returnData=TRUE
          )[["tstPred"]] %>%
    mutate(wcType=case_when(weathercode %in% c(0, 1, 2, 3)~"No Precip", 
                            weathercode %in% c(51, 53, 55, 61, 63, 65)~"Rain only", 
                            weathercode %in% c(71, 73, 75)~"Snow", 
                            TRUE~"Other"
                            )
           ) %>% 
    group_by(wcType) %>%
    summarize(acc=mean(weathercode==pred))

```

The model with rain and snow as predictors is very accurate at predicting weathercode during precipitation events, but can do no better than predicting the null '0' for no-precipitation observations (0, 1, 2, 3)

Accuracy by type of weathercode is explored for the three main predictors with mtry=1:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

runFullRF(dfTrain=dfTrainCloud[,] %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("rain", "snowfall", "cloudcover"), 
          dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)),
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          mtry=1,
          makePlots=TRUE,
          returnData=TRUE
          )[["tstPred"]] %>%
    mutate(wcType=case_when(weathercode %in% c(0, 1, 2, 3)~"No Precip", 
                            weathercode %in% c(51, 53, 55, 61, 63, 65)~"Rain only", 
                            weathercode %in% c(71, 73, 75)~"Snow", 
                            TRUE~"Other"
                            )
           ) %>% 
    group_by(wcType) %>%
    summarize(acc=mean(weathercode==pred))

```

The model with three predictors and mtry=1 drives ~90% accuracy, lowest during "rain only" events

Accuracy by type of weathercode is explored for the three main predictors with mtry=2:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

runFullRF(dfTrain=dfTrainCloud[,] %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("rain", "snowfall", "cloudcover"), 
          dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)),
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          mtry=2,
          makePlots=TRUE,
          returnData=TRUE
          )[["tstPred"]] %>%
    mutate(wcType=case_when(weathercode %in% c(0, 1, 2, 3)~"No Precip", 
                            weathercode %in% c(51, 53, 55, 61, 63, 65)~"Rain only", 
                            weathercode %in% c(71, 73, 75)~"Snow", 
                            TRUE~"Other"
                            )
           ) %>% 
    group_by(wcType) %>%
    summarize(acc=mean(weathercode==pred))

```

The model with three predictors and mtry=2 drives ~99% accuracy, strong during all three event types


Accuracy by type of weathercode is explored for the three main predictors with mtry=3:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

runFullRF(dfTrain=dfTrainCloud[,] %>% mutate(weathercode=factor(weathercode)), 
          yVar="weathercode", 
          xVars=c("rain", "snowfall", "cloudcover"), 
          dfTest=dfTestCloud %>% mutate(weathercode=factor(weathercode)),
          useLabel=keyLabel, 
          useSub=stringr::str_to_sentence(keyLabel), 
          isContVar=FALSE,
          mtry=3,
          makePlots=TRUE,
          returnData=TRUE
          )[["tstPred"]] %>%
    mutate(wcType=case_when(weathercode %in% c(0, 1, 2, 3)~"No Precip", 
                            weathercode %in% c(51, 53, 55, 61, 63, 65)~"Rain only", 
                            weathercode %in% c(71, 73, 75)~"Snow", 
                            TRUE~"Other"
                            )
           ) %>% 
    group_by(wcType) %>%
    summarize(acc=mean(weathercode==pred))

```

The model with three predictors and mtry=3 drives the same accuracy as the model with three predictors and mtry=2

A model is run to predict ground-level wind speed, at first allowing high-level wind speed as a predictor:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

keyLabel <- "predictions based on pre-2022 training data applied to 2022 holdout dataset"
rfWindFull <- runFullRF(dfTrain=allCity %>% filter(tt=="train", year<2022), 
                      yVar="windspeed_10m", 
                      xVars=c(varsTrain[!str_detect(varsTrain, "^windspeed_10m$")]), 
                      dfTest=allCity %>% filter(tt=="test", year==2022), 
                      useLabel=keyLabel, 
                      useSub=stringr::str_to_sentence(keyLabel), 
                      isContVar=TRUE,
                      rndTo=-1L,
                      refXY=TRUE,
                      returnData=TRUE
                      )

```

The model is effective (~95% R-squared) at predicting ground-level wind speed, primarily by leveraging high-level wind speed and ground-level wind gusts

Correlations between predictors and ground wind speed are assessed:  
```{r, fig.height=9, fig.width=9}

sapply(varsTrain, FUN=function(x) cor(allCity$windspeed_10m, allCity[[x]])) %>% 
    as.data.frame() %>% 
    rownames_to_column("var") %>% 
    tibble::as_tibble() %>% 
    purrr::set_names(c("var", "cor")) %>% 
    ggplot(aes(x=fct_reorder(var, cor), y=cor)) + 
    geom_col(fill="lightblue") + 
    geom_text(data=~filter(., abs(cor)>0.2), aes(y=cor/2, label=round(cor, 2)), size=2.5) +
    coord_flip() + 
    labs(title="Correlation with ground wind speed (windspeed_10m)", 
         y="Correlation", 
         x=NULL
         ) + 
    lims(y=c(NA, 1))

allCity %>%
    select(windspeed_10m, windspeed_100m, windgusts_10m) %>%
    mutate(across(.cols=everything(), .fns=function(x) round(x)), rn=row_number()) %>%
    pivot_longer(cols=-c(rn, windspeed_10m)) %>%
    count(windspeed_10m, name, value) %>%
    ggplot(aes(x=value, y=windspeed_10m)) + 
    geom_point(aes(size=n), alpha=0.5) + 
    geom_smooth(aes(weight=n), method="lm") +
    facet_wrap(~name) + 
    labs(x=NULL, title="Ground-level (10m) windspeed vs. two strong predictors")

```

The linear model is run for ground wind speed, using all predictors:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Eliminate diffuse radiation due to rank-deficiency
lmWindFull <- lm(windspeed_10m ~ ., 
                 data=allCity %>% 
                     filter(tt=="train", year<2022) %>% 
                     mutate(weathercode=factor(weathercode)) %>%
                     select(all_of(varsTrain)) %>% 
                     select(-diffuse_radiation)
                 )
summary(lmWindFull)

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(weathercode=factor(weathercode)) %>%
    mutate(pred=predict(lmWindFull, newdata=.)) %>%
    summarize(meModel=mean((pred-windspeed_10m)**2), 
              meBase=mean((windspeed_10m-mean(windspeed_10m))**2), 
              r2=1-meModel/meBase, 
              rmse=sqrt(meModel)
              )

summary(lmWindFull)$coefficients %>% 
    as.data.frame() %>% 
    rownames_to_column("Variable") %>% 
    tibble::as_tibble() %>% 
    arrange(desc(abs(`t value`)))

```

Even with many confounders, the linear model largely identifies that high-level wind-speed is a strong predictor for ground-level wind speed. Many other variables have statistically significant impact also, with wind gusts, apparent temperature, and actual temperature being of interest

The model is re-run using only the best four predictors:  
```{r, fig.height=9, fig.width=9, cache=TRUE}

# Best predictors only
lmWindFour <- lm(windspeed_10m ~ windspeed_100m + windgusts_10m + apparent_temperature + temperature_2m, 
                 data=allCity %>% 
                     filter(tt=="train", year<2022) %>% 
                     select(all_of(varsTrain)) %>% 
                     select(-diffuse_radiation)
                 )
summary(lmWindFour)

allCity %>% 
    filter(tt=="test", year==2022) %>%
    mutate(pred=predict(lmWindFour, newdata=.)) %>%
    summarize(meModel=mean((pred-windspeed_10m)**2), 
              meBase=mean((windspeed_10m-mean(windspeed_10m))**2), 
              r2=1-meModel/meBase, 
              rmse=sqrt(meModel)
              )

```

The four-predictor model retains most, but not all, of the explanatory power of the full model. R2 on the test dataset falls from ~94% to ~92% (RMSE increases from ~1.9 to ~2.2)
